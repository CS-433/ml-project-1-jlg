{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers2 import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_folder = Path(\"../data/\")\n",
    "DATA_TRAIN_PATH = \"../data/train.csv\"\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "(250000,)\n",
      "(250000, 30)\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(np.shape(y))\n",
    "print(np.shape(tX))\n",
    "print(tX.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 DER_mass_MMC\n",
      "1 DER_mass_transverse_met_lep\n",
      "2 DER_mass_vis\n",
      "3 DER_pt_h\n",
      "4 DER_deltaeta_jet_jet\n",
      "5 DER_mass_jet_jet\n",
      "6 DER_prodeta_jet_jet\n",
      "7 DER_deltar_tau_lep\n",
      "8 DER_pt_tot\n",
      "9 DER_sum_pt\n",
      "10 DER_pt_ratio_lep_tau\n",
      "11 DER_met_phi_centrality\n",
      "12 DER_lep_eta_centrality\n",
      "13 PRI_tau_pt\n",
      "14 PRI_tau_eta\n",
      "15 PRI_tau_phi\n",
      "16 PRI_lep_pt\n",
      "17 PRI_lep_eta\n",
      "18 PRI_lep_phi\n",
      "19 PRI_met\n",
      "20 PRI_met_phi\n",
      "21 PRI_met_sumet\n",
      "22 PRI_jet_num\n",
      "23 PRI_jet_leading_pt\n",
      "24 PRI_jet_leading_eta\n",
      "25 PRI_jet_leading_phi\n",
      "26 PRI_jet_subleading_pt\n",
      "27 PRI_jet_subleading_eta\n",
      "28 PRI_jet_subleading_phi\n",
      "29 PRI_jet_all_pt\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', \n",
    "                 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', \n",
    "                 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', \n",
    "                 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', \n",
    "                 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi',\n",
    "                 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(i, feature_names[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set is composed of : \n",
    "* a y vector of length 250'000 and type float\n",
    "* a tX float matrix of 250'000 rows and 30 columns\n",
    "\n",
    "It means that our data set is composed of 250'000 different obsevations of 30 different features. In the rest of the notebook, we name the features by their index nummer. So, it means from the feature 0 from the feature  29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAc4UlEQVR4nO3dfbRddZ3f8ffHZIIPFBLIHQaT1ASND4GpircYx1UfCBMCOiazijapluCkpgpYa2eqQdvBpdKCtTKyRGYykCGgQ6DxgcwYmok8lGVLgIsoEBBzCWBuBHIlD0iRYODbP/b3OtuT87sP59x7bkg+r7XOOnt/f7/f3t+zb3K+Zz+csxURmJmZNfOS8U7AzMwOXC4SZmZW5CJhZmZFLhJmZlbkImFmZkUuEmZmVuQiYQc9SZslvWu88xhPkv5Y0jZJT0t683jnYy8eLhL2oibpEUmnNMTOkvSDgfmIOD4ibhliOTMlhaSJY5TqePsycG5EHB4Rd493Mvbi4SJh1gEHQPF5FbB5nHOwFyEXCTvo1fc2JJ0kqUfSU5KekPSV7HZrPu/OQzJvk/QSSf9Z0qOSdki6StKRteWemW1PSvovDev5nKS1kr4h6SngrFz3bZJ2S3pM0tckTaotLySdLWmLpF9K+oKkV0v6v5nvdfX+Da+xaa6SDpP0NDAB+LGkh5qMvVTS/2iIrZP0yTY2ux0kXCTsUPNV4KsRcQTwauC6jL8jnyfnIZnbgLPy8W7gOOBw4GsAkuYAXwc+CBwLHAlMa1jXQmAtMBn4JvA88ElgKvA2YB5wdsOYU4G3AHOBTwErgQ8BM4ATgCWF19U014jYGxGHZ583RsSrm4xdDSyR9JJ8bVOBU4C/LazLDiEuEnYw+G5+Ot8taTfVm3fJr4HXSJoaEU9HxKZB+n4Q+EpEbI2Ip4HzgMV56OgM4O8i4gcR8Rzw50DjD6HdFhHfjYgXIuJXEXFXRGyKiH0R8QjwV8A7G8Z8KSKeiojNwH3AP+T69wA3AKWTzoPlOqiIuAPYQ1W0ABYDt0TEE0ONtYOfi4QdDBZFxOSBB/t/Oq9bBrwW+ImkOyW9d5C+rwQerc0/CkwEjsm2bQMNEfEM8GTD+G31GUmvlfT3kh7PQ1D/lWqvoq7+xvyrJvOH09xguQ7Haqo9FvL56mGOs4Oci4QdUiJiS0QsAX4XuAhYK+kV7L8XAPBzqhO+A/4psI/qjfsxYPpAg6SXAUc3rq5h/jLgJ8DsPNz1GUCtv5ph5zoc3wAWSnoj8Abgu6OUl73IuUjYIUXShyR1RcQLwO4MvwD05/Nxte7XAJ+UNEvS4VSf/K+NiH1U5xr+SNIf5MnkzzH0G/4/AZ4Cnpb0euBjo/Syhsp1SBHRB9xJtQfxrYj41SjmZi9iLhJ2qFkAbM4rfr4KLM7zBc8AFwD/J89tzAVWUb1p3go8DDwLfBwgzxl8HFhDtVfxNLAD2DvIuv8M+NfAL4G/Bq4dxddVzHUEVgO/jw81WY180yGz9uWn991Uh5IeHud0WiLpHVSHnV4VfmOw5D0JsxZJ+iNJL89zGl8G7gUeGd+sWiPpd4BPAJe7QFidi4RZ6xZSnTD+OTCb6tDVi+4NVtIbqPaCjgX+YlyTsQOODzeZmVmR9yTMzKxovH90bNRNnTo1Zs6cOd5pmJm9qNx1112/iIiuxvhBVyRmzpxJT0/PeKdhZvaiIunRZnEfbjIzsyIXCTMzK3KRMDOzIhcJMzMrcpEwM7MiFwkzMytykTAzsyIXCTMzK3KRMDOzooPuG9ftmLnie+O27kcufM+4rdvMrMR7EmZmVjRkkZC0StIOSfc1xD8u6SeSNkv6Ui1+nqReSQ9KOrUWX5CxXkkravFZkm7P+LV5v2AkHZbzvdk+c1ResZmZDdtw9iSupLov8G9IejfVDVfeGBHHU92VC0lzgMXA8Tnm65ImSJoAXAqcBswBlmRfgIuAiyPiNcAuYFnGlwG7Mn5x9jMzsw4askhExK3Azobwx4ALI2Jv9tmR8YXAmojYm/f57QVOykdvRGyNiOeobh6/UJKAk4G1OX41sKi2rNU5vRaYl/3NzKxDWj0n8VrgX+RhoP8t6Z9nfBqwrdavL2Ol+NHA7ojY1xD/rWVl+57svx9JyyX1SOrp7+9v8SWZmVmjVovEROAoYC7wn4DrxvNTfkSsjIjuiOju6trvnhlmZtaiVotEH/DtqNwBvABMBbYDM2r9pmesFH8SmCxpYkOc+phsPzL7m5lZh7RaJL4LvBtA0muBScAvgHXA4rwyaRYwG7gDuBOYnVcyTaI6ub0uIgK4GTgjl7sUuD6n1+U82X5T9jczsw4Z8st0kq4B3gVMldQHnA+sAlblZbHPAUvzDXyzpOuA+4F9wDkR8Xwu51xgAzABWBURm3MVnwbWSPoicDdwRcavAK6W1Et14nzxKLxeMzMbgSGLREQsKTR9qND/AuCCJvH1wPom8a1UVz81xp8F3j9UfmZmNnb8jWszMytykTAzsyIXCTMzK3KRMDOzIhcJMzMrcpEwM7MiFwkzMytykTAzsyIXCTMzK3KRMDOzIhcJMzMrcpEwM7MiFwkzMytykTAzsyIXCTMzK3KRMDOzoiGLhKRVknbkXega2/5UUkiamvOSdImkXkn3SDqx1neppC35WFqLv0XSvTnmEknK+FGSNmb/jZKmjM5LNjOz4RrOnsSVwILGoKQZwHzgZ7XwaVT3tZ4NLAcuy75HUd329K1Ud6E7v/amfxnwkdq4gXWtAG6MiNnAjTlvZmYdNGSRiIhbqe4x3ehi4FNA1GILgauisgmYLOlY4FRgY0TsjIhdwEZgQbYdERGb8h7ZVwGLastandOra3EzM+uQls5JSFoIbI+IHzc0TQO21eb7MjZYvK9JHOCYiHgspx8Hjhkkn+WSeiT19Pf3j/TlmJlZwYiLhKSXA58B/nz002ku9zJikPaVEdEdEd1dXV2dSsvM7KDXyp7Eq4FZwI8lPQJMB34o6feA7cCMWt/pGRssPr1JHOCJPBxFPu9oIVczM2vDiItERNwbEb8bETMjYibVIaITI+JxYB1wZl7lNBfYk4eMNgDzJU3JE9bzgQ3Z9pSkuXlV05nA9bmqdcDAVVBLa3EzM+uQ4VwCew1wG/A6SX2Slg3SfT2wFegF/ho4GyAidgJfAO7Mx+czRva5PMc8BNyQ8QuBP5S0BTgl583MrIMmDtUhIpYM0T6zNh3AOYV+q4BVTeI9wAlN4k8C84bKz8zMxo6/cW1mZkUuEmZmVuQiYWZmRS4SZmZW5CJhZmZFLhJmZlbkImFmZkUuEmZmVuQiYWZmRS4SZmZW5CJhZmZFLhJmZlbkImFmZkUuEmZmVuQiYWZmRS4SZmZWNJw7062StEPSfbXYf5f0E0n3SPqOpMm1tvMk9Up6UNKptfiCjPVKWlGLz5J0e8avlTQp44flfG+2zxytF21mZsMznD2JK4EFDbGNwAkR8c+AnwLnAUiaAywGjs8xX5c0QdIE4FLgNGAOsCT7AlwEXBwRrwF2AQO3R10G7Mr4xdnPzMw6aMgiERG3AjsbYv8QEftydhMwPacXAmsiYm9EPEx13+qT8tEbEVsj4jlgDbBQkoCTgbU5fjWwqLas1Tm9FpiX/c3MrENG45zEnwA35PQ0YFutrS9jpfjRwO5awRmI/9aysn1P9jczsw5pq0hI+iywD/jm6KTTch7LJfVI6unv7x/PVMzMDiotFwlJZwHvBT4YEZHh7cCMWrfpGSvFnwQmS5rYEP+tZWX7kdl/PxGxMiK6I6K7q6ur1ZdkZmYNWioSkhYAnwLeFxHP1JrWAYvzyqRZwGzgDuBOYHZeyTSJ6uT2uiwuNwNn5PilwPW1ZS3N6TOAm2rFyMzMOmDiUB0kXQO8C5gqqQ84n+pqpsOAjXkueVNEfDQiNku6Drif6jDUORHxfC7nXGADMAFYFRGbcxWfBtZI+iJwN3BFxq8ArpbUS3XifPEovF4zMxuBIYtERCxpEr6iSWyg/wXABU3i64H1TeJbqa5+aow/C7x/qPzMzGzs+BvXZmZW5CJhZmZFLhJmZlbkImFmZkUuEmZmVuQiYWZmRS4SZmZW5CJhZmZFLhJmZlbkImFmZkUuEmZmVuQiYWZmRS4SZmZW5CJhZmZFLhJmZlbkImFmZkUuEmZmVjRkkZC0StIOSffVYkdJ2ihpSz5PybgkXSKpV9I9kk6sjVma/bdIWlqLv0XSvTnmEuX9UEvrMDOzzhnOnsSVwIKG2ArgxoiYDdyY8wCnAbPzsRy4DKo3fKp7Y7+V6lal59fe9C8DPlIbt2CIdZiZWYcMWSQi4lZgZ0N4IbA6p1cDi2rxq6KyCZgs6VjgVGBjROyMiF3ARmBBth0REZsiIoCrGpbVbB1mZtYhrZ6TOCYiHsvpx4FjcnoasK3Wry9jg8X7msQHW8d+JC2X1COpp7+/v4WXY2ZmzbR94jr3AGIUcml5HRGxMiK6I6K7q6trLFMxMzuktFoknshDReTzjoxvB2bU+k3P2GDx6U3ig63DzMw6pNUisQ4YuEJpKXB9LX5mXuU0F9iTh4w2APMlTckT1vOBDdn2lKS5eVXTmQ3LarYOMzPrkIlDdZB0DfAuYKqkPqqrlC4ErpO0DHgU+EB2Xw+cDvQCzwAfBoiInZK+ANyZ/T4fEQMnw8+muoLqZcAN+WCQdZiZWYcMWSQiYkmhaV6TvgGcU1jOKmBVk3gPcEKT+JPN1mFmZp3jb1ybmVmRi4SZmRW5SJiZWZGLhJmZFblImJlZkYuEmZkVuUiYmVmRi4SZmRW5SJiZWZGLhJmZFblImJlZkYuEmZkVDfkDf2ZmNnwzV3xv3Nb9yIXvGfVlek/CzMyKXCTMzKzIRcLMzIraKhKSPilps6T7JF0j6aWSZkm6XVKvpGslTcq+h+V8b7bPrC3nvIw/KOnUWnxBxnolrWgnVzMzG7mWi4SkacC/B7oj4gRgArAYuAi4OCJeA+wCluWQZcCujF+c/ZA0J8cdDywAvi5pgqQJwKXAacAcYEn2NTOzDmn3cNNE4GWSJgIvBx4DTgbWZvtqYFFOL8x5sn2eJGV8TUTsjYiHqe6PfVI+eiNia0Q8B6zJvmZm1iEtF4mI2A58GfgZVXHYA9wF7I6IfdmtD5iW09OAbTl2X/Y/uh5vGFOK70fSckk9knr6+/tbfUlmZtagncNNU6g+2c8CXgm8gupwUcdFxMqI6I6I7q6urvFIwczsoNTO4aZTgIcjoj8ifg18G3g7MDkPPwFMB7bn9HZgBkC2Hwk8WY83jCnFzcysQ9opEj8D5kp6eZ5bmAfcD9wMnJF9lgLX5/S6nCfbb4qIyPjivPppFjAbuAO4E5idV0tNojq5va6NfM3MbIRa/lmOiLhd0lrgh8A+4G5gJfA9YI2kL2bsihxyBXC1pF5gJ9WbPhGxWdJ1VAVmH3BORDwPIOlcYAPVlVOrImJzq/mamdnItfXbTRFxPnB+Q3gr1ZVJjX2fBd5fWM4FwAVN4uuB9e3kaGZmrfM3rs3MrMhFwszMilwkzMysyEXCzMyKXCTMzKzIRcLMzIpcJMzMrMhFwszMilwkzMysyEXCzMyKXCTMzKzIRcLMzIpcJMzMrMhFwszMilwkzMysyEXCzMyK2ioSkiZLWivpJ5IekPQ2SUdJ2ihpSz5Pyb6SdImkXkn3SDqxtpyl2X+LpKW1+Fsk3ZtjLsnbpJqZWYe0uyfxVeB/RcTrgTcCDwArgBsjYjZwY84DnEZ1/+rZwHLgMgBJR1Hd3e6tVHe0O3+gsGSfj9TGLWgzXzMzG4GWi4SkI4F3kPewjojnImI3sBBYnd1WA4tyeiFwVVQ2AZMlHQucCmyMiJ0RsQvYCCzItiMiYlNEBHBVbVlmZtYB7exJzAL6gb+RdLekyyW9AjgmIh7LPo8Dx+T0NGBbbXxfxgaL9zWJ70fSckk9knr6+/vbeElmZlbXTpGYCJwIXBYRbwb+H/94aAmA3AOINtYxLBGxMiK6I6K7q6trrFdnZnbIaKdI9AF9EXF7zq+lKhpP5KEi8nlHtm8HZtTGT8/YYPHpTeJmZtYhLReJiHgc2CbpdRmaB9wPrAMGrlBaClyf0+uAM/Mqp7nAnjwstQGYL2lKnrCeD2zItqckzc2rms6sLcvMzDpgYpvjPw58U9IkYCvwYarCc52kZcCjwAey73rgdKAXeCb7EhE7JX0BuDP7fT4idub02cCVwMuAG/JhZmYd0laRiIgfAd1NmuY16RvAOYXlrAJWNYn3ACe0k6OZmbXO37g2M7MiFwkzMytykTAzsyIXCTMzK3KRMDOzIhcJMzMrcpEwM7MiFwkzMytykTAzsyIXCTMzK3KRMDOzIhcJMzMrcpEwM7MiFwkzMytykTAzsyIXCTMzK2q7SEiaIOluSX+f87Mk3S6pV9K1edc6JB2W873ZPrO2jPMy/qCkU2vxBRnrlbSi3VzNzGxkRmNP4hPAA7X5i4CLI+I1wC5gWcaXAbsyfnH2Q9IcYDFwPLAA+HoWngnApcBpwBxgSfY1M7MOaatISJoOvAe4POcFnAyszS6rgUU5vTDnyfZ52X8hsCYi9kbEw1T3wD4pH70RsTUingPWZF8zM+uQdvck/gL4FPBCzh8N7I6IfTnfB0zL6WnANoBs35P9fxNvGFOK70fSckk9knr6+/vbfElmZjag5SIh6b3Ajoi4axTzaUlErIyI7ojo7urqGu90zMwOGhPbGPt24H2STgdeChwBfBWYLGli7i1MB7Zn/+3ADKBP0kTgSODJWnxAfUwpbmZmHdDynkREnBcR0yNiJtWJ55si4oPAzcAZ2W0pcH1Or8t5sv2miIiML86rn2YBs4E7gDuB2Xm11KRcx7pW8zUzs5FrZ0+i5NPAGklfBO4Grsj4FcDVknqBnVRv+kTEZknXAfcD+4BzIuJ5AEnnAhuACcCqiNg8BvmamVnBqBSJiLgFuCWnt1JdmdTY51ng/YXxFwAXNImvB9aPRo5mZjZy/sa1mZkVuUiYmVmRi4SZmRW5SJiZWZGLhJmZFblImJlZkYuEmZkVuUiYmVmRi4SZmRW5SJiZWZGLhJmZFblImJlZkYuEmZkVuUiYmVmRi4SZmRW5SJiZWVHLRULSDEk3S7pf0mZJn8j4UZI2StqSz1MyLkmXSOqVdI+kE2vLWpr9t0haWou/RdK9OeYSSWrnxZqZ2ci0syexD/jTiJgDzAXOkTQHWAHcGBGzgRtzHuA0qvtXzwaWA5dBVVSA84G3Ut3R7vyBwpJ9PlIbt6CNfM3MbIRaLhIR8VhE/DCnfwk8AEwDFgKrs9tqYFFOLwSuisomYLKkY4FTgY0RsTMidgEbgQXZdkREbIqIAK6qLcvMzDpgVM5JSJoJvBm4HTgmIh7LpseBY3J6GrCtNqwvY4PF+5rEzcysQ9ouEpIOB74F/IeIeKrelnsA0e46hpHDckk9knr6+/vHenVmZoeMtoqEpN+hKhDfjIhvZ/iJPFREPu/I+HZgRm349IwNFp/eJL6fiFgZEd0R0d3V1dXOSzIzs5p2rm4ScAXwQER8pda0Dhi4QmkpcH0tfmZe5TQX2JOHpTYA8yVNyRPW84EN2faUpLm5rjNryzIzsw6Y2MbYtwP/BrhX0o8y9hngQuA6ScuAR4EPZNt64HSgF3gG+DBAROyU9AXgzuz3+YjYmdNnA1cCLwNuyIeZmXVIy0UiIn4AlL63MK9J/wDOKSxrFbCqSbwHOKHVHM3MrD3+xrWZmRW5SJiZWZGLhJmZFblImJlZkYuEmZkVuUiYmVmRi4SZmRW5SJiZWZGLhJmZFblImJlZkYuEmZkVuUiYmVmRi4SZmRW5SJiZWZGLhJmZFblImJlZkYuEmZkVHfBFQtICSQ9K6pW0YrzzMTM7lBzQRULSBOBS4DRgDrBE0pzxzcrM7NBxQBcJ4CSgNyK2RsRzwBpg4TjnZGZ2yJg43gkMYRqwrTbfB7y1sZOk5cDynH1a0oMtrm8q8IsWx7ZFFw3aPG55DcF5jYzzGhnnNUK6qK3cXtUseKAXiWGJiJXAynaXI6knIrpHIaVR5bxGxnmNjPMamQM1Lxib3A70w03bgRm1+ekZMzOzDjjQi8SdwGxJsyRNAhYD68Y5JzOzQ8YBfbgpIvZJOhfYAEwAVkXE5jFcZduHrMaI8xoZ5zUyzmtkDtS8YAxyU0SM9jLNzOwgcaAfbjIzs3HkImFmZkWHXJGQ9H5JmyW9IKl4qVjp50DyJPrtGb82T6iPRl5HSdooaUs+T2nS592SflR7PCtpUbZdKenhWtubOpVX9nu+tu51tfh4bq83Sbot/973SPpXtbZR3V5D/XyMpMPy9ffm9phZazsv4w9KOrWdPFrI6z9Kuj+3z42SXlVra/o37VBeZ0nqr63/39balubffYukpR3O6+JaTj+VtLvWNpbba5WkHZLuK7RL0iWZ9z2STqy1tbe9IuKQegBvAF4H3AJ0F/pMAB4CjgMmAT8G5mTbdcDinP5L4GOjlNeXgBU5vQK4aIj+RwE7gZfn/JXAGWOwvYaVF/B0IT5u2wt4LTA7p18JPAZMHu3tNdi/l1qfs4G/zOnFwLU5PSf7HwbMyuVM6GBe7679G/rYQF6D/U07lNdZwNeajD0K2JrPU3J6Sqfyauj/caqLacZ0e+Wy3wGcCNxXaD8duAEQMBe4fbS21yG3JxERD0TEUN/IbvpzIJIEnAyszX6rgUWjlNrCXN5wl3sGcENEPDNK6y8ZaV6/Md7bKyJ+GhFbcvrnwA6ga5TWXzecn4+p57sWmJfbZyGwJiL2RsTDQG8uryN5RcTNtX9Dm6i+izTW2vm5nVOBjRGxMyJ2ARuBBeOU1xLgmlFa96Ai4laqD4UlC4GrorIJmCzpWEZhex1yRWKYmv0cyDTgaGB3ROxriI+GYyLisZx+HDhmiP6L2f8f6AW5q3mxpMM6nNdLJfVI2jRwCIwDaHtJOonq0+FDtfBoba/Sv5emfXJ77KHaPsMZO5Z51S2j+jQ6oNnftJN5/cv8+6yVNPCl2gNie+VhuVnATbXwWG2v4Sjl3vb2OqC/J9EqSd8Hfq9J02cj4vpO5zNgsLzqMxERkorXJucnhN+n+v7IgPOo3iwnUV0r/Wng8x3M61URsV3SccBNku6leiNs2Shvr6uBpRHxQoZb3l4HI0kfArqBd9bC+/1NI+Kh5ksYdX8HXBMReyX9O6q9sJM7tO7hWAysjYjna7Hx3F5j5qAsEhFxSpuLKP0cyJNUu3ET89PgiH4mZLC8JD0h6diIeCzf1HYMsqgPAN+JiF/Xlj3wqXqvpL8B/qyTeUXE9nzeKukW4M3Atxjn7SXpCOB7VB8QNtWW3fL2amI4Px8z0KdP0kTgSKp/T2P50zPDWrakU6gK7zsjYu9AvPA3HY03vSHziogna7OXU52DGhj7roaxt4xCTsPKq2YxcE49MIbbazhKube9vXy4qbmmPwcS1Zmgm6nOBwAsBUZrz2RdLm84y93vWGi+UQ6cB1gENL0KYizykjRl4HCNpKnA24H7x3t75d/uO1THatc2tI3m9hrOz8fU8z0DuCm3zzpgsaqrn2YBs4E72shlRHlJejPwV8D7ImJHLd70b9rBvI6tzb4PeCCnNwDzM78pwHx+e496TPPK3F5PdRL4tlpsLLfXcKwDzsyrnOYCe/KDUPvba6zOxh+oD+CPqY7L7QWeADZk/JXA+lq/04GfUn0S+GwtfhzVf+Je4H8Ch41SXkcDNwJbgO8DR2W8G7i81m8m1aeDlzSMvwm4l+rN7hvA4Z3KC/iDXPeP83nZgbC9gA8BvwZ+VHu8aSy2V7N/L1SHr96X0y/N19+b2+O42tjP5rgHgdNG+d/7UHl9P/8fDGyfdUP9TTuU138DNuf6bwZeXxv7J7kde4EPdzKvnP8ccGHDuLHeXtdQXZ33a6r3r2XAR4GPZruobtD2UK6/uza2re3ln+UwM7MiH24yM7MiFwkzMytykTAzsyIXCTMzK3KRMDOzIhcJMzMrcpEwM7Oi/w9BK3qQej4yXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y)\n",
    "plt.title('Histogram of y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more y = -1 than y = 1 in the data, so there is more y = 'b' than y = 's'. So, we have to pay attention to normalize the data in order to compare them in the next plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "ind = np.arange(30)\n",
    "plt.boxplot(tX[:,], labels = ind)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many outliers depending on the feature. There are also feature that has a long interquantile range. Maybe we have to treat these feature in order to be more efficient in our futur predictions. Let's do more plots to be have a better idea :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind_1 = np.where(y == 1)\n",
    "ind_2 = np.where(y == -1)\n",
    "tX_1 = tX[ind_1[0],:]\n",
    "tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "fig, axs = plt.subplots(5, 6, figsize=(25,20))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].hist(tX_2[:,n], alpha=0.4, density=True, label=['y = -1'])\n",
    "        axs[i,j].hist(tX_1[:,n], alpha=0.4, density=True, label=['y = 1'])\n",
    "        axs[i,j].legend()\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the histograms of the features with a color for each y, we can see that there are useless features as they have almost the same distribution for y=1 than for y = -1. We can cut feature 15, 18, 20. \n",
    "\n",
    "\n",
    "There are also features that are very inequally distributed with value that are about -1000 and values around 0 ; it can be problematic for the prediction with such a large gap between values of a single distribution. Moreover, there is not a big difference in the distribution of y=1 and y=-1. Maybe it can be useful to put off these big negative values of these features. The features in question are : 0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(20,20))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].scatter(tX[:,n], y)\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()\n",
    "\n",
    "#meme constat comment faire pour se debarrasser de ces valeurs ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots confirm the obsevations that we made in the previous plot ; no difference of the distribution of y for features 15, 18, 20 and very large gap in the distributions of features : 0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of -999 in the entire matrix :\n",
      "[(38114,), (0,), (0,), (0,), (177457,), (177457,), (177457,), (0,), (0,), (0,), (0,), (0,), (177457,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (99913,), (99913,), (99913,), (177457,), (177457,), (177457,), (0,)]\n",
      "(1580052,)\n",
      "number of -999 in the rows where y = 1 :\n",
      "[(2835,), (0,), (0,), (0,), (53202,), (53202,), (53202,), (0,), (0,), (0,), (0,), (0,), (53202,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (25492,), (25492,), (25492,), (53202,), (53202,), (53202,), (0,)]\n",
      "(451725,)\n",
      "number of -999 in the rows where y = -1 :\n",
      "[(35279,), (0,), (0,), (0,), (124255,), (124255,), (124255,), (0,), (0,), (0,), (0,), (0,), (124255,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (74421,), (74421,), (74421,), (124255,), (124255,), (124255,), (0,)]\n",
      "(1128327,)\n"
     ]
    }
   ],
   "source": [
    "def nb_outliers(tX, outlier) : \n",
    "    sum = 0\n",
    "    nb_outliers = []\n",
    "    for col in range(tX.shape[1]) :\n",
    "        sum = np.where(tX[:,col] == outlier)[0].shape\n",
    "        nb_outliers.append(sum)   \n",
    "    print(nb_outliers)\n",
    "    print(np.where(tX==outlier)[0].shape)\n",
    "\n",
    "out = -999\n",
    "\n",
    "print('number of -999 in the entire matrix :')\n",
    "nb_outliers(tX, out)\n",
    "\n",
    "ind_1 = np.where(y == 1)\n",
    "ind_2 = np.where(y == -1)\n",
    "tX_1 = tX[ind_1[0],:]\n",
    "tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "print('number of -999 in the rows where y = 1 :')\n",
    "nb_outliers(tX_1, out)\n",
    "print('number of -999 in the rows where y = -1 :')\n",
    "nb_outliers(tX_2, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem with features  0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28. They are inequally distributed; they have a lot of -999 values and the rest is values around 0. So, here we can see how much of these -999 there are. We can see that the -999 appear only in the features that we identified with the histograms. It seems that there is a correlation between features as many features have the same number of -999. We can also see that there is more -999 in the obsevations where y=-1, so we have to take this into account when we filter the data. As there are many -999, we can't delete the rows where there is -999 because we will loose to much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explication sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograms(y, tX):\n",
    "    ind_1 = np.where(y == 1)\n",
    "    ind_2 = np.where(y == -1)\n",
    "    tX_1 = tX[ind_1[0],:]\n",
    "    tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "    fig, axs = plt.subplots(5, 6, figsize=(25,20))\n",
    "\n",
    "    n = 0\n",
    "    for i in range(5) :\n",
    "        for j in range(6) :\n",
    "            axs[i,j].hist(tX_2[:,n], alpha=0.4, density=True, label=['y = -1'])\n",
    "            axs[i,j].hist(tX_1[:,n], alpha=0.4, density=True, label=['y = 1'])\n",
    "            axs[i,j].legend()\n",
    "            axs[i,j].set_title(n)\n",
    "            n = n + 1\n",
    "            if n>=tX.shape[1]: break\n",
    "        if n>=tX.shape[1]: break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 2\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 3\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX, y, ids)\n",
    "\n",
    "print('Set 1')\n",
    "set1_x = outliers(set1_x, -999)\n",
    "_ = outliers(set1_x, -999)\n",
    "\n",
    "print('\\nSet 2')\n",
    "set2_x = outliers(set2_x, -999)\n",
    "_ = outliers(set2_x, -999)\n",
    "\n",
    "print('\\nSet 3')\n",
    "set3_x = outliers(set3_x, -999)\n",
    "_ = outliers(set3_x, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set 2\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set 3\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Set 1')\n",
    "print(np.all(set1_x==set1_x[0], axis = 0))\n",
    "histograms(set1_y, set1_x)\n",
    "\n",
    "print('\\nSet 2')\n",
    "print(np.all(set2_x==set2_x[0], axis = 0))\n",
    "histograms(set2_y, set2_x)\n",
    "\n",
    "print('\\nSet 3')\n",
    "print(np.all(set3_x==set3_x[0], axis = 0))\n",
    "histograms(set3_y, set3_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "things to delete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST FOR y=[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to filter the data, we want to get rif of these -999, but we can't just delete the rows. So, we have the idea to replace the -999 by the mean of the rest of values of the feature. As there is a significant difference of amount of -999 in between y=1 and y=-1 in certain features, we calculate the mean for the rows where y = 1 and y = -1 separatly.\n",
    "\n",
    "Then, we can also standardize the data. It can be a good idea because the features are not all in the same range of values and it can create disproportionality between the importance of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above in the histograms, some features seem to be useless as they have a similar distribution between the y = 1 and y = -1. So, it is useful to have function that cut or keep some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log = [1, 2, 5, 9, 10, 13, 16, 19, 21, 23, 26, 29]\n",
    "#to_log2 = [7, 8, 10, 11, 19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results for least squares regression is with the tX matrix which is standardize, filter with filtering_with_mean_bis function and whose features 15, 18, 20 are cut :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8094823719674634, 0.8367332181680027, 3.915894970097548, 7.460633536153627, 99.75861666852452, 21782.808502372467, 640181.0545578048, 18065042.09027731, 734470157.2616756, 22052422020.83767]\n",
      "Cross validation finished: optimal degree 1\n",
      "Least square loss rmse 0.5723030100580345\n"
     ]
    }
   ],
   "source": [
    "tX_cut = cut(tX, [15,18,20])\n",
    "x_essai = std(filtering_with_mean_bis(tX_cut, y))\n",
    "degrees = np.arange(1,11)\n",
    "# Cross-validation on the degrees\n",
    "degree_opt, _ = best_degree_selection(y, x_essai, degrees, k_fold=10, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "# Best degree model\n",
    "tX_poly = build_poly(x_essai, degree_opt)\n",
    "w_ls, loss_ls = least_squares(y, tX_poly)\n",
    "print(\"Least square loss rmse {loss}\".format(loss=np.sqrt(loss_ls)))\n",
    "degree_ls = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares best model on the entire is degree 1 ; its accuracy is : 0.606. Let's try the same model but applied to the 3 different sets according to the PRI_jet_num parameter thanks to the separate_sets function. We sould normally improve our results : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 2\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 3\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "tX_cut = cut(tX, [15,18,20])\n",
    "\n",
    "# Separation into set according to the PRI_jet_num parameter, as it is the 22th column of the entire matrix\n",
    "# if we cut 3 paramaters before the 22th, PRI_jet_num is the 22-3th now\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_cut, y, ids, 22-3)\n",
    "\n",
    "def best_filtering_ls(set_x, set_y) :\n",
    "        set_x = outliers(set_x, -999)\n",
    "        set_x = std(filtering_with_mean_bis(set_x, set_y))\n",
    "        return set_x\n",
    "\n",
    "print('Set 1')\n",
    "set1_x_ls = best_filtering_ls(set1_x, set1_y)\n",
    "\n",
    "print('\\nSet 2')\n",
    "set2_x_ls = best_filtering_ls(set2_x, set2_y)\n",
    "\n",
    "print('\\nSet 3')\n",
    "set3_x_ls = best_filtering_ls(set3_x, set3_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do least squares regression for each set : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.6992124921989132, 1.9845019446411356, 49.09508655110148, 680.395540586843, 303328.3207567297, 17473312.61822008, 1257541187.5109198, 149638769519.0206, 14766571347092.793, 968707857329139.4]\n",
      "Cross validation finished: optimal degree 1\n",
      "Least square loss rmse 0.49359178738802756\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,11)\n",
    "# Cross-validation on the degrees\n",
    "degree_opt, _ = best_degree_selection(set1_y, set1_x_ls, degrees, k_fold=10, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "# Best degree model \n",
    "tX_poly = build_poly(set1_x_ls, degree_opt)\n",
    "w_set1_ls, loss_ls = least_squares(set1_y, tX_poly)\n",
    "print(\"Least square loss rmse {loss}\".format(loss=np.sqrt(loss_ls)))\n",
    "degree_set1_ls = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8627394947257983, 0.8194579205690277, 0.8118322002522186, 0.8032677941638553, 0.8300628265844276, 0.892754526525992, 1.9111011828648834, 3.795161870621623, 8.013056147923317, 6.684984206978702]\n",
      "Cross validation finished: optimal degree 4\n",
      "Least square loss rmse 0.563695013641898\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,11)\n",
    "# Cross-validation on the degrees\n",
    "degree_opt, _ = best_degree_selection(set2_y, set2_x_ls, degrees, k_fold=10, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "# Best degree model\n",
    "tX_poly = build_poly(set2_x_ls, degree_opt)\n",
    "w_set2_ls, loss_ls = least_squares(set2_y, tX_poly)\n",
    "print(\"Least square loss rmse {loss}\".format(loss=np.sqrt(loss_ls)))\n",
    "degree_set2_ls = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8494265790401101, 0.8048402897267272, 0.7838667573027494, 0.7804477199035814, 0.7723920603771754, 1.1329656722635941, 1.1764287361183663, 3.804494848573087, 17.168085104218402, 52.06761713224065]\n",
      "Cross validation finished: optimal degree 5\n",
      "Least square loss rmse 0.5401761404390758\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,11)\n",
    "# Cross-validation on the degrees\n",
    "degree_opt, _ = best_degree_selection(set3_y, set3_x_ls, degrees, k_fold=10, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "# Best degree model\n",
    "tX_poly = build_poly(set3_x_ls, degree_opt)\n",
    "w_set3_ls, loss_ls = least_squares(set3_y, tX_poly)\n",
    "print(\"Least square loss rmse {loss}\".format(loss=np.sqrt(loss_ls)))\n",
    "degree_set3_ls = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least square best model (standardize + filter_with_mean_bis + cut 15,18,20) with separate_sets is :\n",
    "* set 1 : degree 1\n",
    "* set 2 : degree 4\n",
    "* set 3 : degree 5\n",
    "\n",
    "We have a better accuracy : 0.683"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results for Ridge regression is with tX that is only filtered by filtering_with_mean_bis function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8095450909357379, 0.844544477269849, 2.2534289074606, 0.9046219752885352, 344.6792667724601, 25071.139924639538, 1438117.9697410704]\n",
      "Cross validation finished: optimal lambda 0.0001 and degree 1\n",
      "Ridge regression loss 0.3275363710713481\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,8)\n",
    "x_essai = filtering_with_mean_bis(tX, y)\n",
    "# Cross-validation on the degrees and the lambdas\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x_essai, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "# Best degree and lambda model\n",
    "x_essai = build_poly(x_essai, degree_opt)\n",
    "w_rr, loss_rr = ridge_regression(y, x_essai, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "degree_rr = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look if we can improve the results with the separation of tX into sets according to the feature 22 (PRI_jet_num) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 2\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 3\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX, y, ids)\n",
    "\n",
    "def best_filtering_rr(set_x, set_y) :\n",
    "        set_x = outliers(set_x, -999)\n",
    "        set_x = filtering_with_mean_bis(set_x, set_y)\n",
    "        return set_x\n",
    "    \n",
    "\n",
    "print('Set 1')\n",
    "set1_x_rr = best_filtering_rr(set1_x, set1_y)\n",
    "\n",
    "print('\\nSet 2')\n",
    "set2_x_rr = best_filtering_rr(set2_x, set2_y)\n",
    "\n",
    "print('\\nSet 3')\n",
    "set3_x_rr = best_filtering_rr(set3_x, set3_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression according to the different sets : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.6992441526016957, 2.6438327297625164, 55.80989893325343, 63.684666434180755, 419376.3821326866, 13368684.219238853, 1250756574.718208]\n",
      "Cross validation finished: optimal lambda 0.0001 and degree 1\n",
      "Ridge regression loss 0.24363595402634777\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,8)\n",
    "x_essai = set1_x_rr\n",
    "y = set1_y\n",
    "# Cross-validation on the degrees and the lambdas\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x_essai, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "# Best degree and lambda model\n",
    "x_essai = build_poly(x_essai, degree_opt)\n",
    "w_rr_set1, loss_rr = ridge_regression(y, x_essai, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "lambda_rr_set1 = lambda_opt\n",
    "degree_rr_set1 = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.862899048310838, 0.8169222621606087, 0.8101900728020305, 0.8002962438481538, 0.8261304186737602, 1.2120544270432492, 1.7233468351025003]\n",
      "Cross validation finished: optimal lambda 0.0002592943797404667 and degree 4\n",
      "Ridge regression loss 0.31512983201516304\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,8)\n",
    "x_essai = set2_x_rr\n",
    "y = set2_y\n",
    "# Cross-validation on the degrees and the lambdas\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x_essai, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "# Best degree and lambda model\n",
    "x_essai = build_poly(x_essai, degree_opt)\n",
    "w_rr_set2, loss_rr = ridge_regression(y, x_essai, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "lambda_rr_set2 = lambda_opt\n",
    "degree_rr_set2 = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8495901136277599, 0.8051128492381056, 0.7850629220809948, 0.7798421457960129, 0.8176384928243913, 0.9426713170684411, 2.8857314167790853]\n",
      "Cross validation finished: optimal lambda 0.0002592943797404667 and degree 4\n",
      "Ridge regression loss 0.29715852129995324\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,8)\n",
    "x_essai = set3_x_rr\n",
    "y = set3_y\n",
    "# Cross-validation on the degrees and the lambdas\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x_essai, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "# Best degree and lambda model\n",
    "x_essai = build_poly(x_essai, degree_opt)\n",
    "w_rr_set3, loss_rr = ridge_regression(y, x_essai, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "lambda_rr_set3 = lambda_opt\n",
    "degree_rr_set3 = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least square best model (filter_with_mean_bis) with separate_sets is :\n",
    "* set 1 : degree 1 and lambda = 0.0001\n",
    "* set 2 : degree 4 and lambda = 0.0002592943797404667\n",
    "* set 3 : degree 4 and lambda = 0.0002592943797404667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have better results with the matrix that is separated in sets, so let's do a submission with this method. We have 0.801 of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the data to get better results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Preprocessing for gradient descent done!\n"
     ]
    }
   ],
   "source": [
    "k_fold = 4\n",
    "max_iters = 500\n",
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "tX_log = log_distribution(tX, to_log)\n",
    "\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_log, y, ids)\n",
    "\n",
    "set1_x = outliers(set1_x, -999)\n",
    "set1_x = filtering_with_mean_bis(set1_x, set1_y)\n",
    "#set1_x = filtering_with_mean(set1_x)\n",
    "set1_x = std(set1_x)\n",
    "\n",
    "set2_x = outliers(set2_x, -999)\n",
    "set2_x = filtering_with_mean_bis(set2_x, set2_y)\n",
    "#set2_x = filtering_with_mean(set2_x)\n",
    "set2_x = std(set2_x)\n",
    "\n",
    "set3_x = outliers(set3_x, -999)\n",
    "set3_x = filtering_with_mean_bis(set3_x, set3_y)\n",
    "#set3_x = filtering_with_mean(set3_x)\n",
    "set3_x = std(set3_x)\n",
    "print('')\n",
    "print(\"Preprocessing for gradient descent done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we perform a 4-fold cross validation for the gamma parameter for gradient descent method for each set. Then we we perform a stochastic gradient descent with the optimal gamma found. For the cross validation we use 50 iterations and for the final descent 500 iterations as we want a more precise final result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4947022503378182\n",
      "Gradient Descent(2/49): loss=0.4896703267175971\n",
      "Gradient Descent(3/49): loss=0.4848904379636344\n",
      "Gradient Descent(4/49): loss=0.4803495167342438\n",
      "Gradient Descent(5/49): loss=0.4760351810856928\n",
      "Gradient Descent(6/49): loss=0.4719356981068582\n",
      "Gradient Descent(7/49): loss=0.4680399495109904\n",
      "Gradient Descent(8/49): loss=0.46433739907735627\n",
      "Gradient Descent(9/49): loss=0.46081806184153673\n",
      "Gradient Descent(10/49): loss=0.45747247493881665\n",
      "Gradient Descent(11/49): loss=0.4542916700104386\n",
      "Gradient Descent(12/49): loss=0.451267147087529\n",
      "Gradient Descent(13/49): loss=0.44839084987223826\n",
      "Gradient Descent(14/49): loss=0.4456551423401184\n",
      "Gradient Descent(15/49): loss=0.4430527865919723\n",
      "Gradient Descent(16/49): loss=0.4405769218873861\n",
      "Gradient Descent(17/49): loss=0.4382210447959111\n",
      "Gradient Descent(18/49): loss=0.43597899040539545\n",
      "Gradient Descent(19/49): loss=0.4338449145303076\n",
      "Gradient Descent(20/49): loss=0.43181327686604193\n",
      "Gradient Descent(21/49): loss=0.42987882503816754\n",
      "Gradient Descent(22/49): loss=0.42803657949838836\n",
      "Gradient Descent(23/49): loss=0.4262818192216274\n",
      "Gradient Descent(24/49): loss=0.42461006816114855\n",
      "Gradient Descent(25/49): loss=0.4230170824209845\n",
      "Gradient Descent(26/49): loss=0.421498838107172\n",
      "Gradient Descent(27/49): loss=0.42005151982138816\n",
      "Gradient Descent(28/49): loss=0.41867150976257744\n",
      "Gradient Descent(29/49): loss=0.4173553774040248\n",
      "Gradient Descent(30/49): loss=0.4160998697151048\n",
      "Gradient Descent(31/49): loss=0.4149019018986086\n",
      "Gradient Descent(32/49): loss=0.4137585486161255\n",
      "Gradient Descent(33/49): loss=0.4126670356754565\n",
      "Gradient Descent(34/49): loss=0.4116247321554359\n",
      "Gradient Descent(35/49): loss=0.4106291429448767\n",
      "Gradient Descent(36/49): loss=0.4096779016736105\n",
      "Gradient Descent(37/49): loss=0.40876876401478207\n",
      "Gradient Descent(38/49): loss=0.40789960133868125\n",
      "Gradient Descent(39/49): loss=0.40706839469945943\n",
      "Gradient Descent(40/49): loss=0.40627322913707953\n",
      "Gradient Descent(41/49): loss=0.4055122882777959\n",
      "Gradient Descent(42/49): loss=0.40478384921736205\n",
      "Gradient Descent(43/49): loss=0.4040862776720063\n",
      "Gradient Descent(44/49): loss=0.4034180233830215\n",
      "Gradient Descent(45/49): loss=0.4027776157615711\n",
      "Gradient Descent(46/49): loss=0.4021636597610297\n",
      "Gradient Descent(47/49): loss=0.40157483196485694\n",
      "Gradient Descent(48/49): loss=0.40100987687863976\n",
      "Gradient Descent(49/49): loss=0.40046760341555093\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4946970165493175\n",
      "Gradient Descent(2/49): loss=0.4896601295946725\n",
      "Gradient Descent(3/49): loss=0.484875532907455\n",
      "Gradient Descent(4/49): loss=0.48033014470565466\n",
      "Gradient Descent(5/49): loss=0.47601156921428056\n",
      "Gradient Descent(6/49): loss=0.47190806029352467\n",
      "Gradient Descent(7/49): loss=0.4680084870215269\n",
      "Gradient Descent(8/49): loss=0.4643023011249186\n",
      "Gradient Descent(9/49): loss=0.46077950615628777\n",
      "Gradient Descent(10/49): loss=0.4574306283233193\n",
      "Gradient Descent(11/49): loss=0.45424668887967284\n",
      "Gradient Descent(12/49): loss=0.4512191779926461\n",
      "Gradient Descent(13/49): loss=0.4483400300073921\n",
      "Gradient Descent(14/49): loss=0.44560160003189736\n",
      "Gradient Descent(15/49): loss=0.4429966417711257\n",
      "Gradient Descent(16/49): loss=0.4405182865426807\n",
      "Gradient Descent(17/49): loss=0.4381600234100771\n",
      "Gradient Descent(18/49): loss=0.4359156803732314\n",
      "Gradient Descent(19/49): loss=0.433779406559099\n",
      "Gradient Descent(20/49): loss=0.4317456553585308\n",
      "Gradient Descent(21/49): loss=0.42980916845837075\n",
      "Gradient Descent(22/49): loss=0.4279649607206194\n",
      "Gradient Descent(23/49): loss=0.4262083058631192\n",
      "Gradient Descent(24/49): loss=0.4245347228987089\n",
      "Gradient Descent(25/49): loss=0.42293996329214756\n",
      "Gradient Descent(26/49): loss=0.4214199987963258\n",
      "Gradient Descent(27/49): loss=0.4199710099313795\n",
      "Gradient Descent(28/49): loss=0.4185893750723008\n",
      "Gradient Descent(29/49): loss=0.417271660112512\n",
      "Gradient Descent(30/49): loss=0.4160146086726327\n",
      "Gradient Descent(31/49): loss=0.4148151328253419\n",
      "Gradient Descent(32/49): loss=0.4136703043088099\n",
      "Gradient Descent(33/49): loss=0.4125773462026678\n",
      "Gradient Descent(34/49): loss=0.4115336250418859\n",
      "Gradient Descent(35/49): loss=0.41053664334526746\n",
      "Gradient Descent(36/49): loss=0.40958403253651465\n",
      "Gradient Descent(37/49): loss=0.40867354623701757\n",
      "Gradient Descent(38/49): loss=0.4078030539106357\n",
      "Gradient Descent(39/49): loss=0.40697053484180346\n",
      "Gradient Descent(40/49): loss=0.4061740724292971\n",
      "Gradient Descent(41/49): loss=0.40541184877894376\n",
      "Gradient Descent(42/49): loss=0.40468213957945537\n",
      "Gradient Descent(43/49): loss=0.4039833092464139\n",
      "Gradient Descent(44/49): loss=0.4033138063202377\n",
      "Gradient Descent(45/49): loss=0.40267215910471815\n",
      "Gradient Descent(46/49): loss=0.40205697153342795\n",
      "Gradient Descent(47/49): loss=0.4014669192519881\n",
      "Gradient Descent(48/49): loss=0.4009007459048114\n",
      "Gradient Descent(49/49): loss=0.4003572596155589\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49470728638201206\n",
      "Gradient Descent(2/49): loss=0.4896806337001474\n",
      "Gradient Descent(3/49): loss=0.48490621635614095\n",
      "Gradient Descent(4/49): loss=0.48037093502977324\n",
      "Gradient Descent(5/49): loss=0.4760623781247805\n",
      "Gradient Descent(6/49): loss=0.47196878528752795\n",
      "Gradient Descent(7/49): loss=0.46807901288524484\n",
      "Gradient Descent(8/49): loss=0.4643825013369257\n",
      "Gradient Descent(9/49): loss=0.4608692441959389\n",
      "Gradient Descent(10/49): loss=0.4575297588889944\n",
      "Gradient Descent(11/49): loss=0.4543550590214104\n",
      "Gradient Descent(12/49): loss=0.4513366281635987\n",
      "Gradient Descent(13/49): loss=0.44846639503840335\n",
      "Gradient Descent(14/49): loss=0.4457367100333582\n",
      "Gradient Descent(15/49): loss=0.4431403229661224\n",
      "Gradient Descent(16/49): loss=0.4406703620353015\n",
      "Gradient Descent(17/49): loss=0.4383203138925963\n",
      "Gradient Descent(18/49): loss=0.43608400477573733\n",
      "Gradient Descent(19/49): loss=0.4339555826449898\n",
      "Gradient Descent(20/49): loss=0.43192950026914906\n",
      "Gradient Descent(21/49): loss=0.4300004992099074\n",
      "Gradient Descent(22/49): loss=0.4281635946562713\n",
      "Gradient Descent(23/49): loss=0.42641406106334495\n",
      "Gradient Descent(24/49): loss=0.4247474185522947\n",
      "Gradient Descent(25/49): loss=0.42315942003065504\n",
      "Gradient Descent(26/49): loss=0.42164603899436987\n",
      "Gradient Descent(27/49): loss=0.4202034579750552\n",
      "Gradient Descent(28/49): loss=0.41882805759795827\n",
      "Gradient Descent(29/49): loss=0.4175164062179607\n",
      "Gradient Descent(30/49): loss=0.4162652501027433\n",
      "Gradient Descent(31/49): loss=0.4150715041339042\n",
      "Gradient Descent(32/49): loss=0.41393224299840303\n",
      "Gradient Descent(33/49): loss=0.41284469284419545\n",
      "Gradient Descent(34/49): loss=0.4118062233753366\n",
      "Gradient Descent(35/49): loss=0.41081434036316533\n",
      "Gradient Descent(36/49): loss=0.4098666785514403\n",
      "Gradient Descent(37/49): loss=0.40896099493449367\n",
      "Gradient Descent(38/49): loss=0.4080951623885925\n",
      "Gradient Descent(39/49): loss=0.40726716363776433\n",
      "Gradient Descent(40/49): loss=0.40647508553635087\n",
      "Gradient Descent(41/49): loss=0.40571711365150215\n",
      "Gradient Descent(42/49): loss=0.40499152712972913\n",
      "Gradient Descent(43/49): loss=0.4042966938324794\n",
      "Gradient Descent(44/49): loss=0.4036310657265078\n",
      "Gradient Descent(45/49): loss=0.40299317451557376\n",
      "Gradient Descent(46/49): loss=0.4023816275007186\n",
      "Gradient Descent(47/49): loss=0.40179510365705345\n",
      "Gradient Descent(48/49): loss=0.40123234991563894\n",
      "Gradient Descent(49/49): loss=0.4006921776396377\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4947187082270816\n",
      "Gradient Descent(2/49): loss=0.4896955647877981\n",
      "Gradient Descent(3/49): loss=0.4849175804333833\n",
      "Gradient Descent(4/49): loss=0.48037242409817804\n",
      "Gradient Descent(5/49): loss=0.4760483893815246\n",
      "Gradient Descent(6/49): loss=0.47193436274696526\n",
      "Gradient Descent(7/49): loss=0.4680197933500186\n",
      "Gradient Descent(8/49): loss=0.46429466441044664\n",
      "Gradient Descent(9/49): loss=0.4607494660493216\n",
      "Gradient Descent(10/49): loss=0.45737516951534934\n",
      "Gradient Descent(11/49): loss=0.45416320272885913\n",
      "Gradient Descent(12/49): loss=0.45110542707558465\n",
      "Gradient Descent(13/49): loss=0.448194115385905\n",
      "Gradient Descent(14/49): loss=0.4454219310385543\n",
      "Gradient Descent(15/49): loss=0.44278190813097856\n",
      "Gradient Descent(16/49): loss=0.4402674326615231\n",
      "Gradient Descent(17/49): loss=0.43787222467147524\n",
      "Gradient Descent(18/49): loss=0.4355903212976852\n",
      "Gradient Descent(19/49): loss=0.43341606068903626\n",
      "Gradient Descent(20/49): loss=0.43134406674246034\n",
      "Gradient Descent(21/49): loss=0.4293692346164824\n",
      "Gradient Descent(22/49): loss=0.4274867169824526\n",
      "Gradient Descent(23/49): loss=0.4256919109756826\n",
      "Gradient Descent(24/49): loss=0.42398044581065203\n",
      "Gradient Descent(25/49): loss=0.4223481710263045\n",
      "Gradient Descent(26/49): loss=0.4207911453291981\n",
      "Gradient Descent(27/49): loss=0.4193056260039455\n",
      "Gradient Descent(28/49): loss=0.4178880588619443\n",
      "Gradient Descent(29/49): loss=0.4165350687008978\n",
      "Gradient Descent(30/49): loss=0.4152434502490391\n",
      "Gradient Descent(31/49): loss=0.41401015956930987\n",
      "Gradient Descent(32/49): loss=0.4128323059000255\n",
      "Gradient Descent(33/49): loss=0.41170714390975227\n",
      "Gradient Descent(34/49): loss=0.4106320663452765\n",
      "Gradient Descent(35/49): loss=0.4096045970526238\n",
      "Gradient Descent(36/49): loss=0.4086223843521157\n",
      "Gradient Descent(37/49): loss=0.4076831947494258\n",
      "Gradient Descent(38/49): loss=0.4067849069655236\n",
      "Gradient Descent(39/49): loss=0.40592550626926655\n",
      "Gradient Descent(40/49): loss=0.40510307909723825\n",
      "Gradient Descent(41/49): loss=0.4043158079462141\n",
      "Gradient Descent(42/49): loss=0.4035619665243844\n",
      "Gradient Descent(43/49): loss=0.40283991514817796\n",
      "Gradient Descent(44/49): loss=0.40214809637219656\n",
      "Gradient Descent(45/49): loss=0.4014850308404117\n",
      "Gradient Descent(46/49): loss=0.40084931334738144\n",
      "Gradient Descent(47/49): loss=0.4002396090988146\n",
      "Gradient Descent(48/49): loss=0.39965465016136087\n",
      "Gradient Descent(49/49): loss=0.3990932320920152\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48954097897108784\n",
      "Gradient Descent(2/49): loss=0.4801171219999913\n",
      "Gradient Descent(3/49): loss=0.4716225013963935\n",
      "Gradient Descent(4/49): loss=0.46396215169792826\n",
      "Gradient Descent(5/49): loss=0.4570509225282022\n",
      "Gradient Descent(6/49): loss=0.4508124531547485\n",
      "Gradient Descent(7/49): loss=0.4451782556120623\n",
      "Gradient Descent(8/49): loss=0.44008689470291196\n",
      "Gradient Descent(9/49): loss=0.43548325447538977\n",
      "Gradient Descent(10/49): loss=0.43131788191275033\n",
      "Gradient Descent(11/49): loss=0.4275463995847501\n",
      "Gradient Descent(12/49): loss=0.42412897990773735\n",
      "Gradient Descent(13/49): loss=0.4210298744591476\n",
      "Gradient Descent(14/49): loss=0.41821699250180566\n",
      "Gradient Descent(15/49): loss=0.41566152350463403\n",
      "Gradient Descent(16/49): loss=0.41333759900793815\n",
      "Gradient Descent(17/49): loss=0.41122198968126517\n",
      "Gradient Descent(18/49): loss=0.4092938338668686\n",
      "Gradient Descent(19/49): loss=0.40753439429821225\n",
      "Gradient Descent(20/49): loss=0.40592684003616175\n",
      "Gradient Descent(21/49): loss=0.40445605098033294\n",
      "Gradient Descent(22/49): loss=0.4031084425937962\n",
      "Gradient Descent(23/49): loss=0.40187180872970946\n",
      "Gradient Descent(24/49): loss=0.4007351806718497\n",
      "Gradient Descent(25/49): loss=0.3996887007003903\n",
      "Gradient Descent(26/49): loss=0.3987235086722598\n",
      "Gradient Descent(27/49): loss=0.39783164026436507\n",
      "Gradient Descent(28/49): loss=0.3970059356699382\n",
      "Gradient Descent(29/49): loss=0.396239957665117\n",
      "Gradient Descent(30/49): loss=0.3955279180762366\n",
      "Gradient Descent(31/49): loss=0.39486461177965215\n",
      "Gradient Descent(32/49): loss=0.39424535745652056\n",
      "Gradient Descent(33/49): loss=0.39366594440600916\n",
      "Gradient Descent(34/49): loss=0.39312258479288636\n",
      "Gradient Descent(35/49): loss=0.392611870770307\n",
      "Gradient Descent(36/49): loss=0.39213073597664583\n",
      "Gradient Descent(37/49): loss=0.39167642095718136\n",
      "Gradient Descent(38/49): loss=0.391246442107943\n",
      "Gradient Descent(39/49): loss=0.3908385637806744\n",
      "Gradient Descent(40/49): loss=0.3904507732251621\n",
      "Gradient Descent(41/49): loss=0.39008125807858435\n",
      "Gradient Descent(42/49): loss=0.38972838614145944\n",
      "Gradient Descent(43/49): loss=0.3893906872065863\n",
      "Gradient Descent(44/49): loss=0.3890668367314021\n",
      "Gradient Descent(45/49): loss=0.3887556411657141\n",
      "Gradient Descent(46/49): loss=0.38845602476607205\n",
      "Gradient Descent(47/49): loss=0.3881670177453511\n",
      "Gradient Descent(48/49): loss=0.3878877456216371\n",
      "Gradient Descent(49/49): loss=0.3876174196444205\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4895306504801764\n",
      "Gradient Descent(2/49): loss=0.4800975162686228\n",
      "Gradient Descent(3/49): loss=0.4715945529309665\n",
      "Gradient Descent(4/49): loss=0.46392668801175174\n",
      "Gradient Descent(5/49): loss=0.4570086736611158\n",
      "Gradient Descent(6/49): loss=0.4507640608149442\n",
      "Gradient Descent(7/49): loss=0.4451242818414965\n",
      "Gradient Descent(8/49): loss=0.44002783000471174\n",
      "Gradient Descent(9/49): loss=0.4354195253699859\n",
      "Gradient Descent(10/49): loss=0.4312498579108439\n",
      "Gradient Descent(11/49): loss=0.4274743995810741\n",
      "Gradient Descent(12/49): loss=0.42405327801105697\n",
      "Gradient Descent(13/49): loss=0.42095070528199047\n",
      "Gradient Descent(14/49): loss=0.4181345559387808\n",
      "Gradient Descent(15/49): loss=0.41557598903149656\n",
      "Gradient Descent(16/49): loss=0.4132491095352756\n",
      "Gradient Descent(17/49): loss=0.4111306649972044\n",
      "Gradient Descent(18/49): loss=0.4091997737028498\n",
      "Gradient Descent(19/49): loss=0.40743768105088773\n",
      "Gradient Descent(20/49): loss=0.4058275411770382\n",
      "Gradient Descent(21/49): loss=0.40435422118305897\n",
      "Gradient Descent(22/49): loss=0.4030041256070863\n",
      "Gradient Descent(23/49): loss=0.4017650390219088\n",
      "Gradient Descent(24/49): loss=0.40062598487112877\n",
      "Gradient Descent(25/49): loss=0.3995770988525727\n",
      "Gradient Descent(26/49): loss=0.39860951533635924\n",
      "Gradient Descent(27/49): loss=0.39771526546407254\n",
      "Gradient Descent(28/49): loss=0.3968871857175601\n",
      "Gradient Descent(29/49): loss=0.396118835872843\n",
      "Gradient Descent(30/49): loss=0.39540442536810766\n",
      "Gradient Descent(31/49): loss=0.39473874721620633\n",
      "Gradient Descent(32/49): loss=0.39411711868282073\n",
      "Gradient Descent(33/49): loss=0.3935353280325917\n",
      "Gradient Descent(34/49): loss=0.39298958671811307\n",
      "Gradient Descent(35/49): loss=0.3924764864516437\n",
      "Gradient Descent(36/49): loss=0.39199296065753014\n",
      "Gradient Descent(37/49): loss=0.3915362498553638\n",
      "Gradient Descent(38/49): loss=0.39110387057049073\n",
      "Gradient Descent(39/49): loss=0.3906935874102038\n",
      "Gradient Descent(40/49): loss=0.3903033879813129\n",
      "Gradient Descent(41/49): loss=0.3899314603582508\n",
      "Gradient Descent(42/49): loss=0.3895761728408579\n",
      "Gradient Descent(43/49): loss=0.389236055767848\n",
      "Gradient Descent(44/49): loss=0.3889097851760364\n",
      "Gradient Descent(45/49): loss=0.3885961681169823\n",
      "Gradient Descent(46/49): loss=0.38829412946204395\n",
      "Gradient Descent(47/49): loss=0.3880027000441821\n",
      "Gradient Descent(48/49): loss=0.3877210060003931\n",
      "Gradient Descent(49/49): loss=0.3874482591925994\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4895511775071247\n",
      "Gradient Descent(2/49): loss=0.4801383879259219\n",
      "Gradient Descent(3/49): loss=0.47165544302773993\n",
      "Gradient Descent(4/49): loss=0.4640071540237735\n",
      "Gradient Descent(5/49): loss=0.45710818072826337\n",
      "Gradient Descent(6/49): loss=0.4508820025906069\n",
      "Gradient Descent(7/49): loss=0.44525999849769327\n",
      "Gradient Descent(8/49): loss=0.44018062368151323\n",
      "Gradient Descent(9/49): loss=0.435588673340432\n",
      "Gradient Descent(10/49): loss=0.43143462371383756\n",
      "Gradient Descent(11/49): loss=0.42767404235546547\n",
      "Gradient Descent(12/49): loss=0.4242670602448354\n",
      "Gradient Descent(13/49): loss=0.42117789917160997\n",
      "Gradient Descent(14/49): loss=0.4183744485354239\n",
      "Gradient Descent(15/49): loss=0.41582788633376083\n",
      "Gradient Descent(16/49): loss=0.4135123396714562\n",
      "Gradient Descent(17/49): loss=0.41140458062514035\n",
      "Gradient Descent(18/49): loss=0.4094837537412198\n",
      "Gradient Descent(19/49): loss=0.4077311318428981\n",
      "Gradient Descent(20/49): loss=0.4061298971756141\n",
      "Gradient Descent(21/49): loss=0.4046649452358892\n",
      "Gradient Descent(22/49): loss=0.40332270891013755\n",
      "Gradient Descent(23/49): loss=0.40209100080125504\n",
      "Gradient Descent(24/49): loss=0.4009588718450703\n",
      "Gradient Descent(25/49): loss=0.3999164845189871\n",
      "Gradient Descent(26/49): loss=0.39895499912396515\n",
      "Gradient Descent(27/49): loss=0.39806647178072907\n",
      "Gradient Descent(28/49): loss=0.3972437629238137\n",
      "Gradient Descent(29/49): loss=0.39648045520460473\n",
      "Gradient Descent(30/49): loss=0.3957707798285437\n",
      "Gradient Descent(31/49): loss=0.395109550453599\n",
      "Gradient Descent(32/49): loss=0.3944921038682591\n",
      "Gradient Descent(33/49): loss=0.39391424674883074\n",
      "Gradient Descent(34/49): loss=0.39337220786876337\n",
      "Gradient Descent(35/49): loss=0.39286259519797445\n",
      "Gradient Descent(36/49): loss=0.39238235738855737\n",
      "Gradient Descent(37/49): loss=0.39192874919552034\n",
      "Gradient Descent(38/49): loss=0.3914993004280044\n",
      "Gradient Descent(39/49): loss=0.39109178806832484\n",
      "Gradient Descent(40/49): loss=0.3907042112337024\n",
      "Gradient Descent(41/49): loss=0.39033476868915185\n",
      "Gradient Descent(42/49): loss=0.38998183865010216\n",
      "Gradient Descent(43/49): loss=0.3896439606402854\n",
      "Gradient Descent(44/49): loss=0.3893198191945976\n",
      "Gradient Descent(45/49): loss=0.38900822921829104\n",
      "Gradient Descent(46/49): loss=0.38870812283325745\n",
      "Gradient Descent(47/49): loss=0.3884185375595663\n",
      "Gradient Descent(48/49): loss=0.38813860569601116\n",
      "Gradient Descent(49/49): loss=0.3878675447774017\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4895698445871708\n",
      "Gradient Descent(2/49): loss=0.4801457974161256\n",
      "Gradient Descent(3/49): loss=0.4716279447481536\n",
      "Gradient Descent(4/49): loss=0.4639263640514945\n",
      "Gradient Descent(5/49): loss=0.45696012010202736\n",
      "Gradient Descent(6/49): loss=0.45065636253197355\n",
      "Gradient Descent(7/49): loss=0.44494951449460746\n",
      "Gradient Descent(8/49): loss=0.4397805431767737\n",
      "Gradient Descent(9/49): loss=0.4350963038436209\n",
      "Gradient Descent(10/49): loss=0.4308489499532253\n",
      "Gradient Descent(11/49): loss=0.42699540264327335\n",
      "Gradient Descent(12/49): loss=0.42349687357710397\n",
      "Gradient Descent(13/49): loss=0.4203184357505733\n",
      "Gradient Descent(14/49): loss=0.41742863741187164\n",
      "Gradient Descent(15/49): loss=0.4147991547402784\n",
      "Gradient Descent(16/49): loss=0.41240447937282493\n",
      "Gradient Descent(17/49): loss=0.4102216372652871\n",
      "Gradient Descent(18/49): loss=0.4082299357305819\n",
      "Gradient Descent(19/49): loss=0.4064107358177493\n",
      "Gradient Descent(20/49): loss=0.40474724748205737\n",
      "Gradient Descent(21/49): loss=0.4032243452547651\n",
      "Gradient Descent(22/49): loss=0.40182840235274847\n",
      "Gradient Descent(23/49): loss=0.40054714137626846\n",
      "Gradient Descent(24/49): loss=0.39936949993004894\n",
      "Gradient Descent(25/49): loss=0.39828550967073173\n",
      "Gradient Descent(26/49): loss=0.39728618743462574\n",
      "Gradient Descent(27/49): loss=0.39636343723521833\n",
      "Gradient Descent(28/49): loss=0.39550996204173544\n",
      "Gradient Descent(29/49): loss=0.3947191843595213\n",
      "Gradient Descent(30/49): loss=0.39398517473142974\n",
      "Gradient Descent(31/49): loss=0.39330258736788365\n",
      "Gradient Descent(32/49): loss=0.3926666021928071\n",
      "Gradient Descent(33/49): loss=0.3920728726641455\n",
      "Gradient Descent(34/49): loss=0.3915174787920042\n",
      "Gradient Descent(35/49): loss=0.3909968848352623\n",
      "Gradient Descent(36/49): loss=0.39050790120953016\n",
      "Gradient Descent(37/49): loss=0.3900476501861035\n",
      "Gradient Descent(38/49): loss=0.389613535003632\n",
      "Gradient Descent(39/49): loss=0.38920321205208414\n",
      "Gradient Descent(40/49): loss=0.3888145658226258\n",
      "Gradient Descent(41/49): loss=0.38844568634767046\n",
      "Gradient Descent(42/49): loss=0.3880948488829158\n",
      "Gradient Descent(43/49): loss=0.38776049560797593\n",
      "Gradient Descent(44/49): loss=0.3874412191445337\n",
      "Gradient Descent(45/49): loss=0.38713574771101184\n",
      "Gradient Descent(46/49): loss=0.3868429317508297\n",
      "Gradient Descent(47/49): loss=0.38656173188756915\n",
      "Gradient Descent(48/49): loss=0.3862912080750082\n",
      "Gradient Descent(49/49): loss=0.386030509823147\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4845161858998093\n",
      "Gradient Descent(2/49): loss=0.47129903537506\n",
      "Gradient Descent(3/49): loss=0.4600054927733153\n",
      "Gradient Descent(4/49): loss=0.4503449937694284\n",
      "Gradient Descent(5/49): loss=0.4420713481219598\n",
      "Gradient Descent(6/49): loss=0.4349758940617102\n",
      "Gradient Descent(7/49): loss=0.4288817218501094\n",
      "Gradient Descent(8/49): loss=0.42363879693017303\n",
      "Gradient Descent(9/49): loss=0.4191198405063824\n",
      "Gradient Descent(10/49): loss=0.41521684826830196\n",
      "Gradient Descent(11/49): loss=0.4118381470869178\n",
      "Gradient Descent(12/49): loss=0.40890590549833844\n",
      "Gradient Descent(13/49): loss=0.406354027171837\n",
      "Gradient Descent(14/49): loss=0.4041263677726004\n",
      "Gradient Descent(15/49): loss=0.4021752250338731\n",
      "Gradient Descent(16/49): loss=0.40046005974690596\n",
      "Gradient Descent(17/49): loss=0.3989464120081788\n",
      "Gradient Descent(18/49): loss=0.39760498263793514\n",
      "Gradient Descent(19/49): loss=0.396410854373822\n",
      "Gradient Descent(20/49): loss=0.3953428313914795\n",
      "Gradient Descent(21/49): loss=0.3943828790296974\n",
      "Gradient Descent(22/49): loss=0.3935156484010326\n",
      "Gradient Descent(23/49): loss=0.39272807293299433\n",
      "Gradient Descent(24/49): loss=0.39200902587989633\n",
      "Gradient Descent(25/49): loss=0.39134902952978123\n",
      "Gradient Descent(26/49): loss=0.39074000825348965\n",
      "Gradient Descent(27/49): loss=0.3901750787451904\n",
      "Gradient Descent(28/49): loss=0.38964837182007395\n",
      "Gradient Descent(29/49): loss=0.38915488099454326\n",
      "Gradient Descent(30/49): loss=0.3886903338015524\n",
      "Gradient Descent(31/49): loss=0.3882510824093433\n",
      "Gradient Descent(32/49): loss=0.3878340106330511\n",
      "Gradient Descent(33/49): loss=0.38743645487010486\n",
      "Gradient Descent(34/49): loss=0.38705613686436047\n",
      "Gradient Descent(35/49): loss=0.38669110652086686\n",
      "Gradient Descent(36/49): loss=0.38633969326186335\n",
      "Gradient Descent(37/49): loss=0.38600046464243587\n",
      "Gradient Descent(38/49): loss=0.38567219113749873\n",
      "Gradient Descent(39/49): loss=0.3853538161757007\n",
      "Gradient Descent(40/49): loss=0.385044430634949\n",
      "Gradient Descent(41/49): loss=0.38474325113231583\n",
      "Gradient Descent(42/49): loss=0.38444960154129554\n",
      "Gradient Descent(43/49): loss=0.3841628972544881\n",
      "Gradient Descent(44/49): loss=0.38388263178203286\n",
      "Gradient Descent(45/49): loss=0.38360836533750536\n",
      "Gradient Descent(46/49): loss=0.3833397151151217\n",
      "Gradient Descent(47/49): loss=0.3830763470064026\n",
      "Gradient Descent(48/49): loss=0.3828179685420928\n",
      "Gradient Descent(49/49): loss=0.3825643228771312\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48450090179257616\n",
      "Gradient Descent(2/49): loss=0.4712707643907081\n",
      "Gradient Descent(3/49): loss=0.4599661505375554\n",
      "Gradient Descent(4/49): loss=0.45029616321754923\n",
      "Gradient Descent(5/49): loss=0.4420143258988508\n",
      "Gradient Descent(6/49): loss=0.434911733166814\n",
      "Gradient Descent(7/49): loss=0.4288112699376789\n",
      "Gradient Descent(8/49): loss=0.42356273010455775\n",
      "Gradient Descent(9/49): loss=0.4190386927244756\n",
      "Gradient Descent(10/49): loss=0.4151310366242257\n",
      "Gradient Descent(11/49): loss=0.4117479933418517\n",
      "Gradient Descent(12/49): loss=0.40881165425579796\n",
      "Gradient Descent(13/49): loss=0.40625586110321826\n",
      "Gradient Descent(14/49): loss=0.40402442028180907\n",
      "Gradient Descent(15/49): loss=0.40206959072190857\n",
      "Gradient Descent(16/49): loss=0.40035080300324183\n",
      "Gradient Descent(17/49): loss=0.3988335740195542\n",
      "Gradient Descent(18/49): loss=0.3974885870692587\n",
      "Gradient Descent(19/49): loss=0.39629091194183225\n",
      "Gradient Descent(20/49): loss=0.39521934352046184\n",
      "Gradient Descent(21/49): loss=0.39425584075034054\n",
      "Gradient Descent(22/49): loss=0.393385050628528\n",
      "Gradient Descent(23/49): loss=0.3925939042386553\n",
      "Gradient Descent(24/49): loss=0.3918712738517149\n",
      "Gradient Descent(25/49): loss=0.3912076818011766\n",
      "Gradient Descent(26/49): loss=0.39059505326576993\n",
      "Gradient Descent(27/49): loss=0.390026506297642\n",
      "Gradient Descent(28/49): loss=0.38949617345185655\n",
      "Gradient Descent(29/49): loss=0.3889990502344253\n",
      "Gradient Descent(30/49): loss=0.38853086631475753\n",
      "Gradient Descent(31/49): loss=0.3880879760651737\n",
      "Gradient Descent(32/49): loss=0.38766726551233704\n",
      "Gradient Descent(33/49): loss=0.3872660732277337\n",
      "Gradient Descent(34/49): loss=0.38688212305903225\n",
      "Gradient Descent(35/49): loss=0.3865134669216946\n",
      "Gradient Descent(36/49): loss=0.38615843613937645\n",
      "Gradient Descent(37/49): loss=0.38581560004988075\n",
      "Gradient Descent(38/49): loss=0.38548373078699116\n",
      "Gradient Descent(39/49): loss=0.3851617733127002\n",
      "Gradient Descent(40/49): loss=0.3848488199136719\n",
      "Gradient Descent(41/49): loss=0.3845440884940099\n",
      "Gradient Descent(42/49): loss=0.38424690409676193\n",
      "Gradient Descent(43/49): loss=0.3839566831718063\n",
      "Gradient Descent(44/49): loss=0.3836729201801136\n",
      "Gradient Descent(45/49): loss=0.38339517618583424\n",
      "Gradient Descent(46/49): loss=0.38312306913986266\n",
      "Gradient Descent(47/49): loss=0.3828562656028779\n",
      "Gradient Descent(48/49): loss=0.3825944736935448\n",
      "Gradient Descent(49/49): loss=0.38233743707959106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48453167337533853\n",
      "Gradient Descent(2/49): loss=0.471331809004944\n",
      "Gradient Descent(3/49): loss=0.4600565208035324\n",
      "Gradient Descent(4/49): loss=0.450414592928314\n",
      "Gradient Descent(5/49): loss=0.4421593352297591\n",
      "Gradient Descent(6/49): loss=0.435081712592729\n",
      "Gradient Descent(7/49): loss=0.4290045461032982\n",
      "Gradient Descent(8/49): loss=0.4237776165821778\n",
      "Gradient Descent(9/49): loss=0.4192735282603911\n",
      "Gradient Descent(10/49): loss=0.4153842131431112\n",
      "Gradient Descent(11/49): loss=0.4120179756624338\n",
      "Gradient Descent(12/49): loss=0.40909699317940473\n",
      "Gradient Descent(13/49): loss=0.4065552012736337\n",
      "Gradient Descent(14/49): loss=0.40433650398173293\n",
      "Gradient Descent(15/49): loss=0.4023932585677767\n",
      "Gradient Descent(16/49): loss=0.4006849923246431\n",
      "Gradient Descent(17/49): loss=0.39917731555973185\n",
      "Gradient Descent(18/49): loss=0.3978410005166604\n",
      "Gradient Descent(19/49): loss=0.39665120069670723\n",
      "Gradient Descent(20/49): loss=0.39558678901251243\n",
      "Gradient Descent(21/49): loss=0.3946297965509551\n",
      "Gradient Descent(22/49): loss=0.393764936541926\n",
      "Gradient Descent(23/49): loss=0.39297920050831964\n",
      "Gradient Descent(24/49): loss=0.39226151557999656\n",
      "Gradient Descent(25/49): loss=0.3916024536493593\n",
      "Gradient Descent(26/49): loss=0.3909939844778228\n",
      "Gradient Descent(27/49): loss=0.3904292660722109\n",
      "Gradient Descent(28/49): loss=0.3899024666727518\n",
      "Gradient Descent(29/49): loss=0.3894086135591386\n",
      "Gradient Descent(30/49): loss=0.3889434646126702\n",
      "Gradient Descent(31/49): loss=0.388503399191535\n",
      "Gradient Descent(32/49): loss=0.3880853254003058\n",
      "Gradient Descent(33/49): loss=0.3876866012784001\n",
      "Gradient Descent(34/49): loss=0.38730496780804763\n",
      "Gradient Descent(35/49): loss=0.38693849196066865\n",
      "Gradient Descent(36/49): loss=0.38658551827035076\n",
      "Gradient Descent(37/49): loss=0.3862446276517837\n",
      "Gradient Descent(38/49): loss=0.3859146023738817\n",
      "Gradient Descent(39/49): loss=0.38559439626473146\n",
      "Gradient Descent(40/49): loss=0.38528310936294313\n",
      "Gradient Descent(41/49): loss=0.38497996634878745\n",
      "Gradient Descent(42/49): loss=0.38468429818887573\n",
      "Gradient Descent(43/49): loss=0.38439552651333836\n",
      "Gradient Descent(44/49): loss=0.3841131503167645\n",
      "Gradient Descent(45/49): loss=0.3838367346355643\n",
      "Gradient Descent(46/49): loss=0.38356590090654236\n",
      "Gradient Descent(47/49): loss=0.38330031875574677\n",
      "Gradient Descent(48/49): loss=0.3830396990042633\n",
      "Gradient Descent(49/49): loss=0.38278378770956867\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48455340908026734\n",
      "Gradient Descent(2/49): loss=0.47131175017857097\n",
      "Gradient Descent(3/49): loss=0.4599509623985761\n",
      "Gradient Descent(4/49): loss=0.45019494006038846\n",
      "Gradient Descent(5/49): loss=0.44180840357970674\n",
      "Gradient Descent(6/49): loss=0.4345908358817696\n",
      "Gradient Descent(7/49): loss=0.4283713239327565\n",
      "Gradient Descent(8/49): loss=0.4230041693671269\n",
      "Gradient Descent(9/49): loss=0.4183651528230387\n",
      "Gradient Descent(10/49): loss=0.4143483540607045\n",
      "Gradient Descent(11/49): loss=0.41086344472634845\n",
      "Gradient Descent(12/49): loss=0.4078333831538901\n",
      "Gradient Descent(13/49): loss=0.4051924512178655\n",
      "Gradient Descent(14/49): loss=0.4028845822591901\n",
      "Gradient Descent(15/49): loss=0.40086193674854226\n",
      "Gradient Descent(16/49): loss=0.3990836888397963\n",
      "Gradient Descent(17/49): loss=0.3975149924748107\n",
      "Gradient Descent(18/49): loss=0.39612610038024143\n",
      "Gradient Descent(19/49): loss=0.3948916132730596\n",
      "Gradient Descent(20/49): loss=0.3937898399708243\n",
      "Gradient Descent(21/49): loss=0.39280225197578733\n",
      "Gradient Descent(22/49): loss=0.3919130185450757\n",
      "Gradient Descent(23/49): loss=0.39110861033728767\n",
      "Gradient Descent(24/49): loss=0.3903774614937912\n",
      "Gradient Descent(25/49): loss=0.38970968151739616\n",
      "Gradient Descent(26/49): loss=0.3890968095914192\n",
      "Gradient Descent(27/49): loss=0.3885316050720145\n",
      "Gradient Descent(28/49): loss=0.38800786881448673\n",
      "Gradient Descent(29/49): loss=0.3875202907843437\n",
      "Gradient Descent(30/49): loss=0.3870643200766221\n",
      "Gradient Descent(31/49): loss=0.3866360540400275\n",
      "Gradient Descent(32/49): loss=0.38623214369050074\n",
      "Gradient Descent(33/49): loss=0.3858497130146098\n",
      "Gradient Descent(34/49): loss=0.3854862901173906\n",
      "Gradient Descent(35/49): loss=0.38513974847108595\n",
      "Gradient Descent(36/49): loss=0.3848082567784115\n",
      "Gradient Descent(37/49): loss=0.38449023618315026\n",
      "Gradient Descent(38/49): loss=0.3841843237476645\n",
      "Gradient Descent(39/49): loss=0.38388934127611063\n",
      "Gradient Descent(40/49): loss=0.3836042686978498\n",
      "Gradient Descent(41/49): loss=0.3833282213412071\n",
      "Gradient Descent(42/49): loss=0.38306043052635236\n",
      "Gradient Descent(43/49): loss=0.3828002269901345\n",
      "Gradient Descent(44/49): loss=0.38254702672737523\n",
      "Gradient Descent(45/49): loss=0.38230031889424004\n",
      "Gradient Descent(46/49): loss=0.38205965547140835\n",
      "Gradient Descent(47/49): loss=0.3818246424291958\n",
      "Gradient Descent(48/49): loss=0.38159493217467205\n",
      "Gradient Descent(49/49): loss=0.3813702170931288\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4796278711239821\n",
      "Gradient Descent(2/49): loss=0.46317586262790655\n",
      "Gradient Descent(3/49): loss=0.44986411048796554\n",
      "Gradient Descent(4/49): loss=0.43906954916518415\n",
      "Gradient Descent(5/49): loss=0.43029406710700435\n",
      "Gradient Descent(6/49): loss=0.42313920921973097\n",
      "Gradient Descent(7/49): loss=0.4172860596822474\n",
      "Gradient Descent(8/49): loss=0.4124792291336434\n",
      "Gradient Descent(9/49): loss=0.4085140977010925\n",
      "Gradient Descent(10/49): loss=0.40522664366029704\n",
      "Gradient Descent(11/49): loss=0.4024853276118663\n",
      "Gradient Descent(12/49): loss=0.4001846123090254\n",
      "Gradient Descent(13/49): loss=0.39823978518598807\n",
      "Gradient Descent(14/49): loss=0.3965828192590132\n",
      "Gradient Descent(15/49): loss=0.39515906233131876\n",
      "Gradient Descent(16/49): loss=0.3939245873925084\n",
      "Gradient Descent(17/49): loss=0.3928440711583562\n",
      "Gradient Descent(18/49): loss=0.3918890947242422\n",
      "Gradient Descent(19/49): loss=0.3910367817782518\n",
      "Gradient Descent(20/49): loss=0.3902687068962782\n",
      "Gradient Descent(21/49): loss=0.38957002003384184\n",
      "Gradient Descent(22/49): loss=0.38892874415762263\n",
      "Gradient Descent(23/49): loss=0.38833521159266715\n",
      "Gradient Descent(24/49): loss=0.3877816115489064\n",
      "Gradient Descent(25/49): loss=0.38726162678946585\n",
      "Gradient Descent(26/49): loss=0.38677014179603436\n",
      "Gradient Descent(27/49): loss=0.3863030082977461\n",
      "Gradient Descent(28/49): loss=0.38585685683804577\n",
      "Gradient Descent(29/49): loss=0.38542894530078203\n",
      "Gradient Descent(30/49): loss=0.3850170371153026\n",
      "Gradient Descent(31/49): loss=0.3846193033006556\n",
      "Gradient Descent(32/49): loss=0.3842342436629387\n",
      "Gradient Descent(33/49): loss=0.38386062338466725\n",
      "Gradient Descent(34/49): loss=0.3834974219865055\n",
      "Gradient Descent(35/49): loss=0.3831437922363742\n",
      "Gradient Descent(36/49): loss=0.3827990270580182\n",
      "Gradient Descent(37/49): loss=0.3824625328739612\n",
      "Gradient Descent(38/49): loss=0.38213380812509523\n",
      "Gradient Descent(39/49): loss=0.38181242595589476\n",
      "Gradient Descent(40/49): loss=0.38149802025241647\n",
      "Gradient Descent(41/49): loss=0.38119027437944086\n",
      "Gradient Descent(42/49): loss=0.3808889120910151\n",
      "Gradient Descent(43/49): loss=0.380593690191458\n",
      "Gradient Descent(44/49): loss=0.3803043926065162\n",
      "Gradient Descent(45/49): loss=0.380020825590798\n",
      "Gradient Descent(46/49): loss=0.3797428138510329\n",
      "Gradient Descent(47/49): loss=0.3794701974076763\n",
      "Gradient Descent(48/49): loss=0.379202829051943\n",
      "Gradient Descent(49/49): loss=0.37894057228316824\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47960777048651726\n",
      "Gradient Descent(2/49): loss=0.46313962550668836\n",
      "Gradient Descent(3/49): loss=0.44981482585406984\n",
      "Gradient Descent(4/49): loss=0.4390095834097197\n",
      "Gradient Descent(5/49): loss=0.4302252034141913\n",
      "Gradient Descent(6/49): loss=0.42306276877674376\n",
      "Gradient Descent(7/49): loss=0.417203003622145\n",
      "Gradient Descent(8/49): loss=0.4123902421799329\n",
      "Gradient Descent(9/49): loss=0.4084196554800738\n",
      "Gradient Descent(10/49): loss=0.40512706600898774\n",
      "Gradient Descent(11/49): loss=0.4023808202125421\n",
      "Gradient Descent(12/49): loss=0.40007529879838655\n",
      "Gradient Descent(13/49): loss=0.39812573162163545\n",
      "Gradient Descent(14/49): loss=0.39646405253876194\n",
      "Gradient Descent(15/49): loss=0.3950355838849656\n",
      "Gradient Descent(16/49): loss=0.39379638321734933\n",
      "Gradient Descent(17/49): loss=0.39271111905531864\n",
      "Gradient Descent(18/49): loss=0.39175136941159117\n",
      "Gradient Descent(19/49): loss=0.3908942584120613\n",
      "Gradient Descent(20/49): loss=0.3901213634074787\n",
      "Gradient Descent(21/49): loss=0.3894178385964766\n",
      "Gradient Descent(22/49): loss=0.38877171202789135\n",
      "Gradient Descent(23/49): loss=0.38817332149972117\n",
      "Gradient Descent(24/49): loss=0.38761486177294524\n",
      "Gradient Descent(25/49): loss=0.3870900210277811\n",
      "Gradient Descent(26/49): loss=0.38659368889099993\n",
      "Gradient Descent(27/49): loss=0.3861217218805513\n",
      "Gradient Descent(28/49): loss=0.38567075492675773\n",
      "Gradient Descent(29/49): loss=0.38523804987994775\n",
      "Gradient Descent(30/49): loss=0.38482137371585773\n",
      "Gradient Descent(31/49): loss=0.3844189005926955\n",
      "Gradient Descent(32/49): loss=0.3840291330693734\n",
      "Gradient Descent(33/49): loss=0.38365083872051464\n",
      "Gradient Descent(34/49): loss=0.3832829991262544\n",
      "Gradient Descent(35/49): loss=0.38292476881022297\n",
      "Gradient Descent(36/49): loss=0.3825754421766864\n",
      "Gradient Descent(37/49): loss=0.3822344268810422\n",
      "Gradient Descent(38/49): loss=0.3819012223754568\n",
      "Gradient Descent(39/49): loss=0.3815754026183708\n",
      "Gradient Descent(40/49): loss=0.381256602134911\n",
      "Gradient Descent(41/49): loss=0.38094450477453223\n",
      "Gradient Descent(42/49): loss=0.38063883464018305\n",
      "Gradient Descent(43/49): loss=0.38033934876613706\n",
      "Gradient Descent(44/49): loss=0.3800458312042751\n",
      "Gradient Descent(45/49): loss=0.3797580882450673\n",
      "Gradient Descent(46/49): loss=0.37947594455292233\n",
      "Gradient Descent(47/49): loss=0.37919924003854916\n",
      "Gradient Descent(48/49): loss=0.37892782732553565\n",
      "Gradient Descent(49/49): loss=0.3786615696961545\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47964877398665307\n",
      "Gradient Descent(2/49): loss=0.4632205933673069\n",
      "Gradient Descent(3/49): loss=0.44993373870171327\n",
      "Gradient Descent(4/49): loss=0.439163824963954\n",
      "Gradient Descent(5/49): loss=0.4304118456118534\n",
      "Gradient Descent(6/49): loss=0.4232787777044713\n",
      "Gradient Descent(7/49): loss=0.4174453830711382\n",
      "Gradient Descent(8/49): loss=0.41265612912177724\n",
      "Gradient Descent(9/49): loss=0.4087063792722653\n",
      "Gradient Descent(10/49): loss=0.405432180854777\n",
      "Gradient Descent(11/49): loss=0.40270211840309295\n",
      "Gradient Descent(12/49): loss=0.4004108105777341\n",
      "Gradient Descent(13/49): loss=0.39847371612481597\n",
      "Gradient Descent(14/49): loss=0.3968229831319286\n",
      "Gradient Descent(15/49): loss=0.395404130347485\n",
      "Gradient Descent(16/49): loss=0.39417339251283434\n",
      "Gradient Descent(17/49): loss=0.39309559590649823\n",
      "Gradient Descent(18/49): loss=0.3921424574917724\n",
      "Gradient Descent(19/49): loss=0.39129122266698063\n",
      "Gradient Descent(20/49): loss=0.39052357380314523\n",
      "Gradient Descent(21/49): loss=0.38982475543278783\n",
      "Gradient Descent(22/49): loss=0.3891828728494955\n",
      "Gradient Descent(23/49): loss=0.3885883295629911\n",
      "Gradient Descent(24/49): loss=0.38803337598174137\n",
      "Gradient Descent(25/49): loss=0.38751174722371207\n",
      "Gradient Descent(26/49): loss=0.38701837237061815\n",
      "Gradient Descent(27/49): loss=0.3865491410081753\n",
      "Gradient Descent(28/49): loss=0.38610071571425675\n",
      "Gradient Descent(29/49): loss=0.3856703814115587\n",
      "Gradient Descent(30/49): loss=0.3852559243052553\n",
      "Gradient Descent(31/49): loss=0.38485553456990673\n",
      "Gradient Descent(32/49): loss=0.38446772810589996\n",
      "Gradient Descent(33/49): loss=0.3840912836116361\n",
      "Gradient Descent(34/49): loss=0.38372519195957505\n",
      "Gradient Descent(35/49): loss=0.3833686154588879\n",
      "Gradient Descent(36/49): loss=0.3830208550642093\n",
      "Gradient Descent(37/49): loss=0.38268132397232824\n",
      "Gradient Descent(38/49): loss=0.3823495263553643\n",
      "Gradient Descent(39/49): loss=0.3820250402250945\n",
      "Gradient Descent(40/49): loss=0.38170750362062517\n",
      "Gradient Descent(41/49): loss=0.3813966034701825\n",
      "Gradient Descent(42/49): loss=0.3810920666051417\n",
      "Gradient Descent(43/49): loss=0.38079365250668196\n",
      "Gradient Descent(44/49): loss=0.38050114744762426\n",
      "Gradient Descent(45/49): loss=0.3802143597580248\n",
      "Gradient Descent(46/49): loss=0.37993311599615603\n",
      "Gradient Descent(47/49): loss=0.3796572578491563\n",
      "Gradient Descent(48/49): loss=0.3793866396219222\n",
      "Gradient Descent(49/49): loss=0.3791211262003882\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47966940170637157\n",
      "Gradient Descent(2/49): loss=0.4631555152790177\n",
      "Gradient Descent(3/49): loss=0.4497205366373557\n",
      "Gradient Descent(4/49): loss=0.4387702560975397\n",
      "Gradient Descent(5/49): loss=0.42982598863150245\n",
      "Gradient Descent(6/49): loss=0.4225020188698072\n",
      "Gradient Descent(7/49): loss=0.416487470070845\n",
      "Gradient Descent(8/49): loss=0.4115317240411562\n",
      "Gradient Descent(9/49): loss=0.407432692719963\n",
      "Gradient Descent(10/49): loss=0.40402738089504087\n",
      "Gradient Descent(11/49): loss=0.40118429044970905\n",
      "Gradient Descent(12/49): loss=0.3987973053110271\n",
      "Gradient Descent(13/49): loss=0.39678076736282014\n",
      "Gradient Descent(14/49): loss=0.3950655105655827\n",
      "Gradient Descent(15/49): loss=0.39359566622101716\n",
      "Gradient Descent(16/49): loss=0.3923260889876528\n",
      "Gradient Descent(17/49): loss=0.39122028269439835\n",
      "Gradient Descent(18/49): loss=0.39024872864699395\n",
      "Gradient Descent(19/49): loss=0.3893875381257749\n",
      "Gradient Descent(20/49): loss=0.3886173660499404\n",
      "Gradient Descent(21/49): loss=0.3879225350686255\n",
      "Gradient Descent(22/49): loss=0.38729032922135015\n",
      "Gradient Descent(23/49): loss=0.38671042426201524\n",
      "Gradient Descent(24/49): loss=0.3861744281402884\n",
      "Gradient Descent(25/49): loss=0.385675510286011\n",
      "Gradient Descent(26/49): loss=0.38520810249030685\n",
      "Gradient Descent(27/49): loss=0.38476765751757486\n",
      "Gradient Descent(28/49): loss=0.3843504542731366\n",
      "Gradient Descent(29/49): loss=0.3839534405187736\n",
      "Gradient Descent(30/49): loss=0.3835741058746791\n",
      "Gradient Descent(31/49): loss=0.383210379253476\n",
      "Gradient Descent(32/49): loss=0.38286054600594027\n",
      "Gradient Descent(33/49): loss=0.38252318097202104\n",
      "Gradient Descent(34/49): loss=0.38219709436745103\n",
      "Gradient Descent(35/49): loss=0.38188128803012\n",
      "Gradient Descent(36/49): loss=0.38157492002917553\n",
      "Gradient Descent(37/49): loss=0.3812772760258608\n",
      "Gradient Descent(38/49): loss=0.380987746086384\n",
      "Gradient Descent(39/49): loss=0.3807058058981496\n",
      "Gradient Descent(40/49): loss=0.3804310015431316\n",
      "Gradient Descent(41/49): loss=0.38016293714546684\n",
      "Gradient Descent(42/49): loss=0.3799012648420594\n",
      "Gradient Descent(43/49): loss=0.3796456766312462\n",
      "Gradient Descent(44/49): loss=0.3793958977402977\n",
      "Gradient Descent(45/49): loss=0.3791516812216926\n",
      "Gradient Descent(46/49): loss=0.37891280354392154\n",
      "Gradient Descent(47/49): loss=0.3786790609876075\n",
      "Gradient Descent(48/49): loss=0.37845026669409015\n",
      "Gradient Descent(49/49): loss=0.37822624824295537\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4748760346436066\n",
      "Gradient Descent(2/49): loss=0.45570854580085923\n",
      "Gradient Descent(3/49): loss=0.44103758578257657\n",
      "Gradient Descent(4/49): loss=0.4297647403395375\n",
      "Gradient Descent(5/49): loss=0.4210627421661724\n",
      "Gradient Descent(6/49): loss=0.4143079434764678\n",
      "Gradient Descent(7/49): loss=0.40902978356740266\n",
      "Gradient Descent(8/49): loss=0.40487291814238874\n",
      "Gradient Descent(9/49): loss=0.4015687998857241\n",
      "Gradient Descent(10/49): loss=0.3989143258833269\n",
      "Gradient Descent(11/49): loss=0.3967557771678945\n",
      "Gradient Descent(12/49): loss=0.39497672685917884\n",
      "Gradient Descent(13/49): loss=0.39348892810370034\n",
      "Gradient Descent(14/49): loss=0.39222544191242936\n",
      "Gradient Descent(15/49): loss=0.3911354504398061\n",
      "Gradient Descent(16/49): loss=0.3901803396722211\n",
      "Gradient Descent(17/49): loss=0.389330738992602\n",
      "Gradient Descent(18/49): loss=0.3885642825884621\n",
      "Gradient Descent(19/49): loss=0.38786391578271157\n",
      "Gradient Descent(20/49): loss=0.387216612993627\n",
      "Gradient Descent(21/49): loss=0.3866124068193846\n",
      "Gradient Descent(22/49): loss=0.3860436524106639\n",
      "Gradient Descent(23/49): loss=0.38550446987025094\n",
      "Gradient Descent(24/49): loss=0.3849903214177136\n",
      "Gradient Descent(25/49): loss=0.38449769061547745\n",
      "Gradient Descent(26/49): loss=0.38402383892124337\n",
      "Gradient Descent(27/49): loss=0.38356662084963006\n",
      "Gradient Descent(28/49): loss=0.3831243435733601\n",
      "Gradient Descent(29/49): loss=0.3826956602323961\n",
      "Gradient Descent(30/49): loss=0.382279488820097\n",
      "Gradient Descent(31/49): loss=0.3818749504835854\n",
      "Gradient Descent(32/49): loss=0.3814813225655776\n",
      "Gradient Descent(33/49): loss=0.3810980028435142\n",
      "Gradient Descent(34/49): loss=0.3807244822769432\n",
      "Gradient Descent(35/49): loss=0.3803603242222591\n",
      "Gradient Descent(36/49): loss=0.380005148565337\n",
      "Gradient Descent(37/49): loss=0.3796586195953397\n",
      "Gradient Descent(38/49): loss=0.37932043672577104\n",
      "Gradient Descent(39/49): loss=0.37899032738347277\n",
      "Gradient Descent(40/49): loss=0.37866804154919054\n",
      "Gradient Descent(41/49): loss=0.3783533475570559\n",
      "Gradient Descent(42/49): loss=0.3780460288543139\n",
      "Gradient Descent(43/49): loss=0.3777458814940222\n",
      "Gradient Descent(44/49): loss=0.3774527121877207\n",
      "Gradient Descent(45/49): loss=0.37716633678632194\n",
      "Gradient Descent(46/49): loss=0.3768865790888465\n",
      "Gradient Descent(47/49): loss=0.3766132699024912\n",
      "Gradient Descent(48/49): loss=0.3763462462956769\n",
      "Gradient Descent(49/49): loss=0.37608535099954443\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4748512565619997\n",
      "Gradient Descent(2/49): loss=0.45566499833922497\n",
      "Gradient Descent(3/49): loss=0.4409796255103976\n",
      "Gradient Descent(4/49): loss=0.4296954381383608\n",
      "Gradient Descent(5/49): loss=0.42098420279347426\n",
      "Gradient Descent(6/49): loss=0.4142215659833289\n",
      "Gradient Descent(7/49): loss=0.4089364643624528\n",
      "Gradient Descent(8/49): loss=0.4047732043534576\n",
      "Gradient Descent(9/49): loss=0.40146300229446874\n",
      "Gradient Descent(10/49): loss=0.39880260030903925\n",
      "Gradient Descent(11/49): loss=0.3966381819495596\n",
      "Gradient Descent(12/49): loss=0.3948532626440833\n",
      "Gradient Descent(13/49): loss=0.3933595647876229\n",
      "Gradient Descent(14/49): loss=0.3920901364076841\n",
      "Gradient Descent(15/49): loss=0.39099415800099024\n",
      "Gradient Descent(16/49): loss=0.39003302077140517\n",
      "Gradient Descent(17/49): loss=0.3891773631752955\n",
      "Gradient Descent(18/49): loss=0.38840483032485934\n",
      "Gradient Descent(19/49): loss=0.38769837902372806\n",
      "Gradient Descent(20/49): loss=0.3870449949211721\n",
      "Gradient Descent(21/49): loss=0.38643472112345234\n",
      "Gradient Descent(22/49): loss=0.3858599223152879\n",
      "Gradient Descent(23/49): loss=0.3853147270533363\n",
      "Gradient Descent(24/49): loss=0.38479460491669343\n",
      "Gradient Descent(25/49): loss=0.3842960457746406\n",
      "Gradient Descent(26/49): loss=0.3838163164123778\n",
      "Gradient Descent(27/49): loss=0.38335327578169\n",
      "Gradient Descent(28/49): loss=0.3829052346966134\n",
      "Gradient Descent(29/49): loss=0.38247084923612895\n",
      "Gradient Descent(30/49): loss=0.38204903971917376\n",
      "Gradient Descent(31/49): loss=0.3816389290871081\n",
      "Gradient Descent(32/49): loss=0.381239796019959\n",
      "Gradient Descent(33/49): loss=0.38085103924205116\n",
      "Gradient Descent(34/49): loss=0.38047215032819626\n",
      "Gradient Descent(35/49): loss=0.3801026929699987\n",
      "Gradient Descent(36/49): loss=0.379742287153411\n",
      "Gradient Descent(37/49): loss=0.37939059707145667\n",
      "Gradient Descent(38/49): loss=0.37904732187884044\n",
      "Gradient Descent(39/49): loss=0.37871218860976036\n",
      "Gradient Descent(40/49): loss=0.37838494674312456\n",
      "Gradient Descent(41/49): loss=0.3780653640230471\n",
      "Gradient Descent(42/49): loss=0.3777532232364267\n",
      "Gradient Descent(43/49): loss=0.3774483197207583\n",
      "Gradient Descent(44/49): loss=0.37715045942954994\n",
      "Gradient Descent(45/49): loss=0.3768594574239237\n",
      "Gradient Descent(46/49): loss=0.3765751366903147\n",
      "Gradient Descent(47/49): loss=0.3762973272080046\n",
      "Gradient Descent(48/49): loss=0.37602586520835535\n",
      "Gradient Descent(49/49): loss=0.3757605925814007\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47490247934106855\n",
      "Gradient Descent(2/49): loss=0.4557655875455755\n",
      "Gradient Descent(3/49): loss=0.441125968236976\n",
      "Gradient Descent(4/49): loss=0.4298830358880223\n",
      "Gradient Descent(5/49): loss=0.42120826155294977\n",
      "Gradient Descent(6/49): loss=0.4144773750549215\n",
      "Gradient Descent(7/49): loss=0.4092196221764727\n",
      "Gradient Descent(8/49): loss=0.40507973734790975\n",
      "Gradient Descent(9/49): loss=0.40178941190380363\n",
      "Gradient Descent(10/49): loss=0.3991458638367953\n",
      "Gradient Descent(11/49): loss=0.39699572571760605\n",
      "Gradient Descent(12/49): loss=0.39522292014816457\n",
      "Gradient Descent(13/49): loss=0.39373952837448034\n",
      "Gradient Descent(14/49): loss=0.3924789079353446\n",
      "Gradient Descent(15/49): loss=0.3913905017872916\n",
      "Gradient Descent(16/49): loss=0.39043592065327193\n",
      "Gradient Descent(17/49): loss=0.38958598451287774\n",
      "Gradient Descent(18/49): loss=0.3888184871504527\n",
      "Gradient Descent(19/49): loss=0.3881165061494863\n",
      "Gradient Descent(20/49): loss=0.3874671246035717\n",
      "Gradient Descent(21/49): loss=0.38686046377941186\n",
      "Gradient Descent(22/49): loss=0.38628895075433745\n",
      "Gradient Descent(23/49): loss=0.3857467637040888\n",
      "Gradient Descent(24/49): loss=0.38522941156486806\n",
      "Gradient Descent(25/49): loss=0.3847334153812707\n",
      "Gradient Descent(26/49): loss=0.38425606663639433\n",
      "Gradient Descent(27/49): loss=0.38379524388577074\n",
      "Gradient Descent(28/49): loss=0.3833492735661402\n",
      "Gradient Descent(29/49): loss=0.38291682428684576\n",
      "Gradient Descent(30/49): loss=0.3824968265091488\n",
      "Gradient Descent(31/49): loss=0.38208841148289757\n",
      "Gradient Descent(32/49): loss=0.38169086479580927\n",
      "Gradient Descent(33/49): loss=0.38130359101508604\n",
      "Gradient Descent(34/49): loss=0.38092608675239364\n",
      "Gradient Descent(35/49): loss=0.38055792012798595\n",
      "Gradient Descent(36/49): loss=0.38019871509823616\n",
      "Gradient Descent(37/49): loss=0.3798481394810494\n",
      "Gradient Descent(38/49): loss=0.379505895794302\n",
      "Gradient Descent(39/49): loss=0.379171714235307\n",
      "Gradient Descent(40/49): loss=0.37884534729077507\n",
      "Gradient Descent(41/49): loss=0.3785265655892761\n",
      "Gradient Descent(42/49): loss=0.37821515470121647\n",
      "Gradient Descent(43/49): loss=0.37791091266197013\n",
      "Gradient Descent(44/49): loss=0.3776136480474466\n",
      "Gradient Descent(45/49): loss=0.37732317847213315\n",
      "Gradient Descent(46/49): loss=0.3770393294106266\n",
      "Gradient Descent(47/49): loss=0.37676193326721946\n",
      "Gradient Descent(48/49): loss=0.3764908286360182\n",
      "Gradient Descent(49/49): loss=0.3762258597077012\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4749178224654833\n",
      "Gradient Descent(2/49): loss=0.4556402248316454\n",
      "Gradient Descent(3/49): loss=0.4407838517101148\n",
      "Gradient Descent(4/49): loss=0.4292972497810827\n",
      "Gradient Descent(5/49): loss=0.4203809015914056\n",
      "Gradient Descent(6/49): loss=0.4134266167887285\n",
      "Gradient Descent(7/49): loss=0.40797156567007214\n",
      "Gradient Descent(8/49): loss=0.4036633982788362\n",
      "Gradient Descent(9/49): loss=0.40023376272230343\n",
      "Gradient Descent(10/49): loss=0.3974781916048643\n",
      "Gradient Descent(11/49): loss=0.3952408194398054\n",
      "Gradient Descent(12/49): loss=0.39340276679157365\n",
      "Gradient Descent(13/49): loss=0.3918733087108203\n",
      "Gradient Descent(14/49): loss=0.3905831582133168\n",
      "Gradient Descent(15/49): loss=0.3894793569689242\n",
      "Gradient Descent(16/49): loss=0.38852138767240035\n",
      "Gradient Descent(17/49): loss=0.38767821529947283\n",
      "Gradient Descent(18/49): loss=0.3869260347996295\n",
      "Gradient Descent(19/49): loss=0.3862465561703197\n",
      "Gradient Descent(20/49): loss=0.3856256983991683\n",
      "Gradient Descent(21/49): loss=0.38505259455594004\n",
      "Gradient Descent(22/49): loss=0.38451883371519446\n",
      "Gradient Descent(23/49): loss=0.3840178831751528\n",
      "Gradient Descent(24/49): loss=0.38354464795892546\n",
      "Gradient Descent(25/49): loss=0.3830951348655537\n",
      "Gradient Descent(26/49): loss=0.38266619615799596\n",
      "Gradient Descent(27/49): loss=0.38225533392372635\n",
      "Gradient Descent(28/49): loss=0.38186055066952923\n",
      "Gradient Descent(29/49): loss=0.38148023515614576\n",
      "Gradient Descent(30/49): loss=0.38111307509964365\n",
      "Gradient Descent(31/49): loss=0.3807579903616432\n",
      "Gradient Descent(32/49): loss=0.380414081769509\n",
      "Gradient Descent(33/49): loss=0.38008059186418275\n",
      "Gradient Descent(34/49): loss=0.37975687475406367\n",
      "Gradient Descent(35/49): loss=0.37944237292412764\n",
      "Gradient Descent(36/49): loss=0.3791365993604152\n",
      "Gradient Descent(37/49): loss=0.3788391237392927\n",
      "Gradient Descent(38/49): loss=0.37854956172747706\n",
      "Gradient Descent(39/49): loss=0.3782675666648549\n",
      "Gradient Descent(40/49): loss=0.3779928230744066\n",
      "Gradient Descent(41/49): loss=0.37772504157488873\n",
      "Gradient Descent(42/49): loss=0.37746395487208084\n",
      "Gradient Descent(43/49): loss=0.3772093145807803\n",
      "Gradient Descent(44/49): loss=0.37696088868800626\n",
      "Gradient Descent(45/49): loss=0.3767184595123319\n",
      "Gradient Descent(46/49): loss=0.37648182204821057\n",
      "Gradient Descent(47/49): loss=0.3762507826100758\n",
      "Gradient Descent(48/49): loss=0.37602515771079476\n",
      "Gradient Descent(49/49): loss=0.37580477312418276\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4702606764586826\n",
      "Gradient Descent(2/49): loss=0.4488591731934721\n",
      "Gradient Descent(3/49): loss=0.4333788450194108\n",
      "Gradient Descent(4/49): loss=0.42211050109350085\n",
      "Gradient Descent(5/49): loss=0.4138434246459872\n",
      "Gradient Descent(6/49): loss=0.4077187955152349\n",
      "Gradient Descent(7/49): loss=0.40312663209882993\n",
      "Gradient Descent(8/49): loss=0.3996331772999103\n",
      "Gradient Descent(9/49): loss=0.39692963935164877\n",
      "Gradient Descent(10/49): loss=0.39479594766022696\n",
      "Gradient Descent(11/49): loss=0.39307508807909936\n",
      "Gradient Descent(12/49): loss=0.3916549060600824\n",
      "Gradient Descent(13/49): loss=0.390455189826229\n",
      "Gradient Descent(14/49): loss=0.38941849203636747\n",
      "Gradient Descent(15/49): loss=0.38850360183539906\n",
      "Gradient Descent(16/49): loss=0.3876808980118706\n",
      "Gradient Descent(17/49): loss=0.38692903862295686\n",
      "Gradient Descent(18/49): loss=0.386232601004521\n",
      "Gradient Descent(19/49): loss=0.38558039817549483\n",
      "Gradient Descent(20/49): loss=0.38496427699904084\n",
      "Gradient Descent(21/49): loss=0.38437825970930517\n",
      "Gradient Descent(22/49): loss=0.3838179303246579\n",
      "Gradient Descent(23/49): loss=0.38327999581775984\n",
      "Gradient Descent(24/49): loss=0.3827619720672926\n",
      "Gradient Descent(25/49): loss=0.38226195895620313\n",
      "Gradient Descent(26/49): loss=0.38177847919177627\n",
      "Gradient Descent(27/49): loss=0.3813103626978362\n",
      "Gradient Descent(28/49): loss=0.38085666361603077\n",
      "Gradient Descent(29/49): loss=0.3804166006530452\n",
      "Gradient Descent(30/49): loss=0.3799895141513079\n",
      "Gradient Descent(31/49): loss=0.37957483514645746\n",
      "Gradient Descent(32/49): loss=0.379172063022019\n",
      "Gradient Descent(33/49): loss=0.3787807493346392\n",
      "Gradient Descent(34/49): loss=0.3784004860717518\n",
      "Gradient Descent(35/49): loss=0.3780308970960957\n",
      "Gradient Descent(36/49): loss=0.3776716318840019\n",
      "Gradient Descent(37/49): loss=0.37732236091674415\n",
      "Gradient Descent(38/49): loss=0.3769827722650215\n",
      "Gradient Descent(39/49): loss=0.376652569036175\n",
      "Gradient Descent(40/49): loss=0.376331467446616\n",
      "Gradient Descent(41/49): loss=0.3760191953485549\n",
      "Gradient Descent(42/49): loss=0.37571549108792435\n",
      "Gradient Descent(43/49): loss=0.3754201026047212\n",
      "Gradient Descent(44/49): loss=0.37513278671165634\n",
      "Gradient Descent(45/49): loss=0.3748533085047382\n",
      "Gradient Descent(46/49): loss=0.37458144087218115\n",
      "Gradient Descent(47/49): loss=0.3743169640772212\n",
      "Gradient Descent(48/49): loss=0.37405966539705454\n",
      "Gradient Descent(49/49): loss=0.3738093388048972\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4702313600190232\n",
      "Gradient Descent(2/49): loss=0.4488089287878819\n",
      "Gradient Descent(3/49): loss=0.43331330478154845\n",
      "Gradient Descent(4/49): loss=0.4220332856040302\n",
      "Gradient Descent(5/49): loss=0.41375675833196734\n",
      "Gradient Descent(6/49): loss=0.4076239723489979\n",
      "Gradient Descent(7/49): loss=0.40302434911466656\n",
      "Gradient Descent(8/49): loss=0.39952376407904383\n",
      "Gradient Descent(9/49): loss=0.39681321081183557\n",
      "Gradient Descent(10/49): loss=0.3946725027131634\n",
      "Gradient Descent(11/49): loss=0.392944571306599\n",
      "Gradient Descent(12/49): loss=0.3915172448005337\n",
      "Gradient Descent(13/49): loss=0.39031031531290267\n",
      "Gradient Descent(14/49): loss=0.3892663504528629\n",
      "Gradient Descent(15/49): loss=0.38834415918009213\n",
      "Gradient Descent(16/49): loss=0.38751414130109\n",
      "Gradient Descent(17/49): loss=0.386754975043715\n",
      "Gradient Descent(18/49): loss=0.386051256019546\n",
      "Gradient Descent(19/49): loss=0.3853918131878358\n",
      "Gradient Descent(20/49): loss=0.3847685069306446\n",
      "Gradient Descent(21/49): loss=0.3841753706891651\n",
      "Gradient Descent(22/49): loss=0.3836079975844755\n",
      "Gradient Descent(23/49): loss=0.38306310183457776\n",
      "Gradient Descent(24/49): loss=0.3825382049588132\n",
      "Gradient Descent(25/49): loss=0.38203141111611844\n",
      "Gradient Descent(26/49): loss=0.3815412461434038\n",
      "Gradient Descent(27/49): loss=0.38106654214087143\n",
      "Gradient Descent(28/49): loss=0.38060635464090786\n",
      "Gradient Descent(29/49): loss=0.3801599030987896\n",
      "Gradient Descent(30/49): loss=0.3797265280849769\n",
      "Gradient Descent(31/49): loss=0.3793056604447789\n",
      "Gradient Descent(32/49): loss=0.3788967990383645\n",
      "Gradient Descent(33/49): loss=0.37849949463684346\n",
      "Gradient Descent(34/49): loss=0.3781133382384486\n",
      "Gradient Descent(35/49): loss=0.3777379525611455\n",
      "Gradient Descent(36/49): loss=0.37737298582025\n",
      "Gradient Descent(37/49): loss=0.37701810715177947\n",
      "Gradient Descent(38/49): loss=0.3766730032228247\n",
      "Gradient Descent(39/49): loss=0.3763373756995831\n",
      "Gradient Descent(40/49): loss=0.3760109393364051\n",
      "Gradient Descent(41/49): loss=0.37569342051567545\n",
      "Gradient Descent(42/49): loss=0.3753845561160438\n",
      "Gradient Descent(43/49): loss=0.3750840926207392\n",
      "Gradient Descent(44/49): loss=0.3747917854022908\n",
      "Gradient Descent(45/49): loss=0.3745073981376356\n",
      "Gradient Descent(46/49): loss=0.37423070232030037\n",
      "Gradient Descent(47/49): loss=0.3739614768454856\n",
      "Gradient Descent(48/49): loss=0.37369950765047016\n",
      "Gradient Descent(49/49): loss=0.3734445873974988\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4702927894385848\n",
      "Gradient Descent(2/49): loss=0.4489287881747865\n",
      "Gradient Descent(3/49): loss=0.43348582304344735\n",
      "Gradient Descent(4/49): loss=0.42225159632929243\n",
      "Gradient Descent(5/49): loss=0.41401392297809286\n",
      "Gradient Descent(6/49): loss=0.4079135487693796\n",
      "Gradient Descent(7/49): loss=0.4033406609253345\n",
      "Gradient Descent(8/49): loss=0.3998619842595743\n",
      "Gradient Descent(9/49): loss=0.39716933792624076\n",
      "Gradient Descent(10/49): loss=0.3950432814575226\n",
      "Gradient Descent(11/49): loss=0.393327389896534\n",
      "Gradient Descent(12/49): loss=0.3919100285062979\n",
      "Gradient Descent(13/49): loss=0.3907114267189823\n",
      "Gradient Descent(14/49): loss=0.3896745014756235\n",
      "Gradient Descent(15/49): loss=0.38875833651939495\n",
      "Gradient Descent(16/49): loss=0.38793354509234473\n",
      "Gradient Descent(17/49): loss=0.38717896949397385\n",
      "Gradient Descent(18/49): loss=0.3864793303981867\n",
      "Gradient Descent(19/49): loss=0.3858235514622992\n",
      "Gradient Descent(20/49): loss=0.385203564438085\n",
      "Gradient Descent(21/49): loss=0.38461345642083006\n",
      "Gradient Descent(22/49): loss=0.3840488608745352\n",
      "Gradient Descent(23/49): loss=0.38350652245714123\n",
      "Gradient Descent(24/49): loss=0.38298398582974214\n",
      "Gradient Descent(25/49): loss=0.3824793729631745\n",
      "Gradient Descent(26/49): loss=0.3819912236478787\n",
      "Gradient Descent(27/49): loss=0.3815183811677375\n",
      "Gradient Descent(28/49): loss=0.3810599102656436\n",
      "Gradient Descent(29/49): loss=0.38061503821079173\n",
      "Gradient Descent(30/49): loss=0.3801831124032519\n",
      "Gradient Descent(31/49): loss=0.37976356982446763\n",
      "Gradient Descent(32/49): loss=0.37935591497925447\n",
      "Gradient Descent(33/49): loss=0.37895970392958495\n",
      "Gradient Descent(34/49): loss=0.3785745327025218\n",
      "Gradient Descent(35/49): loss=0.3782000288421905\n",
      "Gradient Descent(36/49): loss=0.377835845224309\n",
      "Gradient Descent(37/49): loss=0.377481655501208\n",
      "Gradient Descent(38/49): loss=0.3771371507238052\n",
      "Gradient Descent(39/49): loss=0.3768020368148366\n",
      "Gradient Descent(40/49): loss=0.37647603265925283\n",
      "Gradient Descent(41/49): loss=0.3761588686433545\n",
      "Gradient Descent(42/49): loss=0.37585028552134747\n",
      "Gradient Descent(43/49): loss=0.37555003352181077\n",
      "Gradient Descent(44/49): loss=0.37525787163086094\n",
      "Gradient Descent(45/49): loss=0.37497356700625595\n",
      "Gradient Descent(46/49): loss=0.3746968944892521\n",
      "Gradient Descent(47/49): loss=0.374427636190076\n",
      "Gradient Descent(48/49): loss=0.3741655811294063\n",
      "Gradient Descent(49/49): loss=0.3739105249229747\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47029867135760256\n",
      "Gradient Descent(2/49): loss=0.44873005086092843\n",
      "Gradient Descent(3/49): loss=0.43300064095985114\n",
      "Gradient Descent(4/49): loss=0.4214680395382122\n",
      "Gradient Descent(5/49): loss=0.4129553605907347\n",
      "Gradient Descent(6/49): loss=0.4066187828105744\n",
      "Gradient Descent(7/49): loss=0.40185292903856795\n",
      "Gradient Descent(8/49): loss=0.3982232184578272\n",
      "Gradient Descent(9/49): loss=0.3954174704862349\n",
      "Gradient Descent(10/49): loss=0.3932112618427826\n",
      "Gradient Descent(11/49): loss=0.39144311613994504\n",
      "Gradient Descent(12/49): loss=0.3899967275196293\n",
      "Gradient Descent(13/49): loss=0.3887882190839625\n",
      "Gradient Descent(14/49): loss=0.38775700679954844\n",
      "Gradient Descent(15/49): loss=0.3868592463782481\n",
      "Gradient Descent(16/49): loss=0.38606313128342745\n",
      "Gradient Descent(17/49): loss=0.385345517803926\n",
      "Gradient Descent(18/49): loss=0.38468950178641154\n",
      "Gradient Descent(19/49): loss=0.3840826780091604\n",
      "Gradient Descent(20/49): loss=0.3835158893618751\n",
      "Gradient Descent(21/49): loss=0.38298232756596107\n",
      "Gradient Descent(22/49): loss=0.38247688627154125\n",
      "Gradient Descent(23/49): loss=0.3819956953939513\n",
      "Gradient Descent(24/49): loss=0.3815357856458172\n",
      "Gradient Descent(25/49): loss=0.3810948466300509\n",
      "Gradient Descent(26/49): loss=0.3806710521944663\n",
      "Gradient Descent(27/49): loss=0.3802629341635995\n",
      "Gradient Descent(28/49): loss=0.3798692908840556\n",
      "Gradient Descent(29/49): loss=0.37948912083853725\n",
      "Gradient Descent(30/49): loss=0.3791215743251523\n",
      "Gradient Descent(31/49): loss=0.37876591816704314\n",
      "Gradient Descent(32/49): loss=0.37842150983110595\n",
      "Gradient Descent(33/49): loss=0.3780877783501473\n",
      "Gradient Descent(34/49): loss=0.37776421017258177\n",
      "Gradient Descent(35/49): loss=0.3774503385882988\n",
      "Gradient Descent(36/49): loss=0.37714573575645877\n",
      "Gradient Descent(37/49): loss=0.376850006632239\n",
      "Gradient Descent(38/49): loss=0.3765627842847326\n",
      "Gradient Descent(39/49): loss=0.37628372623871975\n",
      "Gradient Descent(40/49): loss=0.37601251157423626\n",
      "Gradient Descent(41/49): loss=0.3757488385908172\n",
      "Gradient Descent(42/49): loss=0.3754924228959078\n",
      "Gradient Descent(43/49): loss=0.3752429958149163\n",
      "Gradient Descent(44/49): loss=0.3750003030478422\n",
      "Gradient Descent(45/49): loss=0.374764103517273\n",
      "Gradient Descent(46/49): loss=0.3745341683669426\n",
      "Gradient Descent(47/49): loss=0.3743102800804984\n",
      "Gradient Descent(48/49): loss=0.3740922316977319\n",
      "Gradient Descent(49/49): loss=0.37387982611107595\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46578179656921015\n",
      "Gradient Descent(2/49): loss=0.4425909793625242\n",
      "Gradient Descent(3/49): loss=0.42675369729334656\n",
      "Gradient Descent(4/49): loss=0.4158319322765183\n",
      "Gradient Descent(5/49): loss=0.40820422297731085\n",
      "Gradient Descent(6/49): loss=0.40279007991479654\n",
      "Gradient Descent(7/49): loss=0.3988682336244702\n",
      "Gradient Descent(8/49): loss=0.3959564206241145\n",
      "Gradient Descent(9/49): loss=0.39373167506318046\n",
      "Gradient Descent(10/49): loss=0.39197736552531093\n",
      "Gradient Descent(11/49): loss=0.39054793833661955\n",
      "Gradient Descent(12/49): loss=0.38934541024338826\n",
      "Gradient Descent(13/49): loss=0.38830367292001156\n",
      "Gradient Descent(14/49): loss=0.3873780002332395\n",
      "Gradient Descent(15/49): loss=0.38653802576767204\n",
      "Gradient Descent(16/49): loss=0.3857630380711293\n",
      "Gradient Descent(17/49): loss=0.38503882566491693\n",
      "Gradient Descent(18/49): loss=0.38435555940244\n",
      "Gradient Descent(19/49): loss=0.3837063698446359\n",
      "Gradient Descent(20/49): loss=0.38308639069856165\n",
      "Gradient Descent(21/49): loss=0.38249211504238445\n",
      "Gradient Descent(22/49): loss=0.38192096163148936\n",
      "Gradient Descent(23/49): loss=0.3813709824102055\n",
      "Gradient Descent(24/49): loss=0.3808406650052682\n",
      "Gradient Descent(25/49): loss=0.3803287991569718\n",
      "Gradient Descent(26/49): loss=0.3798343862245376\n",
      "Gradient Descent(27/49): loss=0.3793565777348597\n",
      "Gradient Descent(28/49): loss=0.37889463353257125\n",
      "Gradient Descent(29/49): loss=0.3784478931731432\n",
      "Gradient Descent(30/49): loss=0.37801575627437334\n",
      "Gradient Descent(31/49): loss=0.37759766893685254\n",
      "Gradient Descent(32/49): loss=0.37719311428333113\n",
      "Gradient Descent(33/49): loss=0.3768016057996945\n",
      "Gradient Descent(34/49): loss=0.3764226825868069\n",
      "Gradient Descent(35/49): loss=0.37605590592020627\n",
      "Gradient Descent(36/49): loss=0.3757008567088553\n",
      "Gradient Descent(37/49): loss=0.37535713357536127\n",
      "Gradient Descent(38/49): loss=0.37502435136880247\n",
      "Gradient Descent(39/49): loss=0.37470213998134944\n",
      "Gradient Descent(40/49): loss=0.37439014338057236\n",
      "Gradient Descent(41/49): loss=0.3740880187969414\n",
      "Gradient Descent(42/49): loss=0.3737954360248067\n",
      "Gradient Descent(43/49): loss=0.37351207680792947\n",
      "Gradient Descent(44/49): loss=0.3732376342893709\n",
      "Gradient Descent(45/49): loss=0.3729718125115211\n",
      "Gradient Descent(46/49): loss=0.3727143259561593\n",
      "Gradient Descent(47/49): loss=0.3724648991172723\n",
      "Gradient Descent(48/49): loss=0.3722232661013203\n",
      "Gradient Descent(49/49): loss=0.3719891702510101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4657480808575879\n",
      "Gradient Descent(2/49): loss=0.4425346099291246\n",
      "Gradient Descent(3/49): loss=0.42668151502860246\n",
      "Gradient Descent(4/49): loss=0.4157479048831384\n",
      "Gradient Descent(5/49): loss=0.40811049242006125\n",
      "Gradient Descent(6/49): loss=0.4026876920439774\n",
      "Gradient Descent(7/49): loss=0.39875761345894695\n",
      "Gradient Descent(8/49): loss=0.3958376668478149\n",
      "Gradient Descent(9/49): loss=0.39360473452084466\n",
      "Gradient Descent(10/49): loss=0.3918421317588591\n",
      "Gradient Descent(11/49): loss=0.3904043038482578\n",
      "Gradient Descent(12/49): loss=0.389193291567123\n",
      "Gradient Descent(13/49): loss=0.38814302033637277\n",
      "Gradient Descent(14/49): loss=0.3872087992471695\n",
      "Gradient Descent(15/49): loss=0.3863602945081279\n",
      "Gradient Descent(16/49): loss=0.38557682296230095\n",
      "Gradient Descent(17/49): loss=0.38484419664147557\n",
      "Gradient Descent(18/49): loss=0.3841526053149816\n",
      "Gradient Descent(19/49): loss=0.38349519434748164\n",
      "Gradient Descent(20/49): loss=0.38286710871895846\n",
      "Gradient Descent(21/49): loss=0.38226484983014297\n",
      "Gradient Descent(22/49): loss=0.38168584234071345\n",
      "Gradient Descent(23/49): loss=0.3811281421462641\n",
      "Gradient Descent(24/49): loss=0.380590239266831\n",
      "Gradient Descent(25/49): loss=0.3800709246070449\n",
      "Gradient Descent(26/49): loss=0.3795691997317241\n",
      "Gradient Descent(27/49): loss=0.37908421563427863\n",
      "Gradient Descent(28/49): loss=0.37861523106384304\n",
      "Gradient Descent(29/49): loss=0.37816158406008504\n",
      "Gradient Descent(30/49): loss=0.37772267241735213\n",
      "Gradient Descent(31/49): loss=0.37729794019410756\n",
      "Gradient Descent(32/49): loss=0.37688686832206997\n",
      "Gradient Descent(33/49): loss=0.3764889680014858\n",
      "Gradient Descent(34/49): loss=0.37610377599485234\n",
      "Gradient Descent(35/49): loss=0.3757308512185703\n",
      "Gradient Descent(36/49): loss=0.3753697722257636\n",
      "Gradient Descent(37/49): loss=0.3750201353043243\n",
      "Gradient Descent(38/49): loss=0.37468155300264666\n",
      "Gradient Descent(39/49): loss=0.3743536529553106\n",
      "Gradient Descent(40/49): loss=0.3740360769214587\n",
      "Gradient Descent(41/49): loss=0.3737284799760627\n",
      "Gradient Descent(42/49): loss=0.37343052981290814\n",
      "Gradient Descent(43/49): loss=0.37314190613080506\n",
      "Gradient Descent(44/49): loss=0.37286230008317167\n",
      "Gradient Descent(45/49): loss=0.3725914137770439\n",
      "Gradient Descent(46/49): loss=0.3723289598116123\n",
      "Gradient Descent(47/49): loss=0.3720746608491769\n",
      "Gradient Descent(48/49): loss=0.3718282492133354\n",
      "Gradient Descent(49/49): loss=0.37158946651056396\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46581970427920194\n",
      "Gradient Descent(2/49): loss=0.44267334199244923\n",
      "Gradient Descent(3/49): loss=0.4268788438535168\n",
      "Gradient Descent(4/49): loss=0.4159941957021524\n",
      "Gradient Descent(5/49): loss=0.4083965467925483\n",
      "Gradient Descent(6/49): loss=0.40300546679429605\n",
      "Gradient Descent(7/49): loss=0.3991004450625717\n",
      "Gradient Descent(8/49): loss=0.3962002309951249\n",
      "Gradient Descent(9/49): loss=0.3939828830656185\n",
      "Gradient Descent(10/49): loss=0.39223268919849885\n",
      "Gradient Descent(11/49): loss=0.3908048669295275\n",
      "Gradient Descent(12/49): loss=0.3896020522533043\n",
      "Gradient Descent(13/49): loss=0.3885586190558583\n",
      "Gradient Descent(14/49): loss=0.38763020832298123\n",
      "Gradient Descent(15/49): loss=0.3867867284030348\n",
      "Gradient Descent(16/49): loss=0.3860076708296582\n",
      "Gradient Descent(17/49): loss=0.385278972639738\n",
      "Gradient Descent(18/49): loss=0.38459091262969364\n",
      "Gradient Descent(19/49): loss=0.3839366995353763\n",
      "Gradient Descent(20/49): loss=0.3833115236705262\n",
      "Gradient Descent(21/49): loss=0.38271191925853276\n",
      "Gradient Descent(22/49): loss=0.3821353352167028\n",
      "Gradient Descent(23/49): loss=0.38157984590887306\n",
      "Gradient Descent(24/49): loss=0.3810439559571258\n",
      "Gradient Descent(25/49): loss=0.3805264683135468\n",
      "Gradient Descent(26/49): loss=0.38002639491485357\n",
      "Gradient Descent(27/49): loss=0.37954289602817204\n",
      "Gradient Descent(28/49): loss=0.3790752389482655\n",
      "Gradient Descent(29/49): loss=0.3786227697623176\n",
      "Gradient Descent(30/49): loss=0.3781848939511077\n",
      "Gradient Descent(31/49): loss=0.3777610629752404\n",
      "Gradient Descent(32/49): loss=0.3773507649232069\n",
      "Gradient Descent(33/49): loss=0.3769535179227552\n",
      "Gradient Descent(34/49): loss=0.3765688654378225\n",
      "Gradient Descent(35/49): loss=0.376196372856916\n",
      "Gradient Descent(36/49): loss=0.3758356249701778\n",
      "Gradient Descent(37/49): loss=0.3754862240615794\n",
      "Gradient Descent(38/49): loss=0.37514778843002894\n",
      "Gradient Descent(39/49): loss=0.37481995121228806\n",
      "Gradient Descent(40/49): loss=0.37450235942065035\n",
      "Gradient Descent(41/49): loss=0.37419467313552757\n",
      "Gradient Descent(42/49): loss=0.37389656481158706\n",
      "Gradient Descent(43/49): loss=0.37360771866869\n",
      "Gradient Descent(44/49): loss=0.3733278301474976\n",
      "Gradient Descent(45/49): loss=0.3730566054155279\n",
      "Gradient Descent(46/49): loss=0.37279376091351085\n",
      "Gradient Descent(47/49): loss=0.37253902293470886\n",
      "Gradient Descent(48/49): loss=0.3722921272318247\n",
      "Gradient Descent(49/49): loss=0.37205281864749606\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46581194838272927\n",
      "Gradient Descent(2/49): loss=0.44239020530163703\n",
      "Gradient Descent(3/49): loss=0.42624247584137687\n",
      "Gradient Descent(4/49): loss=0.4150167412237039\n",
      "Gradient Descent(5/49): loss=0.40712751972080613\n",
      "Gradient Descent(6/49): loss=0.4015051270564532\n",
      "Gradient Descent(7/49): loss=0.3974272426623361\n",
      "Gradient Descent(8/49): loss=0.39440564728871225\n",
      "Gradient Descent(9/49): loss=0.39210999503470934\n",
      "Gradient Descent(10/49): loss=0.39031647339995595\n",
      "Gradient Descent(11/49): loss=0.3888732041680271\n",
      "Gradient Descent(12/49): loss=0.3876769135342858\n",
      "Gradient Descent(13/49): loss=0.3866571930677737\n",
      "Gradient Descent(14/49): loss=0.38576587655483596\n",
      "Gradient Descent(15/49): loss=0.38496986635990915\n",
      "Gradient Descent(16/49): loss=0.3842462867135604\n",
      "Gradient Descent(17/49): loss=0.38357920729796374\n",
      "Gradient Descent(18/49): loss=0.3829574269421723\n",
      "Gradient Descent(19/49): loss=0.38237297328217873\n",
      "Gradient Descent(20/49): loss=0.38182008616252794\n",
      "Gradient Descent(21/49): loss=0.3812945280265065\n",
      "Gradient Descent(22/49): loss=0.3807931154499044\n",
      "Gradient Descent(23/49): loss=0.3803134003238433\n",
      "Gradient Descent(24/49): loss=0.37985345237733453\n",
      "Gradient Descent(25/49): loss=0.37941171038388133\n",
      "Gradient Descent(26/49): loss=0.378986879968225\n",
      "Gradient Descent(27/49): loss=0.3785778630712678\n",
      "Gradient Descent(28/49): loss=0.3781837089575952\n",
      "Gradient Descent(29/49): loss=0.37780357991276947\n",
      "Gradient Descent(30/49): loss=0.37743672698411435\n",
      "Gradient Descent(31/49): loss=0.377082472611616\n",
      "Gradient Descent(32/49): loss=0.3767401980061483\n",
      "Gradient Descent(33/49): loss=0.3764093338167073\n",
      "Gradient Descent(34/49): loss=0.37608935309226094\n",
      "Gradient Descent(35/49): loss=0.37577976585851647\n",
      "Gradient Descent(36/49): loss=0.3754801148435774\n",
      "Gradient Descent(37/49): loss=0.37518997203171695\n",
      "Gradient Descent(38/49): loss=0.3749089358233876\n",
      "Gradient Descent(39/49): loss=0.37463662864703035\n",
      "Gradient Descent(40/49): loss=0.37437269491435426\n",
      "Gradient Descent(41/49): loss=0.37411679924236335\n",
      "Gradient Descent(42/49): loss=0.37386862488715333\n",
      "Gradient Descent(43/49): loss=0.37362787234952705\n",
      "Gradient Descent(44/49): loss=0.3733942581229227\n",
      "Gradient Descent(45/49): loss=0.3731675135614571\n",
      "Gradient Descent(46/49): loss=0.3729473838510451\n",
      "Gradient Descent(47/49): loss=0.37273362707023966\n",
      "Gradient Descent(48/49): loss=0.372526013330096\n",
      "Gradient Descent(49/49): loss=0.3723243239843115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4614393949751893\n",
      "Gradient Descent(2/49): loss=0.43686834512202066\n",
      "Gradient Descent(3/49): loss=0.42104004738568784\n",
      "Gradient Descent(4/49): loss=0.4106939835952835\n",
      "Gradient Descent(5/49): loss=0.40379790992422443\n",
      "Gradient Descent(6/49): loss=0.3990818600905075\n",
      "Gradient Descent(7/49): loss=0.39575036891096355\n",
      "Gradient Descent(8/49): loss=0.3933040890140693\n",
      "Gradient Descent(9/49): loss=0.39142887158937784\n",
      "Gradient Descent(10/49): loss=0.3899266158886555\n",
      "Gradient Descent(11/49): loss=0.38867206302621216\n",
      "Gradient Descent(12/49): loss=0.3875857487499906\n",
      "Gradient Descent(13/49): loss=0.3866170436920318\n",
      "Gradient Descent(14/49): loss=0.3857335032228614\n",
      "Gradient Descent(15/49): loss=0.38491417056762384\n",
      "Gradient Descent(16/49): loss=0.38414536049131387\n",
      "Gradient Descent(17/49): loss=0.38341800152423794\n",
      "Gradient Descent(18/49): loss=0.3827259585834994\n",
      "Gradient Descent(19/49): loss=0.38206497298932424\n",
      "Gradient Descent(20/49): loss=0.38143199168560754\n",
      "Gradient Descent(21/49): loss=0.38082474206378175\n",
      "Gradient Descent(22/49): loss=0.38024146193134634\n",
      "Gradient Descent(23/49): loss=0.3796807275886848\n",
      "Gradient Descent(24/49): loss=0.37914134401866445\n",
      "Gradient Descent(25/49): loss=0.37862227445187174\n",
      "Gradient Descent(26/49): loss=0.3781225949319775\n",
      "Gradient Descent(27/49): loss=0.37764146478351746\n",
      "Gradient Descent(28/49): loss=0.377178107218419\n",
      "Gradient Descent(29/49): loss=0.37673179642547255\n",
      "Gradient Descent(30/49): loss=0.3763018488207664\n",
      "Gradient Descent(31/49): loss=0.3758876169818682\n",
      "Gradient Descent(32/49): loss=0.3754884853241246\n",
      "Gradient Descent(33/49): loss=0.3751038669173924\n",
      "Gradient Descent(34/49): loss=0.3747332010575673\n",
      "Gradient Descent(35/49): loss=0.37437595134480367\n",
      "Gradient Descent(36/49): loss=0.37403160410803143\n",
      "Gradient Descent(37/49): loss=0.3736996670714261\n",
      "Gradient Descent(38/49): loss=0.3733796681944348\n",
      "Gradient Descent(39/49): loss=0.37307115464006224\n",
      "Gradient Descent(40/49): loss=0.37277369184105413\n",
      "Gradient Descent(41/49): loss=0.37248686264330966\n",
      "Gradient Descent(42/49): loss=0.372210266512185\n",
      "Gradient Descent(43/49): loss=0.3719435187915342\n",
      "Gradient Descent(44/49): loss=0.37168625000810146\n",
      "Gradient Descent(45/49): loss=0.3714381052157569\n",
      "Gradient Descent(46/49): loss=0.3711987433753432\n",
      "Gradient Descent(47/49): loss=0.37096783676678796\n",
      "Gradient Descent(48/49): loss=0.37074507043076554\n",
      "Gradient Descent(49/49): loss=0.37053014163764736\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46140141907769394\n",
      "Gradient Descent(2/49): loss=0.436806382016321\n",
      "Gradient Descent(3/49): loss=0.4209620164385305\n",
      "Gradient Descent(4/49): loss=0.41060397383891034\n",
      "Gradient Descent(5/49): loss=0.4036978044075243\n",
      "Gradient Descent(6/49): loss=0.39897236804832\n",
      "Gradient Descent(7/49): loss=0.3956316307835286\n",
      "Gradient Descent(8/49): loss=0.39317600906744665\n",
      "Gradient Descent(9/49): loss=0.39129128910898126\n",
      "Gradient Descent(10/49): loss=0.3897793857994495\n",
      "Gradient Descent(11/49): loss=0.3885150881934054\n",
      "Gradient Descent(12/49): loss=0.38741898795974017\n",
      "Gradient Descent(13/49): loss=0.3864405083892202\n",
      "Gradient Descent(14/49): loss=0.3855472498342749\n",
      "Gradient Descent(15/49): loss=0.3847182917907775\n",
      "Gradient Descent(16/49): loss=0.38393997708994804\n",
      "Gradient Descent(17/49): loss=0.38320325522675774\n",
      "Gradient Descent(18/49): loss=0.38250200621905267\n",
      "Gradient Descent(19/49): loss=0.3818319817881525\n",
      "Gradient Descent(20/49): loss=0.38119013559074727\n",
      "Gradient Descent(21/49): loss=0.3805741988837507\n",
      "Gradient Descent(22/49): loss=0.37998241117372344\n",
      "Gradient Descent(23/49): loss=0.3794133488352683\n",
      "Gradient Descent(24/49): loss=0.378865815725904\n",
      "Gradient Descent(25/49): loss=0.3783387730818779\n",
      "Gradient Descent(26/49): loss=0.37783129433822693\n",
      "Gradient Descent(27/49): loss=0.3773425357911348\n",
      "Gradient Descent(28/49): loss=0.376871717351789\n",
      "Gradient Descent(29/49): loss=0.37641810974631057\n",
      "Gradient Descent(30/49): loss=0.3759810258480373\n",
      "Gradient Descent(31/49): loss=0.3755598146714853\n",
      "Gradient Descent(32/49): loss=0.3751538570915062\n",
      "Gradient Descent(33/49): loss=0.3747625626899811\n",
      "Gradient Descent(34/49): loss=0.37438536734755995\n",
      "Gradient Descent(35/49): loss=0.3740217313347785\n",
      "Gradient Descent(36/49): loss=0.37367113774404465\n",
      "Gradient Descent(37/49): loss=0.3733330911596042\n",
      "Gradient Descent(38/49): loss=0.37300711649819857\n",
      "Gradient Descent(39/49): loss=0.3726927579759667\n",
      "Gradient Descent(40/49): loss=0.37238957817187196\n",
      "Gradient Descent(41/49): loss=0.3720971571674634\n",
      "Gradient Descent(42/49): loss=0.3718150917489959\n",
      "Gradient Descent(43/49): loss=0.3715429946620157\n",
      "Gradient Descent(44/49): loss=0.3712804939112172\n",
      "Gradient Descent(45/49): loss=0.3710272321002007\n",
      "Gradient Descent(46/49): loss=0.3707828658069872\n",
      "Gradient Descent(47/49): loss=0.37054706499201123\n",
      "Gradient Descent(48/49): loss=0.3703195124359104\n",
      "Gradient Descent(49/49): loss=0.37009990320487285\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4614832238629201\n",
      "Gradient Descent(2/49): loss=0.4369635458385442\n",
      "Gradient Descent(3/49): loss=0.4211827088325581\n",
      "Gradient Descent(4/49): loss=0.4108755033232366\n",
      "Gradient Descent(5/49): loss=0.4040087618355295\n",
      "Gradient Descent(6/49): loss=0.399313387105225\n",
      "Gradient Descent(7/49): loss=0.3959954351219342\n",
      "Gradient Descent(8/49): loss=0.3935571464566616\n",
      "Gradient Descent(9/49): loss=0.39168577009463634\n",
      "Gradient Descent(10/49): loss=0.3901843326107617\n",
      "Gradient Descent(11/49): loss=0.38892843674920674\n",
      "Gradient Descent(12/49): loss=0.3878392533212014\n",
      "Gradient Descent(13/49): loss=0.3868666090470993\n",
      "Gradient Descent(14/49): loss=0.3859783807049589\n",
      "Gradient Descent(15/49): loss=0.38515383495933486\n",
      "Gradient Descent(16/49): loss=0.38437944053491635\n",
      "Gradient Descent(17/49): loss=0.3836462316168715\n",
      "Gradient Descent(18/49): loss=0.3829481457340933\n",
      "Gradient Descent(19/49): loss=0.3822809745193831\n",
      "Gradient Descent(20/49): loss=0.38164170035040856\n",
      "Gradient Descent(21/49): loss=0.38102807621568213\n",
      "Gradient Descent(22/49): loss=0.38043835905924167\n",
      "Gradient Descent(23/49): loss=0.37987114008692574\n",
      "Gradient Descent(24/49): loss=0.3793252364080586\n",
      "Gradient Descent(25/49): loss=0.37879962153307345\n",
      "Gradient Descent(26/49): loss=0.3782933805285037\n",
      "Gradient Descent(27/49): loss=0.37780568085148153\n",
      "Gradient Descent(28/49): loss=0.3773357531802132\n",
      "Gradient Descent(29/49): loss=0.3768828786375774\n",
      "Gradient Descent(30/49): loss=0.37644638012039167\n",
      "Gradient Descent(31/49): loss=0.3760256162793359\n",
      "Gradient Descent(32/49): loss=0.37561997722195667\n",
      "Gradient Descent(33/49): loss=0.37522888134577465\n",
      "Gradient Descent(34/49): loss=0.37485177292112254\n",
      "Gradient Descent(35/49): loss=0.37448812017866995\n",
      "Gradient Descent(36/49): loss=0.3741374137429163\n",
      "Gradient Descent(37/49): loss=0.3737991653081504\n",
      "Gradient Descent(38/49): loss=0.37347290648880727\n",
      "Gradient Descent(39/49): loss=0.3731581877989753\n",
      "Gradient Descent(40/49): loss=0.3728545777305863\n",
      "Gradient Descent(41/49): loss=0.3725616619094441\n",
      "Gradient Descent(42/49): loss=0.372279042314562\n",
      "Gradient Descent(43/49): loss=0.37200633655046095\n",
      "Gradient Descent(44/49): loss=0.37174317716487365\n",
      "Gradient Descent(45/49): loss=0.3714892110062019\n",
      "Gradient Descent(46/49): loss=0.3712440986163711\n",
      "Gradient Descent(47/49): loss=0.371007513655645\n",
      "Gradient Descent(48/49): loss=0.3707791423566108\n",
      "Gradient Descent(49/49): loss=0.3705586830050186\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46145765354086354\n",
      "Gradient Descent(2/49): loss=0.4365869399988366\n",
      "Gradient Descent(3/49): loss=0.4203920758457389\n",
      "Gradient Descent(4/49): loss=0.40971472717224344\n",
      "Gradient Descent(5/49): loss=0.4025557279824045\n",
      "Gradient Descent(6/49): loss=0.39764813412091593\n",
      "Gradient Descent(7/49): loss=0.39418803094505\n",
      "Gradient Descent(8/49): loss=0.39166458180128166\n",
      "Gradient Descent(9/49): loss=0.38975261729547045\n",
      "Gradient Descent(10/49): loss=0.3882446913195845\n",
      "Gradient Descent(11/49): loss=0.38700806981425423\n",
      "Gradient Descent(12/49): loss=0.3859574843161031\n",
      "Gradient Descent(13/49): loss=0.3850378606976007\n",
      "Gradient Descent(14/49): loss=0.3842133636735867\n",
      "Gradient Descent(15/49): loss=0.3834604423750453\n",
      "Gradient Descent(16/49): loss=0.38276341197699665\n",
      "Gradient Descent(17/49): loss=0.3821116436562607\n",
      "Gradient Descent(18/49): loss=0.3814977751119905\n",
      "Gradient Descent(19/49): loss=0.3809165690941813\n",
      "Gradient Descent(20/49): loss=0.3803641836896522\n",
      "Gradient Descent(21/49): loss=0.37983770447942933\n",
      "Gradient Descent(22/49): loss=0.379334843425299\n",
      "Gradient Descent(23/49): loss=0.3788537440577275\n",
      "Gradient Descent(24/49): loss=0.3783928545597752\n",
      "Gradient Descent(25/49): loss=0.3779508443186199\n",
      "Gradient Descent(26/49): loss=0.37752654839143035\n",
      "Gradient Descent(27/49): loss=0.3771189299709944\n",
      "Gradient Descent(28/49): loss=0.3767270545212191\n",
      "Gradient Descent(29/49): loss=0.3763500715332824\n",
      "Gradient Descent(30/49): loss=0.3759872013055263\n",
      "Gradient Descent(31/49): loss=0.37563772507602067\n",
      "Gradient Descent(32/49): loss=0.37530097742776036\n",
      "Gradient Descent(33/49): loss=0.3749763402644035\n",
      "Gradient Descent(34/49): loss=0.37466323789667483\n",
      "Gradient Descent(35/49): loss=0.37436113293522516\n",
      "Gradient Descent(36/49): loss=0.37406952278615496\n",
      "Gradient Descent(37/49): loss=0.37378793661048415\n",
      "Gradient Descent(38/49): loss=0.373515932651293\n",
      "Gradient Descent(39/49): loss=0.37325309586015887\n",
      "Gradient Descent(40/49): loss=0.3729990357730464\n",
      "Gradient Descent(41/49): loss=0.37275338459827956\n",
      "Gradient Descent(42/49): loss=0.3725157954877593\n",
      "Gradient Descent(43/49): loss=0.37228594096853496\n",
      "Gradient Descent(44/49): loss=0.3720635115160968\n",
      "Gradient Descent(45/49): loss=0.37184821425386116\n",
      "Gradient Descent(46/49): loss=0.3716397717656669\n",
      "Gradient Descent(47/49): loss=0.371437921009916\n",
      "Gradient Descent(48/49): loss=0.37124241232543326\n",
      "Gradient Descent(49/49): loss=0.37105300852029854\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45723347167662015\n",
      "Gradient Descent(2/49): loss=0.43165679754319175\n",
      "Gradient Descent(3/49): loss=0.4161271340591979\n",
      "Gradient Descent(4/49): loss=0.406496628173678\n",
      "Gradient Descent(5/49): loss=0.400347111361927\n",
      "Gradient Descent(6/49): loss=0.3962640576948828\n",
      "Gradient Descent(7/49): loss=0.3934175253070039\n",
      "Gradient Descent(8/49): loss=0.3913191436687106\n",
      "Gradient Descent(9/49): loss=0.38968065338157426\n",
      "Gradient Descent(10/49): loss=0.3883313038985096\n",
      "Gradient Descent(11/49): loss=0.387169494620935\n",
      "Gradient Descent(12/49): loss=0.38613440099268137\n",
      "Gradient Descent(13/49): loss=0.38518929441829597\n",
      "Gradient Descent(14/49): loss=0.38431171956194954\n",
      "Gradient Descent(15/49): loss=0.38348770045417\n",
      "Gradient Descent(16/49): loss=0.38270831749986634\n",
      "Gradient Descent(17/49): loss=0.38196768181922003\n",
      "Gradient Descent(18/49): loss=0.3812617342813023\n",
      "Gradient Descent(19/49): loss=0.3805875319217027\n",
      "Gradient Descent(20/49): loss=0.3799428227925047\n",
      "Gradient Descent(21/49): loss=0.3793257917533944\n",
      "Gradient Descent(22/49): loss=0.3787349077361323\n",
      "Gradient Descent(23/49): loss=0.37816883135951296\n",
      "Gradient Descent(24/49): loss=0.37762635852068016\n",
      "Gradient Descent(25/49): loss=0.3771063854960879\n",
      "Gradient Descent(26/49): loss=0.37660788695234987\n",
      "Gradient Descent(27/49): loss=0.37612990174544325\n",
      "Gradient Descent(28/49): loss=0.3756715234513358\n",
      "Gradient Descent(29/49): loss=0.37523189379829935\n",
      "Gradient Descent(30/49): loss=0.3748101979017882\n",
      "Gradient Descent(31/49): loss=0.3744056606385715\n",
      "Gradient Descent(32/49): loss=0.37401754375737556\n",
      "Gradient Descent(33/49): loss=0.37364514347954336\n",
      "Gradient Descent(34/49): loss=0.3732877884372626\n",
      "Gradient Descent(35/49): loss=0.37294483785378113\n",
      "Gradient Descent(36/49): loss=0.37261567990462235\n",
      "Gradient Descent(37/49): loss=0.3722997302200261\n",
      "Gradient Descent(38/49): loss=0.37199643050196246\n",
      "Gradient Descent(39/49): loss=0.3717052472373018\n",
      "Gradient Descent(40/49): loss=0.3714256704939504\n",
      "Gradient Descent(41/49): loss=0.3711572127901635\n",
      "Gradient Descent(42/49): loss=0.3708994080294915\n",
      "Gradient Descent(43/49): loss=0.37065181049534635\n",
      "Gradient Descent(44/49): loss=0.3704139939002464\n",
      "Gradient Descent(45/49): loss=0.3701855504855674\n",
      "Gradient Descent(46/49): loss=0.36996609016820114\n",
      "Gradient Descent(47/49): loss=0.3697552397309645\n",
      "Gradient Descent(48/49): loss=0.36955264205394633\n",
      "Gradient Descent(49/49): loss=0.3693579553842613\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.457191374679341\n",
      "Gradient Descent(2/49): loss=0.4315897324797413\n",
      "Gradient Descent(3/49): loss=0.4160439160628807\n",
      "Gradient Descent(4/49): loss=0.4064012394850032\n",
      "Gradient Descent(5/49): loss=0.4002410414653499\n",
      "Gradient Descent(6/49): loss=0.39614764677075065\n",
      "Gradient Descent(7/49): loss=0.3932906637781443\n",
      "Gradient Descent(8/49): loss=0.3911816062999798\n",
      "Gradient Descent(9/49): loss=0.38953224218006033\n",
      "Gradient Descent(10/49): loss=0.3881718976109477\n",
      "Gradient Descent(11/49): loss=0.3869990560402835\n",
      "Gradient Descent(12/49): loss=0.3859529674226283\n",
      "Gradient Descent(13/49): loss=0.3849969631239839\n",
      "Gradient Descent(14/49): loss=0.3841086332958759\n",
      "Gradient Descent(15/49): loss=0.38327403494000983\n",
      "Gradient Descent(16/49): loss=0.3824842713171563\n",
      "Gradient Descent(17/49): loss=0.3817334685419157\n",
      "Gradient Descent(18/49): loss=0.38101757652478035\n",
      "Gradient Descent(19/49): loss=0.3803336569239153\n",
      "Gradient Descent(20/49): loss=0.37967945919193424\n",
      "Gradient Descent(21/49): loss=0.3790531672827681\n",
      "Gradient Descent(22/49): loss=0.37845324760714955\n",
      "Gradient Descent(23/49): loss=0.3778783571630853\n",
      "Gradient Descent(24/49): loss=0.3773272875076975\n",
      "Gradient Descent(25/49): loss=0.37679893013598287\n",
      "Gradient Descent(26/49): loss=0.37629225469193905\n",
      "Gradient Descent(27/49): loss=0.3758062949099694\n",
      "Gradient Descent(28/49): loss=0.37534013924451676\n",
      "Gradient Descent(29/49): loss=0.37489292436950306\n",
      "Gradient Descent(30/49): loss=0.3744638304570042\n",
      "Gradient Descent(31/49): loss=0.37405207757827974\n",
      "Gradient Descent(32/49): loss=0.37365692282922514\n",
      "Gradient Descent(33/49): loss=0.37327765793733325\n",
      "Gradient Descent(34/49): loss=0.3729136072003631\n",
      "Gradient Descent(35/49): loss=0.3725641256630768\n",
      "Gradient Descent(36/49): loss=0.3722285974724739\n",
      "Gradient Descent(37/49): loss=0.37190643437276383\n",
      "Gradient Descent(38/49): loss=0.3715970743141533\n",
      "Gradient Descent(39/49): loss=0.37129998015753196\n",
      "Gradient Descent(40/49): loss=0.37101463846221705\n",
      "Gradient Descent(41/49): loss=0.3707405583471918\n",
      "Gradient Descent(42/49): loss=0.3704772704184372\n",
      "Gradient Descent(43/49): loss=0.37022432575642594\n",
      "Gradient Descent(44/49): loss=0.3699812949588738\n",
      "Gradient Descent(45/49): loss=0.36974776723458114\n",
      "Gradient Descent(46/49): loss=0.3695233495447593\n",
      "Gradient Descent(47/49): loss=0.36930766578865015\n",
      "Gradient Descent(48/49): loss=0.36910035603059976\n",
      "Gradient Descent(49/49): loss=0.3689010757660097\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4572833481897392\n",
      "Gradient Descent(2/49): loss=0.4317648466555245\n",
      "Gradient Descent(3/49): loss=0.41628646926954743\n",
      "Gradient Descent(4/49): loss=0.40669532346912596\n",
      "Gradient Descent(5/49): loss=0.40057323484205043\n",
      "Gradient Descent(6/49): loss=0.39650763591889937\n",
      "Gradient Descent(7/49): loss=0.3936709533603708\n",
      "Gradient Descent(8/49): loss=0.3915769355563944\n",
      "Gradient Descent(9/49): loss=0.3899389920192869\n",
      "Gradient Descent(10/49): loss=0.3885875960166811\n",
      "Gradient Descent(11/49): loss=0.387422004228419\n",
      "Gradient Descent(12/49): loss=0.3863819740685607\n",
      "Gradient Descent(13/49): loss=0.3854311634431806\n",
      "Gradient Descent(14/49): loss=0.38454736998484496\n",
      "Gradient Descent(15/49): loss=0.3837167820373014\n",
      "Gradient Descent(16/49): loss=0.382930586761238\n",
      "Gradient Descent(17/49): loss=0.38218296534041296\n",
      "Gradient Descent(18/49): loss=0.38146990566699523\n",
      "Gradient Descent(19/49): loss=0.3807884975047165\n",
      "Gradient Descent(20/49): loss=0.38013651283608746\n",
      "Gradient Descent(21/49): loss=0.3795121550464675\n",
      "Gradient Descent(22/49): loss=0.3789139082439374\n",
      "Gradient Descent(23/49): loss=0.378340446094495\n",
      "Gradient Descent(24/49): loss=0.37779057612152256\n",
      "Gradient Descent(25/49): loss=0.37726320520713424\n",
      "Gradient Descent(26/49): loss=0.3767573178228002\n",
      "Gradient Descent(27/49): loss=0.376271961945523\n",
      "Gradient Descent(28/49): loss=0.37580623964935295\n",
      "Gradient Descent(29/49): loss=0.3753593005699223\n",
      "Gradient Descent(30/49): loss=0.37493033715846924\n",
      "Gradient Descent(31/49): loss=0.37451858107053576\n",
      "Gradient Descent(32/49): loss=0.37412330029090257\n",
      "Gradient Descent(33/49): loss=0.3737437967501816\n",
      "Gradient Descent(34/49): loss=0.3733794042812105\n",
      "Gradient Descent(35/49): loss=0.3730294868195838\n",
      "Gradient Descent(36/49): loss=0.3726934367869331\n",
      "Gradient Descent(37/49): loss=0.3723706736166749\n",
      "Gradient Descent(38/49): loss=0.372060642395071\n",
      "Gradient Descent(39/49): loss=0.371762812598728\n",
      "Gradient Descent(40/49): loss=0.3714766769149701\n",
      "Gradient Descent(41/49): loss=0.371201750134991\n",
      "Gradient Descent(42/49): loss=0.3709375681120089\n",
      "Gradient Descent(43/49): loss=0.3706836867782455\n",
      "Gradient Descent(44/49): loss=0.37043968121566295\n",
      "Gradient Descent(45/49): loss=0.3702051447762141\n",
      "Gradient Descent(46/49): loss=0.36997968824796407\n",
      "Gradient Descent(47/49): loss=0.3697629390639088\n",
      "Gradient Descent(48/49): loss=0.3695545405506791\n",
      "Gradient Descent(49/49): loss=0.36935415121461956\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45723578683200544\n",
      "Gradient Descent(2/49): loss=0.431287546707888\n",
      "Gradient Descent(3/49): loss=0.41534263933461935\n",
      "Gradient Descent(4/49): loss=0.40536630147782404\n",
      "Gradient Descent(5/49): loss=0.3989650153771168\n",
      "Gradient Descent(6/49): loss=0.39471659436897794\n",
      "Gradient Descent(7/49): loss=0.3917745010530184\n",
      "Gradient Descent(8/49): loss=0.3896337601530617\n",
      "Gradient Descent(9/49): loss=0.38799229519379375\n",
      "Gradient Descent(10/49): loss=0.3866687041859094\n",
      "Gradient Descent(11/49): loss=0.38555345755124526\n",
      "Gradient Descent(12/49): loss=0.3845799075656347\n",
      "Gradient Descent(13/49): loss=0.38370705104020897\n",
      "Gradient Descent(14/49): loss=0.382909269780147\n",
      "Gradient Descent(15/49): loss=0.3821702166104993\n",
      "Gradient Descent(16/49): loss=0.38147916612761784\n",
      "Gradient Descent(17/49): loss=0.38082883202823703\n",
      "Gradient Descent(18/49): loss=0.3802140579215828\n",
      "Gradient Descent(19/49): loss=0.3796310289905546\n",
      "Gradient Descent(20/49): loss=0.37907679469418853\n",
      "Gradient Descent(21/49): loss=0.37854897758310063\n",
      "Gradient Descent(22/49): loss=0.3780455937704446\n",
      "Gradient Descent(23/49): loss=0.3775649406300925\n",
      "Gradient Descent(24/49): loss=0.37710552517290125\n",
      "Gradient Descent(25/49): loss=0.37666601720565884\n",
      "Gradient Descent(26/49): loss=0.3762452177317326\n",
      "Gradient Descent(27/49): loss=0.3758420368470708\n",
      "Gradient Descent(28/49): loss=0.37545547765459586\n",
      "Gradient Descent(29/49): loss=0.37508462407984267\n",
      "Gradient Descent(30/49): loss=0.3747286312875236\n",
      "Gradient Descent(31/49): loss=0.3743867178909887\n",
      "Gradient Descent(32/49): loss=0.37405815944454224\n",
      "Gradient Descent(33/49): loss=0.3737422828900489\n",
      "Gradient Descent(34/49): loss=0.3734384617406501\n",
      "Gradient Descent(35/49): loss=0.373146111853535\n",
      "Gradient Descent(36/49): loss=0.3728646876872212\n",
      "Gradient Descent(37/49): loss=0.37259367896670337\n",
      "Gradient Descent(38/49): loss=0.37233260769815296\n",
      "Gradient Descent(39/49): loss=0.3720810254872348\n",
      "Gradient Descent(40/49): loss=0.371838511123756\n",
      "Gradient Descent(41/49): loss=0.3716046684016194\n",
      "Gradient Descent(42/49): loss=0.37137912414775925\n",
      "Gradient Descent(43/49): loss=0.37116152643738765\n",
      "Gradient Descent(44/49): loss=0.370951542975809\n",
      "Gradient Descent(45/49): loss=0.3707488596294736\n",
      "Gradient Descent(46/49): loss=0.37055317909096336\n",
      "Gradient Descent(47/49): loss=0.3703642196643337\n",
      "Gradient Descent(48/49): loss=0.37018171415873685\n",
      "Gradient Descent(49/49): loss=0.3700054088795505\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4531640266735023\n",
      "Gradient Descent(2/49): loss=0.42692300995449284\n",
      "Gradient Descent(3/49): loss=0.41191479369435563\n",
      "Gradient Descent(4/49): loss=0.40307049763789526\n",
      "Gradient Descent(5/49): loss=0.3976317538674551\n",
      "Gradient Descent(6/49): loss=0.3940911051522525\n",
      "Gradient Descent(7/49): loss=0.3916215330252874\n",
      "Gradient Descent(8/49): loss=0.38976755717407785\n",
      "Gradient Descent(9/49): loss=0.38827721529953585\n",
      "Gradient Descent(10/49): loss=0.3870103411043571\n",
      "Gradient Descent(11/49): loss=0.38588834420369894\n",
      "Gradient Descent(12/49): loss=0.38486664372631024\n",
      "Gradient Descent(13/49): loss=0.3839195028093475\n",
      "Gradient Descent(14/49): loss=0.3830316678605299\n",
      "Gradient Descent(15/49): loss=0.3821937496281797\n",
      "Gradient Descent(16/49): loss=0.381399665693322\n",
      "Gradient Descent(17/49): loss=0.38064522064457607\n",
      "Gradient Descent(18/49): loss=0.37992731523454476\n",
      "Gradient Descent(19/49): loss=0.3792435039195743\n",
      "Gradient Descent(20/49): loss=0.37859174576468296\n",
      "Gradient Descent(21/49): loss=0.3779702629394855\n",
      "Gradient Descent(22/49): loss=0.37737745926693217\n",
      "Gradient Descent(23/49): loss=0.3768118724299343\n",
      "Gradient Descent(24/49): loss=0.37627214514909557\n",
      "Gradient Descent(25/49): loss=0.3757570071380201\n",
      "Gradient Descent(26/49): loss=0.37526526324987225\n",
      "Gradient Descent(27/49): loss=0.37479578523673246\n",
      "Gradient Descent(28/49): loss=0.3743475056636417\n",
      "Gradient Descent(29/49): loss=0.373919413146228\n",
      "Gradient Descent(30/49): loss=0.3735105484330842\n",
      "Gradient Descent(31/49): loss=0.37312000105298776\n",
      "Gradient Descent(32/49): loss=0.37274690636012503\n",
      "Gradient Descent(33/49): loss=0.3723904428753268\n",
      "Gradient Descent(34/49): loss=0.3720498298589292\n",
      "Gradient Descent(35/49): loss=0.37172432507302045\n",
      "Gradient Descent(36/49): loss=0.3714132227041216\n",
      "Gradient Descent(37/49): loss=0.37111585142551656\n",
      "Gradient Descent(38/49): loss=0.3708315725836108\n",
      "Gradient Descent(39/49): loss=0.37055977849607646\n",
      "Gradient Descent(40/49): loss=0.37029989085183374\n",
      "Gradient Descent(41/49): loss=0.3700513592045307\n",
      "Gradient Descent(42/49): loss=0.3698136595523725\n",
      "Gradient Descent(43/49): loss=0.3695862929980441\n",
      "Gradient Descent(44/49): loss=0.36936878448318405\n",
      "Gradient Descent(45/49): loss=0.36916068159242904\n",
      "Gradient Descent(46/49): loss=0.3689615534225281\n",
      "Gradient Descent(47/49): loss=0.36877098951241954\n",
      "Gradient Descent(48/49): loss=0.3685885988305073\n",
      "Gradient Descent(49/49): loss=0.36841400881567343\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45311794766252944\n",
      "Gradient Descent(2/49): loss=0.4268512959265571\n",
      "Gradient Descent(3/49): loss=0.41182693115533586\n",
      "Gradient Descent(4/49): loss=0.4029701486887495\n",
      "Gradient Descent(5/49): loss=0.39751992932773866\n",
      "Gradient Descent(6/49): loss=0.393967792661879\n",
      "Gradient Descent(7/49): loss=0.39148643776903774\n",
      "Gradient Descent(8/49): loss=0.38962039887578914\n",
      "Gradient Descent(9/49): loss=0.3881178234846179\n",
      "Gradient Descent(10/49): loss=0.3868386671809588\n",
      "Gradient Descent(11/49): loss=0.3857044441296978\n",
      "Gradient Descent(12/49): loss=0.38467065383685606\n",
      "Gradient Descent(13/49): loss=0.38371161728681324\n",
      "Gradient Descent(14/49): loss=0.38281212034625023\n",
      "Gradient Descent(15/49): loss=0.38196279913927816\n",
      "Gradient Descent(16/49): loss=0.38115758622910373\n",
      "Gradient Descent(17/49): loss=0.3803922937114406\n",
      "Gradient Descent(18/49): loss=0.37966382458628023\n",
      "Gradient Descent(19/49): loss=0.37896973193867395\n",
      "Gradient Descent(20/49): loss=0.3783079710390095\n",
      "Gradient Descent(21/49): loss=0.3776767586979039\n",
      "Gradient Descent(22/49): loss=0.3770744924242722\n",
      "Gradient Descent(23/49): loss=0.37649970305816566\n",
      "Gradient Descent(24/49): loss=0.3759510262416564\n",
      "Gradient Descent(25/49): loss=0.3754271845714544\n",
      "Gradient Descent(26/49): loss=0.37492697587436086\n",
      "Gradient Descent(27/49): loss=0.3744492650471857\n",
      "Gradient Descent(28/49): loss=0.37399297801762105\n",
      "Gradient Descent(29/49): loss=0.37355709700552336\n",
      "Gradient Descent(30/49): loss=0.3731406566133452\n",
      "Gradient Descent(31/49): loss=0.3727427404712034\n",
      "Gradient Descent(32/49): loss=0.37236247827355295\n",
      "Gradient Descent(33/49): loss=0.3719990431081282\n",
      "Gradient Descent(34/49): loss=0.3716516490145909\n",
      "Gradient Descent(35/49): loss=0.37131954873187817\n",
      "Gradient Descent(36/49): loss=0.3710020316061041\n",
      "Gradient Descent(37/49): loss=0.3706984216387425\n",
      "Gradient Descent(38/49): loss=0.37040807565976847\n",
      "Gradient Descent(39/49): loss=0.3701303816136759\n",
      "Gradient Descent(40/49): loss=0.36986475694847826\n",
      "Gradient Descent(41/49): loss=0.36961064709935126\n",
      "Gradient Descent(42/49): loss=0.36936752405972273\n",
      "Gradient Descent(43/49): loss=0.36913488503348396\n",
      "Gradient Descent(44/49): loss=0.36891225116269455\n",
      "Gradient Descent(45/49): loss=0.36869916632571437\n",
      "Gradient Descent(46/49): loss=0.3684951960011693\n",
      "Gradient Descent(47/49): loss=0.3682999261935586\n",
      "Gradient Descent(48/49): loss=0.3681129624166581\n",
      "Gradient Descent(49/49): loss=0.36793392873118014\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4532200772596591\n",
      "Gradient Descent(2/49): loss=0.4270438414883151\n",
      "Gradient Descent(3/49): loss=0.4120898106235744\n",
      "Gradient Descent(4/49): loss=0.4032842122293585\n",
      "Gradient Descent(5/49): loss=0.3978700645161344\n",
      "Gradient Descent(6/49): loss=0.3943431606448086\n",
      "Gradient Descent(7/49): loss=0.39187968387619637\n",
      "Gradient Descent(8/49): loss=0.39002668725465084\n",
      "Gradient Descent(9/49): loss=0.388534013723694\n",
      "Gradient Descent(10/49): loss=0.3872627063571679\n",
      "Gradient Descent(11/49): loss=0.3861349530542075\n",
      "Gradient Descent(12/49): loss=0.3851066605322129\n",
      "Gradient Descent(13/49): loss=0.3841523923742607\n",
      "Gradient Descent(14/49): loss=0.3832570789425834\n",
      "Gradient Descent(15/49): loss=0.3824114443004626\n",
      "Gradient Descent(16/49): loss=0.38160947740929224\n",
      "Gradient Descent(17/49): loss=0.38084702976814155\n",
      "Gradient Descent(18/49): loss=0.3801210349355864\n",
      "Gradient Descent(19/49): loss=0.37942907207789994\n",
      "Gradient Descent(20/49): loss=0.378769120272654\n",
      "Gradient Descent(21/49): loss=0.3781394188753583\n",
      "Gradient Descent(22/49): loss=0.37753838706527376\n",
      "Gradient Descent(23/49): loss=0.37696457656303606\n",
      "Gradient Descent(24/49): loss=0.37641664305816597\n",
      "Gradient Descent(25/49): loss=0.3758933282804782\n",
      "Gradient Descent(26/49): loss=0.3753934481994223\n",
      "Gradient Descent(27/49): loss=0.37491588481031646\n",
      "Gradient Descent(28/49): loss=0.37445958006817387\n",
      "Gradient Descent(29/49): loss=0.3740235311466089\n",
      "Gradient Descent(30/49): loss=0.373606786546181\n",
      "Gradient Descent(31/49): loss=0.37320844277274373\n",
      "Gradient Descent(32/49): loss=0.372827641418222\n",
      "Gradient Descent(33/49): loss=0.37246356654064283\n",
      "Gradient Descent(34/49): loss=0.3721154422778073\n",
      "Gradient Descent(35/49): loss=0.371782530651266\n",
      "Gradient Descent(36/49): loss=0.371464129530747\n",
      "Gradient Descent(37/49): loss=0.37115957073754874\n",
      "Gradient Descent(38/49): loss=0.37086821827076094\n",
      "Gradient Descent(39/49): loss=0.37058946664370895\n",
      "Gradient Descent(40/49): loss=0.3703227393204294\n",
      "Gradient Descent(41/49): loss=0.3700674872437008\n",
      "Gradient Descent(42/49): loss=0.3698231874474114\n",
      "Gradient Descent(43/49): loss=0.36958934174699515\n",
      "Gradient Descent(44/49): loss=0.3693654755024233\n",
      "Gradient Descent(45/49): loss=0.3691511364488244\n",
      "Gradient Descent(46/49): loss=0.36894589359030794\n",
      "Gradient Descent(47/49): loss=0.3687493361529708\n",
      "Gradient Descent(48/49): loss=0.36856107259341725\n",
      "Gradient Descent(49/49): loss=0.3683807296594259\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45314634825615474\n",
      "Gradient Descent(2/49): loss=0.42646035709444763\n",
      "Gradient Descent(3/49): loss=0.41099719528471784\n",
      "Gradient Descent(4/49): loss=0.40180476610785093\n",
      "Gradient Descent(5/49): loss=0.3961355442677361\n",
      "Gradient Descent(6/49): loss=0.39246185490795843\n",
      "Gradient Descent(7/49): loss=0.38993215650654856\n",
      "Gradient Descent(8/49): loss=0.38807024098368853\n",
      "Gradient Descent(9/49): loss=0.38660857324315007\n",
      "Gradient Descent(10/49): loss=0.3853958368230014\n",
      "Gradient Descent(11/49): loss=0.3843455865632414\n",
      "Gradient Descent(12/49): loss=0.38340769780334727\n",
      "Gradient Descent(13/49): loss=0.38255246984350477\n",
      "Gradient Descent(14/49): loss=0.38176176017922686\n",
      "Gradient Descent(15/49): loss=0.3810240288473126\n",
      "Gradient Descent(16/49): loss=0.38033155976505395\n",
      "Gradient Descent(17/49): loss=0.37967889578719605\n",
      "Gradient Descent(18/49): loss=0.37906195163352463\n",
      "Gradient Descent(19/49): loss=0.37847750631743077\n",
      "Gradient Descent(20/49): loss=0.37792290874139806\n",
      "Gradient Descent(21/49): loss=0.3773959035936618\n",
      "Gradient Descent(22/49): loss=0.3768945255970024\n",
      "Gradient Descent(23/49): loss=0.3764170329729484\n",
      "Gradient Descent(24/49): loss=0.3759618637201402\n",
      "Gradient Descent(25/49): loss=0.3755276054274726\n",
      "Gradient Descent(26/49): loss=0.3751129733340646\n",
      "Gradient Descent(27/49): loss=0.37471679359167437\n",
      "Gradient Descent(28/49): loss=0.3743379899513388\n",
      "Gradient Descent(29/49): loss=0.37397557281454946\n",
      "Gradient Descent(30/49): loss=0.37362863000021035\n",
      "Gradient Descent(31/49): loss=0.3732963188162\n",
      "Gradient Descent(32/49): loss=0.3729778591638075\n",
      "Gradient Descent(33/49): loss=0.3726725274868641\n",
      "Gradient Descent(34/49): loss=0.37237965142884855\n",
      "Gradient Descent(35/49): loss=0.37209860509404385\n",
      "Gradient Descent(36/49): loss=0.37182880483061614\n",
      "Gradient Descent(37/49): loss=0.3715697054686355\n",
      "Gradient Descent(38/49): loss=0.37132079695709924\n",
      "Gradient Descent(39/49): loss=0.3710816013524089\n",
      "Gradient Descent(40/49): loss=0.3708516701173808\n",
      "Gradient Descent(41/49): loss=0.3706305816952692\n",
      "Gradient Descent(42/49): loss=0.37041793932777234\n",
      "Gradient Descent(43/49): loss=0.37021336908980434\n",
      "Gradient Descent(44/49): loss=0.37001651811708025\n",
      "Gradient Descent(45/49): loss=0.36982705300539703\n",
      "Gradient Descent(46/49): loss=0.3696446583629482\n",
      "Gradient Descent(47/49): loss=0.3694690354991705\n",
      "Gradient Descent(48/49): loss=0.36929990123551104\n",
      "Gradient Descent(49/49): loss=0.36913698682516183\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4492310599658361\n",
      "Gradient Descent(2/49): loss=0.42263480194160535\n",
      "Gradient Descent(3/49): loss=0.40831274926683575\n",
      "Gradient Descent(4/49): loss=0.400272946630821\n",
      "Gradient Descent(5/49): loss=0.395478480591483\n",
      "Gradient Descent(6/49): loss=0.39238193477279004\n",
      "Gradient Descent(7/49): loss=0.39019096468558434\n",
      "Gradient Descent(8/49): loss=0.3884977260826962\n",
      "Gradient Descent(9/49): loss=0.3870909139759841\n",
      "Gradient Descent(10/49): loss=0.38586003209347075\n",
      "Gradient Descent(11/49): loss=0.38474647165091264\n",
      "Gradient Descent(12/49): loss=0.3837184383505357\n",
      "Gradient Descent(13/49): loss=0.38275806758984887\n",
      "Gradient Descent(14/49): loss=0.38185478533767714\n",
      "Gradient Descent(15/49): loss=0.38100187710036365\n",
      "Gradient Descent(16/49): loss=0.3801947084142401\n",
      "Gradient Descent(17/49): loss=0.37942979747102173\n",
      "Gradient Descent(18/49): loss=0.3787043285062771\n",
      "Gradient Descent(19/49): loss=0.37801589384204576\n",
      "Gradient Descent(20/49): loss=0.3773623549934855\n",
      "Gradient Descent(21/49): loss=0.3767417660893518\n",
      "Gradient Descent(22/49): loss=0.3761523301397039\n",
      "Gradient Descent(23/49): loss=0.3755923727990242\n",
      "Gradient Descent(24/49): loss=0.37506032559096675\n",
      "Gradient Descent(25/49): loss=0.3745547143650531\n",
      "Gradient Descent(26/49): loss=0.37407415073965494\n",
      "Gradient Descent(27/49): loss=0.3736173253248345\n",
      "Gradient Descent(28/49): loss=0.37318300206611293\n",
      "Gradient Descent(29/49): loss=0.3727700133409341\n",
      "Gradient Descent(30/49): loss=0.3723772555955872\n",
      "Gradient Descent(31/49): loss=0.3720036853952711\n",
      "Gradient Descent(32/49): loss=0.37164831580710755\n",
      "Gradient Descent(33/49): loss=0.37131021306272866\n",
      "Gradient Descent(34/49): loss=0.370988493462809\n",
      "Gradient Descent(35/49): loss=0.37068232049552907\n",
      "Gradient Descent(36/49): loss=0.3703909021470819\n",
      "Gradient Descent(37/49): loss=0.3701134883864333\n",
      "Gradient Descent(38/49): loss=0.3698493688094177\n",
      "Gradient Descent(39/49): loss=0.3695978704293538\n",
      "Gradient Descent(40/49): loss=0.3693583556029721\n",
      "Gradient Descent(41/49): loss=0.3691302200817139\n",
      "Gradient Descent(42/49): loss=0.36891289117949466\n",
      "Gradient Descent(43/49): loss=0.3687058260488785\n",
      "Gradient Descent(44/49): loss=0.3685085100583471\n",
      "Gradient Descent(45/49): loss=0.3683204552639652\n",
      "Gradient Descent(46/49): loss=0.36814119896930375\n",
      "Gradient Descent(47/49): loss=0.36797030236796213\n",
      "Gradient Descent(48/49): loss=0.3678073492634623\n",
      "Gradient Descent(49/49): loss=0.36765194486167985\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44918113802725884\n",
      "Gradient Descent(2/49): loss=0.42255885414084277\n",
      "Gradient Descent(3/49): loss=0.40822067780813803\n",
      "Gradient Descent(4/49): loss=0.40016790822086856\n",
      "Gradient Descent(5/49): loss=0.3953609727930183\n",
      "Gradient Descent(6/49): loss=0.39225164638346066\n",
      "Gradient Descent(7/49): loss=0.39004749444502335\n",
      "Gradient Descent(8/49): loss=0.3883408092786079\n",
      "Gradient Descent(9/49): loss=0.3869204593907177\n",
      "Gradient Descent(10/49): loss=0.3856760970502576\n",
      "Gradient Descent(11/49): loss=0.38454922412105585\n",
      "Gradient Descent(12/49): loss=0.38350812252916305\n",
      "Gradient Descent(13/49): loss=0.3825349769354454\n",
      "Gradient Descent(14/49): loss=0.38161924282994636\n",
      "Gradient Descent(15/49): loss=0.38075422133019304\n",
      "Gradient Descent(16/49): loss=0.3799352840523942\n",
      "Gradient Descent(17/49): loss=0.3791589489042028\n",
      "Gradient Descent(18/49): loss=0.3784223957131656\n",
      "Gradient Descent(19/49): loss=0.37772320982713437\n",
      "Gradient Descent(20/49): loss=0.3770592442877735\n",
      "Gradient Descent(21/49): loss=0.37642854396958936\n",
      "Gradient Descent(22/49): loss=0.3758293023218045\n",
      "Gradient Descent(23/49): loss=0.37525983543596364\n",
      "Gradient Descent(24/49): loss=0.37471856545865423\n",
      "Gradient Descent(25/49): loss=0.3742040091572103\n",
      "Gradient Descent(26/49): loss=0.3737147694190446\n",
      "Gradient Descent(27/49): loss=0.37324952849647486\n",
      "Gradient Descent(28/49): loss=0.3728070423507153\n",
      "Gradient Descent(29/49): loss=0.37238613573539714\n",
      "Gradient Descent(30/49): loss=0.37198569781316126\n",
      "Gradient Descent(31/49): loss=0.37160467818183446\n",
      "Gradient Descent(32/49): loss=0.3712420832324702\n",
      "Gradient Descent(33/49): loss=0.370896972787422\n",
      "Gradient Descent(34/49): loss=0.370568456981738\n",
      "Gradient Descent(35/49): loss=0.37025569336035946\n",
      "Gradient Descent(36/49): loss=0.3699578841694501\n",
      "Gradient Descent(37/49): loss=0.3696742738241073\n",
      "Gradient Descent(38/49): loss=0.3694041465374674\n",
      "Gradient Descent(39/49): loss=0.3691468240982525\n",
      "Gradient Descent(40/49): loss=0.36890166378538564\n",
      "Gradient Descent(41/49): loss=0.3686680564095492\n",
      "Gradient Descent(42/49): loss=0.3684454244725982\n",
      "Gradient Descent(43/49): loss=0.368233220436602\n",
      "Gradient Descent(44/49): loss=0.36803092509503016\n",
      "Gradient Descent(45/49): loss=0.3678380460392399\n",
      "Gradient Descent(46/49): loss=0.3676541162139915\n",
      "Gradient Descent(47/49): loss=0.3674786925562089\n",
      "Gradient Descent(48/49): loss=0.36731135471166076\n",
      "Gradient Descent(49/49): loss=0.3671517038246309\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44929341107267984\n",
      "Gradient Descent(2/49): loss=0.42276827748431284\n",
      "Gradient Descent(3/49): loss=0.40850233892624915\n",
      "Gradient Descent(4/49): loss=0.40049952512462117\n",
      "Gradient Descent(5/49): loss=0.3957261527855759\n",
      "Gradient Descent(6/49): loss=0.39263944658120886\n",
      "Gradient Descent(7/49): loss=0.39045099025265395\n",
      "Gradient Descent(8/49): loss=0.3887557256756835\n",
      "Gradient Descent(9/49): loss=0.38734415245642245\n",
      "Gradient Descent(10/49): loss=0.3861068807717604\n",
      "Gradient Descent(11/49): loss=0.38498595686163234\n",
      "Gradient Descent(12/49): loss=0.38394996626689915\n",
      "Gradient Descent(13/49): loss=0.38298126325975335\n",
      "Gradient Descent(14/49): loss=0.38206940146225765\n",
      "Gradient Descent(15/49): loss=0.3812077436389269\n",
      "Gradient Descent(16/49): loss=0.38039170529760313\n",
      "Gradient Descent(17/49): loss=0.37961784000493537\n",
      "Gradient Descent(18/49): loss=0.3788833595342339\n",
      "Gradient Descent(19/49): loss=0.3781858793875053\n",
      "Gradient Descent(20/49): loss=0.3775232816199903\n",
      "Gradient Descent(21/49): loss=0.3768936390641541\n",
      "Gradient Descent(22/49): loss=0.37629517194708795\n",
      "Gradient Descent(23/49): loss=0.3757262217928003\n",
      "Gradient Descent(24/49): loss=0.37518523469949183\n",
      "Gradient Descent(25/49): loss=0.3746707498219746\n",
      "Gradient Descent(26/49): loss=0.37418139084001084\n",
      "Gradient Descent(27/49): loss=0.37371585921583933\n",
      "Gradient Descent(28/49): loss=0.37327292858374933\n",
      "Gradient Descent(29/49): loss=0.37285143990190456\n",
      "Gradient Descent(30/49): loss=0.37245029715151695\n",
      "Gradient Descent(31/49): loss=0.37206846345333244\n",
      "Gradient Descent(32/49): loss=0.3717049575188864\n",
      "Gradient Descent(33/49): loss=0.3713588503812905\n",
      "Gradient Descent(34/49): loss=0.37102926236654554\n",
      "Gradient Descent(35/49): loss=0.37071536027639207\n",
      "Gradient Descent(36/49): loss=0.37041635476017637\n",
      "Gradient Descent(37/49): loss=0.37013149785755634\n",
      "Gradient Descent(38/49): loss=0.36986008069693493\n",
      "Gradient Descent(39/49): loss=0.36960143133675005\n",
      "Gradient Descent(40/49): loss=0.3693549127384546\n",
      "Gradient Descent(41/49): loss=0.36911992086135076\n",
      "Gradient Descent(42/49): loss=0.36889588287052294\n",
      "Gradient Descent(43/49): loss=0.36868225544999095\n",
      "Gradient Descent(44/49): loss=0.3684785232139553\n",
      "Gradient Descent(45/49): loss=0.36828419720963285\n",
      "Gradient Descent(46/49): loss=0.3680988135057363\n",
      "Gradient Descent(47/49): loss=0.3679219318611269\n",
      "Gradient Descent(48/49): loss=0.36775313446859625\n",
      "Gradient Descent(49/49): loss=0.3675920247691121\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44918933781331166\n",
      "Gradient Descent(2/49): loss=0.42207474273446705\n",
      "Gradient Descent(3/49): loss=0.40726797594211395\n",
      "Gradient Descent(4/49): loss=0.3988888532739576\n",
      "Gradient Descent(5/49): loss=0.39389278828330804\n",
      "Gradient Descent(6/49): loss=0.39069887252176366\n",
      "Gradient Descent(7/49): loss=0.3884833083897216\n",
      "Gradient Descent(8/49): loss=0.38681467954649323\n",
      "Gradient Descent(9/49): loss=0.38546534951251293\n",
      "Gradient Descent(10/49): loss=0.38431379067740085\n",
      "Gradient Descent(11/49): loss=0.3832939290621101\n",
      "Gradient Descent(12/49): loss=0.38236882901535807\n",
      "Gradient Descent(13/49): loss=0.3815169933289751\n",
      "Gradient Descent(14/49): loss=0.38072521000502735\n",
      "Gradient Descent(15/49): loss=0.3799848012762488\n",
      "Gradient Descent(16/49): loss=0.37928964403682525\n",
      "Gradient Descent(17/49): loss=0.3786351148980942\n",
      "Gradient Descent(18/49): loss=0.3780175196005222\n",
      "Gradient Descent(19/49): loss=0.3774337774827229\n",
      "Gradient Descent(20/49): loss=0.37688124131348383\n",
      "Gradient Descent(21/49): loss=0.37635758981050405\n",
      "Gradient Descent(22/49): loss=0.3758607598793841\n",
      "Gradient Descent(23/49): loss=0.37538890112012596\n",
      "Gradient Descent(24/49): loss=0.37494034327247044\n",
      "Gradient Descent(25/49): loss=0.3745135715426133\n",
      "Gradient Descent(26/49): loss=0.3741072070120398\n",
      "Gradient Descent(27/49): loss=0.37371999053283095\n",
      "Gradient Descent(28/49): loss=0.3733507691629046\n",
      "Gradient Descent(29/49): loss=0.3729984845507464\n",
      "Gradient Descent(30/49): loss=0.37266216287932835\n",
      "Gradient Descent(31/49): loss=0.3723409060952484\n",
      "Gradient Descent(32/49): loss=0.3720338842197337\n",
      "Gradient Descent(33/49): loss=0.37174032858334033\n",
      "Gradient Descent(34/49): loss=0.3714595258568092\n",
      "Gradient Descent(35/49): loss=0.37119081277251037\n",
      "Gradient Descent(36/49): loss=0.37093357144751704\n",
      "Gradient Descent(37/49): loss=0.37068722523240627\n",
      "Gradient Descent(38/49): loss=0.3704512350205028\n",
      "Gradient Descent(39/49): loss=0.3702250959610948\n",
      "Gradient Descent(40/49): loss=0.37000833452759224\n",
      "Gradient Descent(41/49): loss=0.3698005058979466\n",
      "Gradient Descent(42/49): loss=0.3696011916100985\n",
      "Gradient Descent(43/49): loss=0.36940999745993136\n",
      "Gradient Descent(44/49): loss=0.369226551613285\n",
      "Gradient Descent(45/49): loss=0.3690505029071298\n",
      "Gradient Descent(46/49): loss=0.3688815193180785\n",
      "Gradient Descent(47/49): loss=0.3687192865791052\n",
      "Gradient Descent(48/49): loss=0.36856350692767914\n",
      "Gradient Descent(49/49): loss=0.36841389797056867\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4454345715536217\n",
      "Gradient Descent(2/49): loss=0.41876113934743564\n",
      "Gradient Descent(3/49): loss=0.40523992466621206\n",
      "Gradient Descent(4/49): loss=0.3979845167897357\n",
      "Gradient Descent(5/49): loss=0.3937517747561384\n",
      "Gradient Descent(6/49): loss=0.39100429399286213\n",
      "Gradient Descent(7/49): loss=0.389008698578815\n",
      "Gradient Descent(8/49): loss=0.38741261388725445\n",
      "Gradient Descent(9/49): loss=0.3860450218870189\n",
      "Gradient Descent(10/49): loss=0.38482160327880766\n",
      "Gradient Descent(11/49): loss=0.38369968564460794\n",
      "Gradient Descent(12/49): loss=0.3826567282706421\n",
      "Gradient Descent(13/49): loss=0.3816800128157791\n",
      "Gradient Descent(14/49): loss=0.3807616850678713\n",
      "Gradient Descent(15/49): loss=0.37989635915522163\n",
      "Gradient Descent(16/49): loss=0.37907995235529546\n",
      "Gradient Descent(17/49): loss=0.37830911280480684\n",
      "Gradient Descent(18/49): loss=0.3775809340238127\n",
      "Gradient Descent(19/49): loss=0.3768928089441128\n",
      "Gradient Descent(20/49): loss=0.37624235232637976\n",
      "Gradient Descent(21/49): loss=0.37562735709948697\n",
      "Gradient Descent(22/49): loss=0.3750457678297778\n",
      "Gradient Descent(23/49): loss=0.3744956630786709\n",
      "Gradient Descent(24/49): loss=0.37397524256069753\n",
      "Gradient Descent(25/49): loss=0.3734828170429678\n",
      "Gradient Descent(26/49): loss=0.37301679992568637\n",
      "Gradient Descent(27/49): loss=0.3725756999401305\n",
      "Gradient Descent(28/49): loss=0.3721581146513827\n",
      "Gradient Descent(29/49): loss=0.3717627245824676\n",
      "Gradient Descent(30/49): loss=0.3713882878451534\n",
      "Gradient Descent(31/49): loss=0.371033635200432\n",
      "Gradient Descent(32/49): loss=0.3706976654934481\n",
      "Gradient Descent(33/49): loss=0.37037934142085827\n",
      "Gradient Descent(34/49): loss=0.3700776855971101\n",
      "Gradient Descent(35/49): loss=0.3697917768919196\n",
      "Gradient Descent(36/49): loss=0.3695207470153881\n",
      "Gradient Descent(37/49): loss=0.3692637773303249\n",
      "Gradient Descent(38/49): loss=0.3690200958737872\n",
      "Gradient Descent(39/49): loss=0.36878897457182197\n",
      "Gradient Descent(40/49): loss=0.36856972663302\n",
      "Gradient Descent(41/49): loss=0.36836170410787494\n",
      "Gradient Descent(42/49): loss=0.3681642956021173\n",
      "Gradient Descent(43/49): loss=0.3679769241332241\n",
      "Gradient Descent(44/49): loss=0.3677990451202112\n",
      "Gradient Descent(45/49): loss=0.3676301444976141\n",
      "Gradient Descent(46/49): loss=0.36746973694528867\n",
      "Gradient Descent(47/49): loss=0.3673173642263016\n",
      "Gradient Descent(48/49): loss=0.3671725936257741\n",
      "Gradient Descent(49/49): loss=0.36703501648406445\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44538094577352955\n",
      "Gradient Descent(2/49): loss=0.4186813360835746\n",
      "Gradient Descent(3/49): loss=0.4051439848873835\n",
      "Gradient Descent(4/49): loss=0.3978749442783713\n",
      "Gradient Descent(5/49): loss=0.3936285655804092\n",
      "Gradient Descent(6/49): loss=0.3908669156753634\n",
      "Gradient Descent(7/49): loss=0.38885672325906595\n",
      "Gradient Descent(8/49): loss=0.38724585158815217\n",
      "Gradient Descent(9/49): loss=0.3858634974247945\n",
      "Gradient Descent(10/49): loss=0.384625499491763\n",
      "Gradient Descent(11/49): loss=0.38348929073621413\n",
      "Gradient Descent(12/49): loss=0.38243239561689707\n",
      "Gradient Descent(13/49): loss=0.3814421326157696\n",
      "Gradient Descent(14/49): loss=0.3805106651882882\n",
      "Gradient Descent(15/49): loss=0.37963261258727576\n",
      "Gradient Descent(16/49): loss=0.37880388925361436\n",
      "Gradient Descent(17/49): loss=0.37802113562965284\n",
      "Gradient Descent(18/49): loss=0.37728143474705333\n",
      "Gradient Descent(19/49): loss=0.3765821676023\n",
      "Gradient Descent(20/49): loss=0.3759209364336865\n",
      "Gradient Descent(21/49): loss=0.37529552159326135\n",
      "Gradient Descent(22/49): loss=0.3747038553317538\n",
      "Gradient Descent(23/49): loss=0.37414400433017114\n",
      "Gradient Descent(24/49): loss=0.3736141569410951\n",
      "Gradient Descent(25/49): loss=0.37311261311481525\n",
      "Gradient Descent(26/49): loss=0.3726377759725439\n",
      "Gradient Descent(27/49): loss=0.37218814447793\n",
      "Gradient Descent(28/49): loss=0.3717623069036871\n",
      "Gradient Descent(29/49): loss=0.37135893491596994\n",
      "Gradient Descent(30/49): loss=0.37097677816539887\n",
      "Gradient Descent(31/49): loss=0.37061465930984905\n",
      "Gradient Descent(32/49): loss=0.3702714694148739\n",
      "Gradient Descent(33/49): loss=0.36994616369022093\n",
      "Gradient Descent(34/49): loss=0.3696377575290109\n",
      "Gradient Descent(35/49): loss=0.3693453228217116\n",
      "Gradient Descent(36/49): loss=0.36906798452107376\n",
      "Gradient Descent(37/49): loss=0.36880491743726196\n",
      "Gradient Descent(38/49): loss=0.36855534324484085\n",
      "Gradient Descent(39/49): loss=0.3683185276852524\n",
      "Gradient Descent(40/49): loss=0.3680937779500722\n",
      "Gradient Descent(41/49): loss=0.3678804402317399\n",
      "Gradient Descent(42/49): loss=0.3676778974296679\n",
      "Gradient Descent(43/49): loss=0.3674855670006955\n",
      "Gradient Descent(44/49): loss=0.3673028989437909\n",
      "Gradient Descent(45/49): loss=0.36712937390973316\n",
      "Gradient Descent(46/49): loss=0.3669645014272521\n",
      "Gradient Descent(47/49): loss=0.36680781823777286\n",
      "Gradient Descent(48/49): loss=0.36665888673151736\n",
      "Gradient Descent(49/49): loss=0.3665172934782605\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4455033496288015\n",
      "Gradient Descent(2/49): loss=0.41890705189338656\n",
      "Gradient Descent(3/49): loss=0.40544289254000104\n",
      "Gradient Descent(4/49): loss=0.3982218655756347\n",
      "Gradient Descent(5/49): loss=0.39400629201414\n",
      "Gradient Descent(6/49): loss=0.3912647823309427\n",
      "Gradient Descent(7/49): loss=0.38926843639234815\n",
      "Gradient Descent(8/49): loss=0.3876677420549533\n",
      "Gradient Descent(9/49): loss=0.38629336932572905\n",
      "Gradient Descent(10/49): loss=0.38506194679210615\n",
      "Gradient Descent(11/49): loss=0.3839313193450274\n",
      "Gradient Descent(12/49): loss=0.38287922599276436\n",
      "Gradient Descent(13/49): loss=0.38189310207676225\n",
      "Gradient Descent(14/49): loss=0.3809651822760633\n",
      "Gradient Descent(15/49): loss=0.38009013711014666\n",
      "Gradient Descent(16/49): loss=0.37926392414773746\n",
      "Gradient Descent(17/49): loss=0.37848322372100984\n",
      "Gradient Descent(18/49): loss=0.37774515717209384\n",
      "Gradient Descent(19/49): loss=0.37704714250377624\n",
      "Gradient Descent(20/49): loss=0.3763868174396229\n",
      "Gradient Descent(21/49): loss=0.37576199598360294\n",
      "Gradient Descent(22/49): loss=0.3751706419517353\n",
      "Gradient Descent(23/49): loss=0.37461085135230066\n",
      "Gradient Descent(24/49): loss=0.3740808395737126\n",
      "Gradient Descent(25/49): loss=0.3735789313352003\n",
      "Gradient Descent(26/49): loss=0.37310355234009807\n",
      "Gradient Descent(27/49): loss=0.3726532220633122\n",
      "Gradient Descent(28/49): loss=0.37222654735440414\n",
      "Gradient Descent(29/49): loss=0.37182221666767973\n",
      "Gradient Descent(30/49): loss=0.37143899480037773\n",
      "Gradient Descent(31/49): loss=0.371075718058922\n",
      "Gradient Descent(32/49): loss=0.37073128979590797\n",
      "Gradient Descent(33/49): loss=0.3704046762744595\n",
      "Gradient Descent(34/49): loss=0.37009490282566676\n",
      "Gradient Descent(35/49): loss=0.3698010502710065\n",
      "Gradient Descent(36/49): loss=0.36952225158609847\n",
      "Gradient Descent(37/49): loss=0.3692576887854658\n",
      "Gradient Descent(38/49): loss=0.36900659001054037\n",
      "Gradient Descent(39/49): loss=0.3687682268051994\n",
      "Gradient Descent(40/49): loss=0.368541911564791\n",
      "Gradient Descent(41/49): loss=0.3683269951460053\n",
      "Gradient Descent(42/49): loss=0.3681228646261261\n",
      "Gradient Descent(43/49): loss=0.36792894120122566\n",
      "Gradient Descent(44/49): loss=0.3677446782137485\n",
      "Gradient Descent(45/49): loss=0.3675695593007191\n",
      "Gradient Descent(46/49): loss=0.36740309665450477\n",
      "Gradient Descent(47/49): loss=0.36724482938868974\n",
      "Gradient Descent(48/49): loss=0.36709432200218206\n",
      "Gradient Descent(49/49): loss=0.3669511629351813\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4453647555034761\n",
      "Gradient Descent(2/49): loss=0.4181011151141937\n",
      "Gradient Descent(3/49): loss=0.4040758103866109\n",
      "Gradient Descent(4/49): loss=0.39649950029591013\n",
      "Gradient Descent(5/49): loss=0.39209922362378746\n",
      "Gradient Descent(6/49): loss=0.38929120536500295\n",
      "Gradient Descent(7/49): loss=0.38730519680413694\n",
      "Gradient Descent(8/49): loss=0.3857637148008319\n",
      "Gradient Descent(9/49): loss=0.38447945432223585\n",
      "Gradient Descent(10/49): loss=0.3833575686217624\n",
      "Gradient Descent(11/49): loss=0.38234843960409054\n",
      "Gradient Descent(12/49): loss=0.38142479327540263\n",
      "Gradient Descent(13/49): loss=0.3805705716149638\n",
      "Gradient Descent(14/49): loss=0.37977549193770843\n",
      "Gradient Descent(15/49): loss=0.3790323642550249\n",
      "Gradient Descent(16/49): loss=0.3783357505361274\n",
      "Gradient Descent(17/49): loss=0.37768128012629626\n",
      "Gradient Descent(18/49): loss=0.37706528847361787\n",
      "Gradient Descent(19/49): loss=0.3764846170774918\n",
      "Gradient Descent(20/49): loss=0.3759364953525601\n",
      "Gradient Descent(21/49): loss=0.3754184653275838\n",
      "Gradient Descent(22/49): loss=0.3749283297101102\n",
      "Gradient Descent(23/49): loss=0.3744641134533915\n",
      "Gradient Descent(24/49): loss=0.37402403369896087\n",
      "Gradient Descent(25/49): loss=0.3736064753281251\n",
      "Gradient Descent(26/49): loss=0.37320997054948984\n",
      "Gradient Descent(27/49): loss=0.3728331815677272\n",
      "Gradient Descent(28/49): loss=0.3724748857098097\n",
      "Gradient Descent(29/49): loss=0.3721339625707479\n",
      "Gradient Descent(30/49): loss=0.3718093828516659\n",
      "Gradient Descent(31/49): loss=0.37150019863387834\n",
      "Gradient Descent(32/49): loss=0.37120553488124525\n",
      "Gradient Descent(33/49): loss=0.37092458199864625\n",
      "Gradient Descent(34/49): loss=0.3706565893017747\n",
      "Gradient Descent(35/49): loss=0.37040085927533156\n",
      "Gradient Descent(36/49): loss=0.37015674251461644\n",
      "Gradient Descent(37/49): loss=0.36992363326046895\n",
      "Gradient Descent(38/49): loss=0.36970096545011527\n",
      "Gradient Descent(39/49): loss=0.3694882092171803\n",
      "Gradient Descent(40/49): loss=0.369284867783268\n",
      "Gradient Descent(41/49): loss=0.3690904746913342\n",
      "Gradient Descent(42/49): loss=0.3689045913377945\n",
      "Gradient Descent(43/49): loss=0.3687268047660847\n",
      "Gradient Descent(44/49): loss=0.36855672568936565\n",
      "Gradient Descent(45/49): loss=0.36839398671434387\n",
      "Gradient Descent(46/49): loss=0.368238240741882\n",
      "Gradient Descent(47/49): loss=0.3680891595232601\n",
      "Gradient Descent(48/49): loss=0.3679464323537055\n",
      "Gradient Descent(49/49): loss=0.36780976488719347\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4417745614368587\n",
      "Gradient Descent(2/49): loss=0.4152721342721159\n",
      "Gradient Descent(3/49): loss=0.40262378435588525\n",
      "Gradient Descent(4/49): loss=0.3961057713514115\n",
      "Gradient Descent(5/49): loss=0.3923465577834114\n",
      "Gradient Descent(6/49): loss=0.38986254509350676\n",
      "Gradient Descent(7/49): loss=0.3879956236522878\n",
      "Gradient Descent(8/49): loss=0.3864508481863518\n",
      "Gradient Descent(9/49): loss=0.38509390705319363\n",
      "Gradient Descent(10/49): loss=0.38386193943867913\n",
      "Gradient Descent(11/49): loss=0.3827240927265849\n",
      "Gradient Descent(12/49): loss=0.3816640062211773\n",
      "Gradient Descent(13/49): loss=0.38067199722304484\n",
      "Gradient Descent(14/49): loss=0.37974155535852283\n",
      "Gradient Descent(15/49): loss=0.37886775676486595\n",
      "Gradient Descent(16/49): loss=0.37804653731093696\n",
      "Gradient Descent(17/49): loss=0.3772743522291565\n",
      "Gradient Descent(18/49): loss=0.37654801087580136\n",
      "Gradient Descent(19/49): loss=0.3758645917752822\n",
      "Gradient Descent(20/49): loss=0.37522139513743064\n",
      "Gradient Descent(21/49): loss=0.37461591337166905\n",
      "Gradient Descent(22/49): loss=0.37404581063453257\n",
      "Gradient Descent(23/49): loss=0.37350890721275165\n",
      "Gradient Descent(24/49): loss=0.3730031667239197\n",
      "Gradient Descent(25/49): loss=0.3725266851267555\n",
      "Gradient Descent(26/49): loss=0.3720776810097839\n",
      "Gradient Descent(27/49): loss=0.37165448685847047\n",
      "Gradient Descent(28/49): loss=0.37125554111719405\n",
      "Gradient Descent(29/49): loss=0.370879380923911\n",
      "Gradient Descent(30/49): loss=0.3705246354298412\n",
      "Gradient Descent(31/49): loss=0.37019001963718984\n",
      "Gradient Descent(32/49): loss=0.3698743287011983\n",
      "Gradient Descent(33/49): loss=0.3695764326519359\n",
      "Gradient Descent(34/49): loss=0.3692952714978366\n",
      "Gradient Descent(35/49): loss=0.3690298506780066\n",
      "Gradient Descent(36/49): loss=0.3687792368342756\n",
      "Gradient Descent(37/49): loss=0.36854255387718227\n",
      "Gradient Descent(38/49): loss=0.3683189793227488\n",
      "Gradient Descent(39/49): loss=0.36810774087917625\n",
      "Gradient Descent(40/49): loss=0.3679081132645324\n",
      "Gradient Descent(41/49): loss=0.3677194152382166\n",
      "Gradient Descent(42/49): loss=0.36754100683048013\n",
      "Gradient Descent(43/49): loss=0.36737228675561373\n",
      "Gradient Descent(44/49): loss=0.36721268999560613\n",
      "Gradient Descent(45/49): loss=0.36706168554214663\n",
      "Gradient Descent(46/49): loss=0.3669187742858102\n",
      "Gradient Descent(47/49): loss=0.3667834870421414\n",
      "Gradient Descent(48/49): loss=0.36665538270514597\n",
      "Gradient Descent(49/49): loss=0.36653404651942384\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44171737090134144\n",
      "Gradient Descent(2/49): loss=0.41518881789263057\n",
      "Gradient Descent(3/49): loss=0.40252423326718484\n",
      "Gradient Descent(4/49): loss=0.3959917327272577\n",
      "Gradient Descent(5/49): loss=0.3922175766680902\n",
      "Gradient Descent(6/49): loss=0.38971795655209845\n",
      "Gradient Descent(7/49): loss=0.3878350453259671\n",
      "Gradient Descent(8/49): loss=0.38627420973658805\n",
      "Gradient Descent(9/49): loss=0.38490137255375745\n",
      "Gradient Descent(10/49): loss=0.3836538265143839\n",
      "Gradient Descent(11/49): loss=0.38250081105561207\n",
      "Gradient Descent(12/49): loss=0.3814260153978358\n",
      "Gradient Descent(13/49): loss=0.3804197793221979\n",
      "Gradient Descent(14/49): loss=0.37947559770304634\n",
      "Gradient Descent(15/49): loss=0.37858854149246346\n",
      "Gradient Descent(16/49): loss=0.3777545353621098\n",
      "Gradient Descent(17/49): loss=0.3769700201509701\n",
      "Gradient Descent(18/49): loss=0.3762317893785783\n",
      "Gradient Descent(19/49): loss=0.3755369053488168\n",
      "Gradient Descent(20/49): loss=0.3748826522808178\n",
      "Gradient Descent(21/49): loss=0.37426650715469584\n",
      "Gradient Descent(22/49): loss=0.3736861194158972\n",
      "Gradient Descent(23/49): loss=0.3731392954099078\n",
      "Gradient Descent(24/49): loss=0.37262398557383325\n",
      "Gradient Descent(25/49): loss=0.372138273404899\n",
      "Gradient Descent(26/49): loss=0.3716803656920232\n",
      "Gradient Descent(27/49): loss=0.3712485837209134\n",
      "Gradient Descent(28/49): loss=0.37084135527508\n",
      "Gradient Descent(29/49): loss=0.37045720731385173\n",
      "Gradient Descent(30/49): loss=0.3700947592412427\n",
      "Gradient Descent(31/49): loss=0.3697527166991646\n",
      "Gradient Descent(32/49): loss=0.3694298658311729\n",
      "Gradient Descent(33/49): loss=0.36912506797171035\n",
      "Gradient Descent(34/49): loss=0.3688372547222718\n",
      "Gradient Descent(35/49): loss=0.36856542338086246\n",
      "Gradient Descent(36/49): loss=0.36830863269509306\n",
      "Gradient Descent(37/49): loss=0.36806599891250297\n",
      "Gradient Descent(38/49): loss=0.3678366921044314\n",
      "Gradient Descent(39/49): loss=0.36761993274208676\n",
      "Gradient Descent(40/49): loss=0.3674149885054778\n",
      "Gradient Descent(41/49): loss=0.3672211713076325\n",
      "Gradient Descent(42/49): loss=0.3670378345180826\n",
      "Gradient Descent(43/49): loss=0.36686437037097547\n",
      "Gradient Descent(44/49): loss=0.3667002075444029\n",
      "Gradient Descent(45/49): loss=0.366544808898648\n",
      "Gradient Descent(46/49): loss=0.36639766936204454\n",
      "Gradient Descent(47/49): loss=0.36625831395405123\n",
      "Gradient Descent(48/49): loss=0.3661262959359534\n",
      "Gradient Descent(49/49): loss=0.3660011950803558\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44184989292802407\n",
      "Gradient Descent(2/49): loss=0.4154302120678773\n",
      "Gradient Descent(3/49): loss=0.40283887927227463\n",
      "Gradient Descent(4/49): loss=0.39635190542387266\n",
      "Gradient Descent(5/49): loss=0.39260573447853336\n",
      "Gradient Descent(6/49): loss=0.39012402785987416\n",
      "Gradient Descent(7/49): loss=0.38825347621003814\n",
      "Gradient Descent(8/49): loss=0.38670190882590244\n",
      "Gradient Descent(9/49): loss=0.38533650404881686\n",
      "Gradient Descent(10/49): loss=0.384095168019758\n",
      "Gradient Descent(11/49): loss=0.3829474361499711\n",
      "Gradient Descent(12/49): loss=0.3818771469479465\n",
      "Gradient Descent(13/49): loss=0.38087472649606946\n",
      "Gradient Descent(14/49): loss=0.3799337312389347\n",
      "Gradient Descent(15/49): loss=0.37904928495861473\n",
      "Gradient Descent(16/49): loss=0.37821736217833724\n",
      "Gradient Descent(17/49): loss=0.3774344520495493\n",
      "Gradient Descent(18/49): loss=0.37669739471987107\n",
      "Gradient Descent(19/49): loss=0.3760032968875472\n",
      "Gradient Descent(20/49): loss=0.37534948443116506\n",
      "Gradient Descent(21/49): loss=0.3747334729344117\n",
      "Gradient Descent(22/49): loss=0.3741529472537053\n",
      "Gradient Descent(23/49): loss=0.3736057459624271\n",
      "Gradient Descent(24/49): loss=0.3730898486537384\n",
      "Gradient Descent(25/49): loss=0.3726033650837505\n",
      "Gradient Descent(26/49): loss=0.3721445256121909\n",
      "Gradient Descent(27/49): loss=0.37171167263063154\n",
      "Gradient Descent(28/49): loss=0.3713032527871566\n",
      "Gradient Descent(29/49): loss=0.3709178098800988\n",
      "Gradient Descent(30/49): loss=0.37055397832979586\n",
      "Gradient Descent(31/49): loss=0.37021047715937416\n",
      "Gradient Descent(32/49): loss=0.36988610442982495\n",
      "Gradient Descent(33/49): loss=0.36957973208442557\n",
      "Gradient Descent(34/49): loss=0.36929030116459427\n",
      "Gradient Descent(35/49): loss=0.3690168173645662\n",
      "Gradient Descent(36/49): loss=0.3687583468963945\n",
      "Gradient Descent(37/49): loss=0.3685140126400829\n",
      "Gradient Descent(38/49): loss=0.36828299055636327\n",
      "Gradient Descent(39/49): loss=0.36806450634190063\n",
      "Gradient Descent(40/49): loss=0.3678578323086434\n",
      "Gradient Descent(41/49): loss=0.3676622844707053\n",
      "Gradient Descent(42/49): loss=0.3674772198236301\n",
      "Gradient Descent(43/49): loss=0.36730203380217585\n",
      "Gradient Descent(44/49): loss=0.36713615790391\n",
      "Gradient Descent(45/49): loss=0.3669790574669315\n",
      "Gradient Descent(46/49): loss=0.36683022959096445\n",
      "Gradient Descent(47/49): loss=0.36668920119190657\n",
      "Gradient Descent(48/49): loss=0.3665555271806761\n",
      "Gradient Descent(49/49): loss=0.3664287887578947\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44167260132664793\n",
      "Gradient Descent(2/49): loss=0.41451092563017\n",
      "Gradient Descent(3/49): loss=0.4013495390060603\n",
      "Gradient Descent(4/49): loss=0.39453694400934924\n",
      "Gradient Descent(5/49): loss=0.3906473386844524\n",
      "Gradient Descent(6/49): loss=0.3881392150109951\n",
      "Gradient Descent(7/49): loss=0.3863139154183665\n",
      "Gradient Descent(8/49): loss=0.3848509994611016\n",
      "Gradient Descent(9/49): loss=0.3836004604307238\n",
      "Gradient Descent(10/49): loss=0.3824895894184634\n",
      "Gradient Descent(11/49): loss=0.38148111033018295\n",
      "Gradient Descent(12/49): loss=0.38055428555493603\n",
      "Gradient Descent(13/49): loss=0.37969633497280386\n",
      "Gradient Descent(14/49): loss=0.37889849856739477\n",
      "Gradient Descent(15/49): loss=0.37815419848689424\n",
      "Gradient Descent(16/49): loss=0.377458156397092\n",
      "Gradient Descent(17/49): loss=0.3768059500172735\n",
      "Gradient Descent(18/49): loss=0.3761937750047801\n",
      "Gradient Descent(19/49): loss=0.3756183055319354\n",
      "Gradient Descent(20/49): loss=0.37507660439014423\n",
      "Gradient Descent(21/49): loss=0.37456605956959194\n",
      "Gradient Descent(22/49): loss=0.37408433620933856\n",
      "Gradient Descent(23/49): loss=0.37362933833827866\n",
      "Gradient Descent(24/49): loss=0.37319917742785785\n",
      "Gradient Descent(25/49): loss=0.37279214603477\n",
      "Gradient Descent(26/49): loss=0.37240669544535066\n",
      "Gradient Descent(27/49): loss=0.372041416571914\n",
      "Gradient Descent(28/49): loss=0.3716950235464908\n",
      "Gradient Descent(29/49): loss=0.37136633958012494\n",
      "Gradient Descent(30/49): loss=0.37105428473974317\n",
      "Gradient Descent(31/49): loss=0.3707578653560818\n",
      "Gradient Descent(32/49): loss=0.3704761648236183\n",
      "Gradient Descent(33/49): loss=0.37020833559140853\n",
      "Gradient Descent(34/49): loss=0.36995359217478485\n",
      "Gradient Descent(35/49): loss=0.3697112050436237\n",
      "Gradient Descent(36/49): loss=0.36948049526446414\n",
      "Gradient Descent(37/49): loss=0.3692608297919092\n",
      "Gradient Descent(38/49): loss=0.3690516173200874\n",
      "Gradient Descent(39/49): loss=0.3688523046179528\n",
      "Gradient Descent(40/49): loss=0.368662373283237\n",
      "Gradient Descent(41/49): loss=0.3684813368592443\n",
      "Gradient Descent(42/49): loss=0.3683087382666731\n",
      "Gradient Descent(43/49): loss=0.3681441475094364\n",
      "Gradient Descent(44/49): loss=0.3679871596192657\n",
      "Gradient Descent(45/49): loss=0.36783739280881794\n",
      "Gradient Descent(46/49): loss=0.3676944868072405\n",
      "Gradient Descent(47/49): loss=0.3675581013557493\n",
      "Gradient Descent(48/49): loss=0.3674279148438707\n",
      "Gradient Descent(49/49): loss=0.3673036230696298\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4382510296155473\n",
      "Gradient Descent(2/49): loss=0.41213904507300353\n",
      "Gradient Descent(3/49): loss=0.4003996983742317\n",
      "Gradient Descent(4/49): loss=0.39455447267867755\n",
      "Gradient Descent(5/49): loss=0.3911820545921173\n",
      "Gradient Descent(6/49): loss=0.3888882523566602\n",
      "Gradient Descent(7/49): loss=0.3870989250030709\n",
      "Gradient Descent(8/49): loss=0.3855740497231836\n",
      "Gradient Descent(9/49): loss=0.3842106381941318\n",
      "Gradient Descent(10/49): loss=0.382962339478245\n",
      "Gradient Descent(11/49): loss=0.3818064753975679\n",
      "Gradient Descent(12/49): loss=0.38073045256329263\n",
      "Gradient Descent(13/49): loss=0.3797261243983159\n",
      "Gradient Descent(14/49): loss=0.3787874290400136\n",
      "Gradient Descent(15/49): loss=0.3779093838106658\n",
      "Gradient Descent(16/49): loss=0.3770876451527509\n",
      "Gradient Descent(17/49): loss=0.37631830657583176\n",
      "Gradient Descent(18/49): loss=0.3755977983530907\n",
      "Gradient Descent(19/49): loss=0.3749228318427397\n",
      "Gradient Descent(20/49): loss=0.37429036421873424\n",
      "Gradient Descent(21/49): loss=0.37369757316761437\n",
      "Gradient Descent(22/49): loss=0.37314183692516606\n",
      "Gradient Descent(23/49): loss=0.37262071751735887\n",
      "Gradient Descent(24/49): loss=0.37213194615881867\n",
      "Gradient Descent(25/49): loss=0.3716734102528598\n",
      "Gradient Descent(26/49): loss=0.37124314166822203\n",
      "Gradient Descent(27/49): loss=0.37083930608309645\n",
      "Gradient Descent(28/49): loss=0.37046019324902973\n",
      "Gradient Descent(29/49): loss=0.3701042080634044\n",
      "Gradient Descent(30/49): loss=0.3697698623619735\n",
      "Gradient Descent(31/49): loss=0.36945576735838187\n",
      "Gradient Descent(32/49): loss=0.36916062666874644\n",
      "Gradient Descent(33/49): loss=0.36888322986779776\n",
      "Gradient Descent(34/49): loss=0.3686224465297066\n",
      "Gradient Descent(35/49): loss=0.3683772207121015\n",
      "Gradient Descent(36/49): loss=0.36814656584623673\n",
      "Gradient Descent(37/49): loss=0.3679295600000528\n",
      "Gradient Descent(38/49): loss=0.3677253414841176\n",
      "Gradient Descent(39/49): loss=0.3675331047732621\n",
      "Gradient Descent(40/49): loss=0.3673520967192124\n",
      "Gradient Descent(41/49): loss=0.36718161303171504\n",
      "Gradient Descent(42/49): loss=0.36702099500762203\n",
      "Gradient Descent(43/49): loss=0.3668696264891485\n",
      "Gradient Descent(44/49): loss=0.36672693103410686\n",
      "Gradient Descent(45/49): loss=0.3665923692823401\n",
      "Gradient Descent(46/49): loss=0.3664654365038713\n",
      "Gradient Descent(47/49): loss=0.36634566031545235\n",
      "Gradient Descent(48/49): loss=0.36623259855326457\n",
      "Gradient Descent(49/49): loss=0.36612583729048737\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4381904134106945\n",
      "Gradient Descent(2/49): loss=0.41205252288279104\n",
      "Gradient Descent(3/49): loss=0.4002967203627034\n",
      "Gradient Descent(4/49): loss=0.3944359724314889\n",
      "Gradient Descent(5/49): loss=0.39104720566559725\n",
      "Gradient Descent(6/49): loss=0.38873634691129355\n",
      "Gradient Descent(7/49): loss=0.3869296857473321\n",
      "Gradient Descent(8/49): loss=0.3853875569449384\n",
      "Gradient Descent(9/49): loss=0.38400720770636554\n",
      "Gradient Descent(10/49): loss=0.382742425909868\n",
      "Gradient Descent(11/49): loss=0.3815706070025165\n",
      "Gradient Descent(12/49): loss=0.38047919007767556\n",
      "Gradient Descent(13/49): loss=0.37946003613469276\n",
      "Gradient Descent(14/49): loss=0.37850707642747117\n",
      "Gradient Descent(15/49): loss=0.3776153134724175\n",
      "Gradient Descent(16/49): loss=0.376780384979828\n",
      "Gradient Descent(17/49): loss=0.37599836417773463\n",
      "Gradient Descent(18/49): loss=0.37526566087063057\n",
      "Gradient Descent(19/49): loss=0.3745789664978765\n",
      "Gradient Descent(20/49): loss=0.3739352192241436\n",
      "Gradient Descent(21/49): loss=0.37333157877454376\n",
      "Gradient Descent(22/49): loss=0.37276540648477424\n",
      "Gradient Descent(23/49): loss=0.3722342484896938\n",
      "Gradient Descent(24/49): loss=0.37173582103882813\n",
      "Gradient Descent(25/49): loss=0.3712679974032876\n",
      "Gradient Descent(26/49): loss=0.3708287960605985\n",
      "Gradient Descent(27/49): loss=0.3704163699538747\n",
      "Gradient Descent(28/49): loss=0.3700289966804889\n",
      "Gradient Descent(29/49): loss=0.3696650694996226\n",
      "Gradient Descent(30/49): loss=0.36932308906982375\n",
      "Gradient Descent(31/49): loss=0.36900165584262873\n",
      "Gradient Descent(32/49): loss=0.36869946304923285\n",
      "Gradient Descent(33/49): loss=0.36841529022557196\n",
      "Gradient Descent(34/49): loss=0.36814799722785957\n",
      "Gradient Descent(35/49): loss=0.36789651869609474\n",
      "Gradient Descent(36/49): loss=0.36765985892763164\n",
      "Gradient Descent(37/49): loss=0.36743708712680156\n",
      "Gradient Descent(38/49): loss=0.36722733299993643\n",
      "Gradient Descent(39/49): loss=0.3670297826680773\n",
      "Gradient Descent(40/49): loss=0.36684367487222086\n",
      "Gradient Descent(41/49): loss=0.36666829744824236\n",
      "Gradient Descent(42/49): loss=0.3665029840506619\n",
      "Gradient Descent(43/49): loss=0.36634711110623114\n",
      "Gradient Descent(44/49): loss=0.366200094979952\n",
      "Gradient Descent(45/49): loss=0.36606138933760257\n",
      "Gradient Descent(46/49): loss=0.36593048269016754\n",
      "Gradient Descent(47/49): loss=0.3658068961067707\n",
      "Gradient Descent(48/49): loss=0.36569018108379286\n",
      "Gradient Descent(49/49): loss=0.3655799175588418\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43833304097034764\n",
      "Gradient Descent(2/49): loss=0.41230895546259794\n",
      "Gradient Descent(3/49): loss=0.40062563884561825\n",
      "Gradient Descent(4/49): loss=0.39480754982089156\n",
      "Gradient Descent(5/49): loss=0.3914440358617002\n",
      "Gradient Descent(6/49): loss=0.3891491840631395\n",
      "Gradient Descent(7/49): loss=0.38735374118599714\n",
      "Gradient Descent(8/49): loss=0.3858202383060121\n",
      "Gradient Descent(9/49): loss=0.3844469356727037\n",
      "Gradient Descent(10/49): loss=0.3831880732258646\n",
      "Gradient Descent(11/49): loss=0.3820212538295315\n",
      "Gradient Descent(12/49): loss=0.3809340264570481\n",
      "Gradient Descent(13/49): loss=0.3799183273407109\n",
      "Gradient Descent(14/49): loss=0.37896815246092425\n",
      "Gradient Descent(15/49): loss=0.3780785662009603\n",
      "Gradient Descent(16/49): loss=0.377245266603313\n",
      "Gradient Descent(17/49): loss=0.37646438503231994\n",
      "Gradient Descent(18/49): loss=0.3757323862662747\n",
      "Gradient Descent(19/49): loss=0.3750460128295014\n",
      "Gradient Descent(20/49): loss=0.37440224969602043\n",
      "Gradient Descent(21/49): loss=0.373798299025813\n",
      "Gradient Descent(22/49): loss=0.3732315603192329\n",
      "Gradient Descent(23/49): loss=0.3726996138364256\n",
      "Gradient Descent(24/49): loss=0.37220020621240024\n",
      "Gradient Descent(25/49): loss=0.37173123769239935\n",
      "Gradient Descent(26/49): loss=0.3712907506484195\n",
      "Gradient Descent(27/49): loss=0.3708769191578361\n",
      "Gradient Descent(28/49): loss=0.3704880394907213\n",
      "Gradient Descent(29/49): loss=0.3701225213911709\n",
      "Gradient Descent(30/49): loss=0.36977888006252013\n",
      "Gradient Descent(31/49): loss=0.36945572878296196\n",
      "Gradient Descent(32/49): loss=0.3691517720899564\n",
      "Gradient Descent(33/49): loss=0.36886579948068915\n",
      "Gradient Descent(34/49): loss=0.3685966795827072\n",
      "Gradient Descent(35/49): loss=0.3683433547543403\n",
      "Gradient Descent(36/49): loss=0.3681048360790101\n",
      "Gradient Descent(37/49): loss=0.36788019872127625\n",
      "Gradient Descent(38/49): loss=0.36766857761566873\n",
      "Gradient Descent(39/49): loss=0.36746916346210934\n",
      "Gradient Descent(40/49): loss=0.36728119900412975\n",
      "Gradient Descent(41/49): loss=0.36710397556822394\n",
      "Gradient Descent(42/49): loss=0.3669368298445477\n",
      "Gradient Descent(43/49): loss=0.36677914089087027\n",
      "Gradient Descent(44/49): loss=0.3666303273431949\n",
      "Gradient Descent(45/49): loss=0.36648984481782165\n",
      "Gradient Descent(46/49): loss=0.3663571834908741\n",
      "Gradient Descent(47/49): loss=0.36623186584241596\n",
      "Gradient Descent(48/49): loss=0.36611344455331146\n",
      "Gradient Descent(49/49): loss=0.3660015005439051\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43811287528282744\n",
      "Gradient Descent(2/49): loss=0.41127666558923404\n",
      "Gradient Descent(3/49): loss=0.3990254488806676\n",
      "Gradient Descent(4/49): loss=0.39291811258249154\n",
      "Gradient Descent(5/49): loss=0.3894537875259732\n",
      "Gradient Descent(6/49): loss=0.38717086753955193\n",
      "Gradient Descent(7/49): loss=0.3854527219438344\n",
      "Gradient Descent(8/49): loss=0.38403433042204566\n",
      "Gradient Descent(9/49): loss=0.3827978985817451\n",
      "Gradient Descent(10/49): loss=0.3816879281890721\n",
      "Gradient Descent(11/49): loss=0.3806757758903043\n",
      "Gradient Descent(12/49): loss=0.3797447641498212\n",
      "Gradient Descent(13/49): loss=0.37888385464812047\n",
      "Gradient Descent(14/49): loss=0.37808490591121424\n",
      "Gradient Descent(15/49): loss=0.37734144274232206\n",
      "Gradient Descent(16/49): loss=0.37664807152673896\n",
      "Gradient Descent(17/49): loss=0.37600017758682197\n",
      "Gradient Descent(18/49): loss=0.37539375037283484\n",
      "Gradient Descent(19/49): loss=0.3748252701639535\n",
      "Gradient Descent(20/49): loss=0.37429162704833563\n",
      "Gradient Descent(21/49): loss=0.3737900587695347\n",
      "Gradient Descent(22/49): loss=0.3733181008854123\n",
      "Gradient Descent(23/49): loss=0.37287354574192166\n",
      "Gradient Descent(24/49): loss=0.3724544081867997\n",
      "Gradient Descent(25/49): loss=0.37205889665556846\n",
      "Gradient Descent(26/49): loss=0.37168538864673833\n",
      "Gradient Descent(27/49): loss=0.3713324098348127\n",
      "Gradient Descent(28/49): loss=0.37099861622397207\n",
      "Gradient Descent(29/49): loss=0.3706827788567117\n",
      "Gradient Descent(30/49): loss=0.37038377067684985\n",
      "Gradient Descent(31/49): loss=0.37010055521383134\n",
      "Gradient Descent(32/49): loss=0.36983217680997293\n",
      "Gradient Descent(33/49): loss=0.36957775215727023\n",
      "Gradient Descent(34/49): loss=0.3693364629476217\n",
      "Gradient Descent(35/49): loss=0.3691075494713415\n",
      "Gradient Descent(36/49): loss=0.3688903050247352\n",
      "Gradient Descent(37/49): loss=0.36868407100920303\n",
      "Gradient Descent(38/49): loss=0.3684882326225316\n",
      "Gradient Descent(39/49): loss=0.36830221505831406\n",
      "Gradient Descent(40/49): loss=0.368125480142291\n",
      "Gradient Descent(41/49): loss=0.3679575233452207\n",
      "Gradient Descent(42/49): loss=0.3677978711209943\n",
      "Gradient Descent(43/49): loss=0.3676460785264006\n",
      "Gradient Descent(44/49): loss=0.36750172708542006\n",
      "Gradient Descent(45/49): loss=0.367364422866407\n",
      "Gradient Descent(46/49): loss=0.3672337947451426\n",
      "Gradient Descent(47/49): loss=0.36710949283065325\n",
      "Gradient Descent(48/49): loss=0.3669911870340036\n",
      "Gradient Descent(49/49): loss=0.36687856576307865\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4348639760896874\n",
      "Gradient Descent(2/49): loss=0.40933427636468117\n",
      "Gradient Descent(3/49): loss=0.3985103326769797\n",
      "Gradient Descent(4/49): loss=0.39326307613252437\n",
      "Gradient Descent(5/49): loss=0.39019674209925337\n",
      "Gradient Descent(6/49): loss=0.38803298286913795\n",
      "Gradient Descent(7/49): loss=0.38628375386495173\n",
      "Gradient Descent(8/49): loss=0.38475844959986805\n",
      "Gradient Descent(9/49): loss=0.38337935110946963\n",
      "Gradient Descent(10/49): loss=0.38211206594950775\n",
      "Gradient Descent(11/49): loss=0.3809391467205958\n",
      "Gradient Descent(12/49): loss=0.3798500121983317\n",
      "Gradient Descent(13/49): loss=0.37883705574074056\n",
      "Gradient Descent(14/49): loss=0.37789411557767977\n",
      "Gradient Descent(15/49): loss=0.3770158536731368\n",
      "Gradient Descent(16/49): loss=0.3761974882318418\n",
      "Gradient Descent(17/49): loss=0.3754346665735823\n",
      "Gradient Descent(18/49): loss=0.3747233956283563\n",
      "Gradient Descent(19/49): loss=0.3740599974703819\n",
      "Gradient Descent(20/49): loss=0.37344107675125066\n",
      "Gradient Descent(21/49): loss=0.37286349452481543\n",
      "Gradient Descent(22/49): loss=0.3723243460130383\n",
      "Gradient Descent(23/49): loss=0.3718209411247498\n",
      "Gradient Descent(24/49): loss=0.3713507870857493\n",
      "Gradient Descent(25/49): loss=0.3709115727911998\n",
      "Gradient Descent(26/49): loss=0.3705011546181017\n",
      "Gradient Descent(27/49): loss=0.37011754350567055\n",
      "Gradient Descent(28/49): loss=0.36975889315391397\n",
      "Gradient Descent(29/49): loss=0.36942348921869855\n",
      "Gradient Descent(30/49): loss=0.36910973940135455\n",
      "Gradient Descent(31/49): loss=0.36881616434557374\n",
      "Gradient Descent(32/49): loss=0.3685413892657794\n",
      "Gradient Descent(33/49): loss=0.36828413624031303\n",
      "Gradient Descent(34/49): loss=0.36804321711033405\n",
      "Gradient Descent(35/49): loss=0.36781752693167913\n",
      "Gradient Descent(36/49): loss=0.36760603793235413\n",
      "Gradient Descent(37/49): loss=0.367407793933033\n",
      "Gradient Descent(38/49): loss=0.3672219051920459\n",
      "Gradient Descent(39/49): loss=0.36704754363996334\n",
      "Gradient Descent(40/49): loss=0.3668839384720965\n",
      "Gradient Descent(41/49): loss=0.3667303720700985\n",
      "Gradient Descent(42/49): loss=0.3665861762264141\n",
      "Gradient Descent(43/49): loss=0.3664507286476302\n",
      "Gradient Descent(44/49): loss=0.36632344971484904\n",
      "Gradient Descent(45/49): loss=0.36620379948107795\n",
      "Gradient Descent(46/49): loss=0.3660912748873209\n",
      "Gradient Descent(47/49): loss=0.3659854071805873\n",
      "Gradient Descent(48/49): loss=0.36588575951842345\n",
      "Gradient Descent(49/49): loss=0.3657919247458366\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4348000733015889\n",
      "Gradient Descent(2/49): loss=0.40924482154573866\n",
      "Gradient Descent(3/49): loss=0.3984040499620499\n",
      "Gradient Descent(4/49): loss=0.39314007515459026\n",
      "Gradient Descent(5/49): loss=0.39005592318881177\n",
      "Gradient Descent(6/49): loss=0.3878736778447799\n",
      "Gradient Descent(7/49): loss=0.38610583597071135\n",
      "Gradient Descent(8/49): loss=0.3845621688888018\n",
      "Gradient Descent(9/49): loss=0.383165179863266\n",
      "Gradient Descent(10/49): loss=0.3818805935919745\n",
      "Gradient Descent(11/49): loss=0.3806910149971696\n",
      "Gradient Descent(12/49): loss=0.3795858771495674\n",
      "Gradient Descent(13/49): loss=0.37855756639257937\n",
      "Gradient Descent(14/49): loss=0.37759990273144706\n",
      "Gradient Descent(15/49): loss=0.37670752463754337\n",
      "Gradient Descent(16/49): loss=0.3758756249431343\n",
      "Gradient Descent(17/49): loss=0.3750998255638664\n",
      "Gradient Descent(18/49): loss=0.3743761089319296\n",
      "Gradient Descent(19/49): loss=0.373700773943545\n",
      "Gradient Descent(20/49): loss=0.37307040351375603\n",
      "Gradient Descent(21/49): loss=0.37248183836923743\n",
      "Gradient Descent(22/49): loss=0.37193215470868124\n",
      "Gradient Descent(23/49): loss=0.37141864458815566\n",
      "Gradient Descent(24/49): loss=0.3709387984145304\n",
      "Gradient Descent(25/49): loss=0.3704902891703896\n",
      "Gradient Descent(26/49): loss=0.3700709581136918\n",
      "Gradient Descent(27/49): loss=0.36967880176158446\n",
      "Gradient Descent(28/49): loss=0.36931196000818833\n",
      "Gradient Descent(29/49): loss=0.36896870525317016\n",
      "Gradient Descent(30/49): loss=0.3686474324372899\n",
      "Gradient Descent(31/49): loss=0.3683466498957555\n",
      "Gradient Descent(32/49): loss=0.36806497095175567\n",
      "Gradient Descent(33/49): loss=0.36780110618189915\n",
      "Gradient Descent(34/49): loss=0.36755385629305165\n",
      "Gradient Descent(35/49): loss=0.36732210555663297\n",
      "Gradient Descent(36/49): loss=0.3671048157520553\n",
      "Gradient Descent(37/49): loss=0.3669010205758651\n",
      "Gradient Descent(38/49): loss=0.36670982047741274\n",
      "Gradient Descent(39/49): loss=0.3665303778856205\n",
      "Gradient Descent(40/49): loss=0.3663619127947517\n",
      "Gradient Descent(41/49): loss=0.36620369868003055\n",
      "Gradient Descent(42/49): loss=0.36605505871660377\n",
      "Gradient Descent(43/49): loss=0.3659153622776999\n",
      "Gradient Descent(44/49): loss=0.3657840216899575\n",
      "Gradient Descent(45/49): loss=0.3656604892258116\n",
      "Gradient Descent(46/49): loss=0.36554425431454357\n",
      "Gradient Descent(47/49): loss=0.36543484095515943\n",
      "Gradient Descent(48/49): loss=0.3653318053156706\n",
      "Gradient Descent(49/49): loss=0.36523473350462987\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4349527937557719\n",
      "Gradient Descent(2/49): loss=0.40951562963483334\n",
      "Gradient Descent(3/49): loss=0.39874583072366704\n",
      "Gradient Descent(4/49): loss=0.39352141991868567\n",
      "Gradient Descent(5/49): loss=0.3904599879580259\n",
      "Gradient Descent(6/49): loss=0.3882921864128638\n",
      "Gradient Descent(7/49): loss=0.38653472237509584\n",
      "Gradient Descent(8/49): loss=0.3849992327399342\n",
      "Gradient Descent(9/49): loss=0.38360899689544536\n",
      "Gradient Descent(10/49): loss=0.3823300603864192\n",
      "Gradient Descent(11/49): loss=0.38114517780091617\n",
      "Gradient Descent(12/49): loss=0.3800438758107683\n",
      "Gradient Descent(13/49): loss=0.37901861959835703\n",
      "Gradient Descent(14/49): loss=0.3780633051331852\n",
      "Gradient Descent(15/49): loss=0.37717264549042817\n",
      "Gradient Descent(16/49): loss=0.37634190540303086\n",
      "Gradient Descent(17/49): loss=0.3755667744258646\n",
      "Gradient Descent(18/49): loss=0.37484329732301563\n",
      "Gradient Descent(19/49): loss=0.37416782953609923\n",
      "Gradient Descent(20/49): loss=0.3735370046914838\n",
      "Gradient Descent(21/49): loss=0.37294770862413895\n",
      "Gradient Descent(22/49): loss=0.37239705742613866\n",
      "Gradient Descent(23/49): loss=0.37188237829308285\n",
      "Gradient Descent(24/49): loss=0.37140119249792747\n",
      "Gradient Descent(25/49): loss=0.3709512000838155\n",
      "Gradient Descent(26/49): loss=0.37053026600183453\n",
      "Gradient Descent(27/49): loss=0.37013640749494076\n",
      "Gradient Descent(28/49): loss=0.369767782575276\n",
      "Gradient Descent(29/49): loss=0.36942267947234575\n",
      "Gradient Descent(30/49): loss=0.3690995069506279\n",
      "Gradient Descent(31/49): loss=0.3687967854106741\n",
      "Gradient Descent(32/49): loss=0.36851313869958474\n",
      "Gradient Descent(33/49): loss=0.36824728656606603\n",
      "Gradient Descent(34/49): loss=0.3679980377028525\n",
      "Gradient Descent(35/49): loss=0.3677642833255575\n",
      "Gradient Descent(36/49): loss=0.36754499124233425\n",
      "Gradient Descent(37/49): loss=0.3673392003732922\n",
      "Gradient Descent(38/49): loss=0.3671460156825884\n",
      "Gradient Descent(39/49): loss=0.36696460348959214\n",
      "Gradient Descent(40/49): loss=0.3667941871286061\n",
      "Gradient Descent(41/49): loss=0.36663404292937185\n",
      "Gradient Descent(42/49): loss=0.3664834964930342\n",
      "Gradient Descent(43/49): loss=0.3663419192404439\n",
      "Gradient Descent(44/49): loss=0.3662087252116605\n",
      "Gradient Descent(45/49): loss=0.36608336809730074\n",
      "Gradient Descent(46/49): loss=0.3659653384840053\n",
      "Gradient Descent(47/49): loss=0.3658541612977532\n",
      "Gradient Descent(48/49): loss=0.36574939343009694\n",
      "Gradient Descent(49/49): loss=0.3656506215335926\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4346855773720145\n",
      "Gradient Descent(2/49): loss=0.40837186620851923\n",
      "Gradient Descent(3/49): loss=0.39704673007727853\n",
      "Gradient Descent(4/49): loss=0.39157429342127814\n",
      "Gradient Descent(5/49): loss=0.3884545309171262\n",
      "Gradient Descent(6/49): loss=0.3863346235974059\n",
      "Gradient Descent(7/49): loss=0.3846836320023424\n",
      "Gradient Descent(8/49): loss=0.3832870342713613\n",
      "Gradient Descent(9/49): loss=0.3820532872244587\n",
      "Gradient Descent(10/49): loss=0.3809394821046713\n",
      "Gradient Descent(11/49): loss=0.3799225822542726\n",
      "Gradient Descent(12/49): loss=0.3789881664880848\n",
      "Gradient Descent(13/49): loss=0.3781259296613989\n",
      "Gradient Descent(14/49): loss=0.377327813318033\n",
      "Gradient Descent(15/49): loss=0.3765871750493527\n",
      "Gradient Descent(16/49): loss=0.3758983789510231\n",
      "Gradient Descent(17/49): loss=0.37525656489603426\n",
      "Gradient Descent(18/49): loss=0.37465749980379504\n",
      "Gradient Descent(19/49): loss=0.3740974709383162\n",
      "Gradient Descent(20/49): loss=0.37357320379945175\n",
      "Gradient Descent(21/49): loss=0.37308179631211535\n",
      "Gradient Descent(22/49): loss=0.3726206648775471\n",
      "Gradient Descent(23/49): loss=0.3721874995890022\n",
      "Gradient Descent(24/49): loss=0.37178022677549444\n",
      "Gradient Descent(25/49): loss=0.37139697751798706\n",
      "Gradient Descent(26/49): loss=0.3710360610854289\n",
      "Gradient Descent(27/49): loss=0.37069594244913323\n",
      "Gradient Descent(28/49): loss=0.3703752231916571\n",
      "Gradient Descent(29/49): loss=0.37007262524924617\n",
      "Gradient Descent(30/49): loss=0.3697869770251657\n",
      "Gradient Descent(31/49): loss=0.36951720149091793\n",
      "Gradient Descent(32/49): loss=0.3692623059575212\n",
      "Gradient Descent(33/49): loss=0.3690213732526036\n",
      "Gradient Descent(34/49): loss=0.3687935540832684\n",
      "Gradient Descent(35/49): loss=0.3685780604012351\n",
      "Gradient Descent(36/49): loss=0.3683741596170412\n",
      "Gradient Descent(37/49): loss=0.36818116953519764\n",
      "Gradient Descent(38/49): loss=0.36799845390305486\n",
      "Gradient Descent(39/49): loss=0.3678254184834706\n",
      "Gradient Descent(40/49): loss=0.3676615075758032\n",
      "Gradient Descent(41/49): loss=0.3675062009217731\n",
      "Gradient Descent(42/49): loss=0.36735901094275103\n",
      "Gradient Descent(43/49): loss=0.367219480263397\n",
      "Gradient Descent(44/49): loss=0.36708717948355235\n",
      "Gradient Descent(45/49): loss=0.36696170516612714\n",
      "Gradient Descent(46/49): loss=0.36684267801360976\n",
      "Gradient Descent(47/49): loss=0.36672974120991725\n",
      "Gradient Descent(48/49): loss=0.3666225589077385\n",
      "Gradient Descent(49/49): loss=0.36652081484440335\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4316134008592792\n",
      "Gradient Descent(2/49): loss=0.4068313790189575\n",
      "Gradient Descent(3/49): loss=0.3969050648208042\n",
      "Gradient Descent(4/49): loss=0.39217651484382476\n",
      "Gradient Descent(5/49): loss=0.3893442185147288\n",
      "Gradient Descent(6/49): loss=0.38726285166918184\n",
      "Gradient Descent(7/49): loss=0.3855273734874112\n",
      "Gradient Descent(8/49): loss=0.3839894212474995\n",
      "Gradient Descent(9/49): loss=0.3825906238935812\n",
      "Gradient Descent(10/49): loss=0.38130471242837183\n",
      "Gradient Descent(11/49): loss=0.38011725374246746\n",
      "Gradient Descent(12/49): loss=0.3790184776990498\n",
      "Gradient Descent(13/49): loss=0.3780006910563077\n",
      "Gradient Descent(14/49): loss=0.377057312074572\n",
      "Gradient Descent(15/49): loss=0.37618248539221455\n",
      "Gradient Descent(16/49): loss=0.37537090945233376\n",
      "Gradient Descent(17/49): loss=0.37461774503952705\n",
      "Gradient Descent(18/49): loss=0.373918557070469\n",
      "Gradient Descent(19/49): loss=0.3732692716650397\n",
      "Gradient Descent(20/49): loss=0.37266614138988746\n",
      "Gradient Descent(21/49): loss=0.3721057156263263\n",
      "Gradient Descent(22/49): loss=0.37158481459926623\n",
      "Gradient Descent(23/49): loss=0.37110050626452096\n",
      "Gradient Descent(24/49): loss=0.37065008555315027\n",
      "Gradient Descent(25/49): loss=0.3702310556245117\n",
      "Gradient Descent(26/49): loss=0.36984111086651444\n",
      "Gradient Descent(27/49): loss=0.36947812143594366\n",
      "Gradient Descent(28/49): loss=0.36914011916864853\n",
      "Gradient Descent(29/49): loss=0.36882528471607723\n",
      "Gradient Descent(30/49): loss=0.3685319357848986\n",
      "Gradient Descent(31/49): loss=0.36825851637240714\n",
      "Gradient Descent(32/49): loss=0.3680035869033711\n",
      "Gradient Descent(33/49): loss=0.3677658151847591\n",
      "Gradient Descent(34/49): loss=0.36754396810389484\n",
      "Gradient Descent(35/49): loss=0.3673369040034264\n",
      "Gradient Descent(36/49): loss=0.36714356567329964\n",
      "Gradient Descent(37/49): loss=0.36696297390587224\n",
      "Gradient Descent(38/49): loss=0.3667942215655628\n",
      "Gradient Descent(39/49): loss=0.3666364681290739\n",
      "Gradient Descent(40/49): loss=0.3664889346563702\n",
      "Gradient Descent(41/49): loss=0.3663508991562833\n",
      "Gradient Descent(42/49): loss=0.36622169231392115\n",
      "Gradient Descent(43/49): loss=0.366100693550032\n",
      "Gradient Descent(44/49): loss=0.3659873273851327\n",
      "Gradient Descent(45/49): loss=0.36588106008361937\n",
      "Gradient Descent(46/49): loss=0.36578139655524666\n",
      "Gradient Descent(47/49): loss=0.3656878774933163\n",
      "Gradient Descent(48/49): loss=0.3656000767306943\n",
      "Gradient Descent(49/49): loss=0.3655175987963774\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4315463505740243\n",
      "Gradient Descent(2/49): loss=0.40673923155005753\n",
      "Gradient Descent(3/49): loss=0.3967955473570536\n",
      "Gradient Descent(4/49): loss=0.39204894664012074\n",
      "Gradient Descent(5/49): loss=0.3891973336972344\n",
      "Gradient Descent(6/49): loss=0.3870960924656866\n",
      "Gradient Descent(7/49): loss=0.38534079571812996\n",
      "Gradient Descent(8/49): loss=0.3837834546830075\n",
      "Gradient Descent(9/49): loss=0.3823658968191324\n",
      "Gradient Descent(10/49): loss=0.3810619443112963\n",
      "Gradient Descent(11/49): loss=0.3798571938147262\n",
      "Gradient Descent(12/49): loss=0.37874187151262306\n",
      "Gradient Descent(13/49): loss=0.3777082634104377\n",
      "Gradient Descent(14/49): loss=0.37674975924596543\n",
      "Gradient Descent(15/49): loss=0.3758604724726897\n",
      "Gradient Descent(16/49): loss=0.3750350703549743\n",
      "Gradient Descent(17/49): loss=0.37426868378763267\n",
      "Gradient Descent(18/49): loss=0.37355684960476865\n",
      "Gradient Descent(19/49): loss=0.3728954677648121\n",
      "Gradient Descent(20/49): loss=0.3722807665084451\n",
      "Gradient Descent(21/49): loss=0.37170927255493985\n",
      "Gradient Descent(22/49): loss=0.3711777849344362\n",
      "Gradient Descent(23/49): loss=0.37068335168427624\n",
      "Gradient Descent(24/49): loss=0.37022324892203445\n",
      "Gradient Descent(25/49): loss=0.3697949619518036\n",
      "Gradient Descent(26/49): loss=0.3693961681423945\n",
      "Gradient Descent(27/49): loss=0.3690247213682307\n",
      "Gradient Descent(28/49): loss=0.3686786378397482\n",
      "Gradient Descent(29/49): loss=0.3683560831766157\n",
      "Gradient Descent(30/49): loss=0.3680553605975245\n",
      "Gradient Descent(31/49): loss=0.36777490011658537\n",
      "Gradient Descent(32/49): loss=0.36751324864971496\n",
      "Gradient Descent(33/49): loss=0.3672690609455398\n",
      "Gradient Descent(34/49): loss=0.36704109126481144\n",
      "Gradient Descent(35/49): loss=0.3668281857404532\n",
      "Gradient Descent(36/49): loss=0.36662927535741935\n",
      "Gradient Descent(37/49): loss=0.36644336949771283\n",
      "Gradient Descent(38/49): loss=0.3662695500013309\n",
      "Gradient Descent(39/49): loss=0.36610696569870715\n",
      "Gradient Descent(40/49): loss=0.3659548273744651\n",
      "Gradient Descent(41/49): loss=0.3658124031260908\n",
      "Gradient Descent(42/49): loss=0.3656790140845084\n",
      "Gradient Descent(43/49): loss=0.3655540304665704\n",
      "Gradient Descent(44/49): loss=0.3654368679321843\n",
      "Gradient Descent(45/49): loss=0.3653269842212387\n",
      "Gradient Descent(46/49): loss=0.36522387604768486\n",
      "Gradient Descent(47/49): loss=0.3651270762301081\n",
      "Gradient Descent(48/49): loss=0.3650361510399143\n",
      "Gradient Descent(49/49): loss=0.3649506977498703\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4317091512842971\n",
      "Gradient Descent(2/49): loss=0.40702373224434063\n",
      "Gradient Descent(3/49): loss=0.39714884729302086\n",
      "Gradient Descent(4/49): loss=0.3924386279091102\n",
      "Gradient Descent(5/49): loss=0.38960747731688544\n",
      "Gradient Descent(6/49): loss=0.38751945027104556\n",
      "Gradient Descent(7/49): loss=0.3857739329806056\n",
      "Gradient Descent(8/49): loss=0.3842244468887278\n",
      "Gradient Descent(9/49): loss=0.3828133868849014\n",
      "Gradient Descent(10/49): loss=0.3815148016883056\n",
      "Gradient Descent(11/49): loss=0.3803144086697018\n",
      "Gradient Descent(12/49): loss=0.37920252937824195\n",
      "Gradient Descent(13/49): loss=0.3781715418345242\n",
      "Gradient Descent(14/49): loss=0.3772149270871563\n",
      "Gradient Descent(15/49): loss=0.37632688694295396\n",
      "Gradient Descent(16/49): loss=0.3755021715887731\n",
      "Gradient Descent(17/49): loss=0.3747359878495336\n",
      "Gradient Descent(18/49): loss=0.37402394084850127\n",
      "Gradient Descent(19/49): loss=0.37336199119192853\n",
      "Gradient Descent(20/49): loss=0.3727464205163848\n",
      "Gradient Descent(21/49): loss=0.3721738022752493\n",
      "Gradient Descent(22/49): loss=0.37164097623968245\n",
      "Gradient Descent(23/49): loss=0.37114502586942544\n",
      "Gradient Descent(24/49): loss=0.37068325802615243\n",
      "Gradient Descent(25/49): loss=0.37025318466651475\n",
      "Gradient Descent(26/49): loss=0.3698525062463134\n",
      "Gradient Descent(27/49): loss=0.3694790966262775\n",
      "Gradient Descent(28/49): loss=0.36913098930961324\n",
      "Gradient Descent(29/49): loss=0.3688063648697239\n",
      "Gradient Descent(30/49): loss=0.36850353944752995\n",
      "Gradient Descent(31/49): loss=0.36822095421409085\n",
      "Gradient Descent(32/49): loss=0.36795716570722325\n",
      "Gradient Descent(33/49): loss=0.36771083696146467\n",
      "Gradient Descent(34/49): loss=0.36748072935965265\n",
      "Gradient Descent(35/49): loss=0.3672656951419859\n",
      "Gradient Descent(36/49): loss=0.3670646705149938\n",
      "Gradient Descent(37/49): loss=0.3668766693085572\n",
      "Gradient Descent(38/49): loss=0.3667007771341551\n",
      "Gradient Descent(39/49): loss=0.3665361460019568\n",
      "Gradient Descent(40/49): loss=0.366381989358336\n",
      "Gradient Descent(41/49): loss=0.36623757750891556\n",
      "Gradient Descent(42/49): loss=0.36610223339540876\n",
      "Gradient Descent(43/49): loss=0.3659753286973665\n",
      "Gradient Descent(44/49): loss=0.36585628023249017\n",
      "Gradient Descent(45/49): loss=0.36574454663147626\n",
      "Gradient Descent(46/49): loss=0.3656396252654378\n",
      "Gradient Descent(47/49): loss=0.365541049405829\n",
      "Gradient Descent(48/49): loss=0.3654483855985051\n",
      "Gradient Descent(49/49): loss=0.36536123123509734\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43139070759420906\n",
      "Gradient Descent(2/49): loss=0.40577109861545446\n",
      "Gradient Descent(3/49): loss=0.3953629528536459\n",
      "Gradient Descent(4/49): loss=0.3904490566568352\n",
      "Gradient Descent(5/49): loss=0.3876008255283507\n",
      "Gradient Descent(6/49): loss=0.3855939944841832\n",
      "Gradient Descent(7/49): loss=0.38398144593421324\n",
      "Gradient Descent(8/49): loss=0.38259228321903965\n",
      "Gradient Descent(9/49): loss=0.38135524479994426\n",
      "Gradient Descent(10/49): loss=0.3802360695887423\n",
      "Gradient Descent(11/49): loss=0.37921504112169446\n",
      "Gradient Descent(12/49): loss=0.3782787711916766\n",
      "Gradient Descent(13/49): loss=0.3774170727291455\n",
      "Gradient Descent(14/49): loss=0.37662167948588776\n",
      "Gradient Descent(15/49): loss=0.37588565534004714\n",
      "Gradient Descent(16/49): loss=0.37520307522162955\n",
      "Gradient Descent(17/49): loss=0.3745688233518337\n",
      "Gradient Descent(18/49): loss=0.3739784495215575\n",
      "Gradient Descent(19/49): loss=0.3734280589811177\n",
      "Gradient Descent(20/49): loss=0.37291422468806307\n",
      "Gradient Descent(21/49): loss=0.37243391591659836\n",
      "Gradient Descent(22/49): loss=0.37198443952386273\n",
      "Gradient Descent(23/49): loss=0.37156339130215155\n",
      "Gradient Descent(24/49): loss=0.37116861549509966\n",
      "Gradient Descent(25/49): loss=0.3707981709791587\n",
      "Gradient Descent(26/49): loss=0.370450302915006\n",
      "Gradient Descent(27/49): loss=0.3701234189037178\n",
      "Gradient Descent(28/49): loss=0.36981606886297896\n",
      "Gradient Descent(29/49): loss=0.3695269279826056\n",
      "Gradient Descent(30/49): loss=0.36925478223472186\n",
      "Gradient Descent(31/49): loss=0.3689985160080559\n",
      "Gradient Descent(32/49): loss=0.36875710151242813\n",
      "Gradient Descent(33/49): loss=0.36852958966201593\n",
      "Gradient Descent(34/49): loss=0.3683151021971025\n",
      "Gradient Descent(35/49): loss=0.36811282484586744\n",
      "Gradient Descent(36/49): loss=0.36792200136210396\n",
      "Gradient Descent(37/49): loss=0.3677419283029168\n",
      "Gradient Descent(38/49): loss=0.36757195043360963\n",
      "Gradient Descent(39/49): loss=0.3674114566660149\n",
      "Gradient Descent(40/49): loss=0.3672598764522024\n",
      "Gradient Descent(41/49): loss=0.36711667656843006\n",
      "Gradient Descent(42/49): loss=0.36698135823486716\n",
      "Gradient Descent(43/49): loss=0.3668534545254378\n",
      "Gradient Descent(44/49): loss=0.36673252802941375\n",
      "Gradient Descent(45/49): loss=0.3666181687324301\n",
      "Gradient Descent(46/49): loss=0.3665099920896017\n",
      "Gradient Descent(47/49): loss=0.3664076372675733\n",
      "Gradient Descent(48/49): loss=0.36631076553580166\n",
      "Gradient Descent(49/49): loss=0.3662190587902466\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4284993039243225\n",
      "Gradient Descent(2/49): loss=0.404605050164866\n",
      "Gradient Descent(3/49): loss=0.3955394249881494\n",
      "Gradient Descent(4/49): loss=0.39125025106874944\n",
      "Gradient Descent(5/49): loss=0.38858985071761176\n",
      "Gradient Descent(6/49): loss=0.3865544300059746\n",
      "Gradient Descent(7/49): loss=0.38481509682097276\n",
      "Gradient Descent(8/49): loss=0.3832579675322551\n",
      "Gradient Descent(9/49): loss=0.38183872389530726\n",
      "Gradient Descent(10/49): loss=0.38053619748766376\n",
      "Gradient Descent(11/49): loss=0.3793373934107265\n",
      "Gradient Descent(12/49): loss=0.37823257163674573\n",
      "Gradient Descent(13/49): loss=0.3772135755261453\n",
      "Gradient Descent(14/49): loss=0.37627322341675234\n",
      "Gradient Descent(15/49): loss=0.37540505690691633\n",
      "Gradient Descent(16/49): loss=0.3746032144595925\n",
      "Gradient Descent(17/49): loss=0.3738623529034323\n",
      "Gradient Descent(18/49): loss=0.373177589930538\n",
      "Gradient Descent(19/49): loss=0.3725444576195978\n",
      "Gradient Descent(20/49): loss=0.3719588628885409\n",
      "Gradient Descent(21/49): loss=0.3714170529409545\n",
      "Gradient Descent(22/49): loss=0.3709155846369048\n",
      "Gradient Descent(23/49): loss=0.3704512971087758\n",
      "Gradient Descent(24/49): loss=0.3700212871424065\n",
      "Gradient Descent(25/49): loss=0.3696228869592347\n",
      "Gradient Descent(26/49): loss=0.3692536441088885\n",
      "Gradient Descent(27/49): loss=0.3689113032325558\n",
      "Gradient Descent(28/49): loss=0.36859378949470917\n",
      "Gradient Descent(29/49): loss=0.3682991935092996\n",
      "Gradient Descent(30/49): loss=0.36802575760920875\n",
      "Gradient Descent(31/49): loss=0.36777186332626083\n",
      "Gradient Descent(32/49): loss=0.3675360199645619\n",
      "Gradient Descent(33/49): loss=0.3673168541630575\n",
      "Gradient Descent(34/49): loss=0.36711310035449585\n",
      "Gradient Descent(35/49): loss=0.36692359203777347\n",
      "Gradient Descent(36/49): loss=0.366747253789226\n",
      "Gradient Descent(37/49): loss=0.36658309394595745\n",
      "Gradient Descent(38/49): loss=0.36643019790096926\n",
      "Gradient Descent(39/49): loss=0.366287721955763\n",
      "Gradient Descent(40/49): loss=0.3661548876813388\n",
      "Gradient Descent(41/49): loss=0.36603097674320845\n",
      "Gradient Descent(42/49): loss=0.3659153261502227\n",
      "Gradient Descent(43/49): loss=0.36580732389077053\n",
      "Gradient Descent(44/49): loss=0.36570640492326745\n",
      "Gradient Descent(45/49): loss=0.3656120474908788\n",
      "Gradient Descent(46/49): loss=0.3655237697331401\n",
      "Gradient Descent(47/49): loss=0.36544112656959327\n",
      "Gradient Descent(48/49): loss=0.3653637068327652\n",
      "Gradient Descent(49/49): loss=0.36529113062981855\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.428429245228001\n",
      "Gradient Descent(2/49): loss=0.4045104177412345\n",
      "Gradient Descent(3/49): loss=0.3954266997729007\n",
      "Gradient Descent(4/49): loss=0.39111803459712424\n",
      "Gradient Descent(5/49): loss=0.3884368179095163\n",
      "Gradient Descent(6/49): loss=0.38638019022966213\n",
      "Gradient Descent(7/49): loss=0.3846199090180902\n",
      "Gradient Descent(8/49): loss=0.38304244451558594\n",
      "Gradient Descent(9/49): loss=0.3816036462332957\n",
      "Gradient Descent(10/49): loss=0.38028240866621105\n",
      "Gradient Descent(11/49): loss=0.3790657440116741\n",
      "Gradient Descent(12/49): loss=0.37794389143047463\n",
      "Gradient Descent(13/49): loss=0.3769086609415001\n",
      "Gradient Descent(14/49): loss=0.3759528331795782\n",
      "Gradient Descent(15/49): loss=0.3750699117973946\n",
      "Gradient Descent(16/49): loss=0.3742539989703096\n",
      "Gradient Descent(17/49): loss=0.37349971762242906\n",
      "Gradient Descent(18/49): loss=0.3728021540551918\n",
      "Gradient Descent(19/49): loss=0.37215681132453643\n",
      "Gradient Descent(20/49): loss=0.371559569439006\n",
      "Gradient Descent(21/49): loss=0.371006650528458\n",
      "Gradient Descent(22/49): loss=0.37049458795435214\n",
      "Gradient Descent(23/49): loss=0.37002019869857317\n",
      "Gradient Descent(24/49): loss=0.36958055855514327\n",
      "Gradient Descent(25/49): loss=0.36917297975870444\n",
      "Gradient Descent(26/49): loss=0.36879499075489586\n",
      "Gradient Descent(27/49): loss=0.3684443178679087\n",
      "Gradient Descent(28/49): loss=0.3681188686578844\n",
      "Gradient Descent(29/49): loss=0.3678167167898717\n",
      "Gradient Descent(30/49): loss=0.36753608825937445\n",
      "Gradient Descent(31/49): loss=0.36727534883867885\n",
      "Gradient Descent(32/49): loss=0.36703299262420225\n",
      "Gradient Descent(33/49): loss=0.36680763157874424\n",
      "Gradient Descent(34/49): loss=0.3665979859742353\n",
      "Gradient Descent(35/49): loss=0.3664028756507406\n",
      "Gradient Descent(36/49): loss=0.3662212120163298\n",
      "Gradient Descent(37/49): loss=0.36605199072020655\n",
      "Gradient Descent(38/49): loss=0.3658942849383276\n",
      "Gradient Descent(39/49): loss=0.36574723921680624\n",
      "Gradient Descent(40/49): loss=0.36561006382375844\n",
      "Gradient Descent(41/49): loss=0.3654820295650275\n",
      "Gradient Descent(42/49): loss=0.3653624630234793\n",
      "Gradient Descent(43/49): loss=0.3652507421853616\n",
      "Gradient Descent(44/49): loss=0.36514629242062263\n",
      "Gradient Descent(45/49): loss=0.36504858278714025\n",
      "Gradient Descent(46/49): loss=0.36495712263154384\n",
      "Gradient Descent(47/49): loss=0.3648714584617881\n",
      "Gradient Descent(48/49): loss=0.3647911710688481\n",
      "Gradient Descent(49/49): loss=0.36471587287691554\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42860211355592315\n",
      "Gradient Descent(2/49): loss=0.4048079110533483\n",
      "Gradient Descent(3/49): loss=0.39579025240101673\n",
      "Gradient Descent(4/49): loss=0.3915148200760544\n",
      "Gradient Descent(5/49): loss=0.3888521262556812\n",
      "Gradient Descent(6/49): loss=0.38680778407426425\n",
      "Gradient Descent(7/49): loss=0.38505686385895876\n",
      "Gradient Descent(8/49): loss=0.3834870009990749\n",
      "Gradient Descent(9/49): loss=0.3820544457476045\n",
      "Gradient Descent(10/49): loss=0.3807382619219626\n",
      "Gradient Descent(11/49): loss=0.37952557721108354\n",
      "Gradient Descent(12/49): loss=0.37840674032825655\n",
      "Gradient Descent(13/49): loss=0.377373671673246\n",
      "Gradient Descent(14/49): loss=0.37641925977179125\n",
      "Gradient Descent(15/49): loss=0.375537109725802\n",
      "Gradient Descent(16/49): loss=0.37472141626004213\n",
      "Gradient Descent(17/49): loss=0.37396688495745783\n",
      "Gradient Descent(18/49): loss=0.37326867489388094\n",
      "Gradient Descent(19/49): loss=0.3726223525891045\n",
      "Gradient Descent(20/49): loss=0.3720238530502335\n",
      "Gradient Descent(21/49): loss=0.3714694458757589\n",
      "Gradient Descent(22/49): loss=0.3709557052897427\n",
      "Gradient Descent(23/49): loss=0.37047948339139936\n",
      "Gradient Descent(24/49): loss=0.3700378861221967\n",
      "Gradient Descent(25/49): loss=0.3696282515786359\n",
      "Gradient Descent(26/49): loss=0.369248130378846\n",
      "Gradient Descent(27/49): loss=0.36889526784545806\n",
      "Gradient Descent(28/49): loss=0.36856758780622245\n",
      "Gradient Descent(29/49): loss=0.3682631778431064\n",
      "Gradient Descent(30/49): loss=0.3679802758434359\n",
      "Gradient Descent(31/49): loss=0.3677172577249992\n",
      "Gradient Descent(32/49): loss=0.3674726262221713\n",
      "Gradient Descent(33/49): loss=0.36724500063285265\n",
      "Gradient Descent(34/49): loss=0.36703310743689965\n",
      "Gradient Descent(35/49): loss=0.3668357717061302\n",
      "Gradient Descent(36/49): loss=0.36665190923419677\n",
      "Gradient Descent(37/49): loss=0.36648051932183046\n",
      "Gradient Descent(38/49): loss=0.3663206781593207\n",
      "Gradient Descent(39/49): loss=0.3661715327537523\n",
      "Gradient Descent(40/49): loss=0.3660322953535389\n",
      "Gradient Descent(41/49): loss=0.3659022383272817\n",
      "Gradient Descent(42/49): loss=0.36578068945799375\n",
      "Gradient Descent(43/49): loss=0.36566702761732783\n",
      "Gradient Descent(44/49): loss=0.36556067878767623\n",
      "Gradient Descent(45/49): loss=0.3654611124029154\n",
      "Gradient Descent(46/49): loss=0.3653678379811917\n",
      "Gradient Descent(47/49): loss=0.3652804020254995\n",
      "Gradient Descent(48/49): loss=0.36519838516994524\n",
      "Gradient Descent(49/49): loss=0.3651213995515145\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42822826594941105\n",
      "Gradient Descent(2/49): loss=0.4034499738477638\n",
      "Gradient Descent(3/49): loss=0.39392956577267857\n",
      "Gradient Descent(4/49): loss=0.38949641452348094\n",
      "Gradient Descent(5/49): loss=0.3868559374104509\n",
      "Gradient Descent(6/49): loss=0.3849234158917554\n",
      "Gradient Descent(7/49): loss=0.38332955855944734\n",
      "Gradient Descent(8/49): loss=0.38193940166438295\n",
      "Gradient Descent(9/49): loss=0.3806965439545556\n",
      "Gradient Descent(10/49): loss=0.37957224448351257\n",
      "Gradient Descent(11/49): loss=0.3785484884658059\n",
      "Gradient Descent(12/49): loss=0.3776121445461091\n",
      "Gradient Descent(13/49): loss=0.3767527969969679\n",
      "Gradient Descent(14/49): loss=0.3759618308598147\n",
      "Gradient Descent(15/49): loss=0.3752319690489882\n",
      "Gradient Descent(16/49): loss=0.37455699023745537\n",
      "Gradient Descent(17/49): loss=0.37393153148886377\n",
      "Gradient Descent(18/49): loss=0.3733509386771916\n",
      "Gradient Descent(19/49): loss=0.3728111484125276\n",
      "Gradient Descent(20/49): loss=0.37230859293461793\n",
      "Gradient Descent(21/49): loss=0.3718401226828828\n",
      "Gradient Descent(22/49): loss=0.37140294284583236\n",
      "Gradient Descent(23/49): loss=0.37099456111935564\n",
      "Gradient Descent(24/49): loss=0.37061274452017157\n",
      "Gradient Descent(25/49): loss=0.3702554835486025\n",
      "Gradient Descent(26/49): loss=0.3699209623361616\n",
      "Gradient Descent(27/49): loss=0.36960753368027444\n",
      "Gradient Descent(28/49): loss=0.36931369807993436\n",
      "Gradient Descent(29/49): loss=0.3690380860549706\n",
      "Gradient Descent(30/49): loss=0.36877944316710803\n",
      "Gradient Descent(31/49): loss=0.3685366172700517\n",
      "Gradient Descent(32/49): loss=0.36830854760379655\n",
      "Gradient Descent(33/49): loss=0.3680942554194345\n",
      "Gradient Descent(34/49): loss=0.3678928358782589\n",
      "Gradient Descent(35/49): loss=0.3677034510155645\n",
      "Gradient Descent(36/49): loss=0.3675253235973518\n",
      "Gradient Descent(37/49): loss=0.36735773172885017\n",
      "Gradient Descent(38/49): loss=0.3672000040987453\n",
      "Gradient Descent(39/49): loss=0.36705151576331885\n",
      "Gradient Descent(40/49): loss=0.36691168439127714\n",
      "Gradient Descent(41/49): loss=0.3667799669035677\n",
      "Gradient Descent(42/49): loss=0.36665585645353255\n",
      "Gradient Descent(43/49): loss=0.3665388797017918\n",
      "Gradient Descent(44/49): loss=0.36642859434766734\n",
      "Gradient Descent(45/49): loss=0.36632458688504627\n",
      "Gradient Descent(46/49): loss=0.36622647055559876\n",
      "Gradient Descent(47/49): loss=0.3661338834763913\n",
      "Gradient Descent(48/49): loss=0.36604648692236147\n",
      "Gradient Descent(49/49): loss=0.3659639637469448\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42552168528481743\n",
      "Gradient Descent(2/49): loss=0.40263113318866595\n",
      "Gradient Descent(3/49): loss=0.39437456235327223\n",
      "Gradient Descent(4/49): loss=0.3904485709419528\n",
      "Gradient Descent(5/49): loss=0.38790807493798496\n",
      "Gradient Descent(6/49): loss=0.3858917092821415\n",
      "Gradient Descent(7/49): loss=0.3841375059190075\n",
      "Gradient Descent(8/49): loss=0.3825585006226158\n",
      "Gradient Descent(9/49): loss=0.38111999940668895\n",
      "Gradient Descent(10/49): loss=0.379803679417486\n",
      "Gradient Descent(11/49): loss=0.37859691672697177\n",
      "Gradient Descent(12/49): loss=0.3774895148977863\n",
      "Gradient Descent(13/49): loss=0.3764726350625088\n",
      "Gradient Descent(14/49): loss=0.37553839715355997\n",
      "Gradient Descent(15/49): loss=0.37467969574925314\n",
      "Gradient Descent(16/49): loss=0.37389009115727345\n",
      "Gradient Descent(17/49): loss=0.3731637313421317\n",
      "Gradient Descent(18/49): loss=0.37249528944034715\n",
      "Gradient Descent(19/49): loss=0.3718799109316828\n",
      "Gradient Descent(20/49): loss=0.3713131677406715\n",
      "Gradient Descent(21/49): loss=0.3707910177720434\n",
      "Gradient Descent(22/49): loss=0.37030976892627465\n",
      "Gradient Descent(23/49): loss=0.3698660469191144\n",
      "Gradient Descent(24/49): loss=0.369456766390571\n",
      "Gradient Descent(25/49): loss=0.36907910489283957\n",
      "Gradient Descent(26/49): loss=0.3687304794187957\n",
      "Gradient Descent(27/49): loss=0.3684085251857181\n",
      "Gradient Descent(28/49): loss=0.36811107642966595\n",
      "Gradient Descent(29/49): loss=0.36783614899842126\n",
      "Gradient Descent(30/49): loss=0.36758192455749433\n",
      "Gradient Descent(31/49): loss=0.3673467362459149\n",
      "Gradient Descent(32/49): loss=0.3671290556374063\n",
      "Gradient Descent(33/49): loss=0.3669274808787464\n",
      "Gradient Descent(34/49): loss=0.36674072589117196\n",
      "Gradient Descent(35/49): loss=0.3665676105329271\n",
      "Gradient Descent(36/49): loss=0.36640705163181014\n",
      "Gradient Descent(37/49): loss=0.36625805480602025\n",
      "Gradient Descent(38/49): loss=0.3661197069999657\n",
      "Gradient Descent(39/49): loss=0.3659911696690888\n",
      "Gradient Descent(40/49): loss=0.3658716725543379\n",
      "Gradient Descent(41/49): loss=0.36576050799275694\n",
      "Gradient Descent(42/49): loss=0.36565702571587755\n",
      "Gradient Descent(43/49): loss=0.36556062809225304\n",
      "Gradient Descent(44/49): loss=0.36547076577463855\n",
      "Gradient Descent(45/49): loss=0.3653869337160513\n",
      "Gradient Descent(46/49): loss=0.365308667522297\n",
      "Gradient Descent(47/49): loss=0.36523554011155107\n",
      "Gradient Descent(48/49): loss=0.3651671586542996\n",
      "Gradient Descent(49/49): loss=0.36510316176937424\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42544875726351894\n",
      "Gradient Descent(2/49): loss=0.40253419214165803\n",
      "Gradient Descent(3/49): loss=0.3942586220966417\n",
      "Gradient Descent(4/49): loss=0.3903116204365538\n",
      "Gradient Descent(5/49): loss=0.38774882982291703\n",
      "Gradient Descent(6/49): loss=0.3857099886453089\n",
      "Gradient Descent(7/49): loss=0.38393378328383915\n",
      "Gradient Descent(8/49): loss=0.38233357065832646\n",
      "Gradient Descent(9/49): loss=0.3808747893565629\n",
      "Gradient Descent(10/49): loss=0.37953915031578433\n",
      "Gradient Descent(11/49): loss=0.37831401471589304\n",
      "Gradient Descent(12/49): loss=0.37718914940458453\n",
      "Gradient Descent(13/49): loss=0.3761556708485173\n",
      "Gradient Descent(14/49): loss=0.3752056531633159\n",
      "Gradient Descent(15/49): loss=0.37433194701402994\n",
      "Gradient Descent(16/49): loss=0.37352807183735526\n",
      "Gradient Descent(17/49): loss=0.37278813797682775\n",
      "Gradient Descent(18/49): loss=0.3721067839914135\n",
      "Gradient Descent(19/49): loss=0.3714791234683196\n",
      "Gradient Descent(20/49): loss=0.37090069874127624\n",
      "Gradient Descent(21/49): loss=0.3703674400742168\n",
      "Gradient Descent(22/49): loss=0.3698756293770599\n",
      "Gradient Descent(23/49): loss=0.3694218677805793\n",
      "Gradient Descent(24/49): loss=0.3690030465512529\n",
      "Gradient Descent(25/49): loss=0.36861632092813984\n",
      "Gradient Descent(26/49): loss=0.3682590865355205\n",
      "Gradient Descent(27/49): loss=0.36792895807866616\n",
      "Gradient Descent(28/49): loss=0.3676237500718874\n",
      "Gradient Descent(29/49): loss=0.36734145938156376\n",
      "Gradient Descent(30/49): loss=0.3670802493944607\n",
      "Gradient Descent(31/49): loss=0.3668384356447466\n",
      "Gradient Descent(32/49): loss=0.366614472752728\n",
      "Gradient Descent(33/49): loss=0.36640694254513495\n",
      "Gradient Descent(34/49): loss=0.3662145432413245\n",
      "Gradient Descent(35/49): loss=0.36603607960240164\n",
      "Gradient Descent(36/49): loss=0.36587045395130074\n",
      "Gradient Descent(37/49): loss=0.3657166579815702\n",
      "Gradient Descent(38/49): loss=0.3655737652811223\n",
      "Gradient Descent(39/49): loss=0.3654409245047603\n",
      "Gradient Descent(40/49): loss=0.36531735313595376\n",
      "Gradient Descent(41/49): loss=0.3652023317842657\n",
      "Gradient Descent(42/49): loss=0.36509519897009457\n",
      "Gradient Descent(43/49): loss=0.3649953463530911\n",
      "Gradient Descent(44/49): loss=0.36490221436479964\n",
      "Gradient Descent(45/49): loss=0.3648152882098236\n",
      "Gradient Descent(46/49): loss=0.3647340942031727\n",
      "Gradient Descent(47/49): loss=0.36465819641445896\n",
      "Gradient Descent(48/49): loss=0.3645871935923253\n",
      "Gradient Descent(49/49): loss=0.36452071634491673\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4256316805706501\n",
      "Gradient Descent(2/49): loss=0.4028439639265576\n",
      "Gradient Descent(3/49): loss=0.3946312452493941\n",
      "Gradient Descent(4/49): loss=0.39071446463967896\n",
      "Gradient Descent(5/49): loss=0.38816859063910036\n",
      "Gradient Descent(6/49): loss=0.38614136093106055\n",
      "Gradient Descent(7/49): loss=0.38437421968604935\n",
      "Gradient Descent(8/49): loss=0.38278138103474185\n",
      "Gradient Descent(9/49): loss=0.3813285652127984\n",
      "Gradient Descent(10/49): loss=0.3799976281725679\n",
      "Gradient Descent(11/49): loss=0.3787760588818239\n",
      "Gradient Descent(12/49): loss=0.37765375529898276\n",
      "Gradient Descent(13/49): loss=0.3766219645468026\n",
      "Gradient Descent(14/49): loss=0.3756728846081433\n",
      "Gradient Descent(15/49): loss=0.3747994791517943\n",
      "Gradient Descent(16/49): loss=0.37399536806081546\n",
      "Gradient Descent(17/49): loss=0.3732547494612679\n",
      "Gradient Descent(18/49): loss=0.372572337795661\n",
      "Gradient Descent(19/49): loss=0.37194331179586604\n",
      "Gradient Descent(20/49): loss=0.3713632694786888\n",
      "Gradient Descent(21/49): loss=0.3708281885761205\n",
      "Gradient Descent(22/49): loss=0.3703343913966391\n",
      "Gradient Descent(23/49): loss=0.3698785134177513\n",
      "Gradient Descent(24/49): loss=0.3694574750869587\n",
      "Gradient Descent(25/49): loss=0.36906845642086217\n",
      "Gradient Descent(26/49): loss=0.3687088740686696\n",
      "Gradient Descent(27/49): loss=0.36837636056140916\n",
      "Gradient Descent(28/49): loss=0.36806874550956414\n",
      "Gradient Descent(29/49): loss=0.36778403854424135\n",
      "Gradient Descent(30/49): loss=0.36752041382311657\n",
      "Gradient Descent(31/49): loss=0.3672761959440129\n",
      "Gradient Descent(32/49): loss=0.36704984712717703\n",
      "Gradient Descent(33/49): loss=0.36683995554288323\n",
      "Gradient Descent(34/49): loss=0.36664522467444083\n",
      "Gradient Descent(35/49): loss=0.36646446361839546\n",
      "Gradient Descent(36/49): loss=0.36629657823397205\n",
      "Gradient Descent(37/49): loss=0.3661405630628475\n",
      "Gradient Descent(38/49): loss=0.3659954939483241\n",
      "Gradient Descent(39/49): loss=0.36586052129005475\n",
      "Gradient Descent(40/49): loss=0.3657348638767661\n",
      "Gradient Descent(41/49): loss=0.3656178032450296\n",
      "Gradient Descent(42/49): loss=0.36550867851713587\n",
      "Gradient Descent(43/49): loss=0.36540688167560426\n",
      "Gradient Descent(44/49): loss=0.36531185323586973\n",
      "Gradient Descent(45/49): loss=0.3652230782822816\n",
      "Gradient Descent(46/49): loss=0.3651400828357866\n",
      "Gradient Descent(47/49): loss=0.3650624305245698\n",
      "Gradient Descent(48/49): loss=0.36498971953155274\n",
      "Gradient Descent(49/49): loss=0.36492157979500256\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4251982524376208\n",
      "Gradient Descent(2/49): loss=0.40138514285346727\n",
      "Gradient Descent(3/49): loss=0.39270741472667037\n",
      "Gradient Descent(4/49): loss=0.3886791977498809\n",
      "Gradient Descent(5/49): loss=0.3861924702085147\n",
      "Gradient Descent(6/49): loss=0.38430515440665514\n",
      "Gradient Descent(7/49): loss=0.3827170600385405\n",
      "Gradient Descent(8/49): loss=0.381321505143509\n",
      "Gradient Descent(9/49): loss=0.38007236791970733\n",
      "Gradient Descent(10/49): loss=0.3789440931293431\n",
      "Gradient Descent(11/49): loss=0.37791928935308244\n",
      "Gradient Descent(12/49): loss=0.37698462291197343\n",
      "Gradient Descent(13/49): loss=0.3761292726467391\n",
      "Gradient Descent(14/49): loss=0.3753442193648316\n",
      "Gradient Descent(15/49): loss=0.3746218381900663\n",
      "Gradient Descent(16/49): loss=0.37395562215732175\n",
      "Gradient Descent(17/49): loss=0.37333997633826216\n",
      "Gradient Descent(18/49): loss=0.3727700574066684\n",
      "Gradient Descent(19/49): loss=0.37224164592847403\n",
      "Gradient Descent(20/49): loss=0.3717510435884885\n",
      "Gradient Descent(21/49): loss=0.3712949899339756\n",
      "Gradient Descent(22/49): loss=0.37087059459352373\n",
      "Gradient Descent(23/49): loss=0.3704752818534929\n",
      "Gradient Descent(24/49): loss=0.37010674514677605\n",
      "Gradient Descent(25/49): loss=0.3697629095193735\n",
      "Gradient Descent(26/49): loss=0.3694419005366285\n",
      "Gradient Descent(27/49): loss=0.369142018402053\n",
      "Gradient Descent(28/49): loss=0.3688617163073609\n",
      "Gradient Descent(29/49): loss=0.36859958222718864\n",
      "Gradient Descent(30/49): loss=0.36835432352692987\n",
      "Gradient Descent(31/49): loss=0.3681247538756061\n",
      "Gradient Descent(32/49): loss=0.36790978205426633\n",
      "Gradient Descent(33/49): loss=0.36770840232922\n",
      "Gradient Descent(34/49): loss=0.3675196861224982\n",
      "Gradient Descent(35/49): loss=0.36734277476251065\n",
      "Gradient Descent(36/49): loss=0.3671768731384519\n",
      "Gradient Descent(37/49): loss=0.3670212441146414\n",
      "Gradient Descent(38/49): loss=0.36687520358722786\n",
      "Gradient Descent(39/49): loss=0.36673811608685236\n",
      "Gradient Descent(40/49): loss=0.36660939084794747\n",
      "Gradient Descent(41/49): loss=0.3664884782791614\n",
      "Gradient Descent(42/49): loss=0.3663748667805882\n",
      "Gradient Descent(43/49): loss=0.36626807986256915\n",
      "Gradient Descent(44/49): loss=0.36616767352822666\n",
      "Gradient Descent(45/49): loss=0.36607323388791735\n",
      "Gradient Descent(46/49): loss=0.3659843749787276\n",
      "Gradient Descent(47/49): loss=0.3659007367661783\n",
      "Gradient Descent(48/49): loss=0.36582198330863963\n",
      "Gradient Descent(49/49): loss=0.36574780106770405\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42268054494076385\n",
      "Gradient Descent(2/49): loss=0.4008866177338421\n",
      "Gradient Descent(3/49): loss=0.3933767367895098\n",
      "Gradient Descent(4/49): loss=0.38974310057161216\n",
      "Gradient Descent(5/49): loss=0.3872802422319952\n",
      "Gradient Descent(6/49): loss=0.3852638746953224\n",
      "Gradient Descent(7/49): loss=0.38348857666662667\n",
      "Gradient Descent(8/49): loss=0.38188746343495866\n",
      "Gradient Descent(9/49): loss=0.380431958666572\n",
      "Gradient Descent(10/49): loss=0.37910498127878006\n",
      "Gradient Descent(11/49): loss=0.3778935854976697\n",
      "Gradient Descent(12/49): loss=0.3767868256505637\n",
      "Gradient Descent(13/49): loss=0.37577505662160243\n",
      "Gradient Descent(14/49): loss=0.3748496472041265\n",
      "Gradient Descent(15/49): loss=0.3740028237773374\n",
      "Gradient Descent(16/49): loss=0.37322756269713536\n",
      "Gradient Descent(17/49): loss=0.3725175057765015\n",
      "Gradient Descent(18/49): loss=0.3718668895894611\n",
      "Gradient Descent(19/49): loss=0.37127048453261474\n",
      "Gradient Descent(20/49): loss=0.370723541462166\n",
      "Gradient Descent(21/49): loss=0.3702217445301594\n",
      "Gradient Descent(22/49): loss=0.36976116925062263\n",
      "Gradient Descent(23/49): loss=0.36933824506192087\n",
      "Gradient Descent(24/49): loss=0.36894972180263447\n",
      "Gradient Descent(25/49): loss=0.3685926396226798\n",
      "Gradient Descent(26/49): loss=0.36826430192793286\n",
      "Gradient Descent(27/49): loss=0.367962251015385\n",
      "Gradient Descent(28/49): loss=0.3676842461026367\n",
      "Gradient Descent(29/49): loss=0.3674282434937879\n",
      "Gradient Descent(30/49): loss=0.3671923786557253\n",
      "Gradient Descent(31/49): loss=0.3669749500058604\n",
      "Gradient Descent(32/49): loss=0.366774404235555\n",
      "Gradient Descent(33/49): loss=0.366589323013488\n",
      "Gradient Descent(34/49): loss=0.3664184109306105\n",
      "Gradient Descent(35/49): loss=0.36626048456352456\n",
      "Gradient Descent(36/49): loss=0.36611446254642804\n",
      "Gradient Descent(37/49): loss=0.365979356553476\n",
      "Gradient Descent(38/49): loss=0.3658542631037197\n",
      "Gradient Descent(39/49): loss=0.3657383561099084\n",
      "Gradient Descent(40/49): loss=0.3656308801005158\n",
      "Gradient Descent(41/49): loss=0.3655311440515159\n",
      "Gradient Descent(42/49): loss=0.36543851577081254\n",
      "Gradient Descent(43/49): loss=0.365352416783897\n",
      "Gradient Descent(44/49): loss=0.3652723176743773\n",
      "Gradient Descent(45/49): loss=0.3651977338375422\n",
      "Gradient Descent(46/49): loss=0.3651282216091748\n",
      "Gradient Descent(47/49): loss=0.36506337473545564\n",
      "Gradient Descent(48/49): loss=0.36500282115304994\n",
      "Gradient Descent(49/49): loss=0.3649462200513956\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.422604886680578\n",
      "Gradient Descent(2/49): loss=0.400787513950619\n",
      "Gradient Descent(3/49): loss=0.3932575479045667\n",
      "Gradient Descent(4/49): loss=0.3896013327245386\n",
      "Gradient Descent(5/49): loss=0.3871147396140677\n",
      "Gradient Descent(6/49): loss=0.38507469584280624\n",
      "Gradient Descent(7/49): loss=0.38327641440980065\n",
      "Gradient Descent(8/49): loss=0.3816532901953397\n",
      "Gradient Descent(9/49): loss=0.38017684179467165\n",
      "Gradient Descent(10/49): loss=0.37882999297661357\n",
      "Gradient Descent(11/49): loss=0.37759976230026693\n",
      "Gradient Descent(12/49): loss=0.37647515289618416\n",
      "Gradient Descent(13/49): loss=0.3754464649040605\n",
      "Gradient Descent(14/49): loss=0.3745050141215593\n",
      "Gradient Descent(15/49): loss=0.37364297766415694\n",
      "Gradient Descent(16/49): loss=0.3728532867800776\n",
      "Gradient Descent(17/49): loss=0.3721295420832214\n",
      "Gradient Descent(18/49): loss=0.37146594237189884\n",
      "Gradient Descent(19/49): loss=0.3708572231685722\n",
      "Gradient Descent(20/49): loss=0.37029860288330624\n",
      "Gradient Descent(21/49): loss=0.3697857352529923\n",
      "Gradient Descent(22/49): loss=0.3693146670894399\n",
      "Gradient Descent(23/49): loss=0.3688818005942833\n",
      "Gradient Descent(24/49): loss=0.3684838596462219\n",
      "Gradient Descent(25/49): loss=0.3681178595704347\n",
      "Gradient Descent(26/49): loss=0.3677810799778515\n",
      "Gradient Descent(27/49): loss=0.36747104032245037\n",
      "Gradient Descent(28/49): loss=0.36718547787323835\n",
      "Gradient Descent(29/49): loss=0.36692232783735673\n",
      "Gradient Descent(30/49): loss=0.36667970540396866\n",
      "Gradient Descent(31/49): loss=0.36645588950669683\n",
      "Gradient Descent(32/49): loss=0.36624930812638895\n",
      "Gradient Descent(33/49): loss=0.36605852497666475\n",
      "Gradient Descent(34/49): loss=0.3658822274325984\n",
      "Gradient Descent(35/49): loss=0.36571921557846804\n",
      "Gradient Descent(36/49): loss=0.3655683922641057\n",
      "Gradient Descent(37/49): loss=0.36542875407130926\n",
      "Gradient Descent(38/49): loss=0.3652993831022575\n",
      "Gradient Descent(39/49): loss=0.3651794395111063\n",
      "Gradient Descent(40/49): loss=0.36506815470810405\n",
      "Gradient Descent(41/49): loss=0.36496482517279677\n",
      "Gradient Descent(42/49): loss=0.3648688068192952\n",
      "Gradient Descent(43/49): loss=0.3647795098622855\n",
      "Gradient Descent(44/49): loss=0.3646963941375337\n",
      "Gradient Descent(45/49): loss=0.36461896483517003\n",
      "Gradient Descent(46/49): loss=0.36454676860807944\n",
      "Gradient Descent(47/49): loss=0.3644793900213533\n",
      "Gradient Descent(48/49): loss=0.364416448311999\n",
      "Gradient Descent(49/49): loss=0.36435759443101995\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.422797852328478\n",
      "Gradient Descent(2/49): loss=0.40110883883114135\n",
      "Gradient Descent(3/49): loss=0.3936381496438567\n",
      "Gradient Descent(4/49): loss=0.3900093622876663\n",
      "Gradient Descent(5/49): loss=0.3875384051348084\n",
      "Gradient Descent(6/49): loss=0.3855095011149851\n",
      "Gradient Descent(7/49): loss=0.383720058234548\n",
      "Gradient Descent(8/49): loss=0.3821040752762061\n",
      "Gradient Descent(9/49): loss=0.380633280082311\n",
      "Gradient Descent(10/49): loss=0.3792907435367257\n",
      "Gradient Descent(11/49): loss=0.3780636357936578\n",
      "Gradient Descent(12/49): loss=0.3769411160450001\n",
      "Gradient Descent(13/49): loss=0.375913635076966\n",
      "Gradient Descent(14/49): loss=0.37497264690020554\n",
      "Gradient Descent(15/49): loss=0.3741104512772847\n",
      "Gradient Descent(16/49): loss=0.37332008603091693\n",
      "Gradient Descent(17/49): loss=0.3725952431848901\n",
      "Gradient Descent(18/49): loss=0.37193019931584204\n",
      "Gradient Descent(19/49): loss=0.3713197558059084\n",
      "Gradient Descent(20/49): loss=0.3707591866720789\n",
      "Gradient Descent(21/49): loss=0.37024419252221485\n",
      "Gradient Descent(22/49): loss=0.3697708596355977\n",
      "Gradient Descent(23/49): loss=0.3693356234244528\n",
      "Gradient Descent(24/49): loss=0.36893523569600406\n",
      "Gradient Descent(25/49): loss=0.3685667352448729\n",
      "Gradient Descent(26/49): loss=0.3682274213845459\n",
      "Gradient Descent(27/49): loss=0.36791483008590825\n",
      "Gradient Descent(28/49): loss=0.36762671243716183\n",
      "Gradient Descent(29/49): loss=0.3673610151768268\n",
      "Gradient Descent(30/49): loss=0.36711586308242466\n",
      "Gradient Descent(31/49): loss=0.36688954302345345\n",
      "Gradient Descent(32/49): loss=0.3666804895094718\n",
      "Gradient Descent(33/49): loss=0.36648727158323585\n",
      "Gradient Descent(34/49): loss=0.36630858092544816\n",
      "Gradient Descent(35/49): loss=0.36614322105218444\n",
      "Gradient Descent(36/49): loss=0.36599009749877826\n",
      "Gradient Descent(37/49): loss=0.36584820889514696\n",
      "Gradient Descent(38/49): loss=0.365716638847418\n",
      "Gradient Descent(39/49): loss=0.3655945485494606\n",
      "Gradient Descent(40/49): loss=0.3654811700556877\n",
      "Gradient Descent(41/49): loss=0.36537580015337817\n",
      "Gradient Descent(42/49): loss=0.3652777947789135\n",
      "Gradient Descent(43/49): loss=0.36518656392779003\n",
      "Gradient Descent(44/49): loss=0.36510156701316\n",
      "Gradient Descent(45/49): loss=0.36502230863202545\n",
      "Gradient Descent(46/49): loss=0.364948334702129\n",
      "Gradient Descent(47/49): loss=0.3648792289360959\n",
      "Gradient Descent(48/49): loss=0.36481460962254436\n",
      "Gradient Descent(49/49): loss=0.36475412668671325\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4223006670588379\n",
      "Gradient Descent(2/49): loss=0.3995542964908798\n",
      "Gradient Descent(3/49): loss=0.39166228287150984\n",
      "Gradient Descent(4/49): loss=0.387967630900333\n",
      "Gradient Descent(5/49): loss=0.3855902116913366\n",
      "Gradient Descent(6/49): loss=0.3837270156021964\n",
      "Gradient Descent(7/49): loss=0.3821367596401269\n",
      "Gradient Descent(8/49): loss=0.38073401693390807\n",
      "Gradient Descent(9/49): loss=0.3794792972947318\n",
      "Gradient Descent(10/49): loss=0.37834858312188024\n",
      "Gradient Descent(11/49): loss=0.3773244307548455\n",
      "Gradient Descent(12/49): loss=0.3763930545551376\n",
      "Gradient Descent(13/49): loss=0.37554315183625403\n",
      "Gradient Descent(14/49): loss=0.3747652888532349\n",
      "Gradient Descent(15/49): loss=0.37405150475297877\n",
      "Gradient Descent(16/49): loss=0.37339502401453717\n",
      "Gradient Descent(17/49): loss=0.3727900359916262\n",
      "Gradient Descent(18/49): loss=0.37223152178031615\n",
      "Gradient Descent(19/49): loss=0.37171511664211015\n",
      "Gradient Descent(20/49): loss=0.3712369999060073\n",
      "Gradient Descent(21/49): loss=0.37079380639043147\n",
      "Gradient Descent(22/49): loss=0.3703825547978174\n",
      "Gradient Descent(23/49): loss=0.37000058955671705\n",
      "Gradient Descent(24/49): loss=0.36964553335625955\n",
      "Gradient Descent(25/49): loss=0.3693152482092297\n",
      "Gradient Descent(26/49): loss=0.36900780333890604\n",
      "Gradient Descent(27/49): loss=0.36872144854292543\n",
      "Gradient Descent(28/49): loss=0.36845459196796176\n",
      "Gradient Descent(29/49): loss=0.3682057814493502\n",
      "Gradient Descent(30/49): loss=0.3679736887432026\n",
      "Gradient Descent(31/49): loss=0.3677570961152898\n",
      "Gradient Descent(32/49): loss=0.367554884858937\n",
      "Gradient Descent(33/49): loss=0.36736602539955976\n",
      "Gradient Descent(34/49): loss=0.3671895687110906\n",
      "Gradient Descent(35/49): loss=0.36702463882317127\n",
      "Gradient Descent(36/49): loss=0.366870426240591\n",
      "Gradient Descent(37/49): loss=0.36672618213033475\n",
      "Gradient Descent(38/49): loss=0.3665912131586263\n",
      "Gradient Descent(39/49): loss=0.36646487688191476\n",
      "Gradient Descent(40/49): loss=0.3663465776130188\n",
      "Gradient Descent(41/49): loss=0.36623576269748315\n",
      "Gradient Descent(42/49): loss=0.36613191914634036\n",
      "Gradient Descent(43/49): loss=0.36603457058044253\n",
      "Gradient Descent(44/49): loss=0.3659432744487982\n",
      "Gradient Descent(45/49): loss=0.3658576194892354\n",
      "Gradient Descent(46/49): loss=0.3657772234045263\n",
      "Gradient Descent(47/49): loss=0.36570173073102963\n",
      "Gradient Descent(48/49): loss=0.365630810880145\n",
      "Gradient Descent(49/49): loss=0.365564156335534\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41997588289216176\n",
      "Gradient Descent(2/49): loss=0.3993496397011045\n",
      "Gradient Descent(3/49): loss=0.3925168359177709\n",
      "Gradient Descent(4/49): loss=0.3891115225504002\n",
      "Gradient Descent(5/49): loss=0.3866929149194913\n",
      "Gradient Descent(6/49): loss=0.3846636934113485\n",
      "Gradient Descent(7/49): loss=0.3828644342457436\n",
      "Gradient Descent(8/49): loss=0.38124248974193437\n",
      "Gradient Descent(9/49): loss=0.3797727533903591\n",
      "Gradient Descent(10/49): loss=0.3784382924988991\n",
      "Gradient Descent(11/49): loss=0.3772254053764297\n",
      "Gradient Descent(12/49): loss=0.37612222514419935\n",
      "Gradient Descent(13/49): loss=0.37511823121100746\n",
      "Gradient Descent(14/49): loss=0.3742040140060924\n",
      "Gradient Descent(15/49): loss=0.3733711235724401\n",
      "Gradient Descent(16/49): loss=0.3726119542482714\n",
      "Gradient Descent(17/49): loss=0.3719196497277193\n",
      "Gradient Descent(18/49): loss=0.3712880220704587\n",
      "Gradient Descent(19/49): loss=0.37071148135164356\n",
      "Gradient Descent(20/49): loss=0.3701849739161727\n",
      "Gradient Descent(21/49): loss=0.36970392782658745\n",
      "Gradient Descent(22/49): loss=0.36926420444944924\n",
      "Gradient Descent(23/49): loss=0.36886205534973454\n",
      "Gradient Descent(24/49): loss=0.3684940838164065\n",
      "Gradient Descent(25/49): loss=0.3681572104540382\n",
      "Gradient Descent(26/49): loss=0.3678486423606485\n",
      "Gradient Descent(27/49): loss=0.36756584547951215\n",
      "Gradient Descent(28/49): loss=0.3673065197678046\n",
      "Gradient Descent(29/49): loss=0.36706857687078454\n",
      "Gradient Descent(30/49): loss=0.36685012002890477\n",
      "Gradient Descent(31/49): loss=0.36664942597826045\n",
      "Gradient Descent(32/49): loss=0.36646492863316243\n",
      "Gradient Descent(33/49): loss=0.3662952043641751\n",
      "Gradient Descent(34/49): loss=0.366138958706285\n",
      "Gradient Descent(35/49): loss=0.365995014350469\n",
      "Gradient Descent(36/49): loss=0.3658623002882095\n",
      "Gradient Descent(37/49): loss=0.36573984199277215\n",
      "Gradient Descent(38/49): loss=0.36562675253363314\n",
      "Gradient Descent(39/49): loss=0.3655222245314967\n",
      "Gradient Descent(40/49): loss=0.36542552287113284\n",
      "Gradient Descent(41/49): loss=0.36533597809790913\n",
      "Gradient Descent(42/49): loss=0.3652529804315637\n",
      "Gradient Descent(43/49): loss=0.36517597433758076\n",
      "Gradient Descent(44/49): loss=0.36510445360258775\n",
      "Gradient Descent(45/49): loss=0.3650379568655911\n",
      "Gradient Descent(46/49): loss=0.36497606356168594\n",
      "Gradient Descent(47/49): loss=0.36491839023917455\n",
      "Gradient Descent(48/49): loss=0.3648645872148811\n",
      "Gradient Descent(49/49): loss=0.3648143355358891\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41989763347917824\n",
      "Gradient Descent(2/49): loss=0.39924848954430997\n",
      "Gradient Descent(3/49): loss=0.3923943457884514\n",
      "Gradient Descent(4/49): loss=0.388964861438245\n",
      "Gradient Descent(5/49): loss=0.3865211283807544\n",
      "Gradient Descent(6/49): loss=0.384467098411362\n",
      "Gradient Descent(7/49): loss=0.38264394290140996\n",
      "Gradient Descent(8/49): loss=0.3809992463317749\n",
      "Gradient Descent(9/49): loss=0.3795079584716537\n",
      "Gradient Descent(10/49): loss=0.37815312349083063\n",
      "Gradient Descent(11/49): loss=0.37692098479697966\n",
      "Gradient Descent(12/49): loss=0.3757996112994902\n",
      "Gradient Descent(13/49): loss=0.3747784187185269\n",
      "Gradient Descent(14/49): loss=0.37384793806704963\n",
      "Gradient Descent(15/49): loss=0.37299966520477107\n",
      "Gradient Descent(16/49): loss=0.37222594530468106\n",
      "Gradient Descent(17/49): loss=0.371519877284359\n",
      "Gradient Descent(18/49): loss=0.3708752321125595\n",
      "Gradient Descent(19/49): loss=0.37028638182076234\n",
      "Gradient Descent(20/49): loss=0.36974823722791267\n",
      "Gradient Descent(21/49): loss=0.3692561929705185\n",
      "Gradient Descent(22/49): loss=0.3688060787696255\n",
      "Gradient Descent(23/49): loss=0.36839411608628986\n",
      "Gradient Descent(24/49): loss=0.3680168794712022\n",
      "Gradient Descent(25/49): loss=0.3676712620281319\n",
      "Gradient Descent(26/49): loss=0.36735444449891547\n",
      "Gradient Descent(27/49): loss=0.36706386754793513\n",
      "Gradient Descent(28/49): loss=0.3667972068813946\n",
      "Gradient Descent(29/49): loss=0.3665523508843938\n",
      "Gradient Descent(30/49): loss=0.36632738049897107\n",
      "Gradient Descent(31/49): loss=0.3661205511004449\n",
      "Gradient Descent(32/49): loss=0.36593027615865414\n",
      "Gradient Descent(33/49): loss=0.3657551124959159\n",
      "Gradient Descent(34/49): loss=0.3655937469753518\n",
      "Gradient Descent(35/49): loss=0.36544498447220686\n",
      "Gradient Descent(36/49): loss=0.36530773699733443\n",
      "Gradient Descent(37/49): loss=0.36518101385649027\n",
      "Gradient Descent(38/49): loss=0.36506391274177585\n",
      "Gradient Descent(39/49): loss=0.3649556116627309\n",
      "Gradient Descent(40/49): loss=0.36485536163441046\n",
      "Gradient Descent(41/49): loss=0.36476248004847284\n",
      "Gradient Descent(42/49): loss=0.3646763446609916\n",
      "Gradient Descent(43/49): loss=0.3645963881375236\n",
      "Gradient Descent(44/49): loss=0.3645220931020224\n",
      "Gradient Descent(45/49): loss=0.3644529876415755\n",
      "Gradient Descent(46/49): loss=0.36438864122374814\n",
      "Gradient Descent(47/49): loss=0.36432866098760286\n",
      "Gradient Descent(48/49): loss=0.36427268837329935\n",
      "Gradient Descent(49/49): loss=0.36422039605860473\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4201006288294067\n",
      "Gradient Descent(2/49): loss=0.39958063383674436\n",
      "Gradient Descent(3/49): loss=0.3927819285995271\n",
      "Gradient Descent(4/49): loss=0.3893773584040669\n",
      "Gradient Descent(5/49): loss=0.38694828140421245\n",
      "Gradient Descent(6/49): loss=0.38490506919277107\n",
      "Gradient Descent(7/49): loss=0.38309055772200623\n",
      "Gradient Descent(8/49): loss=0.3814527451586883\n",
      "Gradient Descent(9/49): loss=0.3799667595043907\n",
      "Gradient Descent(10/49): loss=0.37861581341563827\n",
      "Gradient Descent(11/49): loss=0.37738633213411515\n",
      "Gradient Descent(12/49): loss=0.37626656614827025\n",
      "Gradient Descent(13/49): loss=0.37524610022260646\n",
      "Gradient Descent(14/49): loss=0.3743156158293186\n",
      "Gradient Descent(15/49): loss=0.3734667391457061\n",
      "Gradient Descent(16/49): loss=0.3726919263899194\n",
      "Gradient Descent(17/49): loss=0.3719843701897651\n",
      "Gradient Descent(18/49): loss=0.3713379201479549\n",
      "Gradient Descent(19/49): loss=0.3707470140650304\n",
      "Gradient Descent(20/49): loss=0.3702066176686468\n",
      "Gradient Descent(21/49): loss=0.3697121713894141\n",
      "Gradient Descent(22/49): loss=0.3692595431147242\n",
      "Gradient Descent(23/49): loss=0.3688449860946615\n",
      "Gradient Descent(24/49): loss=0.368465101335914\n",
      "Gradient Descent(25/49): loss=0.3681168039342918\n",
      "Gradient Descent(26/49): loss=0.3677972928820436\n",
      "Gradient Descent(27/49): loss=0.3675040239527958\n",
      "Gradient Descent(28/49): loss=0.3672346853205414\n",
      "Gradient Descent(29/49): loss=0.3669871756133071\n",
      "Gradient Descent(30/49): loss=0.3667595841392462\n",
      "Gradient Descent(31/49): loss=0.36655017305446125\n",
      "Gradient Descent(32/49): loss=0.3663573612689759\n",
      "Gradient Descent(33/49): loss=0.3661797099107059\n",
      "Gradient Descent(34/49): loss=0.36601590918765753\n",
      "Gradient Descent(35/49): loss=0.3658647665063659\n",
      "Gradient Descent(36/49): loss=0.3657251957201662\n",
      "Gradient Descent(37/49): loss=0.3655962073945808\n",
      "Gradient Descent(38/49): loss=0.3654768999891536\n",
      "Gradient Descent(39/49): loss=0.36536645186571226\n",
      "Gradient Descent(40/49): loss=0.3652641140424452\n",
      "Gradient Descent(41/49): loss=0.36516920362152366\n",
      "Gradient Descent(42/49): loss=0.3650810978254078\n",
      "Gradient Descent(43/49): loss=0.364999228583555\n",
      "Gradient Descent(44/49): loss=0.36492307761712034\n",
      "Gradient Descent(45/49): loss=0.36485217197446707\n",
      "Gradient Descent(46/49): loss=0.3647860799749779\n",
      "Gradient Descent(47/49): loss=0.3647244075228399\n",
      "Gradient Descent(48/49): loss=0.3646667947562131\n",
      "Gradient Descent(49/49): loss=0.36461291300055476\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4195355098130625\n",
      "Gradient Descent(2/49): loss=0.39793616552861194\n",
      "Gradient Descent(3/49): loss=0.3907644514708716\n",
      "Gradient Descent(4/49): loss=0.387338089417524\n",
      "Gradient Descent(5/49): loss=0.38503441417606493\n",
      "Gradient Descent(6/49): loss=0.3831806677095833\n",
      "Gradient Descent(7/49): loss=0.38158386001325406\n",
      "Gradient Descent(8/49): loss=0.3801737528891514\n",
      "Gradient Descent(9/49): loss=0.37891473180885155\n",
      "Gradient Descent(10/49): loss=0.3777832150195756\n",
      "Gradient Descent(11/49): loss=0.3767613117603981\n",
      "Gradient Descent(12/49): loss=0.37583466494377965\n",
      "Gradient Descent(13/49): loss=0.37499147006179034\n",
      "Gradient Descent(14/49): loss=0.37422189038996084\n",
      "Gradient Descent(15/49): loss=0.37351764810951754\n",
      "Gradient Descent(16/49): loss=0.3728717173321661\n",
      "Gradient Descent(17/49): loss=0.37227808672026136\n",
      "Gradient Descent(18/49): loss=0.37173157342203084\n",
      "Gradient Descent(19/49): loss=0.3712276760876895\n",
      "Gradient Descent(20/49): loss=0.3707624580804284\n",
      "Gradient Descent(21/49): loss=0.3703324541921189\n",
      "Gradient Descent(22/49): loss=0.3699345957444649\n",
      "Gradient Descent(23/49): loss=0.36956615012608524\n",
      "Gradient Descent(24/49): loss=0.3692246717036274\n",
      "Gradient Descent(25/49): loss=0.36890796172509827\n",
      "Gradient Descent(26/49): loss=0.3686140353576276\n",
      "Gradient Descent(27/49): loss=0.36834109440715224\n",
      "Gradient Descent(28/49): loss=0.36808750458178024\n",
      "Gradient Descent(29/49): loss=0.36785177640481287\n",
      "Gradient Descent(30/49): loss=0.367632549073546\n",
      "Gradient Descent(31/49): loss=0.3674285767082305\n",
      "Gradient Descent(32/49): loss=0.36723871655139245\n",
      "Gradient Descent(33/49): loss=0.3670619187683035\n",
      "Gradient Descent(34/49): loss=0.36689721757039456\n",
      "Gradient Descent(35/49): loss=0.36674372343912987\n",
      "Gradient Descent(36/49): loss=0.36660061627169666\n",
      "Gradient Descent(37/49): loss=0.3664671393044057\n",
      "Gradient Descent(38/49): loss=0.3663425936969912\n",
      "Gradient Descent(39/49): loss=0.3662263336826057\n",
      "Gradient Descent(40/49): loss=0.36611776220547604\n",
      "Gradient Descent(41/49): loss=0.3660163269818543\n",
      "Gradient Descent(42/49): loss=0.3659215169308366\n",
      "Gradient Descent(43/49): loss=0.36583285893039225\n",
      "Gradient Descent(44/49): loss=0.3657499148610245\n",
      "Gradient Descent(45/49): loss=0.3656722789052045\n",
      "Gradient Descent(46/49): loss=0.36559957507538693\n",
      "Gradient Descent(47/49): loss=0.3655314549472306\n",
      "Gradient Descent(48/49): loss=0.36546759557778447\n",
      "Gradient Descent(49/49): loss=0.3654076975910058\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4174076991390114\n",
      "Gradient Descent(2/49): loss=0.3979994812483884\n",
      "Gradient Descent(3/49): loss=0.39176991749625034\n",
      "Gradient Descent(4/49): loss=0.3885364730864752\n",
      "Gradient Descent(5/49): loss=0.38613653334451814\n",
      "Gradient Descent(6/49): loss=0.3840863638261574\n",
      "Gradient Descent(7/49): loss=0.38226254148523797\n",
      "Gradient Descent(8/49): loss=0.38062190276195806\n",
      "Gradient Descent(9/49): loss=0.3791408953083016\n",
      "Gradient Descent(10/49): loss=0.3778020167638215\n",
      "Gradient Descent(11/49): loss=0.3765905448803524\n",
      "Gradient Descent(12/49): loss=0.37549359190672277\n",
      "Gradient Descent(13/49): loss=0.3744997237112832\n",
      "Gradient Descent(14/49): loss=0.37359873985846503\n",
      "Gradient Descent(15/49): loss=0.37278151414627547\n",
      "Gradient Descent(16/49): loss=0.37203986632744046\n",
      "Gradient Descent(17/49): loss=0.3713664541559576\n",
      "Gradient Descent(18/49): loss=0.3707546805265622\n",
      "Gradient Descent(19/49): loss=0.37019861261481835\n",
      "Gradient Descent(20/49): loss=0.36969291093144874\n",
      "Gradient Descent(21/49): loss=0.3692327667592089\n",
      "Gradient Descent(22/49): loss=0.36881384678288837\n",
      "Gradient Descent(23/49): loss=0.3684322439527197\n",
      "Gradient Descent(24/49): loss=0.3680844337862185\n",
      "Gradient Descent(25/49): loss=0.36776723543802564\n",
      "Gradient Descent(26/49): loss=0.36747777696533424\n",
      "Gradient Descent(27/49): loss=0.36721346429590923\n",
      "Gradient Descent(28/49): loss=0.36697195347144934\n",
      "Gradient Descent(29/49): loss=0.3667511257942435\n",
      "Gradient Descent(30/49): loss=0.3665490655519432\n",
      "Gradient Descent(31/49): loss=0.3663640400353477\n",
      "Gradient Descent(32/49): loss=0.36619448159858486\n",
      "Gradient Descent(33/49): loss=0.3660389715408699\n",
      "Gradient Descent(34/49): loss=0.36589622561488006\n",
      "Gradient Descent(35/49): loss=0.3657650809892756\n",
      "Gradient Descent(36/49): loss=0.36564448451254483\n",
      "Gradient Descent(37/49): loss=0.3655334821425232\n",
      "Gradient Descent(38/49): loss=0.36543120942101315\n",
      "Gradient Descent(39/49): loss=0.36533688288617533\n",
      "Gradient Descent(40/49): loss=0.3652497923270272\n",
      "Gradient Descent(41/49): loss=0.36516929379468094\n",
      "Gradient Descent(42/49): loss=0.365094803294054\n",
      "Gradient Descent(43/49): loss=0.3650257910878435\n",
      "Gradient Descent(44/49): loss=0.3649617765516999\n",
      "Gradient Descent(45/49): loss=0.36490232352589275\n",
      "Gradient Descent(46/49): loss=0.3648470361143948\n",
      "Gradient Descent(47/49): loss=0.3647955548873524\n",
      "Gradient Descent(48/49): loss=0.36474755344738824\n",
      "Gradient Descent(49/49): loss=0.3647027353241838\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41732699765931974\n",
      "Gradient Descent(2/49): loss=0.397896372475826\n",
      "Gradient Descent(3/49): loss=0.39164406098066906\n",
      "Gradient Descent(4/49): loss=0.3883848532299536\n",
      "Gradient Descent(5/49): loss=0.3859584538859751\n",
      "Gradient Descent(6/49): loss=0.38388241075438145\n",
      "Gradient Descent(7/49): loss=0.3820338429644103\n",
      "Gradient Descent(8/49): loss=0.38036976801864714\n",
      "Gradient Descent(9/49): loss=0.378866651300432\n",
      "Gradient Descent(10/49): loss=0.3775069408128638\n",
      "Gradient Descent(11/49): loss=0.376275841872871\n",
      "Gradient Descent(12/49): loss=0.37516039090238423\n",
      "Gradient Descent(13/49): loss=0.3741490821019955\n",
      "Gradient Descent(14/49): loss=0.37323164976378004\n",
      "Gradient Descent(15/49): loss=0.37239890883561505\n",
      "Gradient Descent(16/49): loss=0.371642625879911\n",
      "Gradient Descent(17/49): loss=0.3709554101854667\n",
      "Gradient Descent(18/49): loss=0.37033062004437856\n",
      "Gradient Descent(19/49): loss=0.36976228117930937\n",
      "Gradient Descent(20/49): loss=0.36924501524177894\n",
      "Gradient Descent(21/49): loss=0.3687739768303717\n",
      "Gradient Descent(22/49): loss=0.3683447978129701\n",
      "Gradient Descent(23/49): loss=0.36795353796776015\n",
      "Gradient Descent(24/49): loss=0.3675966411262388\n",
      "Gradient Descent(25/49): loss=0.3672708961302901\n",
      "Gradient Descent(26/49): loss=0.36697340201736717\n",
      "Gradient Descent(27/49): loss=0.36670153693056845\n",
      "Gradient Descent(28/49): loss=0.3664529303188002\n",
      "Gradient Descent(29/49): loss=0.3662254380495036\n",
      "Gradient Descent(30/49): loss=0.36601712010487614\n",
      "Gradient Descent(31/49): loss=0.3658262205737967\n",
      "Gradient Descent(32/49): loss=0.36565114968703216\n",
      "Gradient Descent(33/49): loss=0.36549046767376125\n",
      "Gradient Descent(34/49): loss=0.36534287024377154\n",
      "Gradient Descent(35/49): loss=0.3652071755225249\n",
      "Gradient Descent(36/49): loss=0.3650823122861521\n",
      "Gradient Descent(37/49): loss=0.364967309360779\n",
      "Gradient Descent(38/49): loss=0.36486128606575685\n",
      "Gradient Descent(39/49): loss=0.36476344359367485\n",
      "Gradient Descent(40/49): loss=0.3646730572317289\n",
      "Gradient Descent(41/49): loss=0.36458946933932707\n",
      "Gradient Descent(42/49): loss=0.3645120830059099\n",
      "Gradient Descent(43/49): loss=0.36444035632100313\n",
      "Gradient Descent(44/49): loss=0.3643737971956573\n",
      "Gradient Descent(45/49): loss=0.36431195868074795\n",
      "Gradient Descent(46/49): loss=0.3642544347332345\n",
      "Gradient Descent(47/49): loss=0.3642008563864848\n",
      "Gradient Descent(48/49): loss=0.36415088828522807\n",
      "Gradient Descent(49/49): loss=0.36410422554968663\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41754001007343644\n",
      "Gradient Descent(2/49): loss=0.39823859711548365\n",
      "Gradient Descent(3/49): loss=0.3920377243022958\n",
      "Gradient Descent(4/49): loss=0.3888012371219598\n",
      "Gradient Descent(5/49): loss=0.38638877795729265\n",
      "Gradient Descent(6/49): loss=0.38432333162518084\n",
      "Gradient Descent(7/49): loss=0.3824832143891157\n",
      "Gradient Descent(8/49): loss=0.38082573110860435\n",
      "Gradient Descent(9/49): loss=0.3793275280702385\n",
      "Gradient Descent(10/49): loss=0.37797125575148505\n",
      "Gradient Descent(11/49): loss=0.3767423340940974\n",
      "Gradient Descent(12/49): loss=0.37562800536806784\n",
      "Gradient Descent(13/49): loss=0.3746169489560673\n",
      "Gradient Descent(14/49): loss=0.3736990596335879\n",
      "Gradient Descent(15/49): loss=0.3728652884653501\n",
      "Gradient Descent(16/49): loss=0.3721075160464808\n",
      "Gradient Descent(17/49): loss=0.37141844653131006\n",
      "Gradient Descent(18/49): loss=0.3707915168273041\n",
      "Gradient Descent(19/49): loss=0.37022081767083603\n",
      "Gradient Descent(20/49): loss=0.369701024418887\n",
      "Gradient Descent(21/49): loss=0.3692273360034547\n",
      "Gradient Descent(22/49): loss=0.36879542086585904\n",
      "Gradient Descent(23/49): loss=0.36840136893007647\n",
      "Gradient Descent(24/49): loss=0.3680416488430457\n",
      "Gradient Descent(25/49): loss=0.36771306983453117\n",
      "Gradient Descent(26/49): loss=0.36741274764543336\n",
      "Gradient Descent(27/49): loss=0.3671380740504887\n",
      "Gradient Descent(28/49): loss=0.36688668956455284\n",
      "Gradient Descent(29/49): loss=0.3666564589745254\n",
      "Gradient Descent(30/49): loss=0.36644544938373963\n",
      "Gradient Descent(31/49): loss=0.36625191049389777\n",
      "Gradient Descent(32/49): loss=0.36607425688255235\n",
      "Gradient Descent(33/49): loss=0.36591105206260693\n",
      "Gradient Descent(34/49): loss=0.3657609941350434\n",
      "Gradient Descent(35/49): loss=0.36562290286763394\n",
      "Gradient Descent(36/49): loss=0.36549570805123976\n",
      "Gradient Descent(37/49): loss=0.3653784390018056\n",
      "Gradient Descent(38/49): loss=0.3652702150906643\n",
      "Gradient Descent(39/49): loss=0.3651702371985306\n",
      "Gradient Descent(40/49): loss=0.3650777799998239\n",
      "Gradient Descent(41/49): loss=0.3649921849939129\n",
      "Gradient Descent(42/49): loss=0.3649128542086763\n",
      "Gradient Descent(43/49): loss=0.3648392445095912\n",
      "Gradient Descent(44/49): loss=0.36477086245448936\n",
      "Gradient Descent(45/49): loss=0.36470725964029216\n",
      "Gradient Descent(46/49): loss=0.3646480284935272\n",
      "Gradient Descent(47/49): loss=0.3645927984613228\n",
      "Gradient Descent(48/49): loss=0.3645412325639523\n",
      "Gradient Descent(49/49): loss=0.3644930242739047\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41690278070029474\n",
      "Gradient Descent(2/49): loss=0.3965105206455697\n",
      "Gradient Descent(3/49): loss=0.38998828165038785\n",
      "Gradient Descent(4/49): loss=0.38677202193248267\n",
      "Gradient Descent(5/49): loss=0.38451443535121976\n",
      "Gradient Descent(6/49): loss=0.3826604325646509\n",
      "Gradient Descent(7/49): loss=0.3810550830009855\n",
      "Gradient Descent(8/49): loss=0.37963836737191964\n",
      "Gradient Descent(9/49): loss=0.3783765656767358\n",
      "Gradient Descent(10/49): loss=0.3772458367504235\n",
      "Gradient Descent(11/49): loss=0.37622763228938605\n",
      "Gradient Descent(12/49): loss=0.37530698035519405\n",
      "Gradient Descent(13/49): loss=0.3744715829863081\n",
      "Gradient Descent(14/49): loss=0.3737112212316987\n",
      "Gradient Descent(15/49): loss=0.3730173208638667\n",
      "Gradient Descent(16/49): loss=0.3723826225641245\n",
      "Gradient Descent(17/49): loss=0.3718009272370391\n",
      "Gradient Descent(18/49): loss=0.3712668975726241\n",
      "Gradient Descent(19/49): loss=0.37077590243401487\n",
      "Gradient Descent(20/49): loss=0.3703238941248721\n",
      "Gradient Descent(21/49): loss=0.369907311034054\n",
      "Gradient Descent(22/49): loss=0.3695229999482633\n",
      "Gradient Descent(23/49): loss=0.36916815366549405\n",
      "Gradient Descent(24/49): loss=0.36884026055667146\n",
      "Gradient Descent(25/49): loss=0.3685370634942805\n",
      "Gradient Descent(26/49): loss=0.36825652615553034\n",
      "Gradient Descent(27/49): loss=0.3679968051582135\n",
      "Gradient Descent(28/49): loss=0.36775622683306664\n",
      "Gradient Descent(29/49): loss=0.36753326770210054\n",
      "Gradient Descent(30/49): loss=0.36732653793690456\n",
      "Gradient Descent(31/49): loss=0.36713476722870036\n",
      "Gradient Descent(32/49): loss=0.3669567926238111\n",
      "Gradient Descent(33/49): loss=0.36679154797258534\n",
      "Gradient Descent(34/49): loss=0.36663805471301303\n",
      "Gradient Descent(35/49): loss=0.3664954137671735\n",
      "Gradient Descent(36/49): loss=0.3663627983730097\n",
      "Gradient Descent(37/49): loss=0.3662394477085734\n",
      "Gradient Descent(38/49): loss=0.3661246611930543\n",
      "Gradient Descent(39/49): loss=0.3660177933702672\n",
      "Gradient Descent(40/49): loss=0.3659182492971405\n",
      "Gradient Descent(41/49): loss=0.36582548037312196\n",
      "Gradient Descent(42/49): loss=0.36573898055706694\n",
      "Gradient Descent(43/49): loss=0.36565828292671165\n",
      "Gradient Descent(44/49): loss=0.3655829565426899\n",
      "Gradient Descent(45/49): loss=0.3655126035846306\n",
      "Gradient Descent(46/49): loss=0.3654468567313974\n",
      "Gradient Descent(47/49): loss=0.36538537676127275\n",
      "Gradient Descent(48/49): loss=0.36532785035096066\n",
      "Gradient Descent(49/49): loss=0.3652739880548615\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4149759936813126\n",
      "Gradient Descent(2/49): loss=0.39681657079085514\n",
      "Gradient Descent(3/49): loss=0.3911147771513668\n",
      "Gradient Descent(4/49): loss=0.38800460108857265\n",
      "Gradient Descent(5/49): loss=0.38560438410964726\n",
      "Gradient Descent(6/49): loss=0.38352870647029536\n",
      "Gradient Descent(7/49): loss=0.3816811794679608\n",
      "Gradient Descent(8/49): loss=0.3800244222119222\n",
      "Gradient Descent(9/49): loss=0.3785351038328378\n",
      "Gradient Descent(10/49): loss=0.37719469537912015\n",
      "Gradient Descent(11/49): loss=0.37598729533779884\n",
      "Gradient Descent(12/49): loss=0.3748989375323587\n",
      "Gradient Descent(13/49): loss=0.3739172542698939\n",
      "Gradient Descent(14/49): loss=0.3730312509974243\n",
      "Gradient Descent(15/49): loss=0.3722311312365681\n",
      "Gradient Descent(16/49): loss=0.3715081519315929\n",
      "Gradient Descent(17/49): loss=0.3708545005079841\n",
      "Gradient Descent(18/49): loss=0.3702631887952311\n",
      "Gradient Descent(19/49): loss=0.3697279606673884\n",
      "Gradient Descent(20/49): loss=0.3692432111517137\n",
      "Gradient Descent(21/49): loss=0.368803915290502\n",
      "Gradient Descent(22/49): loss=0.36840556539069447\n",
      "Gradient Descent(23/49): loss=0.36804411554155114\n",
      "Gradient Descent(24/49): loss=0.36771593246379825\n",
      "Gradient Descent(25/49): loss=0.36741775189628173\n",
      "Gradient Descent(26/49): loss=0.36714663984082546\n",
      "Gradient Descent(27/49): loss=0.3668999580802421\n",
      "Gradient Descent(28/49): loss=0.36667533346311004\n",
      "Gradient Descent(29/49): loss=0.36647063051530415\n",
      "Gradient Descent(30/49): loss=0.3662839269947003\n",
      "Gradient Descent(31/49): loss=0.3661134920537398\n",
      "Gradient Descent(32/49): loss=0.36595776671601854\n",
      "Gradient Descent(33/49): loss=0.3658153464088398\n",
      "Gradient Descent(34/49): loss=0.36568496532463646\n",
      "Gradient Descent(35/49): loss=0.36556548241104625\n",
      "Gradient Descent(36/49): loss=0.3654558688128194\n",
      "Gradient Descent(37/49): loss=0.36535519660915106\n",
      "Gradient Descent(38/49): loss=0.36526262870788545\n",
      "Gradient Descent(39/49): loss=0.36517740977368\n",
      "Gradient Descent(40/49): loss=0.3650988580809657\n",
      "Gradient Descent(41/49): loss=0.36502635819462276\n",
      "Gradient Descent(42/49): loss=0.36495935439195026\n",
      "Gradient Descent(43/49): loss=0.36489734474891816\n",
      "Gradient Descent(44/49): loss=0.364839875822004\n",
      "Gradient Descent(45/49): loss=0.36478653786428655\n",
      "Gradient Descent(46/49): loss=0.36473696052100335\n",
      "Gradient Descent(47/49): loss=0.36469080895557526\n",
      "Gradient Descent(48/49): loss=0.3646477803622564\n",
      "Gradient Descent(49/49): loss=0.364607600826164\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41489297922100243\n",
      "Gradient Descent(2/49): loss=0.3967115634751639\n",
      "Gradient Descent(3/49): loss=0.39098548227817437\n",
      "Gradient Descent(4/49): loss=0.38784796902486224\n",
      "Gradient Descent(5/49): loss=0.3854200182748758\n",
      "Gradient Descent(6/49): loss=0.38331746630455\n",
      "Gradient Descent(7/49): loss=0.38144440381358374\n",
      "Gradient Descent(8/49): loss=0.3797635778625872\n",
      "Gradient Descent(9/49): loss=0.3782516377282602\n",
      "Gradient Descent(10/49): loss=0.37688998017066905\n",
      "Gradient Descent(11/49): loss=0.37566261543183527\n",
      "Gradient Descent(12/49): loss=0.3745554911489592\n",
      "Gradient Descent(13/49): loss=0.3735561607492158\n",
      "Gradient Descent(14/49): loss=0.3726535589243295\n",
      "Gradient Descent(15/49): loss=0.371837825771942\n",
      "Gradient Descent(16/49): loss=0.37110016093161075\n",
      "Gradient Descent(17/49): loss=0.3704326994777072\n",
      "Gradient Descent(18/49): loss=0.3698284048732495\n",
      "Gradient Descent(19/49): loss=0.3692809758596731\n",
      "Gradient Descent(20/49): loss=0.3687847650071426\n",
      "Gradient Descent(21/49): loss=0.3683347071723245\n",
      "Gradient Descent(22/49): loss=0.36792625646115323\n",
      "Gradient Descent(23/49): loss=0.36755533054557377\n",
      "Gradient Descent(24/49): loss=0.3672182613728955\n",
      "Gradient Descent(25/49): loss=0.3669117514549293\n",
      "Gradient Descent(26/49): loss=0.3666328350436247\n",
      "Gradient Descent(27/49): loss=0.3663788435980189\n",
      "Gradient Descent(28/49): loss=0.3661473750289159\n",
      "Gradient Descent(29/49): loss=0.36593626627629305\n",
      "Gradient Descent(30/49): loss=0.36574356883249504\n",
      "Gradient Descent(31/49): loss=0.3655675268737352\n",
      "Gradient Descent(32/49): loss=0.36540655770474884\n",
      "Gradient Descent(33/49): loss=0.3652592342578282\n",
      "Gradient Descent(34/49): loss=0.3651242694188465\n",
      "Gradient Descent(35/49): loss=0.3650005019800387\n",
      "Gradient Descent(36/49): loss=0.36488688404288183\n",
      "Gradient Descent(37/49): loss=0.36478246971493916\n",
      "Gradient Descent(38/49): loss=0.36468640496244176\n",
      "Gradient Descent(39/49): loss=0.364597918496049\n",
      "Gradient Descent(40/49): loss=0.36451631358096925\n",
      "Gradient Descent(41/49): loss=0.36444096067469156\n",
      "Gradient Descent(42/49): loss=0.3643712908062102\n",
      "Gradient Descent(43/49): loss=0.36430678961999685\n",
      "Gradient Descent(44/49): loss=0.36424699201625993\n",
      "Gradient Descent(45/49): loss=0.36419147732636425\n",
      "Gradient Descent(46/49): loss=0.36413986496877787\n",
      "Gradient Descent(47/49): loss=0.36409181053668793\n",
      "Gradient Descent(48/49): loss=0.36404700227354736\n",
      "Gradient Descent(49/49): loss=0.3640051578973824\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41511599606056687\n",
      "Gradient Descent(2/49): loss=0.3970631269419482\n",
      "Gradient Descent(3/49): loss=0.3913844234260655\n",
      "Gradient Descent(4/49): loss=0.38826777844177907\n",
      "Gradient Descent(5/49): loss=0.38585327227984195\n",
      "Gradient Descent(6/49): loss=0.38376115497741026\n",
      "Gradient Descent(7/49): loss=0.3818963299547012\n",
      "Gradient Descent(8/49): loss=0.380221764261887\n",
      "Gradient Descent(9/49): loss=0.37871431599359906\n",
      "Gradient Descent(10/49): loss=0.3773556254188077\n",
      "Gradient Descent(11/49): loss=0.37612995003638383\n",
      "Gradient Descent(12/49): loss=0.37502346531377134\n",
      "Gradient Descent(13/49): loss=0.3740239234900871\n",
      "Gradient Descent(14/49): loss=0.3731204276142441\n",
      "Gradient Descent(15/49): loss=0.3723032580040378\n",
      "Gradient Descent(16/49): loss=0.37156373002611154\n",
      "Gradient Descent(17/49): loss=0.3708940738028687\n",
      "Gradient Descent(18/49): loss=0.3702873306656673\n",
      "Gradient Descent(19/49): loss=0.36973726306921234\n",
      "Gradient Descent(20/49): loss=0.3692382756777223\n",
      "Gradient Descent(21/49): loss=0.36878534591479184\n",
      "Gradient Descent(22/49): loss=0.3683739626379109\n",
      "Gradient Descent(23/49): loss=0.36800007185056305\n",
      "Gradient Descent(24/49): loss=0.3676600285479311\n",
      "Gradient Descent(25/49): loss=0.3673505539321608\n",
      "Gradient Descent(26/49): loss=0.3670686973441851\n",
      "Gradient Descent(27/49): loss=0.3668118023496591\n",
      "Gradient Descent(28/49): loss=0.36657747649180095\n",
      "Gradient Descent(29/49): loss=0.3663635642872971\n",
      "Gradient Descent(30/49): loss=0.36616812309526714\n",
      "Gradient Descent(31/49): loss=0.36598940153537857\n",
      "Gradient Descent(32/49): loss=0.3658258201708279\n",
      "Gradient Descent(33/49): loss=0.3656759542061523\n",
      "Gradient Descent(34/49): loss=0.36553851797951864\n",
      "Gradient Descent(35/49): loss=0.36541235105493913\n",
      "Gradient Descent(36/49): loss=0.36529640574236455\n",
      "Gradient Descent(37/49): loss=0.3651897358932696\n",
      "Gradient Descent(38/49): loss=0.36509148683656195\n",
      "Gradient Descent(39/49): loss=0.3650008863347694\n",
      "Gradient Descent(40/49): loss=0.36491723645374646\n",
      "Gradient Descent(41/49): loss=0.3648399062508613\n",
      "Gradient Descent(42/49): loss=0.3647683251969571\n",
      "Gradient Descent(43/49): loss=0.36470197725651926\n",
      "Gradient Descent(44/49): loss=0.36464039555857614\n",
      "Gradient Descent(45/49): loss=0.36458315759802573\n",
      "Gradient Descent(46/49): loss=0.36452988091345684\n",
      "Gradient Descent(47/49): loss=0.36448021919318613\n",
      "Gradient Descent(48/49): loss=0.3644338587662752\n",
      "Gradient Descent(49/49): loss=0.3643905154397767\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41440247972053446\n",
      "Gradient Descent(2/49): loss=0.39525817243095446\n",
      "Gradient Descent(3/49): loss=0.38931181706180157\n",
      "Gradient Descent(4/49): loss=0.38625502222181307\n",
      "Gradient Descent(5/49): loss=0.38402267590342937\n",
      "Gradient Descent(6/49): loss=0.3821624267725511\n",
      "Gradient Descent(7/49): loss=0.380548103700258\n",
      "Gradient Descent(8/49): loss=0.3791260240878722\n",
      "Gradient Descent(9/49): loss=0.3778630075310952\n",
      "Gradient Descent(10/49): loss=0.37673454352084074\n",
      "Gradient Descent(11/49): loss=0.3757213302828825\n",
      "Gradient Descent(12/49): loss=0.3748077793201936\n",
      "Gradient Descent(13/49): loss=0.3739811207870977\n",
      "Gradient Descent(14/49): loss=0.3732307768908248\n",
      "Gradient Descent(15/49): loss=0.3725478964894404\n",
      "Gradient Descent(16/49): loss=0.3719250016161251\n",
      "Gradient Descent(17/49): loss=0.3713557161003894\n",
      "Gradient Descent(18/49): loss=0.37083455569817303\n",
      "Gradient Descent(19/49): loss=0.3703567647615501\n",
      "Gradient Descent(20/49): loss=0.36991818833151996\n",
      "Gradient Descent(21/49): loss=0.3695151713159664\n",
      "Gradient Descent(22/49): loss=0.3691444784632032\n",
      "Gradient Descent(23/49): loss=0.3688032303679489\n",
      "Gradient Descent(24/49): loss=0.3684888518911552\n",
      "Gradient Descent(25/49): loss=0.3681990302368779\n",
      "Gradient Descent(26/49): loss=0.3679316805801372\n",
      "Gradient Descent(27/49): loss=0.3676849176323338\n",
      "Gradient Descent(28/49): loss=0.3674570319044492\n",
      "Gradient Descent(29/49): loss=0.3672464697122699\n",
      "Gradient Descent(30/49): loss=0.3670518161841329\n",
      "Gradient Descent(31/49): loss=0.3668717806967172\n",
      "Gradient Descent(32/49): loss=0.36670518429056714\n",
      "Gradient Descent(33/49): loss=0.36655094871374405\n",
      "Gradient Descent(34/49): loss=0.3664080868162936\n",
      "Gradient Descent(35/49): loss=0.3662756940754688\n",
      "Gradient Descent(36/49): loss=0.3661529410758977\n",
      "Gradient Descent(37/49): loss=0.366039066803218\n",
      "Gradient Descent(38/49): loss=0.36593337263643644\n",
      "Gradient Descent(39/49): loss=0.365835216945193\n",
      "Gradient Descent(40/49): loss=0.36574401021456115\n",
      "Gradient Descent(41/49): loss=0.3656592106330252\n",
      "Gradient Descent(42/49): loss=0.36558032008962865\n",
      "Gradient Descent(43/49): loss=0.36550688053457603\n",
      "Gradient Descent(44/49): loss=0.36543847066426477\n",
      "Gradient Descent(45/49): loss=0.3653747028971635\n",
      "Gradient Descent(46/49): loss=0.3653152206114094\n",
      "Gradient Descent(47/49): loss=0.36525969561869004\n",
      "Gradient Descent(48/49): loss=0.3652078258520433\n",
      "Gradient Descent(49/49): loss=0.36515933324779865\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41268076651906527\n",
      "Gradient Descent(2/49): loss=0.3957824830008909\n",
      "Gradient Descent(3/49): loss=0.3905335414499239\n",
      "Gradient Descent(4/49): loss=0.3875057716692862\n",
      "Gradient Descent(5/49): loss=0.38509181141643517\n",
      "Gradient Descent(6/49): loss=0.38298860458108075\n",
      "Gradient Descent(7/49): loss=0.3811191219271838\n",
      "Gradient Descent(8/49): loss=0.37944899696747264\n",
      "Gradient Descent(9/49): loss=0.37795422570462767\n",
      "Gradient Descent(10/49): loss=0.37661496857689525\n",
      "Gradient Descent(11/49): loss=0.37541404993145855\n",
      "Gradient Descent(12/49): loss=0.37433639209386965\n",
      "Gradient Descent(13/49): loss=0.37336868492845027\n",
      "Gradient Descent(14/49): loss=0.3724991430423728\n",
      "Gradient Descent(15/49): loss=0.3717173103145557\n",
      "Gradient Descent(16/49): loss=0.3710138963021351\n",
      "Gradient Descent(17/49): loss=0.3703806366181212\n",
      "Gradient Descent(18/49): loss=0.36981017241342296\n",
      "Gradient Descent(19/49): loss=0.3692959456066834\n",
      "Gradient Descent(20/49): loss=0.3688321073661976\n",
      "Gradient Descent(21/49): loss=0.3684134378909602\n",
      "Gradient Descent(22/49): loss=0.36803527590933455\n",
      "Gradient Descent(23/49): loss=0.3676934565853459\n",
      "Gradient Descent(24/49): loss=0.36738425673113745\n",
      "Gradient Descent(25/49): loss=0.3671043463900599\n",
      "Gradient Descent(26/49): loss=0.36685074599010553\n",
      "Gradient Descent(27/49): loss=0.3666207883794516\n",
      "Gradient Descent(28/49): loss=0.36641208514977136\n",
      "Gradient Descent(29/49): loss=0.3662224967322845\n",
      "Gradient Descent(30/49): loss=0.366050105818932\n",
      "Gradient Descent(31/49): loss=0.365893193718625\n",
      "Gradient Descent(32/49): loss=0.36575021930788887\n",
      "Gradient Descent(33/49): loss=0.3656198002776936\n",
      "Gradient Descent(34/49): loss=0.3655006964149323\n",
      "Gradient Descent(35/49): loss=0.365391794688743\n",
      "Gradient Descent(36/49): loss=0.36529209593942025\n",
      "Gradient Descent(37/49): loss=0.3652007029916217\n",
      "Gradient Descent(38/49): loss=0.3651168100344802\n",
      "Gradient Descent(39/49): loss=0.36503969312949003\n",
      "Gradient Descent(40/49): loss=0.3649687017230264\n",
      "Gradient Descent(41/49): loss=0.36490325105439003\n",
      "Gradient Descent(42/49): loss=0.36484281536259044\n",
      "Gradient Descent(43/49): loss=0.3647869218059377\n",
      "Gradient Descent(44/49): loss=0.3647351450180726\n",
      "Gradient Descent(45/49): loss=0.36468710223251793\n",
      "Gradient Descent(46/49): loss=0.3646424489152881\n",
      "Gradient Descent(47/49): loss=0.36460087485171006\n",
      "Gradient Descent(48/49): loss=0.3645621006394573\n",
      "Gradient Descent(49/49): loss=0.3645258745449897\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4125955781642264\n",
      "Gradient Descent(2/49): loss=0.39567561044922267\n",
      "Gradient Descent(3/49): loss=0.39040073426535393\n",
      "Gradient Descent(4/49): loss=0.3873440863979931\n",
      "Gradient Descent(5/49): loss=0.3849011792409274\n",
      "Gradient Descent(6/49): loss=0.3827701584893432\n",
      "Gradient Descent(7/49): loss=0.3808744046881243\n",
      "Gradient Descent(8/49): loss=0.3791796254804054\n",
      "Gradient Descent(9/49): loss=0.37766176105459387\n",
      "Gradient Descent(10/49): loss=0.3763008749473765\n",
      "Gradient Descent(11/49): loss=0.3750796890915269\n",
      "Gradient Descent(12/49): loss=0.3739830303307897\n",
      "Gradient Descent(13/49): loss=0.37299750299926054\n",
      "Gradient Descent(14/49): loss=0.37211124566223275\n",
      "Gradient Descent(15/49): loss=0.3713137341576044\n",
      "Gradient Descent(16/49): loss=0.3705956164375547\n",
      "Gradient Descent(17/49): loss=0.36994857161831407\n",
      "Gradient Descent(18/49): loss=0.3693651884334221\n",
      "Gradient Descent(19/49): loss=0.36883885970527136\n",
      "Gradient Descent(20/49): loss=0.36836369028555904\n",
      "Gradient Descent(21/49): loss=0.36793441645817765\n",
      "Gradient Descent(22/49): loss=0.3675463351778692\n",
      "Gradient Descent(23/49): loss=0.3671952417992596\n",
      "Gradient Descent(24/49): loss=0.36687737516824986\n",
      "Gradient Descent(25/49): loss=0.36658936912085793\n",
      "Gradient Descent(26/49): loss=0.3663282095754145\n",
      "Gradient Descent(27/49): loss=0.36609119652026245\n",
      "Gradient Descent(28/49): loss=0.36587591029609895\n",
      "Gradient Descent(29/49): loss=0.3656801816536545\n",
      "Gradient Descent(30/49): loss=0.3655020651364249\n",
      "Gradient Descent(31/49): loss=0.36533981539687255\n",
      "Gradient Descent(32/49): loss=0.36519186610465254\n",
      "Gradient Descent(33/49): loss=0.3650568111484271\n",
      "Gradient Descent(34/49): loss=0.3649333878698351\n",
      "Gradient Descent(35/49): loss=0.36482046210012387\n",
      "Gradient Descent(36/49): loss=0.364717014797622\n",
      "Gradient Descent(37/49): loss=0.36462213010824096\n",
      "Gradient Descent(38/49): loss=0.3645349846921038\n",
      "Gradient Descent(39/49): loss=0.3644548381776411\n",
      "Gradient Descent(40/49): loss=0.36438102462044764\n",
      "Gradient Descent(41/49): loss=0.36431294485817856\n",
      "Gradient Descent(42/49): loss=0.36425005966503426\n",
      "Gradient Descent(43/49): loss=0.36419188362018984\n",
      "Gradient Descent(44/49): loss=0.36413797961403566\n",
      "Gradient Descent(45/49): loss=0.36408795392449655\n",
      "Gradient Descent(46/49): loss=0.3640414518031189\n",
      "Gradient Descent(47/49): loss=0.3639981535171822\n",
      "Gradient Descent(48/49): loss=0.3639577707999122\n",
      "Gradient Descent(49/49): loss=0.363920043666038\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41282858679079826\n",
      "Gradient Descent(2/49): loss=0.3960357716931987\n",
      "Gradient Descent(3/49): loss=0.39080424780588885\n",
      "Gradient Descent(4/49): loss=0.3877669607727916\n",
      "Gradient Descent(5/49): loss=0.385337176416805\n",
      "Gradient Descent(6/49): loss=0.3832164525796596\n",
      "Gradient Descent(7/49): loss=0.3813286908774404\n",
      "Gradient Descent(8/49): loss=0.3796398019744928\n",
      "Gradient Descent(9/49): loss=0.3781259800238152\n",
      "Gradient Descent(10/49): loss=0.3767675758803322\n",
      "Gradient Descent(11/49): loss=0.37554758947937233\n",
      "Gradient Descent(12/49): loss=0.3744510946965951\n",
      "Gradient Descent(13/49): loss=0.3734649058034806\n",
      "Gradient Descent(14/49): loss=0.37257733561328893\n",
      "Gradient Descent(15/49): loss=0.37177800278975565\n",
      "Gradient Descent(16/49): loss=0.37105767159566183\n",
      "Gradient Descent(17/49): loss=0.3704081155586198\n",
      "Gradient Descent(18/49): loss=0.36982199992988274\n",
      "Gradient Descent(19/49): loss=0.3692927794994581\n",
      "Gradient Descent(20/49): loss=0.3688146092729899\n",
      "Gradient Descent(21/49): loss=0.36838226609221747\n",
      "Gradient Descent(22/49): loss=0.3679910796629753\n",
      "Gradient Descent(23/49): loss=0.36763687172651965\n",
      "Gradient Descent(24/49): loss=0.3673159023145462\n",
      "Gradient Descent(25/49): loss=0.3670248221888372\n",
      "Gradient Descent(26/49): loss=0.36676063069630266\n",
      "Gradient Descent(27/49): loss=0.3665206383772743\n",
      "Gradient Descent(28/49): loss=0.3663024337545\n",
      "Gradient Descent(29/49): loss=0.36610385380595245\n",
      "Gradient Descent(30/49): loss=0.36592295768891536\n",
      "Gradient Descent(31/49): loss=0.365758003337848\n",
      "Gradient Descent(32/49): loss=0.36560742660579126\n",
      "Gradient Descent(33/49): loss=0.3654698226598115\n",
      "Gradient Descent(34/49): loss=0.3653439293762069\n",
      "Gradient Descent(35/49): loss=0.3652286125117406\n",
      "Gradient Descent(36/49): loss=0.3651228524537158\n",
      "Gradient Descent(37/49): loss=0.3650257323748475\n",
      "Gradient Descent(38/49): loss=0.3649364276390906\n",
      "Gradient Descent(39/49): loss=0.36485419632226745\n",
      "Gradient Descent(40/49): loss=0.36477837072684594\n",
      "Gradient Descent(41/49): loss=0.3647083497838369\n",
      "Gradient Descent(42/49): loss=0.36464359224676823\n",
      "Gradient Descent(43/49): loss=0.364583610593253\n",
      "Gradient Descent(44/49): loss=0.364527965558994\n",
      "Gradient Descent(45/49): loss=0.3644762612373065\n",
      "Gradient Descent(46/49): loss=0.36442814068453216\n",
      "Gradient Descent(47/49): loss=0.36438328197818015\n",
      "Gradient Descent(48/49): loss=0.36434139468036136\n",
      "Gradient Descent(49/49): loss=0.3643022166641694\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4120346068737817\n",
      "Gradient Descent(2/49): loss=0.39416097138426315\n",
      "Gradient Descent(3/49): loss=0.3887164074570998\n",
      "Gradient Descent(4/49): loss=0.38577603600667737\n",
      "Gradient Descent(5/49): loss=0.38355375928896795\n",
      "Gradient Descent(6/49): loss=0.3816839617110166\n",
      "Gradient Descent(7/49): loss=0.3800611911471663\n",
      "Gradient Descent(8/49): loss=0.37863520399673795\n",
      "Gradient Descent(9/49): loss=0.3773724809043419\n",
      "Gradient Descent(10/49): loss=0.37624762180799\n",
      "Gradient Descent(11/49): loss=0.3752405427194587\n",
      "Gradient Descent(12/49): loss=0.37433505794482075\n",
      "Gradient Descent(13/49): loss=0.3735179521790626\n",
      "Gradient Descent(14/49): loss=0.3727783116201626\n",
      "Gradient Descent(15/49): loss=0.37210702555998443\n",
      "Gradient Descent(16/49): loss=0.37149640979238985\n",
      "Gradient Descent(17/49): loss=0.37093991965322426\n",
      "Gradient Descent(18/49): loss=0.370431929829955\n",
      "Gradient Descent(19/49): loss=0.36996756425577665\n",
      "Gradient Descent(20/49): loss=0.36954256376451705\n",
      "Gradient Descent(21/49): loss=0.36915318234416644\n",
      "Gradient Descent(22/49): loss=0.3687961051472083\n",
      "Gradient Descent(23/49): loss=0.3684683831308513\n",
      "Gradient Descent(24/49): loss=0.36816738047346187\n",
      "Gradient Descent(25/49): loss=0.36789073186180954\n",
      "Gradient Descent(26/49): loss=0.3676363074519384\n",
      "Gradient Descent(27/49): loss=0.3674021838365397\n",
      "Gradient Descent(28/49): loss=0.36718661974928796\n",
      "Gradient Descent(29/49): loss=0.36698803553544546\n",
      "Gradient Descent(30/49): loss=0.3668049956431607\n",
      "Gradient Descent(31/49): loss=0.36663619355987265\n",
      "Gradient Descent(32/49): loss=0.3664804387469134\n",
      "Gradient Descent(33/49): loss=0.36633664522311016\n",
      "Gradient Descent(34/49): loss=0.3662038215226047\n",
      "Gradient Descent(35/49): loss=0.36608106180900474\n",
      "Gradient Descent(36/49): loss=0.36596753797166176\n",
      "Gradient Descent(37/49): loss=0.36586249256356107\n",
      "Gradient Descent(38/49): loss=0.36576523246643133\n",
      "Gradient Descent(39/49): loss=0.3656751231890574\n",
      "Gradient Descent(40/49): loss=0.3655915837207835\n",
      "Gradient Descent(41/49): loss=0.3655140818748382\n",
      "Gradient Descent(42/49): loss=0.3654421300662051\n",
      "Gradient Descent(43/49): loss=0.36537528147686416\n",
      "Gradient Descent(44/49): loss=0.3653131265678026\n",
      "Gradient Descent(45/49): loss=0.36525528990257766\n",
      "Gradient Descent(46/49): loss=0.3652014272516551\n",
      "Gradient Descent(47/49): loss=0.3651512229504596\n",
      "Gradient Descent(48/49): loss=0.3651043874871907\n",
      "Gradient Descent(49/49): loss=0.36506065529911585\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41052201765226953\n",
      "Gradient Descent(2/49): loss=0.39487993880810757\n",
      "Gradient Descent(3/49): loss=0.39001128631249604\n",
      "Gradient Descent(4/49): loss=0.38703239766062497\n",
      "Gradient Descent(5/49): loss=0.38459562240696316\n",
      "Gradient Descent(6/49): loss=0.3824646243665222\n",
      "Gradient Descent(7/49): loss=0.38057543561879537\n",
      "Gradient Descent(8/49): loss=0.3788947116230248\n",
      "Gradient Descent(9/49): loss=0.3773971931206482\n",
      "Gradient Descent(10/49): loss=0.37606155544096387\n",
      "Gradient Descent(11/49): loss=0.3748692915542992\n",
      "Gradient Descent(12/49): loss=0.3738041939710399\n",
      "Gradient Descent(13/49): loss=0.37285200886817405\n",
      "Gradient Descent(14/49): loss=0.37200016843391565\n",
      "Gradient Descent(15/49): loss=0.37123757148325703\n",
      "Gradient Descent(16/49): loss=0.3705543987351127\n",
      "Gradient Descent(17/49): loss=0.36994195496220594\n",
      "Gradient Descent(18/49): loss=0.36939253289624063\n",
      "Gradient Descent(19/49): loss=0.36889929520776155\n",
      "Gradient Descent(20/49): loss=0.36845617174517653\n",
      "Gradient Descent(21/49): loss=0.36805776978872173\n",
      "Gradient Descent(22/49): loss=0.36769929548208646\n",
      "Gradient Descent(23/49): loss=0.36737648491134356\n",
      "Gradient Descent(24/49): loss=0.36708554354185313\n",
      "Gradient Descent(25/49): loss=0.3668230929182614\n",
      "Gradient Descent(26/49): loss=0.3665861236924526\n",
      "Gradient Descent(27/49): loss=0.3663719541771416\n",
      "Gradient Descent(28/49): loss=0.3661781937341964\n",
      "Gradient Descent(29/49): loss=0.36600271040082655\n",
      "Gradient Descent(30/49): loss=0.36584360223656115\n",
      "Gradient Descent(31/49): loss=0.36569917194193535\n",
      "Gradient Descent(32/49): loss=0.36556790435794956\n",
      "Gradient Descent(33/49): loss=0.36544844650526953\n",
      "Gradient Descent(34/49): loss=0.3653395898650808\n",
      "Gradient Descent(35/49): loss=0.3652402546405871\n",
      "Gradient Descent(36/49): loss=0.36514947577021933\n",
      "Gradient Descent(37/49): loss=0.36506639049145434\n",
      "Gradient Descent(38/49): loss=0.3649902272783418\n",
      "Gradient Descent(39/49): loss=0.36492029599691417\n",
      "Gradient Descent(40/49): loss=0.36485597914106876\n",
      "Gradient Descent(41/49): loss=0.3647967240276032\n",
      "Gradient Descent(42/49): loss=0.3647420358431899\n",
      "Gradient Descent(43/49): loss=0.3646914714484544\n",
      "Gradient Descent(44/49): loss=0.3646446338551949\n",
      "Gradient Descent(45/49): loss=0.3646011673023523\n",
      "Gradient Descent(46/49): loss=0.3645607528647719\n",
      "Gradient Descent(47/49): loss=0.364523104536238\n",
      "Gradient Descent(48/49): loss=0.36448796573482856\n",
      "Gradient Descent(49/49): loss=0.3644551061844446\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4104347944889913\n",
      "Gradient Descent(2/49): loss=0.3947712084818035\n",
      "Gradient Descent(3/49): loss=0.38987489483574633\n",
      "Gradient Descent(4/49): loss=0.3868656302954692\n",
      "Gradient Descent(5/49): loss=0.384398755408459\n",
      "Gradient Descent(6/49): loss=0.38223906139392333\n",
      "Gradient Descent(7/49): loss=0.3803229157500376\n",
      "Gradient Descent(8/49): loss=0.37861699461463527\n",
      "Gradient Descent(9/49): loss=0.377095949066511\n",
      "Gradient Descent(10/49): loss=0.375738337010398\n",
      "Gradient Descent(11/49): loss=0.37452553631507096\n",
      "Gradient Descent(12/49): loss=0.3734412355632549\n",
      "Gradient Descent(13/49): loss=0.37247108911608773\n",
      "Gradient Descent(14/49): loss=0.3716024478740788\n",
      "Gradient Descent(15/49): loss=0.3708241378394665\n",
      "Gradient Descent(16/49): loss=0.3701262735671913\n",
      "Gradient Descent(17/49): loss=0.36950009888601104\n",
      "Gradient Descent(18/49): loss=0.3689378497546989\n",
      "Gradient Descent(19/49): loss=0.3684326355006974\n",
      "Gradient Descent(20/49): loss=0.3679783355489007\n",
      "Gradient Descent(21/49): loss=0.3675695093308414\n",
      "Gradient Descent(22/49): loss=0.3672013174859435\n",
      "Gradient Descent(23/49): loss=0.3668694527865829\n",
      "Gradient Descent(24/49): loss=0.3665700794703487\n",
      "Gradient Descent(25/49): loss=0.3662997798654936\n",
      "Gradient Descent(26/49): loss=0.366055507361342\n",
      "Gradient Descent(27/49): loss=0.3658345449126362\n",
      "Gradient Descent(28/49): loss=0.36563446838133706\n",
      "Gradient Descent(29/49): loss=0.36545311411564363\n",
      "Gradient Descent(30/49): loss=0.36528855024730467\n",
      "Gradient Descent(31/49): loss=0.3651390512573113\n",
      "Gradient Descent(32/49): loss=0.3650030754188815\n",
      "Gradient Descent(33/49): loss=0.3648792447769672\n",
      "Gradient Descent(34/49): loss=0.36476632736670733\n",
      "Gradient Descent(35/49): loss=0.36466322141044877\n",
      "Gradient Descent(36/49): loss=0.36456894126507633\n",
      "Gradient Descent(37/49): loss=0.36448260491921924\n",
      "Gradient Descent(38/49): loss=0.36440342286404814\n",
      "Gradient Descent(39/49): loss=0.36433068818240466\n",
      "Gradient Descent(40/49): loss=0.36426376771933483\n",
      "Gradient Descent(41/49): loss=0.36420209421312527\n",
      "Gradient Descent(42/49): loss=0.3641451592799698\n",
      "Gradient Descent(43/49): loss=0.36409250715770464\n",
      "Gradient Descent(44/49): loss=0.36404372912486266\n",
      "Gradient Descent(45/49): loss=0.3639984585208113\n",
      "Gradient Descent(46/49): loss=0.36395636630112427\n",
      "Gradient Descent(47/49): loss=0.3639171570697319\n",
      "Gradient Descent(48/49): loss=0.3638805655359291\n",
      "Gradient Descent(49/49): loss=0.363846353350092\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4106777822641306\n",
      "Gradient Descent(2/49): loss=0.39513922984876826\n",
      "Gradient Descent(3/49): loss=0.3902823704670008\n",
      "Gradient Descent(4/49): loss=0.38729129237084803\n",
      "Gradient Descent(5/49): loss=0.3848373465537672\n",
      "Gradient Descent(6/49): loss=0.3826878096446436\n",
      "Gradient Descent(7/49): loss=0.3807793720352267\n",
      "Gradient Descent(8/49): loss=0.3790789360176329\n",
      "Gradient Descent(9/49): loss=0.37756146212650826\n",
      "Gradient Descent(10/49): loss=0.3762058391420903\n",
      "Gradient Descent(11/49): loss=0.3749937508139927\n",
      "Gradient Descent(12/49): loss=0.3739091490348646\n",
      "Gradient Descent(13/49): loss=0.37293790686357936\n",
      "Gradient Descent(14/49): loss=0.3720675535610832\n",
      "Gradient Descent(15/49): loss=0.37128705925869193\n",
      "Gradient Descent(16/49): loss=0.3705866544595454\n",
      "Gradient Descent(17/49): loss=0.3699576760809762\n",
      "Gradient Descent(18/49): loss=0.3693924347504597\n",
      "Gradient Descent(19/49): loss=0.3688840996548724\n",
      "Gradient Descent(20/49): loss=0.36842659816921514\n",
      "Gradient Descent(21/49): loss=0.36801452808237706\n",
      "Gradient Descent(22/49): loss=0.3676430806463575\n",
      "Gradient Descent(23/49): loss=0.3673079729768884\n",
      "Gradient Descent(24/49): loss=0.3670053885666912\n",
      "Gradient Descent(25/49): loss=0.36673192485920186\n",
      "Gradient Descent(26/49): loss=0.3664845469831994\n",
      "Gradient Descent(27/49): loss=0.36626054687546244\n",
      "Gradient Descent(28/49): loss=0.3660575071248274\n",
      "Gradient Descent(29/49): loss=0.3658732689608029\n",
      "Gradient Descent(30/49): loss=0.3657059038861761\n",
      "Gradient Descent(31/49): loss=0.3655536885181567\n",
      "Gradient Descent(32/49): loss=0.36541508225840014\n",
      "Gradient Descent(33/49): loss=0.3652887074602008\n",
      "Gradient Descent(34/49): loss=0.36517333180250755\n",
      "Gradient Descent(35/49): loss=0.36506785261616387\n",
      "Gradient Descent(36/49): loss=0.36497128293876585\n",
      "Gradient Descent(37/49): loss=0.3648827391014633\n",
      "Gradient Descent(38/49): loss=0.3648014296744709\n",
      "Gradient Descent(39/49): loss=0.36472664561851775\n",
      "Gradient Descent(40/49): loss=0.364657751507349\n",
      "Gradient Descent(41/49): loss=0.3645941777020555\n",
      "Gradient Descent(42/49): loss=0.3645354133717491\n",
      "Gradient Descent(43/49): loss=0.36448100026717156\n",
      "Gradient Descent(44/49): loss=0.3644305271644489\n",
      "Gradient Descent(45/49): loss=0.3643836249055607\n",
      "Gradient Descent(46/49): loss=0.3643399619703454\n",
      "Gradient Descent(47/49): loss=0.36429924052215834\n",
      "Gradient Descent(48/49): loss=0.3642611928757355\n",
      "Gradient Descent(49/49): loss=0.36422557834152697\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40979916216003653\n",
      "Gradient Descent(2/49): loss=0.3932018079152877\n",
      "Gradient Descent(3/49): loss=0.38818635317262873\n",
      "Gradient Descent(4/49): loss=0.38532668860235225\n",
      "Gradient Descent(5/49): loss=0.38310390701524666\n",
      "Gradient Descent(6/49): loss=0.38122313190491\n",
      "Gradient Descent(7/49): loss=0.37959298477708026\n",
      "Gradient Descent(8/49): loss=0.37816459486522214\n",
      "Gradient Descent(9/49): loss=0.37690356857627394\n",
      "Gradient Descent(10/49): loss=0.3757835157964667\n",
      "Gradient Descent(11/49): loss=0.37478357859530437\n",
      "Gradient Descent(12/49): loss=0.3738870027834562\n",
      "Gradient Descent(13/49): loss=0.3730801545936255\n",
      "Gradient Descent(14/49): loss=0.37235180505837134\n",
      "Gradient Descent(15/49): loss=0.3716925987637921\n",
      "Gradient Descent(16/49): loss=0.371094655399272\n",
      "Gradient Descent(17/49): loss=0.37055126857045123\n",
      "Gradient Descent(18/49): loss=0.37005667645550977\n",
      "Gradient Descent(19/49): loss=0.36960588583788007\n",
      "Gradient Descent(20/49): loss=0.3691945359932536\n",
      "Gradient Descent(21/49): loss=0.36881879247911553\n",
      "Gradient Descent(22/49): loss=0.3684752634741912\n",
      "Gradient Descent(23/49): loss=0.3681609332170679\n",
      "Gradient Descent(24/49): loss=0.3678731084899976\n",
      "Gradient Descent(25/49): loss=0.36760937512259123\n",
      "Gradient Descent(26/49): loss=0.3673675622496447\n",
      "Gradient Descent(27/49): loss=0.36714571261940454\n",
      "Gradient Descent(28/49): loss=0.36694205766549565\n",
      "Gradient Descent(29/49): loss=0.3667549963657164\n",
      "Gradient Descent(30/49): loss=0.36658307714198707\n",
      "Gradient Descent(31/49): loss=0.36642498222850683\n",
      "Gradient Descent(32/49): loss=0.3662795140647501\n",
      "Gradient Descent(33/49): loss=0.36614558336748787\n",
      "Gradient Descent(34/49): loss=0.36602219860976826\n",
      "Gradient Descent(35/49): loss=0.3659084566908015\n",
      "Gradient Descent(36/49): loss=0.3658035346234799\n",
      "Gradient Descent(37/49): loss=0.3657066820991281\n",
      "Gradient Descent(38/49): loss=0.3656172148145142\n",
      "Gradient Descent(39/49): loss=0.36553450846596897\n",
      "Gradient Descent(40/49): loss=0.36545799333104806\n",
      "Gradient Descent(41/49): loss=0.36538714937052824\n",
      "Gradient Descent(42/49): loss=0.36532150179342376\n",
      "Gradient Descent(43/49): loss=0.36526061703571966\n",
      "Gradient Descent(44/49): loss=0.365204099110056\n",
      "Gradient Descent(45/49): loss=0.3651515862890032\n",
      "Gradient Descent(46/49): loss=0.36510274808907656\n",
      "Gradient Descent(47/49): loss=0.36505728252643915\n",
      "Gradient Descent(48/49): loss=0.36501491361847055\n",
      "Gradient Descent(49/49): loss=0.3649753891081613\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40849974708092546\n",
      "Gradient Descent(2/49): loss=0.3940928053993424\n",
      "Gradient Descent(3/49): loss=0.38953568076803574\n",
      "Gradient Descent(4/49): loss=0.3865788838659369\n",
      "Gradient Descent(5/49): loss=0.38411364553620303\n",
      "Gradient Descent(6/49): loss=0.3819557624282531\n",
      "Gradient Descent(7/49): loss=0.38004936081576\n",
      "Gradient Descent(8/49): loss=0.3783607354176341\n",
      "Gradient Descent(9/49): loss=0.37686300188465177\n",
      "Gradient Descent(10/49): loss=0.37553324278724\n",
      "Gradient Descent(11/49): loss=0.3743515847707084\n",
      "Gradient Descent(12/49): loss=0.3733006818627157\n",
      "Gradient Descent(13/49): loss=0.3723653411742474\n",
      "Gradient Descent(14/49): loss=0.3715322252500056\n",
      "Gradient Descent(15/49): loss=0.37078960584784226\n",
      "Gradient Descent(16/49): loss=0.37012715608708263\n",
      "Gradient Descent(17/49): loss=0.3695357729247284\n",
      "Gradient Descent(18/49): loss=0.36900742442812445\n",
      "Gradient Descent(19/49): loss=0.3685350177380394\n",
      "Gradient Descent(20/49): loss=0.3681122845149719\n",
      "Gradient Descent(21/49): loss=0.36773368128033845\n",
      "Gradient Descent(22/49): loss=0.3673943025200383\n",
      "Gradient Descent(23/49): loss=0.3670898047699193\n",
      "Gradient Descent(24/49): loss=0.3668163401832303\n",
      "Gradient Descent(25/49): loss=0.3665704983083324\n",
      "Gradient Descent(26/49): loss=0.3663492549930694\n",
      "Gradient Descent(27/49): loss=0.36614992748877073\n",
      "Gradient Descent(28/49): loss=0.36597013495805636\n",
      "Gradient Descent(29/49): loss=0.36580776370117657\n",
      "Gradient Descent(30/49): loss=0.36566093650919856\n",
      "Gradient Descent(31/49): loss=0.3655279856318803\n",
      "Gradient Descent(32/49): loss=0.36540742891590927\n",
      "Gradient Descent(33/49): loss=0.36529794872721494\n",
      "Gradient Descent(34/49): loss=0.36519837332088106\n",
      "Gradient Descent(35/49): loss=0.36510766036505216\n",
      "Gradient Descent(36/49): loss=0.36502488236221525\n",
      "Gradient Descent(37/49): loss=0.3649492137432381\n",
      "Gradient Descent(38/49): loss=0.36487991943727394\n",
      "Gradient Descent(39/49): loss=0.3648163447447425\n",
      "Gradient Descent(40/49): loss=0.3647579063615622\n",
      "Gradient Descent(41/49): loss=0.36470408442109853\n",
      "Gradient Descent(42/49): loss=0.36465441543625743\n",
      "Gradient Descent(43/49): loss=0.36460848603812696\n",
      "Gradient Descent(44/49): loss=0.3645659274198029\n",
      "Gradient Descent(45/49): loss=0.3645264104047666\n",
      "Gradient Descent(46/49): loss=0.36448964106860776\n",
      "Gradient Descent(47/49): loss=0.3644553568511695\n",
      "Gradient Descent(48/49): loss=0.3644233231034802\n",
      "Gradient Descent(49/49): loss=0.3643933300202617\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4084106281952976\n",
      "Gradient Descent(2/49): loss=0.39398219983360966\n",
      "Gradient Descent(3/49): loss=0.38939563801263133\n",
      "Gradient Descent(4/49): loss=0.38640701678529615\n",
      "Gradient Descent(5/49): loss=0.3839105848630241\n",
      "Gradient Descent(6/49): loss=0.3817231775510601\n",
      "Gradient Descent(7/49): loss=0.3797891790182452\n",
      "Gradient Descent(8/49): loss=0.37807485249980416\n",
      "Gradient Descent(9/49): loss=0.37655319256651965\n",
      "Gradient Descent(10/49): loss=0.37520114586801584\n",
      "Gradient Descent(11/49): loss=0.3739987125372889\n",
      "Gradient Descent(12/49): loss=0.37292843486037325\n",
      "Gradient Descent(13/49): loss=0.3719750220313581\n",
      "Gradient Descent(14/49): loss=0.37112504997036794\n",
      "Gradient Descent(15/49): loss=0.3703667126083731\n",
      "Gradient Descent(16/49): loss=0.3696896120311102\n",
      "Gradient Descent(17/49): loss=0.36908457947880213\n",
      "Gradient Descent(18/49): loss=0.3685435215823414\n",
      "Gradient Descent(19/49): loss=0.36805928762127604\n",
      "Gradient Descent(20/49): loss=0.36762555450107054\n",
      "Gradient Descent(21/49): loss=0.36723672678685665\n",
      "Gradient Descent(22/49): loss=0.36688784960629617\n",
      "Gradient Descent(23/49): loss=0.3665745326020563\n",
      "Gradient Descent(24/49): loss=0.3662928834069755\n",
      "Gradient Descent(25/49): loss=0.36603944935199173\n",
      "Gradient Descent(26/49): loss=0.3658111663113395\n",
      "Gradient Descent(27/49): loss=0.3656053137505278\n",
      "Gradient Descent(28/49): loss=0.36541947517687495\n",
      "Gradient Descent(29/49): loss=0.36525150330501227\n",
      "Gradient Descent(30/49): loss=0.3650994893447235\n",
      "Gradient Descent(31/49): loss=0.36496173589889525\n",
      "Gradient Descent(32/49): loss=0.3648367330277129\n",
      "Gradient Descent(33/49): loss=0.3647231370935714\n",
      "Gradient Descent(34/49): loss=0.36461975205111774\n",
      "Gradient Descent(35/49): loss=0.3645255128897375\n",
      "Gradient Descent(36/49): loss=0.36443947097275425\n",
      "Gradient Descent(37/49): loss=0.36436078104953146\n",
      "Gradient Descent(38/49): loss=0.3642886897443012\n",
      "Gradient Descent(39/49): loss=0.36422252534954025\n",
      "Gradient Descent(40/49): loss=0.3641616887725748\n",
      "Gradient Descent(41/49): loss=0.36410564550228736\n",
      "Gradient Descent(42/49): loss=0.3640539184786696\n",
      "Gradient Descent(43/49): loss=0.3640060817618633\n",
      "Gradient Descent(44/49): loss=0.36396175490948335\n",
      "Gradient Descent(45/49): loss=0.3639205979816975\n",
      "Gradient Descent(46/49): loss=0.3638823071029034\n",
      "Gradient Descent(47/49): loss=0.3638466105170832\n",
      "Gradient Descent(48/49): loss=0.3638132650811748\n",
      "Gradient Descent(49/49): loss=0.363782053147185\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40866358248056384\n",
      "Gradient Descent(2/49): loss=0.39435734999066174\n",
      "Gradient Descent(3/49): loss=0.38980655700974576\n",
      "Gradient Descent(4/49): loss=0.3868352562611605\n",
      "Gradient Descent(5/49): loss=0.38435164536708116\n",
      "Gradient Descent(6/49): loss=0.3821742344016316\n",
      "Gradient Descent(7/49): loss=0.38024761936293594\n",
      "Gradient Descent(8/49): loss=0.37853834236544404\n",
      "Gradient Descent(9/49): loss=0.377019767805411\n",
      "Gradient Descent(10/49): loss=0.3756692145396459\n",
      "Gradient Descent(11/49): loss=0.37446701313334513\n",
      "Gradient Descent(12/49): loss=0.373395982601236\n",
      "Gradient Descent(13/49): loss=0.37244105743294287\n",
      "Gradient Descent(14/49): loss=0.37158899445306925\n",
      "Gradient Descent(15/49): loss=0.3708281319024791\n",
      "Gradient Descent(16/49): loss=0.3701481866514193\n",
      "Gradient Descent(17/49): loss=0.3695400811330079\n",
      "Gradient Descent(18/49): loss=0.3689957943874083\n",
      "Gradient Descent(19/49): loss=0.36850823315114667\n",
      "Gradient Descent(20/49): loss=0.3680711198653323\n",
      "Gradient Descent(21/49): loss=0.36767889510210405\n",
      "Gradient Descent(22/49): loss=0.36732663235785473\n",
      "Gradient Descent(23/49): loss=0.36700996350296067\n",
      "Gradient Descent(24/49): loss=0.3667250134470093\n",
      "Gradient Descent(25/49): loss=0.36646834279647345\n",
      "Gradient Descent(26/49): loss=0.36623689746115123\n",
      "Gradient Descent(27/49): loss=0.3660279643149839\n",
      "Gradient Descent(28/49): loss=0.365839132142091\n",
      "Gradient Descent(29/49): loss=0.365668257204564\n",
      "Gradient Descent(30/49): loss=0.36551343285818855\n",
      "Gradient Descent(31/49): loss=0.3653729627185794\n",
      "Gradient Descent(32/49): loss=0.36524533694543104\n",
      "Gradient Descent(33/49): loss=0.3651292112684833\n",
      "Gradient Descent(34/49): loss=0.36502338842687065\n",
      "Gradient Descent(35/49): loss=0.3649268017349535\n",
      "Gradient Descent(36/49): loss=0.3648385005235389\n",
      "Gradient Descent(37/49): loss=0.36475763723641996\n",
      "Gradient Descent(38/49): loss=0.36468345598909274\n",
      "Gradient Descent(39/49): loss=0.3646152824199363\n",
      "Gradient Descent(40/49): loss=0.3645525146845592\n",
      "Gradient Descent(41/49): loss=0.36449461546184503\n",
      "Gradient Descent(42/49): loss=0.36444110485581416\n",
      "Gradient Descent(43/49): loss=0.3643915540910761\n",
      "Gradient Descent(44/49): loss=0.36434557991161876\n",
      "Gradient Descent(45/49): loss=0.36430283960319426\n",
      "Gradient Descent(46/49): loss=0.36426302656880555\n",
      "Gradient Descent(47/49): loss=0.36422586639493404\n",
      "Gradient Descent(48/49): loss=0.3641911133533109\n",
      "Gradient Descent(49/49): loss=0.3641585472893578\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40769614557929884\n",
      "Gradient Descent(2/49): loss=0.39236461234411607\n",
      "Gradient Descent(3/49): loss=0.3877085705231888\n",
      "Gradient Descent(4/49): loss=0.3849007202415712\n",
      "Gradient Descent(5/49): loss=0.3826704699645612\n",
      "Gradient Descent(6/49): loss=0.3807785381646869\n",
      "Gradient Descent(7/49): loss=0.3791423581823467\n",
      "Gradient Descent(8/49): loss=0.3777130282766843\n",
      "Gradient Descent(9/49): loss=0.3764549803220292\n",
      "Gradient Descent(10/49): loss=0.37534080525303526\n",
      "Gradient Descent(11/49): loss=0.37434889826940737\n",
      "Gradient Descent(12/49): loss=0.3734619683696703\n",
      "Gradient Descent(13/49): loss=0.37266598881201146\n",
      "Gradient Descent(14/49): loss=0.3719494337613851\n",
      "Gradient Descent(15/49): loss=0.3713027154865997\n",
      "Gradient Descent(16/49): loss=0.37071776568007614\n",
      "Gradient Descent(17/49): loss=0.3701877214985576\n",
      "Gradient Descent(18/49): loss=0.3697066882180656\n",
      "Gradient Descent(19/49): loss=0.36926955825131336\n",
      "Gradient Descent(20/49): loss=0.3688718718470909\n",
      "Gradient Descent(21/49): loss=0.368509708782646\n",
      "Gradient Descent(22/49): loss=0.3681796032371541\n",
      "Gradient Descent(23/49): loss=0.36787847611682\n",
      "Gradient Descent(24/49): loss=0.3676035806142642\n",
      "Gradient Descent(25/49): loss=0.3673524578858119\n",
      "Gradient Descent(26/49): loss=0.36712290053391505\n",
      "Gradient Descent(27/49): loss=0.3669129221699841\n",
      "Gradient Descent(28/49): loss=0.3667207317643311\n",
      "Gradient Descent(29/49): loss=0.3665447118073539\n",
      "Gradient Descent(30/49): loss=0.36638339954038746\n",
      "Gradient Descent(31/49): loss=0.3662354706882009\n",
      "Gradient Descent(32/49): loss=0.36609972525421336\n",
      "Gradient Descent(33/49): loss=0.3659750750359719\n",
      "Gradient Descent(34/49): loss=0.36586053259091605\n",
      "Gradient Descent(35/49): loss=0.3657552014372451\n",
      "Gradient Descent(36/49): loss=0.36565826731641415\n",
      "Gradient Descent(37/49): loss=0.36556899037577684\n",
      "Gradient Descent(38/49): loss=0.36548669815465795\n",
      "Gradient Descent(39/49): loss=0.36541077927647564\n",
      "Gradient Descent(40/49): loss=0.3653406777648008\n",
      "Gradient Descent(41/49): loss=0.3652758879134157\n",
      "Gradient Descent(42/49): loss=0.3652159496502582\n",
      "Gradient Descent(43/49): loss=0.3651604443431545\n",
      "Gradient Descent(44/49): loss=0.36510899100185557\n",
      "Gradient Descent(45/49): loss=0.36506124283641683\n",
      "Gradient Descent(46/49): loss=0.3650168841366103\n",
      "Gradient Descent(47/49): loss=0.36497562744102763\n",
      "Gradient Descent(48/49): loss=0.3649372109679358\n",
      "Gradient Descent(49/49): loss=0.3649013962829013\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4066139548050328\n",
      "Gradient Descent(2/49): loss=0.3934060962186582\n",
      "Gradient Descent(3/49): loss=0.38909665604970545\n",
      "Gradient Descent(4/49): loss=0.3861411699022893\n",
      "Gradient Descent(5/49): loss=0.3836444081026234\n",
      "Gradient Descent(6/49): loss=0.3814612814899497\n",
      "Gradient Descent(7/49): loss=0.379540241545872\n",
      "Gradient Descent(8/49): loss=0.37784629477942144\n",
      "Gradient Descent(9/49): loss=0.37635069970378027\n",
      "Gradient Descent(10/49): loss=0.3750288783143802\n",
      "Gradient Descent(11/49): loss=0.3738595697771665\n",
      "Gradient Descent(12/49): loss=0.3728242880214534\n",
      "Gradient Descent(13/49): loss=0.37190691062638204\n",
      "Gradient Descent(14/49): loss=0.371093347086814\n",
      "Gradient Descent(15/49): loss=0.37037126310616825\n",
      "Gradient Descent(16/49): loss=0.3697298477365208\n",
      "Gradient Descent(17/49): loss=0.3691596148256343\n",
      "Gradient Descent(18/49): loss=0.36865223268261665\n",
      "Gradient Descent(19/49): loss=0.36820037733175043\n",
      "Gradient Descent(20/49): loss=0.36779760568399955\n",
      "Gradient Descent(21/49): loss=0.36743824564111727\n",
      "Gradient Descent(22/49): loss=0.36711730066552456\n",
      "Gradient Descent(23/49): loss=0.3668303667559675\n",
      "Gradient Descent(24/49): loss=0.36657356009607533\n",
      "Gradient Descent(25/49): loss=0.36634345391005435\n",
      "Gradient Descent(26/49): loss=0.36613702328016223\n",
      "Gradient Descent(27/49): loss=0.36595159686388257\n",
      "Gradient Descent(28/49): loss=0.3657848146020119\n",
      "Gradient Descent(29/49): loss=0.3656345906377321\n",
      "Gradient Descent(30/49): loss=0.3654990807755255\n",
      "Gradient Descent(31/49): loss=0.3653766539009719\n",
      "Gradient Descent(32/49): loss=0.3652658668608738\n",
      "Gradient Descent(33/49): loss=0.3651654423700356\n",
      "Gradient Descent(34/49): loss=0.36507424956826684\n",
      "Gradient Descent(35/49): loss=0.3649912869002904\n",
      "Gradient Descent(36/49): loss=0.3649156670334962\n",
      "Gradient Descent(37/49): loss=0.3648466035649168\n",
      "Gradient Descent(38/49): loss=0.3647833993003012\n",
      "Gradient Descent(39/49): loss=0.3647254359154317\n",
      "Gradient Descent(40/49): loss=0.364672164833503\n",
      "Gradient Descent(41/49): loss=0.36462309917294833\n",
      "Gradient Descent(42/49): loss=0.3645778066380065\n",
      "Gradient Descent(43/49): loss=0.3645359032399419\n",
      "Gradient Descent(44/49): loss=0.3644970477504522\n",
      "Gradient Descent(45/49): loss=0.36446093680072056\n",
      "Gradient Descent(46/49): loss=0.36442730054998995\n",
      "Gradient Descent(47/49): loss=0.3643958988566718\n",
      "Gradient Descent(48/49): loss=0.364366517893007\n",
      "Gradient Descent(49/49): loss=0.3643389671513227\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4065230792831451\n",
      "Gradient Descent(2/49): loss=0.3932935739422461\n",
      "Gradient Descent(3/49): loss=0.38895290206848754\n",
      "Gradient Descent(4/49): loss=0.38596419564266854\n",
      "Gradient Descent(5/49): loss=0.38343520288761485\n",
      "Gradient Descent(6/49): loss=0.38122177399140444\n",
      "Gradient Descent(7/49): loss=0.3792725389708132\n",
      "Gradient Descent(8/49): loss=0.37755242275261375\n",
      "Gradient Descent(9/49): loss=0.3760325339447096\n",
      "Gradient Descent(10/49): loss=0.3746881420021555\n",
      "Gradient Descent(11/49): loss=0.37349784921456797\n",
      "Gradient Descent(12/49): loss=0.37244305037909337\n",
      "Gradient Descent(13/49): loss=0.37150751908318064\n",
      "Gradient Descent(14/49): loss=0.37067707266126876\n",
      "Gradient Descent(15/49): loss=0.3699392936673994\n",
      "Gradient Descent(16/49): loss=0.3692832949063123\n",
      "Gradient Descent(17/49): loss=0.3686995194012238\n",
      "Gradient Descent(18/49): loss=0.3681795690616127\n",
      "Gradient Descent(19/49): loss=0.367716057281947\n",
      "Gradient Descent(20/49): loss=0.3673024816912603\n",
      "Gradient Descent(21/49): loss=0.3669331139879357\n",
      "Gradient Descent(22/49): loss=0.36660290433605525\n",
      "Gradient Descent(23/49): loss=0.3663073982244573\n",
      "Gradient Descent(24/49): loss=0.366042664029773\n",
      "Gradient Descent(25/49): loss=0.36580522980102137\n",
      "Gradient Descent(26/49): loss=0.36559202801013563\n",
      "Gradient Descent(27/49): loss=0.36540034720040665\n",
      "Gradient Descent(28/49): loss=0.3652277896210217\n",
      "Gradient Descent(29/49): loss=0.36507223406661093\n",
      "Gradient Descent(30/49): loss=0.36493180325067287\n",
      "Gradient Descent(31/49): loss=0.36480483513462564\n",
      "Gradient Descent(32/49): loss=0.3646898577129957\n",
      "Gradient Descent(33/49): loss=0.3645855668222922\n",
      "Gradient Descent(34/49): loss=0.36449080659836663\n",
      "Gradient Descent(35/49): loss=0.36440455225610524\n",
      "Gradient Descent(36/49): loss=0.3643258949074374\n",
      "Gradient Descent(37/49): loss=0.3642540281699444\n",
      "Gradient Descent(38/49): loss=0.3641882363497024\n",
      "Gradient Descent(39/49): loss=0.36412788400912643\n",
      "Gradient Descent(40/49): loss=0.36407240675411434\n",
      "Gradient Descent(41/49): loss=0.36402130309524355\n",
      "Gradient Descent(42/49): loss=0.36397412725557166\n",
      "Gradient Descent(43/49): loss=0.36393048281312035\n",
      "Gradient Descent(44/49): loss=0.3638900170796671\n",
      "Gradient Descent(45/49): loss=0.36385241612932834\n",
      "Gradient Descent(46/49): loss=0.3638174004007797\n",
      "Gradient Descent(47/49): loss=0.36378472080606195\n",
      "Gradient Descent(48/49): loss=0.36375415528688015\n",
      "Gradient Descent(49/49): loss=0.3637255057663157\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40678598744009786\n",
      "Gradient Descent(2/49): loss=0.3936751308033559\n",
      "Gradient Descent(3/49): loss=0.38936683235039804\n",
      "Gradient Descent(4/49): loss=0.38639485435049753\n",
      "Gradient Descent(5/49): loss=0.3838786230985976\n",
      "Gradient Descent(6/49): loss=0.3816749965477691\n",
      "Gradient Descent(7/49): loss=0.3797327814042901\n",
      "Gradient Descent(8/49): loss=0.37801725413521114\n",
      "Gradient Descent(9/49): loss=0.3764999543870387\n",
      "Gradient Descent(10/49): loss=0.3751565617670086\n",
      "Gradient Descent(11/49): loss=0.3739660300999256\n",
      "Gradient Descent(12/49): loss=0.37291004164666447\n",
      "Gradient Descent(13/49): loss=0.3719725999487255\n",
      "Gradient Descent(14/49): loss=0.3711397044196569\n",
      "Gradient Descent(15/49): loss=0.3703990811422282\n",
      "Gradient Descent(16/49): loss=0.3697399558654113\n",
      "Gradient Descent(17/49): loss=0.36915286043247464\n",
      "Gradient Descent(18/49): loss=0.36862946656562817\n",
      "Gradient Descent(19/49): loss=0.3681624424759053\n",
      "Gradient Descent(20/49): loss=0.3677453287464063\n",
      "Gradient Descent(21/49): loss=0.367372430616161\n",
      "Gradient Descent(22/49): loss=0.3670387242953839\n",
      "Gradient Descent(23/49): loss=0.3667397753336948\n",
      "Gradient Descent(24/49): loss=0.3664716673753298\n",
      "Gradient Descent(25/49): loss=0.366230939889979\n",
      "Gradient Descent(26/49): loss=0.36601453367798636\n",
      "Gradient Descent(27/49): loss=0.36581974312353777\n",
      "Gradient Descent(28/49): loss=0.3656441743159983\n",
      "Gradient Descent(29/49): loss=0.36548570828297333\n",
      "Gradient Descent(30/49): loss=0.36534246868306425\n",
      "Gradient Descent(31/49): loss=0.3652127933949315\n",
      "Gradient Descent(32/49): loss=0.36509520951481267\n",
      "Gradient Descent(33/49): loss=0.36498841133919935\n",
      "Gradient Descent(34/49): loss=0.3648912409647172\n",
      "Gradient Descent(35/49): loss=0.3648026711848221\n",
      "Gradient Descent(36/49): loss=0.36472179040391167\n",
      "Gradient Descent(37/49): loss=0.36464778932484976\n",
      "Gradient Descent(38/49): loss=0.3645799491965429\n",
      "Gradient Descent(39/49): loss=0.364517631434779\n",
      "Gradient Descent(40/49): loss=0.3644602684526243\n",
      "Gradient Descent(41/49): loss=0.36440735555677206\n",
      "Gradient Descent(42/49): loss=0.36435844378374627\n",
      "Gradient Descent(43/49): loss=0.36431313356515743\n",
      "Gradient Descent(44/49): loss=0.36427106912456925\n",
      "Gradient Descent(45/49): loss=0.3642319335202316\n",
      "Gradient Descent(46/49): loss=0.36419544425817857\n",
      "Gradient Descent(47/49): loss=0.3641613494091787\n",
      "Gradient Descent(48/49): loss=0.36412942417090827\n",
      "Gradient Descent(49/49): loss=0.36409946782364855\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40572555713156866\n",
      "Gradient Descent(2/49): loss=0.3916343549011311\n",
      "Gradient Descent(3/49): loss=0.38727227810611153\n",
      "Gradient Descent(4/49): loss=0.38449351670922216\n",
      "Gradient Descent(5/49): loss=0.38225158265743076\n",
      "Gradient Descent(6/49): loss=0.3803491053222667\n",
      "Gradient Descent(7/49): loss=0.3787083376822722\n",
      "Gradient Descent(8/49): loss=0.3772794435467231\n",
      "Gradient Descent(9/49): loss=0.3760255329579372\n",
      "Gradient Descent(10/49): loss=0.37491818938039817\n",
      "Gradient Descent(11/49): loss=0.37393509654880147\n",
      "Gradient Descent(12/49): loss=0.3730584580185143\n",
      "Gradient Descent(13/49): loss=0.37227387711757726\n",
      "Gradient Descent(14/49): loss=0.3715695467749572\n",
      "Gradient Descent(15/49): loss=0.3709356570484017\n",
      "Gradient Descent(16/49): loss=0.37036395802536737\n",
      "Gradient Descent(17/49): loss=0.36984743455960445\n",
      "Gradient Descent(18/49): loss=0.36938006201356005\n",
      "Gradient Descent(19/49): loss=0.3689566210106756\n",
      "Gradient Descent(20/49): loss=0.36857255542315\n",
      "Gradient Descent(21/49): loss=0.36822386223510606\n",
      "Gradient Descent(22/49): loss=0.3679070050686379\n",
      "Gradient Descent(23/49): loss=0.3676188454126257\n",
      "Gradient Descent(24/49): loss=0.3673565872109097\n",
      "Gradient Descent(25/49): loss=0.36711773162998085\n",
      "Gradient Descent(26/49): loss=0.3669000396660589\n",
      "Gradient Descent(27/49): loss=0.3667015008591335\n",
      "Gradient Descent(28/49): loss=0.36652030682269787\n",
      "Gradient Descent(29/49): loss=0.36635482861926894\n",
      "Gradient Descent(30/49): loss=0.36620359724680784\n",
      "Gradient Descent(31/49): loss=0.366065286673803\n",
      "Gradient Descent(32/49): loss=0.36593869898827514\n",
      "Gradient Descent(33/49): loss=0.3658227513206773\n",
      "Gradient Descent(34/49): loss=0.3657164642715047\n",
      "Gradient Descent(35/49): loss=0.36561895162783575\n",
      "Gradient Descent(36/49): loss=0.36552941119363846\n",
      "Gradient Descent(37/49): loss=0.3654471165898585\n",
      "Gradient Descent(38/49): loss=0.3653714099045148\n",
      "Gradient Descent(39/49): loss=0.36530169509202254\n",
      "Gradient Descent(40/49): loss=0.3652374320360635\n",
      "Gradient Descent(41/49): loss=0.3651781312024648\n",
      "Gradient Descent(42/49): loss=0.36512334881843117\n",
      "Gradient Descent(43/49): loss=0.36507268252263664\n",
      "Gradient Descent(44/49): loss=0.36502576743747206\n",
      "Gradient Descent(45/49): loss=0.36498227262048616\n",
      "Gradient Descent(46/49): loss=0.36494189785694175\n",
      "Gradient Descent(47/49): loss=0.36490437075961024\n",
      "Gradient Descent(48/49): loss=0.36486944414556555\n",
      "Gradient Descent(49/49): loss=0.3648368936629248\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40486464082459184\n",
      "Gradient Descent(2/49): loss=0.392805970967343\n",
      "Gradient Descent(3/49): loss=0.38868610003193316\n",
      "Gradient Descent(4/49): loss=0.38571635861739956\n",
      "Gradient Descent(5/49): loss=0.3831869052157709\n",
      "Gradient Descent(6/49): loss=0.38098060617256685\n",
      "Gradient Descent(7/49): loss=0.37904748589824\n",
      "Gradient Descent(8/49): loss=0.37735065865278494\n",
      "Gradient Descent(9/49): loss=0.3758593795132532\n",
      "Gradient Descent(10/49): loss=0.3745473658196912\n",
      "Gradient Descent(11/49): loss=0.3733919574308815\n",
      "Gradient Descent(12/49): loss=0.3723735322800102\n",
      "Gradient Descent(13/49): loss=0.3714750522673756\n",
      "Gradient Descent(14/49): loss=0.3706816938172351\n",
      "Gradient Descent(15/49): loss=0.36998054018681037\n",
      "Gradient Descent(16/49): loss=0.3693603218211426\n",
      "Gradient Descent(17/49): loss=0.3688111955064835\n",
      "Gradient Descent(18/49): loss=0.36832455553798255\n",
      "Gradient Descent(19/49): loss=0.3678928716478332\n",
      "Gradient Descent(20/49): loss=0.36750954948862324\n",
      "Gradient Descent(21/49): loss=0.3671688102378773\n",
      "Gradient Descent(22/49): loss=0.366865586483985\n",
      "Gradient Descent(23/49): loss=0.36659543202500144\n",
      "Gradient Descent(24/49): loss=0.36635444359247543\n",
      "Gradient Descent(25/49): loss=0.36613919282366525\n",
      "Gradient Descent(26/49): loss=0.3659466670620856\n",
      "Gradient Descent(27/49): loss=0.3657742177792798\n",
      "Gradient Descent(28/49): loss=0.365619515588392\n",
      "Gradient Descent(29/49): loss=0.36548051096907197\n",
      "Gradient Descent(30/49): loss=0.36535539994863275\n",
      "Gradient Descent(31/49): loss=0.3652425940903369\n",
      "Gradient Descent(32/49): loss=0.3651406942295269\n",
      "Gradient Descent(33/49): loss=0.3650484674747487\n",
      "Gradient Descent(34/49): loss=0.36496482705621835\n",
      "Gradient Descent(35/49): loss=0.3648888146597815\n",
      "Gradient Descent(36/49): loss=0.36481958493236477\n",
      "Gradient Descent(37/49): loss=0.3647563918860658\n",
      "Gradient Descent(38/49): loss=0.3646985769634784\n",
      "Gradient Descent(39/49): loss=0.36464555855746084\n",
      "Gradient Descent(40/49): loss=0.3645968228050235\n",
      "Gradient Descent(41/49): loss=0.36455191549795113\n",
      "Gradient Descent(42/49): loss=0.36451043497267444\n",
      "Gradient Descent(43/49): loss=0.3644720258591999\n",
      "Gradient Descent(44/49): loss=0.36443637358394565\n",
      "Gradient Descent(45/49): loss=0.3644031995344391\n",
      "Gradient Descent(46/49): loss=0.36437225680525404\n",
      "Gradient Descent(47/49): loss=0.36434332645453243\n",
      "Gradient Descent(48/49): loss=0.3643162142091513\n",
      "Gradient Descent(49/49): loss=0.36429074756419866\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4047721477525337\n",
      "Gradient Descent(2/49): loss=0.39269146742222033\n",
      "Gradient Descent(3/49): loss=0.38853858294331867\n",
      "Gradient Descent(4/49): loss=0.385534278694696\n",
      "Gradient Descent(5/49): loss=0.3829716111400891\n",
      "Gradient Descent(6/49): loss=0.38073427829541734\n",
      "Gradient Descent(7/49): loss=0.37877240315221267\n",
      "Gradient Descent(8/49): loss=0.3770489709673705\n",
      "Gradient Descent(9/49): loss=0.3755330607000908\n",
      "Gradient Descent(10/49): loss=0.3741982221983096\n",
      "Gradient Descent(11/49): loss=0.3730216488920318\n",
      "Gradient Descent(12/49): loss=0.37198359242392515\n",
      "Gradient Descent(13/49): loss=0.3710669045191512\n",
      "Gradient Descent(14/49): loss=0.37025666362132004\n",
      "Gradient Descent(15/49): loss=0.36953986416066986\n",
      "Gradient Descent(16/49): loss=0.3689051547590517\n",
      "Gradient Descent(17/49): loss=0.3683426159383265\n",
      "Gradient Descent(18/49): loss=0.36784357034877696\n",
      "Gradient Descent(19/49): loss=0.3674004201012749\n",
      "Gradient Descent(20/49): loss=0.3670065068780317\n",
      "Gradient Descent(21/49): loss=0.366655991303799\n",
      "Gradient Descent(22/49): loss=0.36634374868067676\n",
      "Gradient Descent(23/49): loss=0.36606527868048216\n",
      "Gradient Descent(24/49): loss=0.36581662698293615\n",
      "Gradient Descent(25/49): loss=0.3655943171684632\n",
      "Gradient Descent(26/49): loss=0.36539529143725746\n",
      "Gradient Descent(27/49): loss=0.3652168589433307\n",
      "Gradient Descent(28/49): loss=0.3650566507125734\n",
      "Gradient Descent(29/49): loss=0.36491258026442225\n",
      "Gradient Descent(30/49): loss=0.36478280918304623\n",
      "Gradient Descent(31/49): loss=0.3646657169903848\n",
      "Gradient Descent(32/49): loss=0.3645598747633913\n",
      "Gradient Descent(33/49): loss=0.3644640220142611\n",
      "Gradient Descent(34/49): loss=0.36437704641751145\n",
      "Gradient Descent(35/49): loss=0.36429796602340686\n",
      "Gradient Descent(36/49): loss=0.36422591364487683\n",
      "Gradient Descent(37/49): loss=0.3641601231460195\n",
      "Gradient Descent(38/49): loss=0.3640999173955408\n",
      "Gradient Descent(39/49): loss=0.3640446976789133\n",
      "Gradient Descent(40/49): loss=0.36399393438935307\n",
      "Gradient Descent(41/49): loss=0.3639471588405114\n",
      "Gradient Descent(42/49): loss=0.3639039560635652\n",
      "Gradient Descent(43/49): loss=0.36386395846858505\n",
      "Gradient Descent(44/49): loss=0.3638268402650214\n",
      "Gradient Descent(45/49): loss=0.3637923125491921\n",
      "Gradient Descent(46/49): loss=0.36376011897802746\n",
      "Gradient Descent(47/49): loss=0.36373003195825926\n",
      "Gradient Descent(48/49): loss=0.3637018492889225\n",
      "Gradient Descent(49/49): loss=0.36367539120262976\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40504499714273284\n",
      "Gradient Descent(2/49): loss=0.3930787210737998\n",
      "Gradient Descent(3/49): loss=0.38895517281787795\n",
      "Gradient Descent(4/49): loss=0.3859672375488189\n",
      "Gradient Descent(5/49): loss=0.38341728954058335\n",
      "Gradient Descent(6/49): loss=0.3811895249374334\n",
      "Gradient Descent(7/49): loss=0.37923427036879337\n",
      "Gradient Descent(8/49): loss=0.37751494704757715\n",
      "Gradient Descent(9/49): loss=0.3760011242605678\n",
      "Gradient Descent(10/49): loss=0.3746667959854514\n",
      "Gradient Descent(11/49): loss=0.3734895249234144\n",
      "Gradient Descent(12/49): loss=0.37244985845826223\n",
      "Gradient Descent(13/49): loss=0.37153088120934896\n",
      "Gradient Descent(14/49): loss=0.3707178536857636\n",
      "Gradient Descent(15/49): loss=0.36999791224361656\n",
      "Gradient Descent(16/49): loss=0.369359816038664\n",
      "Gradient Descent(17/49): loss=0.3687937316433707\n",
      "Gradient Descent(18/49): loss=0.36829104864816525\n",
      "Gradient Descent(19/49): loss=0.36784422114866033\n",
      "Gradient Descent(20/49): loss=0.3674466310681832\n",
      "Gradient Descent(21/49): loss=0.36709247001746803\n",
      "Gradient Descent(22/49): loss=0.36677663696512075\n",
      "Gradient Descent(23/49): loss=0.3664946494428093\n",
      "Gradient Descent(24/49): loss=0.3662425663719241\n",
      "Gradient Descent(25/49): loss=0.36601692089498516\n",
      "Gradient Descent(26/49): loss=0.36581466183983813\n",
      "Gradient Descent(27/49): loss=0.3656331026481752\n",
      "Gradient Descent(28/49): loss=0.36546987677006465\n",
      "Gradient Descent(29/49): loss=0.36532289866910866\n",
      "Gradient Descent(30/49): loss=0.3651903297034178\n",
      "Gradient Descent(31/49): loss=0.3650705482496838\n",
      "Gradient Descent(32/49): loss=0.36496212352435436\n",
      "Gradient Descent(33/49): loss=0.36486379262983193\n",
      "Gradient Descent(34/49): loss=0.36477444041678253\n",
      "Gradient Descent(35/49): loss=0.36469308180777926\n",
      "Gradient Descent(36/49): loss=0.36461884627400865\n",
      "Gradient Descent(37/49): loss=0.36455096419681193\n",
      "Gradient Descent(38/49): loss=0.3644887548803888\n",
      "Gradient Descent(39/49): loss=0.36443161601186147\n",
      "Gradient Descent(40/49): loss=0.36437901439076914\n",
      "Gradient Descent(41/49): loss=0.3643304777725085\n",
      "Gradient Descent(42/49): loss=0.3642855876897364\n",
      "Gradient Descent(43/49): loss=0.36424397313271084\n",
      "Gradient Descent(44/49): loss=0.36420530498432835\n",
      "Gradient Descent(45/49): loss=0.36416929111849844\n",
      "Gradient Descent(46/49): loss=0.36413567208174297\n",
      "Gradient Descent(47/49): loss=0.36410421728774145\n",
      "Gradient Descent(48/49): loss=0.3640747216631301\n",
      "Gradient Descent(49/49): loss=0.3640470026903882\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4038873968168461\n",
      "Gradient Descent(2/49): loss=0.39099704572701155\n",
      "Gradient Descent(3/49): loss=0.3868687040153165\n",
      "Gradient Descent(4/49): loss=0.38410172374034957\n",
      "Gradient Descent(5/49): loss=0.38184591297053583\n",
      "Gradient Descent(6/49): loss=0.37993396497793486\n",
      "Gradient Descent(7/49): loss=0.378290054433996\n",
      "Gradient Descent(8/49): loss=0.3768628665277069\n",
      "Gradient Descent(9/49): loss=0.37561413683353656\n",
      "Gradient Descent(10/49): loss=0.37451447399543497\n",
      "Gradient Descent(11/49): loss=0.3735408882615838\n",
      "Gradient Descent(12/49): loss=0.3726751071681824\n",
      "Gradient Descent(13/49): loss=0.37190238436673057\n",
      "Gradient Descent(14/49): loss=0.3712106446158246\n",
      "Gradient Descent(15/49): loss=0.37058986386059256\n",
      "Gradient Descent(16/49): loss=0.3700316155946832\n",
      "Gradient Descent(17/49): loss=0.3695287357139513\n",
      "Gradient Descent(18/49): loss=0.369075072336993\n",
      "Gradient Descent(19/49): loss=0.3686652969277329\n",
      "Gradient Descent(20/49): loss=0.36829475993556354\n",
      "Gradient Descent(21/49): loss=0.367959378998315\n",
      "Gradient Descent(22/49): loss=0.36765555115802123\n",
      "Gradient Descent(23/49): loss=0.36738008294763624\n",
      "Gradient Descent(24/49): loss=0.3671301339152677\n",
      "Gradient Descent(25/49): loss=0.36690317036800835\n",
      "Gradient Descent(26/49): loss=0.36669692698488005\n",
      "Gradient Descent(27/49): loss=0.3665093745694787\n",
      "Gradient Descent(28/49): loss=0.3663386926592537\n",
      "Gradient Descent(29/49): loss=0.36618324603047475\n",
      "Gradient Descent(30/49): loss=0.36604156437156543\n",
      "Gradient Descent(31/49): loss=0.36591232456787554\n",
      "Gradient Descent(32/49): loss=0.36579433516606624\n",
      "Gradient Descent(33/49): loss=0.36568652267882285\n",
      "Gradient Descent(34/49): loss=0.3655879194596623\n",
      "Gradient Descent(35/49): loss=0.3654976529296259\n",
      "Gradient Descent(36/49): loss=0.3654149359772781\n",
      "Gradient Descent(37/49): loss=0.3653390583839654\n",
      "Gradient Descent(38/49): loss=0.3652693791501342\n",
      "Gradient Descent(39/49): loss=0.3652053196173477\n",
      "Gradient Descent(40/49): loss=0.365146357295766\n",
      "Gradient Descent(41/49): loss=0.36509202031912014\n",
      "Gradient Descent(42/49): loss=0.3650418824593249\n",
      "Gradient Descent(43/49): loss=0.3649955586412938\n",
      "Gradient Descent(44/49): loss=0.36495270090562254\n",
      "Gradient Descent(45/49): loss=0.3649129947728498\n",
      "Gradient Descent(46/49): loss=0.36487615596820727\n",
      "Gradient Descent(47/49): loss=0.36484192747027394\n",
      "Gradient Descent(48/49): loss=0.36481007685087846\n",
      "Gradient Descent(49/49): loss=0.36478039387704764\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4032518051396025\n",
      "Gradient Descent(2/49): loss=0.3922797356039105\n",
      "Gradient Descent(3/49): loss=0.3882975770086906\n",
      "Gradient Descent(4/49): loss=0.38530241819521055\n",
      "Gradient Descent(5/49): loss=0.38274043777527994\n",
      "Gradient Descent(6/49): loss=0.3805132586480469\n",
      "Gradient Descent(7/49): loss=0.37857054397646234\n",
      "Gradient Descent(8/49): loss=0.37687313053788507\n",
      "Gradient Descent(9/49): loss=0.3753881752643263\n",
      "Gradient Descent(10/49): loss=0.3740876614672961\n",
      "Gradient Descent(11/49): loss=0.3729475249280775\n",
      "Gradient Descent(12/49): loss=0.3719470166581633\n",
      "Gradient Descent(13/49): loss=0.3710682006048996\n",
      "Gradient Descent(14/49): loss=0.3702955431001123\n",
      "Gradient Descent(15/49): loss=0.3696155708059179\n",
      "Gradient Descent(16/49): loss=0.3690165826093055\n",
      "Gradient Descent(17/49): loss=0.36848840531151605\n",
      "Gradient Descent(18/49): loss=0.36802218549250404\n",
      "Gradient Descent(19/49): loss=0.367610211574268\n",
      "Gradient Descent(20/49): loss=0.3672457612720285\n",
      "Gradient Descent(21/49): loss=0.36692297049864114\n",
      "Gradient Descent(22/49): loss=0.36663672047130336\n",
      "Gradient Descent(23/49): loss=0.3663825403149468\n",
      "Gradient Descent(24/49): loss=0.36615652289791445\n",
      "Gradient Descent(25/49): loss=0.36595525199597917\n",
      "Gradient Descent(26/49): loss=0.3657757391774205\n",
      "Gradient Descent(27/49): loss=0.3656153690474773\n",
      "Gradient Descent(28/49): loss=0.36547185169487356\n",
      "Gradient Descent(29/49): loss=0.36534318135394817\n",
      "Gradient Descent(30/49): loss=0.36522760043931923\n",
      "Gradient Descent(31/49): loss=0.3651235682308231\n",
      "Gradient Descent(32/49): loss=0.3650297335886141\n",
      "Gradient Descent(33/49): loss=0.364944911164945\n",
      "Gradient Descent(34/49): loss=0.3648680606528493\n",
      "Gradient Descent(35/49): loss=0.3647982686748168\n",
      "Gradient Descent(36/49): loss=0.36473473296830977\n",
      "Gradient Descent(37/49): loss=0.3646767485710569\n",
      "Gradient Descent(38/49): loss=0.36462369574863834\n",
      "Gradient Descent(39/49): loss=0.3645750294409346\n",
      "Gradient Descent(40/49): loss=0.36453027003338906\n",
      "Gradient Descent(41/49): loss=0.36448899528437695\n",
      "Gradient Descent(42/49): loss=0.3644508332619093\n",
      "Gradient Descent(43/49): loss=0.3644154561618796\n",
      "Gradient Descent(44/49): loss=0.364382574896522\n",
      "Gradient Descent(45/49): loss=0.36435193435602453\n",
      "Gradient Descent(46/49): loss=0.364323309258658\n",
      "Gradient Descent(47/49): loss=0.36429650051554957\n",
      "Gradient Descent(48/49): loss=0.3642713320456296\n",
      "Gradient Descent(49/49): loss=0.36424764798443876\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4031578336034635\n",
      "Gradient Descent(2/49): loss=0.39216316406494184\n",
      "Gradient Descent(3/49): loss=0.3881462529618494\n",
      "Gradient Descent(4/49): loss=0.3851152419693221\n",
      "Gradient Descent(5/49): loss=0.382519115823892\n",
      "Gradient Descent(6/49): loss=0.3802602144839799\n",
      "Gradient Descent(7/49): loss=0.3782882203733745\n",
      "Gradient Descent(8/49): loss=0.3765637969604708\n",
      "Gradient Descent(9/49): loss=0.375053901364895\n",
      "Gradient Descent(10/49): loss=0.37373033587938687\n",
      "Gradient Descent(11/49): loss=0.37256888088767703\n",
      "Gradient Descent(12/49): loss=0.37154865401475873\n",
      "Gradient Descent(13/49): loss=0.37065160262900704\n",
      "Gradient Descent(14/49): loss=0.36986208891202355\n",
      "Gradient Descent(15/49): loss=0.36916654463614446\n",
      "Gradient Descent(16/49): loss=0.3685531809223154\n",
      "Gradient Descent(17/49): loss=0.36801174254830255\n",
      "Gradient Descent(18/49): loss=0.3675332989467937\n",
      "Gradient Descent(19/49): loss=0.36711006573663413\n",
      "Gradient Descent(20/49): loss=0.3667352518495563\n",
      "Gradient Descent(21/49): loss=0.3664029282324691\n",
      "Gradient Descent(22/49): loss=0.3661079148186829\n",
      "Gradient Descent(23/49): loss=0.3658456830273238\n",
      "Gradient Descent(24/49): loss=0.36561227150530595\n",
      "Gradient Descent(25/49): loss=0.3654042131959169\n",
      "Gradient Descent(26/49): loss=0.3652184721207139\n",
      "Gradient Descent(27/49): loss=0.36505238851079835\n",
      "Gradient Descent(28/49): loss=0.3649036311301864\n",
      "Gradient Descent(29/49): loss=0.36477015580611205\n",
      "Gradient Descent(30/49): loss=0.3646501693251328\n",
      "Gradient Descent(31/49): loss=0.36454209797494197\n",
      "Gradient Descent(32/49): loss=0.36444456011391774\n",
      "Gradient Descent(33/49): loss=0.3643563422368934\n",
      "Gradient Descent(34/49): loss=0.3642763780791035\n",
      "Gradient Descent(35/49): loss=0.3642037303628455\n",
      "Gradient Descent(36/49): loss=0.36413757484489345\n",
      "Gradient Descent(37/49): loss=0.36407718636851777\n",
      "Gradient Descent(38/49): loss=0.3640219266633193\n",
      "Gradient Descent(39/49): loss=0.3639712336699398\n",
      "Gradient Descent(40/49): loss=0.3639246121959034\n",
      "Gradient Descent(41/49): loss=0.36388162573405497\n",
      "Gradient Descent(42/49): loss=0.3638418892968633\n",
      "Gradient Descent(43/49): loss=0.3638050631387469\n",
      "Gradient Descent(44/49): loss=0.3637708472549617\n",
      "Gradient Descent(45/49): loss=0.3637389765598093\n",
      "Gradient Descent(46/49): loss=0.36370921665928874\n",
      "Gradient Descent(47/49): loss=0.36368136014406865\n",
      "Gradient Descent(48/49): loss=0.3636552233380167\n",
      "Gradient Descent(49/49): loss=0.3636306434456835\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4034406115884686\n",
      "Gradient Descent(2/49): loss=0.39255541969141433\n",
      "Gradient Descent(3/49): loss=0.3885652236063613\n",
      "Gradient Descent(4/49): loss=0.3855504098360079\n",
      "Gradient Descent(5/49): loss=0.3829669544771185\n",
      "Gradient Descent(6/49): loss=0.38071734451977585\n",
      "Gradient Descent(7/49): loss=0.37875154047080123\n",
      "Gradient Descent(8/49): loss=0.3770307314637088\n",
      "Gradient Descent(9/49): loss=0.3755224205651192\n",
      "Gradient Descent(10/49): loss=0.37419888401350226\n",
      "Gradient Descent(11/49): loss=0.3730362860250619\n",
      "Gradient Descent(12/49): loss=0.37201404602690036\n",
      "Gradient Descent(13/49): loss=0.37111434571043983\n",
      "Gradient Descent(14/49): loss=0.37032172828201015\n",
      "Gradient Descent(15/49): loss=0.36962276513663744\n",
      "Gradient Descent(16/49): loss=0.3690057750394087\n",
      "Gradient Descent(17/49): loss=0.36846058572037776\n",
      "Gradient Descent(18/49): loss=0.3679783304546616\n",
      "Gradient Descent(19/49): loss=0.36755127386117287\n",
      "Gradient Descent(20/49): loss=0.36717266229792356\n",
      "Gradient Descent(21/49): loss=0.3668365950776544\n",
      "Gradient Descent(22/49): loss=0.3665379133814423\n",
      "Gradient Descent(23/49): loss=0.36627210426764756\n",
      "Gradient Descent(24/49): loss=0.36603521759379987\n",
      "Gradient Descent(25/49): loss=0.36582379401271486\n",
      "Gradient Descent(26/49): loss=0.365634802487493\n",
      "Gradient Descent(27/49): loss=0.36546558600516427\n",
      "Gradient Descent(28/49): loss=0.3653138143648082\n",
      "Gradient Descent(29/49): loss=0.3651774430802286\n",
      "Gradient Descent(30/49): loss=0.365054677575424\n",
      "Gradient Descent(31/49): loss=0.36494394196771873\n",
      "Gradient Descent(32/49): loss=0.3648438518322106\n",
      "Gradient Descent(33/49): loss=0.3647531904251319\n",
      "Gradient Descent(34/49): loss=0.36467088791524455\n",
      "Gradient Descent(35/49): loss=0.36459600323350216\n",
      "Gradient Descent(36/49): loss=0.3645277082035512\n",
      "Gradient Descent(37/49): loss=0.3644652736605734\n",
      "Gradient Descent(38/49): loss=0.36440805730461234\n",
      "Gradient Descent(39/49): loss=0.3643554930678357\n",
      "Gradient Descent(40/49): loss=0.36430708180393373\n",
      "Gradient Descent(41/49): loss=0.3642623831327079\n",
      "Gradient Descent(42/49): loss=0.36422100829443166\n",
      "Gradient Descent(43/49): loss=0.36418261388721795\n",
      "Gradient Descent(44/49): loss=0.36414689637682707\n",
      "Gradient Descent(45/49): loss=0.3641135872824144\n",
      "Gradient Descent(46/49): loss=0.3640824489539585\n",
      "Gradient Descent(47/49): loss=0.3640532708677573\n",
      "Gradient Descent(48/49): loss=0.3640258663756585\n",
      "Gradient Descent(49/49): loss=0.3640000698517783\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40218166463513105\n",
      "Gradient Descent(2/49): loss=0.3904397348727311\n",
      "Gradient Descent(3/49): loss=0.3864908139653497\n",
      "Gradient Descent(4/49): loss=0.38372293444777955\n",
      "Gradient Descent(5/49): loss=0.38145248474935783\n",
      "Gradient Descent(6/49): loss=0.3795323818808504\n",
      "Gradient Descent(7/49): loss=0.37788671650918504\n",
      "Gradient Descent(8/49): loss=0.3764623964824772\n",
      "Gradient Descent(9/49): loss=0.3752197857707916\n",
      "Gradient Descent(10/49): loss=0.37412856075032674\n",
      "Gradient Descent(11/49): loss=0.3731650956846681\n",
      "Gradient Descent(12/49): loss=0.37231066881659236\n",
      "Gradient Descent(13/49): loss=0.3715502015388517\n",
      "Gradient Descent(14/49): loss=0.37087136115375535\n",
      "Gradient Descent(15/49): loss=0.3702639158975934\n",
      "Gradient Descent(16/49): loss=0.3697192666338797\n",
      "Gradient Descent(17/49): loss=0.3692301031535605\n",
      "Gradient Descent(18/49): loss=0.3687901489411277\n",
      "Gradient Descent(19/49): loss=0.36839396917664985\n",
      "Gradient Descent(20/49): loss=0.36803682427833\n",
      "Gradient Descent(21/49): loss=0.36771455651960183\n",
      "Gradient Descent(22/49): loss=0.3674235008981948\n",
      "Gradient Descent(23/49): loss=0.36716041398140126\n",
      "Gradient Descent(24/49): loss=0.3669224162375655\n",
      "Gradient Descent(25/49): loss=0.3667069446200395\n",
      "Gradient Descent(26/49): loss=0.36651171305662295\n",
      "Gradient Descent(27/49): loss=0.3663346791259623\n",
      "Gradient Descent(28/49): loss=0.3661740156497723\n",
      "Gradient Descent(29/49): loss=0.36602808624991834\n",
      "Gradient Descent(30/49): loss=0.36589542414992476\n",
      "Gradient Descent(31/49): loss=0.36577471366764375\n",
      "Gradient Descent(32/49): loss=0.3656647739680486\n",
      "Gradient Descent(33/49): loss=0.3655645447353248\n",
      "Gradient Descent(34/49): loss=0.36547307349073943\n",
      "Gradient Descent(35/49): loss=0.36538950433357736\n",
      "Gradient Descent(36/49): loss=0.3653130679212953\n",
      "Gradient Descent(37/49): loss=0.3652430725351871\n",
      "Gradient Descent(38/49): loss=0.36517889610156484\n",
      "Gradient Descent(39/49): loss=0.365119979057394\n",
      "Gradient Descent(40/49): loss=0.3650658179646638\n",
      "Gradient Descent(41/49): loss=0.3650159597903621\n",
      "Gradient Descent(42/49): loss=0.36496999677941744\n",
      "Gradient Descent(43/49): loss=0.36492756185678843\n",
      "Gradient Descent(44/49): loss=0.3648883245023986\n",
      "Gradient Descent(45/49): loss=0.3648519870490623\n",
      "Gradient Descent(46/49): loss=0.3648182813591356\n",
      "Gradient Descent(47/49): loss=0.3647869658404923\n",
      "Gradient Descent(48/49): loss=0.3647578227666973\n",
      "Gradient Descent(49/49): loss=0.36473065587000303\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4017754477500647\n",
      "Gradient Descent(2/49): loss=0.39181584234409955\n",
      "Gradient Descent(3/49): loss=0.38792607281299535\n",
      "Gradient Descent(4/49): loss=0.38489794719420545\n",
      "Gradient Descent(5/49): loss=0.38230450156666673\n",
      "Gradient Descent(6/49): loss=0.3800588200811288\n",
      "Gradient Descent(7/49): loss=0.378108895876785\n",
      "Gradient Descent(8/49): loss=0.3764130439567301\n",
      "Gradient Descent(9/49): loss=0.37493625893247395\n",
      "Gradient Descent(10/49): loss=0.3736487706477237\n",
      "Gradient Descent(11/49): loss=0.3725251119329403\n",
      "Gradient Descent(12/49): loss=0.371543420431308\n",
      "Gradient Descent(13/49): loss=0.37068488335038724\n",
      "Gradient Descent(14/49): loss=0.36993328254653\n",
      "Gradient Descent(15/49): loss=0.3692746158418173\n",
      "Gradient Descent(16/49): loss=0.36869677888927443\n",
      "Gradient Descent(17/49): loss=0.368189296324828\n",
      "Gradient Descent(18/49): loss=0.36774309361064306\n",
      "Gradient Descent(19/49): loss=0.3673503027726634\n",
      "Gradient Descent(20/49): loss=0.36700409654522703\n",
      "Gradient Descent(21/49): loss=0.36669854643648425\n",
      "Gradient Descent(22/49): loss=0.3664285010148928\n",
      "Gradient Descent(23/49): loss=0.36618948134600543\n",
      "Gradient Descent(24/49): loss=0.36597759101746064\n",
      "Gradient Descent(25/49): loss=0.36578943860499225\n",
      "Gradient Descent(26/49): loss=0.3656220707729084\n",
      "Gradient Descent(27/49): loss=0.36547291448374397\n",
      "Gradient Descent(28/49): loss=0.36533972702515044\n",
      "Gradient Descent(29/49): loss=0.3652205527565941\n",
      "Gradient Descent(30/49): loss=0.36511368564121\n",
      "Gradient Descent(31/49): loss=0.36501763676490284\n",
      "Gradient Descent(32/49): loss=0.3649311061600566\n",
      "Gradient Descent(33/49): loss=0.3648529583486972\n",
      "Gradient Descent(34/49): loss=0.3647822011026287\n",
      "Gradient Descent(35/49): loss=0.3647179669883796\n",
      "Gradient Descent(36/49): loss=0.3646594973247356\n",
      "Gradient Descent(37/49): loss=0.36460612823185734\n",
      "Gradient Descent(38/49): loss=0.3645572784948265\n",
      "Gradient Descent(39/49): loss=0.36451243900208413\n",
      "Gradient Descent(40/49): loss=0.3644711635515353\n",
      "Gradient Descent(41/49): loss=0.364433060844908\n",
      "Gradient Descent(42/49): loss=0.3643977875149105\n",
      "Gradient Descent(43/49): loss=0.36436504205040576\n",
      "Gradient Descent(44/49): loss=0.3643345595026734\n",
      "Gradient Descent(45/49): loss=0.3643061068712627\n",
      "Gradient Descent(46/49): loss=0.36427947908129543\n",
      "Gradient Descent(47/49): loss=0.3642544954756434\n",
      "Gradient Descent(48/49): loss=0.3642309967554222\n",
      "Gradient Descent(49/49): loss=0.3642088423109378\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40168013683593456\n",
      "Gradient Descent(2/49): loss=0.39169709483872184\n",
      "Gradient Descent(3/49): loss=0.38777090484958915\n",
      "Gradient Descent(4/49): loss=0.38470569081308903\n",
      "Gradient Descent(5/49): loss=0.3820772169488675\n",
      "Gradient Descent(6/49): loss=0.3797991646636174\n",
      "Gradient Descent(7/49): loss=0.3778194688908353\n",
      "Gradient Descent(8/49): loss=0.37609623039037393\n",
      "Gradient Descent(9/49): loss=0.37459422260372205\n",
      "Gradient Descent(10/49): loss=0.3732834819974662\n",
      "Gradient Descent(11/49): loss=0.3721383774070347\n",
      "Gradient Descent(12/49): loss=0.3711369059086176\n",
      "Gradient Descent(13/49): loss=0.37026013141660497\n",
      "Gradient Descent(14/49): loss=0.3694917250713168\n",
      "Gradient Descent(15/49): loss=0.3688175833412407\n",
      "Gradient Descent(16/49): loss=0.36822550780948693\n",
      "Gradient Descent(17/49): loss=0.36770493503105445\n",
      "Gradient Descent(18/49): loss=0.36724670759179306\n",
      "Gradient Descent(19/49): loss=0.36684287937846616\n",
      "Gradient Descent(20/49): loss=0.3664865494427856\n",
      "Gradient Descent(21/49): loss=0.36617171988879865\n",
      "Gradient Descent(22/49): loss=0.3658931740310169\n",
      "Gradient Descent(23/49): loss=0.3656463717207185\n",
      "Gradient Descent(24/49): loss=0.3654273592604054\n",
      "Gradient Descent(25/49): loss=0.36523269175017814\n",
      "Gradient Descent(26/49): loss=0.36505936605595873\n",
      "Gradient Descent(27/49): loss=0.3649047628740426\n",
      "Gradient Descent(28/49): loss=0.36476659660166116\n",
      "Gradient Descent(29/49): loss=0.36464287191864386\n",
      "Gradient Descent(30/49): loss=0.3645318461483541\n",
      "Gradient Descent(31/49): loss=0.3644319966027868\n",
      "Gradient Descent(32/49): loss=0.36434199223174235\n",
      "Gradient Descent(33/49): loss=0.36426066899314063\n",
      "Gradient Descent(34/49): loss=0.36418700844385166\n",
      "Gradient Descent(35/49): loss=0.36412011912035774\n",
      "Gradient Descent(36/49): loss=0.3640592203381696\n",
      "Gradient Descent(37/49): loss=0.36400362808982933\n",
      "Gradient Descent(38/49): loss=0.36395274276491546\n",
      "Gradient Descent(39/49): loss=0.363906038452859\n",
      "Gradient Descent(40/49): loss=0.36386305362150856\n",
      "Gradient Descent(41/49): loss=0.3638233829920395\n",
      "Gradient Descent(42/49): loss=0.363786670454646\n",
      "Gradient Descent(43/49): loss=0.36375260289003297\n",
      "Gradient Descent(44/49): loss=0.3637209047795089\n",
      "Gradient Descent(45/49): loss=0.36369133350186406\n",
      "Gradient Descent(46/49): loss=0.3636636752285409\n",
      "Gradient Descent(47/49): loss=0.3636377413401442\n",
      "Gradient Descent(48/49): loss=0.36361336529735394\n",
      "Gradient Descent(49/49): loss=0.36359039990798486\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40197283077730533\n",
      "Gradient Descent(2/49): loss=0.3920936756480924\n",
      "Gradient Descent(3/49): loss=0.388192041583783\n",
      "Gradient Descent(4/49): loss=0.38514299532499535\n",
      "Gradient Descent(5/49): loss=0.38252711870714523\n",
      "Gradient Descent(6/49): loss=0.3802580385953682\n",
      "Gradient Descent(7/49): loss=0.3782840760707938\n",
      "Gradient Descent(8/49): loss=0.37656394778925256\n",
      "Gradient Descent(9/49): loss=0.37506302410757564\n",
      "Gradient Descent(10/49): loss=0.37375184113845955\n",
      "Gradient Descent(11/49): loss=0.3726051631811991\n",
      "Gradient Descent(12/49): loss=0.37160129319538704\n",
      "Gradient Descent(13/49): loss=0.37072152952440984\n",
      "Gradient Descent(14/49): loss=0.36994972240687773\n",
      "Gradient Descent(15/49): loss=0.36927190503484064\n",
      "Gradient Descent(16/49): loss=0.3686759833432813\n",
      "Gradient Descent(17/49): loss=0.36815147346928767\n",
      "Gradient Descent(18/49): loss=0.3676892785608265\n",
      "Gradient Descent(19/49): loss=0.367281498399263\n",
      "Gradient Descent(20/49): loss=0.36692126657031565\n",
      "Gradient Descent(21/49): loss=0.3666026108771515\n",
      "Gradient Descent(22/49): loss=0.36632033343912107\n",
      "Gradient Descent(23/49): loss=0.3660699075184581\n",
      "Gradient Descent(24/49): loss=0.36584738860202165\n",
      "Gradient Descent(25/49): loss=0.36564933766123486\n",
      "Gradient Descent(26/49): loss=0.3654727548392897\n",
      "Gradient Descent(27/49): loss=0.36531502208439426\n",
      "Gradient Descent(28/49): loss=0.3651738534721434\n",
      "Gradient Descent(29/49): loss=0.3650472521474543\n",
      "Gradient Descent(30/49): loss=0.36493347297362777\n",
      "Gradient Descent(31/49): loss=0.3648309901083485\n",
      "Gradient Descent(32/49): loss=0.3647384688381232\n",
      "Gradient Descent(33/49): loss=0.3646547410972672\n",
      "Gradient Descent(34/49): loss=0.36457878417793166\n",
      "Gradient Descent(35/49): loss=0.364509702206129\n",
      "Gradient Descent(36/49): loss=0.3644467100171676\n",
      "Gradient Descent(37/49): loss=0.3643891191139327\n",
      "Gradient Descent(38/49): loss=0.36433632543433286\n",
      "Gradient Descent(39/49): loss=0.36428779869107486\n",
      "Gradient Descent(40/49): loss=0.36424307307861736\n",
      "Gradient Descent(41/49): loss=0.3642017391694667\n",
      "Gradient Descent(42/49): loss=0.3641634368455353\n",
      "Gradient Descent(43/49): loss=0.36412784913063867\n",
      "Gradient Descent(44/49): loss=0.3640946968077981\n",
      "Gradient Descent(45/49): loss=0.3640637337202606\n",
      "Gradient Descent(46/49): loss=0.3640347426683319\n",
      "Gradient Descent(47/49): loss=0.3640075318255709\n",
      "Gradient Descent(48/49): loss=0.36398193160781045\n",
      "Gradient Descent(49/49): loss=0.36395779193709654\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40060836058642346\n",
      "Gradient Descent(2/49): loss=0.3899505122995593\n",
      "Gradient Descent(3/49): loss=0.38613306032540295\n",
      "Gradient Descent(4/49): loss=0.38335543986005516\n",
      "Gradient Descent(5/49): loss=0.3810705550429129\n",
      "Gradient Descent(6/49): loss=0.37914370883051834\n",
      "Gradient Descent(7/49): loss=0.3774975925173927\n",
      "Gradient Descent(8/49): loss=0.3760771972780327\n",
      "Gradient Descent(9/49): loss=0.3748415489499606\n",
      "Gradient Descent(10/49): loss=0.3737594377667869\n",
      "Gradient Descent(11/49): loss=0.3728066374701269\n",
      "Gradient Descent(12/49): loss=0.37196400074222774\n",
      "Gradient Descent(13/49): loss=0.3712161314153906\n",
      "Gradient Descent(14/49): loss=0.3705504479726495\n",
      "Gradient Descent(15/49): loss=0.3699565159759677\n",
      "Gradient Descent(16/49): loss=0.3694255668919771\n",
      "Gradient Descent(17/49): loss=0.3689501470422151\n",
      "Gradient Descent(18/49): loss=0.3685238580419585\n",
      "Gradient Descent(19/49): loss=0.36814116205950176\n",
      "Gradient Descent(20/49): loss=0.36779723340002757\n",
      "Gradient Descent(21/49): loss=0.3674878435239877\n",
      "Gradient Descent(22/49): loss=0.3672092704693087\n",
      "Gradient Descent(23/49): loss=0.3669582263127704\n",
      "Gradient Descent(24/49): loss=0.366731798153848\n",
      "Gradient Descent(25/49): loss=0.36652739938989803\n",
      "Gradient Descent(26/49): loss=0.3663427289496426\n",
      "Gradient Descent(27/49): loss=0.36617573678223947\n",
      "Gradient Descent(28/49): loss=0.36602459434410867\n",
      "Gradient Descent(29/49): loss=0.36588766914171217\n",
      "Gradient Descent(30/49): loss=0.36576350261469365\n",
      "Gradient Descent(31/49): loss=0.3656507908071469\n",
      "Gradient Descent(32/49): loss=0.365548367393962\n",
      "Gradient Descent(33/49): loss=0.3654551887171678\n",
      "Gradient Descent(34/49): loss=0.36537032055296975\n",
      "Gradient Descent(35/49): loss=0.36529292638007144\n",
      "Gradient Descent(36/49): loss=0.36522225695828087\n",
      "Gradient Descent(37/49): loss=0.3651576410564508\n",
      "Gradient Descent(38/49): loss=0.36509847719268085\n",
      "Gradient Descent(39/49): loss=0.36504422626897626\n",
      "Gradient Descent(40/49): loss=0.364994404998346\n",
      "Gradient Descent(41/49): loss=0.36494858003542235\n",
      "Gradient Descent(42/49): loss=0.36490636273270266\n",
      "Gradient Descent(43/49): loss=0.3648674044538737\n",
      "Gradient Descent(44/49): loss=0.36483139238370443\n",
      "Gradient Descent(45/49): loss=0.3647980457809326\n",
      "Gradient Descent(46/49): loss=0.36476711262661377\n",
      "Gradient Descent(47/49): loss=0.36473836662567855\n",
      "Gradient Descent(48/49): loss=0.3647116045240963\n",
      "Gradient Descent(49/49): loss=0.3646866437081328\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4004355686559783\n",
      "Gradient Descent(2/49): loss=0.39140388966087486\n",
      "Gradient Descent(3/49): loss=0.3875677652776366\n",
      "Gradient Descent(4/49): loss=0.38450199289256043\n",
      "Gradient Descent(5/49): loss=0.3818787134360872\n",
      "Gradient Descent(6/49): loss=0.3796169082515096\n",
      "Gradient Descent(7/49): loss=0.3776620451639855\n",
      "Gradient Descent(8/49): loss=0.3759697596331071\n",
      "Gradient Descent(9/49): loss=0.3745028381605473\n",
      "Gradient Descent(10/49): loss=0.3732297452151047\n",
      "Gradient Descent(11/49): loss=0.37212361705180214\n",
      "Gradient Descent(12/49): loss=0.3711614955848816\n",
      "Gradient Descent(13/49): loss=0.37032371562523575\n",
      "Gradient Descent(14/49): loss=0.3695934024703131\n",
      "Gradient Descent(15/49): loss=0.3689560544450256\n",
      "Gradient Descent(16/49): loss=0.368399193278434\n",
      "Gradient Descent(17/49): loss=0.36791206974404483\n",
      "Gradient Descent(18/49): loss=0.36748541485265673\n",
      "Gradient Descent(19/49): loss=0.3671112288818238\n",
      "Gradient Descent(20/49): loss=0.36678260200911367\n",
      "Gradient Descent(21/49): loss=0.36649356146079776\n",
      "Gradient Descent(22/49): loss=0.3662389409903781\n",
      "Gradient Descent(23/49): loss=0.3660142692234294\n",
      "Gradient Descent(24/49): loss=0.3658156739884136\n",
      "Gradient Descent(25/49): loss=0.3656398002276552\n",
      "Gradient Descent(26/49): loss=0.3654837394712165\n",
      "Gradient Descent(27/49): loss=0.3653449691762876\n",
      "Gradient Descent(28/49): loss=0.36522130049935386\n",
      "Gradient Descent(29/49): loss=0.36511083328832655\n",
      "Gradient Descent(30/49): loss=0.36501191726533644\n",
      "Gradient Descent(31/49): loss=0.36492311852460035\n",
      "Gradient Descent(32/49): loss=0.3648431905989575\n",
      "Gradient Descent(33/49): loss=0.3647710494575973\n",
      "Gradient Descent(34/49): loss=0.3647057518896054\n",
      "Gradient Descent(35/49): loss=0.36464647680603884\n",
      "Gradient Descent(36/49): loss=0.3645925090596029\n",
      "Gradient Descent(37/49): loss=0.364543225437518\n",
      "Gradient Descent(38/49): loss=0.36449808253139254\n",
      "Gradient Descent(39/49): loss=0.36445660622914605\n",
      "Gradient Descent(40/49): loss=0.3644183826093217\n",
      "Gradient Descent(41/49): loss=0.36438305004839533\n",
      "Gradient Descent(42/49): loss=0.3643502923776709\n",
      "Gradient Descent(43/49): loss=0.3643198329486775\n",
      "Gradient Descent(44/49): loss=0.36429142948520377\n",
      "Gradient Descent(45/49): loss=0.3642648696166446\n",
      "Gradient Descent(46/49): loss=0.36423996700159517\n",
      "Gradient Descent(47/49): loss=0.36421655796292435\n",
      "Gradient Descent(48/49): loss=0.3641944985661739\n",
      "Gradient Descent(49/49): loss=0.3641736620822903\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40033905744994686\n",
      "Gradient Descent(2/49): loss=0.3912828378887742\n",
      "Gradient Descent(3/49): loss=0.38740872104776525\n",
      "Gradient Descent(4/49): loss=0.38430467826227677\n",
      "Gradient Descent(5/49): loss=0.38164553464024326\n",
      "Gradient Descent(6/49): loss=0.37935074682354314\n",
      "Gradient Descent(7/49): loss=0.3773656500443696\n",
      "Gradient Descent(8/49): loss=0.37564562805059104\n",
      "Gradient Descent(9/49): loss=0.3741532269141083\n",
      "Gradient Descent(10/49): loss=0.37285670628055795\n",
      "Gradient Descent(11/49): loss=0.3717290299915427\n",
      "Gradient Descent(12/49): loss=0.37074709200520684\n",
      "Gradient Descent(13/49): loss=0.3698910967445452\n",
      "Gradient Descent(14/49): loss=0.3691440517978303\n",
      "Gradient Descent(15/49): loss=0.36849134726779714\n",
      "Gradient Descent(16/49): loss=0.3679204041636544\n",
      "Gradient Descent(17/49): loss=0.3674203788494026\n",
      "Gradient Descent(18/49): loss=0.3669819135385054\n",
      "Gradient Descent(19/49): loss=0.36659692491751406\n",
      "Gradient Descent(20/49): loss=0.3662584245355079\n",
      "Gradient Descent(21/49): loss=0.3659603657896257\n",
      "Gradient Descent(22/49): loss=0.3656975132723228\n",
      "Gradient Descent(23/49): loss=0.3654653309892693\n",
      "Gradient Descent(24/49): loss=0.36525988655344066\n",
      "Gradient Descent(25/49): loss=0.36507776894379995\n",
      "Gradient Descent(26/49): loss=0.3649160178104326\n",
      "Gradient Descent(27/49): loss=0.36477206263059825\n",
      "Gradient Descent(28/49): loss=0.36464367028614675\n",
      "Gradient Descent(29/49): loss=0.3645288998531411\n",
      "Gradient Descent(30/49): loss=0.3644260635780146\n",
      "Gradient Descent(31/49): loss=0.36433369316798325\n",
      "Gradient Descent(32/49): loss=0.36425051065217556\n",
      "Gradient Descent(33/49): loss=0.36417540317836744\n",
      "Gradient Descent(34/49): loss=0.36410740120182644\n",
      "Gradient Descent(35/49): loss=0.36404565960040347\n",
      "Gradient Descent(36/49): loss=0.3639894413159695\n",
      "Gradient Descent(37/49): loss=0.3639381031784638\n",
      "Gradient Descent(38/49): loss=0.3638910836167655\n",
      "Gradient Descent(39/49): loss=0.36384789200158646\n",
      "Gradient Descent(40/49): loss=0.3638080994006979\n",
      "Gradient Descent(41/49): loss=0.3637713305569168\n",
      "Gradient Descent(42/49): loss=0.36373725692515196\n",
      "Gradient Descent(43/49): loss=0.3637055906270605\n",
      "Gradient Descent(44/49): loss=0.3636760792010155\n",
      "Gradient Descent(45/49): loss=0.36364850104160046\n",
      "Gradient Descent(46/49): loss=0.3636226614370772\n",
      "Gradient Descent(47/49): loss=0.3635983891255703\n",
      "Gradient Descent(48/49): loss=0.3635755333013171\n",
      "Gradient Descent(49/49): loss=0.3635539610115151\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40064165470924284\n",
      "Gradient Descent(2/49): loss=0.3916830880381988\n",
      "Gradient Descent(3/49): loss=0.3878318634562367\n",
      "Gradient Descent(4/49): loss=0.3847440584881983\n",
      "Gradient Descent(5/49): loss=0.38209740165838646\n",
      "Gradient Descent(6/49): loss=0.37981122691966873\n",
      "Gradient Descent(7/49): loss=0.3778313851875934\n",
      "Gradient Descent(8/49): loss=0.37611396357407784\n",
      "Gradient Descent(9/49): loss=0.37462215093523726\n",
      "Gradient Descent(10/49): loss=0.373324728331539\n",
      "Gradient Descent(11/49): loss=0.3721950640323135\n",
      "Gradient Descent(12/49): loss=0.3712103602031531\n",
      "Gradient Descent(13/49): loss=0.3703510546474448\n",
      "Gradient Descent(14/49): loss=0.3696003313628055\n",
      "Gradient Descent(15/49): loss=0.3689437137699848\n",
      "Gradient Descent(16/49): loss=0.3683687236006018\n",
      "Gradient Descent(17/49): loss=0.3678645932066755\n",
      "Gradient Descent(18/49): loss=0.36742202193651197\n",
      "Gradient Descent(19/49): loss=0.3670329691717617\n",
      "Gradient Descent(20/49): loss=0.3666904780461938\n",
      "Gradient Descent(21/49): loss=0.3663885249586332\n",
      "Gradient Descent(22/49): loss=0.3661218908518421\n",
      "Gradient Descent(23/49): loss=0.36588605091675425\n",
      "Gradient Descent(24/49): loss=0.365677079937792\n",
      "Gradient Descent(25/49): loss=0.36549157094870177\n",
      "Gradient Descent(26/49): loss=0.3653265652407422\n",
      "Gradient Descent(27/49): loss=0.3651794920723757\n",
      "Gradient Descent(28/49): loss=0.36504811668444975\n",
      "Gradient Descent(29/49): loss=0.364930495437086\n",
      "Gradient Descent(30/49): loss=0.3648249370619482\n",
      "Gradient Descent(31/49): loss=0.3647299691724737\n",
      "Gradient Descent(32/49): loss=0.3646443093000397\n",
      "Gradient Descent(33/49): loss=0.364566839829932\n",
      "Gradient Descent(34/49): loss=0.3644965863006754\n",
      "Gradient Descent(35/49): loss=0.3644326986064449\n",
      "Gradient Descent(36/49): loss=0.3643744347070995\n",
      "Gradient Descent(37/49): loss=0.36432114650566183\n",
      "Gradient Descent(38/49): loss=0.3642722676003217\n",
      "Gradient Descent(39/49): loss=0.3642273026584781\n",
      "Gradient Descent(40/49): loss=0.3641858181950136\n",
      "Gradient Descent(41/49): loss=0.36414743456676235\n",
      "Gradient Descent(42/49): loss=0.3641118190207306\n",
      "Gradient Descent(43/49): loss=0.36407867965564206\n",
      "Gradient Descent(44/49): loss=0.3640477601753633\n",
      "Gradient Descent(45/49): loss=0.36401883532911455\n",
      "Gradient Descent(46/49): loss=0.3639917069474892\n",
      "Gradient Descent(47/49): loss=0.36396620049549455\n",
      "Gradient Descent(48/49): loss=0.36394216207435404\n",
      "Gradient Descent(49/49): loss=0.3639194558129194\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3991674846707233\n",
      "Gradient Descent(2/49): loss=0.3895185078790607\n",
      "Gradient Descent(3/49): loss=0.3857911520633143\n",
      "Gradient Descent(4/49): loss=0.38299803346474437\n",
      "Gradient Descent(5/49): loss=0.38069953139224005\n",
      "Gradient Descent(6/49): loss=0.37876735967770747\n",
      "Gradient Descent(7/49): loss=0.377122001717926\n",
      "Gradient Descent(8/49): loss=0.3757064909043288\n",
      "Gradient Descent(9/49): loss=0.3744785640012828\n",
      "Gradient Descent(10/49): loss=0.37340617135538495\n",
      "Gradient Descent(11/49): loss=0.3724645188401663\n",
      "Gradient Descent(12/49): loss=0.37163405426833196\n",
      "Gradient Descent(13/49): loss=0.3708990760972552\n",
      "Gradient Descent(14/49): loss=0.3702467608545959\n",
      "Gradient Descent(15/49): loss=0.3696664754141643\n",
      "Gradient Descent(16/49): loss=0.36914928463970303\n",
      "Gradient Descent(17/49): loss=0.3686875940375304\n",
      "Gradient Descent(18/49): loss=0.3682748864449646\n",
      "Gradient Descent(19/49): loss=0.36790552479363414\n",
      "Gradient Descent(20/49): loss=0.36757460176904894\n",
      "Gradient Descent(21/49): loss=0.36727782314016494\n",
      "Gradient Descent(22/49): loss=0.3670114155818421\n",
      "Gradient Descent(23/49): loss=0.36677205257753304\n",
      "Gradient Descent(24/49): loss=0.36655679388408674\n",
      "Gradient Descent(25/49): loss=0.3663630353445182\n",
      "Gradient Descent(26/49): loss=0.3661884667364978\n",
      "Gradient Descent(27/49): loss=0.36603103597174036\n",
      "Gradient Descent(28/49): loss=0.3658889184009226\n",
      "Gradient Descent(29/49): loss=0.3657604902890402\n",
      "Gradient Descent(30/49): loss=0.36564430574725004\n",
      "Gradient Descent(31/49): loss=0.3655390765665869\n",
      "Gradient Descent(32/49): loss=0.365443654515202\n",
      "Gradient Descent(33/49): loss=0.36535701574679097\n",
      "Gradient Descent(34/49): loss=0.3652782470325107\n",
      "Gradient Descent(35/49): loss=0.3652065335780524\n",
      "Gradient Descent(36/49): loss=0.3651411482258856\n",
      "Gradient Descent(37/49): loss=0.36508144187299774\n",
      "Gradient Descent(38/49): loss=0.3650268349587973\n",
      "Gradient Descent(39/49): loss=0.3649768098977241\n",
      "Gradient Descent(40/49): loss=0.3649309043475595\n",
      "Gradient Descent(41/49): loss=0.36488870521822236\n",
      "Gradient Descent(42/49): loss=0.3648498433375341\n",
      "Gradient Descent(43/49): loss=0.36481398870044796\n",
      "Gradient Descent(44/49): loss=0.3647808462368712\n",
      "Gradient Descent(45/49): loss=0.36475015204071537\n",
      "Gradient Descent(46/49): loss=0.36472167000935096\n",
      "Gradient Descent(47/49): loss=0.3646951888483905\n",
      "Gradient Descent(48/49): loss=0.36467051940177164\n",
      "Gradient Descent(49/49): loss=0.3646474922715704\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3992321678573436\n",
      "Gradient Descent(2/49): loss=0.3910346222844262\n",
      "Gradient Descent(3/49): loss=0.3872198200371235\n",
      "Gradient Descent(4/49): loss=0.38411391444423393\n",
      "Gradient Descent(5/49): loss=0.38146276377976035\n",
      "Gradient Descent(6/49): loss=0.3791871649899688\n",
      "Gradient Descent(7/49): loss=0.3772295152537355\n",
      "Gradient Descent(8/49): loss=0.37554266352835575\n",
      "Gradient Descent(9/49): loss=0.37408715426554995\n",
      "Gradient Descent(10/49): loss=0.37282968099590535\n",
      "Gradient Descent(11/49): loss=0.37174199458872187\n",
      "Gradient Descent(12/49): loss=0.3708000626008247\n",
      "Gradient Descent(13/49): loss=0.3699833945807801\n",
      "Gradient Descent(14/49): loss=0.3692744891639674\n",
      "Gradient Descent(15/49): loss=0.368658375814679\n",
      "Gradient Descent(16/49): loss=0.3681222323681663\n",
      "Gradient Descent(17/49): loss=0.36765506428582706\n",
      "Gradient Descent(18/49): loss=0.36724743465756354\n",
      "Gradient Descent(19/49): loss=0.3668912362197867\n",
      "Gradient Descent(20/49): loss=0.3665794983417618\n",
      "Gradient Descent(21/49): loss=0.36630622324006396\n",
      "Gradient Descent(22/49): loss=0.3660662467131983\n",
      "Gradient Descent(23/49): loss=0.3658551195132006\n",
      "Gradient Descent(24/49): loss=0.3656690061356838\n",
      "Gradient Descent(25/49): loss=0.3655045983491873\n",
      "Gradient Descent(26/49): loss=0.36535904122506124\n",
      "Gradient Descent(27/49): loss=0.3652298697906137\n",
      "Gradient Descent(28/49): loss=0.36511495472643674\n",
      "Gradient Descent(29/49): loss=0.36501245577589736\n",
      "Gradient Descent(30/49): loss=0.36492078174034487\n",
      "Gradient Descent(31/49): loss=0.36483855610525007\n",
      "Gradient Descent(32/49): loss=0.36476458748633755\n",
      "Gradient Descent(33/49): loss=0.3646978442056954\n",
      "Gradient Descent(34/49): loss=0.3646374324097698\n",
      "Gradient Descent(35/49): loss=0.3645825772272979\n",
      "Gradient Descent(36/49): loss=0.36453260653819824\n",
      "Gradient Descent(37/49): loss=0.36448693698637696\n",
      "Gradient Descent(38/49): loss=0.3644450619220829\n",
      "Gradient Descent(39/49): loss=0.3644065410043125\n",
      "Gradient Descent(40/49): loss=0.3643709912320431\n",
      "Gradient Descent(41/49): loss=0.3643380792057753\n",
      "Gradient Descent(42/49): loss=0.36430751444882753\n",
      "Gradient Descent(43/49): loss=0.3642790436417722\n",
      "Gradient Descent(44/49): loss=0.36425244564392056\n",
      "Gradient Descent(45/49): loss=0.36422752719335905\n",
      "Gradient Descent(46/49): loss=0.36420411919215084\n",
      "Gradient Descent(47/49): loss=0.3641820734962798\n",
      "Gradient Descent(48/49): loss=0.36416126014107153\n",
      "Gradient Descent(49/49): loss=0.3641415649424055\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3991345954455003\n",
      "Gradient Descent(2/49): loss=0.3909111185372143\n",
      "Gradient Descent(3/49): loss=0.3870568683271249\n",
      "Gradient Descent(4/49): loss=0.3839115680718254\n",
      "Gradient Descent(5/49): loss=0.3812237617203345\n",
      "Gradient Descent(6/49): loss=0.37891460241999086\n",
      "Gradient Descent(7/49): loss=0.3769262847675106\n",
      "Gradient Descent(8/49): loss=0.3752113719863741\n",
      "Gradient Descent(9/49): loss=0.3737301506663714\n",
      "Gradient Descent(10/49): loss=0.3724490987362007\n",
      "Gradient Descent(11/49): loss=0.3713397862425524\n",
      "Gradient Descent(12/49): loss=0.3703780250859506\n",
      "Gradient Descent(13/49): loss=0.369543186899316\n",
      "Gradient Descent(14/49): loss=0.3688176451708655\n",
      "Gradient Descent(15/49): loss=0.3681863138757637\n",
      "Gradient Descent(16/49): loss=0.3676362631556791\n",
      "Gradient Descent(17/49): loss=0.36715639749498513\n",
      "Gradient Descent(18/49): loss=0.3667371851108535\n",
      "Gradient Descent(19/49): loss=0.3663704296206658\n",
      "Gradient Descent(20/49): loss=0.3660490768116613\n",
      "Gradient Descent(21/49): loss=0.36576705069605964\n",
      "Gradient Descent(22/49): loss=0.3655191141002646\n",
      "Gradient Descent(23/49): loss=0.3653007498823548\n",
      "Gradient Descent(24/49): loss=0.3651080595494911\n",
      "Gradient Descent(25/49): loss=0.36493767659377585\n",
      "Gradient Descent(26/49): loss=0.36478669230964045\n",
      "Gradient Descent(27/49): loss=0.3646525922193872\n",
      "Gradient Descent(28/49): loss=0.364533201532469\n",
      "Gradient Descent(29/49): loss=0.3644266383111831\n",
      "Gradient Descent(30/49): loss=0.3643312732206251\n",
      "Gradient Descent(31/49): loss=0.36424569491181935\n",
      "Gradient Descent(32/49): loss=0.3641686802301382\n",
      "Gradient Descent(33/49): loss=0.3640991685613879\n",
      "Gradient Descent(34/49): loss=0.3640362397292735\n",
      "Gradient Descent(35/49): loss=0.36397909494356756\n",
      "Gradient Descent(36/49): loss=0.36392704037083495\n",
      "Gradient Descent(37/49): loss=0.3638794729611259\n",
      "Gradient Descent(38/49): loss=0.3638358682164293\n",
      "Gradient Descent(39/49): loss=0.36379576963130844\n",
      "Gradient Descent(40/49): loss=0.36375877957423525\n",
      "Gradient Descent(41/49): loss=0.3637245514107088\n",
      "Gradient Descent(42/49): loss=0.3636927826971051\n",
      "Gradient Descent(43/49): loss=0.36366320929809004\n",
      "Gradient Descent(44/49): loss=0.36363560030090025\n",
      "Gradient Descent(45/49): loss=0.3636097536173777\n",
      "Gradient Descent(46/49): loss=0.3635854921797424\n",
      "Gradient Descent(47/49): loss=0.3635626606490682\n",
      "Gradient Descent(48/49): loss=0.3635411225665921\n",
      "Gradient Descent(49/49): loss=0.36352075788759364\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39944708338428137\n",
      "Gradient Descent(2/49): loss=0.39131440605857043\n",
      "Gradient Descent(3/49): loss=0.38748189928826654\n",
      "Gradient Descent(4/49): loss=0.3843529688298381\n",
      "Gradient Descent(5/49): loss=0.38167749487759906\n",
      "Gradient Descent(6/49): loss=0.3793765533924944\n",
      "Gradient Descent(7/49): loss=0.37739299585178737\n",
      "Gradient Descent(8/49): loss=0.375680171470774\n",
      "Gradient Descent(9/49): loss=0.37419905029327033\n",
      "Gradient Descent(10/49): loss=0.3729166497550714\n",
      "Gradient Descent(11/49): loss=0.37180495088708615\n",
      "Gradient Descent(12/49): loss=0.3708400745677969\n",
      "Gradient Descent(13/49): loss=0.37000162375620377\n",
      "Gradient Descent(14/49): loss=0.36927214500523936\n",
      "Gradient Descent(15/49): loss=0.3686366817728358\n",
      "Gradient Descent(16/49): loss=0.36808240101191736\n",
      "Gradient Descent(17/49): loss=0.36759827941746637\n",
      "Gradient Descent(18/49): loss=0.36717483879720153\n",
      "Gradient Descent(19/49): loss=0.36680392219250707\n",
      "Gradient Descent(20/49): loss=0.36647850398608744\n",
      "Gradient Descent(21/49): loss=0.3661925284770889\n",
      "Gradient Descent(22/49): loss=0.36594077238683714\n",
      "Gradient Descent(23/49): loss=0.3657187275444384\n",
      "Gradient Descent(24/49): loss=0.36552250063644065\n",
      "Gradient Descent(25/49): loss=0.3653487274213274\n",
      "Gradient Descent(26/49): loss=0.3651944992324448\n",
      "Gradient Descent(27/49): loss=0.3650572999408749\n",
      "Gradient Descent(28/49): loss=0.3649349518374182\n",
      "Gradient Descent(29/49): loss=0.3648255691316813\n",
      "Gradient Descent(30/49): loss=0.36472751796538055\n",
      "Gradient Descent(31/49): loss=0.36463938200355245\n",
      "Gradient Descent(32/49): loss=0.3645599328072061\n",
      "Gradient Descent(33/49): loss=0.36448810430869677\n",
      "Gradient Descent(34/49): loss=0.3644229708105212\n",
      "Gradient Descent(35/49): loss=0.3643637280123786\n",
      "Gradient Descent(36/49): loss=0.3643096766427402\n",
      "Gradient Descent(37/49): loss=0.3642602083318513\n",
      "Gradient Descent(38/49): loss=0.3642147934147772\n",
      "Gradient Descent(39/49): loss=0.3641729703971944\n",
      "Gradient Descent(40/49): loss=0.3641343368542906\n",
      "Gradient Descent(41/49): loss=0.36409854156535537\n",
      "Gradient Descent(42/49): loss=0.36406527771423747\n",
      "Gradient Descent(43/49): loss=0.36403427700949487\n",
      "Gradient Descent(44/49): loss=0.36400530459836183\n",
      "Gradient Descent(45/49): loss=0.36397815466608513\n",
      "Gradient Descent(46/49): loss=0.3639526466271629\n",
      "Gradient Descent(47/49): loss=0.36392862182789665\n",
      "Gradient Descent(48/49): loss=0.36390594069075005\n",
      "Gradient Descent(49/49): loss=0.36388448024055303\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39785903688803087\n",
      "Gradient Descent(2/49): loss=0.38913389139309573\n",
      "Gradient Descent(3/49): loss=0.38546184559954777\n",
      "Gradient Descent(4/49): loss=0.3826498614665504\n",
      "Gradient Descent(5/49): loss=0.3803389177803099\n",
      "Gradient Descent(6/49): loss=0.37840279344091443\n",
      "Gradient Descent(7/49): loss=0.3767593076863121\n",
      "Gradient Descent(8/49): loss=0.3753495522894082\n",
      "Gradient Descent(9/49): loss=0.3741300309335449\n",
      "Gradient Descent(10/49): loss=0.37306789863010514\n",
      "Gradient Descent(11/49): loss=0.3721378228821786\n",
      "Gradient Descent(12/49): loss=0.3713198643719462\n",
      "Gradient Descent(13/49): loss=0.3705980261134388\n",
      "Gradient Descent(14/49): loss=0.36995924808402253\n",
      "Gradient Descent(15/49): loss=0.3693927017124029\n",
      "Gradient Descent(16/49): loss=0.368889287873471\n",
      "Gradient Descent(17/49): loss=0.36844127412731364\n",
      "Gradient Descent(18/49): loss=0.36804202808009234\n",
      "Gradient Descent(19/49): loss=0.36768581777200493\n",
      "Gradient Descent(20/49): loss=0.3673676593519968\n",
      "Gradient Descent(21/49): loss=0.36708319856260047\n",
      "Gradient Descent(22/49): loss=0.3668286167685464\n",
      "Gradient Descent(23/49): loss=0.3666005551040699\n",
      "Gradient Descent(24/49): loss=0.36639605223970473\n",
      "Gradient Descent(25/49): loss=0.3662124925813662\n",
      "Gradient Descent(26/49): loss=0.3660475626136474\n",
      "Gradient Descent(27/49): loss=0.36589921371977446\n",
      "Gradient Descent(28/49): loss=0.3657656302425022\n",
      "Gradient Descent(29/49): loss=0.3656452018537293\n",
      "Gradient Descent(30/49): loss=0.36553649951639805\n",
      "Gradient Descent(31/49): loss=0.36543825447770867\n",
      "Gradient Descent(32/49): loss=0.3653493398463863\n",
      "Gradient Descent(33/49): loss=0.3652687543912875\n",
      "Gradient Descent(34/49): loss=0.36519560826261555\n",
      "Gradient Descent(35/49): loss=0.3651291103863357\n",
      "Gradient Descent(36/49): loss=0.3650685573211\n",
      "Gradient Descent(37/49): loss=0.3650133233979248\n",
      "Gradient Descent(38/49): loss=0.36496285198801065\n",
      "Gradient Descent(39/49): loss=0.3649166477648221\n",
      "Gradient Descent(40/49): loss=0.36487426984388194\n",
      "Gradient Descent(41/49): loss=0.3648353256983901\n",
      "Gradient Descent(42/49): loss=0.36479946576128836\n",
      "Gradient Descent(43/49): loss=0.3647663786351642\n",
      "Gradient Descent(44/49): loss=0.36473578684070873\n",
      "Gradient Descent(45/49): loss=0.36470744304257374\n",
      "Gradient Descent(46/49): loss=0.3646811266985658\n",
      "Gradient Descent(47/49): loss=0.3646566410843549\n",
      "Gradient Descent(48/49): loss=0.36463381065134737\n",
      "Gradient Descent(49/49): loss=0.3646124786802065\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3981652453541604\n",
      "Gradient Descent(2/49): loss=0.39069993120216906\n",
      "Gradient Descent(3/49): loss=0.386880211670858\n",
      "Gradient Descent(4/49): loss=0.3837332834800924\n",
      "Gradient Descent(5/49): loss=0.3810563873647419\n",
      "Gradient Descent(6/49): loss=0.37876924936564993\n",
      "Gradient Descent(7/49): loss=0.37681084728248726\n",
      "Gradient Descent(8/49): loss=0.37513116531979346\n",
      "Gradient Descent(9/49): loss=0.3736884804813194\n",
      "Gradient Descent(10/49): loss=0.3724477155101129\n",
      "Gradient Descent(11/49): loss=0.3713792515390112\n",
      "Gradient Descent(12/49): loss=0.37045800653555505\n",
      "Gradient Descent(13/49): loss=0.36966269438945093\n",
      "Gradient Descent(14/49): loss=0.36897521865130245\n",
      "Gradient Descent(15/49): loss=0.36838017158225944\n",
      "Gradient Descent(16/49): loss=0.3678644176306435\n",
      "Gradient Descent(17/49): loss=0.3674167455305808\n",
      "Gradient Descent(18/49): loss=0.3670275766637682\n",
      "Gradient Descent(19/49): loss=0.36668871984146006\n",
      "Gradient Descent(20/49): loss=0.36639316457688337\n",
      "Gradient Descent(21/49): loss=0.36613490640690916\n",
      "Gradient Descent(22/49): loss=0.36590879899690093\n",
      "Gradient Descent(23/49): loss=0.3657104286996836\n",
      "Gradient Descent(24/49): loss=0.36553600799273706\n",
      "Gradient Descent(25/49): loss=0.3653822848271836\n",
      "Gradient Descent(26/49): loss=0.36524646541828076\n",
      "Gradient Descent(27/49): loss=0.3651261484131825\n",
      "Gradient Descent(28/49): loss=0.3650192687057036\n",
      "Gradient Descent(29/49): loss=0.3649240494437139\n",
      "Gradient Descent(30/49): loss=0.3648389610036669\n",
      "Gradient Descent(31/49): loss=0.3647626858973142\n",
      "Gradient Descent(32/49): loss=0.3646940887348602\n",
      "Gradient Descent(33/49): loss=0.36463219050221124\n",
      "Gradient Descent(34/49): loss=0.36457614652207215\n",
      "Gradient Descent(35/49): loss=0.36452522756307487\n",
      "Gradient Descent(36/49): loss=0.3644788036408372\n",
      "Gradient Descent(37/49): loss=0.36443633012229854\n",
      "Gradient Descent(38/49): loss=0.36439733580181993\n",
      "Gradient Descent(39/49): loss=0.36436141266604144\n",
      "Gradient Descent(40/49): loss=0.3643282071057224\n",
      "Gradient Descent(41/49): loss=0.3642974123678668\n",
      "Gradient Descent(42/49): loss=0.3642687620713276\n",
      "Gradient Descent(43/49): loss=0.3642420246345739\n",
      "Gradient Descent(44/49): loss=0.36421699848605366\n",
      "Gradient Descent(45/49): loss=0.3641935079461694\n",
      "Gradient Descent(46/49): loss=0.364171399685763\n",
      "Gradient Descent(47/49): loss=0.3641505396795868\n",
      "Gradient Descent(48/49): loss=0.36413081058486185\n",
      "Gradient Descent(49/49): loss=0.3641121094849698\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39806675082259496\n",
      "Gradient Descent(2/49): loss=0.3905738092830598\n",
      "Gradient Descent(3/49): loss=0.38671331770060563\n",
      "Gradient Descent(4/49): loss=0.3835259349263283\n",
      "Gradient Descent(5/49): loss=0.3808116345477529\n",
      "Gradient Descent(6/49): loss=0.3784903896804368\n",
      "Gradient Descent(7/49): loss=0.37650091155631094\n",
      "Gradient Descent(8/49): loss=0.3747928680290517\n",
      "Gradient Descent(9/49): loss=0.37332426236206706\n",
      "Gradient Descent(10/49): loss=0.3720597913584284\n",
      "Gradient Descent(11/49): loss=0.37096964678127176\n",
      "Gradient Descent(12/49): loss=0.370028582847935\n",
      "Gradient Descent(13/49): loss=0.3692151675347568\n",
      "Gradient Descent(14/49): loss=0.3685111713564964\n",
      "Gradient Descent(15/49): loss=0.3679010634347432\n",
      "Gradient Descent(16/49): loss=0.36737159325487445\n",
      "Gradient Descent(17/49): loss=0.36691144180342183\n",
      "Gradient Descent(18/49): loss=0.366510929399016\n",
      "Gradient Descent(19/49): loss=0.36616177016951634\n",
      "Gradient Descent(20/49): loss=0.3658568651229454\n",
      "Gradient Descent(21/49): loss=0.36559012730106794\n",
      "Gradient Descent(22/49): loss=0.36535633371246073\n",
      "Gradient Descent(23/49): loss=0.36515099969899717\n",
      "Gradient Descent(24/49): loss=0.3649702721545976\n",
      "Gradient Descent(25/49): loss=0.3648108386310973\n",
      "Gradient Descent(26/49): loss=0.3646698498654969\n",
      "Gradient Descent(27/49): loss=0.3645448536702202\n",
      "Gradient Descent(28/49): loss=0.3644337384621404\n",
      "Gradient Descent(29/49): loss=0.3643346849815685\n",
      "Gradient Descent(30/49): loss=0.3642461249805168\n",
      "Gradient Descent(31/49): loss=0.3641667058492348\n",
      "Gradient Descent(32/49): loss=0.36409526030836226\n",
      "Gradient Descent(33/49): loss=0.3640307804266595\n",
      "Gradient Descent(34/49): loss=0.36397239533568176\n",
      "Gradient Descent(35/49): loss=0.36391935210661436\n",
      "Gradient Descent(36/49): loss=0.36387099933372263\n",
      "Gradient Descent(37/49): loss=0.3638267730359273\n",
      "Gradient Descent(38/49): loss=0.36378618454486444\n",
      "Gradient Descent(39/49): loss=0.36374881009606413\n",
      "Gradient Descent(40/49): loss=0.36371428188094573\n",
      "Gradient Descent(41/49): loss=0.3636822803522894\n",
      "Gradient Descent(42/49): loss=0.36365252760566036\n",
      "Gradient Descent(43/49): loss=0.3636247816847019\n",
      "Gradient Descent(44/49): loss=0.36359883167995005\n",
      "Gradient Descent(45/49): loss=0.36357449350940046\n",
      "Gradient Descent(46/49): loss=0.3635516062849585\n",
      "Gradient Descent(47/49): loss=0.3635300291825024\n",
      "Gradient Descent(48/49): loss=0.36350963874495135\n",
      "Gradient Descent(49/49): loss=0.3634903265577096\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39838911680242073\n",
      "Gradient Descent(2/49): loss=0.39097952900851624\n",
      "Gradient Descent(3/49): loss=0.3871401513790545\n",
      "Gradient Descent(4/49): loss=0.3839693024023305\n",
      "Gradient Descent(5/49): loss=0.38126713343810037\n",
      "Gradient Descent(6/49): loss=0.37895367934281565\n",
      "Gradient Descent(7/49): loss=0.3769684539176481\n",
      "Gradient Descent(8/49): loss=0.37526198764757995\n",
      "Gradient Descent(9/49): loss=0.37379300283698413\n",
      "Gradient Descent(10/49): loss=0.372526750497388\n",
      "Gradient Descent(11/49): loss=0.3714338377727074\n",
      "Gradient Descent(12/49): loss=0.37048932725889716\n",
      "Gradient Descent(13/49): loss=0.36967201532910055\n",
      "Gradient Descent(14/49): loss=0.3689638416549425\n",
      "Gradient Descent(15/49): loss=0.3683494006416054\n",
      "Gradient Descent(16/49): loss=0.36781553444010845\n",
      "Gradient Descent(17/49): loss=0.36735099232114843\n",
      "Gradient Descent(18/49): loss=0.36694614455784336\n",
      "Gradient Descent(19/49): loss=0.3665927413782976\n",
      "Gradient Descent(20/49): loss=0.36628370937139826\n",
      "Gradient Descent(21/49): loss=0.36601297914521413\n",
      "Gradient Descent(22/49): loss=0.36577533915632504\n",
      "Gradient Descent(23/49): loss=0.3655663115226432\n",
      "Gradient Descent(24/49): loss=0.36538204635293564\n",
      "Gradient Descent(25/49): loss=0.36521923171095644\n",
      "Gradient Descent(26/49): loss=0.36507501680928506\n",
      "Gradient Descent(27/49): loss=0.364946946419454\n",
      "Gradient Descent(28/49): loss=0.36483290480763825\n",
      "Gradient Descent(29/49): loss=0.3647310677723324\n",
      "Gradient Descent(30/49): loss=0.36463986158247197\n",
      "Gradient Descent(31/49): loss=0.364557927799668\n",
      "Gradient Descent(32/49): loss=0.364484093123228\n",
      "Gradient Descent(33/49): loss=0.36441734352673744\n",
      "Gradient Descent(34/49): loss=0.3643568020644848\n",
      "Gradient Descent(35/49): loss=0.3643017098184022\n",
      "Gradient Descent(36/49): loss=0.3642514095343075\n",
      "Gradient Descent(37/49): loss=0.3642053315624189\n",
      "Gradient Descent(38/49): loss=0.36416298177327405\n",
      "Gradient Descent(39/49): loss=0.3641239311679238\n",
      "Gradient Descent(40/49): loss=0.36408780694190146\n",
      "Gradient Descent(41/49): loss=0.36405428479709057\n",
      "Gradient Descent(42/49): loss=0.364023082325154\n",
      "Gradient Descent(43/49): loss=0.3639939533114039\n",
      "Gradient Descent(44/49): loss=0.3639666828295493\n",
      "Gradient Descent(45/49): loss=0.3639410830161908\n",
      "Gradient Descent(46/49): loss=0.3639169894297057\n",
      "Gradient Descent(47/49): loss=0.36389425791167784\n",
      "Gradient Descent(48/49): loss=0.3638727618805974\n",
      "Gradient Descent(49/49): loss=0.363852389997477\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39668301723834587\n",
      "Gradient Descent(2/49): loss=0.3887878725338197\n",
      "Gradient Descent(3/49): loss=0.38514275657115665\n",
      "Gradient Descent(4/49): loss=0.3823103112840302\n",
      "Gradient Descent(5/49): loss=0.3799882805535568\n",
      "Gradient Descent(6/49): loss=0.37804950501451984\n",
      "Gradient Descent(7/49): loss=0.3764089138981682\n",
      "Gradient Descent(8/49): loss=0.3750057048933411\n",
      "Gradient Descent(9/49): loss=0.37379520670707017\n",
      "Gradient Descent(10/49): loss=0.3727438208926233\n",
      "Gradient Descent(11/49): loss=0.37182570280901667\n",
      "Gradient Descent(12/49): loss=0.3710205409690514\n",
      "Gradient Descent(13/49): loss=0.37031205090892655\n",
      "Gradient Descent(14/49): loss=0.3696869403133908\n",
      "Gradient Descent(15/49): loss=0.3691341879477952\n",
      "Gradient Descent(16/49): loss=0.3686445333564608\n",
      "Gradient Descent(17/49): loss=0.36821010939346444\n",
      "Gradient Descent(18/49): loss=0.3678241725264767\n",
      "Gradient Descent(19/49): loss=0.3674809008522998\n",
      "Gradient Descent(20/49): loss=0.367175239642822\n",
      "Gradient Descent(21/49): loss=0.36690278077570093\n",
      "Gradient Descent(22/49): loss=0.3666596667452721\n",
      "Gradient Descent(23/49): loss=0.36644251284574536\n",
      "Gradient Descent(24/49): loss=0.36624834306137516\n",
      "Gradient Descent(25/49): loss=0.3660745365090067\n",
      "Gradient Descent(26/49): loss=0.3659187821691373\n",
      "Gradient Descent(27/49): loss=0.3657790402521485\n",
      "Gradient Descent(28/49): loss=0.3656535089691252\n",
      "Gradient Descent(29/49): loss=0.3655405957729604\n",
      "Gradient Descent(30/49): loss=0.3654388923460146\n",
      "Gradient Descent(31/49): loss=0.3653471527626506\n",
      "Gradient Descent(32/49): loss=0.36526427436671044\n",
      "Gradient Descent(33/49): loss=0.36518928098770287\n",
      "Gradient Descent(34/49): loss=0.36512130818339084\n",
      "Gradient Descent(35/49): loss=0.3650595902462776\n",
      "Gradient Descent(36/49): loss=0.3650034487510304\n",
      "Gradient Descent(37/49): loss=0.36495228245183625\n",
      "Gradient Descent(38/49): loss=0.36490555836492694\n",
      "Gradient Descent(39/49): loss=0.36486280389336656\n",
      "Gradient Descent(40/49): loss=0.3648235998696071\n",
      "Gradient Descent(41/49): loss=0.3647875744069957\n",
      "Gradient Descent(42/49): loss=0.36475439746486604\n",
      "Gradient Descent(43/49): loss=0.3647237760434603\n",
      "Gradient Descent(44/49): loss=0.3646954499350175\n",
      "Gradient Descent(45/49): loss=0.3646691879661521\n",
      "Gradient Descent(46/49): loss=0.36464478467433503\n",
      "Gradient Descent(47/49): loss=0.3646220573680352\n",
      "Gradient Descent(48/49): loss=0.36460084352599265\n",
      "Gradient Descent(49/49): loss=0.3645809984963127\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3972348011464289\n",
      "Gradient Descent(2/49): loss=0.3903928536587442\n",
      "Gradient Descent(3/49): loss=0.3865475701875307\n",
      "Gradient Descent(4/49): loss=0.3833598159181763\n",
      "Gradient Descent(5/49): loss=0.3806593468769288\n",
      "Gradient Descent(6/49): loss=0.3783628341927907\n",
      "Gradient Descent(7/49): loss=0.3764055987373626\n",
      "Gradient Descent(8/49): loss=0.3747346971306366\n",
      "Gradient Descent(9/49): loss=0.37330612037314453\n",
      "Gradient Descent(10/49): loss=0.37208302586731734\n",
      "Gradient Descent(11/49): loss=0.37103444478874215\n",
      "Gradient Descent(12/49): loss=0.3701342733577379\n",
      "Gradient Descent(13/49): loss=0.3693604615722627\n",
      "Gradient Descent(14/49): loss=0.36869435087531294\n",
      "Gradient Descent(15/49): loss=0.3681201287510367\n",
      "Gradient Descent(16/49): loss=0.3676243770222023\n",
      "Gradient Descent(17/49): loss=0.3671956961237314\n",
      "Gradient Descent(18/49): loss=0.36682439146391294\n",
      "Gradient Descent(19/49): loss=0.3665022108242188\n",
      "Gradient Descent(20/49): loss=0.3662221239182526\n",
      "Gradient Descent(21/49): loss=0.3659781369194207\n",
      "Gradient Descent(22/49): loss=0.3657651360981366\n",
      "Gradient Descent(23/49): loss=0.36557875576826876\n",
      "Gradient Descent(24/49): loss=0.3654152665912698\n",
      "Gradient Descent(25/49): loss=0.3652714809712\n",
      "Gradient Descent(26/49): loss=0.36514467282965646\n",
      "Gradient Descent(27/49): loss=0.3650325095031605\n",
      "Gradient Descent(28/49): loss=0.36493299387746947\n",
      "Gradient Descent(29/49): loss=0.36484441517963095\n",
      "Gradient Descent(30/49): loss=0.36476530710196486\n",
      "Gradient Descent(31/49): loss=0.36469441214247095\n",
      "Gradient Descent(32/49): loss=0.3646306512213248\n",
      "Gradient Descent(33/49): loss=0.3645730977794443\n",
      "Gradient Descent(34/49): loss=0.36452095568765536\n",
      "Gradient Descent(35/49): loss=0.3644735403978813\n",
      "Gradient Descent(36/49): loss=0.364430262854342\n",
      "Gradient Descent(37/49): loss=0.3643906157557182\n",
      "Gradient Descent(38/49): loss=0.36435416182085123\n",
      "Gradient Descent(39/49): loss=0.36432052376264284\n",
      "Gradient Descent(40/49): loss=0.3642893757189288\n",
      "Gradient Descent(41/49): loss=0.36426043592650037\n",
      "Gradient Descent(42/49): loss=0.36423346045616\n",
      "Gradient Descent(43/49): loss=0.36420823785365997\n",
      "Gradient Descent(44/49): loss=0.3641845845542584\n",
      "Gradient Descent(45/49): loss=0.3641623409581193\n",
      "Gradient Descent(46/49): loss=0.3641413680703521\n",
      "Gradient Descent(47/49): loss=0.36412154462360374\n",
      "Gradient Descent(48/49): loss=0.3641027646131448\n",
      "Gradient Descent(49/49): loss=0.36408493518463314\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3971355235812308\n",
      "Gradient Descent(2/49): loss=0.3902639298022308\n",
      "Gradient Descent(3/49): loss=0.386376689634875\n",
      "Gradient Descent(4/49): loss=0.383147495477257\n",
      "Gradient Descent(5/49): loss=0.3804089164415096\n",
      "Gradient Descent(6/49): loss=0.3780777801771747\n",
      "Gradient Descent(7/49): loss=0.37608908516959355\n",
      "Gradient Descent(8/49): loss=0.37438954456215\n",
      "Gradient Descent(9/49): loss=0.3729348610523248\n",
      "Gradient Descent(10/49): loss=0.37168795600446985\n",
      "Gradient Descent(11/49): loss=0.37061766241504784\n",
      "Gradient Descent(12/49): loss=0.36969770420199005\n",
      "Gradient Descent(13/49): loss=0.3689058769587902\n",
      "Gradient Descent(14/49): loss=0.3682233807572959\n",
      "Gradient Descent(15/49): loss=0.367634271930103\n",
      "Gradient Descent(16/49): loss=0.36712500980485224\n",
      "Gradient Descent(17/49): loss=0.3666840801337605\n",
      "Gradient Descent(18/49): loss=0.3663016809971236\n",
      "Gradient Descent(19/49): loss=0.3659694599316733\n",
      "Gradient Descent(20/49): loss=0.3656802932893994\n",
      "Gradient Descent(21/49): loss=0.365428100574516\n",
      "Gradient Descent(22/49): loss=0.3652076878696085\n",
      "Gradient Descent(23/49): loss=0.36501461553973086\n",
      "Gradient Descent(24/49): loss=0.3648450862624452\n",
      "Gradient Descent(25/49): loss=0.36469585012192374\n",
      "Gradient Descent(26/49): loss=0.36456412406331185\n",
      "Gradient Descent(27/49): loss=0.36444752345758125\n",
      "Gradient Descent(28/49): loss=0.36434400389854954\n",
      "Gradient Descent(29/49): loss=0.36425181165914633\n",
      "Gradient Descent(30/49): loss=0.3641694414862509\n",
      "Gradient Descent(31/49): loss=0.36409560062262575\n",
      "Gradient Descent(32/49): loss=0.3640291781185949\n",
      "Gradient Descent(33/49): loss=0.3639692186415281\n",
      "Gradient Descent(34/49): loss=0.36391490011298355\n",
      "Gradient Descent(35/49): loss=0.3638655146056251\n",
      "Gradient Descent(36/49): loss=0.3638204520181025\n",
      "Gradient Descent(37/49): loss=0.3637791861186676\n",
      "Gradient Descent(38/49): loss=0.3637412626096256\n",
      "Gradient Descent(39/49): loss=0.3637062889166082\n",
      "Gradient Descent(40/49): loss=0.3636739254506231\n",
      "Gradient Descent(41/49): loss=0.3636438781281378\n",
      "Gradient Descent(42/49): loss=0.36361589196612776\n",
      "Gradient Descent(43/49): loss=0.3635897455959518\n",
      "Gradient Descent(44/49): loss=0.36356524656281997\n",
      "Gradient Descent(45/49): loss=0.36354222729712454\n",
      "Gradient Descent(46/49): loss=0.3635205416605185\n",
      "Gradient Descent(47/49): loss=0.363500061983782\n",
      "Gradient Descent(48/49): loss=0.3634806765255979\n",
      "Gradient Descent(49/49): loss=0.3634622872916561\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39746775496366105\n",
      "Gradient Descent(2/49): loss=0.39067150628981717\n",
      "Gradient Descent(3/49): loss=0.38680525849450237\n",
      "Gradient Descent(4/49): loss=0.3835927736805778\n",
      "Gradient Descent(5/49): loss=0.38086607962722496\n",
      "Gradient Descent(6/49): loss=0.37854228000752105\n",
      "Gradient Descent(7/49): loss=0.376557321622083\n",
      "Gradient Descent(8/49): loss=0.374858850466396\n",
      "Gradient Descent(9/49): loss=0.37340331903147334\n",
      "Gradient Descent(10/49): loss=0.3721542144933654\n",
      "Gradient Descent(11/49): loss=0.37108078769564534\n",
      "Gradient Descent(12/49): loss=0.37015706912966206\n",
      "Gradient Descent(13/49): loss=0.3693610790959116\n",
      "Gradient Descent(14/49): loss=0.36867418243202915\n",
      "Gradient Descent(15/49): loss=0.3680805562475457\n",
      "Gradient Descent(16/49): loss=0.3675667481959463\n",
      "Gradient Descent(17/49): loss=0.3671213082672475\n",
      "Gradient Descent(18/49): loss=0.3667344807898106\n",
      "Gradient Descent(19/49): loss=0.3663979460402861\n",
      "Gradient Descent(20/49): loss=0.36610460292399605\n",
      "Gradient Descent(21/49): loss=0.3658483857949134\n",
      "Gradient Descent(22/49): loss=0.3656241097533657\n",
      "Gradient Descent(23/49): loss=0.36542733977164443\n",
      "Gradient Descent(24/49): loss=0.3652542798111342\n",
      "Gradient Descent(25/49): loss=0.36510167875263805\n",
      "Gradient Descent(26/49): loss=0.3649667504970536\n",
      "Gradient Descent(27/49): loss=0.36484710603152753\n",
      "Gradient Descent(28/49): loss=0.3647406956161636\n",
      "Gradient Descent(29/49): loss=0.36464575954344797\n",
      "Gradient Descent(30/49): loss=0.36456078616873866\n",
      "Gradient Descent(31/49): loss=0.36448447611489065\n",
      "Gradient Descent(32/49): loss=0.36441571172488846\n",
      "Gradient Descent(33/49): loss=0.36435353097927875\n",
      "Gradient Descent(34/49): loss=0.3642971052150865\n",
      "Gradient Descent(35/49): loss=0.36424572008372136\n",
      "Gradient Descent(36/49): loss=0.36419875927033024\n",
      "Gradient Descent(37/49): loss=0.364155690568775\n",
      "Gradient Descent(38/49): loss=0.36411605396705216\n",
      "Gradient Descent(39/49): loss=0.3640794514493298\n",
      "Gradient Descent(40/49): loss=0.3640455382643138\n",
      "Gradient Descent(41/49): loss=0.36401401544661816\n",
      "Gradient Descent(42/49): loss=0.3639846234092176\n",
      "Gradient Descent(43/49): loss=0.36395713645176664\n",
      "Gradient Descent(44/49): loss=0.3639313580522985\n",
      "Gradient Descent(45/49): loss=0.36390711682917637\n",
      "Gradient Descent(46/49): loss=0.36388426307666255\n",
      "Gradient Descent(47/49): loss=0.36386266579153637\n",
      "Gradient Descent(48/49): loss=0.36384221012018736\n",
      "Gradient Descent(49/49): loss=0.36382279516585003\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39563942572166844\n",
      "Gradient Descent(2/49): loss=0.3884727009036838\n",
      "Gradient Descent(3/49): loss=0.3848321925057257\n",
      "Gradient Descent(4/49): loss=0.3819789316230929\n",
      "Gradient Descent(5/49): loss=0.37964722790906386\n",
      "Gradient Descent(6/49): loss=0.377707019665041\n",
      "Gradient Descent(7/49): loss=0.37607026035618385\n",
      "Gradient Descent(8/49): loss=0.3746743168233049\n",
      "Gradient Descent(9/49): loss=0.373473400340881\n",
      "Gradient Descent(10/49): loss=0.37243319769271294\n",
      "Gradient Descent(11/49): loss=0.3715273750710783\n",
      "Gradient Descent(12/49): loss=0.37073526123019473\n",
      "Gradient Descent(13/49): loss=0.3700402905291078\n",
      "Gradient Descent(14/49): loss=0.36942894176927477\n",
      "Gradient Descent(15/49): loss=0.36889000362658064\n",
      "Gradient Descent(16/49): loss=0.36841405720520054\n",
      "Gradient Descent(17/49): loss=0.3679931043832711\n",
      "Gradient Descent(18/49): loss=0.3676202951839865\n",
      "Gradient Descent(19/49): loss=0.3672897233151491\n",
      "Gradient Descent(20/49): loss=0.3669962693720353\n",
      "Gradient Descent(21/49): loss=0.3667354779634783\n",
      "Gradient Descent(22/49): loss=0.3665034594624332\n",
      "Gradient Descent(23/49): loss=0.36629681001324277\n",
      "Gradient Descent(24/49): loss=0.3661125453737725\n",
      "Gradient Descent(25/49): loss=0.36594804547191034\n",
      "Gradient Descent(26/49): loss=0.36580100743376576\n",
      "Gradient Descent(27/49): loss=0.3656694054392655\n",
      "Gradient Descent(28/49): loss=0.36555145617378754\n",
      "Gradient Descent(29/49): loss=0.3654455889336471\n",
      "Gradient Descent(30/49): loss=0.36535041964913967\n",
      "Gradient Descent(31/49): loss=0.3652647282382095\n",
      "Gradient Descent(32/49): loss=0.3651874388143673\n",
      "Gradient Descent(33/49): loss=0.3651176023560616\n",
      "Gradient Descent(34/49): loss=0.36505438150922687\n",
      "Gradient Descent(35/49): loss=0.3649970372455875\n",
      "Gradient Descent(36/49): loss=0.3649449171401208\n",
      "Gradient Descent(37/49): loss=0.36489744506443833\n",
      "Gradient Descent(38/49): loss=0.3648541121204977\n",
      "Gradient Descent(39/49): loss=0.3648144686622585\n",
      "Gradient Descent(40/49): loss=0.3647781172725852\n",
      "Gradient Descent(41/49): loss=0.36474470657952573\n",
      "Gradient Descent(42/49): loss=0.3647139258105858\n",
      "Gradient Descent(43/49): loss=0.36468549999615996\n",
      "Gradient Descent(44/49): loss=0.3646591857441687\n",
      "Gradient Descent(45/49): loss=0.3646347675174536\n",
      "Gradient Descent(46/49): loss=0.3646120543537729\n",
      "Gradient Descent(47/49): loss=0.3645908769755104\n",
      "Gradient Descent(48/49): loss=0.36457108524256976\n",
      "Gradient Descent(49/49): loss=0.3645525459075192\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3964408352341489\n",
      "Gradient Descent(2/49): loss=0.3901075731560177\n",
      "Gradient Descent(3/49): loss=0.3862210528507395\n",
      "Gradient Descent(4/49): loss=0.3829933298772071\n",
      "Gradient Descent(5/49): loss=0.3802714256603585\n",
      "Gradient Descent(6/49): loss=0.3779676046230926\n",
      "Gradient Descent(7/49): loss=0.37601334253785307\n",
      "Gradient Descent(8/49): loss=0.374352712432795\n",
      "Gradient Descent(9/49): loss=0.37293940638978096\n",
      "Gradient Descent(10/49): loss=0.37173482681140546\n",
      "Gradient Descent(11/49): loss=0.37070667849570627\n",
      "Gradient Descent(12/49): loss=0.3698278665203621\n",
      "Gradient Descent(13/49): loss=0.36907561063344246\n",
      "Gradient Descent(14/49): loss=0.36843072428543255\n",
      "Gradient Descent(15/49): loss=0.3678770231455933\n",
      "Gradient Descent(16/49): loss=0.3674008372248864\n",
      "Gradient Descent(17/49): loss=0.3669906067593968\n",
      "Gradient Descent(18/49): loss=0.3666365463011225\n",
      "Gradient Descent(19/49): loss=0.36633036466710217\n",
      "Gradient Descent(20/49): loss=0.3660650308514188\n",
      "Gradient Descent(21/49): loss=0.3658345779130227\n",
      "Gradient Descent(22/49): loss=0.3656339383530295\n",
      "Gradient Descent(23/49): loss=0.3654588056855985\n",
      "Gradient Descent(24/49): loss=0.36530551785785303\n",
      "Gradient Descent(25/49): loss=0.3651709589396085\n",
      "Gradient Descent(26/49): loss=0.3650524761229931\n",
      "Gradient Descent(27/49): loss=0.3649478095759217\n",
      "Gradient Descent(28/49): loss=0.36485503310537315\n",
      "Gradient Descent(29/49): loss=0.36477250392474914\n",
      "Gradient Descent(30/49): loss=0.36469882009858084\n",
      "Gradient Descent(31/49): loss=0.3646327844687131\n",
      "Gradient Descent(32/49): loss=0.36457337405776363\n",
      "Gradient Descent(33/49): loss=0.3645197141052475\n",
      "Gradient Descent(34/49): loss=0.36447105602497903\n",
      "Gradient Descent(35/49): loss=0.3644267586838212\n",
      "Gradient Descent(36/49): loss=0.36438627249530786\n",
      "Gradient Descent(37/49): loss=0.3643491259001405\n",
      "Gradient Descent(38/49): loss=0.3643149138715847\n",
      "Gradient Descent(39/49): loss=0.36428328813939986\n",
      "Gradient Descent(40/49): loss=0.3642539488728374\n",
      "Gradient Descent(41/49): loss=0.36422663760283414\n",
      "Gradient Descent(42/49): loss=0.3642011311969945\n",
      "Gradient Descent(43/49): loss=0.36417723672924607\n",
      "Gradient Descent(44/49): loss=0.36415478711000715\n",
      "Gradient Descent(45/49): loss=0.3641336373629852\n",
      "Gradient Descent(46/49): loss=0.36411366145191043\n",
      "Gradient Descent(47/49): loss=0.3640947495750798\n",
      "Gradient Descent(48/49): loss=0.36407680585793695\n",
      "Gradient Descent(49/49): loss=0.36405974638440364\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3963409137214078\n",
      "Gradient Descent(2/49): loss=0.3899756469475488\n",
      "Gradient Descent(3/49): loss=0.3860461245607387\n",
      "Gradient Descent(4/49): loss=0.3827760649704651\n",
      "Gradient Descent(5/49): loss=0.38001539003097407\n",
      "Gradient Descent(6/49): loss=0.37767645738788685\n",
      "Gradient Descent(7/49): loss=0.37569037574322706\n",
      "Gradient Descent(8/49): loss=0.37400085144253126\n",
      "Gradient Descent(9/49): loss=0.37256127488493307\n",
      "Gradient Descent(10/49): loss=0.3713328024174919\n",
      "Gradient Descent(11/49): loss=0.3702829314868523\n",
      "Gradient Descent(12/49): loss=0.3693843858097388\n",
      "Gradient Descent(13/49): loss=0.36861422173370456\n",
      "Gradient Descent(14/49): loss=0.367953102568666\n",
      "Gradient Descent(15/49): loss=0.3673847044864747\n",
      "Gradient Descent(16/49): loss=0.3668952272443222\n",
      "Gradient Descent(17/49): loss=0.36647298933613437\n",
      "Gradient Descent(18/49): loss=0.36610809168685904\n",
      "Gradient Descent(19/49): loss=0.36579213734868116\n",
      "Gradient Descent(20/49): loss=0.3655179971987245\n",
      "Gradient Descent(21/49): loss=0.36527961359872096\n",
      "Gradient Descent(22/49): loss=0.3650718355085993\n",
      "Gradient Descent(23/49): loss=0.36489027975360255\n",
      "Gradient Descent(24/49): loss=0.36473121410484544\n",
      "Gradient Descent(25/49): loss=0.3645914586025553\n",
      "Gradient Descent(26/49): loss=0.36446830217173903\n",
      "Gradient Descent(27/49): loss=0.36435943208353416\n",
      "Gradient Descent(28/49): loss=0.36426287422636217\n",
      "Gradient Descent(29/49): loss=0.36417694248790916\n",
      "Gradient Descent(30/49): loss=0.3641001958264802\n",
      "Gradient Descent(31/49): loss=0.3640314018397752\n",
      "Gradient Descent(32/49): loss=0.3639695058296156\n",
      "Gradient Descent(33/49): loss=0.3639136045197395\n",
      "Gradient Descent(34/49): loss=0.36386292371618667\n",
      "Gradient Descent(35/49): loss=0.36381679931061894\n",
      "Gradient Descent(36/49): loss=0.36377466111986934\n",
      "Gradient Descent(37/49): loss=0.36373601913313564\n",
      "Gradient Descent(38/49): loss=0.36370045180398625\n",
      "Gradient Descent(39/49): loss=0.3636675960797828\n",
      "Gradient Descent(40/49): loss=0.3636371389079087\n",
      "Gradient Descent(41/49): loss=0.3636088099977383\n",
      "Gradient Descent(42/49): loss=0.3635823756507166\n",
      "Gradient Descent(43/49): loss=0.36355763349923315\n",
      "Gradient Descent(44/49): loss=0.3635344080189609\n",
      "Gradient Descent(45/49): loss=0.36351254669965366\n",
      "Gradient Descent(46/49): loss=0.36349191677665144\n",
      "Gradient Descent(47/49): loss=0.36347240243996887\n",
      "Gradient Descent(48/49): loss=0.36345390245027126\n",
      "Gradient Descent(49/49): loss=0.3634363281015922\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3966829978680022\n",
      "Gradient Descent(2/49): loss=0.3903845374067263\n",
      "Gradient Descent(3/49): loss=0.38647636545520725\n",
      "Gradient Descent(4/49): loss=0.3832231924236276\n",
      "Gradient Descent(5/49): loss=0.3804741152513643\n",
      "Gradient Descent(6/49): loss=0.3781420429398282\n",
      "Gradient Descent(7/49): loss=0.37615917657566966\n",
      "Gradient Descent(8/49): loss=0.37447021934317215\n",
      "Gradient Descent(9/49): loss=0.3730293377000469\n",
      "Gradient Descent(10/49): loss=0.3717982626007135\n",
      "Gradient Descent(11/49): loss=0.37074491008553834\n",
      "Gradient Descent(12/49): loss=0.36984230757918335\n",
      "Gradient Descent(13/49): loss=0.3690677317858656\n",
      "Gradient Descent(14/49): loss=0.3684020059762256\n",
      "Gradient Descent(15/49): loss=0.36782892233341735\n",
      "Gradient Descent(16/49): loss=0.3673347644409099\n",
      "Gradient Descent(17/49): loss=0.36690791088893127\n",
      "Gradient Descent(18/49): loss=0.36653850509194724\n",
      "Gradient Descent(19/49): loss=0.36621817945895874\n",
      "Gradient Descent(20/49): loss=0.36593982439074274\n",
      "Gradient Descent(21/49): loss=0.3656973943950542\n",
      "Gradient Descent(22/49): loss=0.3654857450432634\n",
      "Gradient Descent(23/49): loss=0.36530049563149025\n",
      "Gradient Descent(24/49): loss=0.3651379133225961\n",
      "Gradient Descent(25/49): loss=0.3649948152820721\n",
      "Gradient Descent(26/49): loss=0.3648684859184822\n",
      "Gradient Descent(27/49): loss=0.3647566068264834\n",
      "Gradient Descent(28/49): loss=0.36465719742976704\n",
      "Gradient Descent(29/49): loss=0.3645685646498682\n",
      "Gradient Descent(30/49): loss=0.36448926019826006\n",
      "Gradient Descent(31/49): loss=0.3644180443142026\n",
      "Gradient Descent(32/49): loss=0.36435385495799194\n",
      "Gradient Descent(33/49): loss=0.3642957816253572\n",
      "Gradient Descent(34/49): loss=0.36424304307927935\n",
      "Gradient Descent(35/49): loss=0.3641949684048906\n",
      "Gradient Descent(36/49): loss=0.3641509808849499\n",
      "Gradient Descent(37/49): loss=0.36411058427065013\n",
      "Gradient Descent(38/49): loss=0.3640733510875875\n",
      "Gradient Descent(39/49): loss=0.36403891267162775\n",
      "Gradient Descent(40/49): loss=0.3640069506757661\n",
      "Gradient Descent(41/49): loss=0.36397718982828153\n",
      "Gradient Descent(42/49): loss=0.3639493917556556\n",
      "Gradient Descent(43/49): loss=0.36392334971182055\n",
      "Gradient Descent(44/49): loss=0.3638988840791064\n",
      "Gradient Descent(45/49): loss=0.3638758385264505\n",
      "Gradient Descent(46/49): loss=0.3638540767275572\n",
      "Gradient Descent(47/49): loss=0.36383347955623996\n",
      "Gradient Descent(48/49): loss=0.36381394268852585\n",
      "Gradient Descent(49/49): loss=0.3637953745515891\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3947282623379986\n",
      "Gradient Descent(2/49): loss=0.3881816660154346\n",
      "Gradient Descent(3/49): loss=0.38452900640529497\n",
      "Gradient Descent(4/49): loss=0.38165537827982465\n",
      "Gradient Descent(5/49): loss=0.3793153984613386\n",
      "Gradient Descent(6/49): loss=0.37737488969815475\n",
      "Gradient Descent(7/49): loss=0.37574282082301\n",
      "Gradient Descent(8/49): loss=0.37435479734067933\n",
      "Gradient Descent(9/49): loss=0.3731639684826112\n",
      "Gradient Descent(10/49): loss=0.3721353414890096\n",
      "Gradient Descent(11/49): loss=0.37124211322236267\n",
      "Gradient Descent(12/49): loss=0.37046326279995645\n",
      "Gradient Descent(13/49): loss=0.36978194834265143\n",
      "Gradient Descent(14/49): loss=0.369184422609433\n",
      "Gradient Descent(15/49): loss=0.3686592867748615\n",
      "Gradient Descent(16/49): loss=0.3681969667774947\n",
      "Gradient Descent(17/49): loss=0.3677893378230979\n",
      "Gradient Descent(18/49): loss=0.36742944881086526\n",
      "Gradient Descent(19/49): loss=0.36711131520005824\n",
      "Gradient Descent(20/49): loss=0.366829759598548\n",
      "Gradient Descent(21/49): loss=0.36658028630507833\n",
      "Gradient Descent(22/49): loss=0.3663589805482208\n",
      "Gradient Descent(23/49): loss=0.3661624261106653\n",
      "Gradient Descent(24/49): loss=0.36598763696481157\n",
      "Gradient Descent(25/49): loss=0.36583199983085773\n",
      "Gradient Descent(26/49): loss=0.36569322543021066\n",
      "Gradient Descent(27/49): loss=0.3655693067919983\n",
      "Gradient Descent(28/49): loss=0.3654584833735734\n",
      "Gradient Descent(29/49): loss=0.3653592100385407\n",
      "Gradient Descent(30/49): loss=0.3652701301379326\n",
      "Gradient Descent(31/49): loss=0.3651900520877782\n",
      "Gradient Descent(32/49): loss=0.36511792894658357\n",
      "Gradient Descent(33/49): loss=0.36505284058050075\n",
      "Gradient Descent(34/49): loss=0.364993978069778\n",
      "Gradient Descent(35/49): loss=0.36494063006254307\n",
      "Gradient Descent(36/49): loss=0.364892170824551\n",
      "Gradient Descent(37/49): loss=0.364848049768643\n",
      "Gradient Descent(38/49): loss=0.3648077822770004\n",
      "Gradient Descent(39/49): loss=0.3647709416540545\n",
      "Gradient Descent(40/49): loss=0.3647371520690221\n",
      "Gradient Descent(41/49): loss=0.3647060823651415\n",
      "Gradient Descent(42/49): loss=0.3646774406282968\n",
      "Gradient Descent(43/49): loss=0.3646509694212351\n",
      "Gradient Descent(44/49): loss=0.36462644160132235\n",
      "Gradient Descent(45/49): loss=0.36460365665000755\n",
      "Gradient Descent(46/49): loss=0.3645824374510764\n",
      "Gradient Descent(47/49): loss=0.364562627462571\n",
      "Gradient Descent(48/49): loss=0.36454408823405243\n",
      "Gradient Descent(49/49): loss=0.3645266972268493\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3957833476173205\n",
      "Gradient Descent(2/49): loss=0.38983941945308137\n",
      "Gradient Descent(3/49): loss=0.38590024134583284\n",
      "Gradient Descent(4/49): loss=0.38263372571744403\n",
      "Gradient Descent(5/49): loss=0.3798924279597891\n",
      "Gradient Descent(6/49): loss=0.37758325858039227\n",
      "Gradient Descent(7/49): loss=0.3756336665808452\n",
      "Gradient Descent(8/49): loss=0.3739846851247847\n",
      "Gradient Descent(9/49): loss=0.37258769853923196\n",
      "Gradient Descent(10/49): loss=0.37140236889586065\n",
      "Gradient Descent(11/49): loss=0.37039510163277883\n",
      "Gradient Descent(12/49): loss=0.36953784374620813\n",
      "Gradient Descent(13/49): loss=0.36880711997721966\n",
      "Gradient Descent(14/49): loss=0.36818325079256997\n",
      "Gradient Descent(15/49): loss=0.367649713330631\n",
      "Gradient Descent(16/49): loss=0.36719261647359\n",
      "Gradient Descent(17/49): loss=0.36680026787962117\n",
      "Gradient Descent(18/49): loss=0.3664628156230784\n",
      "Gradient Descent(19/49): loss=0.3661719507009167\n",
      "Gradient Descent(20/49): loss=0.3659206594285513\n",
      "Gradient Descent(21/49): loss=0.36570301689504875\n",
      "Gradient Descent(22/49): loss=0.365514014331216\n",
      "Gradient Descent(23/49): loss=0.36534941457583336\n",
      "Gradient Descent(24/49): loss=0.36520563088633196\n",
      "Gradient Descent(25/49): loss=0.3650796251912332\n",
      "Gradient Descent(26/49): loss=0.36496882256829727\n",
      "Gradient Descent(27/49): loss=0.3648710392893392\n",
      "Gradient Descent(28/49): loss=0.36478442222673224\n",
      "Gradient Descent(29/49): loss=0.36470739778839734\n",
      "Gradient Descent(30/49): loss=0.36463862885369747\n",
      "Gradient Descent(31/49): loss=0.36457697843476233\n",
      "Gradient Descent(32/49): loss=0.36452147899641224\n",
      "Gradient Descent(33/49): loss=0.3644713065409824\n",
      "Gradient Descent(34/49): loss=0.3644257587083876\n",
      "Gradient Descent(35/49): loss=0.364384236261849\n",
      "Gradient Descent(36/49): loss=0.3643462274300165\n",
      "Gradient Descent(37/49): loss=0.3643112946601499\n",
      "Gradient Descent(38/49): loss=0.36427906340734956\n",
      "Gradient Descent(39/49): loss=0.36424921264383464\n",
      "Gradient Descent(40/49): loss=0.36422146682183515\n",
      "Gradient Descent(41/49): loss=0.3641955890653289\n",
      "Gradient Descent(42/49): loss=0.3641713754009272\n",
      "Gradient Descent(43/49): loss=0.36414864986774204\n",
      "Gradient Descent(44/49): loss=0.36412726037094995\n",
      "Gradient Descent(45/49): loss=0.3641070751647505\n",
      "Gradient Descent(46/49): loss=0.364087979868116\n",
      "Gradient Descent(47/49): loss=0.36406987493166504\n",
      "Gradient Descent(48/49): loss=0.3640526734866004\n",
      "Gradient Descent(49/49): loss=0.3640362995173056\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39568292124312604\n",
      "Gradient Descent(2/49): loss=0.389704274748738\n",
      "Gradient Descent(3/49): loss=0.38572117868241823\n",
      "Gradient Descent(4/49): loss=0.38241153534788763\n",
      "Gradient Descent(5/49): loss=0.37963085664578217\n",
      "Gradient Descent(6/49): loss=0.37728611689444896\n",
      "Gradient Descent(7/49): loss=0.3753043682950192\n",
      "Gradient Descent(8/49): loss=0.3736262590716142\n",
      "Gradient Descent(9/49): loss=0.37220285976680034\n",
      "Gradient Descent(10/49): loss=0.37099357637860014\n",
      "Gradient Descent(11/49): loss=0.369964597389584\n",
      "Gradient Descent(12/49): loss=0.3690876788386295\n",
      "Gradient Descent(13/49): loss=0.36833917256434506\n",
      "Gradient Descent(14/49): loss=0.3676992397090903\n",
      "Gradient Descent(15/49): loss=0.3671512092664698\n",
      "Gradient Descent(16/49): loss=0.3666810519187647\n",
      "Gradient Descent(17/49): loss=0.3662769464392441\n",
      "Gradient Descent(18/49): loss=0.3659289209819209\n",
      "Gradient Descent(19/49): loss=0.36562855533655036\n",
      "Gradient Descent(20/49): loss=0.36536873307911066\n",
      "Gradient Descent(21/49): loss=0.36514343474585353\n",
      "Gradient Descent(22/49): loss=0.3649475648714497\n",
      "Gradient Descent(23/49): loss=0.36477680707864346\n",
      "Gradient Descent(24/49): loss=0.36462750247505255\n",
      "Gradient Descent(25/49): loss=0.3644965474663159\n",
      "Gradient Descent(26/49): loss=0.36438130778144334\n",
      "Gradient Descent(27/49): loss=0.3642795460619854\n",
      "Gradient Descent(28/49): loss=0.3641893608189237\n",
      "Gradient Descent(29/49): loss=0.3641091349310673\n",
      "Gradient Descent(30/49): loss=0.3640374921625807\n",
      "Gradient Descent(31/49): loss=0.3639732604278015\n",
      "Gradient Descent(32/49): loss=0.36391544073882937\n",
      "Gradient Descent(33/49): loss=0.36386318094343095\n",
      "Gradient Descent(34/49): loss=0.36381575350399753\n",
      "Gradient Descent(35/49): loss=0.3637725366877367\n",
      "Gradient Descent(36/49): loss=0.3637329986381122\n",
      "Gradient Descent(37/49): loss=0.36369668388114207\n",
      "Gradient Descent(38/49): loss=0.3636632018902681\n",
      "Gradient Descent(39/49): loss=0.3636322173923834\n",
      "Gradient Descent(40/49): loss=0.36360344214709767\n",
      "Gradient Descent(41/49): loss=0.3635766279729741\n",
      "Gradient Descent(42/49): loss=0.3635515608295619\n",
      "Gradient Descent(43/49): loss=0.36352805579362363\n",
      "Gradient Descent(44/49): loss=0.3635059527929078\n",
      "Gradient Descent(45/49): loss=0.363485112981876\n",
      "Gradient Descent(46/49): loss=0.36346541566157886\n",
      "Gradient Descent(47/49): loss=0.3634467556608963\n",
      "Gradient Descent(48/49): loss=0.3634290411090582\n",
      "Gradient Descent(49/49): loss=0.3634121915400975\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3960348455154442\n",
      "Gradient Descent(2/49): loss=0.39011397196596814\n",
      "Gradient Descent(3/49): loss=0.38615301808033137\n",
      "Gradient Descent(4/49): loss=0.3828604412687959\n",
      "Gradient Descent(5/49): loss=0.3800910406115365\n",
      "Gradient Descent(6/49): loss=0.37775266797543167\n",
      "Gradient Descent(7/49): loss=0.37577361115060154\n",
      "Gradient Descent(8/49): loss=0.3740955737774735\n",
      "Gradient Descent(9/49): loss=0.372670424701669\n",
      "Gradient Descent(10/49): loss=0.37145815081063555\n",
      "Gradient Descent(11/49): loss=0.37042535840110646\n",
      "Gradient Descent(12/49): loss=0.3695441034233212\n",
      "Gradient Descent(13/49): loss=0.368790953149087\n",
      "Gradient Descent(14/49): loss=0.36814622352244764\n",
      "Gradient Descent(15/49): loss=0.36759335456546066\n",
      "Gradient Descent(16/49): loss=0.36711839615682107\n",
      "Gradient Descent(17/49): loss=0.3667095829506675\n",
      "Gradient Descent(18/49): loss=0.36635698179550097\n",
      "Gradient Descent(19/49): loss=0.36605219844394926\n",
      "Gradient Descent(20/49): loss=0.36578813297327883\n",
      "Gradient Descent(21/49): loss=0.36555877538261533\n",
      "Gradient Descent(22/49): loss=0.3653590344423129\n",
      "Gradient Descent(23/49): loss=0.3651845941477553\n",
      "Gradient Descent(24/49): loss=0.36503179315012874\n",
      "Gradient Descent(25/49): loss=0.364897523357185\n",
      "Gradient Descent(26/49): loss=0.36477914456061805\n",
      "Gradient Descent(27/49): loss=0.36467441248622384\n",
      "Gradient Descent(28/49): loss=0.36458141810376854\n",
      "Gradient Descent(29/49): loss=0.3644985363950912\n",
      "Gradient Descent(30/49): loss=0.36442438307677383\n",
      "Gradient Descent(31/49): loss=0.3643577780198296\n",
      "Gradient Descent(32/49): loss=0.36429771431288593\n",
      "Gradient Descent(33/49): loss=0.3642433320849398\n",
      "Gradient Descent(34/49): loss=0.36419389634508476\n",
      "Gradient Descent(35/49): loss=0.36414877821462105\n",
      "Gradient Descent(36/49): loss=0.3641074390256906\n",
      "Gradient Descent(37/49): loss=0.3640694168433173\n",
      "Gradient Descent(38/49): loss=0.3640343150371641\n",
      "Gradient Descent(39/49): loss=0.36400179258765836\n",
      "Gradient Descent(40/49): loss=0.36397155586021424\n",
      "Gradient Descent(41/49): loss=0.3639433516226015\n",
      "Gradient Descent(42/49): loss=0.36391696111532407\n",
      "Gradient Descent(43/49): loss=0.3638921950142433\n",
      "Gradient Descent(44/49): loss=0.36386888914945104\n",
      "Gradient Descent(45/49): loss=0.36384690086532057\n",
      "Gradient Descent(46/49): loss=0.36382610592433356\n",
      "Gradient Descent(47/49): loss=0.3638063958722155\n",
      "Gradient Descent(48/49): loss=0.363787675794534\n",
      "Gradient Descent(49/49): loss=0.36376986240559483\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3939495270873362\n",
      "Gradient Descent(2/49): loss=0.3879090972921138\n",
      "Gradient Descent(3/49): loss=0.38423247124026433\n",
      "Gradient Descent(4/49): loss=0.3813393806395577\n",
      "Gradient Descent(5/49): loss=0.37899245601336584\n",
      "Gradient Descent(6/49): loss=0.3770526925113657\n",
      "Gradient Descent(7/49): loss=0.37542610048332925\n",
      "Gradient Descent(8/49): loss=0.37404659369913934\n",
      "Gradient Descent(9/49): loss=0.3728663113904765\n",
      "Gradient Descent(10/49): loss=0.3718496128461199\n",
      "Gradient Descent(11/49): loss=0.37096924245504564\n",
      "Gradient Descent(12/49): loss=0.37020383780967925\n",
      "Gradient Descent(13/49): loss=0.36953628466598154\n",
      "Gradient Descent(14/49): loss=0.36895261226859305\n",
      "Gradient Descent(15/49): loss=0.36844123708264376\n",
      "Gradient Descent(16/49): loss=0.36799243365726086\n",
      "Gradient Descent(17/49): loss=0.3675979554550791\n",
      "Gradient Descent(18/49): loss=0.3672507561975706\n",
      "Gradient Descent(19/49): loss=0.36694477978280815\n",
      "Gradient Descent(20/49): loss=0.36667479794513524\n",
      "Gradient Descent(21/49): loss=0.3664362819172098\n",
      "Gradient Descent(22/49): loss=0.3662252989065941\n",
      "Gradient Descent(23/49): loss=0.36603842714150364\n",
      "Gradient Descent(24/49): loss=0.3658726851587982\n",
      "Gradient Descent(25/49): loss=0.36572547227122515\n",
      "Gradient Descent(26/49): loss=0.3655945179942117\n",
      "Gradient Descent(27/49): loss=0.36547783878370893\n",
      "Gradient Descent(28/49): loss=0.3653737008304597\n",
      "Gradient Descent(29/49): loss=0.3652805879332085\n",
      "Gradient Descent(30/49): loss=0.3651971736728213\n",
      "Gradient Descent(31/49): loss=0.3651222972562759\n",
      "Gradient Descent(32/49): loss=0.36505494251047205\n",
      "Gradient Descent(33/49): loss=0.36499421959160977\n",
      "Gradient Descent(34/49): loss=0.3649393490436851\n",
      "Gradient Descent(35/49): loss=0.3648896478942819\n",
      "Gradient Descent(36/49): loss=0.36484451752061486\n",
      "Gradient Descent(37/49): loss=0.36480343305598545\n",
      "Gradient Descent(38/49): loss=0.364765934138097\n",
      "Gradient Descent(39/49): loss=0.3647316168272085\n",
      "Gradient Descent(40/49): loss=0.3647001265447785\n",
      "Gradient Descent(41/49): loss=0.3646711519027241\n",
      "Gradient Descent(42/49): loss=0.36464441931022257\n",
      "Gradient Descent(43/49): loss=0.36461968825951546\n",
      "Gradient Descent(44/49): loss=0.3645967472047905\n",
      "Gradient Descent(45/49): loss=0.3645754099591719\n",
      "Gradient Descent(46/49): loss=0.3645555125443865\n",
      "Gradient Descent(47/49): loss=0.3645369104359779\n",
      "Gradient Descent(48/49): loss=0.3645194761541848\n",
      "Gradient Descent(49/49): loss=0.36450309715690926\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39526233829594365\n",
      "Gradient Descent(2/49): loss=0.3895848685662523\n",
      "Gradient Descent(3/49): loss=0.3855850642879756\n",
      "Gradient Descent(4/49): loss=0.3822809853629943\n",
      "Gradient Descent(5/49): loss=0.37952218669698135\n",
      "Gradient Descent(6/49): loss=0.37720950980176293\n",
      "Gradient Descent(7/49): loss=0.37526617415680436\n",
      "Gradient Descent(8/49): loss=0.37363010891710347\n",
      "Gradient Descent(9/49): loss=0.37225038321471626\n",
      "Gradient Descent(10/49): loss=0.37108493678504106\n",
      "Gradient Descent(11/49): loss=0.3700989056806011\n",
      "Gradient Descent(12/49): loss=0.36926331400965123\n",
      "Gradient Descent(13/49): loss=0.36855402808512233\n",
      "Gradient Descent(14/49): loss=0.3679509110637852\n",
      "Gradient Descent(15/49): loss=0.36743713496229263\n",
      "Gradient Descent(16/49): loss=0.36699861792125976\n",
      "Gradient Descent(17/49): loss=0.3666235620287353\n",
      "Gradient Descent(18/49): loss=0.366302072418522\n",
      "Gradient Descent(19/49): loss=0.36602584241686364\n",
      "Gradient Descent(20/49): loss=0.3657878926150896\n",
      "Gradient Descent(21/49): loss=0.36558235414986534\n",
      "Gradient Descent(22/49): loss=0.365404288352832\n",
      "Gradient Descent(23/49): loss=0.3652495364140515\n",
      "Gradient Descent(24/49): loss=0.36511459388145945\n",
      "Gradient Descent(25/49): loss=0.36499650576037457\n",
      "Gradient Descent(26/49): loss=0.3648927787347836\n",
      "Gradient Descent(27/49): loss=0.3648013076449327\n",
      "Gradient Descent(28/49): loss=0.3647203138538187\n",
      "Gradient Descent(29/49): loss=0.364648293541743\n",
      "Gradient Descent(30/49): loss=0.36458397430124856\n",
      "Gradient Descent(31/49): loss=0.3645262786787297\n",
      "Gradient Descent(32/49): loss=0.3644742935349624\n",
      "Gradient Descent(33/49): loss=0.36442724428368234\n",
      "Gradient Descent(34/49): loss=0.36438447322225404\n",
      "Gradient Descent(35/49): loss=0.36434542129716435\n",
      "Gradient Descent(36/49): loss=0.36430961275415835\n",
      "Gradient Descent(37/49): loss=0.36427664221210443\n",
      "Gradient Descent(38/49): loss=0.36424616377416885\n",
      "Gradient Descent(39/49): loss=0.3642178818521329\n",
      "Gradient Descent(40/49): loss=0.364191543431767\n",
      "Gradient Descent(41/49): loss=0.3641669315507646\n",
      "Gradient Descent(42/49): loss=0.36414385979727726\n",
      "Gradient Descent(43/49): loss=0.3641221676677151\n",
      "Gradient Descent(44/49): loss=0.3641017166481782\n",
      "Gradient Descent(45/49): loss=0.36408238690544864\n",
      "Gradient Descent(46/49): loss=0.3640640744915851\n",
      "Gradient Descent(47/49): loss=0.36404668898138254\n",
      "Gradient Descent(48/49): loss=0.36403015147473894\n",
      "Gradient Descent(49/49): loss=0.36401439290672666\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3951615461463855\n",
      "Gradient Descent(2/49): loss=0.38944627441242424\n",
      "Gradient Descent(3/49): loss=0.3854017450856961\n",
      "Gradient Descent(4/49): loss=0.38205387182723805\n",
      "Gradient Descent(5/49): loss=0.37925514248108044\n",
      "Gradient Descent(6/49): loss=0.37690646877854006\n",
      "Gradient Descent(7/49): loss=0.3749306629346131\n",
      "Gradient Descent(8/49): loss=0.37326525771690766\n",
      "Gradient Descent(9/49): loss=0.37185899815952494\n",
      "Gradient Descent(10/49): loss=0.37066955798167744\n",
      "Gradient Descent(11/49): loss=0.3696618462317032\n",
      "Gradient Descent(12/49): loss=0.3688066859174469\n",
      "Gradient Descent(13/49): loss=0.36807976045180607\n",
      "Gradient Descent(14/49): loss=0.367460764095149\n",
      "Gradient Descent(15/49): loss=0.3669327118066692\n",
      "Gradient Descent(16/49): loss=0.366481375434185\n",
      "Gradient Descent(17/49): loss=0.36609482099600477\n",
      "Gradient Descent(18/49): loss=0.3657630274568269\n",
      "Gradient Descent(19/49): loss=0.3654775716055486\n",
      "Gradient Descent(20/49): loss=0.36523136683349805\n",
      "Gradient Descent(21/49): loss=0.36501844606457606\n",
      "Gradient Descent(22/49): loss=0.3648337809951529\n",
      "Gradient Descent(23/49): loss=0.36467313129703743\n",
      "Gradient Descent(24/49): loss=0.3645329186198148\n",
      "Gradient Descent(25/49): loss=0.3644101211716507\n",
      "Gradient Descent(26/49): loss=0.36430218541414333\n",
      "Gradient Descent(27/49): loss=0.3642069520174793\n",
      "Gradient Descent(28/49): loss=0.36412259371779293\n",
      "Gradient Descent(29/49): loss=0.36404756312286585\n",
      "Gradient Descent(30/49): loss=0.3639805488433964\n",
      "Gradient Descent(31/49): loss=0.3639204385992805\n",
      "Gradient Descent(32/49): loss=0.36386628817489586\n",
      "Gradient Descent(33/49): loss=0.3638172952831424\n",
      "Gradient Descent(34/49): loss=0.3637727775520688\n",
      "Gradient Descent(35/49): loss=0.36373215397597836\n",
      "Gradient Descent(36/49): loss=0.3636949292795655\n",
      "Gradient Descent(37/49): loss=0.3636606807326052\n",
      "Gradient Descent(38/49): loss=0.36362904702704374\n",
      "Gradient Descent(39/49): loss=0.36359971889050496\n",
      "Gradient Descent(40/49): loss=0.3635724311622853\n",
      "Gradient Descent(41/49): loss=0.36354695610153404\n",
      "Gradient Descent(42/49): loss=0.36352309773390984\n",
      "Gradient Descent(43/49): loss=0.3635006870737206\n",
      "Gradient Descent(44/49): loss=0.36347957808434883\n",
      "Gradient Descent(45/49): loss=0.3634596442614409\n",
      "Gradient Descent(46/49): loss=0.36344077574156486\n",
      "Gradient Descent(47/49): loss=0.36342287685436325\n",
      "Gradient Descent(48/49): loss=0.3634058640491285\n",
      "Gradient Descent(49/49): loss=0.36338966413757356\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39552329790598706\n",
      "Gradient Descent(2/49): loss=0.38985630967673984\n",
      "Gradient Descent(3/49): loss=0.38583508348736734\n",
      "Gradient Descent(4/49): loss=0.38250447091898687\n",
      "Gradient Descent(5/49): loss=0.37971667974504625\n",
      "Gradient Descent(6/49): loss=0.37737386923047744\n",
      "Gradient Descent(7/49): loss=0.3754002325432448\n",
      "Gradient Descent(8/49): loss=0.37373441263666096\n",
      "Gradient Descent(9/49): loss=0.3723259717482558\n",
      "Gradient Descent(10/49): loss=0.3711331685827685\n",
      "Gradient Descent(11/49): loss=0.3701213278828338\n",
      "Gradient Descent(12/49): loss=0.3692615679564158\n",
      "Gradient Descent(13/49): loss=0.3685297822298679\n",
      "Gradient Descent(14/49): loss=0.3679058143043738\n",
      "Gradient Descent(15/49): loss=0.36737278500353354\n",
      "Gradient Descent(16/49): loss=0.3669165406367347\n",
      "Gradient Descent(17/49): loss=0.36652519883199797\n",
      "Gradient Descent(18/49): loss=0.3661887734305208\n",
      "Gradient Descent(19/49): loss=0.3658988637897369\n",
      "Gradient Descent(20/49): loss=0.3656483967956149\n",
      "Gradient Descent(21/49): loss=0.3654314121793732\n",
      "Gradient Descent(22/49): loss=0.36524288353384454\n",
      "Gradient Descent(23/49): loss=0.365078568848559\n",
      "Gradient Descent(24/49): loss=0.3649348855168971\n",
      "Gradient Descent(25/49): loss=0.36480880567808166\n",
      "Gradient Descent(26/49): loss=0.3646977684901138\n",
      "Gradient Descent(27/49): loss=0.36459960652420853\n",
      "Gradient Descent(28/49): loss=0.36451248395542696\n",
      "Gradient Descent(29/49): loss=0.3644348446201811\n",
      "Gradient Descent(30/49): loss=0.36436536833638794\n",
      "Gradient Descent(31/49): loss=0.3643029341498592\n",
      "Gradient Descent(32/49): loss=0.3642465893917952\n",
      "Gradient Descent(33/49): loss=0.36419552361556085\n",
      "Gradient Descent(34/49): loss=0.3641490466331342\n",
      "Gradient Descent(35/49): loss=0.36410656999826574\n",
      "Gradient Descent(36/49): loss=0.3640675913889435\n",
      "Gradient Descent(37/49): loss=0.36403168142987624\n",
      "Gradient Descent(38/49): loss=0.3639984725693672\n",
      "Gradient Descent(39/49): loss=0.36396764968659157\n",
      "Gradient Descent(40/49): loss=0.36393894215693023\n",
      "Gradient Descent(41/49): loss=0.3639121171463069\n",
      "Gradient Descent(42/49): loss=0.3638869739418081\n",
      "Gradient Descent(43/49): loss=0.3638633391563649\n",
      "Gradient Descent(44/49): loss=0.36384106267090655\n",
      "Gradient Descent(45/49): loss=0.3638200141989343\n",
      "Gradient Descent(46/49): loss=0.36380008037657796\n",
      "Gradient Descent(47/49): loss=0.36378116229643953\n",
      "Gradient Descent(48/49): loss=0.3637631734163499\n",
      "Gradient Descent(49/49): loss=0.36374603778496173\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3933032199696814\n",
      "Gradient Descent(2/49): loss=0.38765036406705883\n",
      "Gradient Descent(3/49): loss=0.3839421753532795\n",
      "Gradient Descent(4/49): loss=0.3810307246534703\n",
      "Gradient Descent(5/49): loss=0.37867808901299976\n",
      "Gradient Descent(6/49): loss=0.3767400298854423\n",
      "Gradient Descent(7/49): loss=0.3751196340582829\n",
      "Gradient Descent(8/49): loss=0.37374918830873016\n",
      "Gradient Descent(9/49): loss=0.3725798692946889\n",
      "Gradient Descent(10/49): loss=0.3715754161142298\n",
      "Gradient Descent(11/49): loss=0.3707081347277924\n",
      "Gradient Descent(12/49): loss=0.3699563275867627\n",
      "Gradient Descent(13/49): loss=0.3693026111707346\n",
      "Gradient Descent(14/49): loss=0.36873279365385053\n",
      "Gradient Descent(15/49): loss=0.3682351099442973\n",
      "Gradient Descent(16/49): loss=0.36779968756508974\n",
      "Gradient Descent(17/49): loss=0.36741816381517134\n",
      "Gradient Descent(18/49): loss=0.36708340378845805\n",
      "Gradient Descent(19/49): loss=0.3667892870026024\n",
      "Gradient Descent(20/49): loss=0.3665305417851354\n",
      "Gradient Descent(21/49): loss=0.36630261375238216\n",
      "Gradient Descent(22/49): loss=0.36610155928142113\n",
      "Gradient Descent(23/49): loss=0.3659239577987188\n",
      "Gradient Descent(24/49): loss=0.36576683860037434\n",
      "Gradient Descent(25/49): loss=0.36562761915785413\n",
      "Gradient Descent(26/49): loss=0.365504052687081\n",
      "Gradient Descent(27/49): loss=0.3653941833166916\n",
      "Gradient Descent(28/49): loss=0.3652963075770613\n",
      "Gradient Descent(29/49): loss=0.3652089412047469\n",
      "Gradient Descent(30/49): loss=0.36513079045518043\n",
      "Gradient Descent(31/49): loss=0.36506072726405814\n",
      "Gradient Descent(32/49): loss=0.3649977677106277\n",
      "Gradient Descent(33/49): loss=0.364941053324285\n",
      "Gradient Descent(34/49): loss=0.3648898348463656\n",
      "Gradient Descent(35/49): loss=0.3648434581163651\n",
      "Gradient Descent(36/49): loss=0.3648013517991921\n",
      "Gradient Descent(37/49): loss=0.3647630167096747\n",
      "Gradient Descent(38/49): loss=0.36472801652398956\n",
      "Gradient Descent(39/49): loss=0.3646959696961376\n",
      "Gradient Descent(40/49): loss=0.3646665424219333\n",
      "Gradient Descent(41/49): loss=0.3646394425138922\n",
      "Gradient Descent(42/49): loss=0.3646144140684319\n",
      "Gradient Descent(43/49): loss=0.36459123282238465\n",
      "Gradient Descent(44/49): loss=0.3645697021093056\n",
      "Gradient Descent(45/49): loss=0.36454964933775785\n",
      "Gradient Descent(46/49): loss=0.36453092292389516\n",
      "Gradient Descent(47/49): loss=0.3645133896194797\n",
      "Gradient Descent(48/49): loss=0.3644969321841227\n",
      "Gradient Descent(49/49): loss=0.3644814473571917\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3948778072700184\n",
      "Gradient Descent(2/49): loss=0.389341542769073\n",
      "Gradient Descent(3/49): loss=0.385275745071439\n",
      "Gradient Descent(4/49): loss=0.3819351891896873\n",
      "Gradient Descent(5/49): loss=0.3791605804907981\n",
      "Gradient Descent(6/49): loss=0.37684609550916764\n",
      "Gradient Descent(7/49): loss=0.37491048634328733\n",
      "Gradient Descent(8/49): loss=0.37328849747102166\n",
      "Gradient Descent(9/49): loss=0.3719268723178072\n",
      "Gradient Descent(10/49): loss=0.3707818477189967\n",
      "Gradient Descent(11/49): loss=0.36981732247111415\n",
      "Gradient Descent(12/49): loss=0.36900343470361296\n",
      "Gradient Descent(13/49): loss=0.3683154299355348\n",
      "Gradient Descent(14/49): loss=0.3677327501315043\n",
      "Gradient Descent(15/49): loss=0.36723829553872755\n",
      "Gradient Descent(16/49): loss=0.36681782349902364\n",
      "Gradient Descent(17/49): loss=0.3664594568080819\n",
      "Gradient Descent(18/49): loss=0.3661532802680443\n",
      "Gradient Descent(19/49): loss=0.36589100863044494\n",
      "Gradient Descent(20/49): loss=0.3656657125982131\n",
      "Gradient Descent(21/49): loss=0.36547159223627\n",
      "Gradient Descent(22/49): loss=0.3653037892304492\n",
      "Gradient Descent(23/49): loss=0.36515823107776835\n",
      "Gradient Descent(24/49): loss=0.3650315015925999\n",
      "Gradient Descent(25/49): loss=0.3649207331509865\n",
      "Gradient Descent(26/49): loss=0.36482351692766185\n",
      "Gradient Descent(27/49): loss=0.36473782805152394\n",
      "Gradient Descent(28/49): loss=0.36466196314915494\n",
      "Gradient Descent(29/49): loss=0.36459448818855517\n",
      "Gradient Descent(30/49): loss=0.3645341948967654\n",
      "Gradient Descent(31/49): loss=0.3644800643213516\n",
      "Gradient Descent(32/49): loss=0.3644312363492753\n",
      "Gradient Descent(33/49): loss=0.36438698419739274\n",
      "Gradient Descent(34/49): loss=0.3643466930546108\n",
      "Gradient Descent(35/49): loss=0.36430984219292784\n",
      "Gradient Descent(36/49): loss=0.3642759899783256\n",
      "Gradient Descent(37/49): loss=0.36424476130690064\n",
      "Gradient Descent(38/49): loss=0.36421583707011373\n",
      "Gradient Descent(39/49): loss=0.36418894531835383\n",
      "Gradient Descent(40/49): loss=0.3641638538464124\n",
      "Gradient Descent(41/49): loss=0.3641403639698257\n",
      "Gradient Descent(42/49): loss=0.36411830529886957\n",
      "Gradient Descent(43/49): loss=0.3640975313485797\n",
      "Gradient Descent(44/49): loss=0.3640779158495415\n",
      "Gradient Descent(45/49): loss=0.3640593496462399\n",
      "Gradient Descent(46/49): loss=0.3640417380881756\n",
      "Gradient Descent(47/49): loss=0.36402499883436334\n",
      "Gradient Descent(48/49): loss=0.3640090600047169\n",
      "Gradient Descent(49/49): loss=0.3639938586225982\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3947767884311861\n",
      "Gradient Descent(2/49): loss=0.38919925432213587\n",
      "Gradient Descent(3/49): loss=0.3850880001449313\n",
      "Gradient Descent(4/49): loss=0.3817031260833772\n",
      "Gradient Descent(5/49): loss=0.3788881128315327\n",
      "Gradient Descent(6/49): loss=0.37653724386805254\n",
      "Gradient Descent(7/49): loss=0.3745688766693618\n",
      "Gradient Descent(8/49): loss=0.37291735743328136\n",
      "Gradient Descent(9/49): loss=0.37152909811874796\n",
      "Gradient Descent(10/49): loss=0.3703600600564991\n",
      "Gradient Descent(11/49): loss=0.3693739046518317\n",
      "Gradient Descent(12/49): loss=0.3685405582795776\n",
      "Gradient Descent(13/49): loss=0.3678350730933636\n",
      "Gradient Descent(14/49): loss=0.36723671223521936\n",
      "Gradient Descent(15/49): loss=0.36672820975659104\n",
      "Gradient Descent(16/49): loss=0.36629516850879196\n",
      "Gradient Descent(17/49): loss=0.3659255680316259\n",
      "Gradient Descent(18/49): loss=0.36560936079105477\n",
      "Gradient Descent(19/49): loss=0.36533813981532864\n",
      "Gradient Descent(20/49): loss=0.3651048643348248\n",
      "Gradient Descent(21/49): loss=0.36490363275741045\n",
      "Gradient Descent(22/49): loss=0.3647294944244751\n",
      "Gradient Descent(23/49): loss=0.3645782932462429\n",
      "Gradient Descent(24/49): loss=0.36444653761950596\n",
      "Gradient Descent(25/49): loss=0.3643312920679174\n",
      "Gradient Descent(26/49): loss=0.36423008687487596\n",
      "Gradient Descent(27/49): loss=0.3641408426471939\n",
      "Gradient Descent(28/49): loss=0.3640618072885724\n",
      "Gradient Descent(29/49): loss=0.36399150330173674\n",
      "Gradient Descent(30/49): loss=0.3639286836972899\n",
      "Gradient Descent(31/49): loss=0.3638722950817416\n",
      "Gradient Descent(32/49): loss=0.36382144673925354\n",
      "Gradient Descent(33/49): loss=0.3637753847212327\n",
      "Gradient Descent(34/49): loss=0.3637334701228758\n",
      "Gradient Descent(35/49): loss=0.36369516086239073\n",
      "Gradient Descent(36/49): loss=0.3636599963919815\n",
      "Gradient Descent(37/49): loss=0.363627584863878\n",
      "Gradient Descent(38/49): loss=0.3635975923530745\n",
      "Gradient Descent(39/49): loss=0.36356973380372654\n",
      "Gradient Descent(40/49): loss=0.3635437654206034\n",
      "Gradient Descent(41/49): loss=0.36351947827242415\n",
      "Gradient Descent(42/49): loss=0.3634966929118511\n",
      "Gradient Descent(43/49): loss=0.363475254848627\n",
      "Gradient Descent(44/49): loss=0.3634550307388493\n",
      "Gradient Descent(45/49): loss=0.3634359051755613\n",
      "Gradient Descent(46/49): loss=0.3634177779843984\n",
      "Gradient Descent(47/49): loss=0.36340056194356635\n",
      "Gradient Descent(48/49): loss=0.36338418086044594\n",
      "Gradient Descent(49/49): loss=0.363368567948017\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(set1_x.shape[1])\n",
    "gamma_opt1 = cross_validation(set1_y, set1_x, k_fold, gammas, fonction=2)\n",
    "w_gd1, loss_gd1 = least_squares_GD(set1_y, set1_x, gamma_opt1, max_iters=max_iters)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt1))\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "initial_w = np.zeros(set2_x.shape[1])\n",
    "gamma_opt2 = cross_validation(set2_y, set2_x, k_fold, gammas, fonction=2)\n",
    "w_gd2, loss_gd2 = least_squares_GD(set2_y, set2_x, gamma_opt2, max_iters=max_iters)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt2))\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(set3_x.shape[1])\n",
    "gamma_opt3 = cross_validation(set3_y, set3_x, k_fold, gammas, fonction=2)\n",
    "w_gd3, loss_gd3 = least_squares_GD(set3_y, set3_x, gamma_opt3, max_iters=max_iters)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt3))\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Preprocessing for stochastic gradient descent done!\n"
     ]
    }
   ],
   "source": [
    "k_fold = 4\n",
    "max_iters = 500\n",
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "tX_log = log_distribution(tX, to_log)\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_log, y, ids)\n",
    "\n",
    "set1_x = outliers(set1_x, -999)\n",
    "set1_x = filtering_with_mean_bis(set1_x, set1_y)\n",
    "#set1_x = filtering_with_mean(set1_x)\n",
    "set1_x = std(set1_x)\n",
    "\n",
    "set2_x = outliers(set2_x, -999)\n",
    "set2_x = filtering_with_mean_bis(set2_x, set2_y)\n",
    "#set2_x = filtering_with_mean(set2_x)\n",
    "set2_x = std(set2_x)\n",
    "\n",
    "set3_x = outliers(set3_x, -999)\n",
    "set3_x = filtering_with_mean_bis(set3_x, set3_y)\n",
    "#set3_x = filtering_with_mean(set3_x)\n",
    "set3_x = std(set3_x)\n",
    "print('')\n",
    "print(\"Preprocessing for stochastic gradient descent done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we perform a 4-fold cross validation for the gamma parameter for stochastic gradient descent method for each set. Then we we perform a stochastic gradient descent with the optimal gamma found. For the cross validation we use 50 iterations and for the final descent 500 iterations as we want a more precise final result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_opt_sgd1 = cross_validation(set1_y, set1_x, k_fold, gammas, fonction=3)\n",
    "w_sgd1, loss_sgd1 = least_squares_SGD(set1_y, set1_x, gamma_opt_sgd1, max_iters=500)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt_sgd1))\n",
    "print(\"Stochastic gradient descent regression loss {loss}\".format(loss=loss_sgd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_opt2 = cross_validation(set2_y, set2_x, k_fold, gammas, fonction=3)\n",
    "w_sgd2, loss_sgd2 = least_squares_SGD(set2_y, set2_x, gamma_opt2, max_iters=500)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt2))\n",
    "print(\"Stochastic gradient descent regression loss {loss}\".format(loss=loss_sgd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_opt3 = cross_validation(set3_y, set3_x, k_fold, gammas, fonction=3)\n",
    "w_sgd3, loss_sgd3 = least_squares_SGD(set3_y, set3_x, gamma_opt3, max_iters=500)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt3))\n",
    "print(\"Stochastic gradient descent regression loss {loss}\".format(loss=loss_sgd3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Preprocessing for logistic regression done!\n"
     ]
    }
   ],
   "source": [
    "y[y==-1]=0 # transform y=-1 to 0\n",
    "k_fold = 4\n",
    "max_iters = 50\n",
    "gammas = np.arange(0, 0.5, 0.01) \n",
    "tX_log = log_distribution(tX, to_log)\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_log, y, ids)\n",
    "\n",
    "set1_x = outliers(set1_x, -999)\n",
    "set1_x = filtering_with_mean_bis(set1_x, set1_y, lr=1)\n",
    "#set1_x = filtering_with_mean(set1_x)\n",
    "set1_x = std(set1_x)\n",
    "#set1_x = scaling(set1_x)\n",
    "\n",
    "set2_x = outliers(set2_x, -999)\n",
    "set2_x = filtering_with_mean_bis(set2_x, set2_y, lr=1)\n",
    "#set2_x = filtering_with_mean(set2_x)\n",
    "set2_x = std(set2_x)\n",
    "\n",
    "set3_x = outliers(set3_x, -999)\n",
    "set3_x = filtering_with_mean_bis(set3_x, set3_y, lr=1)\n",
    "#set3_x = filtering_with_mean(set3_x)\n",
    "set3_x = std(set3_x)\n",
    "\n",
    "print('')\n",
    "print(\"Preprocessing for logistic regression done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=51940.29082807905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=51940.29082807904\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=51940.29082807905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=51940.29082807904\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.00186535]\n",
      " [-0.00893223]\n",
      " [-0.00523796]\n",
      " ...\n",
      " [ 0.00447915]\n",
      " [ 0.00186535]\n",
      " [-0.00759268]]\n",
      "t [[ 0.00186535]\n",
      " [-0.00893223]\n",
      " [-0.00523796]\n",
      " ...\n",
      " [ 0.00447915]\n",
      " [ 0.00186535]\n",
      " [-0.00759268]]\n",
      "t [[ 0.00370957]\n",
      " [-0.01783104]\n",
      " [-0.01045069]\n",
      " ...\n",
      " [ 0.00890217]\n",
      " [ 0.00370957]\n",
      " [-0.01511593]]\n",
      "t [[ 0.00370957]\n",
      " [-0.01783104]\n",
      " [-0.01045069]\n",
      " ...\n",
      " [ 0.00890217]\n",
      " [ 0.00370957]\n",
      " [-0.01511593]]\n",
      "Current iteration=2, loss=51672.01505723351\n",
      "t [[ 0.00553286]\n",
      " [-0.02669651]\n",
      " [-0.01563839]\n",
      " ...\n",
      " [ 0.01326972]\n",
      " [ 0.00553286]\n",
      " [-0.02257059]]\n",
      "t [[ 0.00553286]\n",
      " [-0.02669651]\n",
      " [-0.01563839]\n",
      " ...\n",
      " [ 0.01326972]\n",
      " [ 0.00553286]\n",
      " [-0.02257059]]\n",
      "t [[ 0.00733541]\n",
      " [-0.03552873]\n",
      " [-0.02080124]\n",
      " ...\n",
      " [ 0.01758248]\n",
      " [ 0.00733541]\n",
      " [-0.0299575 ]]\n",
      "t [[ 0.00733541]\n",
      " [-0.03552873]\n",
      " [-0.02080124]\n",
      " ...\n",
      " [ 0.01758248]\n",
      " [ 0.00733541]\n",
      " [-0.0299575 ]]\n",
      "Current iteration=4, loss=51408.91553735361\n",
      "t [[ 0.00911744]\n",
      " [-0.0443278 ]\n",
      " [-0.02593944]\n",
      " ...\n",
      " [ 0.02184111]\n",
      " [ 0.00911744]\n",
      " [-0.03727747]]\n",
      "t [[ 0.00911744]\n",
      " [-0.0443278 ]\n",
      " [-0.02593944]\n",
      " ...\n",
      " [ 0.02184111]\n",
      " [ 0.00911744]\n",
      " [-0.03727747]]\n",
      "t [[ 0.01087914]\n",
      " [-0.0530938 ]\n",
      " [-0.03105318]\n",
      " ...\n",
      " [ 0.02604626]\n",
      " [ 0.01087914]\n",
      " [-0.04453133]]\n",
      "t [[ 0.01087914]\n",
      " [-0.0530938 ]\n",
      " [-0.03105318]\n",
      " ...\n",
      " [ 0.02604626]\n",
      " [ 0.01087914]\n",
      " [-0.04453133]]\n",
      "Current iteration=6, loss=51150.854606933666\n",
      "t [[ 0.01262071]\n",
      " [-0.06182683]\n",
      " [-0.03614265]\n",
      " ...\n",
      " [ 0.03019858]\n",
      " [ 0.01262071]\n",
      " [-0.05171989]]\n",
      "t [[ 0.01262071]\n",
      " [-0.06182683]\n",
      " [-0.03614265]\n",
      " ...\n",
      " [ 0.03019858]\n",
      " [ 0.01262071]\n",
      " [-0.05171989]]\n",
      "t [[ 0.01434233]\n",
      " [-0.07052698]\n",
      " [-0.04120804]\n",
      " ...\n",
      " [ 0.03429872]\n",
      " [ 0.01434233]\n",
      " [-0.05884395]]\n",
      "t [[ 0.01434233]\n",
      " [-0.07052698]\n",
      " [-0.04120804]\n",
      " ...\n",
      " [ 0.03429872]\n",
      " [ 0.01434233]\n",
      " [-0.05884395]]\n",
      "Current iteration=8, loss=50897.698854672744\n",
      "t [[ 0.01604422]\n",
      " [-0.07919435]\n",
      " [-0.04624952]\n",
      " ...\n",
      " [ 0.0383473 ]\n",
      " [ 0.01604422]\n",
      " [-0.06590429]]\n",
      "t [[ 0.01604422]\n",
      " [-0.07919435]\n",
      " [-0.04624952]\n",
      " ...\n",
      " [ 0.0383473 ]\n",
      " [ 0.01604422]\n",
      " [-0.06590429]]\n",
      "t [[ 0.01772655]\n",
      " [-0.08782903]\n",
      " [-0.0512673 ]\n",
      " ...\n",
      " [ 0.04234497]\n",
      " [ 0.01772655]\n",
      " [-0.07290171]]\n",
      "loss=50649.31900558634\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.00563294]\n",
      " [-0.00328378]\n",
      " [-0.00525539]\n",
      " ...\n",
      " [ 0.00441359]\n",
      " [ 0.00185803]\n",
      " [-0.00747794]]\n",
      "t [[ 0.00563294]\n",
      " [-0.00328378]\n",
      " [-0.00525539]\n",
      " ...\n",
      " [ 0.00441359]\n",
      " [ 0.00185803]\n",
      " [-0.00747794]]\n",
      "t [[ 0.01122763]\n",
      " [-0.00659579]\n",
      " [-0.01048528]\n",
      " ...\n",
      " [ 0.00877145]\n",
      " [ 0.00369485]\n",
      " [-0.01488648]]\n",
      "t [[ 0.01122763]\n",
      " [-0.00659579]\n",
      " [-0.01048528]\n",
      " ...\n",
      " [ 0.00877145]\n",
      " [ 0.00369485]\n",
      " [-0.01488648]]\n",
      "Current iteration=2, loss=51671.32544172018\n",
      "t [[ 0.01678433]\n",
      " [-0.00993539]\n",
      " [-0.01568987]\n",
      " ...\n",
      " [ 0.01307425]\n",
      " [ 0.00551068]\n",
      " [-0.02222648]]\n",
      "t [[ 0.01678433]\n",
      " [-0.00993539]\n",
      " [-0.01568987]\n",
      " ...\n",
      " [ 0.01307425]\n",
      " [ 0.00551068]\n",
      " [-0.02222648]]\n",
      "t [[ 0.02230332]\n",
      " [-0.01330193]\n",
      " [-0.02086935]\n",
      " ...\n",
      " [ 0.01732265]\n",
      " [ 0.00730571]\n",
      " [-0.02949877]]\n",
      "t [[ 0.02230332]\n",
      " [-0.01330193]\n",
      " [-0.02086935]\n",
      " ...\n",
      " [ 0.01732265]\n",
      " [ 0.00730571]\n",
      " [-0.02949877]]\n",
      "Current iteration=4, loss=51407.5712858294\n",
      "t [[ 0.02778487]\n",
      " [-0.01669479]\n",
      " [-0.02602392]\n",
      " ...\n",
      " [ 0.02151731]\n",
      " [ 0.00908014]\n",
      " [-0.03670418]]\n",
      "t [[ 0.02778487]\n",
      " [-0.01669479]\n",
      " [-0.02602392]\n",
      " ...\n",
      " [ 0.02151731]\n",
      " [ 0.00908014]\n",
      " [-0.03670418]]\n",
      "t [[ 0.03322926]\n",
      " [-0.02011334]\n",
      " [-0.03115377]\n",
      " ...\n",
      " [ 0.02565889]\n",
      " [ 0.01083418]\n",
      " [-0.04384355]]\n",
      "t [[ 0.03322926]\n",
      " [-0.02011334]\n",
      " [-0.03115377]\n",
      " ...\n",
      " [ 0.02565889]\n",
      " [ 0.01083418]\n",
      " [-0.04384355]]\n",
      "Current iteration=6, loss=51148.88898778222\n",
      "t [[ 0.03863675]\n",
      " [-0.02355697]\n",
      " [-0.0362591 ]\n",
      " ...\n",
      " [ 0.02974804]\n",
      " [ 0.01256802]\n",
      " [-0.05091768]]\n",
      "t [[ 0.03863675]\n",
      " [-0.02355697]\n",
      " [-0.0362591 ]\n",
      " ...\n",
      " [ 0.02974804]\n",
      " [ 0.01256802]\n",
      " [-0.05091768]]\n",
      "t [[ 0.04400762]\n",
      " [-0.02702507]\n",
      " [-0.04134009]\n",
      " ...\n",
      " [ 0.0337854 ]\n",
      " [ 0.01428186]\n",
      " [-0.05792738]]\n",
      "t [[ 0.04400762]\n",
      " [-0.02702507]\n",
      " [-0.04134009]\n",
      " ...\n",
      " [ 0.0337854 ]\n",
      " [ 0.01428186]\n",
      " [-0.05792738]]\n",
      "Current iteration=8, loss=50895.143501190825\n",
      "t [[ 0.04934214]\n",
      " [-0.03051705]\n",
      " [-0.04639694]\n",
      " ...\n",
      " [ 0.0377716 ]\n",
      " [ 0.0159759 ]\n",
      " [-0.06487345]]\n",
      "t [[ 0.04934214]\n",
      " [-0.03051705]\n",
      " [-0.04639694]\n",
      " ...\n",
      " [ 0.0377716 ]\n",
      " [ 0.0159759 ]\n",
      " [-0.06487345]]\n",
      "t [[ 0.05464057]\n",
      " [-0.03403232]\n",
      " [-0.05142983]\n",
      " ...\n",
      " [ 0.04170726]\n",
      " [ 0.01765033]\n",
      " [-0.07175668]]\n",
      "loss=50646.20398980245\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.00556075]\n",
      " [-0.00314352]\n",
      " [-0.00523571]\n",
      " ...\n",
      " [ 0.00448169]\n",
      " [ 0.00189227]\n",
      " [-0.00770822]]\n",
      "t [[ 0.00556075]\n",
      " [-0.00314352]\n",
      " [-0.00523571]\n",
      " ...\n",
      " [ 0.00448169]\n",
      " [ 0.00189227]\n",
      " [-0.00770822]]\n",
      "t [[ 0.01108374]\n",
      " [-0.0063154 ]\n",
      " [-0.01044592]\n",
      " ...\n",
      " [ 0.00890612]\n",
      " [ 0.00376309]\n",
      " [-0.01534495]]\n",
      "t [[ 0.01108374]\n",
      " [-0.0063154 ]\n",
      " [-0.01044592]\n",
      " ...\n",
      " [ 0.00890612]\n",
      " [ 0.00376309]\n",
      " [-0.01534495]]\n",
      "Current iteration=2, loss=51673.1494836519\n",
      "t [[ 0.01656926]\n",
      " [-0.00951499]\n",
      " [-0.01563082]\n",
      " ...\n",
      " [ 0.01327397]\n",
      " [ 0.00561265]\n",
      " [-0.02291105]]\n",
      "t [[ 0.01656926]\n",
      " [-0.00951499]\n",
      " [-0.01563082]\n",
      " ...\n",
      " [ 0.01327397]\n",
      " [ 0.00561265]\n",
      " [-0.02291105]]\n",
      "t [[ 0.02201757]\n",
      " [-0.01274165]\n",
      " [-0.02079063]\n",
      " ...\n",
      " [ 0.01758595]\n",
      " [ 0.00744116]\n",
      " [-0.03040739]]\n",
      "t [[ 0.02201757]\n",
      " [-0.01274165]\n",
      " [-0.02079063]\n",
      " ...\n",
      " [ 0.01758595]\n",
      " [ 0.00744116]\n",
      " [-0.03040739]]\n",
      "Current iteration=4, loss=51411.23321187365\n",
      "t [[ 0.02742894]\n",
      " [-0.01599474]\n",
      " [-0.02592552]\n",
      " ...\n",
      " [ 0.02184271]\n",
      " [ 0.00924884]\n",
      " [-0.03783483]]\n",
      "t [[ 0.02742894]\n",
      " [-0.01599474]\n",
      " [-0.02592552]\n",
      " ...\n",
      " [ 0.02184271]\n",
      " [ 0.00924884]\n",
      " [-0.03783483]]\n",
      "t [[ 0.03280365]\n",
      " [-0.01927364]\n",
      " [-0.0310357 ]\n",
      " ...\n",
      " [ 0.02604495]\n",
      " [ 0.01103587]\n",
      " [-0.0451942 ]]\n",
      "t [[ 0.03280365]\n",
      " [-0.01927364]\n",
      " [-0.0310357 ]\n",
      " ...\n",
      " [ 0.02604495]\n",
      " [ 0.01103587]\n",
      " [-0.0451942 ]]\n",
      "Current iteration=6, loss=51154.40059192433\n",
      "t [[ 0.03814195]\n",
      " [-0.02257773]\n",
      " [-0.03612137]\n",
      " ...\n",
      " [ 0.03019331]\n",
      " [ 0.01280246]\n",
      " [-0.05248635]]\n",
      "t [[ 0.03814195]\n",
      " [-0.02257773]\n",
      " [-0.03612137]\n",
      " ...\n",
      " [ 0.03019331]\n",
      " [ 0.01280246]\n",
      " [-0.05248635]]\n",
      "t [[ 0.04344413]\n",
      " [-0.02590641]\n",
      " [-0.0411827 ]\n",
      " ...\n",
      " [ 0.03428847]\n",
      " [ 0.01454882]\n",
      " [-0.0597121 ]]\n",
      "t [[ 0.04344413]\n",
      " [-0.02590641]\n",
      " [-0.0411827 ]\n",
      " ...\n",
      " [ 0.03428847]\n",
      " [ 0.01454882]\n",
      " [-0.0597121 ]]\n",
      "Current iteration=8, loss=50902.51464357138\n",
      "t [[ 0.04871044]\n",
      " [-0.02925907]\n",
      " [-0.0462199 ]\n",
      " ...\n",
      " [ 0.03833106]\n",
      " [ 0.01627513]\n",
      " [-0.06687227]]\n",
      "t [[ 0.04871044]\n",
      " [-0.02925907]\n",
      " [-0.0462199 ]\n",
      " ...\n",
      " [ 0.03833106]\n",
      " [ 0.01627513]\n",
      " [-0.06687227]]\n",
      "t [[ 0.05394115]\n",
      " [-0.03263512]\n",
      " [-0.05123315]\n",
      " ...\n",
      " [ 0.04232174]\n",
      " [ 0.0179816 ]\n",
      " [-0.07396768]]\n",
      "loss=50655.442707778784\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.00555185]\n",
      " [-0.00330258]\n",
      " [-0.00521926]\n",
      " ...\n",
      " [ 0.0079964 ]\n",
      " [-0.00521926]\n",
      " [ 0.00748977]]\n",
      "t [[ 0.00555185]\n",
      " [-0.00330258]\n",
      " [-0.00521926]\n",
      " ...\n",
      " [ 0.0079964 ]\n",
      " [-0.00521926]\n",
      " [ 0.00748977]]\n",
      "t [[ 0.01106601]\n",
      " [-0.00663198]\n",
      " [-0.01041325]\n",
      " ...\n",
      " [ 0.0159318 ]\n",
      " [-0.01041325]\n",
      " [ 0.01491251]]\n",
      "t [[ 0.01106601]\n",
      " [-0.00663198]\n",
      " [-0.01041325]\n",
      " ...\n",
      " [ 0.0159318 ]\n",
      " [-0.01041325]\n",
      " [ 0.01491251]]\n",
      "Current iteration=2, loss=51673.74669171749\n",
      "t [[ 0.01654277]\n",
      " [-0.00998754]\n",
      " [-0.01558215]\n",
      " ...\n",
      " [ 0.02380672]\n",
      " [-0.01558215]\n",
      " [ 0.02226889]]\n",
      "t [[ 0.01654277]\n",
      " [-0.00998754]\n",
      " [-0.01558215]\n",
      " ...\n",
      " [ 0.02380672]\n",
      " [-0.01558215]\n",
      " [ 0.02226889]]\n",
      "t [[ 0.0219824 ]\n",
      " [-0.01336867]\n",
      " [-0.02072616]\n",
      " ...\n",
      " [ 0.03162169]\n",
      " [-0.02072616]\n",
      " [ 0.02955955]]\n",
      "t [[ 0.0219824 ]\n",
      " [-0.01336867]\n",
      " [-0.02072616]\n",
      " ...\n",
      " [ 0.03162169]\n",
      " [-0.02072616]\n",
      " [ 0.02955955]]\n",
      "Current iteration=4, loss=51412.371541760534\n",
      "t [[ 0.02738517]\n",
      " [-0.01677473]\n",
      " [-0.02584548]\n",
      " ...\n",
      " [ 0.03937721]\n",
      " [-0.02584548]\n",
      " [ 0.03678516]]\n",
      "t [[ 0.02738517]\n",
      " [-0.01677473]\n",
      " [-0.02584548]\n",
      " ...\n",
      " [ 0.03937721]\n",
      " [-0.02584548]\n",
      " [ 0.03678516]]\n",
      "t [[ 0.03275135]\n",
      " [-0.02020512]\n",
      " [-0.03094029]\n",
      " ...\n",
      " [ 0.0470738 ]\n",
      " [-0.03094029]\n",
      " [ 0.04394636]]\n",
      "t [[ 0.03275135]\n",
      " [-0.02020512]\n",
      " [-0.03094029]\n",
      " ...\n",
      " [ 0.0470738 ]\n",
      " [-0.03094029]\n",
      " [ 0.04394636]]\n",
      "Current iteration=6, loss=51156.02701117622\n",
      "t [[ 0.03808122]\n",
      " [-0.02365924]\n",
      " [-0.03601079]\n",
      " ...\n",
      " [ 0.05471198]\n",
      " [-0.03601079]\n",
      " [ 0.05104378]]\n",
      "t [[ 0.03808122]\n",
      " [-0.02365924]\n",
      " [-0.03601079]\n",
      " ...\n",
      " [ 0.05471198]\n",
      " [-0.03601079]\n",
      " [ 0.05104378]]\n",
      "t [[ 0.04337504]\n",
      " [-0.02713652]\n",
      " [-0.04105716]\n",
      " ...\n",
      " [ 0.06229225]\n",
      " [-0.04105716]\n",
      " [ 0.05807806]]\n",
      "t [[ 0.04337504]\n",
      " [-0.02713652]\n",
      " [-0.04105716]\n",
      " ...\n",
      " [ 0.06229225]\n",
      " [-0.04105716]\n",
      " [ 0.05807806]]\n",
      "Current iteration=8, loss=50904.579030813686\n",
      "t [[ 0.04863308]\n",
      " [-0.03063635]\n",
      " [-0.0460796 ]\n",
      " ...\n",
      " [ 0.06981513]\n",
      " [-0.0460796 ]\n",
      " [ 0.06504983]]\n",
      "t [[ 0.04863308]\n",
      " [-0.03063635]\n",
      " [-0.0460796 ]\n",
      " ...\n",
      " [ 0.06981513]\n",
      " [-0.0460796 ]\n",
      " [ 0.06504983]]\n",
      "t [[ 0.05385562]\n",
      " [-0.03415818]\n",
      " [-0.05107828]\n",
      " ...\n",
      " [ 0.0772811 ]\n",
      " [-0.05107828]\n",
      " [ 0.07195971]]\n",
      "loss=50657.897715290695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0037307 ]\n",
      " [-0.01786446]\n",
      " [-0.01047592]\n",
      " ...\n",
      " [ 0.00895831]\n",
      " [ 0.0037307 ]\n",
      " [-0.01518537]]\n",
      "t [[ 0.0037307 ]\n",
      " [-0.01786446]\n",
      " [-0.01047592]\n",
      " ...\n",
      " [ 0.00895831]\n",
      " [ 0.0037307 ]\n",
      " [-0.01518537]]\n",
      "t [[ 0.00737687]\n",
      " [-0.03559523]\n",
      " [-0.02085093]\n",
      " ...\n",
      " [ 0.01769206]\n",
      " [ 0.00737687]\n",
      " [-0.03009301]]\n",
      "t [[ 0.00737687]\n",
      " [-0.03559523]\n",
      " [-0.02085093]\n",
      " ...\n",
      " [ 0.01769206]\n",
      " [ 0.00737687]\n",
      " [-0.03009301]]\n",
      "Current iteration=2, loss=51407.64740611778\n",
      "t [[ 0.01094014]\n",
      " [-0.05319301]\n",
      " [-0.03112658]\n",
      " ...\n",
      " [ 0.0262067 ]\n",
      " [ 0.01094014]\n",
      " [-0.04472969]]\n",
      "t [[ 0.01094014]\n",
      " [-0.05319301]\n",
      " [-0.03112658]\n",
      " ...\n",
      " [ 0.0262067 ]\n",
      " [ 0.01094014]\n",
      " [-0.04472969]]\n",
      "t [[ 0.01442211]\n",
      " [-0.07065854]\n",
      " [-0.0413044 ]\n",
      " ...\n",
      " [ 0.03450751]\n",
      " [ 0.01442211]\n",
      " [-0.05910203]]\n",
      "t [[ 0.01442211]\n",
      " [-0.07065854]\n",
      " [-0.0413044 ]\n",
      " ...\n",
      " [ 0.03450751]\n",
      " [ 0.01442211]\n",
      " [-0.05910203]]\n",
      "Current iteration=4, loss=50895.2939849141\n",
      "t [[ 0.01782437]\n",
      " [-0.08799256]\n",
      " [-0.05138593]\n",
      " ...\n",
      " [ 0.04259971]\n",
      " [ 0.01782437]\n",
      " [-0.07321653]]\n",
      "t [[ 0.01782437]\n",
      " [-0.08799256]\n",
      " [-0.05138593]\n",
      " ...\n",
      " [ 0.04259971]\n",
      " [ 0.01782437]\n",
      " [-0.07321653]]\n",
      "t [[ 0.02114848]\n",
      " [-0.10519585]\n",
      " [-0.06137266]\n",
      " ...\n",
      " [ 0.05048838]\n",
      " [ 0.02114848]\n",
      " [-0.08707953]]\n",
      "t [[ 0.02114848]\n",
      " [-0.10519585]\n",
      " [-0.06137266]\n",
      " ...\n",
      " [ 0.05048838]\n",
      " [ 0.02114848]\n",
      " [-0.08707953]]\n",
      "Current iteration=6, loss=50402.16634832396\n",
      "t [[ 0.02439597]\n",
      " [-0.12226918]\n",
      " [-0.07126607]\n",
      " ...\n",
      " [ 0.0581785 ]\n",
      " [ 0.02439597]\n",
      " [-0.10069724]]\n",
      "t [[ 0.02439597]\n",
      " [-0.12226918]\n",
      " [-0.07126607]\n",
      " ...\n",
      " [ 0.0581785 ]\n",
      " [ 0.02439597]\n",
      " [-0.10069724]]\n",
      "t [[ 0.02756836]\n",
      " [-0.13921337]\n",
      " [-0.08106762]\n",
      " ...\n",
      " [ 0.06567493]\n",
      " [ 0.02756836]\n",
      " [-0.11407574]]\n",
      "t [[ 0.02756836]\n",
      " [-0.13921337]\n",
      " [-0.08106762]\n",
      " ...\n",
      " [ 0.06567493]\n",
      " [ 0.02756836]\n",
      " [-0.11407574]]\n",
      "Current iteration=8, loss=49927.2659312359\n",
      "t [[ 0.03066714]\n",
      " [-0.15602924]\n",
      " [-0.09077873]\n",
      " ...\n",
      " [ 0.07298243]\n",
      " [ 0.03066714]\n",
      " [-0.12722096]]\n",
      "t [[ 0.03066714]\n",
      " [-0.15602924]\n",
      " [-0.09077873]\n",
      " ...\n",
      " [ 0.07298243]\n",
      " [ 0.03066714]\n",
      " [-0.12722096]]\n",
      "t [[ 0.03369378]\n",
      " [-0.17271762]\n",
      " [-0.10040082]\n",
      " ...\n",
      " [ 0.08010563]\n",
      " [ 0.03369378]\n",
      " [-0.14013869]]\n",
      "loss=49469.65603080118\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.01126589]\n",
      " [-0.00656756]\n",
      " [-0.01051078]\n",
      " ...\n",
      " [ 0.00882719]\n",
      " [ 0.00371605]\n",
      " [-0.01495588]]\n",
      "t [[ 0.01126589]\n",
      " [-0.00656756]\n",
      " [-0.01051078]\n",
      " ...\n",
      " [ 0.00882719]\n",
      " [ 0.00371605]\n",
      " [-0.01495588]]\n",
      "t [[ 0.02237874]\n",
      " [-0.01324805]\n",
      " [-0.02091956]\n",
      " ...\n",
      " [ 0.01743143]\n",
      " [ 0.0073473 ]\n",
      " [-0.02963418]]\n",
      "t [[ 0.02237874]\n",
      " [-0.01324805]\n",
      " [-0.02091956]\n",
      " ...\n",
      " [ 0.01743143]\n",
      " [ 0.0073473 ]\n",
      " [-0.02963418]]\n",
      "Current iteration=2, loss=51406.29473412025\n",
      "t [[ 0.03334078]\n",
      " [-0.02003627]\n",
      " [-0.03122794]\n",
      " ...\n",
      " [ 0.02581815]\n",
      " [ 0.01089537]\n",
      " [-0.04404171]]\n",
      "t [[ 0.03334078]\n",
      " [-0.02003627]\n",
      " [-0.03122794]\n",
      " ...\n",
      " [ 0.02581815]\n",
      " [ 0.01089537]\n",
      " [-0.04404171]]\n",
      "t [[ 0.04415418]\n",
      " [-0.02692717]\n",
      " [-0.04143746]\n",
      " ...\n",
      " [ 0.03399262]\n",
      " [ 0.01436189]\n",
      " [-0.05818517]]\n",
      "t [[ 0.04415418]\n",
      " [-0.02692717]\n",
      " [-0.04143746]\n",
      " ...\n",
      " [ 0.03399262]\n",
      " [ 0.01436189]\n",
      " [-0.05818517]]\n",
      "Current iteration=4, loss=50892.72340100409\n",
      "t [[ 0.05482114]\n",
      " [-0.03391583]\n",
      " [-0.05154969]\n",
      " ...\n",
      " [ 0.04196005]\n",
      " [ 0.01774844]\n",
      " [-0.07207107]]\n",
      "t [[ 0.05482114]\n",
      " [-0.03391583]\n",
      " [-0.05154969]\n",
      " ...\n",
      " [ 0.04196005]\n",
      " [ 0.01774844]\n",
      " [-0.07207107]]\n",
      "t [[ 0.06534384]\n",
      " [-0.04099749]\n",
      " [-0.06156613]\n",
      " ...\n",
      " [ 0.04972551]\n",
      " [ 0.02105661]\n",
      " [-0.08570582]]\n",
      "t [[ 0.06534384]\n",
      " [-0.04099749]\n",
      " [-0.06156613]\n",
      " ...\n",
      " [ 0.04972551]\n",
      " [ 0.02105661]\n",
      " [-0.08570582]]\n",
      "Current iteration=6, loss=50398.49959959005\n",
      "t [[ 0.07572441]\n",
      " [-0.04816748]\n",
      " [-0.0714883 ]\n",
      " ...\n",
      " [ 0.05729395]\n",
      " [ 0.02428793]\n",
      " [-0.09909567]]\n",
      "t [[ 0.07572441]\n",
      " [-0.04816748]\n",
      " [-0.0714883 ]\n",
      " ...\n",
      " [ 0.05729395]\n",
      " [ 0.02428793]\n",
      " [-0.09909567]]\n",
      "t [[ 0.08596502]\n",
      " [-0.05542132]\n",
      " [-0.08131766]\n",
      " ...\n",
      " [ 0.06467023]\n",
      " [ 0.02744393]\n",
      " [-0.11224674]]\n",
      "t [[ 0.08596502]\n",
      " [-0.05542132]\n",
      " [-0.08131766]\n",
      " ...\n",
      " [ 0.06467023]\n",
      " [ 0.02744393]\n",
      " [-0.11224674]]\n",
      "Current iteration=8, loss=49922.612919930776\n",
      "t [[ 0.09606777]\n",
      " [-0.06275463]\n",
      " [-0.09105568]\n",
      " ...\n",
      " [ 0.07185909]\n",
      " [ 0.03052611]\n",
      " [-0.12516498]]\n",
      "t [[ 0.09606777]\n",
      " [-0.06275463]\n",
      " [-0.09105568]\n",
      " ...\n",
      " [ 0.07185909]\n",
      " [ 0.03052611]\n",
      " [-0.12516498]]\n",
      "t [[ 0.10603476]\n",
      " [-0.07016316]\n",
      " [-0.1007038 ]\n",
      " ...\n",
      " [ 0.07886514]\n",
      " [ 0.03353595]\n",
      " [-0.13785624]]\n",
      "loss=49464.11590500273\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01112149]\n",
      " [-0.00628704]\n",
      " [-0.01047141]\n",
      " ...\n",
      " [ 0.00896338]\n",
      " [ 0.00378455]\n",
      " [-0.01541644]]\n",
      "t [[ 0.01112149]\n",
      " [-0.00628704]\n",
      " [-0.01047141]\n",
      " ...\n",
      " [ 0.00896338]\n",
      " [ 0.00378455]\n",
      " [-0.01541644]]\n",
      "t [[ 0.02209199]\n",
      " [-0.01268752]\n",
      " [-0.02084083]\n",
      " ...\n",
      " [ 0.01769771]\n",
      " [ 0.00748326]\n",
      " [-0.03054691]]\n",
      "t [[ 0.02209199]\n",
      " [-0.01268752]\n",
      " [-0.02084083]\n",
      " ...\n",
      " [ 0.01769771]\n",
      " [ 0.00748326]\n",
      " [-0.03054691]]\n",
      "Current iteration=2, loss=51409.953588081305\n",
      "t [[ 0.03291367]\n",
      " [-0.01919622]\n",
      " [-0.03110985]\n",
      " ...\n",
      " [ 0.02620856]\n",
      " [ 0.0110978 ]\n",
      " [-0.0453984 ]]\n",
      "t [[ 0.03291367]\n",
      " [-0.01919622]\n",
      " [-0.03110985]\n",
      " ...\n",
      " [ 0.02620856]\n",
      " [ 0.0110978 ]\n",
      " [-0.0453984 ]]\n",
      "t [[ 0.04358872]\n",
      " [-0.02580806]\n",
      " [-0.04128005]\n",
      " ...\n",
      " [ 0.03450137]\n",
      " [ 0.01462981]\n",
      " [-0.05997777]]\n",
      "t [[ 0.04358872]\n",
      " [-0.02580806]\n",
      " [-0.04128005]\n",
      " ...\n",
      " [ 0.03450137]\n",
      " [ 0.01462981]\n",
      " [-0.05997777]]\n",
      "Current iteration=4, loss=50900.090290211665\n",
      "t [[ 0.05411929]\n",
      " [-0.03251811]\n",
      " [-0.05135296]\n",
      " ...\n",
      " [ 0.04258146]\n",
      " [ 0.01808089]\n",
      " [-0.07429172]]\n",
      "t [[ 0.05411929]\n",
      " [-0.03251811]\n",
      " [-0.05135296]\n",
      " ...\n",
      " [ 0.04258146]\n",
      " [ 0.01808089]\n",
      " [-0.07429172]]\n",
      "t [[ 0.06450753]\n",
      " [-0.03932156]\n",
      " [-0.06133014]\n",
      " ...\n",
      " [ 0.05045407]\n",
      " [ 0.02145265]\n",
      " [-0.08834682]]\n",
      "t [[ 0.06450753]\n",
      " [-0.03932156]\n",
      " [-0.06133014]\n",
      " ...\n",
      " [ 0.05045407]\n",
      " [ 0.02145265]\n",
      " [-0.08834682]]\n",
      "Current iteration=6, loss=50409.60833437936\n",
      "t [[ 0.07475558]\n",
      " [-0.04621376]\n",
      " [-0.07121309]\n",
      " ...\n",
      " [ 0.0581243 ]\n",
      " [ 0.02474665]\n",
      " [-0.10214951]]\n",
      "t [[ 0.07475558]\n",
      " [-0.04621376]\n",
      " [-0.07121309]\n",
      " ...\n",
      " [ 0.0581243 ]\n",
      " [ 0.02474665]\n",
      " [-0.10214951]]\n",
      "t [[ 0.08486555]\n",
      " [-0.05319019]\n",
      " [-0.08100329]\n",
      " ...\n",
      " [ 0.06559714]\n",
      " [ 0.02796444]\n",
      " [-0.11570606]]\n",
      "t [[ 0.08486555]\n",
      " [-0.05319019]\n",
      " [-0.08100329]\n",
      " ...\n",
      " [ 0.06559714]\n",
      " [ 0.02796444]\n",
      " [-0.11570606]]\n",
      "Current iteration=8, loss=49937.48370033768\n",
      "t [[ 0.09483953]\n",
      " [-0.06024646]\n",
      " [-0.09070221]\n",
      " ...\n",
      " [ 0.07287749]\n",
      " [ 0.03110753]\n",
      " [-0.12902261]]\n",
      "t [[ 0.09483953]\n",
      " [-0.06024646]\n",
      " [-0.09070221]\n",
      " ...\n",
      " [ 0.07287749]\n",
      " [ 0.03110753]\n",
      " [-0.12902261]]\n",
      "t [[ 0.10467961]\n",
      " [-0.06737832]\n",
      " [-0.10031129]\n",
      " ...\n",
      " [ 0.07997009]\n",
      " [ 0.03417742]\n",
      " [-0.14210515]]\n",
      "loss=49482.756956635756\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.01110369]\n",
      " [-0.00660517]\n",
      " [-0.01043852]\n",
      " ...\n",
      " [ 0.01599281]\n",
      " [-0.01043852]\n",
      " [ 0.01497953]]\n",
      "t [[ 0.01110369]\n",
      " [-0.00660517]\n",
      " [-0.01043852]\n",
      " ...\n",
      " [ 0.01599281]\n",
      " [-0.01043852]\n",
      " [ 0.01497953]]\n",
      "t [[ 0.02205666]\n",
      " [-0.01331757]\n",
      " [-0.02077594]\n",
      " ...\n",
      " [ 0.03174161]\n",
      " [-0.02077594]\n",
      " [ 0.02969097]]\n",
      "t [[ 0.02205666]\n",
      " [-0.01331757]\n",
      " [-0.02077594]\n",
      " ...\n",
      " [ 0.03174161]\n",
      " [-0.02077594]\n",
      " [ 0.02969097]]\n",
      "Current iteration=2, loss=51411.10536302588\n",
      "t [[ 0.03286113]\n",
      " [-0.02013213]\n",
      " [-0.03101382]\n",
      " ...\n",
      " [ 0.04725061]\n",
      " [-0.03101382]\n",
      " [ 0.04413963]]\n",
      "t [[ 0.03286113]\n",
      " [-0.02013213]\n",
      " [-0.03101382]\n",
      " ...\n",
      " [ 0.04725061]\n",
      " [-0.03101382]\n",
      " [ 0.04413963]]\n",
      "t [[ 0.04351928]\n",
      " [-0.02704394]\n",
      " [-0.0411537 ]\n",
      " ...\n",
      " [ 0.06252397]\n",
      " [-0.0411537 ]\n",
      " [ 0.05833071]]\n",
      "t [[ 0.04351928]\n",
      " [-0.02704394]\n",
      " [-0.0411537 ]\n",
      " ...\n",
      " [ 0.06252397]\n",
      " [-0.0411537 ]\n",
      " [ 0.05833071]]\n",
      "Current iteration=4, loss=50902.17872414069\n",
      "t [[ 0.0540333 ]\n",
      " [-0.03404819]\n",
      " [-0.05119712]\n",
      " ...\n",
      " [ 0.0775658 ]\n",
      " [-0.05119712]\n",
      " [ 0.07226935]]\n",
      "t [[ 0.0540333 ]\n",
      " [-0.03404819]\n",
      " [-0.05119712]\n",
      " ...\n",
      " [ 0.0775658 ]\n",
      " [-0.05119712]\n",
      " [ 0.07226935]]\n",
      "t [[ 0.06440534]\n",
      " [-0.04114026]\n",
      " [-0.06114557]\n",
      " ...\n",
      " [ 0.09238015]\n",
      " [-0.06114557]\n",
      " [ 0.08596061]]\n",
      "t [[ 0.06440534]\n",
      " [-0.04114026]\n",
      " [-0.06114557]\n",
      " ...\n",
      " [ 0.09238015]\n",
      " [-0.06114557]\n",
      " [ 0.08596061]]\n",
      "Current iteration=6, loss=50412.44149512727\n",
      "t [[ 0.07463755]\n",
      " [-0.04831562]\n",
      " [-0.07100054]\n",
      " ...\n",
      " [ 0.10697103]\n",
      " [-0.07100054]\n",
      " [ 0.09940947]]\n",
      "t [[ 0.07463755]\n",
      " [-0.04831562]\n",
      " [-0.07100054]\n",
      " ...\n",
      " [ 0.10697103]\n",
      " [-0.07100054]\n",
      " [ 0.09940947]]\n",
      "t [[ 0.08473205]\n",
      " [-0.0555699 ]\n",
      " [-0.0807635 ]\n",
      " ...\n",
      " [ 0.12134239]\n",
      " [-0.0807635 ]\n",
      " [ 0.11262081]]\n",
      "t [[ 0.08473205]\n",
      " [-0.0555699 ]\n",
      " [-0.0807635 ]\n",
      " ...\n",
      " [ 0.12134239]\n",
      " [-0.0807635 ]\n",
      " [ 0.11262081]]\n",
      "Current iteration=8, loss=49940.890668424654\n",
      "t [[ 0.09469094]\n",
      " [-0.06289887]\n",
      " [-0.09043588]\n",
      " ...\n",
      " [ 0.13549809]\n",
      " [-0.09043588]\n",
      " [ 0.12559944]]\n",
      "t [[ 0.09469094]\n",
      " [-0.06289887]\n",
      " [-0.09043588]\n",
      " ...\n",
      " [ 0.13549809]\n",
      " [-0.09043588]\n",
      " [ 0.12559944]]\n",
      "t [[ 0.10451629]\n",
      " [-0.07029841]\n",
      " [-0.10001911]\n",
      " ...\n",
      " [ 0.14944199]\n",
      " [-0.10001911]\n",
      " [ 0.13835007]]\n",
      "loss=49486.58582960366\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.00559605]\n",
      " [-0.02679669]\n",
      " [-0.01571389]\n",
      " ...\n",
      " [ 0.01343746]\n",
      " [ 0.00559605]\n",
      " [-0.02277805]]\n",
      "t [[ 0.00559605]\n",
      " [-0.02679669]\n",
      " [-0.01571389]\n",
      " ...\n",
      " [ 0.01343746]\n",
      " [ 0.00559605]\n",
      " [-0.02277805]]\n",
      "t [[ 0.01100192]\n",
      " [-0.05329258]\n",
      " [-0.03120071]\n",
      " ...\n",
      " [ 0.0263697 ]\n",
      " [ 0.01100192]\n",
      " [-0.04493124]]\n",
      "t [[ 0.01100192]\n",
      " [-0.05329258]\n",
      " [-0.03120071]\n",
      " ...\n",
      " [ 0.0263697 ]\n",
      " [ 0.01100192]\n",
      " [-0.04493124]]\n",
      "Current iteration=2, loss=51147.13448441545\n",
      "t [[ 0.01622311]\n",
      " [-0.07949009]\n",
      " [-0.04646577]\n",
      " ...\n",
      " [ 0.03881508]\n",
      " [ 0.01622311]\n",
      " [-0.06648247]]\n",
      "t [[ 0.01622311]\n",
      " [-0.07949009]\n",
      " [-0.04646577]\n",
      " ...\n",
      " [ 0.03881508]\n",
      " [ 0.01622311]\n",
      " [-0.06648247]]\n",
      "t [[ 0.02126504]\n",
      " [-0.10539172]\n",
      " [-0.06151424]\n",
      " ...\n",
      " [ 0.05079136]\n",
      " [ 0.02126504]\n",
      " [-0.08745393]]\n",
      "t [[ 0.02126504]\n",
      " [-0.10539172]\n",
      " [-0.06151424]\n",
      " ...\n",
      " [ 0.05079136]\n",
      " [ 0.02126504]\n",
      " [-0.08745393]]\n",
      "Current iteration=4, loss=50398.71526095763\n",
      "t [[ 0.02613298]\n",
      " [-0.1310001 ]\n",
      " [-0.07635119]\n",
      " ...\n",
      " [ 0.06231575]\n",
      " [ 0.02613298]\n",
      " [-0.1078671 ]]\n",
      "t [[ 0.02613298]\n",
      " [-0.1310001 ]\n",
      " [-0.07635119]\n",
      " ...\n",
      " [ 0.06231575]\n",
      " [ 0.02613298]\n",
      " [-0.1078671 ]]\n",
      "t [[ 0.0308321 ]\n",
      " [-0.15631794]\n",
      " [-0.09098161]\n",
      " ...\n",
      " [ 0.07340487]\n",
      " [ 0.0308321 ]\n",
      " [-0.12774271]]\n",
      "t [[ 0.0308321 ]\n",
      " [-0.15631794]\n",
      " [-0.09098161]\n",
      " ...\n",
      " [ 0.07340487]\n",
      " [ 0.0308321 ]\n",
      " [-0.12774271]]\n",
      "Current iteration=6, loss=49691.56442517912\n",
      "t [[ 0.03536743]\n",
      " [-0.18134803]\n",
      " [-0.10541032]\n",
      " ...\n",
      " [ 0.08407475]\n",
      " [ 0.03536743]\n",
      " [-0.1471008 ]]\n",
      "t [[ 0.03536743]\n",
      " [-0.18134803]\n",
      " [-0.10541032]\n",
      " ...\n",
      " [ 0.08407475]\n",
      " [ 0.03536743]\n",
      " [-0.1471008 ]]\n",
      "t [[ 0.03974389]\n",
      " [-0.20609324]\n",
      " [-0.11964208]\n",
      " ...\n",
      " [ 0.09434086]\n",
      " [ 0.03974389]\n",
      " [-0.16596068]]\n",
      "t [[ 0.03974389]\n",
      " [-0.20609324]\n",
      " [-0.11964208]\n",
      " ...\n",
      " [ 0.09434086]\n",
      " [ 0.03974389]\n",
      " [-0.16596068]]\n",
      "Current iteration=8, loss=49022.53277985993\n",
      "t [[ 0.04396626]\n",
      " [-0.23055651]\n",
      " [-0.1336815 ]\n",
      " ...\n",
      " [ 0.10421809]\n",
      " [ 0.04396626]\n",
      " [-0.18434095]]\n",
      "t [[ 0.04396626]\n",
      " [-0.23055651]\n",
      " [-0.1336815 ]\n",
      " ...\n",
      " [ 0.10421809]\n",
      " [ 0.04396626]\n",
      " [-0.18434095]]\n",
      "t [[ 0.0480392 ]\n",
      " [-0.25474083]\n",
      " [-0.14753308]\n",
      " ...\n",
      " [ 0.11372076]\n",
      " [ 0.0480392 ]\n",
      " [-0.20225952]]\n",
      "loss=48388.76160986588\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.01689883]\n",
      " [-0.00985134]\n",
      " [-0.01576617]\n",
      " ...\n",
      " [ 0.01324078]\n",
      " [ 0.00557408]\n",
      " [-0.02243381]]\n",
      "t [[ 0.01689883]\n",
      " [-0.00985134]\n",
      " [-0.01576617]\n",
      " ...\n",
      " [ 0.01324078]\n",
      " [ 0.00557408]\n",
      " [-0.02243381]]\n",
      "t [[ 0.03345336]\n",
      " [-0.01995676]\n",
      " [-0.03130286]\n",
      " ...\n",
      " [ 0.02597996]\n",
      " [ 0.01095735]\n",
      " [-0.04424311]]\n",
      "t [[ 0.03345336]\n",
      " [-0.01995676]\n",
      " [-0.03130286]\n",
      " ...\n",
      " [ 0.02597996]\n",
      " [ 0.01095735]\n",
      " [-0.04424311]]\n",
      "Current iteration=2, loss=51145.14464651521\n",
      "t [[ 0.04967106]\n",
      " [-0.03029868]\n",
      " [-0.04661543]\n",
      " ...\n",
      " [ 0.03823585]\n",
      " [ 0.01615535]\n",
      " [-0.06545095]]\n",
      "t [[ 0.04967106]\n",
      " [-0.03029868]\n",
      " [-0.04661543]\n",
      " ...\n",
      " [ 0.03823585]\n",
      " [ 0.01615535]\n",
      " [-0.06545095]]\n",
      "t [[ 0.0655594 ]\n",
      " [-0.04086028]\n",
      " [-0.06170917]\n",
      " ...\n",
      " [ 0.05002616]\n",
      " [ 0.02117353]\n",
      " [-0.08607969]]\n",
      "t [[ 0.0655594 ]\n",
      " [-0.04086028]\n",
      " [-0.06170917]\n",
      " ...\n",
      " [ 0.05002616]\n",
      " [ 0.02117353]\n",
      " [-0.08607969]]\n",
      "Current iteration=4, loss=50395.02754296204\n",
      "t [[ 0.08112571]\n",
      " [-0.0516254 ]\n",
      " [-0.07658921]\n",
      " ...\n",
      " [ 0.06136807]\n",
      " [ 0.02601719]\n",
      " [-0.10615096]]\n",
      "t [[ 0.08112571]\n",
      " [-0.0516254 ]\n",
      " [-0.07658921]\n",
      " ...\n",
      " [ 0.06136807]\n",
      " [ 0.02601719]\n",
      " [-0.10615096]]\n",
      "t [[ 0.09637729]\n",
      " [-0.06257863]\n",
      " [-0.09126061]\n",
      " ...\n",
      " [ 0.07227812]\n",
      " [ 0.03069153]\n",
      " [-0.12568565]]\n",
      "t [[ 0.09637729]\n",
      " [-0.06257863]\n",
      " [-0.09126061]\n",
      " ...\n",
      " [ 0.07227812]\n",
      " [ 0.03069153]\n",
      " [-0.12568565]]\n",
      "Current iteration=6, loss=49686.42909706644\n",
      "t [[ 0.11132132]\n",
      " [-0.07370522]\n",
      " [-0.10572829]\n",
      " ...\n",
      " [ 0.08277232]\n",
      " [ 0.03520164]\n",
      " [-0.14470392]]\n",
      "t [[ 0.11132132]\n",
      " [-0.07370522]\n",
      " [-0.10572829]\n",
      " ...\n",
      " [ 0.08277232]\n",
      " [ 0.03520164]\n",
      " [-0.14470392]]\n",
      "t [[ 0.12596487]\n",
      " [-0.08499111]\n",
      " [-0.11999705]\n",
      " ...\n",
      " [ 0.09286606]\n",
      " [ 0.03955243]\n",
      " [-0.1632252 ]]\n",
      "t [[ 0.12596487]\n",
      " [-0.08499111]\n",
      " [-0.11999705]\n",
      " ...\n",
      " [ 0.09286606]\n",
      " [ 0.03955243]\n",
      " [-0.1632252 ]]\n",
      "Current iteration=8, loss=49016.16406248758\n",
      "t [[ 0.14031491]\n",
      " [-0.09642289]\n",
      " [-0.1340716 ]\n",
      " ...\n",
      " [ 0.10257418]\n",
      " [ 0.04374874]\n",
      " [-0.18126822]]\n",
      "t [[ 0.14031491]\n",
      " [-0.09642289]\n",
      " [-0.1340716 ]\n",
      " ...\n",
      " [ 0.10257418]\n",
      " [ 0.04374874]\n",
      " [-0.18126822]]\n",
      "t [[ 0.1543783 ]\n",
      " [-0.10798781]\n",
      " [-0.1479565 ]\n",
      " ...\n",
      " [ 0.11191095]\n",
      " [ 0.04779524]\n",
      " [-0.198851  ]]\n",
      "loss=48381.342702145754\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01668224]\n",
      " [-0.00943057]\n",
      " [-0.01570712]\n",
      " ...\n",
      " [ 0.01344507]\n",
      " [ 0.00567682]\n",
      " [-0.02312466]]\n",
      "t [[ 0.01668224]\n",
      " [-0.00943057]\n",
      " [-0.01570712]\n",
      " ...\n",
      " [ 0.01344507]\n",
      " [ 0.00567682]\n",
      " [-0.02312466]]\n",
      "t [[ 0.03302475]\n",
      " [-0.01911635]\n",
      " [-0.03118476]\n",
      " ...\n",
      " [ 0.02637481]\n",
      " [ 0.01116052]\n",
      " [-0.04560591]]\n",
      "t [[ 0.03302475]\n",
      " [-0.01911635]\n",
      " [-0.03118476]\n",
      " ...\n",
      " [ 0.02637481]\n",
      " [ 0.01116052]\n",
      " [-0.04560591]]\n",
      "Current iteration=2, loss=51150.648271306796\n",
      "t [[ 0.04903494]\n",
      " [-0.02903972]\n",
      " [-0.04643833]\n",
      " ...\n",
      " [ 0.03880803]\n",
      " [ 0.01645673]\n",
      " [-0.06746743]]\n",
      "t [[ 0.04903494]\n",
      " [-0.02903972]\n",
      " [-0.04643833]\n",
      " ...\n",
      " [ 0.03880803]\n",
      " [ 0.01645673]\n",
      " [-0.06746743]]\n",
      "t [[ 0.06472018]\n",
      " [-0.03918373]\n",
      " [-0.06147312]\n",
      " ...\n",
      " [ 0.05076295]\n",
      " [ 0.02157097]\n",
      " [-0.08873217]]\n",
      "t [[ 0.06472018]\n",
      " [-0.03918373]\n",
      " [-0.06147312]\n",
      " ...\n",
      " [ 0.05076295]\n",
      " [ 0.02157097]\n",
      " [-0.08873217]]\n",
      "Current iteration=4, loss=50406.1320536689\n",
      "t [[ 0.08008775]\n",
      " [-0.04953221]\n",
      " [-0.07629431]\n",
      " ...\n",
      " [ 0.06225725]\n",
      " [ 0.02650861]\n",
      " [-0.10942234]]\n",
      "t [[ 0.08008775]\n",
      " [-0.04953221]\n",
      " [-0.07629431]\n",
      " ...\n",
      " [ 0.06225725]\n",
      " [ 0.02650861]\n",
      " [-0.10942234]]\n",
      "t [[ 0.09514484]\n",
      " [-0.06006965]\n",
      " [-0.09090699]\n",
      " ...\n",
      " [ 0.07330797]\n",
      " [ 0.03127492]\n",
      " [-0.12955942]]\n",
      "t [[ 0.09514484]\n",
      " [-0.06006965]\n",
      " [-0.09090699]\n",
      " ...\n",
      " [ 0.07330797]\n",
      " [ 0.03127492]\n",
      " [-0.12955942]]\n",
      "Current iteration=6, loss=49703.183243552434\n",
      "t [[ 0.10989855]\n",
      " [-0.07078127]\n",
      " [-0.1053161 ]\n",
      " ...\n",
      " [ 0.0839316 ]\n",
      " [ 0.03587504]\n",
      " [-0.14916413]]\n",
      "t [[ 0.10989855]\n",
      " [-0.07078127]\n",
      " [-0.1053161 ]\n",
      " ...\n",
      " [ 0.0839316 ]\n",
      " [ 0.03587504]\n",
      " [-0.14916413]]\n",
      "t [[ 0.12435588]\n",
      " [-0.08165297]\n",
      " [-0.11952648]\n",
      " ...\n",
      " [ 0.09414401]\n",
      " [ 0.04031399]\n",
      " [-0.16825646]]\n",
      "t [[ 0.12435588]\n",
      " [-0.08165297]\n",
      " [-0.11952648]\n",
      " ...\n",
      " [ 0.09414401]\n",
      " [ 0.04031399]\n",
      " [-0.16825646]]\n",
      "Current iteration=8, loss=49038.57656371454\n",
      "t [[ 0.13852371]\n",
      " [-0.09267132]\n",
      " [-0.13354286]\n",
      " ...\n",
      " [ 0.10396052]\n",
      " [ 0.04459663]\n",
      " [-0.18685567]]\n",
      "t [[ 0.13852371]\n",
      " [-0.09267132]\n",
      " [-0.13354286]\n",
      " ...\n",
      " [ 0.10396052]\n",
      " [ 0.04459663]\n",
      " [-0.18685567]]\n",
      "t [[ 0.1524088 ]\n",
      " [-0.10382353]\n",
      " [-0.14736981]\n",
      " ...\n",
      " [ 0.11339586]\n",
      " [ 0.04872771]\n",
      " [-0.2049803 ]]\n",
      "loss=48409.38969730936\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.01665554]\n",
      " [-0.00990775]\n",
      " [-0.01565778]\n",
      " ...\n",
      " [ 0.02398921]\n",
      " [-0.01565778]\n",
      " [ 0.0224693 ]]\n",
      "t [[ 0.01665554]\n",
      " [-0.00990775]\n",
      " [-0.01565778]\n",
      " ...\n",
      " [ 0.02398921]\n",
      " [-0.01565778]\n",
      " [ 0.0224693 ]]\n",
      "t [[ 0.03297197]\n",
      " [-0.02005676]\n",
      " [-0.03108809]\n",
      " ...\n",
      " [ 0.04742942]\n",
      " [-0.03108809]\n",
      " [ 0.04433542]]\n",
      "t [[ 0.03297197]\n",
      " [-0.02005676]\n",
      " [-0.03108809]\n",
      " ...\n",
      " [ 0.04742942]\n",
      " [-0.03108809]\n",
      " [ 0.04433542]]\n",
      "Current iteration=2, loss=51152.3131770457\n",
      "t [[ 0.04895679]\n",
      " [-0.03042991]\n",
      " [-0.04629622]\n",
      " ...\n",
      " [ 0.0703349 ]\n",
      " [-0.04629622]\n",
      " [ 0.06561631]]\n",
      "t [[ 0.04895679]\n",
      " [-0.03042991]\n",
      " [-0.04629622]\n",
      " ...\n",
      " [ 0.0703349 ]\n",
      " [-0.04629622]\n",
      " [ 0.06561631]]\n",
      "t [[ 0.06461743]\n",
      " [-0.04101079]\n",
      " [-0.06128739]\n",
      " ...\n",
      " [ 0.09271967]\n",
      " [-0.06128739]\n",
      " [ 0.08632951]]\n",
      "t [[ 0.06461743]\n",
      " [-0.04101079]\n",
      " [-0.06128739]\n",
      " ...\n",
      " [ 0.09271967]\n",
      " [-0.06128739]\n",
      " [ 0.08632951]]\n",
      "Current iteration=4, loss=50408.99799575459\n",
      "t [[ 0.0799612 ]\n",
      " [-0.0517837 ]\n",
      " [-0.07606668]\n",
      " ...\n",
      " [ 0.11459744]\n",
      " [-0.07606668]\n",
      " [ 0.10649213]]\n",
      "t [[ 0.0799612 ]\n",
      " [-0.0517837 ]\n",
      " [-0.07606668]\n",
      " ...\n",
      " [ 0.11459744]\n",
      " [-0.07606668]\n",
      " [ 0.10649213]]\n",
      "t [[ 0.09499532]\n",
      " [-0.06273367]\n",
      " [-0.09063909]\n",
      " ...\n",
      " [ 0.13598166]\n",
      " [-0.09063909]\n",
      " [ 0.12612084]]\n",
      "t [[ 0.09499532]\n",
      " [-0.06273367]\n",
      " [-0.09063909]\n",
      " ...\n",
      " [ 0.13598166]\n",
      " [-0.09063909]\n",
      " [ 0.12612084]]\n",
      "Current iteration=6, loss=49706.86050201656\n",
      "t [[ 0.10972689]\n",
      " [-0.0738464 ]\n",
      " [-0.10500948]\n",
      " ...\n",
      " [ 0.15688548]\n",
      " [-0.10500948]\n",
      " [ 0.14523188]]\n",
      "t [[ 0.10972689]\n",
      " [-0.0738464 ]\n",
      " [-0.10500948]\n",
      " ...\n",
      " [ 0.15688548]\n",
      " [-0.10500948]\n",
      " [ 0.14523188]]\n",
      "t [[ 0.12416289]\n",
      " [-0.08510828]\n",
      " [-0.11918262]\n",
      " ...\n",
      " [ 0.17732173]\n",
      " [-0.11918262]\n",
      " [ 0.16384104]]\n",
      "t [[ 0.12416289]\n",
      " [-0.08510828]\n",
      " [-0.11918262]\n",
      " ...\n",
      " [ 0.17732173]\n",
      " [-0.11918262]\n",
      " [ 0.16384104]]\n",
      "Current iteration=8, loss=49042.739186660525\n",
      "t [[ 0.13831018]\n",
      " [-0.09650636]\n",
      " [-0.13316315]\n",
      " ...\n",
      " [ 0.19730294]\n",
      " [-0.13316315]\n",
      " [ 0.18196366]]\n",
      "t [[ 0.13831018]\n",
      " [-0.09650636]\n",
      " [-0.13316315]\n",
      " ...\n",
      " [ 0.19730294]\n",
      " [-0.13316315]\n",
      " [ 0.18196366]]\n",
      "t [[ 0.15217547]\n",
      " [-0.10802832]\n",
      " [-0.14695558]\n",
      " ...\n",
      " [ 0.21684131]\n",
      " [-0.14695558]\n",
      " [ 0.19961463]]\n",
      "loss=48413.766435818296\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0074614 ]\n",
      " [-0.03572892]\n",
      " [-0.02095185]\n",
      " ...\n",
      " [ 0.01791661]\n",
      " [ 0.0074614 ]\n",
      " [-0.03037074]]\n",
      "t [[ 0.0074614 ]\n",
      " [-0.03572892]\n",
      " [-0.02095185]\n",
      " ...\n",
      " [ 0.01791661]\n",
      " [ 0.0074614 ]\n",
      " [-0.03037074]]\n",
      "t [[ 0.01458472]\n",
      " [-0.07092311]\n",
      " [-0.04150005]\n",
      " ...\n",
      " [ 0.03493511]\n",
      " [ 0.01458472]\n",
      " [-0.05963066]]\n",
      "t [[ 0.01458472]\n",
      " [-0.07092311]\n",
      " [-0.04150005]\n",
      " ...\n",
      " [ 0.03493511]\n",
      " [ 0.01458472]\n",
      " [-0.05963066]]\n",
      "Current iteration=2, loss=50890.42293954559\n",
      "t [[ 0.02138306]\n",
      " [-0.10558833]\n",
      " [-0.06165721]\n",
      " ...\n",
      " [ 0.05109909]\n",
      " [ 0.02138306]\n",
      " [-0.08783424]]\n",
      "t [[ 0.02138306]\n",
      " [-0.10558833]\n",
      " [-0.06165721]\n",
      " ...\n",
      " [ 0.05109909]\n",
      " [ 0.02138306]\n",
      " [-0.08783424]]\n",
      "t [[ 0.02786919]\n",
      " [-0.13973069]\n",
      " [-0.08143555]\n",
      " ...\n",
      " [ 0.06645032]\n",
      " [ 0.02786919]\n",
      " [-0.1150336 ]]\n",
      "t [[ 0.02786919]\n",
      " [-0.13973069]\n",
      " [-0.08143555]\n",
      " ...\n",
      " [ 0.06645032]\n",
      " [ 0.02786919]\n",
      " [-0.1150336 ]]\n",
      "Current iteration=4, loss=49918.49329051089\n",
      "t [[ 0.03405547]\n",
      " [-0.17335658]\n",
      " [-0.10084698]\n",
      " ...\n",
      " [ 0.08102868]\n",
      " [ 0.03405547]\n",
      " [-0.14127857]]\n",
      "t [[ 0.03405547]\n",
      " [-0.17335658]\n",
      " [-0.10084698]\n",
      " ...\n",
      " [ 0.08102868]\n",
      " [ 0.03405547]\n",
      " [-0.14127857]]\n",
      "t [[ 0.03995386]\n",
      " [-0.2064727 ]\n",
      " [-0.11990301]\n",
      " ...\n",
      " [ 0.09487218]\n",
      " [ 0.03995386]\n",
      " [-0.16661662]]\n",
      "t [[ 0.03995386]\n",
      " [-0.2064727 ]\n",
      " [-0.11990301]\n",
      " ...\n",
      " [ 0.09487218]\n",
      " [ 0.03995386]\n",
      " [-0.16661662]]\n",
      "Current iteration=6, loss=49016.56523323331\n",
      "t [[ 0.0455759 ]\n",
      " [-0.23908593]\n",
      " [-0.13861479]\n",
      " ...\n",
      " [ 0.108017  ]\n",
      " [ 0.0455759 ]\n",
      " [-0.19109296]]\n",
      "t [[ 0.0455759 ]\n",
      " [-0.23908593]\n",
      " [-0.13861479]\n",
      " ...\n",
      " [ 0.108017  ]\n",
      " [ 0.0455759 ]\n",
      " [-0.19109296]]\n",
      "t [[ 0.05093275]\n",
      " [-0.27120339]\n",
      " [-0.15699308]\n",
      " ...\n",
      " [ 0.12049749]\n",
      " [ 0.05093275]\n",
      " [-0.21475052]]\n",
      "t [[ 0.05093275]\n",
      " [-0.27120339]\n",
      " [-0.15699308]\n",
      " ...\n",
      " [ 0.12049749]\n",
      " [ 0.05093275]\n",
      " [-0.21475052]]\n",
      "Current iteration=8, loss=48177.66971296611\n",
      "t [[ 0.05603511]\n",
      " [-0.30283233]\n",
      " [-0.17504829]\n",
      " ...\n",
      " [ 0.13234626]\n",
      " [ 0.05603511]\n",
      " [-0.23763007]]\n",
      "t [[ 0.05603511]\n",
      " [-0.30283233]\n",
      " [-0.17504829]\n",
      " ...\n",
      " [ 0.13234626]\n",
      " [ 0.05603511]\n",
      " [-0.23763007]]\n",
      "t [[ 0.06089332]\n",
      " [-0.33398016]\n",
      " [-0.19279041]\n",
      " ...\n",
      " [ 0.14359425]\n",
      " [ 0.06089332]\n",
      " [-0.25977025]]\n",
      "loss=47395.684209349325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.02253178]\n",
      " [-0.01313512]\n",
      " [-0.02102156]\n",
      " ...\n",
      " [ 0.01765437]\n",
      " [ 0.00743211]\n",
      " [-0.02991175]]\n",
      "t [[ 0.02253178]\n",
      " [-0.01313512]\n",
      " [-0.02102156]\n",
      " ...\n",
      " [ 0.01765437]\n",
      " [ 0.00743211]\n",
      " [-0.02991175]]\n",
      "t [[ 0.04445148]\n",
      " [-0.0267219 ]\n",
      " [-0.04163517]\n",
      " ...\n",
      " [ 0.03441706]\n",
      " [ 0.01452501]\n",
      " [-0.0587133 ]]\n",
      "t [[ 0.04445148]\n",
      " [-0.0267219 ]\n",
      " [-0.04163517]\n",
      " ...\n",
      " [ 0.03441706]\n",
      " [ 0.01452501]\n",
      " [-0.0587133 ]]\n",
      "Current iteration=2, loss=50887.82116219625\n",
      "t [[ 0.06577698]\n",
      " [-0.04071863]\n",
      " [-0.06185362]\n",
      " ...\n",
      " [ 0.05033155]\n",
      " [ 0.02129191]\n",
      " [-0.0864595 ]]\n",
      "t [[ 0.06577698]\n",
      " [-0.04071863]\n",
      " [-0.06185362]\n",
      " ...\n",
      " [ 0.05033155]\n",
      " [ 0.02129191]\n",
      " [-0.0864595 ]]\n",
      "t [[ 0.08652594]\n",
      " [-0.05508592]\n",
      " [-0.08168935]\n",
      " ...\n",
      " [ 0.0654395 ]\n",
      " [ 0.02774564]\n",
      " [-0.11320288]]\n",
      "t [[ 0.08652594]\n",
      " [-0.05508592]\n",
      " [-0.08168935]\n",
      " ...\n",
      " [ 0.0654395 ]\n",
      " [ 0.02774564]\n",
      " [-0.11320288]]\n",
      "Current iteration=4, loss=49913.78939660083\n",
      "t [[ 0.10671574]\n",
      " [-0.06978663]\n",
      " [-0.10115442]\n",
      " ...\n",
      " [ 0.07978067]\n",
      " [ 0.03389865]\n",
      " [-0.13899359]]\n",
      "t [[ 0.10671574]\n",
      " [-0.06978663]\n",
      " [-0.10115442]\n",
      " ...\n",
      " [ 0.07978067]\n",
      " [ 0.03389865]\n",
      " [-0.13899359]]\n",
      "t [[ 0.12636347]\n",
      " [-0.08478585]\n",
      " [-0.12026056]\n",
      " ...\n",
      " [ 0.09339294]\n",
      " [ 0.03976296]\n",
      " [-0.16387943]]\n",
      "t [[ 0.12636347]\n",
      " [-0.08478585]\n",
      " [-0.12026056]\n",
      " ...\n",
      " [ 0.09339294]\n",
      " [ 0.03976296]\n",
      " [-0.16387943]]\n",
      "Current iteration=6, loss=49010.165210782296\n",
      "t [[ 0.14548586]\n",
      " [-0.10005083]\n",
      " [-0.13901906]\n",
      " ...\n",
      " [ 0.10631236]\n",
      " [ 0.04535021]\n",
      " [-0.1879059 ]]\n",
      "t [[ 0.14548586]\n",
      " [-0.10005083]\n",
      " [-0.13901906]\n",
      " ...\n",
      " [ 0.10631236]\n",
      " [ 0.04535021]\n",
      " [-0.1879059 ]]\n",
      "t [[ 0.16409927]\n",
      " [-0.11555092]\n",
      " [-0.15744088]\n",
      " ...\n",
      " [ 0.11857315]\n",
      " [ 0.05067159]\n",
      " [-0.2111162 ]]\n",
      "t [[ 0.16409927]\n",
      " [-0.11555092]\n",
      " [-0.15744088]\n",
      " ...\n",
      " [ 0.11857315]\n",
      " [ 0.05067159]\n",
      " [-0.2111162 ]]\n",
      "Current iteration=8, loss=48169.902769163076\n",
      "t [[ 0.18221969]\n",
      " [-0.13125745]\n",
      " [-0.17553654]\n",
      " ...\n",
      " [ 0.1302078 ]\n",
      " [ 0.05573789]\n",
      " [-0.23355134]]\n",
      "t [[ 0.18221969]\n",
      " [-0.13125745]\n",
      " [-0.17553654]\n",
      " ...\n",
      " [ 0.1302078 ]\n",
      " [ 0.05573789]\n",
      " [-0.23355134]]\n",
      "t [[ 0.19986267]\n",
      " [-0.14714368]\n",
      " [-0.19331624]\n",
      " ...\n",
      " [ 0.1412471 ]\n",
      " [ 0.06055949]\n",
      " [-0.25525018]]\n",
      "loss=47386.817072618294\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02224299]\n",
      " [-0.01257409]\n",
      " [-0.02094283]\n",
      " ...\n",
      " [ 0.01792676]\n",
      " [ 0.00756909]\n",
      " [-0.03083288]]\n",
      "t [[ 0.02224299]\n",
      " [-0.01257409]\n",
      " [-0.02094283]\n",
      " ...\n",
      " [ 0.01792676]\n",
      " [ 0.00756909]\n",
      " [-0.03083288]]\n",
      "t [[ 0.04388204]\n",
      " [-0.02560187]\n",
      " [-0.04147771]\n",
      " ...\n",
      " [ 0.03493742]\n",
      " [ 0.01479489]\n",
      " [-0.06052199]]\n",
      "t [[ 0.04388204]\n",
      " [-0.02560187]\n",
      " [-0.04147771]\n",
      " ...\n",
      " [ 0.03493742]\n",
      " [ 0.01479489]\n",
      " [-0.06052199]]\n",
      "Current iteration=2, loss=50895.178710066175\n",
      "t [[ 0.06493484]\n",
      " [-0.03904145]\n",
      " [-0.06161752]\n",
      " ...\n",
      " [ 0.0510767 ]\n",
      " [ 0.02169077]\n",
      " [-0.08912362]]\n",
      "t [[ 0.06493484]\n",
      " [-0.03904145]\n",
      " [-0.06161752]\n",
      " ...\n",
      " [ 0.0510767 ]\n",
      " [ 0.02169077]\n",
      " [-0.08912362]]\n",
      "t [[ 0.08541886]\n",
      " [-0.05285326]\n",
      " [-0.08137476]\n",
      " ...\n",
      " [ 0.06638745]\n",
      " [ 0.02826975]\n",
      " [-0.11669172]]\n",
      "t [[ 0.08541886]\n",
      " [-0.05285326]\n",
      " [-0.08137476]\n",
      " ...\n",
      " [ 0.06638745]\n",
      " [ 0.02826975]\n",
      " [-0.11669172]]\n",
      "Current iteration=4, loss=49928.654530242544\n",
      "t [[ 0.10535129]\n",
      " [-0.06700002]\n",
      " [-0.10076158]\n",
      " ...\n",
      " [ 0.08091061]\n",
      " [ 0.03454443]\n",
      " [-0.14327784]]\n",
      "t [[ 0.10535129]\n",
      " [-0.06700002]\n",
      " [-0.10076158]\n",
      " ...\n",
      " [ 0.08091061]\n",
      " [ 0.03454443]\n",
      " [-0.14327784]]\n",
      "t [[ 0.12474902]\n",
      " [-0.08144671]\n",
      " [-0.11978975]\n",
      " ...\n",
      " [ 0.09468525]\n",
      " [ 0.04052701]\n",
      " [-0.16893113]]\n",
      "t [[ 0.12474902]\n",
      " [-0.08144671]\n",
      " [-0.11978975]\n",
      " ...\n",
      " [ 0.09468525]\n",
      " [ 0.04052701]\n",
      " [-0.16893113]]\n",
      "Current iteration=6, loss=49032.58074879649\n",
      "t [[ 0.14362857]\n",
      " [-0.09616052]\n",
      " [-0.13847065]\n",
      " ...\n",
      " [ 0.10774852]\n",
      " [ 0.04622927]\n",
      " [-0.19369837]]\n",
      "t [[ 0.14362857]\n",
      " [-0.09616052]\n",
      " [-0.13847065]\n",
      " ...\n",
      " [ 0.10774852]\n",
      " [ 0.04622927]\n",
      " [-0.19369837]]\n",
      "t [[ 0.1620061 ]\n",
      " [-0.1111107 ]\n",
      " [-0.15681527]\n",
      " ...\n",
      " [ 0.12013576]\n",
      " [ 0.05166256]\n",
      " [-0.21762403]]\n",
      "t [[ 0.1620061 ]\n",
      " [-0.1111107 ]\n",
      " [-0.15681527]\n",
      " ...\n",
      " [ 0.12013576]\n",
      " [ 0.05166256]\n",
      " [-0.21762403]]\n",
      "Current iteration=8, loss=48199.8295677561\n",
      "t [[ 0.17989739]\n",
      " [-0.12626857]\n",
      " [-0.17483422]\n",
      " ...\n",
      " [ 0.13188048]\n",
      " [ 0.05683781]\n",
      " [-0.24075029]]\n",
      "t [[ 0.17989739]\n",
      " [-0.12626857]\n",
      " [-0.17483422]\n",
      " ...\n",
      " [ 0.13188048]\n",
      " [ 0.05683781]\n",
      " [-0.24075029]]\n",
      "t [[ 0.19731779]\n",
      " [-0.14160736]\n",
      " [-0.19253771]\n",
      " ...\n",
      " [ 0.14301447]\n",
      " [ 0.06176553]\n",
      " [-0.26311716]]\n",
      "loss=47424.15483128788\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.02220738]\n",
      " [-0.01321033]\n",
      " [-0.02087705]\n",
      " ...\n",
      " [ 0.03198562]\n",
      " [-0.02087705]\n",
      " [ 0.02995906]]\n",
      "t [[ 0.02220738]\n",
      " [-0.01321033]\n",
      " [-0.02087705]\n",
      " ...\n",
      " [ 0.03198562]\n",
      " [-0.02087705]\n",
      " [ 0.02995906]]\n",
      "t [[ 0.04381194]\n",
      " [-0.02684955]\n",
      " [-0.0413497 ]\n",
      " ...\n",
      " [ 0.06299528]\n",
      " [-0.0413497 ]\n",
      " [ 0.05884587]]\n",
      "t [[ 0.04381194]\n",
      " [-0.02684955]\n",
      " [-0.0413497 ]\n",
      " ...\n",
      " [ 0.06299528]\n",
      " [-0.0413497 ]\n",
      " [ 0.05884587]]\n",
      "Current iteration=2, loss=50897.31651399419\n",
      "t [[ 0.06483154]\n",
      " [-0.040877  ]\n",
      " [-0.06143061]\n",
      " ...\n",
      " [ 0.09306296]\n",
      " [-0.06143061]\n",
      " [ 0.08670313]]\n",
      "t [[ 0.06483154]\n",
      " [-0.040877  ]\n",
      " [-0.06143061]\n",
      " ...\n",
      " [ 0.09306296]\n",
      " [-0.06143061]\n",
      " [ 0.08670313]]\n",
      "t [[ 0.08528378]\n",
      " [-0.05525432]\n",
      " [-0.08113205]\n",
      " ...\n",
      " [ 0.12222177]\n",
      " [-0.08113205]\n",
      " [ 0.11357218]]\n",
      "t [[ 0.08528378]\n",
      " [-0.05525432]\n",
      " [-0.08113205]\n",
      " ...\n",
      " [ 0.12222177]\n",
      " [-0.08113205]\n",
      " [ 0.11357218]]\n",
      "Current iteration=4, loss=49932.140172717045\n",
      "t [[ 0.10518588]\n",
      " [-0.06994546]\n",
      " [-0.10046597]\n",
      " ...\n",
      " [ 0.15050389]\n",
      " [-0.10046597]\n",
      " [ 0.139493  ]]\n",
      "t [[ 0.10518588]\n",
      " [-0.06994546]\n",
      " [-0.10046597]\n",
      " ...\n",
      " [ 0.15050389]\n",
      " [-0.10046597]\n",
      " [ 0.139493  ]]\n",
      "t [[ 0.12455474]\n",
      " [-0.08491658]\n",
      " [-0.11944395]\n",
      " ...\n",
      " [ 0.17794051]\n",
      " [-0.11944395]\n",
      " [ 0.16450411]]\n",
      "t [[ 0.12455474]\n",
      " [-0.08491658]\n",
      " [-0.11944395]\n",
      " ...\n",
      " [ 0.17794051]\n",
      " [-0.11944395]\n",
      " [ 0.16450411]]\n",
      "Current iteration=6, loss=49036.79062727255\n",
      "t [[ 0.14340682]\n",
      " [-0.10013601]\n",
      " [-0.13807717]\n",
      " ...\n",
      " [ 0.20456186]\n",
      " [-0.13807717]\n",
      " [ 0.18864261]]\n",
      "t [[ 0.14340682]\n",
      " [-0.10013601]\n",
      " [-0.13807717]\n",
      " ...\n",
      " [ 0.20456186]\n",
      " [-0.13807717]\n",
      " [ 0.18864261]]\n",
      "t [[ 0.1617582 ]\n",
      " [-0.11557416]\n",
      " [-0.15637646]\n",
      " ...\n",
      " [ 0.23039714]\n",
      " [-0.15637646]\n",
      " [ 0.21194414]]\n",
      "t [[ 0.1617582 ]\n",
      " [-0.11557416]\n",
      " [-0.15637646]\n",
      " ...\n",
      " [ 0.23039714]\n",
      " [-0.15637646]\n",
      " [ 0.21194414]]\n",
      "Current iteration=8, loss=48204.27560371185\n",
      "t [[ 0.17962449]\n",
      " [-0.13120343]\n",
      " [-0.17435227]\n",
      " ...\n",
      " [ 0.25547455]\n",
      " [-0.17435227]\n",
      " [ 0.23444295]]\n",
      "t [[ 0.17962449]\n",
      " [-0.13120343]\n",
      " [-0.17435227]\n",
      " ...\n",
      " [ 0.25547455]\n",
      " [-0.17435227]\n",
      " [ 0.23444295]]\n",
      "t [[ 0.19702089]\n",
      " [-0.14699811]\n",
      " [-0.19201466]\n",
      " ...\n",
      " [ 0.27982129]\n",
      " [-0.19201466]\n",
      " [ 0.25617183]]\n",
      "loss=47428.45893262229\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.00932676]\n",
      " [-0.04466115]\n",
      " [-0.02618981]\n",
      " ...\n",
      " [ 0.02239576]\n",
      " [ 0.00932676]\n",
      " [-0.03796342]]\n",
      "t [[ 0.00932676]\n",
      " [-0.04466115]\n",
      " [-0.02618981]\n",
      " ...\n",
      " [ 0.02239576]\n",
      " [ 0.00932676]\n",
      " [-0.03796342]]\n",
      "t [[ 0.01812528]\n",
      " [-0.08848682]\n",
      " [-0.05174897]\n",
      " ...\n",
      " [ 0.04338831]\n",
      " [ 0.01812528]\n",
      " [-0.07419132]]\n",
      "t [[ 0.01812528]\n",
      " [-0.08848682]\n",
      " [-0.05174897]\n",
      " ...\n",
      " [ 0.04338831]\n",
      " [ 0.01812528]\n",
      " [-0.07419132]]\n",
      "Current iteration=2, loss=50637.459491719994\n",
      "t [[ 0.0264213 ]\n",
      " [-0.13148837]\n",
      " [-0.07670216]\n",
      " ...\n",
      " [ 0.06306301]\n",
      " [ 0.0264213 ]\n",
      " [-0.10879036]]\n",
      "t [[ 0.0264213 ]\n",
      " [-0.13148837]\n",
      " [-0.07670216]\n",
      " ...\n",
      " [ 0.06306301]\n",
      " [ 0.0264213 ]\n",
      " [-0.10879036]]\n",
      "t [[ 0.0342396 ]\n",
      " [-0.17367798]\n",
      " [-0.10107321]\n",
      " ...\n",
      " [ 0.08150063]\n",
      " [ 0.0342396 ]\n",
      " [-0.14186142]]\n",
      "t [[ 0.0342396 ]\n",
      " [-0.17367798]\n",
      " [-0.10107321]\n",
      " ...\n",
      " [ 0.08150063]\n",
      " [ 0.0342396 ]\n",
      " [-0.14186142]]\n",
      "Current iteration=4, loss=49453.967050045896\n",
      "t [[ 0.04160399]\n",
      " [-0.21506854]\n",
      " [-0.12488502]\n",
      " ...\n",
      " [ 0.09877726]\n",
      " [ 0.04160399]\n",
      " [-0.17349958]]\n",
      "t [[ 0.04160399]\n",
      " [-0.21506854]\n",
      " [-0.12488502]\n",
      " ...\n",
      " [ 0.09877726]\n",
      " [ 0.04160399]\n",
      " [-0.17349958]]\n",
      "t [[ 0.04853724]\n",
      " [-0.25567352]\n",
      " [-0.14815958]\n",
      " ...\n",
      " [ 0.11496438]\n",
      " [ 0.04853724]\n",
      " [-0.20379418]]\n",
      "t [[ 0.04853724]\n",
      " [-0.25567352]\n",
      " [-0.14815958]\n",
      " ...\n",
      " [ 0.11496438]\n",
      " [ 0.04853724]\n",
      " [-0.20379418]]\n",
      "Current iteration=6, loss=48374.85668658281\n",
      "t [[ 0.0550611 ]\n",
      " [-0.29550685]\n",
      " [-0.17091793]\n",
      " ...\n",
      " [ 0.13012899]\n",
      " [ 0.0550611 ]\n",
      " [-0.23282898]]\n",
      "t [[ 0.0550611 ]\n",
      " [-0.29550685]\n",
      " [-0.17091793]\n",
      " ...\n",
      " [ 0.13012899]\n",
      " [ 0.0550611 ]\n",
      " [-0.23282898]]\n",
      "t [[ 0.06119632]\n",
      " [-0.33458289]\n",
      " [-0.19318016]\n",
      " ...\n",
      " [ 0.14433379]\n",
      " [ 0.06119632]\n",
      " [-0.26068237]]\n",
      "t [[ 0.06119632]\n",
      " [-0.33458289]\n",
      " [-0.19318016]\n",
      " ...\n",
      " [ 0.14433379]\n",
      " [ 0.06119632]\n",
      " [-0.26068237]]\n",
      "Current iteration=8, loss=47387.428513181985\n",
      "t [[ 0.0669626 ]\n",
      " [-0.37291625]\n",
      " [-0.21496545]\n",
      " ...\n",
      " [ 0.15763744]\n",
      " [ 0.0669626 ]\n",
      " [-0.2874276 ]]\n",
      "t [[ 0.0669626 ]\n",
      " [-0.37291625]\n",
      " [-0.21496545]\n",
      " ...\n",
      " [ 0.15763744]\n",
      " [ 0.0669626 ]\n",
      " [-0.2874276 ]]\n",
      "t [[ 0.07237869]\n",
      " [-0.41052177]\n",
      " [-0.2362921 ]\n",
      " ...\n",
      " [ 0.17009477]\n",
      " [ 0.07237869]\n",
      " [-0.31313305]]\n",
      "loss=46480.87990746132\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.02816472]\n",
      " [-0.0164189 ]\n",
      " [-0.02627695]\n",
      " ...\n",
      " [ 0.02206797]\n",
      " [ 0.00929014]\n",
      " [-0.03738969]]\n",
      "t [[ 0.02816472]\n",
      " [-0.0164189 ]\n",
      " [-0.02627695]\n",
      " ...\n",
      " [ 0.02206797]\n",
      " [ 0.00929014]\n",
      " [-0.03738969]]\n",
      "t [[ 0.05537314]\n",
      " [-0.03354346]\n",
      " [-0.05191652]\n",
      " ...\n",
      " [ 0.04274276]\n",
      " [ 0.0180503 ]\n",
      " [-0.0730448 ]]\n",
      "t [[ 0.05537314]\n",
      " [-0.03354346]\n",
      " [-0.05191652]\n",
      " ...\n",
      " [ 0.04274276]\n",
      " [ 0.0180503 ]\n",
      " [-0.0730448 ]]\n",
      "Current iteration=2, loss=50634.270342145945\n",
      "t [[ 0.08166036]\n",
      " [-0.05129209]\n",
      " [-0.07694378]\n",
      " ...\n",
      " [ 0.06210953]\n",
      " [ 0.02630637]\n",
      " [-0.1070728 ]]\n",
      "t [[ 0.08166036]\n",
      " [-0.05129209]\n",
      " [-0.07694378]\n",
      " ...\n",
      " [ 0.06210953]\n",
      " [ 0.02630637]\n",
      " [-0.1070728 ]]\n",
      "t [[ 0.10706091]\n",
      " [-0.06958889]\n",
      " [-0.10138294]\n",
      " ...\n",
      " [ 0.08024884]\n",
      " [ 0.03408331]\n",
      " [-0.13957526]]\n",
      "t [[ 0.10706091]\n",
      " [-0.06958889]\n",
      " [-0.10138294]\n",
      " ...\n",
      " [ 0.08024884]\n",
      " [ 0.03408331]\n",
      " [-0.13957526]]\n",
      "Current iteration=4, loss=49448.34017753279\n",
      "t [[ 0.13160855]\n",
      " [-0.0883636 ]\n",
      " [-0.12525725]\n",
      " ...\n",
      " [ 0.09723651]\n",
      " [ 0.04140505]\n",
      " [-0.17064792]]\n",
      "t [[ 0.13160855]\n",
      " [-0.0883636 ]\n",
      " [-0.12525725]\n",
      " ...\n",
      " [ 0.09723651]\n",
      " [ 0.04140505]\n",
      " [-0.17064792]]\n",
      "t [[ 0.15533625]\n",
      " [-0.1075513 ]\n",
      " [-0.14858905]\n",
      " ...\n",
      " [ 0.11314377]\n",
      " [ 0.04829452]\n",
      " [-0.2003807 ]]\n",
      "t [[ 0.15533625]\n",
      " [-0.1075513 ]\n",
      " [-0.14858905]\n",
      " ...\n",
      " [ 0.11314377]\n",
      " [ 0.04829452]\n",
      " [-0.2003807 ]]\n",
      "Current iteration=6, loss=48367.37028399317\n",
      "t [[ 0.178276  ]\n",
      " [-0.12709228]\n",
      " [-0.17139969]\n",
      " ...\n",
      " [ 0.12803737]\n",
      " [ 0.0547736 ]\n",
      " [-0.22885789]]\n",
      "t [[ 0.178276  ]\n",
      " [-0.12709228]\n",
      " [-0.17139969]\n",
      " ...\n",
      " [ 0.12803737]\n",
      " [ 0.0547736 ]\n",
      " [-0.22885789]]\n",
      "t [[ 0.20045884]\n",
      " [-0.14693174]\n",
      " [-0.19370959]\n",
      " ...\n",
      " [ 0.14197977]\n",
      " [ 0.06086315]\n",
      " [-0.25615832]]\n",
      "t [[ 0.20045884]\n",
      " [-0.14693174]\n",
      " [-0.19370959]\n",
      " ...\n",
      " [ 0.14197977]\n",
      " [ 0.06086315]\n",
      " [-0.25615832]]\n",
      "Current iteration=8, loss=47378.526434457344\n",
      "t [[ 0.22191471]\n",
      " [-0.16701953]\n",
      " [-0.21553821]\n",
      " ...\n",
      " [ 0.15502937]\n",
      " [ 0.06658299]\n",
      " [-0.28235568]]\n",
      "t [[ 0.22191471]\n",
      " [-0.16701953]\n",
      " [-0.21553821]\n",
      " ...\n",
      " [ 0.15502937]\n",
      " [ 0.06658299]\n",
      " [-0.28235568]]\n",
      "t [[ 0.24267251]\n",
      " [-0.18730987]\n",
      " [-0.23690412]\n",
      " ...\n",
      " [ 0.16724078]\n",
      " [ 0.07195197]\n",
      " [-0.3075187 ]]\n",
      "loss=46470.902803270415\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02780374]\n",
      " [-0.01571761]\n",
      " [-0.02617854]\n",
      " ...\n",
      " [ 0.02240845]\n",
      " [ 0.00946136]\n",
      " [-0.0385411 ]]\n",
      "t [[ 0.02780374]\n",
      " [-0.01571761]\n",
      " [-0.02617854]\n",
      " ...\n",
      " [ 0.02240845]\n",
      " [ 0.00946136]\n",
      " [-0.0385411 ]]\n",
      "t [[ 0.05466388]\n",
      " [-0.03214407]\n",
      " [-0.0517197 ]\n",
      " ...\n",
      " [ 0.0433856 ]\n",
      " [ 0.01838636]\n",
      " [-0.07529518]]\n",
      "t [[ 0.05466388]\n",
      " [-0.03214407]\n",
      " [-0.0517197 ]\n",
      " ...\n",
      " [ 0.0433856 ]\n",
      " [ 0.01838636]\n",
      " [-0.07529518]]\n",
      "Current iteration=2, loss=50643.49016486426\n",
      "t [[ 0.08061517]\n",
      " [-0.04919739]\n",
      " [-0.07664872]\n",
      " ...\n",
      " [ 0.063019  ]\n",
      " [ 0.02680124]\n",
      " [-0.11037253]]\n",
      "t [[ 0.08061517]\n",
      " [-0.04919739]\n",
      " [-0.07664872]\n",
      " ...\n",
      " [ 0.063019  ]\n",
      " [ 0.02680124]\n",
      " [-0.11037253]]\n",
      "t [[ 0.10569176]\n",
      " [-0.06680138]\n",
      " [-0.10098993]\n",
      " ...\n",
      " [ 0.08139156]\n",
      " [ 0.03473128]\n",
      " [-0.14387753]]\n",
      "t [[ 0.10569176]\n",
      " [-0.06680138]\n",
      " [-0.10098993]\n",
      " ...\n",
      " [ 0.08139156]\n",
      " [ 0.03473128]\n",
      " [-0.14387753]]\n",
      "Current iteration=4, loss=49466.980007699996\n",
      "t [[ 0.12992703]\n",
      " [-0.08488551]\n",
      " [-0.12476675]\n",
      " ...\n",
      " [ 0.09858144]\n",
      " [ 0.04220075]\n",
      " [-0.1759086 ]]\n",
      "t [[ 0.12992703]\n",
      " [-0.08488551]\n",
      " [-0.12476675]\n",
      " ...\n",
      " [ 0.09858144]\n",
      " [ 0.04220075]\n",
      " [-0.1759086 ]]\n",
      "t [[ 0.15335353]\n",
      " [-0.10338472]\n",
      " [-0.14800162]\n",
      " ...\n",
      " [ 0.1146621 ]\n",
      " [ 0.04923287]\n",
      " [-0.20655821]]\n",
      "t [[ 0.15335353]\n",
      " [-0.10338472]\n",
      " [-0.14800162]\n",
      " ...\n",
      " [ 0.1146621 ]\n",
      " [ 0.04923287]\n",
      " [-0.20655821]]\n",
      "Current iteration=6, loss=48395.43555921479\n",
      "t [[ 0.17600286]\n",
      " [-0.12223918]\n",
      " [-0.17071602]\n",
      " ...\n",
      " [ 0.1297024 ]\n",
      " [ 0.05584983]\n",
      " [-0.23591309]]\n",
      "t [[ 0.17600286]\n",
      " [-0.12223918]\n",
      " [-0.17071602]\n",
      " ...\n",
      " [ 0.1297024 ]\n",
      " [ 0.05584983]\n",
      " [-0.23591309]]\n",
      "t [[ 0.19790565]\n",
      " [-0.14139401]\n",
      " [-0.19293044]\n",
      " ...\n",
      " [ 0.14376681]\n",
      " [ 0.06207274]\n",
      " [-0.26405437]]\n",
      "t [[ 0.19790565]\n",
      " [-0.14139401]\n",
      " [-0.19293044]\n",
      " ...\n",
      " [ 0.14376681]\n",
      " [ 0.06207274]\n",
      " [-0.26405437]]\n",
      "Current iteration=8, loss=47415.88549024683\n",
      "t [[ 0.21909149]\n",
      " [-0.16079903]\n",
      " [-0.21466446]\n",
      " ...\n",
      " [ 0.15691559]\n",
      " [ 0.06792169]\n",
      " [-0.29105786]]\n",
      "t [[ 0.21909149]\n",
      " [-0.16079903]\n",
      " [-0.21466446]\n",
      " ...\n",
      " [ 0.15691559]\n",
      " [ 0.06792169]\n",
      " [-0.29105786]]\n",
      "t [[ 0.23958888]\n",
      " [-0.18040845]\n",
      " [-0.23593669]\n",
      " ...\n",
      " [ 0.16920503]\n",
      " [ 0.07341577]\n",
      " [-0.31699432]]\n",
      "loss=46517.33112160947\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.02775923]\n",
      " [-0.01651292]\n",
      " [-0.02609631]\n",
      " ...\n",
      " [ 0.03998202]\n",
      " [-0.02609631]\n",
      " [ 0.03744883]]\n",
      "t [[ 0.02775923]\n",
      " [-0.01651292]\n",
      " [-0.02609631]\n",
      " ...\n",
      " [ 0.03998202]\n",
      " [-0.02609631]\n",
      " [ 0.03744883]]\n",
      "t [[ 0.0545766 ]\n",
      " [-0.03369592]\n",
      " [-0.0515608 ]\n",
      " ...\n",
      " [ 0.07843921]\n",
      " [-0.0515608 ]\n",
      " [ 0.07322237]]\n",
      "t [[ 0.0545766 ]\n",
      " [-0.03369592]\n",
      " [-0.0515608 ]\n",
      " ...\n",
      " [ 0.07843921]\n",
      " [-0.0515608 ]\n",
      " [ 0.07322237]]\n",
      "Current iteration=2, loss=50646.06183470582\n",
      "t [[ 0.0804872 ]\n",
      " [-0.05146947]\n",
      " [-0.07641824]\n",
      " ...\n",
      " [ 0.1154382 ]\n",
      " [-0.07641824]\n",
      " [ 0.10740433]]\n",
      "t [[ 0.0804872 ]\n",
      " [-0.05146947]\n",
      " [-0.07641824]\n",
      " ...\n",
      " [ 0.1154382 ]\n",
      " [-0.07641824]\n",
      " [ 0.10740433]]\n",
      "t [[ 0.10552533]\n",
      " [-0.06975979]\n",
      " [-0.10069257]\n",
      " ...\n",
      " [ 0.15104341]\n",
      " [-0.10069257]\n",
      " [ 0.14007502]]\n",
      "t [[ 0.10552533]\n",
      " [-0.06975979]\n",
      " [-0.10069257]\n",
      " ...\n",
      " [ 0.15104341]\n",
      " [-0.10069257]\n",
      " [ 0.14007502]]\n",
      "Current iteration=4, loss=49470.94148664939\n",
      "t [[ 0.12972442]\n",
      " [-0.08849869]\n",
      " [-0.12440678]\n",
      " ...\n",
      " [ 0.18531685]\n",
      " [-0.12440678]\n",
      " [ 0.1713112 ]]\n",
      "t [[ 0.12972442]\n",
      " [-0.08849869]\n",
      " [-0.12440678]\n",
      " ...\n",
      " [ 0.18531685]\n",
      " [-0.12440678]\n",
      " [ 0.1713112 ]]\n",
      "t [[ 0.1531169 ]\n",
      " [-0.10762341]\n",
      " [-0.14758296]\n",
      " ...\n",
      " [ 0.2183181 ]\n",
      " [-0.14758296]\n",
      " [ 0.20118608]]\n",
      "t [[ 0.1531169 ]\n",
      " [-0.10762341]\n",
      " [-0.14758296]\n",
      " ...\n",
      " [ 0.2183181 ]\n",
      " [-0.14758296]\n",
      " [ 0.20118608]]\n",
      "Current iteration=6, loss=48399.91222309224\n",
      "t [[ 0.17573419]\n",
      " [-0.12707631]\n",
      " [-0.17024225]\n",
      " ...\n",
      " [ 0.25010423]\n",
      " [-0.17024225]\n",
      " [ 0.22976927]]\n",
      "t [[ 0.17573419]\n",
      " [-0.12707631]\n",
      " [-0.17024225]\n",
      " ...\n",
      " [ 0.25010423]\n",
      " [-0.17024225]\n",
      " [ 0.22976927]]\n",
      "t [[ 0.19760661]\n",
      " [-0.14680467]\n",
      " [-0.19240486]\n",
      " ...\n",
      " [ 0.28072978]\n",
      " [-0.19240486]\n",
      " [ 0.25712684]]\n",
      "t [[ 0.19760661]\n",
      " [-0.14680467]\n",
      " [-0.19240486]\n",
      " ...\n",
      " [ 0.28072978]\n",
      " [-0.19240486]\n",
      " [ 0.25712684]]\n",
      "Current iteration=8, loss=47420.23942541274\n",
      "t [[ 0.21876337]\n",
      " [-0.16676035]\n",
      " [-0.21409007]\n",
      " ...\n",
      " [ 0.31024683]\n",
      " [-0.21409007]\n",
      " [ 0.28332144]]\n",
      "t [[ 0.21876337]\n",
      " [-0.16676035]\n",
      " [-0.21409007]\n",
      " ...\n",
      " [ 0.31024683]\n",
      " [-0.21409007]\n",
      " [ 0.28332144]]\n",
      "t [[ 0.2392326 ]\n",
      " [-0.18689951]\n",
      " [-0.23531625]\n",
      " ...\n",
      " [ 0.33870501]\n",
      " [-0.23531625]\n",
      " [ 0.30841234]]\n",
      "loss=46521.105742624975\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01119211]\n",
      " [-0.05359338]\n",
      " [-0.03142777]\n",
      " ...\n",
      " [ 0.02687492]\n",
      " [ 0.01119211]\n",
      " [-0.0455561 ]]\n",
      "t [[ 0.01119211]\n",
      " [-0.05359338]\n",
      " [-0.03142777]\n",
      " ...\n",
      " [ 0.02687492]\n",
      " [ 0.01119211]\n",
      " [-0.0455561 ]]\n",
      "t [[ 0.02162362]\n",
      " [-0.10598373]\n",
      " [-0.06194747]\n",
      " ...\n",
      " [ 0.05172935]\n",
      " [ 0.02162362]\n",
      " [-0.08861326]]\n",
      "t [[ 0.02162362]\n",
      " [-0.10598373]\n",
      " [-0.06194747]\n",
      " ...\n",
      " [ 0.05172935]\n",
      " [ 0.02162362]\n",
      " [-0.08861326]]\n",
      "Current iteration=2, loss=50388.19096799019\n",
      "t [[ 0.03133916]\n",
      " [-0.15719087]\n",
      " [-0.09160192]\n",
      " ...\n",
      " [ 0.07471119]\n",
      " [ 0.03133916]\n",
      " [-0.12935629]]\n",
      "t [[ 0.03133916]\n",
      " [-0.15719087]\n",
      " [-0.09160192]\n",
      " ...\n",
      " [ 0.07471119]\n",
      " [ 0.03133916]\n",
      " [-0.12935629]]\n",
      "t [[ 0.04038133]\n",
      " [-0.20723629]\n",
      " [-0.12043209]\n",
      " ...\n",
      " [ 0.09595849]\n",
      " [ 0.04038133]\n",
      " [-0.16795773]]\n",
      "t [[ 0.04038133]\n",
      " [-0.20723629]\n",
      " [-0.12043209]\n",
      " ...\n",
      " [ 0.09595849]\n",
      " [ 0.04038133]\n",
      " [-0.16795773]]\n",
      "Current iteration=4, loss=49004.500592529585\n",
      "t [[ 0.0487906 ]\n",
      " [-0.25614282]\n",
      " [-0.14847699]\n",
      " ...\n",
      " [ 0.11559951]\n",
      " [ 0.0487906 ]\n",
      " [-0.2045779 ]]\n",
      "t [[ 0.0487906 ]\n",
      " [-0.25614282]\n",
      " [-0.14847699]\n",
      " ...\n",
      " [ 0.11559951]\n",
      " [ 0.0487906 ]\n",
      " [-0.2045779 ]]\n",
      "t [[ 0.05660527]\n",
      " [-0.30393442]\n",
      " [-0.17577365]\n",
      " ...\n",
      " [ 0.13375291]\n",
      " [ 0.05660527]\n",
      " [-0.23936518]]\n",
      "t [[ 0.05660527]\n",
      " [-0.30393442]\n",
      " [-0.17577365]\n",
      " ...\n",
      " [ 0.13375291]\n",
      " [ 0.05660527]\n",
      " [-0.23936518]]\n",
      "Current iteration=6, loss=47764.29028244347\n",
      "t [[ 0.06386149]\n",
      " [-0.35063586]\n",
      " [-0.2023571 ]\n",
      " ...\n",
      " [ 0.15052825]\n",
      " [ 0.06386149]\n",
      " [-0.27245656]]\n",
      "t [[ 0.06386149]\n",
      " [-0.35063586]\n",
      " [-0.2023571 ]\n",
      " ...\n",
      " [ 0.15052825]\n",
      " [ 0.06386149]\n",
      " [-0.27245656]]\n",
      "t [[ 0.07059328]\n",
      " [-0.39627253]\n",
      " [-0.22826044]\n",
      " ...\n",
      " [ 0.16602656]\n",
      " [ 0.07059328]\n",
      " [-0.3039782 ]]\n",
      "t [[ 0.07059328]\n",
      " [-0.39627253]\n",
      " [-0.22826044]\n",
      " ...\n",
      " [ 0.16602656]\n",
      " [ 0.07059328]\n",
      " [-0.3039782 ]]\n",
      "Current iteration=8, loss=46647.09141677129\n",
      "t [[ 0.07683262]\n",
      " [-0.4408702 ]\n",
      " [-0.25351491]\n",
      " ...\n",
      " [ 0.18034095]\n",
      " [ 0.07683262]\n",
      " [-0.33404617]]\n",
      "t [[ 0.07683262]\n",
      " [-0.4408702 ]\n",
      " [-0.25351491]\n",
      " ...\n",
      " [ 0.18034095]\n",
      " [ 0.07683262]\n",
      " [-0.33404617]]\n",
      "t [[ 0.08260951]\n",
      " [-0.48445487]\n",
      " [-0.27814993]\n",
      " ...\n",
      " [ 0.19355728]\n",
      " [ 0.08260951]\n",
      " [-0.3627671 ]]\n",
      "loss=45636.03506009193\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.03379767]\n",
      " [-0.01970268]\n",
      " [-0.03153234]\n",
      " ...\n",
      " [ 0.02648156]\n",
      " [ 0.01114816]\n",
      " [-0.04486763]]\n",
      "t [[ 0.03379767]\n",
      " [-0.01970268]\n",
      " [-0.03153234]\n",
      " ...\n",
      " [ 0.02648156]\n",
      " [ 0.01114816]\n",
      " [-0.04486763]]\n",
      "t [[ 0.06621837]\n",
      " [-0.04042141]\n",
      " [-0.06214691]\n",
      " ...\n",
      " [ 0.05095709]\n",
      " [ 0.02153322]\n",
      " [-0.08723767]]\n",
      "t [[ 0.06621837]\n",
      " [-0.04042141]\n",
      " [-0.06214691]\n",
      " ...\n",
      " [ 0.05095709]\n",
      " [ 0.02153322]\n",
      " [-0.08723767]]\n",
      "Current iteration=2, loss=50384.43835963416\n",
      "t [[ 0.09732311]\n",
      " [-0.06201497]\n",
      " [-0.09188723]\n",
      " ...\n",
      " [ 0.07357412]\n",
      " [ 0.03120008]\n",
      " [-0.12729632]]\n",
      "t [[ 0.09732311]\n",
      " [-0.06201497]\n",
      " [-0.09188723]\n",
      " ...\n",
      " [ 0.07357412]\n",
      " [ 0.03120008]\n",
      " [-0.12729632]]\n",
      "t [[ 0.12717152]\n",
      " [-0.08435417]\n",
      " [-0.12079491]\n",
      " ...\n",
      " [ 0.0944703 ]\n",
      " [ 0.04019161]\n",
      " [-0.16521731]]\n",
      "t [[ 0.12717152]\n",
      " [-0.08435417]\n",
      " [-0.12079491]\n",
      " ...\n",
      " [ 0.0944703 ]\n",
      " [ 0.04019161]\n",
      " [-0.16521731]]\n",
      "Current iteration=4, loss=48998.03665103969\n",
      "t [[ 0.15582158]\n",
      " [-0.10732147]\n",
      " [-0.14890955]\n",
      " ...\n",
      " [ 0.11377345]\n",
      " [ 0.04854853]\n",
      " [-0.20116205]]\n",
      "t [[ 0.15582158]\n",
      " [-0.10732147]\n",
      " [-0.14890955]\n",
      " ...\n",
      " [ 0.11377345]\n",
      " [ 0.04854853]\n",
      " [-0.20116205]]\n",
      "t [[ 0.18332938]\n",
      " [-0.13081042]\n",
      " [-0.17626875]\n",
      " ...\n",
      " [ 0.13160178]\n",
      " [ 0.05630937]\n",
      " [-0.23527984]]\n",
      "t [[ 0.18332938]\n",
      " [-0.13081042]\n",
      " [-0.17626875]\n",
      " ...\n",
      " [ 0.13160178]\n",
      " [ 0.05630937]\n",
      " [-0.23527984]]\n",
      "Current iteration=6, loss=47755.87312274046\n",
      "t [[ 0.20974889]\n",
      " [-0.15472502]\n",
      " [-0.2029081 ]\n",
      " ...\n",
      " [ 0.14806443]\n",
      " [ 0.0635105 ]\n",
      " [-0.26770849]]\n",
      "t [[ 0.20974889]\n",
      " [-0.15472502]\n",
      " [-0.2029081 ]\n",
      " ...\n",
      " [ 0.14806443]\n",
      " [ 0.0635105 ]\n",
      " [-0.26770849]]\n",
      "t [[ 0.23513187]\n",
      " [-0.17897908]\n",
      " [-0.2288612 ]\n",
      " ...\n",
      " [ 0.163262  ]\n",
      " [ 0.07018613]\n",
      " [-0.29857487]]\n",
      "t [[ 0.23513187]\n",
      " [-0.17897908]\n",
      " [-0.2288612 ]\n",
      " ...\n",
      " [ 0.163262  ]\n",
      " [ 0.07018613]\n",
      " [-0.29857487]]\n",
      "Current iteration=8, loss=46637.27173495265\n",
      "t [[ 0.25952779]\n",
      " [-0.20349545]\n",
      " [-0.25415975]\n",
      " ...\n",
      " [ 0.17728721]\n",
      " [ 0.0763684 ]\n",
      " [-0.32799564]]\n",
      "t [[ 0.25952779]\n",
      " [-0.20349545]\n",
      " [-0.25415975]\n",
      " ...\n",
      " [ 0.17728721]\n",
      " [ 0.0763684 ]\n",
      " [-0.32799564]]\n",
      "t [[ 0.28298377]\n",
      " [-0.22820536]\n",
      " [-0.27883361]\n",
      " ...\n",
      " [ 0.19022554]\n",
      " [ 0.08208748]\n",
      " [-0.35607794]]\n",
      "loss=45625.21325225715\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03336448]\n",
      " [-0.01886113]\n",
      " [-0.03141424]\n",
      " ...\n",
      " [ 0.02689014]\n",
      " [ 0.01135364]\n",
      " [-0.04624932]]\n",
      "t [[ 0.03336448]\n",
      " [-0.01886113]\n",
      " [-0.03141424]\n",
      " ...\n",
      " [ 0.02689014]\n",
      " [ 0.01135364]\n",
      " [-0.04624932]]\n",
      "t [[ 0.0653703 ]\n",
      " [-0.0387429 ]\n",
      " [-0.06191073]\n",
      " ...\n",
      " [ 0.05171937]\n",
      " [ 0.02193497]\n",
      " [-0.08992555]]\n",
      "t [[ 0.0653703 ]\n",
      " [-0.0387429 ]\n",
      " [-0.06191073]\n",
      " ...\n",
      " [ 0.05171937]\n",
      " [ 0.02193497]\n",
      " [-0.08992555]]\n",
      "Current iteration=2, loss=50395.5280158329\n",
      "t [[ 0.09607783]\n",
      " [-0.05950342]\n",
      " [-0.09153325]\n",
      " ...\n",
      " [ 0.07463939]\n",
      " [ 0.03178952]\n",
      " [-0.13121981]]\n",
      "t [[ 0.09607783]\n",
      " [-0.05950342]\n",
      " [-0.09153325]\n",
      " ...\n",
      " [ 0.07463939]\n",
      " [ 0.03178952]\n",
      " [-0.13121981]]\n",
      "t [[ 0.12554602]\n",
      " [-0.081013  ]\n",
      " [-0.12032364]\n",
      " ...\n",
      " [ 0.09579196]\n",
      " [ 0.04096073]\n",
      " [-0.17031069]]\n",
      "t [[ 0.12554602]\n",
      " [-0.081013  ]\n",
      " [-0.12032364]\n",
      " ...\n",
      " [ 0.09579196]\n",
      " [ 0.04096073]\n",
      " [-0.17031069]]\n",
      "Current iteration=4, loss=49020.45704791873\n",
      "t [[ 0.15383218]\n",
      " [-0.10315373]\n",
      " [-0.14832177]\n",
      " ...\n",
      " [ 0.11530886]\n",
      " [ 0.04948988]\n",
      " [-0.20736411]]\n",
      "t [[ 0.15383218]\n",
      " [-0.10315373]\n",
      " [-0.14832177]\n",
      " ...\n",
      " [ 0.11530886]\n",
      " [ 0.04948988]\n",
      " [-0.20736411]]\n",
      "t [[ 0.18099169]\n",
      " [-0.12581893]\n",
      " [-0.17556542]\n",
      " ...\n",
      " [ 0.13331207]\n",
      " [ 0.057416  ]\n",
      " [-0.24253371]]\n",
      "t [[ 0.18099169]\n",
      " [-0.12581893]\n",
      " [-0.17556542]\n",
      " ...\n",
      " [ 0.13331207]\n",
      " [ 0.057416  ]\n",
      " [-0.24253371]]\n",
      "Current iteration=6, loss=47789.55314516799\n",
      "t [[ 0.20707783]\n",
      " [-0.1489125 ]\n",
      " [-0.20209035]\n",
      " ...\n",
      " [ 0.14991419]\n",
      " [ 0.06477593]\n",
      " [-0.27596126]]\n",
      "t [[ 0.20707783]\n",
      " [-0.1489125 ]\n",
      " [-0.20209035]\n",
      " ...\n",
      " [ 0.14991419]\n",
      " [ 0.06477593]\n",
      " [-0.27596126]]\n",
      "t [[ 0.23214173]\n",
      " [-0.17234816]\n",
      " [-0.2279303 ]\n",
      " ...\n",
      " [ 0.16521902]\n",
      " [ 0.07160433]\n",
      " [-0.30777736]]\n",
      "t [[ 0.23214173]\n",
      " [-0.17234816]\n",
      " [-0.2279303 ]\n",
      " ...\n",
      " [ 0.16521902]\n",
      " [ 0.07160433]\n",
      " [-0.30777736]]\n",
      "Current iteration=8, loss=46681.939082999925\n",
      "t [[ 0.25623221]\n",
      " [-0.19604876]\n",
      " [-0.25311709]\n",
      " ...\n",
      " [ 0.17932213]\n",
      " [ 0.07793374]\n",
      " [-0.33810204]]\n",
      "t [[ 0.25623221]\n",
      " [-0.19604876]\n",
      " [-0.25311709]\n",
      " ...\n",
      " [ 0.17932213]\n",
      " [ 0.07793374]\n",
      " [-0.33810204]]\n",
      "t [[ 0.27939584]\n",
      " [-0.21994553]\n",
      " [-0.27768067]\n",
      " ...\n",
      " [ 0.19231154]\n",
      " [ 0.08379471]\n",
      " [-0.36704558]]\n",
      "loss=45680.473517600316\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.03331107]\n",
      " [-0.0198155 ]\n",
      " [-0.03131557]\n",
      " ...\n",
      " [ 0.04797842]\n",
      " [-0.03131557]\n",
      " [ 0.04493859]]\n",
      "t [[ 0.03331107]\n",
      " [-0.0198155 ]\n",
      " [-0.03131557]\n",
      " ...\n",
      " [ 0.04797842]\n",
      " [-0.03131557]\n",
      " [ 0.04493859]]\n",
      "t [[ 0.06526598]\n",
      " [-0.04059583]\n",
      " [-0.06172138]\n",
      " ...\n",
      " [ 0.09376127]\n",
      " [-0.06172138]\n",
      " [ 0.08746497]]\n",
      "t [[ 0.06526598]\n",
      " [-0.04059583]\n",
      " [-0.06172138]\n",
      " ...\n",
      " [ 0.09376127]\n",
      " [-0.06172138]\n",
      " [ 0.08746497]]\n",
      "Current iteration=2, loss=50398.49571501711\n",
      "t [[ 0.09592562]\n",
      " [-0.06220339]\n",
      " [-0.09126043]\n",
      " ...\n",
      " [ 0.13746412]\n",
      " [-0.09126043]\n",
      " [ 0.12772428]]\n",
      "t [[ 0.09592562]\n",
      " [-0.06220339]\n",
      " [-0.09126043]\n",
      " ...\n",
      " [ 0.13746412]\n",
      " [-0.09126043]\n",
      " [ 0.12772428]]\n",
      "t [[ 0.1253492 ]\n",
      " [-0.08451264]\n",
      " [-0.11997385]\n",
      " ...\n",
      " [ 0.17919777]\n",
      " [-0.11997385]\n",
      " [ 0.16585438]]\n",
      "t [[ 0.1253492 ]\n",
      " [-0.08451264]\n",
      " [-0.11997385]\n",
      " ...\n",
      " [ 0.17919777]\n",
      " [-0.11997385]\n",
      " [ 0.16585438]]\n",
      "Current iteration=4, loss=49024.763605911525\n",
      "t [[ 0.15359392]\n",
      " [-0.10740973]\n",
      " [-0.14790083]\n",
      " ...\n",
      " [ 0.21906787]\n",
      " [-0.14790083]\n",
      " [ 0.20198561]]\n",
      "t [[ 0.15359392]\n",
      " [-0.10740973]\n",
      " [-0.14790083]\n",
      " ...\n",
      " [ 0.21906787]\n",
      " [-0.14790083]\n",
      " [ 0.20198561]]\n",
      "t [[ 0.18071486]\n",
      " [-0.13079187]\n",
      " [-0.17507856]\n",
      " ...\n",
      " [ 0.25717481]\n",
      " [-0.17507856]\n",
      " [ 0.2362407 ]]\n",
      "t [[ 0.18071486]\n",
      " [-0.13079187]\n",
      " [-0.17507856]\n",
      " ...\n",
      " [ 0.25717481]\n",
      " [-0.17507856]\n",
      " [ 0.2362407 ]]\n",
      "Current iteration=6, loss=47794.071269121036\n",
      "t [[ 0.20676479]\n",
      " [-0.15456669]\n",
      " [-0.20154228]\n",
      " ...\n",
      " [ 0.29361368]\n",
      " [-0.20154228]\n",
      " [ 0.26873494]]\n",
      "t [[ 0.20676479]\n",
      " [-0.15456669]\n",
      " [-0.20154228]\n",
      " ...\n",
      " [ 0.29361368]\n",
      " [-0.20154228]\n",
      " [ 0.26873494]]\n",
      "t [[ 0.2317942 ]\n",
      " [-0.17865144]\n",
      " [-0.22732525]\n",
      " ...\n",
      " [ 0.32847432]\n",
      " [-0.22732525]\n",
      " [ 0.29957633]]\n",
      "t [[ 0.2317942 ]\n",
      " [-0.17865144]\n",
      " [-0.22732525]\n",
      " ...\n",
      " [ 0.32847432]\n",
      " [-0.22732525]\n",
      " [ 0.29957633]]\n",
      "Current iteration=8, loss=46685.90635391552\n",
      "t [[ 0.25585121]\n",
      " [-0.20297227]\n",
      " [-0.25245887]\n",
      " ...\n",
      " [ 0.3618415 ]\n",
      " [-0.25245887]\n",
      " [ 0.32886592]]\n",
      "t [[ 0.25585121]\n",
      " [-0.20297227]\n",
      " [-0.25245887]\n",
      " ...\n",
      " [ 0.3618415 ]\n",
      " [-0.25245887]\n",
      " [ 0.32886592]]\n",
      "t [[ 0.27898166]\n",
      " [-0.22746346]\n",
      " [-0.2769727 ]\n",
      " ...\n",
      " [ 0.39379506]\n",
      " [-0.2769727 ]\n",
      " [ 0.35669809]]\n",
      "loss=45683.390578324965\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01305746]\n",
      " [-0.06252561]\n",
      " [-0.03666573]\n",
      " ...\n",
      " [ 0.03135407]\n",
      " [ 0.01305746]\n",
      " [-0.05314879]]\n",
      "t [[ 0.01305746]\n",
      " [-0.06252561]\n",
      " [-0.03666573]\n",
      " ...\n",
      " [ 0.03135407]\n",
      " [ 0.01305746]\n",
      " [-0.05314879]]\n",
      "t [[ 0.02507976]\n",
      " [-0.12341386]\n",
      " [-0.07209558]\n",
      " ...\n",
      " [ 0.05995828]\n",
      " [ 0.02507976]\n",
      " [-0.10289656]]\n",
      "t [[ 0.02507976]\n",
      " [-0.12341386]\n",
      " [-0.07209558]\n",
      " ...\n",
      " [ 0.05995828]\n",
      " [ 0.02507976]\n",
      " [-0.10289656]]\n",
      "Current iteration=2, loss=50142.56433524297\n",
      "t [[ 0.03613802]\n",
      " [-0.18269652]\n",
      " [-0.10635781]\n",
      " ...\n",
      " [ 0.08604799]\n",
      " [ 0.03613802]\n",
      " [-0.14953756]]\n",
      "t [[ 0.03613802]\n",
      " [-0.18269652]\n",
      " [-0.10635781]\n",
      " ...\n",
      " [ 0.08604799]\n",
      " [ 0.03613802]\n",
      " [-0.14953756]]\n",
      "t [[ 0.04629945]\n",
      " [-0.24040837]\n",
      " [-0.13951712]\n",
      " ...\n",
      " [ 0.10983997]\n",
      " [ 0.04629945]\n",
      " [-0.19334277]]\n",
      "t [[ 0.04629945]\n",
      " [-0.24040837]\n",
      " [-0.13951712]\n",
      " ...\n",
      " [ 0.10983997]\n",
      " [ 0.04629945]\n",
      " [-0.19334277]]\n",
      "Current iteration=4, loss=48569.48290636854\n",
      "t [[ 0.05562722]\n",
      " [-0.29658655]\n",
      " [-0.17163447]\n",
      " ...\n",
      " [ 0.13153255]\n",
      " [ 0.05562722]\n",
      " [-0.23456026]]\n",
      "t [[ 0.05562722]\n",
      " [-0.29658655]\n",
      " [-0.17163447]\n",
      " ...\n",
      " [ 0.13153255]\n",
      " [ 0.05562722]\n",
      " [-0.23456026]]\n",
      "t [[ 0.06418041]\n",
      " [-0.35127001]\n",
      " [-0.20276707]\n",
      " ...\n",
      " [ 0.15130648]\n",
      " [ 0.06418041]\n",
      " [-0.27341608]]\n",
      "t [[ 0.06418041]\n",
      " [-0.35127001]\n",
      " [-0.20276707]\n",
      " ...\n",
      " [ 0.15130648]\n",
      " [ 0.06418041]\n",
      " [-0.27341608]]\n",
      "Current iteration=6, loss=47182.87226097152\n",
      "t [[ 0.07201408]\n",
      " [-0.40449896]\n",
      " [-0.23296842]\n",
      " ...\n",
      " [ 0.16932609]\n",
      " [ 0.07201408]\n",
      " [-0.31011556]]\n",
      "t [[ 0.07201408]\n",
      " [-0.40449896]\n",
      " [-0.23296842]\n",
      " ...\n",
      " [ 0.16932609]\n",
      " [ 0.07201408]\n",
      " [-0.31011556]]\n",
      "t [[ 0.07917941]\n",
      " [-0.4563144 ]\n",
      " [-0.26228851]\n",
      " ...\n",
      " [ 0.18574069]\n",
      " [ 0.07917941]\n",
      " [-0.34484484]]\n",
      "t [[ 0.07917941]\n",
      " [-0.4563144 ]\n",
      " [-0.26228851]\n",
      " ...\n",
      " [ 0.18574069]\n",
      " [ 0.07917941]\n",
      " [-0.34484484]]\n",
      "Current iteration=8, loss=45952.42021947871\n",
      "t [[ 0.08572389]\n",
      " [-0.50675773]\n",
      " [-0.29077393]\n",
      " ...\n",
      " [ 0.20068598]\n",
      " [ 0.08572389]\n",
      " [-0.37777237]]\n",
      "t [[ 0.08572389]\n",
      " [-0.50675773]\n",
      " [-0.29077393]\n",
      " ...\n",
      " [ 0.20068598]\n",
      " [ 0.08572389]\n",
      " [-0.37777237]]\n",
      "t [[ 0.0916915 ]\n",
      " [-0.55587039]\n",
      " [-0.31846809]\n",
      " ...\n",
      " [ 0.21428532]\n",
      " [ 0.0916915 ]\n",
      " [-0.40905052]]\n",
      "loss=44853.90186563758\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.03943061]\n",
      " [-0.02298647]\n",
      " [-0.03678772]\n",
      " ...\n",
      " [ 0.03089515]\n",
      " [ 0.01300619]\n",
      " [-0.05234556]]\n",
      "t [[ 0.03943061]\n",
      " [-0.02298647]\n",
      " [-0.03678772]\n",
      " ...\n",
      " [ 0.03089515]\n",
      " [ 0.01300619]\n",
      " [-0.05234556]]\n",
      "t [[ 0.07698721]\n",
      " [-0.04735572]\n",
      " [-0.07232639]\n",
      " ...\n",
      " [ 0.05906012]\n",
      " [ 0.02497381]\n",
      " [-0.10129197]]\n",
      "t [[ 0.07698721]\n",
      " [-0.04735572]\n",
      " [-0.07232639]\n",
      " ...\n",
      " [ 0.05906012]\n",
      " [ 0.02497381]\n",
      " [-0.10129197]]\n",
      "Current iteration=2, loss=50138.271533734434\n",
      "t [[ 0.11276715]\n",
      " [-0.07288319]\n",
      " [-0.10668532]\n",
      " ...\n",
      " [ 0.08472967]\n",
      " [ 0.03597442]\n",
      " [-0.14713563]]\n",
      "t [[ 0.11276715]\n",
      " [-0.07288319]\n",
      " [-0.10668532]\n",
      " ...\n",
      " [ 0.08472967]\n",
      " [ 0.03597442]\n",
      " [-0.14713563]]\n",
      "t [[ 0.14686507]\n",
      " [-0.09936691]\n",
      " [-0.13993025]\n",
      " ...\n",
      " [ 0.10811991]\n",
      " [ 0.04607566]\n",
      " [-0.19014941]]\n",
      "t [[ 0.14686507]\n",
      " [-0.09936691]\n",
      " [-0.13993025]\n",
      " ...\n",
      " [ 0.10811991]\n",
      " [ 0.04607566]\n",
      " [-0.19014941]]\n",
      "Current iteration=4, loss=48562.26098105233\n",
      "t [[ 0.17937228]\n",
      " [-0.12662653]\n",
      " [-0.17212307]\n",
      " ...\n",
      " [ 0.12942848]\n",
      " [ 0.05534108]\n",
      " [-0.230583  ]]\n",
      "t [[ 0.17937228]\n",
      " [-0.12662653]\n",
      " [-0.17212307]\n",
      " ...\n",
      " [ 0.12942848]\n",
      " [ 0.05534108]\n",
      " [-0.230583  ]]\n",
      "t [[ 0.21037627]\n",
      " [-0.15450199]\n",
      " [-0.20332186]\n",
      " ...\n",
      " [ 0.14883542]\n",
      " [ 0.06383012]\n",
      " [-0.26866382]]\n",
      "t [[ 0.21037627]\n",
      " [-0.15450199]\n",
      " [-0.20332186]\n",
      " ...\n",
      " [ 0.14883542]\n",
      " [ 0.06383012]\n",
      " [-0.26866382]]\n",
      "Current iteration=6, loss=47173.659921431354\n",
      "t [[ 0.23996043]\n",
      " [-0.18285204]\n",
      " [-0.23358098]\n",
      " ...\n",
      " [ 0.16650439]\n",
      " [ 0.07159815]\n",
      " [-0.30459838]]\n",
      "t [[ 0.23996043]\n",
      " [-0.18285204]\n",
      " [-0.23358098]\n",
      " ...\n",
      " [ 0.16650439]\n",
      " [ 0.07159815]\n",
      " [-0.30459838]]\n",
      "t [[ 0.26820382]\n",
      " [-0.21155268]\n",
      " [-0.26295112]\n",
      " ...\n",
      " [ 0.18258407]\n",
      " [ 0.07869662]\n",
      " [-0.33857378]]\n",
      "t [[ 0.26820382]\n",
      " [-0.21155268]\n",
      " [-0.26295112]\n",
      " ...\n",
      " [ 0.18258407]\n",
      " [ 0.07869662]\n",
      " [-0.33857378]]\n",
      "Current iteration=8, loss=45941.862502032454\n",
      "t [[ 0.2951811 ]\n",
      " [-0.24049554]\n",
      " [-0.29147959]\n",
      " ...\n",
      " [ 0.19720954]\n",
      " [ 0.08517327]\n",
      " [-0.37075928]]\n",
      "t [[ 0.2951811 ]\n",
      " [-0.24049554]\n",
      " [-0.29147959]\n",
      " ...\n",
      " [ 0.19720954]\n",
      " [ 0.08517327]\n",
      " [-0.37075928]]\n",
      "t [[ 0.32096259]\n",
      " [-0.26958634]\n",
      " [-0.31921043]\n",
      " ...\n",
      " [ 0.21050363]\n",
      " [ 0.0910723 ]\n",
      " [-0.40130788]]\n",
      "loss=44842.44314483965\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03892523]\n",
      " [-0.02200465]\n",
      " [-0.03664995]\n",
      " ...\n",
      " [ 0.03137183]\n",
      " [ 0.01324591]\n",
      " [-0.05395755]]\n",
      "t [[ 0.03892523]\n",
      " [-0.02200465]\n",
      " [-0.03664995]\n",
      " ...\n",
      " [ 0.03137183]\n",
      " [ 0.01324591]\n",
      " [-0.05395755]]\n",
      "t [[ 0.07600135]\n",
      " [-0.04539835]\n",
      " [-0.07205085]\n",
      " ...\n",
      " [ 0.0599388 ]\n",
      " [ 0.02544074]\n",
      " [-0.10441316]]\n",
      "t [[ 0.07600135]\n",
      " [-0.04539835]\n",
      " [-0.07205085]\n",
      " ...\n",
      " [ 0.0599388 ]\n",
      " [ 0.02544074]\n",
      " [-0.10441316]]\n",
      "Current iteration=2, loss=50151.237797442234\n",
      "t [[ 0.1113247 ]\n",
      " [-0.06995545]\n",
      " [-0.10627247]\n",
      " ...\n",
      " [ 0.08594237]\n",
      " [ 0.03665701]\n",
      " [-0.15167121]]\n",
      "t [[ 0.1113247 ]\n",
      " [-0.06995545]\n",
      " [-0.10627247]\n",
      " ...\n",
      " [ 0.08594237]\n",
      " [ 0.03665701]\n",
      " [-0.15167121]]\n",
      "t [[ 0.14498884]\n",
      " [-0.09547324]\n",
      " [-0.13938092]\n",
      " ...\n",
      " [ 0.10960519]\n",
      " [ 0.04696329]\n",
      " [-0.19601216]]\n",
      "t [[ 0.14498884]\n",
      " [-0.09547324]\n",
      " [-0.13938092]\n",
      " ...\n",
      " [ 0.10960519]\n",
      " [ 0.04696329]\n",
      " [-0.19601216]]\n",
      "Current iteration=4, loss=48588.46032141039\n",
      "t [[ 0.17708397]\n",
      " [-0.12177086]\n",
      " [-0.17143846]\n",
      " ...\n",
      " [ 0.1311311 ]\n",
      " [ 0.05642398]\n",
      " [-0.23769281]]\n",
      "t [[ 0.17708397]\n",
      " [-0.12177086]\n",
      " [-0.17143846]\n",
      " ...\n",
      " [ 0.1311311 ]\n",
      " [ 0.05642398]\n",
      " [-0.23769281]]\n",
      "t [[ 0.20769648]\n",
      " [-0.14868802]\n",
      " [-0.20250346]\n",
      " ...\n",
      " [ 0.15070588]\n",
      " [ 0.06509929]\n",
      " [-0.27694717]]\n",
      "t [[ 0.20769648]\n",
      " [-0.14868802]\n",
      " [-0.20250346]\n",
      " ...\n",
      " [ 0.15070588]\n",
      " [ 0.06509929]\n",
      " [-0.27694717]]\n",
      "Current iteration=6, loss=47212.90024761482\n",
      "t [[ 0.23690869]\n",
      " [-0.17608332]\n",
      " [-0.2326305 ]\n",
      " ...\n",
      " [ 0.16849837]\n",
      " [ 0.07304531]\n",
      " [-0.31398775]]\n",
      "t [[ 0.23690869]\n",
      " [-0.17608332]\n",
      " [-0.2326305 ]\n",
      " ...\n",
      " [ 0.16849837]\n",
      " [ 0.07304531]\n",
      " [-0.31398775]]\n",
      "t [[ 0.26479869]\n",
      " [-0.20383273]\n",
      " [-0.26187049]\n",
      " ...\n",
      " [ 0.18466183]\n",
      " [ 0.08031415]\n",
      " [-0.34900708]]\n",
      "t [[ 0.26479869]\n",
      " [-0.20383273]\n",
      " [-0.26187049]\n",
      " ...\n",
      " [ 0.18466183]\n",
      " [ 0.08031415]\n",
      " [-0.34900708]]\n",
      "Current iteration=8, loss=45993.68282144274\n",
      "t [[ 0.29144023]\n",
      " [-0.23182793]\n",
      " [-0.29027085]\n",
      " ...\n",
      " [ 0.19933534]\n",
      " [ 0.08695412]\n",
      " [-0.38217935]]\n",
      "t [[ 0.29144023]\n",
      " [-0.23182793]\n",
      " [-0.29027085]\n",
      " ...\n",
      " [ 0.19933534]\n",
      " [ 0.08695412]\n",
      " [-0.38217935]]\n",
      "t [[ 0.31690279]\n",
      " [-0.25997472]\n",
      " [-0.31787572]\n",
      " ...\n",
      " [ 0.21264516]\n",
      " [ 0.09300995]\n",
      " [-0.41366197]]\n",
      "loss=44906.23879999541\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.03886292]\n",
      " [-0.02311808]\n",
      " [-0.03653483]\n",
      " ...\n",
      " [ 0.05597483]\n",
      " [-0.03653483]\n",
      " [ 0.05242836]]\n",
      "t [[ 0.03886292]\n",
      " [-0.02311808]\n",
      " [-0.03653483]\n",
      " ...\n",
      " [ 0.05597483]\n",
      " [-0.03653483]\n",
      " [ 0.05242836]]\n",
      "t [[ 0.07588012]\n",
      " [-0.04754927]\n",
      " [-0.07183149]\n",
      " ...\n",
      " [ 0.1089615 ]\n",
      " [-0.07183149]\n",
      " [ 0.10157374]]\n",
      "t [[ 0.07588012]\n",
      " [-0.04754927]\n",
      " [-0.07183149]\n",
      " ...\n",
      " [ 0.1089615 ]\n",
      " [-0.07183149]\n",
      " [ 0.10157374]]\n",
      "Current iteration=2, loss=50154.564879058424\n",
      "t [[ 0.11114873]\n",
      " [-0.07307479]\n",
      " [-0.10595851]\n",
      " ...\n",
      " [ 0.15914427]\n",
      " [-0.10595851]\n",
      " [ 0.1476674 ]]\n",
      "t [[ 0.11114873]\n",
      " [-0.07307479]\n",
      " [-0.10595851]\n",
      " ...\n",
      " [ 0.15914427]\n",
      " [-0.10595851]\n",
      " [ 0.1476674 ]]\n",
      "t [[ 0.14476255]\n",
      " [-0.09949854]\n",
      " [-0.13898085]\n",
      " ...\n",
      " [ 0.20669812]\n",
      " [-0.13898085]\n",
      " [ 0.19092666]]\n",
      "t [[ 0.14476255]\n",
      " [-0.09949854]\n",
      " [-0.13898085]\n",
      " ...\n",
      " [ 0.20669812]\n",
      " [-0.13898085]\n",
      " [ 0.19092666]]\n",
      "Current iteration=4, loss=48592.9934842592\n",
      "t [[ 0.17681147]\n",
      " [-0.12664606]\n",
      " [-0.17095976]\n",
      " ...\n",
      " [ 0.25178821]\n",
      " [-0.17095976]\n",
      " [ 0.23155457]]\n",
      "t [[ 0.17681147]\n",
      " [-0.12664606]\n",
      " [-0.17095976]\n",
      " ...\n",
      " [ 0.25178821]\n",
      " [-0.17095976]\n",
      " [ 0.23155457]]\n",
      "t [[ 0.20738119]\n",
      " [-0.15436312]\n",
      " [-0.20195273]\n",
      " ...\n",
      " [ 0.29456972]\n",
      " [-0.20195273]\n",
      " [ 0.26973994]]\n",
      "t [[ 0.20738119]\n",
      " [-0.15436312]\n",
      " [-0.20195273]\n",
      " ...\n",
      " [ 0.29456972]\n",
      " [-0.20195273]\n",
      " [ 0.26973994]]\n",
      "Current iteration=6, loss=47217.27028246354\n",
      "t [[ 0.23655305]\n",
      " [-0.18251405]\n",
      " [-0.23201356]\n",
      " ...\n",
      " [ 0.3351879 ]\n",
      " [-0.23201356]\n",
      " [ 0.30565777]]\n",
      "t [[ 0.23655305]\n",
      " [-0.18251405]\n",
      " [-0.23201356]\n",
      " ...\n",
      " [ 0.3351879 ]\n",
      " [-0.23201356]\n",
      " [ 0.30565777]]\n",
      "t [[ 0.26440401]\n",
      " [-0.21098011]\n",
      " [-0.26119248]\n",
      " ...\n",
      " [ 0.37377837]\n",
      " [-0.26119248]\n",
      " [ 0.33946984]]\n",
      "t [[ 0.26440401]\n",
      " [-0.21098011]\n",
      " [-0.26119248]\n",
      " ...\n",
      " [ 0.37377837]\n",
      " [-0.26119248]\n",
      " [ 0.33946984]]\n",
      "Current iteration=8, loss=45997.036044600434\n",
      "t [[ 0.2910067 ]\n",
      " [-0.23965777]\n",
      " [-0.28953633]\n",
      " ...\n",
      " [ 0.41046757]\n",
      " [-0.28953633]\n",
      " [ 0.37132551]]\n",
      "t [[ 0.2910067 ]\n",
      " [-0.23965777]\n",
      " [-0.28953633]\n",
      " ...\n",
      " [ 0.41046757]\n",
      " [-0.28953633]\n",
      " [ 0.37132551]]\n",
      "t [[ 0.31642956]\n",
      " [-0.26845712]\n",
      " [-0.31708873]\n",
      " ...\n",
      " [ 0.44537316]\n",
      " [-0.31708873]\n",
      " [ 0.40136244]]\n",
      "loss=44908.070116017254\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01492281]\n",
      " [-0.07145783]\n",
      " [-0.04190369]\n",
      " ...\n",
      " [ 0.03583322]\n",
      " [ 0.01492281]\n",
      " [-0.06074147]]\n",
      "t [[ 0.01492281]\n",
      " [-0.07145783]\n",
      " [-0.04190369]\n",
      " ...\n",
      " [ 0.03583322]\n",
      " [ 0.01492281]\n",
      " [-0.06074147]]\n",
      "t [[ 0.02849373]\n",
      " [-0.14077724]\n",
      " [-0.08219333]\n",
      " ...\n",
      " [ 0.06807516]\n",
      " [ 0.02849373]\n",
      " [-0.1170413 ]]\n",
      "t [[ 0.02849373]\n",
      " [-0.14077724]\n",
      " [-0.08219333]\n",
      " ...\n",
      " [ 0.06807516]\n",
      " [ 0.02849373]\n",
      " [-0.1170413 ]]\n",
      "Current iteration=2, loss=49900.52673212012\n",
      "t [[ 0.04081925]\n",
      " [-0.20800605]\n",
      " [-0.12097118]\n",
      " ...\n",
      " [ 0.09707786]\n",
      " [ 0.04081925]\n",
      " [-0.16933974]]\n",
      "t [[ 0.04081925]\n",
      " [-0.20800605]\n",
      " [-0.12097118]\n",
      " ...\n",
      " [ 0.09707786]\n",
      " [ 0.04081925]\n",
      " [-0.16933974]]\n",
      "t [[ 0.05199903]\n",
      " [-0.27319713]\n",
      " [-0.15833323]\n",
      " ...\n",
      " [ 0.12316099]\n",
      " [ 0.05199903]\n",
      " [-0.21803662]]\n",
      "t [[ 0.05199903]\n",
      " [-0.27319713]\n",
      " [-0.15833323]\n",
      " ...\n",
      " [ 0.12316099]\n",
      " [ 0.05199903]\n",
      " [-0.21803662]]\n",
      "Current iteration=4, loss=48148.327632222354\n",
      "t [[ 0.06212562]\n",
      " [-0.33640712]\n",
      " [-0.1943689 ]\n",
      " ...\n",
      " [ 0.14661257]\n",
      " [ 0.06212562]\n",
      " [-0.26349228]]\n",
      "t [[ 0.06212562]\n",
      " [-0.33640712]\n",
      " [-0.1943689 ]\n",
      " ...\n",
      " [ 0.14661257]\n",
      " [ 0.06212562]\n",
      " [-0.26349228]]\n",
      "t [[ 0.07128453]\n",
      " [-0.39769535]\n",
      " [-0.22916116]\n",
      " ...\n",
      " [ 0.16769099]\n",
      " [ 0.07128453]\n",
      " [-0.30602982]]\n",
      "t [[ 0.07128453]\n",
      " [-0.39769535]\n",
      " [-0.22916116]\n",
      " ...\n",
      " [ 0.16769099]\n",
      " [ 0.07128453]\n",
      " [-0.30602982]]\n",
      "Current iteration=6, loss=46628.75427374328\n",
      "t [[ 0.07955443]\n",
      " [-0.45712284]\n",
      " [-0.26278669]\n",
      " ...\n",
      " [ 0.18662762]\n",
      " [ 0.07955443]\n",
      " [-0.34593797]]\n",
      "t [[ 0.07955443]\n",
      " [-0.45712284]\n",
      " [-0.26278669]\n",
      " ...\n",
      " [ 0.18662762]\n",
      " [ 0.07955443]\n",
      " [-0.34593797]]\n",
      "t [[ 0.08700752]\n",
      " [-0.51475146]\n",
      " [-0.29531627]\n",
      " ...\n",
      " [ 0.20362967]\n",
      " [ 0.08700752]\n",
      " [-0.38347421]]\n",
      "t [[ 0.08700752]\n",
      " [-0.51475146]\n",
      " [-0.29531627]\n",
      " ...\n",
      " [ 0.20362967]\n",
      " [ 0.08700752]\n",
      " [-0.38347421]]\n",
      "Current iteration=8, loss=45299.607411363206\n",
      "t [[ 0.09370992]\n",
      " [-0.57064323]\n",
      " [-0.32681511]\n",
      " ...\n",
      " [ 0.21888279]\n",
      " [ 0.09370992]\n",
      " [-0.41886787]]\n",
      "t [[ 0.09370992]\n",
      " [-0.57064323]\n",
      " [-0.32681511]\n",
      " ...\n",
      " [ 0.21888279]\n",
      " [ 0.09370992]\n",
      " [-0.41886787]]\n",
      "t [[ 0.09972214]\n",
      " [-0.6248598 ]\n",
      " [-0.35734331]\n",
      " ...\n",
      " [ 0.23255351]\n",
      " [ 0.09972214]\n",
      " [-0.45232306]]\n",
      "loss=44128.15089609153\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.04506356]\n",
      " [-0.02627025]\n",
      " [-0.04204311]\n",
      " ...\n",
      " [ 0.03530875]\n",
      " [ 0.01486422]\n",
      " [-0.0598235 ]]\n",
      "t [[ 0.04506356]\n",
      " [-0.02627025]\n",
      " [-0.04204311]\n",
      " ...\n",
      " [ 0.03530875]\n",
      " [ 0.01486422]\n",
      " [-0.0598235 ]]\n",
      "t [[ 0.08767971]\n",
      " [-0.05434634]\n",
      " [-0.08245496]\n",
      " ...\n",
      " [ 0.06705189]\n",
      " [ 0.02837209]\n",
      " [-0.11520779]]\n",
      "t [[ 0.08767971]\n",
      " [-0.05434634]\n",
      " [-0.08245496]\n",
      " ...\n",
      " [ 0.06705189]\n",
      " [ 0.02837209]\n",
      " [-0.11520779]]\n",
      "Current iteration=2, loss=49895.71636173284\n",
      "t [[ 0.12799448]\n",
      " [-0.08389262]\n",
      " [-0.12133942]\n",
      " ...\n",
      " [ 0.0955806 ]\n",
      " [ 0.04063077]\n",
      " [-0.16659636]]\n",
      "t [[ 0.12799448]\n",
      " [-0.08389262]\n",
      " [-0.12133942]\n",
      " ...\n",
      " [ 0.0955806 ]\n",
      " [ 0.04063077]\n",
      " [-0.16659636]]\n",
      "t [[ 0.16614894]\n",
      " [-0.1146125 ]\n",
      " [-0.15879396]\n",
      " ...\n",
      " [ 0.12121353]\n",
      " [ 0.05174055]\n",
      " [-0.21439178]]\n",
      "t [[ 0.16614894]\n",
      " [-0.1146125 ]\n",
      " [-0.15879396]\n",
      " ...\n",
      " [ 0.12121353]\n",
      " [ 0.05174055]\n",
      " [-0.21439178]]\n",
      "Current iteration=4, loss=48140.4204338345\n",
      "t [[ 0.20227804]\n",
      " [-0.14624625]\n",
      " [-0.19490943]\n",
      " ...\n",
      " [ 0.14423766]\n",
      " [ 0.06179455]\n",
      " [-0.25895668]]\n",
      "t [[ 0.20227804]\n",
      " [-0.14624625]\n",
      " [-0.19490943]\n",
      " ...\n",
      " [ 0.14423766]\n",
      " [ 0.06179455]\n",
      " [-0.25895668]]\n",
      "t [[ 0.23650985]\n",
      " [-0.17856823]\n",
      " [-0.22977003]\n",
      " ...\n",
      " [ 0.16491032]\n",
      " [ 0.07087875]\n",
      " [-0.30061609]]\n",
      "t [[ 0.23650985]\n",
      " [-0.17856823]\n",
      " [-0.22977003]\n",
      " ...\n",
      " [ 0.16491032]\n",
      " [ 0.07087875]\n",
      " [-0.30061609]]\n",
      "Current iteration=6, loss=46618.86468929843\n",
      "t [[ 0.26896506]\n",
      " [-0.21138377]\n",
      " [-0.26345363]\n",
      " ...\n",
      " [ 0.18346195]\n",
      " [ 0.07907228]\n",
      " [-0.33966029]]\n",
      "t [[ 0.26896506]\n",
      " [-0.21138377]\n",
      " [-0.26345363]\n",
      " ...\n",
      " [ 0.18346195]\n",
      " [ 0.07907228]\n",
      " [-0.33966029]]\n",
      "t [[ 0.29975678]\n",
      " [-0.24452597]\n",
      " [-0.29603203]\n",
      " ...\n",
      " [ 0.20009882]\n",
      " [ 0.08644771]\n",
      " [-0.376348  ]]\n",
      "t [[ 0.29975678]\n",
      " [-0.24452597]\n",
      " [-0.29603203]\n",
      " ...\n",
      " [ 0.20009882]\n",
      " [ 0.08644771]\n",
      " [-0.376348  ]]\n",
      "Current iteration=8, loss=45288.45970494573\n",
      "t [[ 0.32899057]\n",
      " [-0.2778526 ]\n",
      " [-0.32757141]\n",
      " ...\n",
      " [ 0.21500575]\n",
      " [ 0.09307148]\n",
      " [-0.41090952]]\n",
      "t [[ 0.32899057]\n",
      " [-0.2778526 ]\n",
      " [-0.32757141]\n",
      " ...\n",
      " [ 0.21500575]\n",
      " [ 0.09307148]\n",
      " [-0.41090952]]\n",
      "t [[ 0.3567647 ]\n",
      " [-0.31124311]\n",
      " [-0.35813272]\n",
      " ...\n",
      " [ 0.22834843]\n",
      " [ 0.09900437]\n",
      " [-0.4435497 ]]\n",
      "loss=44116.21789129369\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04448598]\n",
      " [-0.02514818]\n",
      " [-0.04188566]\n",
      " ...\n",
      " [ 0.03585351]\n",
      " [ 0.01513818]\n",
      " [-0.06166577]]\n",
      "t [[ 0.04448598]\n",
      " [-0.02514818]\n",
      " [-0.04188566]\n",
      " ...\n",
      " [ 0.03585351]\n",
      " [ 0.01513818]\n",
      " [-0.06166577]]\n",
      "t [[ 0.08655707]\n",
      " [-0.05211037]\n",
      " [-0.08214006]\n",
      " ...\n",
      " [ 0.06804395]\n",
      " [ 0.02890368]\n",
      " [-0.11875812]]\n",
      "t [[ 0.08655707]\n",
      " [-0.05211037]\n",
      " [-0.08214006]\n",
      " ...\n",
      " [ 0.06804395]\n",
      " [ 0.02890368]\n",
      " [-0.11875812]]\n",
      "Current iteration=2, loss=49910.565232028064\n",
      "t [[ 0.12635777]\n",
      " [-0.08054933]\n",
      " [-0.12086775]\n",
      " ...\n",
      " [ 0.09693253]\n",
      " [ 0.04140511]\n",
      " [-0.17173251]]\n",
      "t [[ 0.12635777]\n",
      " [-0.08054933]\n",
      " [-0.12086775]\n",
      " ...\n",
      " [ 0.09693253]\n",
      " [ 0.04140511]\n",
      " [-0.17173251]]\n",
      "t [[ 0.16402751]\n",
      " [-0.11016742]\n",
      " [-0.1581668 ]\n",
      " ...\n",
      " [ 0.12284768]\n",
      " [ 0.05274411]\n",
      " [-0.22100278]]\n",
      "t [[ 0.16402751]\n",
      " [-0.11016742]\n",
      " [-0.1581668 ]\n",
      " ...\n",
      " [ 0.12284768]\n",
      " [ 0.05274411]\n",
      " [-0.22100278]]\n",
      "Current iteration=4, loss=48170.39028866656\n",
      "t [[ 0.19969957]\n",
      " [-0.14070432]\n",
      " [-0.1941285 ]\n",
      " ...\n",
      " [ 0.14608543]\n",
      " [ 0.06301504]\n",
      " [-0.26694198]]\n",
      "t [[ 0.19969957]\n",
      " [-0.14070432]\n",
      " [-0.1941285 ]\n",
      " ...\n",
      " [ 0.14608543]\n",
      " [ 0.06301504]\n",
      " [-0.26694198]]\n",
      "t [[ 0.2335004 ]\n",
      " [-0.17193408]\n",
      " [-0.2288375 ]\n",
      " ...\n",
      " [ 0.16691134]\n",
      " [ 0.07230503]\n",
      " [-0.30988454]]\n",
      "t [[ 0.2335004 ]\n",
      " [-0.17193408]\n",
      " [-0.2288375 ]\n",
      " ...\n",
      " [ 0.16691134]\n",
      " [ 0.07230503]\n",
      " [-0.30988454]]\n",
      "Current iteration=6, loss=46663.59487517049\n",
      "t [[ 0.26554919]\n",
      " [-0.20366195]\n",
      " [-0.26237194]\n",
      " ...\n",
      " [ 0.18556297]\n",
      " [ 0.08069418]\n",
      " [-0.35012919]]\n",
      "t [[ 0.26554919]\n",
      " [-0.20366195]\n",
      " [-0.26237194]\n",
      " ...\n",
      " [ 0.18556297]\n",
      " [ 0.08069418]\n",
      " [-0.35012919]]\n",
      "t [[ 0.29595766]\n",
      " [-0.23572107]\n",
      " [-0.29480386]\n",
      " ...\n",
      " [ 0.20225273]\n",
      " [ 0.08825596]\n",
      " [-0.38794215]]\n",
      "t [[ 0.29595766]\n",
      " [-0.23572107]\n",
      " [-0.29480386]\n",
      " ...\n",
      " [ 0.20225273]\n",
      " [ 0.08825596]\n",
      " [-0.38794215]]\n",
      "Current iteration=8, loss=45347.25497652479\n",
      "t [[ 0.32483012]\n",
      " [-0.26796932]\n",
      " [-0.3261996 ]\n",
      " ...\n",
      " [ 0.21717064]\n",
      " [ 0.09505761]\n",
      " [-0.42356035]]\n",
      "t [[ 0.32483012]\n",
      " [-0.26796932]\n",
      " [-0.3261996 ]\n",
      " ...\n",
      " [ 0.21717064]\n",
      " [ 0.09505761]\n",
      " [-0.42356035]]\n",
      "t [[ 0.35226365]\n",
      " [-0.30028631]\n",
      " [-0.35662019]\n",
      " ...\n",
      " [ 0.23048684]\n",
      " [ 0.10116062]\n",
      " [-0.45719455]]\n",
      "loss=44188.22990267445\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.04441477]\n",
      " [-0.02642067]\n",
      " [-0.04175409]\n",
      " ...\n",
      " [ 0.06397123]\n",
      " [-0.04175409]\n",
      " [ 0.05991812]]\n",
      "t [[ 0.04441477]\n",
      " [-0.02642067]\n",
      " [-0.04175409]\n",
      " ...\n",
      " [ 0.06397123]\n",
      " [-0.04175409]\n",
      " [ 0.05991812]]\n",
      "t [[ 0.08641905]\n",
      " [-0.0545562 ]\n",
      " [-0.08189114]\n",
      " ...\n",
      " [ 0.12403999]\n",
      " [-0.08189114]\n",
      " [ 0.11554876]]\n",
      "t [[ 0.08641905]\n",
      " [-0.0545562 ]\n",
      " [-0.08189114]\n",
      " ...\n",
      " [ 0.12403999]\n",
      " [-0.08189114]\n",
      " [ 0.11554876]]\n",
      "Current iteration=2, loss=49914.21623142274\n",
      "t [[ 0.12615847]\n",
      " [-0.08407968]\n",
      " [-0.12051381]\n",
      " ...\n",
      " [ 0.18048227]\n",
      " [-0.12051381]\n",
      " [ 0.16723817]]\n",
      "t [[ 0.12615847]\n",
      " [-0.08407968]\n",
      " [-0.12051381]\n",
      " ...\n",
      " [ 0.18048227]\n",
      " [-0.12051381]\n",
      " [ 0.16723817]]\n",
      "t [[ 0.1637726 ]\n",
      " [-0.11470337]\n",
      " [-0.15771849]\n",
      " ...\n",
      " [ 0.23355774]\n",
      " [-0.15771849]\n",
      " [ 0.21530821]]\n",
      "t [[ 0.1637726 ]\n",
      " [-0.11470337]\n",
      " [-0.15771849]\n",
      " ...\n",
      " [ 0.23355774]\n",
      " [-0.15771849]\n",
      " [ 0.21530821]]\n",
      "Current iteration=4, loss=48175.043061741424\n",
      "t [[ 0.19939405]\n",
      " [-0.1461764 ]\n",
      " [-0.19359506]\n",
      " ...\n",
      " [ 0.28350878]\n",
      " [-0.19359506]\n",
      " [ 0.26005585]]\n",
      "t [[ 0.19939405]\n",
      " [-0.1461764 ]\n",
      " [-0.19359506]\n",
      " ...\n",
      " [ 0.28350878]\n",
      " [-0.19359506]\n",
      " [ 0.26005585]]\n",
      "t [[ 0.23314795]\n",
      " [-0.17828168]\n",
      " [-0.22822689]\n",
      " ...\n",
      " [ 0.33056046]\n",
      " [-0.22822689]\n",
      " [ 0.30175361]]\n",
      "t [[ 0.23314795]\n",
      " [-0.17828168]\n",
      " [-0.22822689]\n",
      " ...\n",
      " [ 0.33056046]\n",
      " [-0.22822689]\n",
      " [ 0.30175361]]\n",
      "Current iteration=6, loss=46667.6587085491\n",
      "t [[ 0.26515186]\n",
      " [-0.21083262]\n",
      " [-0.26169107]\n",
      " ...\n",
      " [ 0.37492094]\n",
      " [-0.26169107]\n",
      " [ 0.34065071]]\n",
      "t [[ 0.26515186]\n",
      " [-0.21083262]\n",
      " [-0.26169107]\n",
      " ...\n",
      " [ 0.37492094]\n",
      " [-0.26169107]\n",
      " [ 0.34065071]]\n",
      "t [[ 0.29551583]\n",
      " [-0.24366968]\n",
      " [-0.29405873]\n",
      " ...\n",
      " [ 0.41678224]\n",
      " [-0.29405873]\n",
      " [ 0.37697444]]\n",
      "t [[ 0.29551583]\n",
      " [-0.24366968]\n",
      " [-0.29405873]\n",
      " ...\n",
      " [ 0.41678224]\n",
      " [-0.29405873]\n",
      " [ 0.37697444]]\n",
      "Current iteration=8, loss=45349.822003538226\n",
      "t [[ 0.32434259]\n",
      " [-0.27665722]\n",
      " [-0.32539542]\n",
      " ...\n",
      " [ 0.45632118]\n",
      " [-0.32539542]\n",
      " [ 0.4109318 ]]\n",
      "t [[ 0.32434259]\n",
      " [-0.27665722]\n",
      " [-0.32539542]\n",
      " ...\n",
      " [ 0.45632118]\n",
      " [-0.32539542]\n",
      " [ 0.4109318 ]]\n",
      "t [[ 0.35172793]\n",
      " [-0.30968044]\n",
      " [-0.35576149]\n",
      " ...\n",
      " [ 0.49370042]\n",
      " [-0.35576149]\n",
      " [ 0.44271113]]\n",
      "loss=44188.82359307853\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01678816]\n",
      " [-0.08039006]\n",
      " [-0.04714166]\n",
      " ...\n",
      " [ 0.04031237]\n",
      " [ 0.01678816]\n",
      " [-0.06833416]]\n",
      "t [[ 0.01678816]\n",
      " [-0.08039006]\n",
      " [-0.04714166]\n",
      " ...\n",
      " [ 0.04031237]\n",
      " [ 0.01678816]\n",
      " [-0.06833416]]\n",
      "t [[ 0.03186555]\n",
      " [-0.15807389]\n",
      " [-0.09224074]\n",
      " ...\n",
      " [ 0.07608006]\n",
      " [ 0.03186555]\n",
      " [-0.13104756]]\n",
      "t [[ 0.03186555]\n",
      " [-0.15807389]\n",
      " [-0.09224074]\n",
      " ...\n",
      " [ 0.07608006]\n",
      " [ 0.03186555]\n",
      " [-0.13104756]]\n",
      "Current iteration=2, loss=49662.025499829295\n",
      "t [[ 0.04538427]\n",
      " [-0.2331202 ]\n",
      " [-0.13544339]\n",
      " ...\n",
      " [ 0.10780525]\n",
      " [ 0.04538427]\n",
      " [-0.1887685 ]]\n",
      "t [[ 0.04538427]\n",
      " [-0.2331202 ]\n",
      " [-0.13544339]\n",
      " ...\n",
      " [ 0.10780525]\n",
      " [ 0.04538427]\n",
      " [-0.1887685 ]]\n",
      "t [[ 0.05748513]\n",
      " [-0.30560554]\n",
      " [-0.17688532]\n",
      " ...\n",
      " [ 0.13593732]\n",
      " [ 0.05748513]\n",
      " [-0.24205921]]\n",
      "t [[ 0.05748513]\n",
      " [-0.30560554]\n",
      " [-0.17688532]\n",
      " ...\n",
      " [ 0.13593732]\n",
      " [ 0.05748513]\n",
      " [-0.24205921]]\n",
      "Current iteration=4, loss=47740.4726598888\n",
      "t [[ 0.06829736]\n",
      " [-0.37561213]\n",
      " [-0.21669156]\n",
      " ...\n",
      " [ 0.16087478]\n",
      " [ 0.06829736]\n",
      " [-0.29141837]]\n",
      "t [[ 0.06829736]\n",
      " [-0.37561213]\n",
      " [-0.21669156]\n",
      " ...\n",
      " [ 0.16087478]\n",
      " [ 0.06829736]\n",
      " [-0.29141837]]\n",
      "t [[ 0.07793881]\n",
      " [-0.44322588]\n",
      " [-0.25497667]\n",
      " ...\n",
      " [ 0.18296953]\n",
      " [ 0.07793881]\n",
      " [-0.33728577]]\n",
      "t [[ 0.07793881]\n",
      " [-0.44322588]\n",
      " [-0.25497667]\n",
      " ...\n",
      " [ 0.18296953]\n",
      " [ 0.07793881]\n",
      " [-0.33728577]]\n",
      "Current iteration=6, loss=46100.223877781056\n",
      "t [[ 0.08651651]\n",
      " [-0.50853471]\n",
      " [-0.29184532]\n",
      " ...\n",
      " [ 0.20253199]\n",
      " [ 0.08651651]\n",
      " [-0.38004794]]\n",
      "t [[ 0.08651651]\n",
      " [-0.50853471]\n",
      " [-0.29184532]\n",
      " ...\n",
      " [ 0.20253199]\n",
      " [ 0.08651651]\n",
      " [-0.38004794]]\n",
      "t [[ 0.0941274 ]\n",
      " [-0.57162718]\n",
      " [-0.32739292]\n",
      " ...\n",
      " [ 0.21983613]\n",
      " [ 0.0941274 ]\n",
      " [-0.42004394]]\n",
      "t [[ 0.0941274 ]\n",
      " [-0.57162718]\n",
      " [-0.32739292]\n",
      " ...\n",
      " [ 0.21983613]\n",
      " [ 0.0941274 ]\n",
      " [-0.42004394]]\n",
      "Current iteration=8, loss=44685.23046101026\n",
      "t [[ 0.10085915]\n",
      " [-0.6325914 ]\n",
      " [-0.36170645]\n",
      " ...\n",
      " [ 0.23512405]\n",
      " [ 0.10085915]\n",
      " [-0.45757088]]\n",
      "t [[ 0.10085915]\n",
      " [-0.6325914 ]\n",
      " [-0.36170645]\n",
      " ...\n",
      " [ 0.23512405]\n",
      " [ 0.10085915]\n",
      " [-0.45757088]]\n",
      "t [[ 0.10679096]\n",
      " [-0.69151426]\n",
      " [-0.39486513]\n",
      " ...\n",
      " [ 0.2486099 ]\n",
      " [ 0.10679096]\n",
      " [-0.49288906]]\n",
      "loss=43453.24171901587\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.0506965 ]\n",
      " [-0.02955403]\n",
      " [-0.0472985 ]\n",
      " ...\n",
      " [ 0.03972234]\n",
      " [ 0.01672225]\n",
      " [-0.06730144]]\n",
      "t [[ 0.0506965 ]\n",
      " [-0.02955403]\n",
      " [-0.0472985 ]\n",
      " ...\n",
      " [ 0.03972234]\n",
      " [ 0.01672225]\n",
      " [-0.06730144]]\n",
      "t [[ 0.09829591]\n",
      " [-0.06139323]\n",
      " [-0.09253266]\n",
      " ...\n",
      " [ 0.07493249]\n",
      " [ 0.03172808]\n",
      " [-0.12898521]]\n",
      "t [[ 0.09829591]\n",
      " [-0.06139323]\n",
      " [-0.09253266]\n",
      " ...\n",
      " [ 0.07493249]\n",
      " [ 0.03172808]\n",
      " [-0.12898521]]\n",
      "Current iteration=2, loss=49656.71955040064\n",
      "t [[ 0.14300711]\n",
      " [-0.09503912]\n",
      " [-0.13585089]\n",
      " ...\n",
      " [ 0.10613137]\n",
      " [ 0.04517056]\n",
      " [-0.1856842 ]]\n",
      "t [[ 0.14300711]\n",
      " [-0.09503912]\n",
      " [-0.13585089]\n",
      " ...\n",
      " [ 0.10613137]\n",
      " [ 0.04517056]\n",
      " [-0.1856842 ]]\n",
      "t [[ 0.18503053]\n",
      " [-0.13007656]\n",
      " [-0.17739105]\n",
      " ...\n",
      " [ 0.13376688]\n",
      " [ 0.05719138]\n",
      " [-0.23796446]]\n",
      "t [[ 0.18503053]\n",
      " [-0.13007656]\n",
      " [-0.17739105]\n",
      " ...\n",
      " [ 0.13376688]\n",
      " [ 0.05719138]\n",
      " [-0.23796446]]\n",
      "Current iteration=4, loss=47731.94695920196\n",
      "t [[ 0.22455616]\n",
      " [-0.16614935]\n",
      " [-0.21728009]\n",
      " ...\n",
      " [ 0.15823605]\n",
      " [ 0.06792055]\n",
      " [-0.28632778]]\n",
      "t [[ 0.22455616]\n",
      " [-0.16614935]\n",
      " [-0.21728009]\n",
      " ...\n",
      " [ 0.15823605]\n",
      " [ 0.06792055]\n",
      " [-0.28632778]]\n",
      "t [[ 0.26176242]\n",
      " [-0.20295481]\n",
      " [-0.25563432]\n",
      " ...\n",
      " [ 0.17988934]\n",
      " [ 0.07747657]\n",
      " [-0.33121648]]\n",
      "t [[ 0.26176242]\n",
      " [-0.20295481]\n",
      " [-0.25563432]\n",
      " ...\n",
      " [ 0.17988934]\n",
      " [ 0.07747657]\n",
      " [-0.33121648]]\n",
      "Current iteration=6, loss=46089.759503792164\n",
      "t [[ 0.29681548]\n",
      " [-0.24023791]\n",
      " [-0.29255996]\n",
      " ...\n",
      " [ 0.19903584]\n",
      " [ 0.08596706]\n",
      " [-0.37301902]]\n",
      "t [[ 0.29681548]\n",
      " [-0.24023791]\n",
      " [-0.29255996]\n",
      " ...\n",
      " [ 0.19903584]\n",
      " [ 0.08596706]\n",
      " [-0.37301902]]\n",
      "t [[ 0.32986925]\n",
      " [-0.27778541]\n",
      " [-0.32815385]\n",
      " ...\n",
      " [ 0.21594829]\n",
      " [ 0.09348944]\n",
      " [-0.41207598]]\n",
      "t [[ 0.32986925]\n",
      " [-0.27778541]\n",
      " [-0.32815385]\n",
      " ...\n",
      " [ 0.21594829]\n",
      " [ 0.09348944]\n",
      " [-0.41207598]]\n",
      "Current iteration=8, loss=44673.61469946476\n",
      "t [[ 0.36106569]\n",
      " [-0.3154203 ]\n",
      " [-0.36250418]\n",
      " ...\n",
      " [ 0.2308676 ]\n",
      " [ 0.10013179]\n",
      " [-0.44868556]]\n",
      "t [[ 0.36106569]\n",
      " [-0.3154203 ]\n",
      " [-0.36250418]\n",
      " ...\n",
      " [ 0.2308676 ]\n",
      " [ 0.10013179]\n",
      " [-0.44868556]]\n",
      "t [[ 0.39053543]\n",
      " [-0.35299663]\n",
      " [-0.39569128]\n",
      " ...\n",
      " [ 0.24400677]\n",
      " [ 0.10597366]\n",
      " [-0.48310883]]\n",
      "loss=43440.96157365698\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05004672]\n",
      " [-0.0282917 ]\n",
      " [-0.04712136]\n",
      " ...\n",
      " [ 0.0403352 ]\n",
      " [ 0.01703045]\n",
      " [-0.06937399]]\n",
      "t [[ 0.05004672]\n",
      " [-0.0282917 ]\n",
      " [-0.04712136]\n",
      " ...\n",
      " [ 0.0403352 ]\n",
      " [ 0.01703045]\n",
      " [-0.06937399]]\n",
      "t [[ 0.0970375 ]\n",
      " [-0.05887891]\n",
      " [-0.0921784 ]\n",
      " ...\n",
      " [ 0.0760349 ]\n",
      " [ 0.03232383]\n",
      " [-0.13296051]]\n",
      "t [[ 0.0970375 ]\n",
      " [-0.05887891]\n",
      " [-0.0921784 ]\n",
      " ...\n",
      " [ 0.0760349 ]\n",
      " [ 0.03232383]\n",
      " [-0.13296051]]\n",
      "Current iteration=2, loss=49673.456262111424\n",
      "t [[ 0.14117902]\n",
      " [-0.09128091]\n",
      " [-0.1353205 ]\n",
      " ...\n",
      " [ 0.10761447]\n",
      " [ 0.04603526]\n",
      " [-0.19140958]]\n",
      "t [[ 0.14117902]\n",
      " [-0.09128091]\n",
      " [-0.1353205 ]\n",
      " ...\n",
      " [ 0.10761447]\n",
      " [ 0.04603526]\n",
      " [-0.19140958]]\n",
      "t [[ 0.18266933]\n",
      " [-0.12508115]\n",
      " [-0.1766863 ]\n",
      " ...\n",
      " [ 0.13553567]\n",
      " [ 0.05830837]\n",
      " [-0.2453032 ]]\n",
      "t [[ 0.18266933]\n",
      " [-0.12508115]\n",
      " [-0.1766863 ]\n",
      " ...\n",
      " [ 0.13553567]\n",
      " [ 0.05830837]\n",
      " [-0.2453032 ]]\n",
      "Current iteration=4, loss=47765.67275303522\n",
      "t [[ 0.22169606]\n",
      " [-0.15992274]\n",
      " [-0.21640342]\n",
      " ...\n",
      " [ 0.16020812]\n",
      " [ 0.06927487]\n",
      " [-0.29515763]]\n",
      "t [[ 0.22169606]\n",
      " [-0.15992274]\n",
      " [-0.21640342]\n",
      " ...\n",
      " [ 0.16020812]\n",
      " [ 0.06927487]\n",
      " [-0.29515763]]\n",
      "t [[ 0.25843537]\n",
      " [-0.19550271]\n",
      " [-0.2545887 ]\n",
      " ...\n",
      " [ 0.18199332]\n",
      " [ 0.0790548 ]\n",
      " [-0.341428  ]]\n",
      "t [[ 0.25843537]\n",
      " [-0.19550271]\n",
      " [-0.2545887 ]\n",
      " ...\n",
      " [ 0.18199332]\n",
      " [ 0.0790548 ]\n",
      " [-0.341428  ]]\n",
      "Current iteration=6, loss=46139.89613927779\n",
      "t [[ 0.2930514 ]\n",
      " [-0.23156601]\n",
      " [-0.29134871]\n",
      " ...\n",
      " [ 0.20120972]\n",
      " [ 0.08775712]\n",
      " [-0.38451405]]\n",
      "t [[ 0.2930514 ]\n",
      " [-0.23156601]\n",
      " [-0.29134871]\n",
      " ...\n",
      " [ 0.20120972]\n",
      " [ 0.08775712]\n",
      " [-0.38451405]]\n",
      "t [[ 0.3256962 ]\n",
      " [-0.26789954]\n",
      " [-0.32678053]\n",
      " ...\n",
      " [ 0.21813783]\n",
      " [ 0.09548042]\n",
      " [-0.42476617]]\n",
      "t [[ 0.3256962 ]\n",
      " [-0.26789954]\n",
      " [-0.32678053]\n",
      " ...\n",
      " [ 0.21813783]\n",
      " [ 0.09548042]\n",
      " [-0.42476617]]\n",
      "Current iteration=8, loss=44739.19121299153\n",
      "t [[ 0.35651006]\n",
      " [-0.30432653]\n",
      " [-0.3609725 ]\n",
      " ...\n",
      " [ 0.23302506]\n",
      " [ 0.10231381]\n",
      " [-0.46249113]]\n",
      "t [[ 0.35651006]\n",
      " [-0.30432653]\n",
      " [-0.3609725 ]\n",
      " ...\n",
      " [ 0.23302506]\n",
      " [ 0.10231381]\n",
      " [-0.46249113]]\n",
      "t [[ 0.38562203]\n",
      " [-0.34070133]\n",
      " [-0.39400502]\n",
      " ...\n",
      " [ 0.24608986]\n",
      " [ 0.10833772]\n",
      " [-0.49795748]]\n",
      "loss=43520.859947786004\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.04996661]\n",
      " [-0.02972325]\n",
      " [-0.04697335]\n",
      " ...\n",
      " [ 0.07196764]\n",
      " [-0.04697335]\n",
      " [ 0.06740789]]\n",
      "t [[ 0.04996661]\n",
      " [-0.02972325]\n",
      " [-0.04697335]\n",
      " ...\n",
      " [ 0.07196764]\n",
      " [-0.04697335]\n",
      " [ 0.06740789]]\n",
      "t [[ 0.09688282]\n",
      " [-0.06161657]\n",
      " [-0.09190036]\n",
      " ...\n",
      " [ 0.1389968 ]\n",
      " [-0.09190036]\n",
      " [ 0.1293901 ]]\n",
      "t [[ 0.09688282]\n",
      " [-0.06161657]\n",
      " [-0.09190036]\n",
      " ...\n",
      " [ 0.1389968 ]\n",
      " [-0.09190036]\n",
      " [ 0.1293901 ]]\n",
      "Current iteration=2, loss=49677.396888184914\n",
      "t [[ 0.14095682]\n",
      " [-0.09521406]\n",
      " [-0.13492772]\n",
      " ...\n",
      " [ 0.20148181]\n",
      " [-0.13492772]\n",
      " [ 0.18644114]]\n",
      "t [[ 0.14095682]\n",
      " [-0.09521406]\n",
      " [-0.13492772]\n",
      " ...\n",
      " [ 0.20148181]\n",
      " [-0.13492772]\n",
      " [ 0.18644114]]\n",
      "t [[ 0.1823866 ]\n",
      " [-0.1301133 ]\n",
      " [-0.17619175]\n",
      " ...\n",
      " [ 0.25978991]\n",
      " [-0.17619175]\n",
      " [ 0.23901533]]\n",
      "t [[ 0.1823866 ]\n",
      " [-0.1301133 ]\n",
      " [-0.17619175]\n",
      " ...\n",
      " [ 0.25978991]\n",
      " [-0.17619175]\n",
      " [ 0.23901533]]\n",
      "Current iteration=4, loss=47770.34884106092\n",
      "t [[ 0.22135848]\n",
      " [-0.16597068]\n",
      " [-0.21581808]\n",
      " ...\n",
      " [ 0.31426003]\n",
      " [-0.21581808]\n",
      " [ 0.28752648]]\n",
      "t [[ 0.22135848]\n",
      " [-0.16597068]\n",
      " [-0.21581808]\n",
      " ...\n",
      " [ 0.31426003]\n",
      " [-0.21581808]\n",
      " [ 0.28752648]]\n",
      "t [[ 0.25804651]\n",
      " [-0.20249549]\n",
      " [-0.25392189]\n",
      " ...\n",
      " [ 0.36520307]\n",
      " [-0.25392189]\n",
      " [ 0.33234923]]\n",
      "t [[ 0.25804651]\n",
      " [-0.20249549]\n",
      " [-0.25392189]\n",
      " ...\n",
      " [ 0.36520307]\n",
      " [-0.25392189]\n",
      " [ 0.33234923]]\n",
      "Current iteration=6, loss=46143.52313539799\n",
      "t [[ 0.29261241]\n",
      " [-0.23944367]\n",
      " [-0.29060838]\n",
      " ...\n",
      " [ 0.41290306]\n",
      " [-0.29060838]\n",
      " [ 0.37382144]]\n",
      "t [[ 0.29261241]\n",
      " [-0.23944367]\n",
      " [-0.29060838]\n",
      " ...\n",
      " [ 0.41290306]\n",
      " [-0.29060838]\n",
      " [ 0.37382144]]\n",
      "t [[ 0.32520596]\n",
      " [-0.27661172]\n",
      " [-0.32597344]\n",
      " ...\n",
      " [ 0.4576188 ]\n",
      " [-0.32597344]\n",
      " [ 0.41224711]]\n",
      "t [[ 0.32520596]\n",
      " [-0.27661172]\n",
      " [-0.32597344]\n",
      " ...\n",
      " [ 0.4576188 ]\n",
      " [-0.32597344]\n",
      " [ 0.41224711]]\n",
      "Current iteration=8, loss=44740.84485166036\n",
      "t [[ 0.3559655 ]\n",
      " [-0.31383102]\n",
      " [-0.36010443]\n",
      " ...\n",
      " [ 0.49958577]\n",
      " [-0.36010443]\n",
      " [ 0.44789941]]\n",
      "t [[ 0.3559655 ]\n",
      " [-0.31383102]\n",
      " [-0.36010443]\n",
      " ...\n",
      " [ 0.49958577]\n",
      " [-0.36010443]\n",
      " [ 0.44789941]]\n",
      "t [[ 0.38501862]\n",
      " [-0.35096276]\n",
      " [-0.39308091]\n",
      " ...\n",
      " [ 0.53901807]\n",
      " [-0.39308091]\n",
      " [ 0.48102368]]\n",
      "loss=43520.12146939334\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01865351]\n",
      " [-0.08932229]\n",
      " [-0.05237962]\n",
      " ...\n",
      " [ 0.04479153]\n",
      " [ 0.01865351]\n",
      " [-0.07592684]]\n",
      "t [[ 0.01865351]\n",
      " [-0.08932229]\n",
      " [-0.05237962]\n",
      " ...\n",
      " [ 0.04479153]\n",
      " [ 0.01865351]\n",
      " [-0.07592684]]\n",
      "t [[ 0.03519526]\n",
      " [-0.17530385]\n",
      " [-0.10223785]\n",
      " ...\n",
      " [ 0.08397305]\n",
      " [ 0.03519526]\n",
      " [-0.14491546]]\n",
      "t [[ 0.03519526]\n",
      " [-0.17530385]\n",
      " [-0.10223785]\n",
      " ...\n",
      " [ 0.08397305]\n",
      " [ 0.03519526]\n",
      " [-0.14491546]]\n",
      "Current iteration=2, loss=49427.00821183044\n",
      "t [[ 0.0498345 ]\n",
      " [-0.25803974]\n",
      " [-0.14977582]\n",
      " ...\n",
      " [ 0.11823466]\n",
      " [ 0.0498345 ]\n",
      " [-0.20782951]]\n",
      "t [[ 0.0498345 ]\n",
      " [-0.25803974]\n",
      " [-0.14977582]\n",
      " ...\n",
      " [ 0.11823466]\n",
      " [ 0.0498345 ]\n",
      " [-0.20782951]]\n",
      "t [[ 0.0627628 ]\n",
      " [-0.33763668]\n",
      " [-0.19517833]\n",
      " ...\n",
      " [ 0.14818454]\n",
      " [ 0.0627628 ]\n",
      " [-0.26543021]]\n",
      "t [[ 0.0627628 ]\n",
      " [-0.33763668]\n",
      " [-0.19517833]\n",
      " ...\n",
      " [ 0.14818454]\n",
      " [ 0.0627628 ]\n",
      " [-0.26543021]]\n",
      "Current iteration=4, loss=47345.37962553825\n",
      "t [[ 0.0741538 ]\n",
      " [-0.4142094 ]\n",
      " [-0.23861355]\n",
      " ...\n",
      " [ 0.17435338]\n",
      " [ 0.0741538 ]\n",
      " [-0.31838166]]\n",
      "t [[ 0.0741538 ]\n",
      " [-0.4142094 ]\n",
      " [-0.23861355]\n",
      " ...\n",
      " [ 0.17435338]\n",
      " [ 0.0741538 ]\n",
      " [-0.31838166]]\n",
      "t [[ 0.08416375]\n",
      " [-0.48787737]\n",
      " [-0.28023374]\n",
      " ...\n",
      " [ 0.19720244]\n",
      " [ 0.08416375]\n",
      " [-0.36725971]]\n",
      "t [[ 0.08416375]\n",
      " [-0.48787737]\n",
      " [-0.28023374]\n",
      " ...\n",
      " [ 0.19720244]\n",
      " [ 0.08416375]\n",
      " [-0.36725971]]\n",
      "Current iteration=6, loss=45595.69509237472\n",
      "t [[ 0.09293272]\n",
      " [-0.55876214]\n",
      " [-0.32017629]\n",
      " ...\n",
      " [ 0.21713254]\n",
      " [ 0.09293272]\n",
      " [-0.412562  ]]\n",
      "t [[ 0.09293272]\n",
      " [-0.55876214]\n",
      " [-0.32017629]\n",
      " ...\n",
      " [ 0.21713254]\n",
      " [ 0.09293272]\n",
      " [-0.412562  ]]\n",
      "t [[ 0.10058591]\n",
      " [-0.62698525]\n",
      " [-0.35856503]\n",
      " ...\n",
      " [ 0.23449244]\n",
      " [ 0.10058591]\n",
      " [-0.45471791]]\n",
      "t [[ 0.10058591]\n",
      " [-0.62698525]\n",
      " [-0.35856503]\n",
      " ...\n",
      " [ 0.23449244]\n",
      " [ 0.10058591]\n",
      " [-0.45471791]]\n",
      "Current iteration=8, loss=44106.209831713466\n",
      "t [[ 0.1072351 ]\n",
      " [-0.69266671]\n",
      " [-0.39551151]\n",
      " ...\n",
      " [ 0.24958598]\n",
      " [ 0.1072351 ]\n",
      " [-0.49409763]]\n",
      "t [[ 0.1072351 ]\n",
      " [-0.69266671]\n",
      " [-0.39551151]\n",
      " ...\n",
      " [ 0.24958598]\n",
      " [ 0.1072351 ]\n",
      " [-0.49409763]]\n",
      "t [[ 0.11298005]\n",
      " [-0.75592385]\n",
      " [-0.43111629]\n",
      " ...\n",
      " [ 0.26267818]\n",
      " [ 0.11298005]\n",
      " [-0.53102043]]\n",
      "loss=42824.31107030198\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.05632945]\n",
      " [-0.03283781]\n",
      " [-0.05255389]\n",
      " ...\n",
      " [ 0.04413593]\n",
      " [ 0.01858027]\n",
      " [-0.07477938]]\n",
      "t [[ 0.05632945]\n",
      " [-0.03283781]\n",
      " [-0.05255389]\n",
      " ...\n",
      " [ 0.04413593]\n",
      " [ 0.01858027]\n",
      " [-0.07477938]]\n",
      "t [[ 0.10883587]\n",
      " [-0.06849634]\n",
      " [-0.10255952]\n",
      " ...\n",
      " [ 0.08270197]\n",
      " [ 0.03504182]\n",
      " [-0.14262435]]\n",
      "t [[ 0.10883587]\n",
      " [-0.06849634]\n",
      " [-0.10255952]\n",
      " ...\n",
      " [ 0.08270197]\n",
      " [ 0.03504182]\n",
      " [-0.14262435]]\n",
      "Current iteration=2, loss=49421.2280461105\n",
      "t [[ 0.15780713]\n",
      " [-0.10631854]\n",
      " [-0.15022117]\n",
      " ...\n",
      " [ 0.11638644]\n",
      " [ 0.04959521]\n",
      " [-0.20440487]]\n",
      "t [[ 0.15780713]\n",
      " [-0.10631854]\n",
      " [-0.15022117]\n",
      " ...\n",
      " [ 0.11638644]\n",
      " [ 0.04959521]\n",
      " [-0.20440487]]\n",
      "t [[ 0.2035173 ]\n",
      " [-0.145745  ]\n",
      " [-0.19572651]\n",
      " ...\n",
      " [ 0.14579547]\n",
      " [ 0.06243323]\n",
      " [-0.26088727]]\n",
      "t [[ 0.2035173 ]\n",
      " [-0.145745  ]\n",
      " [-0.19572651]\n",
      " ...\n",
      " [ 0.14579547]\n",
      " [ 0.06243323]\n",
      " [-0.26088727]]\n",
      "Current iteration=4, loss=47336.296670123236\n",
      "t [[ 0.24622379]\n",
      " [-0.18630581]\n",
      " [-0.23924632]\n",
      " ...\n",
      " [ 0.17145773]\n",
      " [ 0.07373051]\n",
      " [-0.3127397 ]]\n",
      "t [[ 0.24622379]\n",
      " [-0.18630581]\n",
      " [-0.23924632]\n",
      " ...\n",
      " [ 0.17145773]\n",
      " [ 0.07373051]\n",
      " [-0.3127397 ]]\n",
      "t [[ 0.28616554]\n",
      " [-0.22761077]\n",
      " [-0.28093516]\n",
      " ...\n",
      " [ 0.19383256]\n",
      " [ 0.08364419]\n",
      " [-0.36054116]]\n",
      "t [[ 0.28616554]\n",
      " [-0.22761077]\n",
      " [-0.28093516]\n",
      " ...\n",
      " [ 0.19383256]\n",
      " [ 0.08364419]\n",
      " [-0.36054116]]\n",
      "Current iteration=6, loss=45584.74483515097\n",
      "t [[ 0.32356252]\n",
      " [-0.26933912]\n",
      " [-0.32093246]\n",
      " ...\n",
      " [ 0.21331904]\n",
      " [ 0.09231505]\n",
      " [-0.40479167]]\n",
      "t [[ 0.32356252]\n",
      " [-0.26933912]\n",
      " [-0.32093246]\n",
      " ...\n",
      " [ 0.21331904]\n",
      " [ 0.09231505]\n",
      " [-0.40479167]]\n",
      "t [[ 0.35861599]\n",
      " [-0.3112296 ]\n",
      " [-0.35936382]\n",
      " ...\n",
      " [ 0.23026426]\n",
      " [ 0.0998689 ]\n",
      " [-0.4459223 ]]\n",
      "t [[ 0.35861599]\n",
      " [-0.3112296 ]\n",
      " [-0.35936382]\n",
      " ...\n",
      " [ 0.23026426]\n",
      " [ 0.0998689 ]\n",
      " [-0.4459223 ]]\n",
      "Current iteration=8, loss=44094.22634002665\n",
      "t [[ 0.39150945]\n",
      " [-0.35307124]\n",
      " [-0.39634235]\n",
      " ...\n",
      " [ 0.24497047]\n",
      " [ 0.10641801]\n",
      " [-0.48430442]]\n",
      "t [[ 0.39150945]\n",
      " [-0.35307124]\n",
      " [-0.39634235]\n",
      " ...\n",
      " [ 0.24497047]\n",
      " [ 0.10641801]\n",
      " [-0.48430442]]\n",
      "t [[ 0.42240994]\n",
      " [-0.39469513]\n",
      " [-0.43196997]\n",
      " ...\n",
      " [ 0.25770115]\n",
      " [ 0.11206259]\n",
      " [-0.52025796]]\n",
      "loss=42811.78301306418\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05560747]\n",
      " [-0.03143522]\n",
      " [-0.05235707]\n",
      " ...\n",
      " [ 0.04481689]\n",
      " [ 0.01892273]\n",
      " [-0.07708221]]\n",
      "t [[ 0.05560747]\n",
      " [-0.03143522]\n",
      " [-0.05235707]\n",
      " ...\n",
      " [ 0.04481689]\n",
      " [ 0.01892273]\n",
      " [-0.07708221]]\n",
      "t [[ 0.10744272]\n",
      " [-0.06570393]\n",
      " [-0.10216591]\n",
      " ...\n",
      " [ 0.08391173]\n",
      " [ 0.03570121]\n",
      " [-0.14702045]]\n",
      "t [[ 0.10744272]\n",
      " [-0.06570393]\n",
      " [-0.10216591]\n",
      " ...\n",
      " [ 0.08391173]\n",
      " [ 0.03570121]\n",
      " [-0.14702045]]\n",
      "Current iteration=2, loss=49439.85708148578\n",
      "t [[ 0.1557905 ]\n",
      " [-0.10214601]\n",
      " [-0.14963212]\n",
      " ...\n",
      " [ 0.11799283]\n",
      " [ 0.05054892]\n",
      " [-0.21070832]]\n",
      "t [[ 0.1557905 ]\n",
      " [-0.10214601]\n",
      " [-0.14963212]\n",
      " ...\n",
      " [ 0.11799283]\n",
      " [ 0.05054892]\n",
      " [-0.21070832]]\n",
      "t [[ 0.20092166]\n",
      " [-0.14020028]\n",
      " [-0.19494445]\n",
      " ...\n",
      " [ 0.14768522]\n",
      " [ 0.06366121]\n",
      " [-0.26893385]]\n",
      "t [[ 0.20092166]\n",
      " [-0.14020028]\n",
      " [-0.19494445]\n",
      " ...\n",
      " [ 0.14768522]\n",
      " [ 0.06366121]\n",
      " [-0.26893385]]\n",
      "Current iteration=4, loss=47373.758307696175\n",
      "t [[ 0.24309035]\n",
      " [-0.17939604]\n",
      " [-0.23827455]\n",
      " ...\n",
      " [ 0.17353434]\n",
      " [ 0.07521502]\n",
      " [-0.32238447]]\n",
      "t [[ 0.24309035]\n",
      " [-0.17939604]\n",
      " [-0.23827455]\n",
      " ...\n",
      " [ 0.17353434]\n",
      " [ 0.07521502]\n",
      " [-0.32238447]]\n",
      "t [[ 0.28253254]\n",
      " [-0.21934287]\n",
      " [-0.27977758]\n",
      " ...\n",
      " [ 0.19601378]\n",
      " [ 0.08536949]\n",
      " [-0.37165599]]\n",
      "t [[ 0.28253254]\n",
      " [-0.21934287]\n",
      " [-0.27977758]\n",
      " ...\n",
      " [ 0.19601378]\n",
      " [ 0.08536949]\n",
      " [-0.37165599]]\n",
      "Current iteration=6, loss=45640.19418667483\n",
      "t [[ 0.31946554]\n",
      " [-0.2597201 ]\n",
      " [-0.31959338]\n",
      " ...\n",
      " [ 0.21553431]\n",
      " [ 0.09426712]\n",
      " [-0.41726284]]\n",
      "t [[ 0.31946554]\n",
      " [-0.2597201 ]\n",
      " [-0.31959338]\n",
      " ...\n",
      " [ 0.21553431]\n",
      " [ 0.09426712]\n",
      " [-0.41726284]]\n",
      "t [[ 0.3540882 ]\n",
      " [-0.30026675]\n",
      " [-0.35784781]\n",
      " ...\n",
      " [ 0.23245253]\n",
      " [ 0.10203519]\n",
      " [-0.45964848]]\n",
      "t [[ 0.3540882 ]\n",
      " [-0.30026675]\n",
      " [-0.35784781]\n",
      " ...\n",
      " [ 0.23245253]\n",
      " [ 0.10203519]\n",
      " [-0.45964848]]\n",
      "Current iteration=8, loss=44166.38030851222\n",
      "t [[ 0.38658185]\n",
      " [-0.34077225]\n",
      " [-0.39465409]\n",
      " ...\n",
      " [ 0.24707851]\n",
      " [ 0.10878726]\n",
      " [-0.49919491]]\n",
      "t [[ 0.38658185]\n",
      " [-0.34077225]\n",
      " [-0.39465409]\n",
      " ...\n",
      " [ 0.24707851]\n",
      " [ 0.10878726]\n",
      " [-0.49919491]]\n",
      "t [[ 0.41711148]\n",
      " [-0.38106822]\n",
      " [-0.43011415]\n",
      " ...\n",
      " [ 0.25968227]\n",
      " [ 0.11462459]\n",
      " [-0.53623129]]\n",
      "loss=42899.23522536875\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.05551846]\n",
      " [-0.03302584]\n",
      " [-0.05219261]\n",
      " ...\n",
      " [ 0.07996404]\n",
      " [-0.05219261]\n",
      " [ 0.07489765]]\n",
      "t [[ 0.05551846]\n",
      " [-0.03302584]\n",
      " [-0.05219261]\n",
      " ...\n",
      " [ 0.07996404]\n",
      " [-0.05219261]\n",
      " [ 0.07489765]]\n",
      "t [[ 0.10727149]\n",
      " [-0.06873035]\n",
      " [-0.10185919]\n",
      " ...\n",
      " [ 0.15383202]\n",
      " [-0.10185919]\n",
      " [ 0.14309788]]\n",
      "t [[ 0.10727149]\n",
      " [-0.06873035]\n",
      " [-0.10185919]\n",
      " ...\n",
      " [ 0.15383202]\n",
      " [-0.10185919]\n",
      " [ 0.14309788]]\n",
      "Current iteration=2, loss=49444.05420674929\n",
      "t [[ 0.1555458 ]\n",
      " [-0.10647394]\n",
      " [-0.14920163]\n",
      " ...\n",
      " [ 0.2221466 ]\n",
      " [-0.14920163]\n",
      " [ 0.20528091]]\n",
      "t [[ 0.1555458 ]\n",
      " [-0.10647394]\n",
      " [-0.14920163]\n",
      " ...\n",
      " [ 0.2221466 ]\n",
      " [-0.14920163]\n",
      " [ 0.20528091]]\n",
      "t [[ 0.20061179]\n",
      " [-0.14571478]\n",
      " [-0.19440554]\n",
      " ...\n",
      " [ 0.28540787]\n",
      " [-0.19440554]\n",
      " [ 0.26206424]]\n",
      "t [[ 0.20061179]\n",
      " [-0.14571478]\n",
      " [-0.19440554]\n",
      " ...\n",
      " [ 0.28540787]\n",
      " [-0.19440554]\n",
      " [ 0.26206424]]\n",
      "Current iteration=4, loss=47378.37136080803\n",
      "t [[ 0.24272142]\n",
      " [-0.18600013]\n",
      " [-0.23763997]\n",
      " ...\n",
      " [ 0.3440719 ]\n",
      " [-0.23763997]\n",
      " [ 0.31400276]]\n",
      "t [[ 0.24272142]\n",
      " [-0.18600013]\n",
      " [-0.23763997]\n",
      " ...\n",
      " [ 0.3440719 ]\n",
      " [-0.23763997]\n",
      " [ 0.31400276]]\n",
      "t [[ 0.28210752]\n",
      " [-0.22695581]\n",
      " [-0.27905798]\n",
      " ...\n",
      " [ 0.39855189]\n",
      " [-0.27905798]\n",
      " [ 0.3615919 ]]\n",
      "t [[ 0.28210752]\n",
      " [-0.22695581]\n",
      " [-0.27905798]\n",
      " ...\n",
      " [ 0.39855189]\n",
      " [-0.27905798]\n",
      " [ 0.3615919 ]]\n",
      "Current iteration=6, loss=45643.277588695615\n",
      "t [[ 0.31898411]\n",
      " [-0.26827527]\n",
      " [-0.31879764]\n",
      " ...\n",
      " [ 0.4492206 ]\n",
      " [-0.31879764]\n",
      " [ 0.40527228]]\n",
      "t [[ 0.31898411]\n",
      " [-0.26827527]\n",
      " [-0.31879764]\n",
      " ...\n",
      " [ 0.4492206 ]\n",
      " [-0.31879764]\n",
      " [ 0.40527228]]\n",
      "t [[ 0.35354721]\n",
      " [-0.3097095 ]\n",
      " [-0.35698337]\n",
      " ...\n",
      " [ 0.49641356]\n",
      " [-0.35698337]\n",
      " [ 0.44543495]]\n",
      "t [[ 0.35354721]\n",
      " [-0.3097095 ]\n",
      " [-0.35698337]\n",
      " ...\n",
      " [ 0.49641356]\n",
      " [-0.35698337]\n",
      " [ 0.44543495]]\n",
      "Current iteration=8, loss=44167.02956812945\n",
      "t [[ 0.38597596]\n",
      " [-0.35105773]\n",
      " [-0.39372719]\n",
      " ...\n",
      " [ 0.54043244]\n",
      " [-0.39372719]\n",
      " [ 0.48242669]]\n",
      "t [[ 0.38597596]\n",
      " [-0.35105773]\n",
      " [-0.39372719]\n",
      " ...\n",
      " [ 0.54043244]\n",
      " [-0.39372719]\n",
      " [ 0.48242669]]\n",
      "t [[ 0.41643392]\n",
      " [-0.3921595 ]\n",
      " [-0.42913001]\n",
      " ...\n",
      " [ 0.58154844]\n",
      " [-0.42913001]\n",
      " [ 0.51655506]]\n",
      "loss=42897.11240708742\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02051886]\n",
      " [-0.09825452]\n",
      " [-0.05761758]\n",
      " ...\n",
      " [ 0.04927068]\n",
      " [ 0.02051886]\n",
      " [-0.08351952]]\n",
      "t [[ 0.02051886]\n",
      " [-0.09825452]\n",
      " [-0.05761758]\n",
      " ...\n",
      " [ 0.04927068]\n",
      " [ 0.02051886]\n",
      " [-0.08351952]]\n",
      "t [[ 0.03848289]\n",
      " [-0.19246716]\n",
      " [-0.11218469]\n",
      " ...\n",
      " [ 0.09175422]\n",
      " [ 0.03848289]\n",
      " [-0.15864511]]\n",
      "t [[ 0.03848289]\n",
      " [-0.19246716]\n",
      " [-0.11218469]\n",
      " ...\n",
      " [ 0.09175422]\n",
      " [ 0.03848289]\n",
      " [-0.15864511]]\n",
      "Current iteration=2, loss=49195.42270237483\n",
      "t [[ 0.05417137]\n",
      " [-0.28276547]\n",
      " [-0.16396989]\n",
      " ...\n",
      " [ 0.12837062]\n",
      " [ 0.05417137]\n",
      " [-0.22652852]]\n",
      "t [[ 0.05417137]\n",
      " [-0.28276547]\n",
      " [-0.16396989]\n",
      " ...\n",
      " [ 0.12837062]\n",
      " [ 0.05417137]\n",
      " [-0.22652852]]\n",
      "t [[ 0.06783705]\n",
      " [-0.36929371]\n",
      " [-0.21321713]\n",
      " ...\n",
      " [ 0.15991802]\n",
      " [ 0.06783705]\n",
      " [-0.28816909]]\n",
      "t [[ 0.06783705]\n",
      " [-0.36929371]\n",
      " [-0.21321713]\n",
      " ...\n",
      " [ 0.15991802]\n",
      " [ 0.06783705]\n",
      " [-0.28816909]]\n",
      "Current iteration=4, loss=46962.53332762555\n",
      "t [[ 0.07970603]\n",
      " [-0.45220691]\n",
      " [-0.26014576]\n",
      " ...\n",
      " [ 0.18708154]\n",
      " [ 0.07970603]\n",
      " [-0.34442399]]\n",
      "t [[ 0.07970603]\n",
      " [-0.45220691]\n",
      " [-0.26014576]\n",
      " ...\n",
      " [ 0.18708154]\n",
      " [ 0.07970603]\n",
      " [-0.34442399]]\n",
      "t [[ 0.08997912]\n",
      " [-0.5316658 ]\n",
      " [-0.30495185]\n",
      " ...\n",
      " [ 0.21044731]\n",
      " [ 0.08997912]\n",
      " [-0.3960238 ]]\n",
      "t [[ 0.08997912]\n",
      " [-0.5316658 ]\n",
      " [-0.30495185]\n",
      " ...\n",
      " [ 0.21044731]\n",
      " [ 0.08997912]\n",
      " [-0.3960238 ]]\n",
      "Current iteration=6, loss=45113.69919208919\n",
      "t [[ 0.09883388]\n",
      " [-0.60783286]\n",
      " [-0.3478102 ]\n",
      " ...\n",
      " [ 0.23051712]\n",
      " [ 0.09883388]\n",
      " [-0.44358925]]\n",
      "t [[ 0.09883388]\n",
      " [-0.60783286]\n",
      " [-0.3478102 ]\n",
      " ...\n",
      " [ 0.23051712]\n",
      " [ 0.09883388]\n",
      " [-0.44358925]]\n",
      "t [[ 0.106427  ]\n",
      " [-0.68086931]\n",
      " [-0.38887654]\n",
      " ...\n",
      " [ 0.24772131]\n",
      " [ 0.106427  ]\n",
      " [-0.48764704]]\n",
      "t [[ 0.106427  ]\n",
      " [-0.68086931]\n",
      " [-0.38887654]\n",
      " ...\n",
      " [ 0.24772131]\n",
      " [ 0.106427  ]\n",
      " [-0.48764704]]\n",
      "Current iteration=8, loss=43559.77094919375\n",
      "t [[ 0.1128966 ]\n",
      " [-0.75093303]\n",
      " [-0.42828958]\n",
      " ...\n",
      " [ 0.26242956]\n",
      " [ 0.1128966 ]\n",
      " [-0.52864407]]\n",
      "t [[ 0.1128966 ]\n",
      " [-0.75093303]\n",
      " [-0.42828958]\n",
      " ...\n",
      " [ 0.26242956]\n",
      " [ 0.1128966 ]\n",
      " [-0.52864407]]\n",
      "t [[ 0.1183645 ]\n",
      " [-0.81817709]\n",
      " [-0.46617313]\n",
      " ...\n",
      " [ 0.27495974]\n",
      " [ 0.1183645 ]\n",
      " [-0.5669597 ]]\n",
      "loss=42237.0771533831\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.06196239]\n",
      " [-0.03612159]\n",
      " [-0.05780928]\n",
      " ...\n",
      " [ 0.04854953]\n",
      " [ 0.0204383 ]\n",
      " [-0.08225731]]\n",
      "t [[ 0.06196239]\n",
      " [-0.03612159]\n",
      " [-0.05780928]\n",
      " ...\n",
      " [ 0.04854953]\n",
      " [ 0.0204383 ]\n",
      " [-0.08225731]]\n",
      "t [[ 0.11929967]\n",
      " [-0.07565561]\n",
      " [-0.11253558]\n",
      " ...\n",
      " [ 0.09036044]\n",
      " [ 0.03831334]\n",
      " [-0.15612531]]\n",
      "t [[ 0.11929967]\n",
      " [-0.07565561]\n",
      " [-0.11253558]\n",
      " ...\n",
      " [ 0.09036044]\n",
      " [ 0.03831334]\n",
      " [-0.15612531]]\n",
      "Current iteration=2, loss=49189.189063778744\n",
      "t [[ 0.17239661]\n",
      " [-0.11772671]\n",
      " [-0.16445167]\n",
      " ...\n",
      " [ 0.12635032]\n",
      " [ 0.05390617]\n",
      " [-0.22276415]]\n",
      "t [[ 0.17239661]\n",
      " [-0.11772671]\n",
      " [-0.16445167]\n",
      " ...\n",
      " [ 0.12635032]\n",
      " [ 0.05390617]\n",
      " [-0.22276415]]\n",
      "t [[ 0.22161671]\n",
      " [-0.16160401]\n",
      " [-0.2138053 ]\n",
      " ...\n",
      " [ 0.15731461]\n",
      " [ 0.06747112]\n",
      " [-0.28317978]]\n",
      "t [[ 0.22161671]\n",
      " [-0.16160401]\n",
      " [-0.2138053 ]\n",
      " ...\n",
      " [ 0.15731461]\n",
      " [ 0.06747112]\n",
      " [-0.28317978]]\n",
      "Current iteration=4, loss=46952.949239538124\n",
      "t [[ 0.26729782]\n",
      " [-0.20668691]\n",
      " [-0.26081918]\n",
      " ...\n",
      " [ 0.18393573]\n",
      " [ 0.0792356 ]\n",
      " [-0.3382345 ]]\n",
      "t [[ 0.26729782]\n",
      " [-0.20668691]\n",
      " [-0.26081918]\n",
      " ...\n",
      " [ 0.18393573]\n",
      " [ 0.0792356 ]\n",
      " [-0.3382345 ]]\n",
      "t [[ 0.30974996]\n",
      " [-0.25248853]\n",
      " [-0.30569235]\n",
      " ...\n",
      " [ 0.20679735]\n",
      " [ 0.08940148]\n",
      " [-0.38866269]]\n",
      "t [[ 0.30974996]\n",
      " [-0.25248853]\n",
      " [-0.30569235]\n",
      " ...\n",
      " [ 0.20679735]\n",
      " [ 0.08940148]\n",
      " [-0.38866269]]\n",
      "Current iteration=6, loss=45102.3401205487\n",
      "t [[ 0.34925503]\n",
      " [-0.29861877]\n",
      " [-0.34860215]\n",
      " ...\n",
      " [ 0.22639899]\n",
      " [ 0.09814722]\n",
      " [-0.43508783]]\n",
      "t [[ 0.34925503]\n",
      " [-0.29861877]\n",
      " [-0.34860215]\n",
      " ...\n",
      " [ 0.22639899]\n",
      " [ 0.09814722]\n",
      " [-0.43508783]]\n",
      "t [[ 0.38606794]\n",
      " [-0.34476849]\n",
      " [-0.3897065 ]\n",
      " ...\n",
      " [ 0.24316884]\n",
      " [ 0.10563024]\n",
      " [-0.47803847]]\n",
      "t [[ 0.38606794]\n",
      " [-0.34476849]\n",
      " [-0.3897065 ]\n",
      " ...\n",
      " [ 0.24316884]\n",
      " [ 0.10563024]\n",
      " [-0.47803847]]\n",
      "Current iteration=8, loss=43547.50217545025\n",
      "t [[ 0.42041848]\n",
      " [-0.39069522]\n",
      " [-0.42914603]\n",
      " ...\n",
      " [ 0.25747448]\n",
      " [ 0.11198925]\n",
      " [-0.51796261]]\n",
      "t [[ 0.42041848]\n",
      " [-0.39069522]\n",
      " [-0.42914603]\n",
      " ...\n",
      " [ 0.25747448]\n",
      " [ 0.11198925]\n",
      " [-0.51796261]]\n",
      "t [[ 0.45251369]\n",
      " [-0.43621088]\n",
      " [-0.46704616]\n",
      " ...\n",
      " [ 0.26963175]\n",
      " [ 0.11734656]\n",
      " [-0.55524012]]\n",
      "loss=42224.378413405764\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06116822]\n",
      " [-0.03457874]\n",
      " [-0.05759278]\n",
      " ...\n",
      " [ 0.04929858]\n",
      " [ 0.020815  ]\n",
      " [-0.08479043]]\n",
      "t [[ 0.06116822]\n",
      " [-0.03457874]\n",
      " [-0.05759278]\n",
      " ...\n",
      " [ 0.04929858]\n",
      " [ 0.020815  ]\n",
      " [-0.08479043]]\n",
      "t [[ 0.11777277]\n",
      " [-0.07258537]\n",
      " [-0.11210263]\n",
      " ...\n",
      " [ 0.09167453]\n",
      " [ 0.03903588]\n",
      " [-0.16093806]]\n",
      "t [[ 0.11777277]\n",
      " [-0.07258537]\n",
      " [-0.11210263]\n",
      " ...\n",
      " [ 0.09167453]\n",
      " [ 0.03903588]\n",
      " [-0.16093806]]\n",
      "Current iteration=2, loss=49209.714165054975\n",
      "t [[ 0.17019428]\n",
      " [-0.11314046]\n",
      " [-0.16380406]\n",
      " ...\n",
      " [ 0.12807228]\n",
      " [ 0.05494755]\n",
      " [-0.2296347 ]]\n",
      "t [[ 0.17019428]\n",
      " [-0.11314046]\n",
      " [-0.16380406]\n",
      " ...\n",
      " [ 0.12807228]\n",
      " [ 0.05494755]\n",
      " [-0.2296347 ]]\n",
      "t [[ 0.21879187]\n",
      " [-0.15551096]\n",
      " [-0.21294626]\n",
      " ...\n",
      " [ 0.15931218]\n",
      " [ 0.06880774]\n",
      " [-0.29191488]]\n",
      "t [[ 0.21879187]\n",
      " [-0.15551096]\n",
      " [-0.21294626]\n",
      " ...\n",
      " [ 0.15931218]\n",
      " [ 0.06880774]\n",
      " [-0.29191488]]\n",
      "Current iteration=4, loss=46994.12169708167\n",
      "t [[ 0.26389914]\n",
      " [-0.19909546]\n",
      " [-0.25975299]\n",
      " ...\n",
      " [ 0.18609823]\n",
      " [ 0.08084683]\n",
      " [-0.34866583]]\n",
      "t [[ 0.26389914]\n",
      " [-0.19909546]\n",
      " [-0.25975299]\n",
      " ...\n",
      " [ 0.18609823]\n",
      " [ 0.08084683]\n",
      " [-0.34866583]]\n",
      "t [[ 0.30582231]\n",
      " [-0.24340693]\n",
      " [-0.30442399]\n",
      " ...\n",
      " [ 0.20903182]\n",
      " [ 0.09126923]\n",
      " [-0.40064318]]\n",
      "t [[ 0.30582231]\n",
      " [-0.24340693]\n",
      " [-0.30442399]\n",
      " ...\n",
      " [ 0.20903182]\n",
      " [ 0.09126923]\n",
      " [-0.40064318]]\n",
      "Current iteration=6, loss=45163.00041507535\n",
      "t [[ 0.3448399 ]\n",
      " [-0.28805554]\n",
      " [-0.34713707]\n",
      " ...\n",
      " [ 0.22862661]\n",
      " [ 0.10025553]\n",
      " [-0.4484883 ]]\n",
      "t [[ 0.3448399 ]\n",
      " [-0.28805554]\n",
      " [-0.34713707]\n",
      " ...\n",
      " [ 0.22862661]\n",
      " [ 0.10025553]\n",
      " [-0.4484883 ]]\n",
      "t [[ 0.38120378]\n",
      " [-0.33273262]\n",
      " [-0.38805035]\n",
      " ...\n",
      " [ 0.24532212]\n",
      " [ 0.10796494]\n",
      " [-0.4927449 ]]\n",
      "t [[ 0.38120378]\n",
      " [-0.33273262]\n",
      " [-0.38805035]\n",
      " ...\n",
      " [ 0.24532212]\n",
      " [ 0.10796494]\n",
      " [-0.4927449 ]]\n",
      "Current iteration=8, loss=43626.024187674506\n",
      "t [[ 0.41514098]\n",
      " [-0.37719637]\n",
      " [-0.42730451]\n",
      " ...\n",
      " [ 0.2594951 ]\n",
      " [ 0.11453772]\n",
      " [-0.53387381]]\n",
      "t [[ 0.41514098]\n",
      " [-0.37719637]\n",
      " [-0.42730451]\n",
      " ...\n",
      " [ 0.2594951 ]\n",
      " [ 0.11453772]\n",
      " [-0.53387381]]\n",
      "t [[ 0.44685591]\n",
      " [-0.42125948]\n",
      " [-0.46502493]\n",
      " ...\n",
      " [ 0.2714691 ]\n",
      " [ 0.12009745]\n",
      " [-0.57226593]]\n",
      "loss=42319.055444430116\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.0610703 ]\n",
      " [-0.03632842]\n",
      " [-0.05741187]\n",
      " ...\n",
      " [ 0.08796044]\n",
      " [-0.05741187]\n",
      " [ 0.08238742]]\n",
      "t [[ 0.0610703 ]\n",
      " [-0.03632842]\n",
      " [-0.05741187]\n",
      " ...\n",
      " [ 0.08796044]\n",
      " [-0.05741187]\n",
      " [ 0.08238742]]\n",
      "t [[ 0.11758512]\n",
      " [-0.07589749]\n",
      " [-0.11176766]\n",
      " ...\n",
      " [ 0.16854576]\n",
      " [-0.11176766]\n",
      " [ 0.1566722 ]]\n",
      "t [[ 0.11758512]\n",
      " [-0.07589749]\n",
      " [-0.11176766]\n",
      " ...\n",
      " [ 0.16854576]\n",
      " [-0.11176766]\n",
      " [ 0.1566722 ]]\n",
      "Current iteration=2, loss=49214.13581450993\n",
      "t [[ 0.16992746]\n",
      " [-0.11785529]\n",
      " [-0.16333694]\n",
      " ...\n",
      " [ 0.24248043]\n",
      " [-0.16333694]\n",
      " [ 0.22376215]]\n",
      "t [[ 0.16992746]\n",
      " [-0.11785529]\n",
      " [-0.16333694]\n",
      " ...\n",
      " [ 0.24248043]\n",
      " [-0.16333694]\n",
      " [ 0.22376215]]\n",
      "t [[ 0.21845544]\n",
      " [-0.16149452]\n",
      " [-0.2123648 ]\n",
      " ...\n",
      " [ 0.31042481]\n",
      " [-0.2123648 ]\n",
      " [ 0.28447104]]\n",
      "t [[ 0.21845544]\n",
      " [-0.16149452]\n",
      " [-0.2123648 ]\n",
      " ...\n",
      " [ 0.31042481]\n",
      " [-0.2123648 ]\n",
      " [ 0.28447104]]\n",
      "Current iteration=4, loss=46998.594585061684\n",
      "t [[ 0.26349929]\n",
      " [-0.20623724]\n",
      " [-0.25907169]\n",
      " ...\n",
      " [ 0.37297379]\n",
      " [-0.25907169]\n",
      " [ 0.33952013]]\n",
      "t [[ 0.26349929]\n",
      " [-0.20623724]\n",
      " [-0.25907169]\n",
      " ...\n",
      " [ 0.37297379]\n",
      " [-0.25907169]\n",
      " [ 0.33952013]]\n",
      "t [[ 0.30536084]\n",
      " [-0.25161713]\n",
      " [-0.30365474]\n",
      " ...\n",
      " [ 0.43065948]\n",
      " [-0.30365474]\n",
      " [ 0.38954428]]\n",
      "t [[ 0.30536084]\n",
      " [-0.25161713]\n",
      " [-0.30365474]\n",
      " ...\n",
      " [ 0.43065948]\n",
      " [-0.30365474]\n",
      " [ 0.38954428]]\n",
      "Current iteration=6, loss=45165.454094129964\n",
      "t [[ 0.34431449]\n",
      " [-0.29726188]\n",
      " [-0.34628959]\n",
      " ...\n",
      " [ 0.48395598]\n",
      " [-0.34628959]\n",
      " [ 0.43510025]]\n",
      "t [[ 0.34431449]\n",
      " [-0.29726188]\n",
      " [-0.34628959]\n",
      " ...\n",
      " [ 0.48395598]\n",
      " [-0.34628959]\n",
      " [ 0.43510025]]\n",
      "t [[ 0.3806088 ]\n",
      " [-0.3428771 ]\n",
      " [-0.38713265]\n",
      " ...\n",
      " [ 0.53328479]\n",
      " [-0.38713265]\n",
      " [ 0.47667549]]\n",
      "t [[ 0.3806088 ]\n",
      " [-0.3428771 ]\n",
      " [-0.38713265]\n",
      " ...\n",
      " [ 0.53328479]\n",
      " [-0.38713265]\n",
      " [ 0.47667549]]\n",
      "Current iteration=8, loss=43625.60691114378\n",
      "t [[ 0.4144686 ]\n",
      " [-0.3882323 ]\n",
      " [-0.42632318]\n",
      " ...\n",
      " [ 0.57902051]\n",
      " [-0.42632318]\n",
      " [ 0.51469666]]\n",
      "t [[ 0.4144686 ]\n",
      " [-0.3882323 ]\n",
      " [-0.42632318]\n",
      " ...\n",
      " [ 0.57902051]\n",
      " [-0.42632318]\n",
      " [ 0.51469666]]\n",
      "t [[ 0.44609701]\n",
      " [-0.43314899]\n",
      " [-0.46398537]\n",
      " ...\n",
      " [ 0.62149614]\n",
      " [-0.46398537]\n",
      " [ 0.54953752]]\n",
      "loss=42315.52689475991\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02238421]\n",
      " [-0.10718675]\n",
      " [-0.06285554]\n",
      " ...\n",
      " [ 0.05374983]\n",
      " [ 0.02238421]\n",
      " [-0.09111221]]\n",
      "t [[ 0.02238421]\n",
      " [-0.10718675]\n",
      " [-0.06285554]\n",
      " ...\n",
      " [ 0.05374983]\n",
      " [ 0.02238421]\n",
      " [-0.09111221]]\n",
      "t [[ 0.04172848]\n",
      " [-0.20956384]\n",
      " [-0.12208131]\n",
      " ...\n",
      " [ 0.09942368]\n",
      " [ 0.04172848]\n",
      " [-0.17223664]]\n",
      "t [[ 0.04172848]\n",
      " [-0.20956384]\n",
      " [-0.12208131]\n",
      " ...\n",
      " [ 0.09942368]\n",
      " [ 0.04172848]\n",
      " [-0.17223664]]\n",
      "Current iteration=2, loss=48967.21709389023\n",
      "t [[ 0.05839633]\n",
      " [-0.30729822]\n",
      " [-0.17802701]\n",
      " ...\n",
      " [ 0.13821765]\n",
      " [ 0.05839633]\n",
      " [-0.24487129]]\n",
      "t [[ 0.05839633]\n",
      " [-0.30729822]\n",
      " [-0.17802701]\n",
      " ...\n",
      " [ 0.13821765]\n",
      " [ 0.05839633]\n",
      " [-0.24487129]]\n",
      "t [[ 0.07271284]\n",
      " [-0.40057987]\n",
      " [-0.23100661]\n",
      " ...\n",
      " [ 0.1711529 ]\n",
      " [ 0.07271284]\n",
      " [-0.31029502]]\n",
      "t [[ 0.07271284]\n",
      " [-0.40057987]\n",
      " [-0.23100661]\n",
      " ...\n",
      " [ 0.1711529 ]\n",
      " [ 0.07271284]\n",
      " [-0.31029502]]\n",
      "Current iteration=4, loss=46591.44107788619\n",
      "t [[ 0.08496493]\n",
      " [-0.48961277]\n",
      " [-0.28129887]\n",
      " ...\n",
      " [ 0.19909142]\n",
      " [ 0.08496493]\n",
      " [-0.36958585]]\n",
      "t [[ 0.08496493]\n",
      " [-0.48961277]\n",
      " [-0.28129887]\n",
      " ...\n",
      " [ 0.19909142]\n",
      " [ 0.08496493]\n",
      " [-0.36958585]]\n",
      "t [[ 0.09540397]\n",
      " [-0.57460732]\n",
      " [-0.32914983]\n",
      " ...\n",
      " [ 0.22275911]\n",
      " [ 0.09540397]\n",
      " [-0.42364669]]\n",
      "t [[ 0.09540397]\n",
      " [-0.57460732]\n",
      " [-0.32914983]\n",
      " ...\n",
      " [ 0.22275911]\n",
      " [ 0.09540397]\n",
      " [-0.42364669]]\n",
      "Current iteration=6, loss=44652.87585871165\n",
      "t [[ 0.10424927]\n",
      " [-0.65577468]\n",
      " [-0.37477622]\n",
      " ...\n",
      " [ 0.24276823]\n",
      " [ 0.10424927]\n",
      " [-0.47323164]]\n",
      "t [[ 0.10424927]\n",
      " [-0.65577468]\n",
      " [-0.37477622]\n",
      " ...\n",
      " [ 0.24276823]\n",
      " [ 0.10424927]\n",
      " [-0.47323164]]\n",
      "t [[ 0.11169185]\n",
      " [-0.73332272]\n",
      " [-0.41836883]\n",
      " ...\n",
      " [ 0.25963632]\n",
      " [ 0.11169185]\n",
      " [-0.51897006]]\n",
      "t [[ 0.11169185]\n",
      " [-0.73332272]\n",
      " [-0.41836883]\n",
      " ...\n",
      " [ 0.25963632]\n",
      " [ 0.11169185]\n",
      " [-0.51897006]]\n",
      "Current iteration=8, loss=43043.41009914562\n",
      "t [[ 0.11789808]\n",
      " [-0.8074533 ]\n",
      " [-0.4600959 ]\n",
      " ...\n",
      " [ 0.27380159]\n",
      " [ 0.11789808]\n",
      " [-0.56138737]]\n",
      "t [[ 0.11789808]\n",
      " [-0.8074533 ]\n",
      " [-0.4600959 ]\n",
      " ...\n",
      " [ 0.27380159]\n",
      " [ 0.11789808]\n",
      " [-0.56138737]]\n",
      "t [[ 0.12301295]\n",
      " [-0.87836045]\n",
      " [-0.50010602]\n",
      " ...\n",
      " [ 0.28563557]\n",
      " [ 0.12301295]\n",
      " [-0.60092279]]\n",
      "loss=41687.75824607316\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.06759533]\n",
      " [-0.03940537]\n",
      " [-0.06306467]\n",
      " ...\n",
      " [ 0.05296312]\n",
      " [ 0.02229633]\n",
      " [-0.08973525]]\n",
      "t [[ 0.06759533]\n",
      " [-0.03940537]\n",
      " [-0.06306467]\n",
      " ...\n",
      " [ 0.05296312]\n",
      " [ 0.02229633]\n",
      " [-0.08973525]]\n",
      "t [[ 0.12968738]\n",
      " [-0.08287098]\n",
      " [-0.12246089]\n",
      " ...\n",
      " [ 0.09790797]\n",
      " [ 0.04154268]\n",
      " [-0.16948823]]\n",
      "t [[ 0.12968738]\n",
      " [-0.08287098]\n",
      " [-0.12246089]\n",
      " ...\n",
      " [ 0.09790797]\n",
      " [ 0.04154268]\n",
      " [-0.16948823]]\n",
      "Current iteration=2, loss=48960.55011462279\n",
      "t [[ 0.18677771]\n",
      " [-0.12925946]\n",
      " [-0.17854384]\n",
      " ...\n",
      " [ 0.13602754]\n",
      " [ 0.05810491]\n",
      " [-0.24076784]]\n",
      "t [[ 0.18677771]\n",
      " [-0.12925946]\n",
      " [-0.17854384]\n",
      " ...\n",
      " [ 0.13602754]\n",
      " [ 0.05810491]\n",
      " [-0.24076784]]\n",
      "t [[ 0.23933621]\n",
      " [-0.17764008]\n",
      " [-0.23163237]\n",
      " ...\n",
      " [ 0.16833939]\n",
      " [ 0.07231007]\n",
      " [-0.3048613 ]]\n",
      "t [[ 0.23933621]\n",
      " [-0.17764008]\n",
      " [-0.23163237]\n",
      " ...\n",
      " [ 0.16833939]\n",
      " [ 0.07231007]\n",
      " [-0.3048613 ]]\n",
      "Current iteration=4, loss=46581.40723126406\n",
      "t [[ 0.28779492]\n",
      " [-0.22726523]\n",
      " [-0.28200951]\n",
      " ...\n",
      " [ 0.19570209]\n",
      " [ 0.08444673]\n",
      " [-0.3628529 ]]\n",
      "t [[ 0.28779492]\n",
      " [-0.22726523]\n",
      " [-0.28200951]\n",
      " ...\n",
      " [ 0.19570209]\n",
      " [ 0.08444673]\n",
      " [-0.3628529 ]]\n",
      "t [[ 0.33254552]\n",
      " [-0.27754375]\n",
      " [-0.32992498]\n",
      " ...\n",
      " [ 0.21883845]\n",
      " [ 0.09476758]\n",
      " [-0.41565004]]\n",
      "t [[ 0.33254552]\n",
      " [-0.27754375]\n",
      " [-0.32992498]\n",
      " ...\n",
      " [ 0.21883845]\n",
      " [ 0.09476758]\n",
      " [-0.41565004]]\n",
      "Current iteration=6, loss=44641.174713791115\n",
      "t [[ 0.37393985]\n",
      " [-0.32801451]\n",
      " [-0.3755986 ]\n",
      " ...\n",
      " [ 0.23835784]\n",
      " [ 0.10349298]\n",
      " [-0.46400989]]\n",
      "t [[ 0.37393985]\n",
      " [-0.32801451]\n",
      " [-0.3755986 ]\n",
      " ...\n",
      " [ 0.23835784]\n",
      " [ 0.10349298]\n",
      " [-0.46400989]]\n",
      "t [[ 0.41229231]\n",
      " [-0.37832238]\n",
      " [-0.41922386]\n",
      " ...\n",
      " [ 0.25477504]\n",
      " [ 0.11081483]\n",
      " [-0.50856366]]\n",
      "t [[ 0.41229231]\n",
      " [-0.37832238]\n",
      " [-0.41922386]\n",
      " ...\n",
      " [ 0.25477504]\n",
      " [ 0.11081483]\n",
      " [-0.50856366]]\n",
      "Current iteration=8, loss=43030.92369748092\n",
      "t [[ 0.44788332]\n",
      " [-0.42819737]\n",
      " [-0.4609712 ]\n",
      " ...\n",
      " [ 0.26852558]\n",
      " [ 0.11690015]\n",
      " [-0.54983773]]\n",
      "t [[ 0.44788332]\n",
      " [-0.42819737]\n",
      " [-0.4609712 ]\n",
      " ...\n",
      " [ 0.26852558]\n",
      " [ 0.11690015]\n",
      " [-0.54983773]]\n",
      "t [[ 0.48096303]\n",
      " [-0.47743714]\n",
      " [-0.50099113]\n",
      " ...\n",
      " [ 0.2799784 ]\n",
      " [ 0.12189449]\n",
      " [-0.58827155]]\n",
      "loss=41674.948677381224\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06672897]\n",
      " [-0.03772226]\n",
      " [-0.06282848]\n",
      " ...\n",
      " [ 0.05378027]\n",
      " [ 0.02270727]\n",
      " [-0.09249865]]\n",
      "t [[ 0.06672897]\n",
      " [-0.03772226]\n",
      " [-0.06282848]\n",
      " ...\n",
      " [ 0.05378027]\n",
      " [ 0.02270727]\n",
      " [-0.09249865]]\n",
      "t [[ 0.12802774]\n",
      " [-0.07952316]\n",
      " [-0.12198859]\n",
      " ...\n",
      " [ 0.09932342]\n",
      " [ 0.04232787]\n",
      " [-0.17471349]]\n",
      "t [[ 0.12802774]\n",
      " [-0.07952316]\n",
      " [-0.12198859]\n",
      " ...\n",
      " [ 0.09932342]\n",
      " [ 0.04232787]\n",
      " [-0.17471349]]\n",
      "Current iteration=2, loss=48982.97429741201\n",
      "t [[ 0.18439247]\n",
      " [-0.12426006]\n",
      " [-0.17783778]\n",
      " ...\n",
      " [ 0.13785752]\n",
      " [ 0.05923264]\n",
      " [-0.24819469]]\n",
      "t [[ 0.18439247]\n",
      " [-0.12426006]\n",
      " [-0.17783778]\n",
      " ...\n",
      " [ 0.13785752]\n",
      " [ 0.05923264]\n",
      " [-0.24819469]]\n",
      "t [[ 0.23628731]\n",
      " [-0.17099965]\n",
      " [-0.2306967 ]\n",
      " ...\n",
      " [ 0.17043214]\n",
      " [ 0.07375304]\n",
      " [-0.31426619]]\n",
      "t [[ 0.23628731]\n",
      " [-0.17099965]\n",
      " [-0.2306967 ]\n",
      " ...\n",
      " [ 0.17043214]\n",
      " [ 0.07375304]\n",
      " [-0.31426619]]\n",
      "Current iteration=4, loss=46626.26111087784\n",
      "t [[ 0.28413891]\n",
      " [-0.21899352]\n",
      " [-0.28084964]\n",
      " ...\n",
      " [ 0.1979328 ]\n",
      " [ 0.08618135]\n",
      " [-0.37404361]]\n",
      "t [[ 0.28413891]\n",
      " [-0.21899352]\n",
      " [-0.28084964]\n",
      " ...\n",
      " [ 0.1979328 ]\n",
      " [ 0.08618135]\n",
      " [-0.37404361]]\n",
      "t [[ 0.32833418]\n",
      " [-0.26765049]\n",
      " [-0.32854712]\n",
      " ...\n",
      " [ 0.22110372]\n",
      " [ 0.09677338]\n",
      " [-0.42846052]]\n",
      "t [[ 0.32833418]\n",
      " [-0.26765049]\n",
      " [-0.32854712]\n",
      " ...\n",
      " [ 0.22110372]\n",
      " [ 0.09677338]\n",
      " [-0.42846052]]\n",
      "Current iteration=6, loss=44706.93810042018\n",
      "t [[ 0.36922077]\n",
      " [-0.31650994]\n",
      " [-0.37400942]\n",
      " ...\n",
      " [ 0.24057096]\n",
      " [ 0.10575209]\n",
      " [-0.47829575]]\n",
      "t [[ 0.36922077]\n",
      " [-0.31650994]\n",
      " [-0.37400942]\n",
      " ...\n",
      " [ 0.24057096]\n",
      " [ 0.10575209]\n",
      " [-0.47829575]]\n",
      "t [[ 0.40710935]\n",
      " [-0.36521752]\n",
      " [-0.41743016]\n",
      " ...\n",
      " [ 0.25686244]\n",
      " [ 0.1133115 ]\n",
      " [-0.52419857]]\n",
      "t [[ 0.40710935]\n",
      " [-0.36521752]\n",
      " [-0.41743016]\n",
      " ...\n",
      " [ 0.25686244]\n",
      " [ 0.1133115 ]\n",
      " [-0.52419857]]\n",
      "Current iteration=8, loss=43115.6022086139\n",
      "t [[ 0.44227686]\n",
      " [-0.41350419]\n",
      " [-0.45897977]\n",
      " ...\n",
      " [ 0.27042435]\n",
      " [ 0.11962044]\n",
      " [-0.56671051]]\n",
      "t [[ 0.44227686]\n",
      " [-0.41350419]\n",
      " [-0.45897977]\n",
      " ...\n",
      " [ 0.27042435]\n",
      " [ 0.11962044]\n",
      " [-0.56671051]]\n",
      "t [[ 0.47497014]\n",
      " [-0.4611687 ]\n",
      " [-0.49880861]\n",
      " ...\n",
      " [ 0.28163447]\n",
      " [ 0.12482592]\n",
      " [-0.60628387]]\n",
      "loss=41776.52921266821\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.06662215]\n",
      " [-0.039631  ]\n",
      " [-0.06263114]\n",
      " ...\n",
      " [ 0.09595685]\n",
      " [-0.06263114]\n",
      " [ 0.08987718]]\n",
      "t [[ 0.06662215]\n",
      " [-0.039631  ]\n",
      " [-0.06263114]\n",
      " ...\n",
      " [ 0.09595685]\n",
      " [-0.06263114]\n",
      " [ 0.08987718]]\n",
      "t [[ 0.12782377]\n",
      " [-0.08311792]\n",
      " [-0.12162582]\n",
      " ...\n",
      " [ 0.18313811]\n",
      " [-0.12162582]\n",
      " [ 0.17011319]]\n",
      "t [[ 0.12782377]\n",
      " [-0.08311792]\n",
      " [-0.12162582]\n",
      " ...\n",
      " [ 0.18313811]\n",
      " [-0.12162582]\n",
      " [ 0.17011319]]\n",
      "Current iteration=2, loss=48987.58963631379\n",
      "t [[ 0.18410388]\n",
      " [-0.12935412]\n",
      " [-0.17733509]\n",
      " ...\n",
      " [ 0.2624871 ]\n",
      " [-0.17733509]\n",
      " [ 0.24188954]]\n",
      "t [[ 0.18410388]\n",
      " [-0.12935412]\n",
      " [-0.17733509]\n",
      " ...\n",
      " [ 0.2624871 ]\n",
      " [-0.17733509]\n",
      " [ 0.24188954]]\n",
      "t [[ 0.23592478]\n",
      " [-0.17743958]\n",
      " [-0.23007441]\n",
      " ...\n",
      " [ 0.33485384]\n",
      " [-0.23007441]\n",
      " [ 0.30625169]]\n",
      "t [[ 0.23592478]\n",
      " [-0.17743958]\n",
      " [-0.23007441]\n",
      " ...\n",
      " [ 0.33485384]\n",
      " [-0.23007441]\n",
      " [ 0.30625169]]\n",
      "Current iteration=4, loss=46630.525226911566\n",
      "t [[ 0.28370825]\n",
      " [-0.22665575]\n",
      " [-0.28012397]\n",
      " ...\n",
      " [ 0.40099445]\n",
      " [-0.28012397]\n",
      " [ 0.36411315]]\n",
      "t [[ 0.28370825]\n",
      " [-0.22665575]\n",
      " [-0.28012397]\n",
      " ...\n",
      " [ 0.40099445]\n",
      " [-0.28012397]\n",
      " [ 0.36411315]]\n",
      "t [[ 0.32783549]\n",
      " [-0.27643711]\n",
      " [-0.32773109]\n",
      " ...\n",
      " [ 0.46157662]\n",
      " [-0.32773109]\n",
      " [ 0.41626654]]\n",
      "t [[ 0.32783549]\n",
      " [-0.27643711]\n",
      " [-0.32773109]\n",
      " ...\n",
      " [ 0.46157662]\n",
      " [-0.32773109]\n",
      " [ 0.41626654]]\n",
      "Current iteration=6, loss=44708.69363592554\n",
      "t [[ 0.3686492 ]\n",
      " [-0.326344  ]\n",
      " [-0.37311351]\n",
      " ...\n",
      " [ 0.51718773]\n",
      " [-0.37311351]\n",
      " [ 0.46339719]]\n",
      "t [[ 0.3686492 ]\n",
      " [-0.326344  ]\n",
      " [-0.37311351]\n",
      " ...\n",
      " [ 0.51718773]\n",
      " [-0.37311351]\n",
      " [ 0.46339719]]\n",
      "t [[ 0.4064565 ]\n",
      " [-0.37603854]\n",
      " [-0.41646282]\n",
      " ...\n",
      " [ 0.56834377]\n",
      " [-0.41646282]\n",
      " [ 0.50609711]]\n",
      "t [[ 0.4064565 ]\n",
      " [-0.37603854]\n",
      " [-0.41646282]\n",
      " ...\n",
      " [ 0.56834377]\n",
      " [-0.41646282]\n",
      " [ 0.50609711]]\n",
      "Current iteration=8, loss=43114.07894936678\n",
      "t [[ 0.44153224]\n",
      " [-0.42526433]\n",
      " [-0.45794783]\n",
      " ...\n",
      " [ 0.61549806]\n",
      " [-0.45794783]\n",
      " [ 0.54487791]]\n",
      "t [[ 0.44153224]\n",
      " [-0.42526433]\n",
      " [-0.45794783]\n",
      " ...\n",
      " [ 0.61549806]\n",
      " [-0.45794783]\n",
      " [ 0.54487791]]\n",
      "t [[ 0.47412237]\n",
      " [-0.47382963]\n",
      " [-0.49771756]\n",
      " ...\n",
      " [ 0.65904924]\n",
      " [-0.49771756]\n",
      " [ 0.58018241]]\n",
      "loss=41771.59549283171\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02424956]\n",
      " [-0.11611898]\n",
      " [-0.0680935 ]\n",
      " ...\n",
      " [ 0.05822899]\n",
      " [ 0.02424956]\n",
      " [-0.09870489]]\n",
      "t [[ 0.02424956]\n",
      " [-0.11611898]\n",
      " [-0.0680935 ]\n",
      " ...\n",
      " [ 0.05822899]\n",
      " [ 0.02424956]\n",
      " [-0.09870489]]\n",
      "t [[ 0.04493207]\n",
      " [-0.22659395]\n",
      " [-0.13192775]\n",
      " ...\n",
      " [ 0.10698151]\n",
      " [ 0.04493207]\n",
      " [-0.18569019]]\n",
      "t [[ 0.04493207]\n",
      " [-0.22659395]\n",
      " [-0.13192775]\n",
      " ...\n",
      " [ 0.10698151]\n",
      " [ 0.04493207]\n",
      " [-0.18569019]]\n",
      "Current iteration=2, loss=48742.339823196264\n",
      "t [[ 0.06251086]\n",
      " [-0.33163883]\n",
      " [-0.19194862]\n",
      " ...\n",
      " [ 0.14778034]\n",
      " [ 0.06251086]\n",
      " [-0.26286362]]\n",
      "t [[ 0.06251086]\n",
      " [-0.33163883]\n",
      " [-0.19194862]\n",
      " ...\n",
      " [ 0.14778034]\n",
      " [ 0.06251086]\n",
      " [-0.26286362]]\n",
      "t [[ 0.07739511]\n",
      " [-0.43149847]\n",
      " [-0.2485516 ]\n",
      " ...\n",
      " [ 0.1819041 ]\n",
      " [ 0.07739511]\n",
      " [-0.33182688]]\n",
      "t [[ 0.07739511]\n",
      " [-0.43149847]\n",
      " [-0.2485516 ]\n",
      " ...\n",
      " [ 0.1819041 ]\n",
      " [ 0.07739511]\n",
      " [-0.33182688]]\n",
      "Current iteration=4, loss=46231.63200200881\n",
      "t [[ 0.08994108]\n",
      " [-0.52643522]\n",
      " [-0.30208332]\n",
      " ...\n",
      " [ 0.21041413]\n",
      " [ 0.08994108]\n",
      " [-0.39390636]]\n",
      "t [[ 0.08994108]\n",
      " [-0.52643522]\n",
      " [-0.30208332]\n",
      " ...\n",
      " [ 0.21041413]\n",
      " [ 0.08994108]\n",
      " [-0.39390636]]\n",
      "t [[ 0.10045658]\n",
      " [-0.61671818]\n",
      " [-0.35284588]\n",
      " ...\n",
      " [ 0.23419019]\n",
      " [ 0.10045658]\n",
      " [-0.4501935 ]]\n",
      "t [[ 0.10045658]\n",
      " [-0.61671818]\n",
      " [-0.35284588]\n",
      " ...\n",
      " [ 0.23419019]\n",
      " [ 0.10045658]\n",
      " [-0.4501935 ]]\n",
      "Current iteration=6, loss=44211.96477489799\n",
      "t [[ 0.1092066 ]\n",
      " [-0.70261536]\n",
      " [-0.40110212]\n",
      " ...\n",
      " [ 0.25396331]\n",
      " [ 0.1092066 ]\n",
      " [-0.50158432]]\n",
      "t [[ 0.1092066 ]\n",
      " [-0.70261536]\n",
      " [-0.40110212]\n",
      " ...\n",
      " [ 0.25396331]\n",
      " [ 0.1092066 ]\n",
      " [-0.50158432]]\n",
      "t [[ 0.11641901]\n",
      " [-0.78438843]\n",
      " [-0.44708091]\n",
      " ...\n",
      " [ 0.27034243]\n",
      " [ 0.11641901]\n",
      " [-0.54881425]]\n",
      "t [[ 0.11641901]\n",
      " [-0.78438843]\n",
      " [-0.44708091]\n",
      " ...\n",
      " [ 0.27034243]\n",
      " [ 0.11641901]\n",
      " [-0.54881425]]\n",
      "Current iteration=8, loss=42554.864078556646\n",
      "t [[ 0.12228985]\n",
      " [-0.86228931]\n",
      " [-0.4909819 ]\n",
      " ...\n",
      " [ 0.28383575]\n",
      " [ 0.12228985]\n",
      " [-0.59248758]]\n",
      "t [[ 0.12228985]\n",
      " [-0.86228931]\n",
      " [-0.4909819 ]\n",
      " ...\n",
      " [ 0.28383575]\n",
      " [ 0.12228985]\n",
      " [-0.59248758]]\n",
      "t [[ 0.12698809]\n",
      " [-0.93655807]\n",
      " [-0.53297982]\n",
      " ...\n",
      " [ 0.29486812]\n",
      " [ 0.12698809]\n",
      " [-0.63310182]]\n",
      "loss=41173.003706388634\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.07322828]\n",
      " [-0.04268915]\n",
      " [-0.06832006]\n",
      " ...\n",
      " [ 0.05737671]\n",
      " [ 0.02415436]\n",
      " [-0.09721319]]\n",
      "t [[ 0.07322828]\n",
      " [-0.04268915]\n",
      " [-0.06832006]\n",
      " ...\n",
      " [ 0.05737671]\n",
      " [ 0.02415436]\n",
      " [-0.09721319]]\n",
      "t [[ 0.13999907]\n",
      " [-0.09014238]\n",
      " [-0.13233548]\n",
      " ...\n",
      " [ 0.10534469]\n",
      " [ 0.04472989]\n",
      " [-0.18271326]]\n",
      "t [[ 0.13999907]\n",
      " [-0.09014238]\n",
      " [-0.13233548]\n",
      " ...\n",
      " [ 0.10534469]\n",
      " [ 0.04472989]\n",
      " [-0.18271326]]\n",
      "Current iteration=2, loss=48735.25903272216\n",
      "t [[ 0.20095258]\n",
      " [-0.14091262]\n",
      " [-0.19249912]\n",
      " ...\n",
      " [ 0.14542264]\n",
      " [ 0.0621929 ]\n",
      " [-0.25842178]]\n",
      "t [[ 0.20095258]\n",
      " [-0.14091262]\n",
      " [-0.19249912]\n",
      " ...\n",
      " [ 0.14542264]\n",
      " [ 0.0621929 ]\n",
      " [-0.25842178]]\n",
      "t [[ 0.25668324]\n",
      " [-0.19384002]\n",
      " [-0.24921263]\n",
      " ...\n",
      " [ 0.17888466]\n",
      " [ 0.07695504]\n",
      " [-0.32595082]]\n",
      "t [[ 0.25668324]\n",
      " [-0.19384002]\n",
      " [-0.24921263]\n",
      " ...\n",
      " [ 0.17888466]\n",
      " [ 0.07695504]\n",
      " [-0.32595082]]\n",
      "Current iteration=4, loss=46221.19537998829\n",
      "t [[ 0.30773146]\n",
      " [-0.24801463]\n",
      " [-0.30282791]\n",
      " ...\n",
      " [ 0.20678781]\n",
      " [ 0.08937456]\n",
      " [-0.38663424]]\n",
      "t [[ 0.30773146]\n",
      " [-0.24801463]\n",
      " [-0.30282791]\n",
      " ...\n",
      " [ 0.20678781]\n",
      " [ 0.08937456]\n",
      " [-0.38663424]]\n",
      "t [[ 0.35458117]\n",
      " [-0.30273523]\n",
      " [-0.3536515 ]\n",
      " ...\n",
      " [ 0.230008  ]\n",
      " [ 0.09976087]\n",
      " [-0.44156864]]\n",
      "t [[ 0.35458117]\n",
      " [-0.30273523]\n",
      " [-0.3536515 ]\n",
      " ...\n",
      " [ 0.230008  ]\n",
      " [ 0.09976087]\n",
      " [-0.44156864]]\n",
      "Current iteration=6, loss=44199.979294995326\n",
      "t [[ 0.39766177]\n",
      " [-0.35746989]\n",
      " [-0.40195001]\n",
      " ...\n",
      " [ 0.24927265]\n",
      " [ 0.1083802 ]\n",
      " [-0.49165332]]\n",
      "t [[ 0.39766177]\n",
      " [-0.35746989]\n",
      " [-0.40195001]\n",
      " ...\n",
      " [ 0.24927265]\n",
      " [ 0.1083802 ]\n",
      " [-0.49165332]]\n",
      "t [[ 0.43735265]\n",
      " [-0.4118211 ]\n",
      " [-0.4479554 ]\n",
      " ...\n",
      " [ 0.26518725]\n",
      " [ 0.11546137]\n",
      " [-0.53762549]]\n",
      "t [[ 0.43735265]\n",
      " [-0.4118211 ]\n",
      " [-0.4479554 ]\n",
      " ...\n",
      " [ 0.26518725]\n",
      " [ 0.11546137]\n",
      " [-0.53762549]]\n",
      "Current iteration=8, loss=42542.21545221131\n",
      "t [[ 0.47398869]\n",
      " [-0.46549656]\n",
      " [-0.49186996]\n",
      " ...\n",
      " [ 0.27825663]\n",
      " [ 0.12120122]\n",
      " [-0.58009008]]\n",
      "t [[ 0.47398869]\n",
      " [-0.46549656]\n",
      " [-0.49186996]\n",
      " ...\n",
      " [ 0.27825663]\n",
      " [ 0.12120122]\n",
      " [-0.58009008]]\n",
      "t [[ 0.50786579]\n",
      " [-0.51828545]\n",
      " [-0.53387057]\n",
      " ...\n",
      " [ 0.2889024 ]\n",
      " [ 0.1257693 ]\n",
      " [-0.61954442]]\n",
      "loss=41160.12940965386\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07228971]\n",
      " [-0.04086579]\n",
      " [-0.06806419]\n",
      " ...\n",
      " [ 0.05826196]\n",
      " [ 0.02459954]\n",
      " [-0.10020687]]\n",
      "t [[ 0.07228971]\n",
      " [-0.04086579]\n",
      " [-0.06806419]\n",
      " ...\n",
      " [ 0.05826196]\n",
      " [ 0.02459954]\n",
      " [-0.10020687]]\n",
      "t [[ 0.1382077 ]\n",
      " [-0.08651724]\n",
      " [-0.13182384]\n",
      " ...\n",
      " [ 0.1068585 ]\n",
      " [ 0.04557721]\n",
      " [-0.18834689]]\n",
      "t [[ 0.1382077 ]\n",
      " [-0.08651724]\n",
      " [-0.13182384]\n",
      " ...\n",
      " [ 0.1068585 ]\n",
      " [ 0.04557721]\n",
      " [-0.18834689]]\n",
      "Current iteration=2, loss=48759.58460015023\n",
      "t [[ 0.19838719]\n",
      " [-0.13550063]\n",
      " [-0.19173472]\n",
      " ...\n",
      " [ 0.14735325]\n",
      " [ 0.06340569]\n",
      " [-0.26639433]]\n",
      "t [[ 0.19838719]\n",
      " [-0.13550063]\n",
      " [-0.19173472]\n",
      " ...\n",
      " [ 0.14735325]\n",
      " [ 0.06340569]\n",
      " [-0.26639433]]\n",
      "t [[ 0.25341533]\n",
      " [-0.18665314]\n",
      " [-0.24820069]\n",
      " ...\n",
      " [ 0.18106047]\n",
      " [ 0.07850213]\n",
      " [-0.33600734]]\n",
      "t [[ 0.25341533]\n",
      " [-0.18665314]\n",
      " [-0.24820069]\n",
      " ...\n",
      " [ 0.18106047]\n",
      " [ 0.07850213]\n",
      " [-0.33600734]]\n",
      "Current iteration=4, loss=46269.69742661914\n",
      "t [[ 0.30382581]\n",
      " [-0.23906403]\n",
      " [-0.30157513]\n",
      " ...\n",
      " [ 0.20907001]\n",
      " [ 0.09122937]\n",
      " [-0.39855832]]\n",
      "t [[ 0.30382581]\n",
      " [-0.23906403]\n",
      " [-0.30157513]\n",
      " ...\n",
      " [ 0.20907001]\n",
      " [ 0.09122937]\n",
      " [-0.39855832]]\n",
      "t [[ 0.35009674]\n",
      " [-0.29203233]\n",
      " [-0.35216545]\n",
      " ...\n",
      " [ 0.23228308]\n",
      " [ 0.10190056]\n",
      " [-0.45517536]]\n",
      "t [[ 0.35009674]\n",
      " [-0.29203233]\n",
      " [-0.35216545]\n",
      " ...\n",
      " [ 0.23228308]\n",
      " [ 0.10190056]\n",
      " [-0.45517536]]\n",
      "Current iteration=6, loss=44270.733519474175\n",
      "t [[ 0.39265245]\n",
      " [-0.34502688]\n",
      " [-0.40023865]\n",
      " ...\n",
      " [ 0.25144637]\n",
      " [ 0.11078499]\n",
      " [-0.50678339]]\n",
      "t [[ 0.39265245]\n",
      " [-0.34502688]\n",
      " [-0.40023865]\n",
      " ...\n",
      " [ 0.25144637]\n",
      " [ 0.11078499]\n",
      " [-0.50678339]]\n",
      "t [[ 0.43186771]\n",
      " [-0.39765134]\n",
      " [-0.44602677]\n",
      " ...\n",
      " [ 0.26718042]\n",
      " [ 0.118114  ]\n",
      " [-0.55414074]]\n",
      "t [[ 0.43186771]\n",
      " [-0.39765134]\n",
      " [-0.44602677]\n",
      " ...\n",
      " [ 0.26718042]\n",
      " [ 0.118114  ]\n",
      " [-0.55414074]]\n",
      "Current iteration=8, loss=42632.83948275994\n",
      "t [[ 0.46807313]\n",
      " [-0.44961476]\n",
      " [-0.48973195]\n",
      " ...\n",
      " [ 0.28000225]\n",
      " [ 0.12408645]\n",
      " [-0.59786988]]\n",
      "t [[ 0.46807313]\n",
      " [-0.44961476]\n",
      " [-0.48973195]\n",
      " ...\n",
      " [ 0.28000225]\n",
      " [ 0.12408645]\n",
      " [-0.59786988]]\n",
      "t [[ 0.50156055]\n",
      " [-0.50070777]\n",
      " [-0.5315308 ]\n",
      " ...\n",
      " [ 0.29034354]\n",
      " [ 0.12887357]\n",
      " [-0.63848288]]\n",
      "loss=41268.30264682483\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.072174  ]\n",
      " [-0.04293359]\n",
      " [-0.0678504 ]\n",
      " ...\n",
      " [ 0.10395325]\n",
      " [-0.0678504 ]\n",
      " [ 0.09736695]]\n",
      "t [[ 0.072174  ]\n",
      " [-0.04293359]\n",
      " [-0.0678504 ]\n",
      " ...\n",
      " [ 0.10395325]\n",
      " [-0.0678504 ]\n",
      " [ 0.09736695]]\n",
      "t [[ 0.13798752]\n",
      " [-0.09039159]\n",
      " [-0.1314337 ]\n",
      " ...\n",
      " [ 0.19760921]\n",
      " [-0.1314337 ]\n",
      " [ 0.18342099]]\n",
      "t [[ 0.13798752]\n",
      " [-0.09039159]\n",
      " [-0.1314337 ]\n",
      " ...\n",
      " [ 0.19760921]\n",
      " [-0.1314337 ]\n",
      " [ 0.18342099]]\n",
      "Current iteration=2, loss=48764.36392071444\n",
      "t [[ 0.19807716]\n",
      " [-0.1409664 ]\n",
      " [-0.19119752]\n",
      " ...\n",
      " [ 0.28217048]\n",
      " [-0.19119752]\n",
      " [ 0.25966781]]\n",
      "t [[ 0.19807716]\n",
      " [-0.1409664 ]\n",
      " [-0.19119752]\n",
      " ...\n",
      " [ 0.28217048]\n",
      " [-0.19119752]\n",
      " [ 0.25966781]]\n",
      "t [[ 0.25302703]\n",
      " [-0.19353731]\n",
      " [-0.24753924]\n",
      " ...\n",
      " [ 0.35870797]\n",
      " [-0.24753924]\n",
      " [ 0.32742198]]\n",
      "t [[ 0.25302703]\n",
      " [-0.19353731]\n",
      " [-0.24753924]\n",
      " ...\n",
      " [ 0.35870797]\n",
      " [-0.24753924]\n",
      " [ 0.32742198]]\n",
      "Current iteration=4, loss=46273.692021511044\n",
      "t [[ 0.30336418]\n",
      " [-0.24723068]\n",
      " [-0.30080729]\n",
      " ...\n",
      " [ 0.42816202]\n",
      " [-0.30080729]\n",
      " [ 0.3878155 ]]\n",
      "t [[ 0.30336418]\n",
      " [-0.24723068]\n",
      " [-0.30080729]\n",
      " ...\n",
      " [ 0.42816202]\n",
      " [-0.30080729]\n",
      " [ 0.3878155 ]]\n",
      "t [[ 0.34955963]\n",
      " [-0.30137642]\n",
      " [-0.3513053 ]\n",
      " ...\n",
      " [ 0.49135222]\n",
      " [-0.3513053 ]\n",
      " [ 0.44181636]]\n",
      "t [[ 0.34955963]\n",
      " [-0.30137642]\n",
      " [-0.3513053 ]\n",
      " ...\n",
      " [ 0.49135222]\n",
      " [-0.3513053 ]\n",
      " [ 0.44181636]]\n",
      "Current iteration=6, loss=44271.73759499824\n",
      "t [[ 0.39203203]\n",
      " [-0.35546778]\n",
      " [-0.39929729]\n",
      " ...\n",
      " [ 0.54899055]\n",
      " [-0.39929729]\n",
      " [ 0.49024988]]\n",
      "t [[ 0.39203203]\n",
      " [-0.35546778]\n",
      " [-0.39929729]\n",
      " ...\n",
      " [ 0.54899055]\n",
      " [-0.39929729]\n",
      " [ 0.49024988]]\n",
      "t [[ 0.4311526 ]\n",
      " [-0.40912691]\n",
      " [-0.445013  ]\n",
      " ...\n",
      " [ 0.60169505]\n",
      " [-0.445013  ]\n",
      " [ 0.53381949]]\n",
      "t [[ 0.4311526 ]\n",
      " [-0.40912691]\n",
      " [-0.445013  ]\n",
      " ...\n",
      " [ 0.60169505]\n",
      " [-0.445013  ]\n",
      " [ 0.53381949]]\n",
      "Current iteration=8, loss=42630.188479025455\n",
      "t [[ 0.46725021]\n",
      " [-0.46207653]\n",
      " [-0.48865272]\n",
      " ...\n",
      " [ 0.65000262]\n",
      " [-0.48865272]\n",
      " [ 0.57312562]]\n",
      "t [[ 0.46725021]\n",
      " [-0.46207653]\n",
      " [-0.48865272]\n",
      " ...\n",
      " [ 0.65000262]\n",
      " [-0.48865272]\n",
      " [ 0.57312562]]\n",
      "t [[ 0.50061631]\n",
      " [-0.51411731]\n",
      " [-0.53039161]\n",
      " ...\n",
      " [ 0.69438054]\n",
      " [-0.53039161]\n",
      " [ 0.60868191]]\n",
      "loss=41261.97965252866\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02611491]\n",
      " [-0.12505121]\n",
      " [-0.07333147]\n",
      " ...\n",
      " [ 0.06270814]\n",
      " [ 0.02611491]\n",
      " [-0.10629758]]\n",
      "t [[ 0.02611491]\n",
      " [-0.12505121]\n",
      " [-0.07333147]\n",
      " ...\n",
      " [ 0.06270814]\n",
      " [ 0.02611491]\n",
      " [-0.10629758]]\n",
      "t [[ 0.04809371]\n",
      " [-0.24355753]\n",
      " [-0.14172406]\n",
      " ...\n",
      " [ 0.11442785]\n",
      " [ 0.04809371]\n",
      " [-0.19900591]]\n",
      "t [[ 0.04809371]\n",
      " [-0.24355753]\n",
      " [-0.14172406]\n",
      " ...\n",
      " [ 0.11442785]\n",
      " [ 0.04809371]\n",
      " [-0.19900591]]\n",
      "Current iteration=2, loss=48520.73966655198\n",
      "t [[ 0.06651643]\n",
      " [-0.35578817]\n",
      " [-0.20573615]\n",
      " ...\n",
      " [ 0.15706324]\n",
      " [ 0.06651643]\n",
      " [-0.28051134]]\n",
      "t [[ 0.06651643]\n",
      " [-0.35578817]\n",
      " [-0.20573615]\n",
      " ...\n",
      " [ 0.15706324]\n",
      " [ 0.06651643]\n",
      " [-0.28051134]]\n",
      "t [[ 0.08188874]\n",
      " [-0.46205287]\n",
      " [-0.26585688]\n",
      " ...\n",
      " [ 0.19218628]\n",
      " [ 0.08188874]\n",
      " [-0.35278325]]\n",
      "t [[ 0.08188874]\n",
      " [-0.46205287]\n",
      " [-0.26585688]\n",
      " ...\n",
      " [ 0.19218628]\n",
      " [ 0.08188874]\n",
      " [-0.35278325]]\n",
      "Current iteration=4, loss=45882.65630286461\n",
      "t [[ 0.09464479]\n",
      " [-0.5626826 ]\n",
      " [-0.32250932]\n",
      " ...\n",
      " [ 0.22107978]\n",
      " [ 0.09464479]\n",
      " [-0.41742328]]\n",
      "t [[ 0.09464479]\n",
      " [-0.5626826 ]\n",
      " [-0.32250932]\n",
      " ...\n",
      " [ 0.22107978]\n",
      " [ 0.09464479]\n",
      " [-0.41742328]]\n",
      "t [[ 0.10515452]\n",
      " [-0.65801471]\n",
      " [-0.37605748]\n",
      " ...\n",
      " [ 0.24479042]\n",
      " [ 0.10515452]\n",
      " [-0.47572597]]\n",
      "t [[ 0.10515452]\n",
      " [-0.65801471]\n",
      " [-0.37605748]\n",
      " ...\n",
      " [ 0.24479042]\n",
      " [ 0.10515452]\n",
      " [-0.47572597]]\n",
      "Current iteration=6, loss=43789.79771080013\n",
      "t [[ 0.11373214]\n",
      " [-0.74838253]\n",
      " [-0.42681434]\n",
      " ...\n",
      " [ 0.26417493]\n",
      " [ 0.11373214]\n",
      " [-0.52873589]]\n",
      "t [[ 0.11373214]\n",
      " [-0.74838253]\n",
      " [-0.42681434]\n",
      " ...\n",
      " [ 0.26417493]\n",
      " [ 0.11373214]\n",
      " [-0.52873589]]\n",
      "t [[ 0.12064446]\n",
      " [-0.83410877]\n",
      " [-0.4750494 ]\n",
      " ...\n",
      " [ 0.27993653]\n",
      " [ 0.12064446]\n",
      " [-0.57729626]]\n",
      "t [[ 0.12064446]\n",
      " [-0.83410877]\n",
      " [-0.4750494 ]\n",
      " ...\n",
      " [ 0.27993653]\n",
      " [ 0.12064446]\n",
      " [-0.57729626]]\n",
      "Current iteration=8, loss=42092.08333360234\n",
      "t [[ 0.12611835]\n",
      " [-0.91550145]\n",
      " [-0.52099544]\n",
      " ...\n",
      " [ 0.29265364]\n",
      " [ 0.12611835]\n",
      " [-0.62208906]]\n",
      "t [[ 0.12611835]\n",
      " [-0.91550145]\n",
      " [-0.52099544]\n",
      " ...\n",
      " [ 0.29265364]\n",
      " [ 0.12611835]\n",
      " [-0.62208906]]\n",
      "t [[ 0.13034716]\n",
      " [-0.99285152]\n",
      " [-0.56485434]\n",
      " ...\n",
      " [ 0.30280305]\n",
      " [ 0.13034716]\n",
      " [-0.66366752]]\n",
      "loss=40689.83555593919\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.07886122]\n",
      " [-0.04597293]\n",
      " [-0.07357545]\n",
      " ...\n",
      " [ 0.06179031]\n",
      " [ 0.02601238]\n",
      " [-0.10469113]]\n",
      "t [[ 0.07886122]\n",
      " [-0.04597293]\n",
      " [-0.07357545]\n",
      " ...\n",
      " [ 0.06179031]\n",
      " [ 0.02601238]\n",
      " [-0.10469113]]\n",
      "t [[ 0.15023484]\n",
      " [-0.09746974]\n",
      " [-0.14215941]\n",
      " ...\n",
      " [ 0.11267069]\n",
      " [ 0.04787501]\n",
      " [-0.19580054]]\n",
      "t [[ 0.15023484]\n",
      " [-0.09746974]\n",
      " [-0.14215941]\n",
      " ...\n",
      " [ 0.11267069]\n",
      " [ 0.04787501]\n",
      " [-0.19580054]]\n",
      "Current iteration=2, loss=48513.26400038219\n",
      "t [[ 0.21492341]\n",
      " [-0.15268202]\n",
      " [-0.206319  ]\n",
      " ...\n",
      " [ 0.15454017]\n",
      " [ 0.06617162]\n",
      " [-0.27573184]]\n",
      "t [[ 0.21492341]\n",
      " [-0.15268202]\n",
      " [-0.206319  ]\n",
      " ...\n",
      " [ 0.15454017]\n",
      " [ 0.06617162]\n",
      " [-0.27573184]]\n",
      "t [[ 0.27366518]\n",
      " [-0.21019098]\n",
      " [-0.26655095]\n",
      " ...\n",
      " [ 0.18896504]\n",
      " [ 0.08141092]\n",
      " [-0.34646702]]\n",
      "t [[ 0.27366518]\n",
      " [-0.21019098]\n",
      " [-0.26655095]\n",
      " ...\n",
      " [ 0.18896504]\n",
      " [ 0.08141092]\n",
      " [-0.34646702]]\n",
      "Current iteration=4, loss=45871.85983359079\n",
      "t [[ 0.32712347]\n",
      " [-0.26891027]\n",
      " [-0.32328472]\n",
      " ...\n",
      " [ 0.21722287]\n",
      " [ 0.09402945]\n",
      " [-0.40961647]]\n",
      "t [[ 0.32712347]\n",
      " [-0.26891027]\n",
      " [-0.32328472]\n",
      " ...\n",
      " [ 0.21722287]\n",
      " [ 0.09402945]\n",
      " [-0.40961647]]\n",
      "t [[ 0.37588487]\n",
      " [-0.32802483]\n",
      " [-0.37688968]\n",
      " ...\n",
      " [ 0.24035564]\n",
      " [ 0.10439901]\n",
      " [-0.4664805 ]]\n",
      "t [[ 0.37588487]\n",
      " [-0.32802483]\n",
      " [-0.37688968]\n",
      " ...\n",
      " [ 0.24035564]\n",
      " [ 0.10439901]\n",
      " [-0.4664805 ]]\n",
      "Current iteration=6, loss=43777.577791959586\n",
      "t [[ 0.4204636 ]\n",
      " [-0.38693402]\n",
      " [-0.42768316]\n",
      " ...\n",
      " [ 0.25921563]\n",
      " [ 0.11283523]\n",
      " [-0.51810705]]\n",
      "t [[ 0.4204636 ]\n",
      " [-0.38693402]\n",
      " [-0.42768316]\n",
      " ...\n",
      " [ 0.25921563]\n",
      " [ 0.11283523]\n",
      " [-0.51810705]]\n",
      "t [[ 0.46130902]\n",
      " [-0.44520311]\n",
      " [-0.47593828]\n",
      " ...\n",
      " [ 0.27450179]\n",
      " [ 0.11960602]\n",
      " [-0.56534087]]\n",
      "t [[ 0.46130902]\n",
      " [-0.44520311]\n",
      " [-0.47593828]\n",
      " ...\n",
      " [ 0.27450179]\n",
      " [ 0.11960602]\n",
      " [-0.56534087]]\n",
      "Current iteration=8, loss=42079.317731775576\n",
      "t [[ 0.49881384]\n",
      " [-0.50252377]\n",
      " [-0.52189079]\n",
      " ...\n",
      " [ 0.28678839]\n",
      " [ 0.12493908]\n",
      " [-0.60886417]]\n",
      "t [[ 0.49881384]\n",
      " [-0.50252377]\n",
      " [-0.52189079]\n",
      " ...\n",
      " [ 0.28678839]\n",
      " [ 0.12493908]\n",
      " [-0.60886417]]\n",
      "t [[ 0.53332189]\n",
      " [-0.55868314]\n",
      " [-0.565745  ]\n",
      " ...\n",
      " [ 0.29654832]\n",
      " [ 0.12902843]\n",
      " [-0.64922944]]\n",
      "loss=40676.93172204031\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07785046]\n",
      " [-0.04400931]\n",
      " [-0.0732999 ]\n",
      " ...\n",
      " [ 0.06274365]\n",
      " [ 0.02649182]\n",
      " [-0.10791509]]\n",
      "t [[ 0.07785046]\n",
      " [-0.04400931]\n",
      " [-0.0732999 ]\n",
      " ...\n",
      " [ 0.06274365]\n",
      " [ 0.02649182]\n",
      " [-0.10791509]]\n",
      "t [[ 0.14831274]\n",
      " [-0.09356753]\n",
      " [-0.14160844]\n",
      " ...\n",
      " [ 0.1142799 ]\n",
      " [ 0.04878397]\n",
      " [-0.20183841]]\n",
      "t [[ 0.14831274]\n",
      " [-0.09356753]\n",
      " [-0.14160844]\n",
      " ...\n",
      " [ 0.1142799 ]\n",
      " [ 0.04878397]\n",
      " [-0.20183841]]\n",
      "Current iteration=2, loss=48539.49255790251\n",
      "t [[ 0.2121806 ]\n",
      " [-0.14685799]\n",
      " [-0.20549639]\n",
      " ...\n",
      " [ 0.1565642 ]\n",
      " [ 0.0674682 ]\n",
      " [-0.28423967]]\n",
      "t [[ 0.2121806 ]\n",
      " [-0.14685799]\n",
      " [-0.20549639]\n",
      " ...\n",
      " [ 0.1565642 ]\n",
      " [ 0.0674682 ]\n",
      " [-0.28423967]]\n",
      "t [[ 0.27018321]\n",
      " [-0.20245853]\n",
      " [-0.26546314]\n",
      " ...\n",
      " [ 0.19121225]\n",
      " [ 0.08305998]\n",
      " [-0.35715759]]\n",
      "t [[ 0.27018321]\n",
      " [-0.20245853]\n",
      " [-0.26546314]\n",
      " ...\n",
      " [ 0.19121225]\n",
      " [ 0.08305998]\n",
      " [-0.35715759]]\n",
      "Current iteration=4, loss=45923.97341489662\n",
      "t [[ 0.3229757 ]\n",
      " [-0.25928212]\n",
      " [-0.32193986]\n",
      " ...\n",
      " [ 0.21954072]\n",
      " [ 0.09600139]\n",
      " [-0.42224902]]\n",
      "t [[ 0.3229757 ]\n",
      " [-0.25928212]\n",
      " [-0.32193986]\n",
      " ...\n",
      " [ 0.21954072]\n",
      " [ 0.09600139]\n",
      " [-0.42224902]]\n",
      "t [[ 0.37113766]\n",
      " [-0.31651424]\n",
      " [-0.37529678]\n",
      " ...\n",
      " [ 0.24262083]\n",
      " [ 0.10666863]\n",
      " [-0.48085144]]\n",
      "t [[ 0.37113766]\n",
      " [-0.31651424]\n",
      " [-0.37529678]\n",
      " ...\n",
      " [ 0.24262083]\n",
      " [ 0.10666863]\n",
      " [-0.48085144]]\n",
      "Current iteration=6, loss=43853.20761731478\n",
      "t [[ 0.41517722]\n",
      " [-0.37355547]\n",
      " [-0.42585158]\n",
      " ...\n",
      " [ 0.26132684]\n",
      " [ 0.11538091]\n",
      " [-0.53404263]]\n",
      "t [[ 0.41517722]\n",
      " [-0.37355547]\n",
      " [-0.42585158]\n",
      " ...\n",
      " [ 0.26132684]\n",
      " [ 0.11538091]\n",
      " [-0.53404263]]\n",
      "t [[ 0.45553815]\n",
      " [-0.42997267]\n",
      " [-0.47387732]\n",
      " ...\n",
      " [ 0.2763747 ]\n",
      " [ 0.12240897]\n",
      " [-0.58269163]]\n",
      "t [[ 0.45553815]\n",
      " [-0.42997267]\n",
      " [-0.47387732]\n",
      " ...\n",
      " [ 0.2763747 ]\n",
      " [ 0.12240897]\n",
      " [-0.58269163]]\n",
      "Current iteration=8, loss=42175.678919313315\n",
      "t [[ 0.49260797]\n",
      " [-0.48545934]\n",
      " [-0.51960946]\n",
      " ...\n",
      " [ 0.28835245]\n",
      " [ 0.12798283]\n",
      " [-0.6275006 ]]\n",
      "t [[ 0.49260797]\n",
      " [-0.48545934]\n",
      " [-0.51960946]\n",
      " ...\n",
      " [ 0.28835245]\n",
      " [ 0.12798283]\n",
      " [-0.6275006 ]]\n",
      "t [[ 0.52672563]\n",
      " [-0.53980444]\n",
      " [-0.56325192]\n",
      " ...\n",
      " [ 0.29774428]\n",
      " [ 0.13229839]\n",
      " [-0.66903865]]\n",
      "loss=40791.39913596168\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.07772584]\n",
      " [-0.04623617]\n",
      " [-0.07306966]\n",
      " ...\n",
      " [ 0.11194965]\n",
      " [-0.07306966]\n",
      " [ 0.10485671]]\n",
      "t [[ 0.07772584]\n",
      " [-0.04623617]\n",
      " [-0.07306966]\n",
      " ...\n",
      " [ 0.11194965]\n",
      " [-0.07306966]\n",
      " [ 0.10485671]]\n",
      "t [[ 0.14807645]\n",
      " [-0.09771843]\n",
      " [-0.14119137]\n",
      " ...\n",
      " [ 0.21195918]\n",
      " [-0.14119137]\n",
      " [ 0.19659573]]\n",
      "t [[ 0.14807645]\n",
      " [-0.09771843]\n",
      " [-0.14119137]\n",
      " ...\n",
      " [ 0.21195918]\n",
      " [-0.14119137]\n",
      " [ 0.19659573]]\n",
      "Current iteration=2, loss=48544.407265018075\n",
      "t [[ 0.21184941]\n",
      " [-0.15268813]\n",
      " [-0.20492567]\n",
      " ...\n",
      " [ 0.30153444]\n",
      " [-0.20492567]\n",
      " [ 0.27710175]]\n",
      "t [[ 0.21184941]\n",
      " [-0.15268813]\n",
      " [-0.20492567]\n",
      " ...\n",
      " [ 0.30153444]\n",
      " [-0.20492567]\n",
      " [ 0.27710175]]\n",
      "t [[ 0.26976934]\n",
      " [-0.2097754 ]\n",
      " [-0.2647641 ]\n",
      " ...\n",
      " [ 0.38200008]\n",
      " [-0.2647641 ]\n",
      " [ 0.34799752]]\n",
      "t [[ 0.26976934]\n",
      " [-0.2097754 ]\n",
      " [-0.2647641 ]\n",
      " ...\n",
      " [ 0.38200008]\n",
      " [-0.2647641 ]\n",
      " [ 0.34799752]]\n",
      "Current iteration=4, loss=45927.64496242599\n",
      "t [[ 0.32248268]\n",
      " [-0.26793827]\n",
      " [-0.32113192]\n",
      " ...\n",
      " [ 0.45450393]\n",
      " [-0.32113192]\n",
      " [ 0.41065989]]\n",
      "t [[ 0.32248268]\n",
      " [-0.26793827]\n",
      " [-0.32113192]\n",
      " ...\n",
      " [ 0.45450393]\n",
      " [-0.32113192]\n",
      " [ 0.41065989]]\n",
      "t [[ 0.37056051]\n",
      " [-0.3263986 ]\n",
      " [-0.37439494]\n",
      " ...\n",
      " [ 0.52003335]\n",
      " [-0.37439494]\n",
      " [ 0.46624899]]\n",
      "t [[ 0.37056051]\n",
      " [-0.3263986 ]\n",
      " [-0.37439494]\n",
      " ...\n",
      " [ 0.52003335]\n",
      " [-0.37439494]\n",
      " [ 0.46624899]]\n",
      "Current iteration=6, loss=43853.41971498275\n",
      "t [[ 0.41450485]\n",
      " [-0.38458465]\n",
      " [-0.42486747]\n",
      " ...\n",
      " [ 0.57943539]\n",
      " [-0.42486747]\n",
      " [ 0.51574021]]\n",
      "t [[ 0.41450485]\n",
      " [-0.38458465]\n",
      " [-0.42486747]\n",
      " ...\n",
      " [ 0.57943539]\n",
      " [-0.42486747]\n",
      " [ 0.51574021]]\n",
      "t [[ 0.45475611]\n",
      " [-0.44208353]\n",
      " [-0.47281995]\n",
      " ...\n",
      " [ 0.63343672]\n",
      " [-0.47281995]\n",
      " [ 0.5599541 ]]\n",
      "t [[ 0.45475611]\n",
      " [-0.44208353]\n",
      " [-0.47281995]\n",
      " ...\n",
      " [ 0.63343672]\n",
      " [-0.47281995]\n",
      " [ 0.5599541 ]]\n",
      "Current iteration=8, loss=42171.892017298305\n",
      "t [[ 0.49170061]\n",
      " [-0.49860323]\n",
      " [-0.51848582]\n",
      " ...\n",
      " [ 0.68266183]\n",
      " [-0.51848582]\n",
      " [ 0.59958253]]\n",
      "t [[ 0.49170061]\n",
      " [-0.49860323]\n",
      " [-0.51848582]\n",
      " ...\n",
      " [ 0.68266183]\n",
      " [-0.51848582]\n",
      " [ 0.59958253]]\n",
      "t [[ 0.52567752]\n",
      " [-0.55394303]\n",
      " [-0.56206743]\n",
      " ...\n",
      " [ 0.72764875]\n",
      " [-0.56206743]\n",
      " [ 0.6352108 ]]\n",
      "loss=40783.71320351164\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02798027]\n",
      " [-0.13398344]\n",
      " [-0.07856943]\n",
      " ...\n",
      " [ 0.06718729]\n",
      " [ 0.02798027]\n",
      " [-0.11389026]]\n",
      "t [[ 0.02798027]\n",
      " [-0.13398344]\n",
      " [-0.07856943]\n",
      " ...\n",
      " [ 0.06718729]\n",
      " [ 0.02798027]\n",
      " [-0.11389026]]\n",
      "t [[ 0.05121345]\n",
      " [-0.26045464]\n",
      " [-0.15147029]\n",
      " ...\n",
      " [ 0.12176281]\n",
      " [ 0.05121345]\n",
      " [-0.21218398]]\n",
      "t [[ 0.05121345]\n",
      " [-0.26045464]\n",
      " [-0.15147029]\n",
      " ...\n",
      " [ 0.12176281]\n",
      " [ 0.05121345]\n",
      " [-0.21218398]]\n",
      "Current iteration=2, loss=48302.365763529204\n",
      "t [[ 0.07041453]\n",
      " [-0.37974714]\n",
      " [-0.21939108]\n",
      " ...\n",
      " [ 0.16607094]\n",
      " [ 0.07041453]\n",
      " [-0.29782029]]\n",
      "t [[ 0.07041453]\n",
      " [-0.37974714]\n",
      " [-0.21939108]\n",
      " ...\n",
      " [ 0.16607094]\n",
      " [ 0.07041453]\n",
      " [-0.29782029]]\n",
      "t [[ 0.08619854]\n",
      " [-0.49224649]\n",
      " [-0.28292721]\n",
      " ...\n",
      " [ 0.20201387]\n",
      " [ 0.08619854]\n",
      " [-0.37318238]]\n",
      "t [[ 0.08619854]\n",
      " [-0.49224649]\n",
      " [-0.28292721]\n",
      " ...\n",
      " [ 0.20201387]\n",
      " [ 0.08619854]\n",
      " [-0.37318238]]\n",
      "Current iteration=4, loss=45544.084497598255\n",
      "t [[ 0.0990861 ]\n",
      " [-0.59836331]\n",
      " [-0.3425868 ]\n",
      " ...\n",
      " [ 0.23111747]\n",
      " [ 0.0990861 ]\n",
      " [-0.44017299]]\n",
      "t [[ 0.0990861 ]\n",
      " [-0.59836331]\n",
      " [-0.3425868 ]\n",
      " ...\n",
      " [ 0.23111747]\n",
      " [ 0.0990861 ]\n",
      " [-0.44017299]]\n",
      "t [[ 0.10951462]\n",
      " [-0.69851321]\n",
      " [-0.39880149]\n",
      " ...\n",
      " [ 0.2546072 ]\n",
      " [ 0.10951462]\n",
      " [-0.50030255]]\n",
      "t [[ 0.10951462]\n",
      " [-0.69851321]\n",
      " [-0.39880149]\n",
      " ...\n",
      " [ 0.2546072 ]\n",
      " [ 0.10951462]\n",
      " [-0.50030255]]\n",
      "Current iteration=6, loss=43385.29113038871\n",
      "t [[ 0.11785071]\n",
      " [-0.79310358]\n",
      " [-0.45193801]\n",
      " ...\n",
      " [ 0.27347104]\n",
      " [ 0.11785071]\n",
      " [-0.55476882]]\n",
      "t [[ 0.11785071]\n",
      " [-0.79310358]\n",
      " [-0.45193801]\n",
      " ...\n",
      " [ 0.27347104]\n",
      " [ 0.11785071]\n",
      " [-0.55476882]]\n",
      "t [[ 0.12440184]\n",
      " [-0.8825254 ]\n",
      " [-0.50230877]\n",
      " ...\n",
      " [ 0.28850796]\n",
      " [ 0.12440184]\n",
      " [-0.60452285]]\n",
      "t [[ 0.12440184]\n",
      " [-0.8825254 ]\n",
      " [-0.50230877]\n",
      " ...\n",
      " [ 0.28850796]\n",
      " [ 0.12440184]\n",
      " [-0.60452285]]\n",
      "Current iteration=8, loss=41653.20827124179\n",
      "t [[ 0.12942643]\n",
      " [-0.96714861]\n",
      " [-0.55018105]\n",
      " ...\n",
      " [ 0.30036569]\n",
      " [ 0.12942643]\n",
      " [-0.65032195]]\n",
      "t [[ 0.12942643]\n",
      " [-0.96714861]\n",
      " [-0.55018105]\n",
      " ...\n",
      " [ 0.30036569]\n",
      " [ 0.12942643]\n",
      " [-0.65032195]]\n",
      "t [[ 0.13314243]\n",
      " [-1.0473196 ]\n",
      " [-0.5957847 ]\n",
      " ...\n",
      " [ 0.30957086]\n",
      " [ 0.13314243]\n",
      " [-0.69277162]]\n",
      "loss=40235.598996310764\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.08449417]\n",
      " [-0.04925671]\n",
      " [-0.07883084]\n",
      " ...\n",
      " [ 0.0662039 ]\n",
      " [ 0.02787041]\n",
      " [-0.11216906]]\n",
      "t [[ 0.08449417]\n",
      " [-0.04925671]\n",
      " [-0.07883084]\n",
      " ...\n",
      " [ 0.0662039 ]\n",
      " [ 0.02787041]\n",
      " [-0.11216906]]\n",
      "t [[ 0.16039477]\n",
      " [-0.10485297]\n",
      " [-0.15193273]\n",
      " ...\n",
      " [ 0.11988611]\n",
      " [ 0.05097809]\n",
      " [-0.20875024]]\n",
      "t [[ 0.16039477]\n",
      " [-0.10485297]\n",
      " [-0.15193273]\n",
      " ...\n",
      " [ 0.11988611]\n",
      " [ 0.05097809]\n",
      " [-0.20875024]]\n",
      "Current iteration=2, loss=48294.51357229767\n",
      "t [[ 0.22869242]\n",
      " [-0.1645635 ]\n",
      " [-0.22000495]\n",
      " ...\n",
      " [ 0.16338471]\n",
      " [ 0.07004257]\n",
      " [-0.2927039 ]]\n",
      "t [[ 0.22869242]\n",
      " [-0.1645635 ]\n",
      " [-0.22000495]\n",
      " ...\n",
      " [ 0.16338471]\n",
      " [ 0.07004257]\n",
      " [-0.2927039 ]]\n",
      "t [[ 0.29028939]\n",
      " [-0.22668044]\n",
      " [-0.28365214]\n",
      " ...\n",
      " [ 0.19859489]\n",
      " [ 0.08568256]\n",
      " [-0.36642826]]\n",
      "t [[ 0.29028939]\n",
      " [-0.22668044]\n",
      " [-0.28365214]\n",
      " ...\n",
      " [ 0.19859489]\n",
      " [ 0.08568256]\n",
      " [-0.36642826]]\n",
      "Current iteration=4, loss=45532.967369409445\n",
      "t [[ 0.34598665]\n",
      " [-0.28992858]\n",
      " [-0.34339005]\n",
      " ...\n",
      " [ 0.22703625]\n",
      " [ 0.09842148]\n",
      " [-0.43183616]]\n",
      "t [[ 0.34598665]\n",
      " [-0.28992858]\n",
      " [-0.34339005]\n",
      " ...\n",
      " [ 0.22703625]\n",
      " [ 0.09842148]\n",
      " [-0.43183616]]\n",
      "t [[ 0.39648362]\n",
      " [-0.35337725]\n",
      " [-0.39965658]\n",
      " ...\n",
      " [ 0.24992854]\n",
      " [ 0.1086989 ]\n",
      " [-0.49044429]]\n",
      "t [[ 0.39648362]\n",
      " [-0.35337725]\n",
      " [-0.39965658]\n",
      " ...\n",
      " [ 0.24992854]\n",
      " [ 0.1086989 ]\n",
      " [-0.49044429]]\n",
      "Current iteration=6, loss=43372.8798394657\n",
      "t [[ 0.44238616]\n",
      " [-0.41636113]\n",
      " [-0.45282352]\n",
      " ...\n",
      " [ 0.26825435]\n",
      " [ 0.11688301]\n",
      " [-0.54345375]]\n",
      "t [[ 0.44238616]\n",
      " [-0.41636113]\n",
      " [-0.45282352]\n",
      " ...\n",
      " [ 0.26825435]\n",
      " [ 0.11688301]\n",
      " [-0.54345375]]\n",
      "t [[ 0.48421809]\n",
      " [-0.47841468]\n",
      " [-0.50320739]\n",
      " ...\n",
      " [ 0.28280742]\n",
      " [ 0.12328251]\n",
      " [-0.59181673]]\n",
      "t [[ 0.48421809]\n",
      " [-0.47841468]\n",
      " [-0.50320739]\n",
      " ...\n",
      " [ 0.28280742]\n",
      " [ 0.12328251]\n",
      " [-0.59181673]]\n",
      "Current iteration=8, loss=41640.36251858216\n",
      "t [[ 0.52243284]\n",
      " [-0.53922073]\n",
      " [-0.55107878]\n",
      " ...\n",
      " [ 0.29423051]\n",
      " [ 0.12815673]\n",
      " [-0.63629016]]\n",
      "t [[ 0.52243284]\n",
      " [-0.53922073]\n",
      " [-0.55107878]\n",
      " ...\n",
      " [ 0.29423051]\n",
      " [ 0.12815673]\n",
      " [-0.63629016]]\n",
      "t [[ 0.55742394]\n",
      " [-0.59857123]\n",
      " [-0.59667021]\n",
      " ...\n",
      " [ 0.30304557]\n",
      " [ 0.13172431]\n",
      " [-0.67747812]]\n",
      "loss=40222.69214546318\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08341121]\n",
      " [-0.04715283]\n",
      " [-0.07853561]\n",
      " ...\n",
      " [ 0.06722534]\n",
      " [ 0.02838409]\n",
      " [-0.11562331]]\n",
      "t [[ 0.08341121]\n",
      " [-0.04715283]\n",
      " [-0.07853561]\n",
      " ...\n",
      " [ 0.06722534]\n",
      " [ 0.02838409]\n",
      " [-0.11562331]]\n",
      "t [[ 0.15834294]\n",
      " [-0.10067395]\n",
      " [-0.15134243]\n",
      " ...\n",
      " [ 0.12158776]\n",
      " [ 0.05194819]\n",
      " [-0.21518824]]\n",
      "t [[ 0.15834294]\n",
      " [-0.10067395]\n",
      " [-0.15134243]\n",
      " ...\n",
      " [ 0.12158776]\n",
      " [ 0.05194819]\n",
      " [-0.21518824]]\n",
      "Current iteration=2, loss=48322.646043114335\n",
      "t [[ 0.2257749 ]\n",
      " [-0.15832796]\n",
      " [-0.21912426]\n",
      " ...\n",
      " [ 0.16549512]\n",
      " [ 0.0714217 ]\n",
      " [-0.30173676]]\n",
      "t [[ 0.2257749 ]\n",
      " [-0.15832796]\n",
      " [-0.21912426]\n",
      " ...\n",
      " [ 0.16549512]\n",
      " [ 0.0714217 ]\n",
      " [-0.30173676]]\n",
      "t [[ 0.28659823]\n",
      " [-0.21840327]\n",
      " [-0.28248889]\n",
      " ...\n",
      " [ 0.20090233]\n",
      " [ 0.08743151]\n",
      " [-0.37773584]]\n",
      "t [[ 0.28659823]\n",
      " [-0.21840327]\n",
      " [-0.28248889]\n",
      " ...\n",
      " [ 0.20090233]\n",
      " [ 0.08743151]\n",
      " [-0.37773584]]\n",
      "Current iteration=4, loss=45588.652919358916\n",
      "t [[ 0.34160409]\n",
      " [-0.27962417]\n",
      " [-0.34195395]\n",
      " ...\n",
      " [ 0.22937472]\n",
      " [ 0.10050761]\n",
      " [-0.44515332]]\n",
      "t [[ 0.34160409]\n",
      " [-0.27962417]\n",
      " [-0.34195395]\n",
      " ...\n",
      " [ 0.22937472]\n",
      " [ 0.10050761]\n",
      " [-0.44515332]]\n",
      "t [[ 0.39148362]\n",
      " [-0.34106095]\n",
      " [-0.39795822]\n",
      " ...\n",
      " [ 0.25216539]\n",
      " [ 0.1110947 ]\n",
      " [-0.50554912]]\n",
      "t [[ 0.39148362]\n",
      " [-0.34106095]\n",
      " [-0.39795822]\n",
      " ...\n",
      " [ 0.25216539]\n",
      " [ 0.1110947 ]\n",
      " [-0.50554912]]\n",
      "Current iteration=6, loss=43453.26824321544\n",
      "t [[ 0.43683542]\n",
      " [-0.40204999]\n",
      " [-0.45087369]\n",
      " ...\n",
      " [ 0.27028158]\n",
      " [ 0.11956504]\n",
      " [-0.56015852]]\n",
      "t [[ 0.43683542]\n",
      " [-0.40204999]\n",
      " [-0.45087369]\n",
      " ...\n",
      " [ 0.27028158]\n",
      " [ 0.11956504]\n",
      " [-0.56015852]]\n",
      "t [[ 0.47817662]\n",
      " [-0.46212792]\n",
      " [-0.50101668]\n",
      " ...\n",
      " [ 0.28453615]\n",
      " [ 0.12623051]\n",
      " [-0.60996122]]\n",
      "t [[ 0.47817662]\n",
      " [-0.46212792]\n",
      " [-0.50101668]\n",
      " ...\n",
      " [ 0.28453615]\n",
      " [ 0.12623051]\n",
      " [-0.60996122]]\n",
      "Current iteration=8, loss=41742.25664447836\n",
      "t [[ 0.51595444]\n",
      " [-0.5209799 ]\n",
      " [-0.54865735]\n",
      " ...\n",
      " [ 0.29558718]\n",
      " [ 0.13135304]\n",
      " [-0.65573659]]\n",
      "t [[ 0.51595444]\n",
      " [-0.5209799 ]\n",
      " [-0.54865735]\n",
      " ...\n",
      " [ 0.29558718]\n",
      " [ 0.13135304]\n",
      " [-0.65573659]]\n",
      "t [[ 0.55055661]\n",
      " [-0.57840012]\n",
      " [-0.59402764]\n",
      " ...\n",
      " [ 0.30396922]\n",
      " [ 0.13515331]\n",
      " [-0.6981072 ]]\n",
      "loss=40343.16848589705\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.08327769]\n",
      " [-0.04953875]\n",
      " [-0.07828892]\n",
      " ...\n",
      " [ 0.11994606]\n",
      " [-0.07828892]\n",
      " [ 0.11234648]]\n",
      "t [[ 0.08327769]\n",
      " [-0.04953875]\n",
      " [-0.07828892]\n",
      " ...\n",
      " [ 0.11994606]\n",
      " [-0.07828892]\n",
      " [ 0.11234648]]\n",
      "t [[ 0.15809063]\n",
      " [-0.10509838]\n",
      " [-0.15089886]\n",
      " ...\n",
      " [ 0.22618815]\n",
      " [-0.15089886]\n",
      " [ 0.20963759]]\n",
      "t [[ 0.15809063]\n",
      " [-0.10509838]\n",
      " [-0.15089886]\n",
      " ...\n",
      " [ 0.22618815]\n",
      " [-0.15089886]\n",
      " [ 0.20963759]]\n",
      "Current iteration=2, loss=48327.66863911961\n",
      "t [[ 0.22542279]\n",
      " [-0.16451533]\n",
      " [-0.21852103]\n",
      " ...\n",
      " [ 0.32058289]\n",
      " [-0.21852103]\n",
      " [ 0.29419612]]\n",
      "t [[ 0.22542279]\n",
      " [-0.16451533]\n",
      " [-0.21852103]\n",
      " ...\n",
      " [ 0.32058289]\n",
      " [-0.21852103]\n",
      " [ 0.29419612]]\n",
      "t [[ 0.28615885]\n",
      " [-0.22614187]\n",
      " [-0.28175377]\n",
      " ...\n",
      " [ 0.40474294]\n",
      " [-0.28175377]\n",
      " [ 0.36799372]]\n",
      "t [[ 0.28615885]\n",
      " [-0.22614187]\n",
      " [-0.28175377]\n",
      " ...\n",
      " [ 0.40474294]\n",
      " [-0.28175377]\n",
      " [ 0.36799372]]\n",
      "Current iteration=4, loss=45591.95451326032\n",
      "t [[ 0.34107899]\n",
      " [-0.28875602]\n",
      " [-0.34110784]\n",
      " ...\n",
      " [ 0.48004695]\n",
      " [-0.34110784]\n",
      " [ 0.43267807]]\n",
      "t [[ 0.34107899]\n",
      " [-0.28875602]\n",
      " [-0.34110784]\n",
      " ...\n",
      " [ 0.48004695]\n",
      " [-0.34110784]\n",
      " [ 0.43267807]]\n",
      "t [[ 0.39086451]\n",
      " [-0.35146999]\n",
      " [-0.39701693]\n",
      " ...\n",
      " [ 0.54766528]\n",
      " [-0.39701693]\n",
      " [ 0.48961721]]\n",
      "t [[ 0.39086451]\n",
      " [-0.35146999]\n",
      " [-0.39701693]\n",
      " ...\n",
      " [ 0.54766528]\n",
      " [-0.39701693]\n",
      " [ 0.48961721]]\n",
      "Current iteration=6, loss=43452.65861785799\n",
      "t [[ 0.43610771]\n",
      " [-0.41365094]\n",
      " [-0.44984924]\n",
      " ...\n",
      " [ 0.60858957]\n",
      " [-0.44984924]\n",
      " [ 0.53994539]]\n",
      "t [[ 0.43610771]\n",
      " [-0.41365094]\n",
      " [-0.44984924]\n",
      " ...\n",
      " [ 0.60858957]\n",
      " [-0.44984924]\n",
      " [ 0.53994539]]\n",
      "t [[ 0.47732283]\n",
      " [-0.47485721]\n",
      " [-0.4999182 ]\n",
      " ...\n",
      " [ 0.66366081]\n",
      " [-0.4999182 ]\n",
      " [ 0.58460464]]\n",
      "t [[ 0.47732283]\n",
      " [-0.47485721]\n",
      " [-0.4999182 ]\n",
      " ...\n",
      " [ 0.66366081]\n",
      " [-0.4999182 ]\n",
      " [ 0.58460464]]\n",
      "Current iteration=8, loss=41737.33602711762\n",
      "t [[ 0.51495656]\n",
      " [-0.53478901]\n",
      " [-0.54749177]\n",
      " ...\n",
      " [ 0.71359411]\n",
      " [-0.54749177]\n",
      " [ 0.62437988]]\n",
      "t [[ 0.51495656]\n",
      " [-0.53478901]\n",
      " [-0.54749177]\n",
      " ...\n",
      " [ 0.71359411]\n",
      " [-0.54749177]\n",
      " [ 0.62437988]]\n",
      "t [[ 0.54939756]\n",
      " [-0.59325091]\n",
      " [-0.59280026]\n",
      " ...\n",
      " [ 0.75899962]\n",
      " [-0.59280026]\n",
      " [ 0.65992799]]\n",
      "loss=40334.15281957277\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02984562]\n",
      " [-0.14291567]\n",
      " [-0.08380739]\n",
      " ...\n",
      " [ 0.07166644]\n",
      " [ 0.02984562]\n",
      " [-0.12148294]]\n",
      "t [[ 0.02984562]\n",
      " [-0.14291567]\n",
      " [-0.08380739]\n",
      " ...\n",
      " [ 0.07166644]\n",
      " [ 0.02984562]\n",
      " [-0.12148294]]\n",
      "t [[ 0.05429134]\n",
      " [-0.27728532]\n",
      " [-0.16116649]\n",
      " ...\n",
      " [ 0.12898652]\n",
      " [ 0.05429134]\n",
      " [-0.22522456]]\n",
      "t [[ 0.05429134]\n",
      " [-0.27728532]\n",
      " [-0.16116649]\n",
      " ...\n",
      " [ 0.12898652]\n",
      " [ 0.05429134]\n",
      " [-0.22522456]]\n",
      "Current iteration=2, loss=48087.16763971762\n",
      "t [[ 0.07420664]\n",
      " [-0.40351664]\n",
      " [-0.23291487]\n",
      " ...\n",
      " [ 0.17480804]\n",
      " [ 0.07420664]\n",
      " [-0.31479631]]\n",
      "t [[ 0.07420664]\n",
      " [-0.40351664]\n",
      " [-0.23291487]\n",
      " ...\n",
      " [ 0.17480804]\n",
      " [ 0.07420664]\n",
      " [-0.31479631]]\n",
      "t [[ 0.09032927]\n",
      " [-0.52208282]\n",
      " [-0.29976728]\n",
      " ...\n",
      " [ 0.21140102]\n",
      " [ 0.09032927]\n",
      " [-0.39304215]]\n",
      "t [[ 0.09032927]\n",
      " [-0.52208282]\n",
      " [-0.29976728]\n",
      " ...\n",
      " [ 0.21140102]\n",
      " [ 0.09032927]\n",
      " [-0.39304215]]\n",
      "Current iteration=4, loss=45215.50663843034\n",
      "t [[ 0.10327475]\n",
      " [-0.63348581]\n",
      " [-0.36232547]\n",
      " ...\n",
      " [ 0.24055528]\n",
      " [ 0.10327475]\n",
      " [-0.46219052]]\n",
      "t [[ 0.10327475]\n",
      " [-0.63348581]\n",
      " [-0.36232547]\n",
      " ...\n",
      " [ 0.24055528]\n",
      " [ 0.10327475]\n",
      " [-0.46219052]]\n",
      "t [[ 0.11355299]\n",
      " [-0.73822998]\n",
      " [-0.42109411]\n",
      " ...\n",
      " [ 0.26368556]\n",
      " [ 0.11355299]\n",
      " [-0.52397847]]\n",
      "t [[ 0.11355299]\n",
      " [-0.73822998]\n",
      " [-0.42109411]\n",
      " ...\n",
      " [ 0.26368556]\n",
      " [ 0.11355299]\n",
      " [-0.52397847]]\n",
      "Current iteration=6, loss=42997.439325322644\n",
      "t [[ 0.12158576]\n",
      " [-0.83680561]\n",
      " [-0.47649698]\n",
      " ...\n",
      " [ 0.28191515]\n",
      " [ 0.12158576]\n",
      " [-0.57975973]]\n",
      "t [[ 0.12158576]\n",
      " [-0.83680561]\n",
      " [-0.47649698]\n",
      " ...\n",
      " [ 0.28191515]\n",
      " [ 0.12158576]\n",
      " [-0.57975973]]\n",
      "t [[ 0.1277225 ]\n",
      " [-0.92967915]\n",
      " [-0.52889137]\n",
      " ...\n",
      " [ 0.29613896]\n",
      " [ 0.1277225 ]\n",
      " [-0.63059167]]\n",
      "t [[ 0.1277225 ]\n",
      " [-0.92967915]\n",
      " [-0.52889137]\n",
      " ...\n",
      " [ 0.29613896]\n",
      " [ 0.1277225 ]\n",
      " [-0.63059167]]\n",
      "Current iteration=8, loss=41236.54841750037\n",
      "t [[ 0.13225365]\n",
      " [-1.01728802]\n",
      " [-0.57858017]\n",
      " ...\n",
      " [ 0.30707215]\n",
      " [ 0.13225365]\n",
      " [-0.67730347]]\n",
      "t [[ 0.13225365]\n",
      " [-1.01728802]\n",
      " [-0.57858017]\n",
      " ...\n",
      " [ 0.30707215]\n",
      " [ 0.13225365]\n",
      " [-0.67730347]]\n",
      "t [[ 0.1354216 ]\n",
      " [-1.10003827]\n",
      " [-0.62582176]\n",
      " ...\n",
      " [ 0.31528841]\n",
      " [ 0.1354216 ]\n",
      " [-0.72054889]]\n",
      "loss=39807.920424239885\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.09012711]\n",
      " [-0.05254049]\n",
      " [-0.08408623]\n",
      " ...\n",
      " [ 0.0706175 ]\n",
      " [ 0.02972844]\n",
      " [-0.119647  ]]\n",
      "t [[ 0.09012711]\n",
      " [-0.05254049]\n",
      " [-0.08408623]\n",
      " ...\n",
      " [ 0.0706175 ]\n",
      " [ 0.02972844]\n",
      " [-0.119647  ]]\n",
      "t [[ 0.17047897]\n",
      " [-0.11229199]\n",
      " [-0.16165549]\n",
      " ...\n",
      " [ 0.12699107]\n",
      " [ 0.05403919]\n",
      " [-0.22156254]]\n",
      "t [[ 0.17047897]\n",
      " [-0.11229199]\n",
      " [-0.16165549]\n",
      " ...\n",
      " [ 0.12699107]\n",
      " [ 0.05403919]\n",
      " [-0.22156254]]\n",
      "Current iteration=2, loss=48078.956698518676\n",
      "t [[ 0.24226182]\n",
      " [-0.17655291]\n",
      " [-0.23355847]\n",
      " ...\n",
      " [ 0.17196081]\n",
      " [ 0.07380725]\n",
      " [-0.30934383]]\n",
      "t [[ 0.24226182]\n",
      " [-0.17655291]\n",
      " [-0.23355847]\n",
      " ...\n",
      " [ 0.17196081]\n",
      " [ 0.07380725]\n",
      " [-0.30934383]]\n",
      "t [[ 0.30656314]\n",
      " [-0.24329623]\n",
      " [-0.30052097]\n",
      " ...\n",
      " [ 0.20778832]\n",
      " [ 0.08977474]\n",
      " [-0.38585253]]\n",
      "t [[ 0.30656314]\n",
      " [-0.24329623]\n",
      " [-0.30052097]\n",
      " ...\n",
      " [ 0.20778832]\n",
      " [ 0.08977474]\n",
      " [-0.38585253]]\n",
      "Current iteration=4, loss=45204.10459456141\n",
      "t [[ 0.36433634]\n",
      " [-0.31104722]\n",
      " [-0.36315373]\n",
      " ...\n",
      " [ 0.23625592]\n",
      " [ 0.10256045]\n",
      " [-0.45332849]]\n",
      "t [[ 0.36433634]\n",
      " [-0.31104722]\n",
      " [-0.36315373]\n",
      " ...\n",
      " [ 0.23625592]\n",
      " [ 0.10256045]\n",
      " [-0.45332849]]\n",
      "t [[ 0.41640346]\n",
      " [-0.37875998]\n",
      " [-0.42196865]\n",
      " ...\n",
      " [ 0.25877154]\n",
      " [ 0.11267671]\n",
      " [-0.51351545]]\n",
      "t [[ 0.41640346]\n",
      " [-0.37875998]\n",
      " [-0.42196865]\n",
      " ...\n",
      " [ 0.25877154]\n",
      " [ 0.11267671]\n",
      " [-0.51351545]]\n",
      "Current iteration=6, loss=42984.8737833061\n",
      "t [[ 0.46346837]\n",
      " [-0.44571017]\n",
      " [-0.47739527]\n",
      " ...\n",
      " [ 0.27645196]\n",
      " [ 0.12054709]\n",
      " [-0.56777023]]\n",
      "t [[ 0.46346837]\n",
      " [-0.44571017]\n",
      " [-0.47739527]\n",
      " ...\n",
      " [ 0.27645196]\n",
      " [ 0.12054709]\n",
      " [-0.56777023]]\n",
      "t [[ 0.50613336]\n",
      " [-0.51140914]\n",
      " [-0.52979549]\n",
      " ...\n",
      " [ 0.29018584]\n",
      " [ 0.12652234]\n",
      " [-0.61715079]]\n",
      "t [[ 0.50613336]\n",
      " [-0.51140914]\n",
      " [-0.52979549]\n",
      " ...\n",
      " [ 0.29018584]\n",
      " [ 0.12652234]\n",
      " [-0.61715079]]\n",
      "Current iteration=8, loss=41223.65234173763\n",
      "t [[ 0.54491498]\n",
      " [-0.57553864]\n",
      " [-0.57947585]\n",
      " ...\n",
      " [ 0.30068244]\n",
      " [ 0.13089388]\n",
      " [-0.66248519]]\n",
      "t [[ 0.54491498]\n",
      " [-0.57553864]\n",
      " [-0.57947585]\n",
      " ...\n",
      " [ 0.30068244]\n",
      " [ 0.13089388]\n",
      " [-0.66248519]]\n",
      "t [[ 0.58025777]\n",
      " [-0.63790248]\n",
      " [-0.62669764]\n",
      " ...\n",
      " [ 0.30851001]\n",
      " [ 0.13390482]\n",
      " [-0.70442495]]\n",
      "loss=39795.030174887695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08897195]\n",
      " [-0.05029635]\n",
      " [-0.08377131]\n",
      " ...\n",
      " [ 0.07170703]\n",
      " [ 0.03027636]\n",
      " [-0.12333153]]\n",
      "t [[ 0.08897195]\n",
      " [-0.05029635]\n",
      " [-0.08377131]\n",
      " ...\n",
      " [ 0.07170703]\n",
      " [ 0.03027636]\n",
      " [-0.12333153]]\n",
      "t [[ 0.16829842]\n",
      " [-0.10783641]\n",
      " [-0.16102588]\n",
      " ...\n",
      " [ 0.12878222]\n",
      " [ 0.05506992]\n",
      " [-0.22839657]]\n",
      "t [[ 0.16829842]\n",
      " [-0.10783641]\n",
      " [-0.16102588]\n",
      " ...\n",
      " [ 0.12878222]\n",
      " [ 0.05506992]\n",
      " [-0.22839657]]\n",
      "Current iteration=2, loss=48108.99333954983\n",
      "t [[ 0.23917228]\n",
      " [-0.16990638]\n",
      " [-0.23261985]\n",
      " ...\n",
      " [ 0.17415073]\n",
      " [ 0.07526769]\n",
      " [-0.31889169]]\n",
      "t [[ 0.23917228]\n",
      " [-0.16990638]\n",
      " [-0.23261985]\n",
      " ...\n",
      " [ 0.17415073]\n",
      " [ 0.07526769]\n",
      " [-0.31889169]]\n",
      "t [[ 0.30266758]\n",
      " [-0.23447517]\n",
      " [-0.29928272]\n",
      " ...\n",
      " [ 0.21014524]\n",
      " [ 0.09162156]\n",
      " [-0.39776063]]\n",
      "t [[ 0.30266758]\n",
      " [-0.23447517]\n",
      " [-0.29928272]\n",
      " ...\n",
      " [ 0.21014524]\n",
      " [ 0.09162156]\n",
      " [-0.39776063]]\n",
      "Current iteration=4, loss=45263.32002200395\n",
      "t [[ 0.35972616]\n",
      " [-0.30006782]\n",
      " [-0.36162727]\n",
      " ...\n",
      " [ 0.23860076]\n",
      " [ 0.10475796]\n",
      " [-0.46730744]]\n",
      "t [[ 0.35972616]\n",
      " [-0.30006782]\n",
      " [-0.36162727]\n",
      " ...\n",
      " [ 0.23860076]\n",
      " [ 0.10475796]\n",
      " [-0.46730744]]\n",
      "t [[ 0.41116037]\n",
      " [-0.36563991]\n",
      " [-0.42016621]\n",
      " ...\n",
      " [ 0.26096267]\n",
      " [ 0.11519512]\n",
      " [-0.52932538]]\n",
      "t [[ 0.41116037]\n",
      " [-0.36563991]\n",
      " [-0.42016621]\n",
      " ...\n",
      " [ 0.26096267]\n",
      " [ 0.11519512]\n",
      " [-0.52932538]]\n",
      "Current iteration=6, loss=43069.902958909704\n",
      "t [[ 0.45766548]\n",
      " [-0.43046945]\n",
      " [-0.47532916]\n",
      " ...\n",
      " [ 0.27837525]\n",
      " [ 0.12336117]\n",
      " [-0.58521   ]]\n",
      "t [[ 0.45766548]\n",
      " [-0.43046945]\n",
      " [-0.47532916]\n",
      " ...\n",
      " [ 0.27837525]\n",
      " [ 0.12336117]\n",
      " [-0.58521   ]]\n",
      "t [[ 0.49983588]\n",
      " [-0.49407059]\n",
      " [-0.5274776 ]\n",
      " ...\n",
      " [ 0.29174837]\n",
      " [ 0.12961044]\n",
      " [-0.63605005]]\n",
      "t [[ 0.49983588]\n",
      " [-0.49407059]\n",
      " [-0.5274776 ]\n",
      " ...\n",
      " [ 0.29174837]\n",
      " [ 0.12961044]\n",
      " [-0.63605005]]\n",
      "Current iteration=8, loss=41330.88043575989\n",
      "t [[ 0.53818079]\n",
      " [-0.55612794]\n",
      " [-0.57691746]\n",
      " ...\n",
      " [ 0.30180826]\n",
      " [ 0.13423714]\n",
      " [-0.68269846]]\n",
      "t [[ 0.53818079]\n",
      " [-0.55612794]\n",
      " [-0.57691746]\n",
      " ...\n",
      " [ 0.30180826]\n",
      " [ 0.13423714]\n",
      " [-0.68269846]]\n",
      "t [[ 0.57313804]\n",
      " [-0.61644797]\n",
      " [-0.62390926]\n",
      " ...\n",
      " [ 0.30913699]\n",
      " [ 0.13748665]\n",
      " [-0.72582706]]\n",
      "loss=39921.24390840758\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.08882953]\n",
      " [-0.05284134]\n",
      " [-0.08350818]\n",
      " ...\n",
      " [ 0.12794246]\n",
      " [-0.08350818]\n",
      " [ 0.11983624]]\n",
      "t [[ 0.08882953]\n",
      " [-0.05284134]\n",
      " [-0.08350818]\n",
      " ...\n",
      " [ 0.12794246]\n",
      " [-0.08350818]\n",
      " [ 0.11983624]]\n",
      "t [[ 0.16803018]\n",
      " [-0.11253135]\n",
      " [-0.16055625]\n",
      " ...\n",
      " [ 0.24029628]\n",
      " [-0.16055625]\n",
      " [ 0.22254672]]\n",
      "t [[ 0.16803018]\n",
      " [-0.11253135]\n",
      " [-0.16055625]\n",
      " ...\n",
      " [ 0.24029628]\n",
      " [-0.16055625]\n",
      " [ 0.22254672]]\n",
      "Current iteration=2, loss=48114.09740813433\n",
      "t [[ 0.23879946]\n",
      " [-0.17644401]\n",
      " [-0.23198506]\n",
      " ...\n",
      " [ 0.33931978]\n",
      " [-0.23198506]\n",
      " [ 0.31095576]]\n",
      "t [[ 0.23879946]\n",
      " [-0.17644401]\n",
      " [-0.23198506]\n",
      " ...\n",
      " [ 0.33931978]\n",
      " [-0.23198506]\n",
      " [ 0.31095576]]\n",
      "t [[ 0.30220261]\n",
      " [-0.24262508]\n",
      " [-0.29851296]\n",
      " ...\n",
      " [ 0.42694916]\n",
      " [-0.29851296]\n",
      " [ 0.38742576]]\n",
      "t [[ 0.30220261]\n",
      " [-0.24262508]\n",
      " [-0.29851296]\n",
      " ...\n",
      " [ 0.42694916]\n",
      " [-0.29851296]\n",
      " [ 0.38742576]]\n",
      "Current iteration=4, loss=45266.21080490663\n",
      "t [[ 0.35916804]\n",
      " [-0.30966256]\n",
      " [-0.36074478]\n",
      " ...\n",
      " [ 0.5048171 ]\n",
      " [-0.36074478]\n",
      " [ 0.45390086]]\n",
      "t [[ 0.35916804]\n",
      " [-0.30966256]\n",
      " [-0.36074478]\n",
      " ...\n",
      " [ 0.5048171 ]\n",
      " [-0.36074478]\n",
      " [ 0.45390086]]\n",
      "t [[ 0.41049711]\n",
      " [-0.37655952]\n",
      " [-0.41918752]\n",
      " ...\n",
      " [ 0.57429143]\n",
      " [-0.41918752]\n",
      " [ 0.51197145]]\n",
      "t [[ 0.41049711]\n",
      " [-0.37655952]\n",
      " [-0.41918752]\n",
      " ...\n",
      " [ 0.57429143]\n",
      " [-0.41918752]\n",
      " [ 0.51197145]]\n",
      "Current iteration=6, loss=43068.45087139119\n",
      "t [[ 0.45687888]\n",
      " [-0.44262749]\n",
      " [-0.47426654]\n",
      " ...\n",
      " [ 0.63651692]\n",
      " [-0.47426654]\n",
      " [ 0.56293815]]\n",
      "t [[ 0.45687888]\n",
      " [-0.44262749]\n",
      " [-0.47426654]\n",
      " ...\n",
      " [ 0.63651692]\n",
      " [-0.47426654]\n",
      " [ 0.56293815]]\n",
      "t [[ 0.49890549]\n",
      " [-0.5074035 ]\n",
      " [-0.52634021]\n",
      " ...\n",
      " [ 0.69245356]\n",
      " [-0.52634021]\n",
      " [ 0.60786757]]\n",
      "t [[ 0.49890549]\n",
      " [-0.5074035 ]\n",
      " [-0.52634021]\n",
      " ...\n",
      " [ 0.69245356]\n",
      " [-0.52634021]\n",
      " [ 0.60786757]]\n",
      "Current iteration=8, loss=41324.83602409204\n",
      "t [[ 0.53708651]\n",
      " [-0.57058753]\n",
      " [-0.57571209]\n",
      " ...\n",
      " [ 0.74290929]\n",
      " [-0.57571209]\n",
      " [ 0.64763833]]\n",
      "t [[ 0.53708651]\n",
      " [-0.57058753]\n",
      " [-0.57571209]\n",
      " ...\n",
      " [ 0.74290929]\n",
      " [-0.57571209]\n",
      " [ 0.64763833]]\n",
      "t [[ 0.5718614 ]\n",
      " [-0.63199632]\n",
      " [-0.62264104]\n",
      " ...\n",
      " [ 0.78856708]\n",
      " [-0.62264104]\n",
      " [ 0.68297805]]\n",
      "loss=39910.93600472868\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03171097]\n",
      " [-0.1518479 ]\n",
      " [-0.08904535]\n",
      " ...\n",
      " [ 0.0761456 ]\n",
      " [ 0.03171097]\n",
      " [-0.12907563]]\n",
      "t [[ 0.03171097]\n",
      " [-0.1518479 ]\n",
      " [-0.08904535]\n",
      " ...\n",
      " [ 0.0761456 ]\n",
      " [ 0.03171097]\n",
      " [-0.12907563]]\n",
      "t [[ 0.05732744]\n",
      " [-0.29404963]\n",
      " [-0.17081273]\n",
      " ...\n",
      " [ 0.13609912]\n",
      " [ 0.05732744]\n",
      " [-0.23812785]]\n",
      "t [[ 0.05732744]\n",
      " [-0.29404963]\n",
      " [-0.17081273]\n",
      " ...\n",
      " [ 0.13609912]\n",
      " [ 0.05732744]\n",
      " [-0.23812785]]\n",
      "Current iteration=2, loss=47875.09522826213\n",
      "t [[ 0.07789426]\n",
      " [-0.42709761]\n",
      " [-0.24630899]\n",
      " ...\n",
      " [ 0.18327912]\n",
      " [ 0.07789426]\n",
      " [-0.33144526]]\n",
      "t [[ 0.07789426]\n",
      " [-0.42709761]\n",
      " [-0.24630899]\n",
      " ...\n",
      " [ 0.18327912]\n",
      " [ 0.07789426]\n",
      " [-0.33144526]]\n",
      "t [[ 0.09428562]\n",
      " [-0.55156538]\n",
      " [-0.31638173]\n",
      " ...\n",
      " [ 0.22036163]\n",
      " [ 0.09428562]\n",
      " [-0.41238011]]\n",
      "t [[ 0.09428562]\n",
      " [-0.55156538]\n",
      " [-0.31638173]\n",
      " ...\n",
      " [ 0.22036163]\n",
      " [ 0.09428562]\n",
      " [-0.41238011]]\n",
      "Current iteration=4, loss=44896.53152568758\n",
      "t [[ 0.10722019]\n",
      " [-0.66805859]\n",
      " [-0.38173476]\n",
      " ...\n",
      " [ 0.24942032]\n",
      " [ 0.10722019]\n",
      " [-0.48350954]]\n",
      "t [[ 0.10722019]\n",
      " [-0.66805859]\n",
      " [-0.38173476]\n",
      " ...\n",
      " [ 0.24942032]\n",
      " [ 0.10722019]\n",
      " [-0.48350954]]\n",
      "t [[ 0.11728504]\n",
      " [-0.77718124]\n",
      " [-0.44295087]\n",
      " ...\n",
      " [ 0.27206825]\n",
      " [ 0.11728504]\n",
      " [-0.54680589]]\n",
      "t [[ 0.11728504]\n",
      " [-0.77718124]\n",
      " [-0.44295087]\n",
      " ...\n",
      " [ 0.27206825]\n",
      " [ 0.11728504]\n",
      " [-0.54680589]]\n",
      "Current iteration=6, loss=42625.30807011229\n",
      "t [[ 0.12495943]\n",
      " [-0.87951538]\n",
      " [-0.50051388]\n",
      " ...\n",
      " [ 0.2895666 ]\n",
      " [ 0.12495943]\n",
      " [-0.60377975]]\n",
      "t [[ 0.12495943]\n",
      " [-0.87951538]\n",
      " [-0.50051388]\n",
      " ...\n",
      " [ 0.2895666 ]\n",
      " [ 0.12495943]\n",
      " [-0.60377975]]\n",
      "t [[ 0.13063573]\n",
      " [-0.97560996]\n",
      " [-0.55482759]\n",
      " ...\n",
      " [ 0.3029052 ]\n",
      " [ 0.13063573]\n",
      " [-0.65559193]]\n",
      "t [[ 0.13063573]\n",
      " [-0.97560996]\n",
      " [-0.55482759]\n",
      " ...\n",
      " [ 0.3029052 ]\n",
      " [ 0.13063573]\n",
      " [-0.65559193]]\n",
      "Current iteration=8, loss=40840.56410193572\n",
      "t [[ 0.1346365 ]\n",
      " [-1.06597518]\n",
      " [-0.60623136]\n",
      " ...\n",
      " [ 0.31286396]\n",
      " [ 0.1346365 ]\n",
      " [-0.7031392 ]]\n",
      "t [[ 0.1346365 ]\n",
      " [-1.06597518]\n",
      " [-0.60623136]\n",
      " ...\n",
      " [ 0.31286396]\n",
      " [ 0.1346365 ]\n",
      " [-0.7031392 ]]\n",
      "t [[ 0.13722828]\n",
      " [-1.15108058]\n",
      " [-0.65501249]\n",
      " ...\n",
      " [ 0.32006033]\n",
      " [ 0.13722828]\n",
      " [-0.74711908]]\n",
      "loss=39404.67172249254\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.09576006]\n",
      " [-0.05582427]\n",
      " [-0.08934162]\n",
      " ...\n",
      " [ 0.07503109]\n",
      " [ 0.03158647]\n",
      " [-0.12712494]]\n",
      "t [[ 0.09576006]\n",
      " [-0.05582427]\n",
      " [-0.08934162]\n",
      " ...\n",
      " [ 0.07503109]\n",
      " [ 0.03158647]\n",
      " [-0.12712494]]\n",
      "t [[ 0.18048754]\n",
      " [-0.1197867 ]\n",
      " [-0.17132776]\n",
      " ...\n",
      " [ 0.13398571]\n",
      " [ 0.05705836]\n",
      " [-0.23423763]]\n",
      "t [[ 0.18048754]\n",
      " [-0.1197867 ]\n",
      " [-0.17132776]\n",
      " ...\n",
      " [ 0.13398571]\n",
      " [ 0.05705836]\n",
      " [-0.23423763]]\n",
      "Current iteration=2, loss=47866.54274622556\n",
      "t [[ 0.25563388]\n",
      " [-0.18864609]\n",
      " [-0.24698105]\n",
      " ...\n",
      " [ 0.18027306]\n",
      " [ 0.07746717]\n",
      " [-0.32565755]]\n",
      "t [[ 0.25563388]\n",
      " [-0.18864609]\n",
      " [-0.24698105]\n",
      " ...\n",
      " [ 0.18027306]\n",
      " [ 0.07746717]\n",
      " [-0.32565755]]\n",
      "t [[ 0.32249366]\n",
      " [-0.26002654]\n",
      " [-0.31716215]\n",
      " ...\n",
      " [ 0.21655918]\n",
      " [ 0.09369218]\n",
      " [-0.40475747]]\n",
      "t [[ 0.32249366]\n",
      " [-0.26002654]\n",
      " [-0.31716215]\n",
      " ...\n",
      " [ 0.21655918]\n",
      " [ 0.09369218]\n",
      " [-0.40475747]]\n",
      "Current iteration=4, loss=44884.87713903797\n",
      "t [[ 0.3821875 ]\n",
      " [-0.33224508]\n",
      " [-0.38258533]\n",
      " ...\n",
      " [ 0.24490888]\n",
      " [ 0.10645585]\n",
      " [-0.4741273 ]]\n",
      "t [[ 0.3821875 ]\n",
      " [-0.33224508]\n",
      " [-0.38258533]\n",
      " ...\n",
      " [ 0.24490888]\n",
      " [ 0.10645585]\n",
      " [-0.4741273 ]]\n",
      "t [[ 0.43566948]\n",
      " [-0.40414308]\n",
      " [-0.44384163]\n",
      " ...\n",
      " [ 0.26692715]\n",
      " [ 0.11634794]\n",
      " [-0.5357463 ]]\n",
      "t [[ 0.43566948]\n",
      " [-0.40414308]\n",
      " [-0.44384163]\n",
      " ...\n",
      " [ 0.26692715]\n",
      " [ 0.11634794]\n",
      " [-0.5357463 ]]\n",
      "Current iteration=6, loss=42612.62022138975\n",
      "t [[ 0.48374732]\n",
      " [-0.4749445 ]\n",
      " [-0.50142134]\n",
      " ...\n",
      " [ 0.28386743]\n",
      " [ 0.1238497 ]\n",
      " [-0.59112775]]\n",
      "t [[ 0.48374732]\n",
      " [-0.4749445 ]\n",
      " [-0.50142134]\n",
      " ...\n",
      " [ 0.28386743]\n",
      " [ 0.1238497 ]\n",
      " [-0.59112775]]\n",
      "t [[ 0.52710529]\n",
      " [-0.54414617]\n",
      " [-0.55573337]\n",
      " ...\n",
      " [ 0.29671217]\n",
      " [ 0.1293549 ]\n",
      " [-0.6414323 ]]\n",
      "t [[ 0.52710529]\n",
      " [-0.54414617]\n",
      " [-0.55573337]\n",
      " ...\n",
      " [ 0.29671217]\n",
      " [ 0.1293549 ]\n",
      " [-0.6414323 ]]\n",
      "Current iteration=8, loss=40827.64171367722\n",
      "t [[ 0.56632501]\n",
      " [-0.61143699]\n",
      " [-0.60712103]\n",
      " ...\n",
      " [ 0.30623438]\n",
      " [ 0.13318713]\n",
      " [-0.68755469]]\n",
      "t [[ 0.56632501]\n",
      " [-0.61143699]\n",
      " [-0.60712103]\n",
      " ...\n",
      " [ 0.30623438]\n",
      " [ 0.13318713]\n",
      " [-0.68755469]]\n",
      "t [[ 0.601903  ]\n",
      " [-0.67663973]\n",
      " [-0.65587478]\n",
      " ...\n",
      " [ 0.31304528]\n",
      " [ 0.13561367]\n",
      " [-0.73018928]]\n",
      "loss=39391.81219355702\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0945327 ]\n",
      " [-0.05343987]\n",
      " [-0.08900702]\n",
      " ...\n",
      " [ 0.07618872]\n",
      " [ 0.03216864]\n",
      " [-0.13103975]]\n",
      "t [[ 0.0945327 ]\n",
      " [-0.05343987]\n",
      " [-0.08900702]\n",
      " ...\n",
      " [ 0.07618872]\n",
      " [ 0.03216864]\n",
      " [-0.13103975]]\n",
      "t [[ 0.17817926]\n",
      " [-0.11505482]\n",
      " [-0.17065884]\n",
      " ...\n",
      " [ 0.13586343]\n",
      " [ 0.05814924]\n",
      " [-0.2414636 ]]\n",
      "t [[ 0.17817926]\n",
      " [-0.11505482]\n",
      " [-0.17065884]\n",
      " ...\n",
      " [ 0.13586343]\n",
      " [ 0.05814924]\n",
      " [-0.2414636 ]]\n",
      "Current iteration=2, loss=47898.4831645451\n",
      "t [[ 0.25237496]\n",
      " [-0.18158908]\n",
      " [-0.24598466]\n",
      " ...\n",
      " [ 0.18253578]\n",
      " [ 0.07900773]\n",
      " [-0.33571054]]\n",
      "t [[ 0.25237496]\n",
      " [-0.18158908]\n",
      " [-0.24598466]\n",
      " ...\n",
      " [ 0.18253578]\n",
      " [ 0.07900773]\n",
      " [-0.33571054]]\n",
      "t [[ 0.31839838]\n",
      " [-0.25066238]\n",
      " [-0.31584937]\n",
      " ...\n",
      " [ 0.21895525]\n",
      " [ 0.09563491]\n",
      " [-0.4172501 ]]\n",
      "t [[ 0.31839838]\n",
      " [-0.25066238]\n",
      " [-0.31584937]\n",
      " ...\n",
      " [ 0.21895525]\n",
      " [ 0.09563491]\n",
      " [-0.4172501 ]]\n",
      "Current iteration=4, loss=44947.57820274364\n",
      "t [[ 0.3773567 ]\n",
      " [-0.32059192]\n",
      " [-0.3809694 ]\n",
      " ...\n",
      " [ 0.24724652]\n",
      " [ 0.10876204]\n",
      " [-0.48874614]]\n",
      "t [[ 0.3773567 ]\n",
      " [-0.32059192]\n",
      " [-0.3809694 ]\n",
      " ...\n",
      " [ 0.24724652]\n",
      " [ 0.10876204]\n",
      " [-0.48874614]]\n",
      "t [[ 0.43019269]\n",
      " [-0.39022121]\n",
      " [-0.44193653]\n",
      " ...\n",
      " [ 0.26905624]\n",
      " [ 0.11898555]\n",
      " [-0.55223402]]\n",
      "t [[ 0.43019269]\n",
      " [-0.39022121]\n",
      " [-0.44193653]\n",
      " ...\n",
      " [ 0.26905624]\n",
      " [ 0.11898555]\n",
      " [-0.55223402]]\n",
      "Current iteration=6, loss=42702.17240891591\n",
      "t [[ 0.47770402]\n",
      " [-0.45877729]\n",
      " [-0.49924091]\n",
      " ...\n",
      " [ 0.28566822]\n",
      " [ 0.12679178]\n",
      " [-0.60927036]]\n",
      "t [[ 0.47770402]\n",
      " [-0.45877729]\n",
      " [-0.49924091]\n",
      " ...\n",
      " [ 0.28566822]\n",
      " [ 0.12679178]\n",
      " [-0.60927036]]\n",
      "t [[ 0.52056569]\n",
      " [-0.52576053]\n",
      " [-0.55329081]\n",
      " ...\n",
      " [ 0.29808827]\n",
      " [ 0.1325784 ]\n",
      " [-0.66104992]]\n",
      "t [[ 0.52056569]\n",
      " [-0.52576053]\n",
      " [-0.55329081]\n",
      " ...\n",
      " [ 0.29808827]\n",
      " [ 0.1325784 ]\n",
      " [-0.66104992]]\n",
      "Current iteration=8, loss=40940.01082242466\n",
      "t [[ 0.55935085]\n",
      " [-0.59086324]\n",
      " [-0.60442871]\n",
      " ...\n",
      " [ 0.30710803]\n",
      " [ 0.13667208]\n",
      " [-0.70849475]]\n",
      "t [[ 0.55935085]\n",
      " [-0.59086324]\n",
      " [-0.60442871]\n",
      " ...\n",
      " [ 0.30710803]\n",
      " [ 0.13667208]\n",
      " [-0.70849475]]\n",
      "t [[ 0.59454829]\n",
      " [-0.65391122]\n",
      " [-0.65294413]\n",
      " ...\n",
      " [ 0.31335375]\n",
      " [ 0.13934254]\n",
      " [-0.75232125]]\n",
      "loss=39523.50555101233\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.09438138]\n",
      " [-0.05614392]\n",
      " [-0.08872744]\n",
      " ...\n",
      " [ 0.13593887]\n",
      " [-0.08872744]\n",
      " [ 0.12732601]]\n",
      "t [[ 0.09438138]\n",
      " [-0.05614392]\n",
      " [-0.08872744]\n",
      " ...\n",
      " [ 0.13593887]\n",
      " [-0.08872744]\n",
      " [ 0.12732601]]\n",
      "t [[ 0.17789517]\n",
      " [-0.12001727]\n",
      " [-0.17016358]\n",
      " ...\n",
      " [ 0.25428373]\n",
      " [-0.17016358]\n",
      " [ 0.2353233 ]]\n",
      "t [[ 0.17789517]\n",
      " [-0.12001727]\n",
      " [-0.17016358]\n",
      " ...\n",
      " [ 0.25428373]\n",
      " [-0.17016358]\n",
      " [ 0.2353233 ]]\n",
      "Current iteration=2, loss=47903.6433538307\n",
      "t [[ 0.25198158]\n",
      " [-0.18847022]\n",
      " [-0.24531925]\n",
      " ...\n",
      " [ 0.35774905]\n",
      " [-0.24531925]\n",
      " [ 0.32738549]]\n",
      "t [[ 0.25198158]\n",
      " [-0.18847022]\n",
      " [-0.24531925]\n",
      " ...\n",
      " [ 0.35774905]\n",
      " [-0.24531925]\n",
      " [ 0.32738549]]\n",
      "t [[ 0.31790762]\n",
      " [-0.25921372]\n",
      " [-0.31504633]\n",
      " ...\n",
      " [ 0.44863117]\n",
      " [-0.31504633]\n",
      " [ 0.40630861]]\n",
      "t [[ 0.31790762]\n",
      " [-0.25921372]\n",
      " [-0.31504633]\n",
      " ...\n",
      " [ 0.44863117]\n",
      " [-0.31504633]\n",
      " [ 0.40630861]]\n",
      "Current iteration=4, loss=44950.02282724235\n",
      "t [[ 0.37676441]\n",
      " [-0.33063774]\n",
      " [-0.38005223]\n",
      " ...\n",
      " [ 0.52883971]\n",
      " [-0.38005223]\n",
      " [ 0.47435807]]\n",
      "t [[ 0.37676441]\n",
      " [-0.33063774]\n",
      " [-0.38005223]\n",
      " ...\n",
      " [ 0.52883971]\n",
      " [-0.38005223]\n",
      " [ 0.47435807]]\n",
      "t [[ 0.42948288]\n",
      " [-0.40163861]\n",
      " [-0.4409223 ]\n",
      " ...\n",
      " [ 0.59995348]\n",
      " [-0.4409223 ]\n",
      " [ 0.53335982]]\n",
      "t [[ 0.42948288]\n",
      " [-0.40163861]\n",
      " [-0.4409223 ]\n",
      " ...\n",
      " [ 0.59995348]\n",
      " [-0.4409223 ]\n",
      " [ 0.53335982]]\n",
      "Current iteration=6, loss=42699.86459674822\n",
      "t [[ 0.47685483]\n",
      " [-0.47147936]\n",
      " [-0.49814206]\n",
      " ...\n",
      " [ 0.66327792]\n",
      " [-0.49814206]\n",
      " [ 0.58478692]]\n",
      "t [[ 0.47685483]\n",
      " [-0.47147936]\n",
      " [-0.49814206]\n",
      " ...\n",
      " [ 0.66327792]\n",
      " [-0.49814206]\n",
      " [ 0.58478692]]\n",
      "t [[ 0.51955392]\n",
      " [-0.53968404]\n",
      " [-0.55211641]\n",
      " ...\n",
      " [ 0.71989576]\n",
      " [-0.55211641]\n",
      " [ 0.62983258]]\n",
      "t [[ 0.51955392]\n",
      " [-0.53968404]\n",
      " [-0.55211641]\n",
      " ...\n",
      " [ 0.71989576]\n",
      " [-0.55211641]\n",
      " [ 0.62983258]]\n",
      "Current iteration=8, loss=40932.85823051646\n",
      "t [[ 0.55815453]\n",
      " [-0.6059604 ]\n",
      " [-0.60318541]\n",
      " ...\n",
      " [ 0.77070924]\n",
      " [-0.60318541]\n",
      " [ 0.6694688 ]]\n",
      "t [[ 0.55815453]\n",
      " [-0.6059604 ]\n",
      " [-0.60318541]\n",
      " ...\n",
      " [ 0.77070924]\n",
      " [-0.60318541]\n",
      " [ 0.6694688 ]]\n",
      "t [[ 0.59314787]\n",
      " [-0.6701443 ]\n",
      " [-0.65163681]\n",
      " ...\n",
      " [ 0.81647422]\n",
      " [-0.65163681]\n",
      " [ 0.70449253]]\n",
      "loss=39511.9453655296\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03357632]\n",
      " [-0.16078013]\n",
      " [-0.09428331]\n",
      " ...\n",
      " [ 0.08062475]\n",
      " [ 0.03357632]\n",
      " [-0.13666831]]\n",
      "t [[ 0.03357632]\n",
      " [-0.16078013]\n",
      " [-0.09428331]\n",
      " ...\n",
      " [ 0.08062475]\n",
      " [ 0.03357632]\n",
      " [-0.13666831]]\n",
      "t [[ 0.06032181]\n",
      " [-0.31074764]\n",
      " [-0.18040907]\n",
      " ...\n",
      " [ 0.14310077]\n",
      " [ 0.06032181]\n",
      " [-0.25089404]]\n",
      "t [[ 0.06032181]\n",
      " [-0.31074764]\n",
      " [-0.18040907]\n",
      " ...\n",
      " [ 0.14310077]\n",
      " [ 0.06032181]\n",
      " [-0.25089404]]\n",
      "Current iteration=2, loss=47666.09889024648\n",
      "t [[ 0.08147891]\n",
      " [-0.45049099]\n",
      " [-0.25957493]\n",
      " ...\n",
      " [ 0.19148877]\n",
      " [ 0.08147891]\n",
      " [-0.34777302]]\n",
      "t [[ 0.08147891]\n",
      " [-0.45049099]\n",
      " [-0.25957493]\n",
      " ...\n",
      " [ 0.19148877]\n",
      " [ 0.08147891]\n",
      " [-0.34777302]]\n",
      "t [[ 0.0980722 ]\n",
      " [-0.58069771]\n",
      " [-0.33277513]\n",
      " ...\n",
      " [ 0.22890934]\n",
      " [ 0.0980722 ]\n",
      " [-0.43121345]]\n",
      "t [[ 0.0980722 ]\n",
      " [-0.58069771]\n",
      " [-0.33277513]\n",
      " ...\n",
      " [ 0.22890934]\n",
      " [ 0.0980722 ]\n",
      " [-0.43121345]]\n",
      "Current iteration=4, loss=44586.78592035404\n",
      "t [[ 0.11093159]\n",
      " [-0.70209017]\n",
      " [-0.40082384]\n",
      " ...\n",
      " [ 0.2577387 ]\n",
      " [ 0.11093159]\n",
      " [-0.50416239]]\n",
      "t [[ 0.11093159]\n",
      " [-0.70209017]\n",
      " [-0.40082384]\n",
      " ...\n",
      " [ 0.2577387 ]\n",
      " [ 0.11093159]\n",
      " [-0.50416239]]\n",
      "t [[ 0.12072548]\n",
      " [-0.81538308]\n",
      " [-0.46438671]\n",
      " ...\n",
      " [ 0.27979577]\n",
      " [ 0.12072548]\n",
      " [-0.56883401]]\n",
      "t [[ 0.12072548]\n",
      " [-0.81538308]\n",
      " [-0.46438671]\n",
      " ...\n",
      " [ 0.27979577]\n",
      " [ 0.12072548]\n",
      " [-0.56883401]]\n",
      "Current iteration=6, loss=42268.02878214176\n",
      "t [[ 0.1279926 ]\n",
      " [-0.92125924]\n",
      " [-0.5240102 ]\n",
      " ...\n",
      " [ 0.29648073]\n",
      " [ 0.1279926 ]\n",
      " [-0.62689484]]\n",
      "t [[ 0.1279926 ]\n",
      " [-0.92125924]\n",
      " [-0.5240102 ]\n",
      " ...\n",
      " [ 0.29648073]\n",
      " [ 0.1279926 ]\n",
      " [-0.62689484]]\n",
      "t [[ 0.1331688 ]\n",
      " [-1.02035684]\n",
      " [-0.58014599]\n",
      " ...\n",
      " [ 0.30887625]\n",
      " [ 0.1331688 ]\n",
      " [-0.67960509]]\n",
      "t [[ 0.1331688 ]\n",
      " [-1.02035684]\n",
      " [-0.58014599]\n",
      " ...\n",
      " [ 0.30887625]\n",
      " [ 0.1331688 ]\n",
      " [-0.67960509]]\n",
      "Current iteration=8, loss=40463.850366758605\n",
      "t [[ 0.13660865]\n",
      " [-1.11326383]\n",
      " [-0.63317052]\n",
      " ...\n",
      " [ 0.31782355]\n",
      " [ 0.13660865]\n",
      " [-0.72792415]]\n",
      "t [[ 0.13660865]\n",
      " [-1.11326383]\n",
      " [-0.63317052]\n",
      " ...\n",
      " [ 0.31782355]\n",
      " [ 0.13660865]\n",
      " [-0.72792415]]\n",
      "t [[ 0.1386023 ]\n",
      " [-1.20051661]\n",
      " [-0.6834003 ]\n",
      " ...\n",
      " [ 0.3239803 ]\n",
      " [ 0.1386023 ]\n",
      " [-0.77258861]]\n",
      "loss=39023.93979821859\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.101393  ]\n",
      " [-0.05910805]\n",
      " [-0.09459701]\n",
      " ...\n",
      " [ 0.07944468]\n",
      " [ 0.03344449]\n",
      " [-0.13460288]]\n",
      "t [[ 0.101393  ]\n",
      " [-0.05910805]\n",
      " [-0.09459701]\n",
      " ...\n",
      " [ 0.07944468]\n",
      " [ 0.03344449]\n",
      " [-0.13460288]]\n",
      "t [[ 0.1904206 ]\n",
      " [-0.12733702]\n",
      " [-0.18094961]\n",
      " ...\n",
      " [ 0.14087019]\n",
      " [ 0.06003567]\n",
      " [-0.24677572]]\n",
      "t [[ 0.1904206 ]\n",
      " [-0.12733702]\n",
      " [-0.18094961]\n",
      " ...\n",
      " [ 0.14087019]\n",
      " [ 0.06003567]\n",
      " [-0.24677572]]\n",
      "Current iteration=2, loss=47657.22152032216\n",
      "t [[ 0.26881086]\n",
      " [-0.20083895]\n",
      " [-0.26027421]\n",
      " ...\n",
      " [ 0.18832604]\n",
      " [ 0.08102383]\n",
      " [-0.34165094]]\n",
      "t [[ 0.26881086]\n",
      " [-0.20083895]\n",
      " [-0.26027421]\n",
      " ...\n",
      " [ 0.18832604]\n",
      " [ 0.08102383]\n",
      " [-0.34165094]]\n",
      "t [[ 0.33808806]\n",
      " [-0.27685991]\n",
      " [-0.33358032]\n",
      " ...\n",
      " [ 0.22492105]\n",
      " [ 0.09743952]\n",
      " [-0.42316037]]\n",
      "t [[ 0.33808806]\n",
      " [-0.27685991]\n",
      " [-0.33358032]\n",
      " ...\n",
      " [ 0.22492105]\n",
      " [ 0.09743952]\n",
      " [-0.42316037]]\n",
      "Current iteration=4, loss=44574.90884903374\n",
      "t [[ 0.39955469]\n",
      " [-0.35350221]\n",
      " [-0.40169414]\n",
      " ...\n",
      " [ 0.25302113]\n",
      " [ 0.11011689]\n",
      " [-0.49426507]]\n",
      "t [[ 0.39955469]\n",
      " [-0.35350221]\n",
      " [-0.40169414]\n",
      " ...\n",
      " [ 0.25302113]\n",
      " [ 0.11011689]\n",
      " [-0.49426507]]\n",
      "t [[ 0.45430582]\n",
      " [-0.42949908]\n",
      " [-0.46529065]\n",
      " ...\n",
      " [ 0.27443568]\n",
      " [ 0.11972736]\n",
      " [-0.55718619]]\n",
      "t [[ 0.45430582]\n",
      " [-0.42949908]\n",
      " [-0.46529065]\n",
      " ...\n",
      " [ 0.27443568]\n",
      " [ 0.11972736]\n",
      " [-0.55718619]]\n",
      "Current iteration=6, loss=42255.246063960425\n",
      "t [[ 0.50325832]\n",
      " [-0.5040315 ]\n",
      " [-0.52492349]\n",
      " ...\n",
      " [ 0.29055576]\n",
      " [ 0.12681179]\n",
      " [-0.61359236]]\n",
      "t [[ 0.50325832]\n",
      " [-0.5040315 ]\n",
      " [-0.52492349]\n",
      " ...\n",
      " [ 0.29055576]\n",
      " [ 0.12681179]\n",
      " [-0.61359236]]\n",
      "t [[ 0.54718147]\n",
      " [-0.57659116]\n",
      " [-0.58104991]\n",
      " ...\n",
      " [ 0.30245543]\n",
      " [ 0.13180755]\n",
      " [-0.66474268]]\n",
      "t [[ 0.54718147]\n",
      " [-0.57659116]\n",
      " [-0.58104991]\n",
      " ...\n",
      " [ 0.30245543]\n",
      " [ 0.13180755]\n",
      " [-0.66474268]]\n",
      "Current iteration=8, loss=40450.92083581905\n",
      "t [[ 0.58672352]\n",
      " [-0.64688257]\n",
      " [-0.63405064]\n",
      " ...\n",
      " [ 0.31096804]\n",
      " [ 0.13507026]\n",
      " [-0.71159347]]\n",
      "t [[ 0.58672352]\n",
      " [-0.64688257]\n",
      " [-0.63405064]\n",
      " ...\n",
      " [ 0.31096804]\n",
      " [ 0.13507026]\n",
      " [-0.71159347]]\n",
      "t [[ 0.62243349]\n",
      " [-0.71475444]\n",
      " [-0.6842455 ]\n",
      " ...\n",
      " [ 0.31674414]\n",
      " [ 0.13689082]\n",
      " [-0.75487707]]\n",
      "loss=39011.12072471351\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.10009345]\n",
      " [-0.0565834 ]\n",
      " [-0.09424273]\n",
      " ...\n",
      " [ 0.08067041]\n",
      " [ 0.03406091]\n",
      " [-0.13874797]]\n",
      "t [[ 0.10009345]\n",
      " [-0.0565834 ]\n",
      " [-0.09424273]\n",
      " ...\n",
      " [ 0.08067041]\n",
      " [ 0.03406091]\n",
      " [-0.13874797]]\n",
      "t [[ 0.18798559]\n",
      " [-0.12232909]\n",
      " [-0.18024139]\n",
      " ...\n",
      " [ 0.14283155]\n",
      " [ 0.06118619]\n",
      " [-0.25438955]]\n",
      "t [[ 0.18798559]\n",
      " [-0.12232909]\n",
      " [-0.18024139]\n",
      " ...\n",
      " [ 0.14283155]\n",
      " [ 0.06118619]\n",
      " [-0.25438955]]\n",
      "Current iteration=2, loss=47691.06469001646\n",
      "t [[ 0.26538517]\n",
      " [-0.19337194]\n",
      " [-0.2592202 ]\n",
      " ...\n",
      " [ 0.19065501]\n",
      " [ 0.08264333]\n",
      " [-0.3521994 ]]\n",
      "t [[ 0.26538517]\n",
      " [-0.19337194]\n",
      " [-0.2592202 ]\n",
      " ...\n",
      " [ 0.19065501]\n",
      " [ 0.08264333]\n",
      " [-0.3521994 ]]\n",
      "t [[ 0.3337977 ]\n",
      " [-0.26695341]\n",
      " [-0.33219349]\n",
      " ...\n",
      " [ 0.22734635]\n",
      " [ 0.09947625]\n",
      " [-0.43622205]]\n",
      "t [[ 0.3337977 ]\n",
      " [-0.26695341]\n",
      " [-0.33219349]\n",
      " ...\n",
      " [ 0.22734635]\n",
      " [ 0.09947625]\n",
      " [-0.43622205]]\n",
      "Current iteration=4, loss=44641.04950084252\n",
      "t [[ 0.3945101 ]\n",
      " [-0.34117651]\n",
      " [-0.39998967]\n",
      " ...\n",
      " [ 0.25533868]\n",
      " [ 0.11252916]\n",
      " [-0.50950283]]\n",
      "t [[ 0.3945101 ]\n",
      " [-0.34117651]\n",
      " [-0.39998967]\n",
      " ...\n",
      " [ 0.25533868]\n",
      " [ 0.11252916]\n",
      " [-0.50950283]]\n",
      "t [[ 0.44860442]\n",
      " [-0.41477738]\n",
      " [-0.46328431]\n",
      " ...\n",
      " [ 0.27648735]\n",
      " [ 0.12248094]\n",
      " [-0.57432578]]\n",
      "t [[ 0.44860442]\n",
      " [-0.41477738]\n",
      " [-0.46328431]\n",
      " ...\n",
      " [ 0.27648735]\n",
      " [ 0.12248094]\n",
      " [-0.57432578]]\n",
      "Current iteration=6, loss=42349.20423246736\n",
      "t [[ 0.49698586]\n",
      " [-0.48694098]\n",
      " [-0.52263068]\n",
      " ...\n",
      " [ 0.29221675]\n",
      " [ 0.12987802]\n",
      " [-0.63240754]]\n",
      "t [[ 0.49698586]\n",
      " [-0.48694098]\n",
      " [-0.52263068]\n",
      " ...\n",
      " [ 0.29221675]\n",
      " [ 0.12987802]\n",
      " [-0.63240754]]\n",
      "t [[ 0.54041296]\n",
      " [-0.55716331]\n",
      " [-0.57848514]\n",
      " ...\n",
      " [ 0.3036265 ]\n",
      " [ 0.13516205]\n",
      " [-0.68504465]]\n",
      "t [[ 0.54041296]\n",
      " [-0.55716331]\n",
      " [-0.57848514]\n",
      " ...\n",
      " [ 0.3036265 ]\n",
      " [ 0.13516205]\n",
      " [-0.68504465]]\n",
      "Current iteration=8, loss=40568.24452616807\n",
      "t [[ 0.57952424]\n",
      " [-0.62515286]\n",
      " [-0.63122735]\n",
      " ...\n",
      " [ 0.31157018]\n",
      " [ 0.13869196]\n",
      " [-0.73322311]]\n",
      "t [[ 0.57952424]\n",
      " [-0.62515286]\n",
      " [-0.63122735]\n",
      " ...\n",
      " [ 0.31157018]\n",
      " [ 0.13869196]\n",
      " [-0.73322311]]\n",
      "t [[ 0.61486003]\n",
      " [-0.69076172]\n",
      " [-0.68117598]\n",
      " ...\n",
      " [ 0.31671452]\n",
      " [ 0.14076127]\n",
      " [-0.77769903]]\n",
      "loss=39148.04947553517\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.09993322]\n",
      " [-0.0594465 ]\n",
      " [-0.0939467 ]\n",
      " ...\n",
      " [ 0.14393527]\n",
      " [-0.0939467 ]\n",
      " [ 0.13481577]]\n",
      "t [[ 0.09993322]\n",
      " [-0.0594465 ]\n",
      " [-0.0939467 ]\n",
      " ...\n",
      " [ 0.14393527]\n",
      " [-0.0939467 ]\n",
      " [ 0.13481577]]\n",
      "t [[ 0.18768571]\n",
      " [-0.12755605]\n",
      " [-0.17972092]\n",
      " ...\n",
      " [ 0.26815067]\n",
      " [-0.17972092]\n",
      " [ 0.24796754]]\n",
      "t [[ 0.18768571]\n",
      " [-0.12755605]\n",
      " [-0.17972092]\n",
      " ...\n",
      " [ 0.26815067]\n",
      " [-0.17972092]\n",
      " [ 0.24796754]]\n",
      "Current iteration=2, loss=47696.25669487748\n",
      "t [[ 0.26497134]\n",
      " [-0.20058999]\n",
      " [-0.25852509]\n",
      " ...\n",
      " [ 0.37587468]\n",
      " [-0.25852509]\n",
      " [ 0.34349015]]\n",
      "t [[ 0.26497134]\n",
      " [-0.20058999]\n",
      " [-0.25852509]\n",
      " ...\n",
      " [ 0.37587468]\n",
      " [-0.25852509]\n",
      " [ 0.34349015]]\n",
      "t [[ 0.3332808 ]\n",
      " [-0.27589685]\n",
      " [-0.33135849]\n",
      " ...\n",
      " [ 0.46980127]\n",
      " [-0.33135849]\n",
      " [ 0.42465696]]\n",
      "t [[ 0.3332808 ]\n",
      " [-0.27589685]\n",
      " [-0.33135849]\n",
      " ...\n",
      " [ 0.46980127]\n",
      " [-0.33135849]\n",
      " [ 0.42465696]]\n",
      "Current iteration=4, loss=44643.01762271552\n",
      "t [[ 0.39388234]\n",
      " [-0.35166248]\n",
      " [-0.39903939]\n",
      " ...\n",
      " [ 0.55213936]\n",
      " [-0.39903939]\n",
      " [ 0.49407855]]\n",
      "t [[ 0.39388234]\n",
      " [-0.35166248]\n",
      " [-0.39903939]\n",
      " ...\n",
      " [ 0.55213936]\n",
      " [-0.39903939]\n",
      " [ 0.49407855]]\n",
      "t [[ 0.44784553]\n",
      " [-0.42668101]\n",
      " [-0.46223626]\n",
      " ...\n",
      " [ 0.62469136]\n",
      " [-0.46223626]\n",
      " [ 0.55382818]]\n",
      "t [[ 0.44784553]\n",
      " [-0.42668101]\n",
      " [-0.46223626]\n",
      " ...\n",
      " [ 0.62469136]\n",
      " [-0.46223626]\n",
      " [ 0.55382818]]\n",
      "Current iteration=6, loss=42346.03359512174\n",
      "t [[ 0.49607039]\n",
      " [-0.50017542]\n",
      " [-0.52149734]\n",
      " ...\n",
      " [ 0.68892983]\n",
      " [-0.52149734]\n",
      " [ 0.60555611]]\n",
      "t [[ 0.49607039]\n",
      " [-0.50017542]\n",
      " [-0.52149734]\n",
      " ...\n",
      " [ 0.68892983]\n",
      " [-0.52149734]\n",
      " [ 0.60555611]]\n",
      "t [[ 0.53931517]\n",
      " [-0.57166591]\n",
      " [-0.57727543]\n",
      " ...\n",
      " [ 0.74606312]\n",
      " [-0.57727543]\n",
      " [ 0.65058302]]\n",
      "t [[ 0.53931517]\n",
      " [-0.57166591]\n",
      " [-0.57727543]\n",
      " ...\n",
      " [ 0.74606312]\n",
      " [-0.57727543]\n",
      " [ 0.65058302]]\n",
      "Current iteration=8, loss=40560.00346594225\n",
      "t [[ 0.5782206 ]\n",
      " [-0.64087625]\n",
      " [-0.62994768]\n",
      " ...\n",
      " [ 0.79708848]\n",
      " [-0.62994768]\n",
      " [ 0.68997329]]\n",
      "t [[ 0.5782206 ]\n",
      " [-0.64087625]\n",
      " [-0.62994768]\n",
      " ...\n",
      " [ 0.79708848]\n",
      " [-0.62994768]\n",
      " [ 0.68997329]]\n",
      "t [[ 0.61333018]\n",
      " [-0.70766819]\n",
      " [-0.67983103]\n",
      " ...\n",
      " [ 0.84283423]\n",
      " [-0.67983103]\n",
      " [ 0.72459122]]\n",
      "loss=39135.27813654507\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03544167]\n",
      " [-0.16971236]\n",
      " [-0.09952128]\n",
      " ...\n",
      " [ 0.0851039 ]\n",
      " [ 0.03544167]\n",
      " [-0.144261  ]]\n",
      "t [[ 0.03544167]\n",
      " [-0.16971236]\n",
      " [-0.09952128]\n",
      " ...\n",
      " [ 0.0851039 ]\n",
      " [ 0.03544167]\n",
      " [-0.144261  ]]\n",
      "t [[ 0.06327451]\n",
      " [-0.32737941]\n",
      " [-0.18995558]\n",
      " ...\n",
      " [ 0.14999163]\n",
      " [ 0.06327451]\n",
      " [-0.26352336]]\n",
      "t [[ 0.06327451]\n",
      " [-0.32737941]\n",
      " [-0.18995558]\n",
      " ...\n",
      " [ 0.14999163]\n",
      " [ 0.06327451]\n",
      " [-0.26352336]]\n",
      "Current iteration=2, loss=47460.12943392856\n",
      "t [[ 0.08496208]\n",
      " [-0.47369774]\n",
      " [-0.27271418]\n",
      " ...\n",
      " [ 0.19944159]\n",
      " [ 0.08496208]\n",
      " [-0.36378543]]\n",
      "t [[ 0.08496208]\n",
      " [-0.47369774]\n",
      " [-0.27271418]\n",
      " ...\n",
      " [ 0.19944159]\n",
      " [ 0.08496208]\n",
      " [-0.36378543]]\n",
      "t [[ 0.10169356]\n",
      " [-0.60948341]\n",
      " [-0.348952  ]\n",
      " ...\n",
      " [ 0.2370575 ]\n",
      " [ 0.10169356]\n",
      " [-0.44955896]]\n",
      "t [[ 0.10169356]\n",
      " [-0.60948341]\n",
      " [-0.348952  ]\n",
      " ...\n",
      " [ 0.2370575 ]\n",
      " [ 0.10169356]\n",
      " [-0.44955896]]\n",
      "Current iteration=4, loss=44285.913762329146\n",
      "t [[ 0.11441781]\n",
      " [-0.73558907]\n",
      " [-0.41960161]\n",
      " ...\n",
      " [ 0.26553559]\n",
      " [ 0.11441781]\n",
      " [-0.5241801 ]]\n",
      "t [[ 0.11441781]\n",
      " [-0.73558907]\n",
      " [-0.41960161]\n",
      " ...\n",
      " [ 0.26553559]\n",
      " [ 0.11441781]\n",
      " [-0.5241801 ]]\n",
      "t [[ 0.12388839]\n",
      " [-0.85285151]\n",
      " [-0.48541592]\n",
      " ...\n",
      " [ 0.2869065 ]\n",
      " [ 0.12388839]\n",
      " [-0.59010918]]\n",
      "t [[ 0.12388839]\n",
      " [-0.85285151]\n",
      " [-0.48541592]\n",
      " ...\n",
      " [ 0.2869065 ]\n",
      " [ 0.12388839]\n",
      " [-0.59010918]]\n",
      "Current iteration=6, loss=41924.79316314314\n",
      "t [[ 0.13070496]\n",
      " [-0.96206309]\n",
      " [-0.54700629]\n",
      " ...\n",
      " [ 0.30270916]\n",
      " [ 0.13070496]\n",
      " [-0.64916616]]\n",
      "t [[ 0.13070496]\n",
      " [-0.96206309]\n",
      " [-0.54700629]\n",
      " ...\n",
      " [ 0.30270916]\n",
      " [ 0.13070496]\n",
      " [-0.64916616]]\n",
      "t [[ 0.13534717]\n",
      " [-1.0639578 ]\n",
      " [-0.60487337]\n",
      " ...\n",
      " [ 0.31411598]\n",
      " [ 0.13534717]\n",
      " [-0.70270552]]\n",
      "t [[ 0.13534717]\n",
      " [-1.0639578 ]\n",
      " [-0.60487337]\n",
      " ...\n",
      " [ 0.31411598]\n",
      " [ 0.13534717]\n",
      " [-0.70270552]]\n",
      "Current iteration=8, loss=40105.122824655395\n",
      "t [[ 0.13820118]\n",
      " [-1.15920586]\n",
      " [-0.65943111]\n",
      " ...\n",
      " [ 0.32202566]\n",
      " [ 0.13820118]\n",
      " [-0.75174388]]\n",
      "t [[ 0.13820118]\n",
      " [-1.15920586]\n",
      " [-0.65943111]\n",
      " ...\n",
      " [ 0.32202566]\n",
      " [ 0.13820118]\n",
      " [-0.75174388]]\n",
      "t [[ 0.13958008]\n",
      " [-1.24841355]\n",
      " [-0.71102533]\n",
      " ...\n",
      " [ 0.3271322 ]\n",
      " [ 0.13958008]\n",
      " [-0.79705209]]\n",
      "loss=38664.00051198092\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.10702595]\n",
      " [-0.06239184]\n",
      " [-0.0998524 ]\n",
      " ...\n",
      " [ 0.08385828]\n",
      " [ 0.03530252]\n",
      " [-0.14208081]]\n",
      "t [[ 0.10702595]\n",
      " [-0.06239184]\n",
      " [-0.0998524 ]\n",
      " ...\n",
      " [ 0.08385828]\n",
      " [ 0.03530252]\n",
      " [-0.14208081]]\n",
      "t [[ 0.20027827]\n",
      " [-0.13494283]\n",
      " [-0.19052109]\n",
      " ...\n",
      " [ 0.14764465]\n",
      " [ 0.06297117]\n",
      " [-0.25917701]]\n",
      "t [[ 0.20027827]\n",
      " [-0.13494283]\n",
      " [-0.19052109]\n",
      " ...\n",
      " [ 0.14764465]\n",
      " [ 0.06297117]\n",
      " [-0.25917701]]\n",
      "Current iteration=2, loss=47450.94328285798\n",
      "t [[ 0.28179502]\n",
      " [-0.21312736]\n",
      " [-0.27343945]\n",
      " ...\n",
      " [ 0.1961243 ]\n",
      " [ 0.08447877]\n",
      " [-0.3573299 ]]\n",
      "t [[ 0.28179502]\n",
      " [-0.21312736]\n",
      " [-0.27343945]\n",
      " ...\n",
      " [ 0.1961243 ]\n",
      " [ 0.08447877]\n",
      " [-0.3573299 ]]\n",
      "t [[ 0.3533534 ]\n",
      " [-0.29378524]\n",
      " [-0.34978007]\n",
      " ...\n",
      " [ 0.23288724]\n",
      " [ 0.10102133]\n",
      " [-0.44107811]]\n",
      "t [[ 0.3533534 ]\n",
      " [-0.29378524]\n",
      " [-0.34978007]\n",
      " ...\n",
      " [ 0.23288724]\n",
      " [ 0.10102133]\n",
      " [-0.44107811]]\n",
      "Current iteration=4, loss=44273.84098681237\n",
      "t [[ 0.41645211]\n",
      " [-0.37479983]\n",
      " [-0.42048921]\n",
      " ...\n",
      " [ 0.26061772]\n",
      " [ 0.11355248]\n",
      " [-0.51377294]]\n",
      "t [[ 0.41645211]\n",
      " [-0.37479983]\n",
      " [-0.42048921]\n",
      " ...\n",
      " [ 0.26061772]\n",
      " [ 0.11355248]\n",
      " [-0.51377294]]\n",
      "t [[ 0.4723357 ]\n",
      " [-0.45480282]\n",
      " [-0.48633021]\n",
      " ...\n",
      " [ 0.28133527]\n",
      " [ 0.1228291 ]\n",
      " [-0.57788158]]\n",
      "t [[ 0.4723357 ]\n",
      " [-0.45480282]\n",
      " [-0.48633021]\n",
      " ...\n",
      " [ 0.28133527]\n",
      " [ 0.1228291 ]\n",
      " [-0.57788158]]\n",
      "Current iteration=6, loss=41911.93908798279\n",
      "t [[ 0.52203499]\n",
      " [-0.53294223]\n",
      " [-0.54792234]\n",
      " ...\n",
      " [ 0.29656819]\n",
      " [ 0.12945314]\n",
      " [-0.63522525]]\n",
      "t [[ 0.52203499]\n",
      " [-0.53294223]\n",
      " [-0.54792234]\n",
      " ...\n",
      " [ 0.29656819]\n",
      " [ 0.12945314]\n",
      " [-0.63522525]]\n",
      "t [[ 0.5664068 ]\n",
      " [-0.60871459]\n",
      " [-0.60577225]\n",
      " ...\n",
      " [ 0.30747901]\n",
      " [ 0.13390583]\n",
      " [-0.68715622]]\n",
      "t [[ 0.5664068 ]\n",
      " [-0.60871459]\n",
      " [-0.60577225]\n",
      " ...\n",
      " [ 0.30747901]\n",
      " [ 0.13390583]\n",
      " [-0.68715622]]\n",
      "Current iteration=8, loss=40092.20128928097\n",
      "t [[ 0.60616716]\n",
      " [-0.68184853]\n",
      " [-0.66029849]\n",
      " ...\n",
      " [ 0.31495745]\n",
      " [ 0.13657444]\n",
      " [-0.73468681]]\n",
      "t [[ 0.60616716]\n",
      " [-0.68184853]\n",
      " [-0.66029849]\n",
      " ...\n",
      " [ 0.31495745]\n",
      " [ 0.13657444]\n",
      " [-0.73468681]]\n",
      "t [[ 0.64191783]\n",
      " [-0.75222545]\n",
      " [-0.71185038]\n",
      " ...\n",
      " [ 0.3196896 ]\n",
      " [ 0.13777278]\n",
      " [-0.77858242]]\n",
      "loss=38651.228135870784\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.1056542 ]\n",
      " [-0.05972692]\n",
      " [-0.09947843]\n",
      " ...\n",
      " [ 0.0851521 ]\n",
      " [ 0.03595318]\n",
      " [-0.1464562 ]]\n",
      "t [[ 0.1056542 ]\n",
      " [-0.05972692]\n",
      " [-0.09947843]\n",
      " ...\n",
      " [ 0.0851521 ]\n",
      " [ 0.03595318]\n",
      " [-0.1464562 ]]\n",
      "t [[ 0.19771752]\n",
      " [-0.1296591 ]\n",
      " [-0.18977359]\n",
      " ...\n",
      " [ 0.14968677]\n",
      " [ 0.06418085]\n",
      " [-0.26717464]]\n",
      "t [[ 0.19771752]\n",
      " [-0.1296591 ]\n",
      " [-0.18977359]\n",
      " ...\n",
      " [ 0.14968677]\n",
      " [ 0.06418085]\n",
      " [-0.26717464]]\n",
      "Current iteration=2, loss=47486.687562239946\n",
      "t [[ 0.27820516]\n",
      " [-0.20525083]\n",
      " [-0.27232801]\n",
      " ...\n",
      " [ 0.19851316]\n",
      " [ 0.08617604]\n",
      " [-0.36836435]]\n",
      "t [[ 0.27820516]\n",
      " [-0.20525083]\n",
      " [-0.27232801]\n",
      " ...\n",
      " [ 0.19851316]\n",
      " [ 0.08617604]\n",
      " [-0.36836435]]\n",
      "t [[ 0.34887248]\n",
      " [-0.28333714]\n",
      " [-0.34831968]\n",
      " ...\n",
      " [ 0.23533223]\n",
      " [ 0.10315022]\n",
      " [-0.45469386]]\n",
      "t [[ 0.34887248]\n",
      " [-0.28333714]\n",
      " [-0.34831968]\n",
      " ...\n",
      " [ 0.23533223]\n",
      " [ 0.10315022]\n",
      " [-0.45469386]]\n",
      "Current iteration=4, loss=44343.37368461593\n",
      "t [[ 0.4112004 ]\n",
      " [-0.36180277]\n",
      " [-0.41869713]\n",
      " ...\n",
      " [ 0.26290293]\n",
      " [ 0.11606835]\n",
      " [-0.52960953]]\n",
      "t [[ 0.4112004 ]\n",
      " [-0.36180277]\n",
      " [-0.41869713]\n",
      " ...\n",
      " [ 0.26290293]\n",
      " [ 0.11606835]\n",
      " [-0.52960953]]\n",
      "t [[ 0.4664185 ]\n",
      " [-0.43928328]\n",
      " [-0.48422406]\n",
      " ...\n",
      " [ 0.28329506]\n",
      " [ 0.12569555]\n",
      " [-0.59564841]]\n",
      "t [[ 0.4664185 ]\n",
      " [-0.43928328]\n",
      " [-0.48422406]\n",
      " ...\n",
      " [ 0.28329506]\n",
      " [ 0.12569555]\n",
      " [-0.59564841]]\n",
      "Current iteration=6, loss=42010.1874897944\n",
      "t [[ 0.51554416]\n",
      " [-0.51493169]\n",
      " [-0.54551906]\n",
      " ...\n",
      " [ 0.2980733 ]\n",
      " [ 0.13263988]\n",
      " [-0.65468446]]\n",
      "t [[ 0.51554416]\n",
      " [-0.51493169]\n",
      " [-0.54551906]\n",
      " ...\n",
      " [ 0.2980733 ]\n",
      " [ 0.13263988]\n",
      " [-0.65468446]]\n",
      "t [[ 0.55942192]\n",
      " [-0.58824963]\n",
      " [-0.60308766]\n",
      " ...\n",
      " [ 0.30842795]\n",
      " [ 0.13738717]\n",
      " [-0.7081107 ]]\n",
      "t [[ 0.55942192]\n",
      " [-0.58824963]\n",
      " [-0.60308766]\n",
      " ...\n",
      " [ 0.30842795]\n",
      " [ 0.13738717]\n",
      " [-0.7081107 ]]\n",
      "Current iteration=8, loss=40214.29994524465\n",
      "t [[ 0.59875675]\n",
      " [-0.65897025]\n",
      " [-0.65734707]\n",
      " ...\n",
      " [ 0.31527053]\n",
      " [ 0.14032823]\n",
      " [-0.75697143]]\n",
      "t [[ 0.59875675]\n",
      " [-0.65897025]\n",
      " [-0.65734707]\n",
      " ...\n",
      " [ 0.31527053]\n",
      " [ 0.14032823]\n",
      " [-0.75697143]]\n",
      "t [[ 0.63414075]\n",
      " [-0.72697865]\n",
      " [-0.70864523]\n",
      " ...\n",
      " [ 0.31930437]\n",
      " [ 0.14177968]\n",
      " [-0.80205748]]\n",
      "loss=38793.16117933752\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.10548507]\n",
      " [-0.06274909]\n",
      " [-0.09916597]\n",
      " ...\n",
      " [ 0.15193167]\n",
      " [-0.09916597]\n",
      " [ 0.14230554]]\n",
      "t [[ 0.10548507]\n",
      " [-0.06274909]\n",
      " [-0.09916597]\n",
      " ...\n",
      " [ 0.15193167]\n",
      " [-0.09916597]\n",
      " [ 0.14230554]]\n",
      "t [[ 0.19740193]\n",
      " [-0.1351476 ]\n",
      " [-0.18922834]\n",
      " ...\n",
      " [ 0.28189726]\n",
      " [-0.18922834]\n",
      " [ 0.26047963]]\n",
      "t [[ 0.19740193]\n",
      " [-0.1351476 ]\n",
      " [-0.18922834]\n",
      " ...\n",
      " [ 0.28189726]\n",
      " [-0.18922834]\n",
      " [ 0.26047963]]\n",
      "Current iteration=2, loss=47491.88810591874\n",
      "t [[ 0.27777096]\n",
      " [-0.21279941]\n",
      " [-0.27160407]\n",
      " ...\n",
      " [ 0.39370065]\n",
      " [-0.27160407]\n",
      " [ 0.35927461]]\n",
      "t [[ 0.27777096]\n",
      " [-0.21279941]\n",
      " [-0.27160407]\n",
      " ...\n",
      " [ 0.39370065]\n",
      " [-0.27160407]\n",
      " [ 0.35927461]]\n",
      "t [[ 0.34832898]\n",
      " [-0.29266383]\n",
      " [-0.34745396]\n",
      " ...\n",
      " [ 0.49047153]\n",
      " [-0.34745396]\n",
      " [ 0.44248528]]\n",
      "t [[ 0.34832898]\n",
      " [-0.29266383]\n",
      " [-0.34745396]\n",
      " ...\n",
      " [ 0.49047153]\n",
      " [-0.34745396]\n",
      " [ 0.44248528]]\n",
      "Current iteration=4, loss=44344.83948802266\n",
      "t [[ 0.41053568]\n",
      " [-0.37271882]\n",
      " [-0.41771519]\n",
      " ...\n",
      " [ 0.57473992]\n",
      " [-0.41771519]\n",
      " [ 0.51309021]]\n",
      "t [[ 0.41053568]\n",
      " [-0.37271882]\n",
      " [-0.41771519]\n",
      " ...\n",
      " [ 0.57473992]\n",
      " [-0.41771519]\n",
      " [ 0.51309021]]\n",
      "t [[ 0.46560788]\n",
      " [-0.4516627 ]\n",
      " [-0.48314373]\n",
      " ...\n",
      " [ 0.64854329]\n",
      " [-0.48314373]\n",
      " [ 0.57342024]]\n",
      "t [[ 0.46560788]\n",
      " [-0.4516627 ]\n",
      " [-0.48314373]\n",
      " ...\n",
      " [ 0.64854329]\n",
      " [-0.48314373]\n",
      " [ 0.57342024]]\n",
      "Current iteration=6, loss=42006.15196610802\n",
      "t [[ 0.51455872]\n",
      " [-0.52868812]\n",
      " [-0.54435278]\n",
      " ...\n",
      " [ 0.71352684]\n",
      " [-0.54435278]\n",
      " [ 0.62530625]]\n",
      "t [[ 0.51455872]\n",
      " [-0.52868812]\n",
      " [-0.54435278]\n",
      " ...\n",
      " [ 0.71352684]\n",
      " [-0.54435278]\n",
      " [ 0.62530625]]\n",
      "t [[ 0.55823364]\n",
      " [-0.60332112]\n",
      " [-0.60184412]\n",
      " ...\n",
      " [ 0.77102652]\n",
      " [-0.60184412]\n",
      " [ 0.67019638]]\n",
      "t [[ 0.55823364]\n",
      " [-0.60332112]\n",
      " [-0.60184412]\n",
      " ...\n",
      " [ 0.77102652]\n",
      " [-0.60184412]\n",
      " [ 0.67019638]]\n",
      "Current iteration=8, loss=40204.99299305852\n",
      "t [[ 0.59734083]\n",
      " [-0.6753098 ]\n",
      " [-0.65603243]\n",
      " ...\n",
      " [ 0.8221347 ]\n",
      " [-0.65603243]\n",
      " [ 0.70924562]]\n",
      "t [[ 0.59734083]\n",
      " [-0.6753098 ]\n",
      " [-0.65603243]\n",
      " ...\n",
      " [ 0.8221347 ]\n",
      " [-0.65603243]\n",
      " [ 0.70924562]]\n",
      "t [[ 0.63247629]\n",
      " [-0.74454837]\n",
      " [-0.70726391]\n",
      " ...\n",
      " [ 0.86775129]\n",
      " [-0.70726391]\n",
      " [ 0.74338329]]\n",
      "loss=38779.220100387494\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03730702]\n",
      " [-0.17864459]\n",
      " [-0.10475924]\n",
      " ...\n",
      " [ 0.08958306]\n",
      " [ 0.03730702]\n",
      " [-0.15185368]]\n",
      "t [[ 0.03730702]\n",
      " [-0.17864459]\n",
      " [-0.10475924]\n",
      " ...\n",
      " [ 0.08958306]\n",
      " [ 0.03730702]\n",
      " [-0.15185368]]\n",
      "t [[ 0.06618562]\n",
      " [-0.34394502]\n",
      " [-0.19945232]\n",
      " ...\n",
      " [ 0.15677187]\n",
      " [ 0.06618562]\n",
      " [-0.27601602]]\n",
      "t [[ 0.06618562]\n",
      " [-0.34394502]\n",
      " [-0.19945232]\n",
      " ...\n",
      " [ 0.15677187]\n",
      " [ 0.06618562]\n",
      " [-0.27601602]]\n",
      "Current iteration=2, loss=47257.13813284489\n",
      "t [[ 0.08834528]\n",
      " [-0.49671885]\n",
      " [-0.28572823]\n",
      " ...\n",
      " [ 0.20714216]\n",
      " [ 0.08834528]\n",
      " [-0.37948834]]\n",
      "t [[ 0.08834528]\n",
      " [-0.49671885]\n",
      " [-0.28572823]\n",
      " ...\n",
      " [ 0.20714216]\n",
      " [ 0.08834528]\n",
      " [-0.37948834]]\n",
      "t [[ 0.10515416]\n",
      " [-0.6379261 ]\n",
      " [-0.36491678]\n",
      " ...\n",
      " [ 0.24481921]\n",
      " [ 0.10515416]\n",
      " [-0.46743307]]\n",
      "t [[ 0.10515416]\n",
      " [-0.6379261 ]\n",
      " [-0.36491678]\n",
      " ...\n",
      " [ 0.24481921]\n",
      " [ 0.10515416]\n",
      " [-0.46743307]]\n",
      "Current iteration=4, loss=43993.57539956295\n",
      "t [[ 0.11768744]\n",
      " [-0.76856381]\n",
      " [-0.43807672]\n",
      " ...\n",
      " [ 0.27283521]\n",
      " [ 0.11768744]\n",
      " [-0.54359241]]\n",
      "t [[ 0.11768744]\n",
      " [-0.76856381]\n",
      " [-0.43807672]\n",
      " ...\n",
      " [ 0.27283521]\n",
      " [ 0.11768744]\n",
      " [-0.54359241]]\n",
      "t [[ 0.12678718]\n",
      " [-0.88960234]\n",
      " [-0.50605222]\n",
      " ...\n",
      " [ 0.29343675]\n",
      " [ 0.12678718]\n",
      " [-0.61067506]]\n",
      "t [[ 0.12678718]\n",
      " [-0.88960234]\n",
      " [-0.50605222]\n",
      " ...\n",
      " [ 0.29343675]\n",
      " [ 0.12678718]\n",
      " [-0.61067506]]\n",
      "Current iteration=6, loss=41594.84829428998\n",
      "t [[ 0.13311508]\n",
      " [-1.00195235]\n",
      " [-0.56952148]\n",
      " ...\n",
      " [ 0.30829993]\n",
      " [ 0.13311508]\n",
      " [-0.67065031]]\n",
      "t [[ 0.13311508]\n",
      " [-1.00195235]\n",
      " [-0.56952148]\n",
      " ...\n",
      " [ 0.30829993]\n",
      " [ 0.13311508]\n",
      " [-0.67065031]]\n",
      "t [[ 0.13719455]\n",
      " [-1.10644981]\n",
      " [-0.62903492]\n",
      " ...\n",
      " [ 0.31868308]\n",
      " [ 0.13719455]\n",
      " [-0.72496105]]\n",
      "t [[ 0.13719455]\n",
      " [-1.10644981]\n",
      " [-0.62903492]\n",
      " ...\n",
      " [ 0.31868308]\n",
      " [ 0.13719455]\n",
      " [-0.72496105]]\n",
      "Current iteration=8, loss=39763.20521759242\n",
      "t [[ 0.13944278]\n",
      " [-1.20385135]\n",
      " [-0.68504432]\n",
      " ...\n",
      " [ 0.32553801]\n",
      " [ 0.13944278]\n",
      " [-0.77467541]]\n",
      "t [[ 0.13944278]\n",
      " [-1.20385135]\n",
      " [-0.68504432]\n",
      " ...\n",
      " [ 0.32553801]\n",
      " [ 0.13944278]\n",
      " [-0.77467541]]\n",
      "t [[ 0.14019493]\n",
      " [-1.29483567]\n",
      " [-0.7379248 ]\n",
      " ...\n",
      " [ 0.3295912 ]\n",
      " [ 0.14019493]\n",
      " [-0.82059369]]\n",
      "loss=38323.29628712096\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.11265889]\n",
      " [-0.06567562]\n",
      " [-0.10510778]\n",
      " ...\n",
      " [ 0.08827187]\n",
      " [ 0.03716055]\n",
      " [-0.14955875]]\n",
      "t [[ 0.11265889]\n",
      " [-0.06567562]\n",
      " [-0.10510778]\n",
      " ...\n",
      " [ 0.08827187]\n",
      " [ 0.03716055]\n",
      " [-0.14955875]]\n",
      "t [[ 0.21006067]\n",
      " [-0.14260402]\n",
      " [-0.20004229]\n",
      " ...\n",
      " [ 0.15430926]\n",
      " [ 0.06586495]\n",
      " [-0.27144173]]\n",
      "t [[ 0.21006067]\n",
      " [-0.14260402]\n",
      " [-0.20004229]\n",
      " ...\n",
      " [ 0.15430926]\n",
      " [ 0.06586495]\n",
      " [-0.27144173]]\n",
      "Current iteration=2, loss=47247.65877129226\n",
      "t [[ 0.29458866]\n",
      " [-0.22550726]\n",
      " [-0.28647828]\n",
      " ...\n",
      " [ 0.20367243]\n",
      " [ 0.08783348]\n",
      " [-0.37270032]]\n",
      "t [[ 0.29458866]\n",
      " [-0.22550726]\n",
      " [-0.28647828]\n",
      " ...\n",
      " [ 0.20367243]\n",
      " [ 0.08783348]\n",
      " [-0.37270032]]\n",
      "t [[ 0.36829661]\n",
      " [-0.31079178]\n",
      " [-0.36576589]\n",
      " ...\n",
      " [ 0.24047079]\n",
      " [ 0.10444208]\n",
      " [-0.4585272 ]]\n",
      "t [[ 0.36829661]\n",
      " [-0.31079178]\n",
      " [-0.36576589]\n",
      " ...\n",
      " [ 0.24047079]\n",
      " [ 0.10444208]\n",
      " [-0.4585272 ]]\n",
      "Current iteration=4, loss=43981.331442357885\n",
      "t [[ 0.43289356]\n",
      " [-0.39612024]\n",
      " [-0.43897929]\n",
      " ...\n",
      " [ 0.26772274]\n",
      " [ 0.11677125]\n",
      " [-0.53268077]]\n",
      "t [[ 0.43289356]\n",
      " [-0.39612024]\n",
      " [-0.43897929]\n",
      " ...\n",
      " [ 0.26772274]\n",
      " [ 0.11677125]\n",
      " [-0.53268077]]\n",
      "t [[ 0.48978143]\n",
      " [-0.48003131]\n",
      " [-0.5069742 ]\n",
      " ...\n",
      " [ 0.28766201]\n",
      " [ 0.12566662]\n",
      " [-0.59787622]]\n",
      "t [[ 0.48978143]\n",
      " [-0.48003131]\n",
      " [-0.5069742 ]\n",
      " ...\n",
      " [ 0.28766201]\n",
      " [ 0.12566662]\n",
      " [-0.59787622]]\n",
      "Current iteration=6, loss=41581.942956384984\n",
      "t [[ 0.54010931]\n",
      " [-0.56165116]\n",
      " [-0.57043744]\n",
      " ...\n",
      " [ 0.30195244]\n",
      " [ 0.13179237]\n",
      " [-0.65608307]]\n",
      "t [[ 0.54010931]\n",
      " [-0.56165116]\n",
      " [-0.57043744]\n",
      " ...\n",
      " [ 0.30195244]\n",
      " [ 0.13179237]\n",
      " [-0.65608307]]\n",
      "t [[ 0.58482365]\n",
      " [-0.64049151]\n",
      " [-0.62992587]\n",
      " ...\n",
      " [ 0.31184108]\n",
      " [ 0.13567355]\n",
      " [-0.70874064]]\n",
      "t [[ 0.58482365]\n",
      " [-0.64049151]\n",
      " [-0.62992587]\n",
      " ...\n",
      " [ 0.31184108]\n",
      " [ 0.13567355]\n",
      " [-0.70874064]]\n",
      "Current iteration=8, loss=39750.303456348134\n",
      "t [[ 0.62470895]\n",
      " [-0.71631359]\n",
      " [-0.68589613]\n",
      " ...\n",
      " [ 0.3182697 ]\n",
      " [ 0.13772845]\n",
      " [-0.7569114 ]]\n",
      "t [[ 0.62470895]\n",
      " [-0.71631359]\n",
      " [-0.68589613]\n",
      " ...\n",
      " [ 0.3182697 ]\n",
      " [ 0.13772845]\n",
      " [-0.7569114 ]]\n",
      "t [[ 0.66041971]\n",
      " [-0.78903787]\n",
      " [-0.73872698]\n",
      " ...\n",
      " [ 0.32195599]\n",
      " [ 0.13829296]\n",
      " [-0.80138896]]\n",
      "loss=38310.57407133321\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.11121494]\n",
      " [-0.06287044]\n",
      " [-0.10471414]\n",
      " ...\n",
      " [ 0.08963379]\n",
      " [ 0.03784545]\n",
      " [-0.15416442]]\n",
      "t [[ 0.11121494]\n",
      " [-0.06287044]\n",
      " [-0.10471414]\n",
      " ...\n",
      " [ 0.08963379]\n",
      " [ 0.03784545]\n",
      " [-0.15416442]]\n",
      "t [[ 0.20737517]\n",
      " [-0.13704476]\n",
      " [-0.19925551]\n",
      " ...\n",
      " [ 0.15642926]\n",
      " [ 0.06713329]\n",
      " [-0.27981912]]\n",
      "t [[ 0.20737517]\n",
      " [-0.13704476]\n",
      " [-0.19925551]\n",
      " ...\n",
      " [ 0.15642926]\n",
      " [ 0.06713329]\n",
      " [-0.27981912]]\n",
      "Current iteration=2, loss=47285.30192041946\n",
      "t [[ 0.29083719]\n",
      " [-0.21722166]\n",
      " [-0.2853096 ]\n",
      " ...\n",
      " [ 0.20611495]\n",
      " [ 0.0896074 ]\n",
      " [-0.38421146]]\n",
      "t [[ 0.29083719]\n",
      " [-0.21722166]\n",
      " [-0.2853096 ]\n",
      " ...\n",
      " [ 0.20611495]\n",
      " [ 0.0896074 ]\n",
      " [-0.38421146]]\n",
      "t [[ 0.3636296 ]\n",
      " [-0.29980279]\n",
      " [-0.36423247]\n",
      " ...\n",
      " [ 0.24292628]\n",
      " [ 0.10666136]\n",
      " [-0.4726825 ]]\n",
      "t [[ 0.3636296 ]\n",
      " [-0.29980279]\n",
      " [-0.36423247]\n",
      " ...\n",
      " [ 0.24292628]\n",
      " [ 0.10666136]\n",
      " [-0.4726825 ]]\n",
      "Current iteration=4, loss=44054.207434661526\n",
      "t [[ 0.42744125]\n",
      " [-0.382453  ]\n",
      " [-0.43710055]\n",
      " ...\n",
      " [ 0.26996393]\n",
      " [ 0.11938832]\n",
      " [-0.54909693]]\n",
      "t [[ 0.42744125]\n",
      " [-0.382453  ]\n",
      " [-0.43710055]\n",
      " ...\n",
      " [ 0.26996393]\n",
      " [ 0.11938832]\n",
      " [-0.54909693]]\n",
      "t [[ 0.48365697]\n",
      " [-0.46371598]\n",
      " [-0.50476964]\n",
      " ...\n",
      " [ 0.28951631]\n",
      " [ 0.128643  ]\n",
      " [-0.61624689]]\n",
      "t [[ 0.48365697]\n",
      " [-0.46371598]\n",
      " [-0.50476964]\n",
      " ...\n",
      " [ 0.28951631]\n",
      " [ 0.128643  ]\n",
      " [-0.61624689]]\n",
      "Current iteration=6, loss=41684.36757130396\n",
      "t [[ 0.53341047]\n",
      " [-0.54272401]\n",
      " [-0.56792559]\n",
      " ...\n",
      " [ 0.3032867 ]\n",
      " [ 0.13509615]\n",
      " [-0.6761594 ]]\n",
      "t [[ 0.53341047]\n",
      " [-0.54272401]\n",
      " [-0.56792559]\n",
      " ...\n",
      " [ 0.3032867 ]\n",
      " [ 0.13509615]\n",
      " [-0.6761594 ]]\n",
      "t [[ 0.57763428]\n",
      " [-0.61899473]\n",
      " [-0.6271238 ]\n",
      " ...\n",
      " [ 0.31255217]\n",
      " [ 0.13927776]\n",
      " [-0.7303178 ]]\n",
      "t [[ 0.57763428]\n",
      " [-0.61899473]\n",
      " [-0.6271238 ]\n",
      " ...\n",
      " [ 0.31255217]\n",
      " [ 0.13927776]\n",
      " [-0.7303178 ]]\n",
      "Current iteration=8, loss=39877.004416806376\n",
      "t [[ 0.61710056]\n",
      " [-0.6922944 ]\n",
      " [-0.68281931]\n",
      " ...\n",
      " [ 0.31827779]\n",
      " [ 0.14160992]\n",
      " [-0.77981878]]\n",
      "t [[ 0.61710056]\n",
      " [-0.6922944 ]\n",
      " [-0.68281931]\n",
      " ...\n",
      " [ 0.31827779]\n",
      " [ 0.14160992]\n",
      " [-0.77981878]]\n",
      "t [[ 0.65245309]\n",
      " [-0.76254742]\n",
      " [-0.7353893 ]\n",
      " ...\n",
      " [ 0.32119948]\n",
      " [ 0.14243149]\n",
      " [-0.82548291]]\n",
      "loss=38457.292910974786\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.11103692]\n",
      " [-0.06605167]\n",
      " [-0.10438523]\n",
      " ...\n",
      " [ 0.15992808]\n",
      " [-0.10438523]\n",
      " [ 0.1497953 ]]\n",
      "t [[ 0.11103692]\n",
      " [-0.06605167]\n",
      " [-0.10438523]\n",
      " ...\n",
      " [ 0.15992808]\n",
      " [-0.10438523]\n",
      " [ 0.1497953 ]]\n",
      "t [[ 0.20704392]\n",
      " [-0.14279183]\n",
      " [-0.19868592]\n",
      " ...\n",
      " [ 0.29552371]\n",
      " [-0.19868592]\n",
      " [ 0.27285979]]\n",
      "t [[ 0.20704392]\n",
      " [-0.14279183]\n",
      " [-0.19868592]\n",
      " ...\n",
      " [ 0.29552371]\n",
      " [-0.19868592]\n",
      " [ 0.27285979]]\n",
      "Current iteration=2, loss=47290.48873548857\n",
      "t [[ 0.29038262]\n",
      " [-0.22509458]\n",
      " [-0.2845577 ]\n",
      " ...\n",
      " [ 0.41123096]\n",
      " [-0.2845577 ]\n",
      " [ 0.37474371]]\n",
      "t [[ 0.29038262]\n",
      " [-0.22509458]\n",
      " [-0.2845577 ]\n",
      " ...\n",
      " [ 0.41123096]\n",
      " [-0.2845577 ]\n",
      " [ 0.37474371]]\n",
      "t [[ 0.36305893]\n",
      " [-0.30950441]\n",
      " [-0.36333721]\n",
      " ...\n",
      " [ 0.51065387]\n",
      " [-0.36333721]\n",
      " [ 0.45980776]]\n",
      "t [[ 0.36305893]\n",
      " [-0.30950441]\n",
      " [-0.36333721]\n",
      " ...\n",
      " [ 0.51065387]\n",
      " [-0.36333721]\n",
      " [ 0.45980776]]\n",
      "Current iteration=4, loss=44055.14918896515\n",
      "t [[ 0.42673797]\n",
      " [-0.39378986]\n",
      " [-0.43608832]\n",
      " ...\n",
      " [ 0.59666452]\n",
      " [-0.43608832]\n",
      " [ 0.53141998]]\n",
      "t [[ 0.42673797]\n",
      " [-0.39378986]\n",
      " [-0.43608832]\n",
      " ...\n",
      " [ 0.59666452]\n",
      " [-0.43608832]\n",
      " [ 0.53141998]]\n",
      "t [[ 0.48279191]\n",
      " [-0.47656172]\n",
      " [-0.50365846]\n",
      " ...\n",
      " [ 0.67154589]\n",
      " [-0.50365846]\n",
      " [ 0.59217758]]\n",
      "t [[ 0.48279191]\n",
      " [-0.47656172]\n",
      " [-0.50365846]\n",
      " ...\n",
      " [ 0.67154589]\n",
      " [-0.50365846]\n",
      " [ 0.59217758]]\n",
      "Current iteration=6, loss=41679.469186998205\n",
      "t [[ 0.53235144]\n",
      " [-0.55699312]\n",
      " [-0.56672773]\n",
      " ...\n",
      " [ 0.73712024]\n",
      " [-0.56672773]\n",
      " [ 0.64409427]]\n",
      "t [[ 0.53235144]\n",
      " [-0.55699312]\n",
      " [-0.56672773]\n",
      " ...\n",
      " [ 0.73712024]\n",
      " [-0.56672773]\n",
      " [ 0.64409427]]\n",
      "t [[ 0.57635127]\n",
      " [-0.63462603]\n",
      " [-0.62584771]\n",
      " ...\n",
      " [ 0.79485237]\n",
      " [-0.62584771]\n",
      " [ 0.68874467]]\n",
      "t [[ 0.57635127]\n",
      " [-0.63462603]\n",
      " [-0.62584771]\n",
      " ...\n",
      " [ 0.79485237]\n",
      " [-0.62584771]\n",
      " [ 0.68874467]]\n",
      "Current iteration=8, loss=39866.65606818814\n",
      "t [[ 0.61556778]\n",
      " [-0.70924111]\n",
      " [-0.68147087]\n",
      " ...\n",
      " [ 0.84592934]\n",
      " [-0.68147087]\n",
      " [ 0.72737214]]\n",
      "t [[ 0.61556778]\n",
      " [-0.70924111]\n",
      " [-0.68147087]\n",
      " ...\n",
      " [ 0.84592934]\n",
      " [-0.68147087]\n",
      " [ 0.72737214]]\n",
      "t [[ 0.65064937]\n",
      " [-0.78077127]\n",
      " [-0.73397269]\n",
      " ...\n",
      " [ 0.89132134]\n",
      " [-0.73397269]\n",
      " [ 0.76096829]]\n",
      "loss=38442.223191319456\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03917237]\n",
      " [-0.18757682]\n",
      " [-0.1099972 ]\n",
      " ...\n",
      " [ 0.09406221]\n",
      " [ 0.03917237]\n",
      " [-0.15944636]]\n",
      "t [[ 0.03917237]\n",
      " [-0.18757682]\n",
      " [-0.1099972 ]\n",
      " ...\n",
      " [ 0.09406221]\n",
      " [ 0.03917237]\n",
      " [-0.15944636]]\n",
      "t [[ 0.0690552 ]\n",
      " [-0.36044452]\n",
      " [-0.20889938]\n",
      " ...\n",
      " [ 0.16344165]\n",
      " [ 0.0690552 ]\n",
      " [-0.28837226]]\n",
      "t [[ 0.0690552 ]\n",
      " [-0.36044452]\n",
      " [-0.20889938]\n",
      " ...\n",
      " [ 0.16344165]\n",
      " [ 0.0690552 ]\n",
      " [-0.28837226]]\n",
      "Current iteration=2, loss=47057.07674279752\n",
      "t [[ 0.09163003]\n",
      " [-0.51955532]\n",
      " [-0.29861858]\n",
      " ...\n",
      " [ 0.21459505]\n",
      " [ 0.09163003]\n",
      " [-0.39488761]]\n",
      "t [[ 0.09163003]\n",
      " [-0.51955532]\n",
      " [-0.29861858]\n",
      " ...\n",
      " [ 0.21459505]\n",
      " [ 0.09163003]\n",
      " [-0.39488761]]\n",
      "t [[ 0.10845839]\n",
      " [-0.66602942]\n",
      " [-0.38067386]\n",
      " ...\n",
      " [ 0.25220727]\n",
      " [ 0.10845839]\n",
      " [-0.48485182]]\n",
      "t [[ 0.10845839]\n",
      " [-0.66602942]\n",
      " [-0.38067386]\n",
      " ...\n",
      " [ 0.25220727]\n",
      " [ 0.10845839]\n",
      " [-0.48485182]]\n",
      "Current iteration=4, loss=43709.44683233382\n",
      "t [[ 0.12074879]\n",
      " [-0.80102288]\n",
      " [-0.45625756]\n",
      " ...\n",
      " [ 0.27966082]\n",
      " [ 0.12074879]\n",
      " [-0.56242782]]\n",
      "t [[ 0.12074879]\n",
      " [-0.80102288]\n",
      " [-0.45625756]\n",
      " ...\n",
      " [ 0.27966082]\n",
      " [ 0.12074879]\n",
      " [-0.56242782]]\n",
      "t [[ 0.12943464]\n",
      " [-0.92565124]\n",
      " [-0.52630875]\n",
      " ...\n",
      " [ 0.29942083]\n",
      " [ 0.12943464]\n",
      " [-0.63057271]]\n",
      "t [[ 0.12943464]\n",
      " [-0.92565124]\n",
      " [-0.52630875]\n",
      " ...\n",
      " [ 0.29942083]\n",
      " [ 0.12943464]\n",
      " [-0.63057271]]\n",
      "Current iteration=6, loss=41277.49215462619\n",
      "t [[ 0.13524045]\n",
      " [-1.04095195]\n",
      " [-0.59157406]\n",
      " ...\n",
      " [ 0.31329779]\n",
      " [ 0.13524045]\n",
      " [-0.69139972]]\n",
      "t [[ 0.13524045]\n",
      " [-1.04095195]\n",
      " [-0.59157406]\n",
      " ...\n",
      " [ 0.31329779]\n",
      " [ 0.13524045]\n",
      " [-0.69139972]]\n",
      "t [[ 0.13873308]\n",
      " [-1.1478688 ]\n",
      " [-0.65265431]\n",
      " ...\n",
      " [ 0.32263137]\n",
      " [ 0.13873308]\n",
      " [-0.74643359]]\n",
      "t [[ 0.13873308]\n",
      " [-1.1478688 ]\n",
      " [-0.65265431]\n",
      " ...\n",
      " [ 0.32263137]\n",
      " [ 0.13873308]\n",
      " [-0.74643359]]\n",
      "Current iteration=8, loss=39437.01845723309\n",
      "t [[ 0.14035994]\n",
      " [-1.24724852]\n",
      " [-0.71003924]\n",
      " ...\n",
      " [ 0.32842199]\n",
      " [ 0.14035994]\n",
      " [-0.79678806]]\n",
      "t [[ 0.14035994]\n",
      " [-1.24724852]\n",
      " [-0.71003924]\n",
      " ...\n",
      " [ 0.32842199]\n",
      " [ 0.14035994]\n",
      " [-0.79678806]]\n",
      "t [[ 0.14047737]\n",
      " [-1.33984443]\n",
      " [-0.76413319]\n",
      " ...\n",
      " [ 0.33142465]\n",
      " [ 0.14047737]\n",
      " [-0.84328833]]\n",
      "loss=38000.416811932155\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.11829184]\n",
      " [-0.0689594 ]\n",
      " [-0.11036317]\n",
      " ...\n",
      " [ 0.09268546]\n",
      " [ 0.03901858]\n",
      " [-0.15703669]]\n",
      "t [[ 0.11829184]\n",
      " [-0.0689594 ]\n",
      " [-0.11036317]\n",
      " ...\n",
      " [ 0.09268546]\n",
      " [ 0.03901858]\n",
      " [-0.15703669]]\n",
      "t [[ 0.21976794]\n",
      " [-0.1503205 ]\n",
      " [-0.20951328]\n",
      " ...\n",
      " [ 0.16086421]\n",
      " [ 0.06871707]\n",
      " [-0.28357012]]\n",
      "t [[ 0.21976794]\n",
      " [-0.1503205 ]\n",
      " [-0.20951328]\n",
      " ...\n",
      " [ 0.16086421]\n",
      " [ 0.06871707]\n",
      " [-0.28357012]]\n",
      "Current iteration=2, loss=47047.31921562202\n",
      "t [[ 0.30719406]\n",
      " [-0.23797459]\n",
      " [-0.29939224]\n",
      " ...\n",
      " [ 0.21097497]\n",
      " [ 0.0910895 ]\n",
      " [-0.38776808]]\n",
      "t [[ 0.30719406]\n",
      " [-0.23797459]\n",
      " [-0.29939224]\n",
      " ...\n",
      " [ 0.21097497]\n",
      " [ 0.0910895 ]\n",
      " [-0.38776808]]\n",
      "t [[ 0.38292454]\n",
      " [-0.32786916]\n",
      " [-0.38154225]\n",
      " ...\n",
      " [ 0.24768445]\n",
      " [ 0.1077062 ]\n",
      " [-0.47552377]]\n",
      "t [[ 0.38292454]\n",
      " [-0.32786916]\n",
      " [-0.38154225]\n",
      " ...\n",
      " [ 0.24768445]\n",
      " [ 0.1077062 ]\n",
      " [-0.47552377]]\n",
      "Current iteration=4, loss=43697.05396112487\n",
      "t [[ 0.44889245]\n",
      " [-0.41744681]\n",
      " [-0.45717289]\n",
      " ...\n",
      " [ 0.27435937]\n",
      " [ 0.11978153]\n",
      " [-0.55101715]]\n",
      "t [[ 0.44889245]\n",
      " [-0.41744681]\n",
      " [-0.45717289]\n",
      " ...\n",
      " [ 0.27435937]\n",
      " [ 0.11978153]\n",
      " [-0.55101715]]\n",
      "t [[ 0.50666447]\n",
      " [-0.50516358]\n",
      " [-0.52723593]\n",
      " ...\n",
      " [ 0.29345002]\n",
      " [ 0.12825278]\n",
      " [-0.61721125]]\n",
      "t [[ 0.50666447]\n",
      " [-0.50516358]\n",
      " [-0.52723593]\n",
      " ...\n",
      " [ 0.29345002]\n",
      " [ 0.12825278]\n",
      " [-0.61721125]]\n",
      "Current iteration=6, loss=41264.55267051778\n",
      "t [[ 0.55751174]\n",
      " [-0.59013583]\n",
      " [-0.59248731]\n",
      " ...\n",
      " [ 0.30675289]\n",
      " [ 0.13384706]\n",
      " [-0.67621819]]\n",
      "t [[ 0.55751174]\n",
      " [-0.59013583]\n",
      " [-0.59248731]\n",
      " ...\n",
      " [ 0.30675289]\n",
      " [ 0.13384706]\n",
      " [-0.67621819]]\n",
      "t [[ 0.60247197]\n",
      " [-0.67190106]\n",
      " [-0.6535347 ]\n",
      " ...\n",
      " [ 0.31559498]\n",
      " [ 0.1371329 ]\n",
      " [-0.72955765]]\n",
      "t [[ 0.60247197]\n",
      " [-0.67190106]\n",
      " [-0.6535347 ]\n",
      " ...\n",
      " [ 0.31559498]\n",
      " [ 0.1371329 ]\n",
      " [-0.72955765]]\n",
      "Current iteration=8, loss=39424.14544790331\n",
      "t [[ 0.64239856]\n",
      " [-0.75026131]\n",
      " [-0.71087294]\n",
      " ...\n",
      " [ 0.32096552]\n",
      " [ 0.13855884]\n",
      " [-0.77833623]]\n",
      "t [[ 0.64239856]\n",
      " [-0.75026131]\n",
      " [-0.71087294]\n",
      " ...\n",
      " [ 0.32096552]\n",
      " [ 0.13855884]\n",
      " [-0.77833623]]\n",
      "t [[ 0.67799837]\n",
      " [-0.82518212]\n",
      " [-0.76491013]\n",
      " ...\n",
      " [ 0.32360988]\n",
      " [ 0.13848192]\n",
      " [-0.82337102]]\n",
      "loss=37987.746014695265\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.11677569]\n",
      " [-0.06601396]\n",
      " [-0.10994985]\n",
      " ...\n",
      " [ 0.09411548]\n",
      " [ 0.03973773]\n",
      " [-0.16187264]]\n",
      "t [[ 0.11677569]\n",
      " [-0.06601396]\n",
      " [-0.10994985]\n",
      " ...\n",
      " [ 0.09411548]\n",
      " [ 0.03973773]\n",
      " [-0.16187264]]\n",
      "t [[ 0.21695868]\n",
      " [-0.14448595]\n",
      " [-0.20868724]\n",
      " ...\n",
      " [ 0.16305921]\n",
      " [ 0.07004358]\n",
      " [-0.29232324]]\n",
      "t [[ 0.21695868]\n",
      " [-0.14448595]\n",
      " [-0.20868724]\n",
      " ...\n",
      " [ 0.16305921]\n",
      " [ 0.07004358]\n",
      " [-0.29232324]]\n",
      "Current iteration=2, loss=47086.85841406272\n",
      "t [[ 0.30328352]\n",
      " [-0.22928036]\n",
      " [-0.2981665 ]\n",
      " ...\n",
      " [ 0.21346509]\n",
      " [ 0.09293895]\n",
      " [-0.39974679]]\n",
      "t [[ 0.30328352]\n",
      " [-0.22928036]\n",
      " [-0.2981665 ]\n",
      " ...\n",
      " [ 0.21346509]\n",
      " [ 0.09293895]\n",
      " [-0.39974679]]\n",
      "t [[ 0.37807583]\n",
      " [-0.31633995]\n",
      " [-0.37993632]\n",
      " ...\n",
      " [ 0.25014161]\n",
      " [ 0.11001412]\n",
      " [-0.49020457]]\n",
      "t [[ 0.37807583]\n",
      " [-0.31633995]\n",
      " [-0.37993632]\n",
      " ...\n",
      " [ 0.25014161]\n",
      " [ 0.11001412]\n",
      " [-0.49020457]]\n",
      "Current iteration=4, loss=43773.223544934706\n",
      "t [[ 0.44324588]\n",
      " [-0.40311056]\n",
      " [-0.45520844]\n",
      " ...\n",
      " [ 0.27654542]\n",
      " [ 0.12249751]\n",
      " [-0.56799442]]\n",
      "t [[ 0.44324588]\n",
      " [-0.40311056]\n",
      " [-0.45520844]\n",
      " ...\n",
      " [ 0.27654542]\n",
      " [ 0.12249751]\n",
      " [-0.56799442]]\n",
      "t [[ 0.50034099]\n",
      " [-0.48805456]\n",
      " [-0.52493438]\n",
      " ...\n",
      " [ 0.29518598]\n",
      " [ 0.13133626]\n",
      " [-0.63616351]]\n",
      "t [[ 0.50034099]\n",
      " [-0.48805456]\n",
      " [-0.52493438]\n",
      " ...\n",
      " [ 0.29518598]\n",
      " [ 0.13133626]\n",
      " [-0.63616351]]\n",
      "Current iteration=6, loss=41371.04155600885\n",
      "t [[ 0.55061479]\n",
      " [-0.57029559]\n",
      " [-0.58986874]\n",
      " ...\n",
      " [ 0.30790239]\n",
      " [ 0.13726457]\n",
      " [-0.69688626]]\n",
      "t [[ 0.55061479]\n",
      " [-0.57029559]\n",
      " [-0.58986874]\n",
      " ...\n",
      " [ 0.30790239]\n",
      " [ 0.13726457]\n",
      " [-0.69688626]]\n",
      "t [[ 0.59508939]\n",
      " [-0.64937793]\n",
      " [-0.65061742]\n",
      " ...\n",
      " [ 0.31605381]\n",
      " [ 0.14085624]\n",
      " [-0.75172954]]\n",
      "t [[ 0.59508939]\n",
      " [-0.64937793]\n",
      " [-0.65061742]\n",
      " ...\n",
      " [ 0.31605381]\n",
      " [ 0.14085624]\n",
      " [-0.75172954]]\n",
      "Current iteration=8, loss=39555.2830234413\n",
      "t [[ 0.63460451]\n",
      " [-0.72510912]\n",
      " [-0.70767336]\n",
      " ...\n",
      " [ 0.3206542 ]\n",
      " [ 0.14256383]\n",
      " [-0.80183631]]\n",
      "t [[ 0.63460451]\n",
      " [-0.72510912]\n",
      " [-0.70767336]\n",
      " ...\n",
      " [ 0.3206542 ]\n",
      " [ 0.14256383]\n",
      " [-0.80183631]]\n",
      "t [[ 0.66985532]\n",
      " [-0.79745875]\n",
      " [-0.76144287]\n",
      " ...\n",
      " [ 0.3224681 ]\n",
      " [ 0.14274753]\n",
      " [-0.84805212]]\n",
      "loss=38139.044163837534\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.11658876]\n",
      " [-0.06935425]\n",
      " [-0.10960449]\n",
      " ...\n",
      " [ 0.16792448]\n",
      " [-0.10960449]\n",
      " [ 0.15728507]]\n",
      "t [[ 0.11658876]\n",
      " [-0.06935425]\n",
      " [-0.10960449]\n",
      " ...\n",
      " [ 0.16792448]\n",
      " [-0.10960449]\n",
      " [ 0.15728507]]\n",
      "t [[ 0.21661181]\n",
      " [-0.15048863]\n",
      " [-0.20809372]\n",
      " ...\n",
      " [ 0.30903021]\n",
      " [-0.20809372]\n",
      " [ 0.28510825]]\n",
      "t [[ 0.21661181]\n",
      " [-0.15048863]\n",
      " [-0.20809372]\n",
      " ...\n",
      " [ 0.30903021]\n",
      " [-0.20809372]\n",
      " [ 0.28510825]]\n",
      "Current iteration=2, loss=47092.0102227916\n",
      "t [[ 0.30280854]\n",
      " [-0.23747161]\n",
      " [-0.29738748]\n",
      " ...\n",
      " [ 0.42846959]\n",
      " [-0.29738748]\n",
      " [ 0.38990232]]\n",
      "t [[ 0.30280854]\n",
      " [-0.23747161]\n",
      " [-0.29738748]\n",
      " ...\n",
      " [ 0.42846959]\n",
      " [-0.29738748]\n",
      " [ 0.38990232]]\n",
      "t [[ 0.3774773 ]\n",
      " [-0.32640865]\n",
      " [-0.37901263]\n",
      " ...\n",
      " [ 0.53035999]\n",
      " [-0.37901263]\n",
      " [ 0.47663831]]\n",
      "t [[ 0.3774773 ]\n",
      " [-0.32640865]\n",
      " [-0.37901263]\n",
      " ...\n",
      " [ 0.53035999]\n",
      " [-0.37901263]\n",
      " [ 0.47663831]]\n",
      "Current iteration=4, loss=43773.6231925994\n",
      "t [[ 0.44250233]\n",
      " [-0.41485968]\n",
      " [-0.45416718]\n",
      " ...\n",
      " [ 0.61793558]\n",
      " [-0.45416718]\n",
      " [ 0.54909386]]\n",
      "t [[ 0.44250233]\n",
      " [-0.41485968]\n",
      " [-0.45416718]\n",
      " ...\n",
      " [ 0.61793558]\n",
      " [-0.45416718]\n",
      " [ 0.54909386]]\n",
      "t [[ 0.49941876]\n",
      " [-0.50135806]\n",
      " [-0.52379362]\n",
      " ...\n",
      " [ 0.69373415]\n",
      " [-0.52379362]\n",
      " [ 0.61013979]]\n",
      "t [[ 0.49941876]\n",
      " [-0.50135806]\n",
      " [-0.52379362]\n",
      " ...\n",
      " [ 0.69373415]\n",
      " [-0.52379362]\n",
      " [ 0.61013979]]\n",
      "Current iteration=6, loss=41365.2856204428\n",
      "t [[ 0.54947867]\n",
      " [-0.58506907]\n",
      " [-0.58864054]\n",
      " ...\n",
      " [ 0.75975854]\n",
      " [-0.58864054]\n",
      " [ 0.66197364]]\n",
      "t [[ 0.54947867]\n",
      " [-0.58506907]\n",
      " [-0.58864054]\n",
      " ...\n",
      " [ 0.75975854]\n",
      " [-0.58864054]\n",
      " [ 0.66197364]]\n",
      "t [[ 0.59370765]\n",
      " [-0.66556093]\n",
      " [-0.64930991]\n",
      " ...\n",
      " [ 0.81760289]\n",
      " [-0.64930991]\n",
      " [ 0.70629483]]\n",
      "t [[ 0.59370765]\n",
      " [-0.66556093]\n",
      " [-0.64930991]\n",
      " ...\n",
      " [ 0.81760289]\n",
      " [-0.64930991]\n",
      " [ 0.70629483]]\n",
      "Current iteration=8, loss=39543.918975447676\n",
      "t [[ 0.63295066]\n",
      " [-0.7426549 ]\n",
      " [-0.70629214]\n",
      " ...\n",
      " [ 0.86854801]\n",
      " [-0.70629214]\n",
      " [ 0.7444323 ]]\n",
      "t [[ 0.63295066]\n",
      " [-0.7426549 ]\n",
      " [-0.70629214]\n",
      " ...\n",
      " [ 0.86854801]\n",
      " [-0.70629214]\n",
      " [ 0.7444323 ]]\n",
      "t [[ 0.66790815]\n",
      " [-0.81632837]\n",
      " [-0.7599919 ]\n",
      " ...\n",
      " [ 0.91363279]\n",
      " [-0.7599919 ]\n",
      " [ 0.77743708]]\n",
      "loss=38122.886194740444\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04103772]\n",
      " [-0.19650905]\n",
      " [-0.11523516]\n",
      " ...\n",
      " [ 0.09854136]\n",
      " [ 0.04103772]\n",
      " [-0.16703905]]\n",
      "t [[ 0.04103772]\n",
      " [-0.19650905]\n",
      " [-0.11523516]\n",
      " ...\n",
      " [ 0.09854136]\n",
      " [ 0.04103772]\n",
      " [-0.16703905]]\n",
      "t [[ 0.07188333]\n",
      " [-0.376878  ]\n",
      " [-0.21829682]\n",
      " ...\n",
      " [ 0.17000118]\n",
      " [ 0.07188333]\n",
      " [-0.30059233]]\n",
      "t [[ 0.07188333]\n",
      " [-0.376878  ]\n",
      " [-0.21829682]\n",
      " ...\n",
      " [ 0.17000118]\n",
      " [ 0.07188333]\n",
      " [-0.30059233]]\n",
      "Current iteration=2, loss=46859.89751774186\n",
      "t [[ 0.09481783]\n",
      " [-0.54220816]\n",
      " [-0.31138673]\n",
      " ...\n",
      " [ 0.22180483]\n",
      " [ 0.09481783]\n",
      " [-0.40998904]]\n",
      "t [[ 0.09481783]\n",
      " [-0.54220816]\n",
      " [-0.31138673]\n",
      " ...\n",
      " [ 0.22180483]\n",
      " [ 0.09481783]\n",
      " [-0.40998904]]\n",
      "t [[ 0.11161054]\n",
      " [-0.69379703]\n",
      " [-0.39622752]\n",
      " ...\n",
      " [ 0.25923422]\n",
      " [ 0.11161054]\n",
      " [-0.50183088]]\n",
      "t [[ 0.11161054]\n",
      " [-0.69379703]\n",
      " [-0.39622752]\n",
      " ...\n",
      " [ 0.25923422]\n",
      " [ 0.11161054]\n",
      " [-0.50183088]]\n",
      "Current iteration=4, loss=43433.218976115895\n",
      "t [[ 0.12360987]\n",
      " [-0.83297475]\n",
      " [-0.47415224]\n",
      " ...\n",
      " [ 0.2860348 ]\n",
      " [ 0.12360987]\n",
      " [-0.58071357]]\n",
      "t [[ 0.12360987]\n",
      " [-0.83297475]\n",
      " [-0.47415224]\n",
      " ...\n",
      " [ 0.2860348 ]\n",
      " [ 0.12360987]\n",
      " [-0.58071357]]\n",
      "t [[ 0.13184298]\n",
      " [-0.96101365]\n",
      " [-0.54619807]\n",
      " ...\n",
      " [ 0.30489117]\n",
      " [ 0.13184298]\n",
      " [-0.64984073]]\n",
      "t [[ 0.13184298]\n",
      " [-0.96101365]\n",
      " [-0.54619807]\n",
      " ...\n",
      " [ 0.30489117]\n",
      " [ 0.13184298]\n",
      " [-0.64984073]]\n",
      "Current iteration=6, loss=40972.069531597655\n",
      "t [[ 0.13709757]\n",
      " [-1.07908628]\n",
      " [-0.61318139]\n",
      " ...\n",
      " [ 0.31774433]\n",
      " [ 0.13709757]\n",
      " [-0.71146287]]\n",
      "t [[ 0.13709757]\n",
      " [-1.07908628]\n",
      " [-0.61318139]\n",
      " ...\n",
      " [ 0.31774433]\n",
      " [ 0.13709757]\n",
      " [-0.71146287]]\n",
      "t [[ 0.13998338]\n",
      " [-1.18824962]\n",
      " [-0.6757538 ]\n",
      " ...\n",
      " [ 0.32601027]\n",
      " [ 0.13998338]\n",
      " [-0.76717955]]\n",
      "t [[ 0.13998338]\n",
      " [-1.18824962]\n",
      " [-0.6757538 ]\n",
      " ...\n",
      " [ 0.32601027]\n",
      " [ 0.13998338]\n",
      " [-0.76717955]]\n",
      "Current iteration=8, loss=39125.57095456665\n",
      "t [[ 0.14097714]\n",
      " [-1.28944382]\n",
      " [-0.73444304]\n",
      " ...\n",
      " [ 0.33073325]\n",
      " [ 0.14097714]\n",
      " [-0.8181443 ]]\n",
      "t [[ 0.14097714]\n",
      " [-1.28944382]\n",
      " [-0.73444304]\n",
      " ...\n",
      " [ 0.33073325]\n",
      " [ 0.14097714]\n",
      " [-0.8181443 ]]\n",
      "t [[ 0.14045533]\n",
      " [-1.3834985 ]\n",
      " [-0.78968254]\n",
      " ...\n",
      " [ 0.33269298]\n",
      " [ 0.14045533]\n",
      " [-0.86520279]]\n",
      "loss=37694.082348914926\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.12392478]\n",
      " [-0.07224318]\n",
      " [-0.11561856]\n",
      " ...\n",
      " [ 0.09709906]\n",
      " [ 0.0408766 ]\n",
      " [-0.16451463]]\n",
      "t [[ 0.12392478]\n",
      " [-0.07224318]\n",
      " [-0.11561856]\n",
      " ...\n",
      " [ 0.09709906]\n",
      " [ 0.0408766 ]\n",
      " [-0.16451463]]\n",
      "t [[ 0.22940021]\n",
      " [-0.15809212]\n",
      " [-0.21893413]\n",
      " ...\n",
      " [ 0.16730968]\n",
      " [ 0.0715276 ]\n",
      " [-0.29556242]]\n",
      "t [[ 0.22940021]\n",
      " [-0.15809212]\n",
      " [-0.21893413]\n",
      " ...\n",
      " [ 0.16730968]\n",
      " [ 0.0715276 ]\n",
      " [-0.29556242]]\n",
      "Current iteration=2, loss=46849.876354383785\n",
      "t [[ 0.31961351]\n",
      " [-0.25052532]\n",
      " [-0.31218284]\n",
      " ...\n",
      " [ 0.21803649]\n",
      " [ 0.09424834]\n",
      " [-0.40253904]]\n",
      "t [[ 0.31961351]\n",
      " [-0.25052532]\n",
      " [-0.31218284]\n",
      " ...\n",
      " [ 0.21803649]\n",
      " [ 0.09424834]\n",
      " [-0.40253904]]\n",
      "t [[ 0.39724392]\n",
      " [-0.34500734]\n",
      " [-0.39711348]\n",
      " ...\n",
      " [ 0.25454072]\n",
      " [ 0.11081799]\n",
      " [-0.49208354]]\n",
      "t [[ 0.39724392]\n",
      " [-0.34500734]\n",
      " [-0.39711348]\n",
      " ...\n",
      " [ 0.25454072]\n",
      " [ 0.11081799]\n",
      " [-0.49208354]]\n",
      "Current iteration=4, loss=43420.69739135708\n",
      "t [[ 0.4644618 ]\n",
      " [-0.43876395]\n",
      " [-0.47507825]\n",
      " ...\n",
      " [ 0.28054985]\n",
      " [ 0.12259138]\n",
      " [-0.56880943]]\n",
      "t [[ 0.4644618 ]\n",
      " [-0.43876395]\n",
      " [-0.47507825]\n",
      " ...\n",
      " [ 0.28054985]\n",
      " [ 0.12259138]\n",
      " [-0.56880943]]\n",
      "t [[ 0.52300544]\n",
      " [-0.53018059]\n",
      " [-0.54712814]\n",
      " ...\n",
      " [ 0.2987315 ]\n",
      " [ 0.13059983]\n",
      " [-0.6359253 ]]\n",
      "t [[ 0.52300544]\n",
      " [-0.53018059]\n",
      " [-0.54712814]\n",
      " ...\n",
      " [ 0.2987315 ]\n",
      " [ 0.13059983]\n",
      " [-0.6359253 ]]\n",
      "Current iteration=6, loss=40959.11042335733\n",
      "t [[ 0.57427125]\n",
      " [-0.61837663]\n",
      " [-0.61408951]\n",
      " ...\n",
      " [ 0.31101083]\n",
      " [ 0.13563373]\n",
      " [-0.69567907]]\n",
      "t [[ 0.57427125]\n",
      " [-0.61837663]\n",
      " [-0.61408951]\n",
      " ...\n",
      " [ 0.31101083]\n",
      " [ 0.13563373]\n",
      " [-0.69567907]]\n",
      "t [[ 0.61938948]\n",
      " [-0.70292598]\n",
      " [-0.67662126]\n",
      " ...\n",
      " [ 0.31878966]\n",
      " [ 0.13830456]\n",
      " [-0.74966352]]\n",
      "t [[ 0.61938948]\n",
      " [-0.70292598]\n",
      " [-0.67662126]\n",
      " ...\n",
      " [ 0.31878966]\n",
      " [ 0.13830456]\n",
      " [-0.74966352]]\n",
      "Current iteration=8, loss=39112.73334000039\n",
      "t [[ 0.65928247]\n",
      " [-0.78367946]\n",
      " [-0.73525637]\n",
      " ...\n",
      " [ 0.32309997]\n",
      " [ 0.13909016]\n",
      " [-0.79902338]]\n",
      "t [[ 0.65928247]\n",
      " [-0.78367946]\n",
      " [-0.73525637]\n",
      " ...\n",
      " [ 0.32309997]\n",
      " [ 0.13909016]\n",
      " [-0.79902338]]\n",
      "t [[ 0.6947089 ]\n",
      " [-0.86065311]\n",
      " [-0.79043217]\n",
      " ...\n",
      " [ 0.32471095]\n",
      " [ 0.13836767]\n",
      " [-0.84459476]]\n",
      "loss=37681.462487324185\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.12233644]\n",
      " [-0.06915748]\n",
      " [-0.11518555]\n",
      " ...\n",
      " [ 0.09859717]\n",
      " [ 0.04163   ]\n",
      " [-0.16958086]]\n",
      "t [[ 0.12233644]\n",
      " [-0.06915748]\n",
      " [-0.11518555]\n",
      " ...\n",
      " [ 0.09859717]\n",
      " [ 0.04163   ]\n",
      " [-0.16958086]]\n",
      "t [[ 0.22646818]\n",
      " [-0.15198255]\n",
      " [-0.21806884]\n",
      " ...\n",
      " [ 0.16957683]\n",
      " [ 0.07291179]\n",
      " [-0.30468726]]\n",
      "t [[ 0.22646818]\n",
      " [-0.15198255]\n",
      " [-0.21806884]\n",
      " ...\n",
      " [ 0.16957683]\n",
      " [ 0.07291179]\n",
      " [-0.30468726]]\n",
      "Current iteration=2, loss=46891.308219187005\n",
      "t [[ 0.31554641]\n",
      " [-0.24142289]\n",
      " [-0.31090025]\n",
      " ...\n",
      " [ 0.22056829]\n",
      " [ 0.09617222]\n",
      " [-0.41497638]]\n",
      "t [[ 0.31554641]\n",
      " [-0.24142289]\n",
      " [-0.31090025]\n",
      " ...\n",
      " [ 0.22056829]\n",
      " [ 0.09617222]\n",
      " [-0.41497638]]\n",
      "t [[ 0.39221782]\n",
      " [-0.33293858]\n",
      " [-0.3954356 ]\n",
      " ...\n",
      " [ 0.25699104]\n",
      " [ 0.11321289]\n",
      " [-0.50727625]]\n",
      "t [[ 0.39221782]\n",
      " [-0.33293858]\n",
      " [-0.3954356 ]\n",
      " ...\n",
      " [ 0.25699104]\n",
      " [ 0.11321289]\n",
      " [-0.50727625]]\n",
      "Current iteration=4, loss=43500.11014510344\n",
      "t [[ 0.45862718]\n",
      " [-0.42375986]\n",
      " [-0.47302904]\n",
      " ...\n",
      " [ 0.28267015]\n",
      " [ 0.12540405]\n",
      " [-0.58633009]]\n",
      "t [[ 0.45862718]\n",
      " [-0.42375986]\n",
      " [-0.47302904]\n",
      " ...\n",
      " [ 0.28267015]\n",
      " [ 0.12540405]\n",
      " [-0.58633009]]\n",
      "t [[ 0.51649088]\n",
      " [-0.51228002]\n",
      " [-0.54473098]\n",
      " ...\n",
      " [ 0.30033705]\n",
      " [ 0.13378771]\n",
      " [-0.655438  ]]\n",
      "t [[ 0.51649088]\n",
      " [-0.51228002]\n",
      " [-0.54473098]\n",
      " ...\n",
      " [ 0.30033705]\n",
      " [ 0.13378771]\n",
      " [-0.655438  ]]\n",
      "Current iteration=6, loss=41069.55398487997\n",
      "t [[ 0.56718566]\n",
      " [-0.59762697]\n",
      " [-0.61136603]\n",
      " ...\n",
      " [ 0.31196262]\n",
      " [ 0.13916182]\n",
      " [-0.71691489]]\n",
      "t [[ 0.56718566]\n",
      " [-0.59762697]\n",
      " [-0.61136603]\n",
      " ...\n",
      " [ 0.31196262]\n",
      " [ 0.13916182]\n",
      " [-0.71691489]]\n",
      "t [[ 0.61182436]\n",
      " [-0.67938219]\n",
      " [-0.67359095]\n",
      " ...\n",
      " [ 0.31898297]\n",
      " [ 0.14214346]\n",
      " [-0.77240387]]\n",
      "t [[ 0.61182436]\n",
      " [-0.67938219]\n",
      " [-0.67359095]\n",
      " ...\n",
      " [ 0.31898297]\n",
      " [ 0.14214346]\n",
      " [-0.77240387]]\n",
      "Current iteration=8, loss=39248.14873938344\n",
      "t [[ 0.65131433]\n",
      " [-0.75740241]\n",
      " [-0.73193656]\n",
      " ...\n",
      " [ 0.32245618]\n",
      " [ 0.14321471]\n",
      " [-0.82308809]]\n",
      "t [[ 0.65131433]\n",
      " [-0.75740241]\n",
      " [-0.73193656]\n",
      " ...\n",
      " [ 0.32245618]\n",
      " [ 0.14321471]\n",
      " [-0.82308809]]\n",
      "t [[ 0.6864016 ]\n",
      " [-0.83170781]\n",
      " [-0.78683814]\n",
      " ...\n",
      " [ 0.32317142]\n",
      " [ 0.14275605]\n",
      " [-0.86983349]]\n",
      "loss=37837.14484012677\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.12214061]\n",
      " [-0.07265684]\n",
      " [-0.11482375]\n",
      " ...\n",
      " [ 0.17592089]\n",
      " [-0.11482375]\n",
      " [ 0.16477483]]\n",
      "t [[ 0.12214061]\n",
      " [-0.07265684]\n",
      " [-0.11482375]\n",
      " ...\n",
      " [ 0.17592089]\n",
      " [-0.11482375]\n",
      " [ 0.16477483]]\n",
      "t [[ 0.22610574]\n",
      " [-0.1582379 ]\n",
      " [-0.21745183]\n",
      " ...\n",
      " [ 0.32241697]\n",
      " [-0.21745183]\n",
      " [ 0.29722523]]\n",
      "t [[ 0.22610574]\n",
      " [-0.1582379 ]\n",
      " [-0.21745183]\n",
      " ...\n",
      " [ 0.32241697]\n",
      " [-0.21745183]\n",
      " [ 0.29722523]]\n",
      "Current iteration=2, loss=46896.404713362776\n",
      "t [[ 0.31505094]\n",
      " [-0.24992666]\n",
      " [-0.31009492]\n",
      " ...\n",
      " [ 0.44542055]\n",
      " [-0.31009492]\n",
      " [ 0.40475529]]\n",
      "t [[ 0.31505094]\n",
      " [-0.24992666]\n",
      " [-0.31009492]\n",
      " ...\n",
      " [ 0.44542055]\n",
      " [-0.31009492]\n",
      " [ 0.40475529]]\n",
      "t [[ 0.39159066]\n",
      " [-0.34336696]\n",
      " [-0.39448455]\n",
      " ...\n",
      " [ 0.54960139]\n",
      " [-0.39448455]\n",
      " [ 0.49299059]]\n",
      "t [[ 0.39159066]\n",
      " [-0.34336696]\n",
      " [-0.39448455]\n",
      " ...\n",
      " [ 0.54960139]\n",
      " [-0.39448455]\n",
      " [ 0.49299059]]\n",
      "Current iteration=4, loss=43499.95291992988\n",
      "t [[ 0.45784157]\n",
      " [-0.43591338]\n",
      " [-0.47195993]\n",
      " ...\n",
      " [ 0.63857481]\n",
      " [-0.47195993]\n",
      " [ 0.56613693]]\n",
      "t [[ 0.45784157]\n",
      " [-0.43591338]\n",
      " [-0.47195993]\n",
      " ...\n",
      " [ 0.63857481]\n",
      " [-0.47195993]\n",
      " [ 0.56613693]]\n",
      "t [[ 0.51550879]\n",
      " [-0.52603354]\n",
      " [-0.54356181]\n",
      " ...\n",
      " [ 0.71514155]\n",
      " [-0.54356181]\n",
      " [ 0.62734451]]\n",
      "t [[ 0.51550879]\n",
      " [-0.52603354]\n",
      " [-0.54356181]\n",
      " ...\n",
      " [ 0.71514155]\n",
      " [-0.54356181]\n",
      " [ 0.62734451]]\n",
      "Current iteration=6, loss=41062.94841761875\n",
      "t [[ 0.56596906]\n",
      " [-0.61289734]\n",
      " [-0.61010859]\n",
      " ...\n",
      " [ 0.78148762]\n",
      " [-0.61010859]\n",
      " [ 0.67899464]]\n",
      "t [[ 0.56596906]\n",
      " [-0.61289734]\n",
      " [-0.61010859]\n",
      " ...\n",
      " [ 0.78148762]\n",
      " [-0.61010859]\n",
      " [ 0.67899464]]\n",
      "t [[ 0.61034015]\n",
      " [-0.6961096 ]\n",
      " [-0.67225301]\n",
      " ...\n",
      " [ 0.83933637]\n",
      " [-0.67225301]\n",
      " [ 0.7229091 ]]\n",
      "t [[ 0.61034015]\n",
      " [-0.6961096 ]\n",
      " [-0.67225301]\n",
      " ...\n",
      " [ 0.83933637]\n",
      " [-0.67225301]\n",
      " [ 0.7229091 ]]\n",
      "Current iteration=8, loss=39235.79535139694\n",
      "t [[ 0.64953558]\n",
      " [-0.77553991]\n",
      " [-0.73052343]\n",
      " ...\n",
      " [ 0.89006098]\n",
      " [-0.73052343]\n",
      " [ 0.76049934]]\n",
      "t [[ 0.64953558]\n",
      " [-0.77553991]\n",
      " [-0.73052343]\n",
      " ...\n",
      " [ 0.89006098]\n",
      " [-0.73052343]\n",
      " [ 0.76049934]]\n",
      "t [[ 0.68430727]\n",
      " [-0.85121548]\n",
      " [-0.7853536 ]\n",
      " ...\n",
      " [ 0.93476725]\n",
      " [-0.7853536 ]\n",
      " [ 0.79287269]]\n",
      "loss=37819.93805671831\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04290307]\n",
      " [-0.20544127]\n",
      " [-0.12047312]\n",
      " ...\n",
      " [ 0.10302051]\n",
      " [ 0.04290307]\n",
      " [-0.17463173]]\n",
      "t [[ 0.04290307]\n",
      " [-0.20544127]\n",
      " [-0.12047312]\n",
      " ...\n",
      " [ 0.10302051]\n",
      " [ 0.04290307]\n",
      " [-0.17463173]]\n",
      "t [[ 0.07467009]\n",
      " [-0.39324553]\n",
      " [-0.22764474]\n",
      " ...\n",
      " [ 0.17645064]\n",
      " [ 0.07467009]\n",
      " [-0.31267648]]\n",
      "t [[ 0.07467009]\n",
      " [-0.39324553]\n",
      " [-0.22764474]\n",
      " ...\n",
      " [ 0.17645064]\n",
      " [ 0.07467009]\n",
      " [-0.31267648]]\n",
      "Current iteration=2, loss=46665.553224595875\n",
      "t [[ 0.09791019]\n",
      " [-0.56467838]\n",
      " [-0.32403417]\n",
      " ...\n",
      " [ 0.22877606]\n",
      " [ 0.09791019]\n",
      " [-0.42479846]]\n",
      "t [[ 0.09791019]\n",
      " [-0.56467838]\n",
      " [-0.32403417]\n",
      " ...\n",
      " [ 0.22877606]\n",
      " [ 0.09791019]\n",
      " [-0.42479846]]\n",
      "t [[ 0.11461484]\n",
      " [-0.7212326 ]\n",
      " [-0.411582  ]\n",
      " ...\n",
      " [ 0.2659123 ]\n",
      " [ 0.11461484]\n",
      " [-0.5183855 ]]\n",
      "t [[ 0.11461484]\n",
      " [-0.7212326 ]\n",
      " [-0.411582  ]\n",
      " ...\n",
      " [ 0.2659123 ]\n",
      " [ 0.11461484]\n",
      " [-0.5183855 ]]\n",
      "Current iteration=4, loss=43164.59694575589\n",
      "t [[ 0.12627842]\n",
      " [-0.86442783]\n",
      " [-0.49176865]\n",
      " ...\n",
      " [ 0.2919786 ]\n",
      " [ 0.12627842]\n",
      " [-0.59847575]]\n",
      "t [[ 0.12627842]\n",
      " [-0.86442783]\n",
      " [-0.49176865]\n",
      " ...\n",
      " [ 0.2919786 ]\n",
      " [ 0.12627842]\n",
      " [-0.59847575]]\n",
      "t [[ 0.13402383]\n",
      " [-0.99570484]\n",
      " [-0.56573223]\n",
      " ...\n",
      " [ 0.30987835]\n",
      " [ 0.13402383]\n",
      " [-0.66851537]]\n",
      "t [[ 0.13402383]\n",
      " [-0.99570484]\n",
      " [-0.56573223]\n",
      " ...\n",
      " [ 0.30987835]\n",
      " [ 0.13402383]\n",
      " [-0.66851537]]\n",
      "Current iteration=6, loss=40677.96829257425\n",
      "t [[ 0.13870197]\n",
      " [-1.11637917]\n",
      " [-0.6343599 ]\n",
      " ...\n",
      " [ 0.32167825]\n",
      " [ 0.13870197]\n",
      " [-0.73088458]]\n",
      "t [[ 0.13870197]\n",
      " [-1.11637917]\n",
      " [-0.6343599 ]\n",
      " ...\n",
      " [ 0.32167825]\n",
      " [ 0.13870197]\n",
      " [-0.73088458]]\n",
      "t [[ 0.14096468]\n",
      " [-1.22762606]\n",
      " [-0.69835433]\n",
      " ...\n",
      " [ 0.32886509]\n",
      " [ 0.14096468]\n",
      " [-0.78725041]]\n",
      "t [[ 0.14096468]\n",
      " [-1.22762606]\n",
      " [-0.69835433]\n",
      " ...\n",
      " [ 0.32886509]\n",
      " [ 0.14096468]\n",
      " [-0.78725041]]\n",
      "Current iteration=8, loss=38827.950071144754\n",
      "t [[ 0.141317  ]\n",
      " [-1.33048185]\n",
      " [-0.75828112]\n",
      " ...\n",
      " [ 0.33252225]\n",
      " [ 0.141317  ]\n",
      " [-0.8388004 ]]\n",
      "t [[ 0.141317  ]\n",
      " [-1.33048185]\n",
      " [-0.75828112]\n",
      " ...\n",
      " [ 0.33252225]\n",
      " [ 0.141317  ]\n",
      " [-0.8388004 ]]\n",
      "t [[ 0.14015446]\n",
      " [-1.42585389]\n",
      " [-0.81460264]\n",
      " ...\n",
      " [ 0.33345042]\n",
      " [ 0.14015446]\n",
      " [-0.88639662]]\n",
      "loss=37403.12924916107\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.12955772]\n",
      " [-0.07552696]\n",
      " [-0.12087395]\n",
      " ...\n",
      " [ 0.10151265]\n",
      " [ 0.04273463]\n",
      " [-0.17199257]]\n",
      "t [[ 0.12955772]\n",
      " [-0.07552696]\n",
      " [-0.12087395]\n",
      " ...\n",
      " [ 0.10151265]\n",
      " [ 0.04273463]\n",
      " [-0.17199257]]\n",
      "t [[ 0.23895763]\n",
      " [-0.16591878]\n",
      " [-0.22830493]\n",
      " ...\n",
      " [ 0.17364585]\n",
      " [ 0.07429662]\n",
      " [-0.30741891]]\n",
      "t [[ 0.23895763]\n",
      " [-0.16591878]\n",
      " [-0.22830493]\n",
      " ...\n",
      " [ 0.17364585]\n",
      " [ 0.07429662]\n",
      " [-0.30741891]]\n",
      "Current iteration=2, loss=46655.282449559854\n",
      "t [[ 0.33184932]\n",
      " [-0.26315546]\n",
      " [-0.3248516 ]\n",
      " ...\n",
      " [ 0.22486151]\n",
      " [ 0.09731151]\n",
      " [-0.41701904]]\n",
      "t [[ 0.33184932]\n",
      " [-0.26315546]\n",
      " [-0.3248516 ]\n",
      " ...\n",
      " [ 0.22486151]\n",
      " [ 0.09731151]\n",
      " [-0.41701904]]\n",
      "t [[ 0.41126138]\n",
      " [-0.36219664]\n",
      " [-0.4124839 ]\n",
      " ...\n",
      " [ 0.26105181]\n",
      " [ 0.1137817 ]\n",
      " [-0.50822186]]\n",
      "t [[ 0.41126138]\n",
      " [-0.36219664]\n",
      " [-0.4124839 ]\n",
      " ...\n",
      " [ 0.26105181]\n",
      " [ 0.1137817 ]\n",
      " [-0.50822186]]\n",
      "Current iteration=4, loss=43151.964953690745\n",
      "t [[ 0.47961426]\n",
      " [-0.46005702]\n",
      " [-0.49270334]\n",
      " ...\n",
      " [ 0.28631553]\n",
      " [ 0.12520857]\n",
      " [-0.58608375]]\n",
      "t [[ 0.47961426]\n",
      " [-0.46005702]\n",
      " [-0.49270334]\n",
      " ...\n",
      " [ 0.28631553]\n",
      " [ 0.12520857]\n",
      " [-0.58608375]]\n",
      "t [[ 0.53882411]\n",
      " [-0.55506507]\n",
      " [-0.566663  ]\n",
      " ...\n",
      " [ 0.30353684]\n",
      " [ 0.13271942]\n",
      " [-0.65405468]]\n",
      "t [[ 0.53882411]\n",
      " [-0.55506507]\n",
      " [-0.566663  ]\n",
      " ...\n",
      " [ 0.30353684]\n",
      " [ 0.13271942]\n",
      " [-0.65405468]]\n",
      "Current iteration=6, loss=40665.00182124511\n",
      "t [[ 0.59041543]\n",
      " [-0.64635652]\n",
      " [-0.6352607 ]\n",
      " ...\n",
      " [ 0.31476462]\n",
      " [ 0.13716798]\n",
      " [-0.71451046]]\n",
      "t [[ 0.59041543]\n",
      " [-0.64635652]\n",
      " [-0.6352607 ]\n",
      " ...\n",
      " [ 0.31476462]\n",
      " [ 0.13716798]\n",
      " [-0.71451046]]\n",
      "t [[ 0.63561179]\n",
      " [-0.73355229]\n",
      " [-0.69920669]\n",
      " ...\n",
      " [ 0.32147001]\n",
      " [ 0.13920783]\n",
      " [-0.76910949]]\n",
      "t [[ 0.63561179]\n",
      " [-0.73355229]\n",
      " [-0.69920669]\n",
      " ...\n",
      " [ 0.32147001]\n",
      " [ 0.13920783]\n",
      " [-0.76910949]]\n",
      "Current iteration=8, loss=38815.152548344726\n",
      "t [[ 0.67540427]\n",
      " [-0.81655945]\n",
      " [-0.75907206]\n",
      " ...\n",
      " [ 0.32472294]\n",
      " [ 0.13934509]\n",
      " [-0.81902872]]\n",
      "t [[ 0.67540427]\n",
      " [-0.81655945]\n",
      " [-0.75907206]\n",
      " ...\n",
      " [ 0.32472294]\n",
      " [ 0.13934509]\n",
      " [-0.81902872]]\n",
      "t [[ 0.71060259]\n",
      " [-0.89544952]\n",
      " [-0.81532315]\n",
      " ...\n",
      " [ 0.32531274]\n",
      " [ 0.1379759 ]\n",
      " [-0.86511911]]\n",
      "loss=37390.558474502504\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.12789718]\n",
      " [-0.07230101]\n",
      " [-0.12042126]\n",
      " ...\n",
      " [ 0.10307886]\n",
      " [ 0.04352227]\n",
      " [-0.17728908]]\n",
      "t [[ 0.12789718]\n",
      " [-0.07230101]\n",
      " [-0.12042126]\n",
      " ...\n",
      " [ 0.10307886]\n",
      " [ 0.04352227]\n",
      " [-0.17728908]]\n",
      "t [[ 0.23590381]\n",
      " [-0.15953442]\n",
      " [-0.22740042]\n",
      " ...\n",
      " [ 0.17598232]\n",
      " [ 0.07573802]\n",
      " [-0.31691147]]\n",
      "t [[ 0.23590381]\n",
      " [-0.15953442]\n",
      " [-0.22740042]\n",
      " ...\n",
      " [ 0.17598232]\n",
      " [ 0.07573802]\n",
      " [-0.31691147]]\n",
      "Current iteration=2, loss=46698.60305337897\n",
      "t [[ 0.32762814]\n",
      " [-0.25364523]\n",
      " [-0.32351238]\n",
      " ...\n",
      " [ 0.22742924]\n",
      " [ 0.09930876]\n",
      " [-0.42990626]]\n",
      "t [[ 0.32762814]\n",
      " [-0.25364523]\n",
      " [-0.32351238]\n",
      " ...\n",
      " [ 0.22742924]\n",
      " [ 0.09930876]\n",
      " [-0.42990626]]\n",
      "t [[ 0.40606214]\n",
      " [-0.34958897]\n",
      " [-0.4107346 ]\n",
      " ...\n",
      " [ 0.26348707]\n",
      " [ 0.11626196]\n",
      " [-0.52391328]]\n",
      "t [[ 0.40606214]\n",
      " [-0.34958897]\n",
      " [-0.4107346 ]\n",
      " ...\n",
      " [ 0.26348707]\n",
      " [ 0.11626196]\n",
      " [-0.52391328]]\n",
      "Current iteration=4, loss=43234.56994686763\n",
      "t [[ 0.47359764]\n",
      " [-0.44438627]\n",
      " [-0.49057034]\n",
      " ...\n",
      " [ 0.28836   ]\n",
      " [ 0.12811582]\n",
      " [-0.60413081]]\n",
      "t [[ 0.47359764]\n",
      " [-0.44438627]\n",
      " [-0.49057034]\n",
      " ...\n",
      " [ 0.28836   ]\n",
      " [ 0.12811582]\n",
      " [-0.60413081]]\n",
      "t [[ 0.53212618]\n",
      " [-0.53637515]\n",
      " [-0.56417162]\n",
      " ...\n",
      " [ 0.3050006 ]\n",
      " [ 0.13600913]\n",
      " [-0.67410767]]\n",
      "t [[ 0.53212618]\n",
      " [-0.53637515]\n",
      " [-0.56417162]\n",
      " ...\n",
      " [ 0.3050006 ]\n",
      " [ 0.13600913]\n",
      " [-0.67410767]]\n",
      "Current iteration=6, loss=40779.29301518467\n",
      "t [[ 0.58315024]\n",
      " [-0.62470121]\n",
      " [-0.63243408]\n",
      " ...\n",
      " [ 0.31550666]\n",
      " [ 0.14080365]\n",
      " [-0.73629138]]\n",
      "t [[ 0.58315024]\n",
      " [-0.62470121]\n",
      " [-0.63243408]\n",
      " ...\n",
      " [ 0.31550666]\n",
      " [ 0.14080365]\n",
      " [-0.73629138]]\n",
      "t [[ 0.62787422]\n",
      " [-0.70899369]\n",
      " [-0.69606548]\n",
      " ...\n",
      " [ 0.32138565]\n",
      " [ 0.1431589 ]\n",
      " [-0.79239363]]\n",
      "t [[ 0.62787422]\n",
      " [-0.70899369]\n",
      " [-0.69606548]\n",
      " ...\n",
      " [ 0.32138565]\n",
      " [ 0.1431589 ]\n",
      " [-0.79239363]]\n",
      "Current iteration=8, loss=38954.69373882777\n",
      "t [[ 0.66727289]\n",
      " [-0.78916592]\n",
      " [-0.75563443]\n",
      " ...\n",
      " [ 0.32373487]\n",
      " [ 0.14358545]\n",
      " [-0.8436318 ]]\n",
      "t [[ 0.66727289]\n",
      " [-0.78916592]\n",
      " [-0.75563443]\n",
      " ...\n",
      " [ 0.32373487]\n",
      " [ 0.14358545]\n",
      " [-0.8436318 ]]\n",
      "t [[ 0.70214238]\n",
      " [-0.86529349]\n",
      " [-0.81160501]\n",
      " ...\n",
      " [ 0.32336434]\n",
      " [ 0.14248297]\n",
      " [-0.89088797]]\n",
      "loss=37550.440666681316\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.12769245]\n",
      " [-0.07595942]\n",
      " [-0.12004301]\n",
      " ...\n",
      " [ 0.18391729]\n",
      " [-0.12004301]\n",
      " [ 0.1722646 ]]\n",
      "t [[ 0.12769245]\n",
      " [-0.07595942]\n",
      " [-0.12004301]\n",
      " ...\n",
      " [ 0.18391729]\n",
      " [-0.12004301]\n",
      " [ 0.1722646 ]]\n",
      "t [[ 0.23552582]\n",
      " [-0.16603953]\n",
      " [-0.22676032]\n",
      " ...\n",
      " [ 0.33568421]\n",
      " [-0.22676032]\n",
      " [ 0.30921099]]\n",
      "t [[ 0.23552582]\n",
      " [-0.16603953]\n",
      " [-0.22676032]\n",
      " ...\n",
      " [ 0.33568421]\n",
      " [-0.22676032]\n",
      " [ 0.30921099]]\n",
      "Current iteration=2, loss=46703.624873633795\n",
      "t [[ 0.32711204]\n",
      " [-0.2624559 ]\n",
      " [-0.32268153]\n",
      " ...\n",
      " [ 0.46208783]\n",
      " [-0.32268153]\n",
      " [ 0.41930746]]\n",
      "t [[ 0.32711204]\n",
      " [-0.2624559 ]\n",
      " [-0.32268153]\n",
      " ...\n",
      " [ 0.46208783]\n",
      " [-0.32268153]\n",
      " [ 0.41930746]]\n",
      "t [[ 0.40540547]\n",
      " [-0.36037007]\n",
      " [-0.40975721]\n",
      " ...\n",
      " [ 0.56838938]\n",
      " [-0.40975721]\n",
      " [ 0.50887797]]\n",
      "t [[ 0.40540547]\n",
      " [-0.36037007]\n",
      " [-0.40975721]\n",
      " ...\n",
      " [ 0.56838938]\n",
      " [-0.40975721]\n",
      " [ 0.50887797]]\n",
      "Current iteration=4, loss=43233.844021650904\n",
      "t [[ 0.47276813]\n",
      " [-0.45693695]\n",
      " [-0.48947445]\n",
      " ...\n",
      " [ 0.65860321]\n",
      " [-0.48947445]\n",
      " [ 0.58257338]]\n",
      "t [[ 0.47276813]\n",
      " [-0.45693695]\n",
      " [-0.48947445]\n",
      " ...\n",
      " [ 0.65860321]\n",
      " [-0.48947445]\n",
      " [ 0.58257338]]\n",
      "t [[ 0.53108153]\n",
      " [-0.55057167]\n",
      " [-0.5629751 ]\n",
      " ...\n",
      " [ 0.73580008]\n",
      " [-0.5629751 ]\n",
      " [ 0.64382753]]\n",
      "t [[ 0.53108153]\n",
      " [-0.55057167]\n",
      " [-0.5629751 ]\n",
      " ...\n",
      " [ 0.73580008]\n",
      " [-0.5629751 ]\n",
      " [ 0.64382753]]\n",
      "Current iteration=6, loss=40771.84778463395\n",
      "t [[ 0.58184994]\n",
      " [-0.64046175]\n",
      " [-0.63114835]\n",
      " ...\n",
      " [ 0.80235088]\n",
      " [-0.63114835]\n",
      " [ 0.69520449]]\n",
      "t [[ 0.58184994]\n",
      " [-0.64046175]\n",
      " [-0.63114835]\n",
      " ...\n",
      " [ 0.80235088]\n",
      " [-0.63114835]\n",
      " [ 0.69520449]]\n",
      "t [[ 0.62628405]\n",
      " [-0.72625892]\n",
      " [-0.69469798]\n",
      " ...\n",
      " [ 0.86010744]\n",
      " [-0.69469798]\n",
      " [ 0.73864536]]\n",
      "t [[ 0.62628405]\n",
      " [-0.72625892]\n",
      " [-0.69469798]\n",
      " ...\n",
      " [ 0.86010744]\n",
      " [-0.69469798]\n",
      " [ 0.73864536]]\n",
      "Current iteration=8, loss=38941.377632222226\n",
      "t [[ 0.66536574]\n",
      " [-0.8078884 ]\n",
      " [-0.75419019]\n",
      " ...\n",
      " [ 0.91053361]\n",
      " [-0.75419019]\n",
      " [ 0.77564071]]\n",
      "t [[ 0.66536574]\n",
      " [-0.8078884 ]\n",
      " [-0.75419019]\n",
      " ...\n",
      " [ 0.91053361]\n",
      " [-0.75419019]\n",
      " [ 0.77564071]]\n",
      "t [[ 0.69989759]\n",
      " [-0.88543203]\n",
      " [-0.81008761]\n",
      " ...\n",
      " [ 0.95480004]\n",
      " [-0.81008761]\n",
      " [ 0.80735103]]\n",
      "loss=37532.223401462026\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04476842]\n",
      " [-0.2143735 ]\n",
      " [-0.12571108]\n",
      " ...\n",
      " [ 0.10749967]\n",
      " [ 0.04476842]\n",
      " [-0.18222442]]\n",
      "t [[ 0.04476842]\n",
      " [-0.2143735 ]\n",
      " [-0.12571108]\n",
      " ...\n",
      " [ 0.10749967]\n",
      " [ 0.04476842]\n",
      " [-0.18222442]]\n",
      "t [[ 0.07741555]\n",
      " [-0.40954721]\n",
      " [-0.23694321]\n",
      " ...\n",
      " [ 0.18279024]\n",
      " [ 0.07741555]\n",
      " [-0.32462499]]\n",
      "t [[ 0.07741555]\n",
      " [-0.40954721]\n",
      " [-0.23694321]\n",
      " ...\n",
      " [ 0.18279024]\n",
      " [ 0.07741555]\n",
      " [-0.32462499]]\n",
      "Current iteration=2, loss=46473.99715699239\n",
      "t [[ 0.10090862]\n",
      " [-0.58696703]\n",
      " [-0.33656241]\n",
      " ...\n",
      " [ 0.23551327]\n",
      " [ 0.10090862]\n",
      " [-0.43932165]]\n",
      "t [[ 0.10090862]\n",
      " [-0.58696703]\n",
      " [-0.33656241]\n",
      " ...\n",
      " [ 0.23551327]\n",
      " [ 0.10090862]\n",
      " [-0.43932165]]\n",
      "t [[ 0.11747542]\n",
      " [-0.74833982]\n",
      " [-0.42674146]\n",
      " ...\n",
      " [ 0.27225351]\n",
      " [ 0.11747542]\n",
      " [-0.53453055]]\n",
      "t [[ 0.11747542]\n",
      " [-0.74833982]\n",
      " [-0.42674146]\n",
      " ...\n",
      " [ 0.27225351]\n",
      " [ 0.11747542]\n",
      " [-0.53453055]]\n",
      "Current iteration=4, loss=42903.2993630445\n",
      "t [[ 0.12876193]\n",
      " [-0.89539051]\n",
      " [-0.50911441]\n",
      " ...\n",
      " [ 0.29751282]\n",
      " [ 0.12876193]\n",
      " [-0.61573924]]\n",
      "t [[ 0.12876193]\n",
      " [-0.89539051]\n",
      " [-0.50911441]\n",
      " ...\n",
      " [ 0.29751282]\n",
      " [ 0.12876193]\n",
      " [-0.61573924]]\n",
      "t [[ 0.13598827]\n",
      " [-1.02973981]\n",
      " [-0.58492275]\n",
      " ...\n",
      " [ 0.31441125]\n",
      " [ 0.13598827]\n",
      " [-0.68663069]]\n",
      "t [[ 0.13598827]\n",
      " [-1.02973981]\n",
      " [-0.58492275]\n",
      " ...\n",
      " [ 0.31441125]\n",
      " [ 0.13598827]\n",
      " [-0.68663069]]\n",
      "Current iteration=6, loss=40394.615987149184\n",
      "t [[ 0.14006827]\n",
      " [-1.15285391]\n",
      " [-0.65512521]\n",
      " ...\n",
      " [ 0.3251355 ]\n",
      " [ 0.14006827]\n",
      " [-0.7497063 ]]\n",
      "t [[ 0.14006827]\n",
      " [-1.15285391]\n",
      " [-0.65512521]\n",
      " ...\n",
      " [ 0.3251355 ]\n",
      " [ 0.14006827]\n",
      " [-0.7497063 ]]\n",
      "t [[ 0.14169495]\n",
      " [-1.26603081]\n",
      " [-0.72047559]\n",
      " ...\n",
      " [ 0.33123742]\n",
      " [ 0.14169495]\n",
      " [-0.80669311]]\n",
      "t [[ 0.14169495]\n",
      " [-1.26603081]\n",
      " [-0.72047559]\n",
      " ...\n",
      " [ 0.33123742]\n",
      " [ 0.14169495]\n",
      " [-0.80669311]]\n",
      "Current iteration=8, loss=38543.31454658446\n",
      "t [[ 0.14140045]\n",
      " [-1.3704055 ]\n",
      " [-0.78157723]\n",
      " ...\n",
      " [ 0.33383477]\n",
      " [ 0.14140045]\n",
      " [-0.85880708]]\n",
      "t [[ 0.14140045]\n",
      " [-1.3704055 ]\n",
      " [-0.78157723]\n",
      " ...\n",
      " [ 0.33383477]\n",
      " [ 0.14140045]\n",
      " [-0.85880708]]\n",
      "t [[ 0.13959825]\n",
      " [-1.46696401]\n",
      " [-0.83892123]\n",
      " ...\n",
      " [ 0.33374567]\n",
      " [ 0.13959825]\n",
      " [-0.90692297]]\n",
      "loss=37126.49733860947\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.13519067]\n",
      " [-0.07881074]\n",
      " [-0.12612934]\n",
      " ...\n",
      " [ 0.10592624]\n",
      " [ 0.04459266]\n",
      " [-0.1794705 ]]\n",
      "t [[ 0.13519067]\n",
      " [-0.07881074]\n",
      " [-0.12612934]\n",
      " ...\n",
      " [ 0.10592624]\n",
      " [ 0.04459266]\n",
      " [-0.1794705 ]]\n",
      "t [[ 0.24844035]\n",
      " [-0.17380034]\n",
      " [-0.23762578]\n",
      " ...\n",
      " [ 0.17987293]\n",
      " [ 0.07702423]\n",
      " [-0.31913984]]\n",
      "t [[ 0.24844035]\n",
      " [-0.17380034]\n",
      " [-0.23762578]\n",
      " ...\n",
      " [ 0.17987293]\n",
      " [ 0.07702423]\n",
      " [-0.31913984]]\n",
      "Current iteration=2, loss=46463.490300403384\n",
      "t [[ 0.34390377]\n",
      " [-0.27586104]\n",
      " [-0.33740005]\n",
      " ...\n",
      " [ 0.23145457]\n",
      " [ 0.10028053]\n",
      " [-0.43121391]]\n",
      "t [[ 0.34390377]\n",
      " [-0.27586104]\n",
      " [-0.33740005]\n",
      " ...\n",
      " [ 0.23145457]\n",
      " [ 0.10028053]\n",
      " [-0.43121391]]\n",
      "t [[ 0.42498344]\n",
      " [-0.37942775]\n",
      " [-0.42765771]\n",
      " ...\n",
      " [ 0.26722962]\n",
      " [ 0.11660149]\n",
      " [-0.52395367]]\n",
      "t [[ 0.42498344]\n",
      " [-0.37942775]\n",
      " [-0.42765771]\n",
      " ...\n",
      " [ 0.26722962]\n",
      " [ 0.11660149]\n",
      " [-0.52395367]]\n",
      "Current iteration=4, loss=42890.57353511754\n",
      "t [[ 0.4943621 ]\n",
      " [-0.48131234]\n",
      " [-0.51005589]\n",
      " ...\n",
      " [ 0.29167689]\n",
      " [ 0.12764061]\n",
      " [-0.60286507]]\n",
      "t [[ 0.4943621 ]\n",
      " [-0.48131234]\n",
      " [-0.51005589]\n",
      " ...\n",
      " [ 0.29167689]\n",
      " [ 0.12764061]\n",
      " [-0.60286507]]\n",
      "t [[ 0.55413951]\n",
      " [-0.57980141]\n",
      " [-0.5858522 ]\n",
      " ...\n",
      " [ 0.30789469]\n",
      " [ 0.1346227 ]\n",
      " [-0.67163343]]\n",
      "t [[ 0.55413951]\n",
      " [-0.57980141]\n",
      " [-0.5858522 ]\n",
      " ...\n",
      " [ 0.30789469]\n",
      " [ 0.1346227 ]\n",
      " [-0.67163343]]\n",
      "Current iteration=6, loss=40381.65244297309\n",
      "t [[ 0.60597052]\n",
      " [-0.6740608 ]\n",
      " [-0.65601663]\n",
      " ...\n",
      " [ 0.3180499 ]\n",
      " [ 0.13846448]\n",
      " [-0.73275372]]\n",
      "t [[ 0.60597052]\n",
      " [-0.6740608 ]\n",
      " [-0.65601663]\n",
      " ...\n",
      " [ 0.3180499 ]\n",
      " [ 0.13846448]\n",
      " [-0.73275372]]\n",
      "t [[ 0.65117251]\n",
      " [-0.76376882]\n",
      " [-0.72131091]\n",
      " ...\n",
      " [ 0.32367716]\n",
      " [ 0.13986071]\n",
      " [-0.78794225]]\n",
      "t [[ 0.65117251]\n",
      " [-0.76376882]\n",
      " [-0.72131091]\n",
      " ...\n",
      " [ 0.32367716]\n",
      " [ 0.13986071]\n",
      " [-0.78794225]]\n",
      "Current iteration=8, loss=38530.56019238812\n",
      "t [[ 0.69080481]\n",
      " [-0.84889585]\n",
      " [-0.78234401]\n",
      " ...\n",
      " [ 0.32587965]\n",
      " [ 0.13934459]\n",
      " [-0.83840255]]\n",
      "t [[ 0.69080481]\n",
      " [-0.84889585]\n",
      " [-0.78234401]\n",
      " ...\n",
      " [ 0.32587965]\n",
      " [ 0.13934459]\n",
      " [-0.83840255]]\n",
      "t [[ 0.72572724]\n",
      " [-0.92957321]\n",
      " [-0.83961104]\n",
      " ...\n",
      " [ 0.32546329]\n",
      " [ 0.13733015]\n",
      " [-0.88499662]]\n",
      "loss=37113.972741060905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.13345793]\n",
      " [-0.07544453]\n",
      " [-0.12565697]\n",
      " ...\n",
      " [ 0.10756054]\n",
      " [ 0.04541454]\n",
      " [-0.1849973 ]]\n",
      "t [[ 0.13345793]\n",
      " [-0.07544453]\n",
      " [-0.12565697]\n",
      " ...\n",
      " [ 0.10756054]\n",
      " [ 0.04541454]\n",
      " [-0.1849973 ]]\n",
      "t [[ 0.24526572]\n",
      " [-0.16714146]\n",
      " [-0.23668205]\n",
      " ...\n",
      " [ 0.18227592]\n",
      " [ 0.07852234]\n",
      " [-0.32899614]]\n",
      "t [[ 0.24526572]\n",
      " [-0.16714146]\n",
      " [-0.23668205]\n",
      " ...\n",
      " [ 0.18227592]\n",
      " [ 0.07852234]\n",
      " [-0.32899614]]\n",
      "Current iteration=2, loss=46508.695189733044\n",
      "t [[ 0.33953097]\n",
      " [-0.26594341]\n",
      " [-0.33600441]\n",
      " ...\n",
      " [ 0.2340526 ]\n",
      " [ 0.1023501 ]\n",
      " [-0.44454243]]\n",
      "t [[ 0.33953097]\n",
      " [-0.26594341]\n",
      " [-0.33600441]\n",
      " ...\n",
      " [ 0.2340526 ]\n",
      " [ 0.1023501 ]\n",
      " [-0.44454243]]\n",
      "t [[ 0.41961523]\n",
      " [-0.36628177]\n",
      " [-0.42583756]\n",
      " ...\n",
      " [ 0.26964192]\n",
      " [ 0.11916552]\n",
      " [-0.54013105]]\n",
      "t [[ 0.41961523]\n",
      " [-0.36628177]\n",
      " [-0.42583756]\n",
      " ...\n",
      " [ 0.26964192]\n",
      " [ 0.11916552]\n",
      " [-0.54013105]]\n",
      "Current iteration=4, loss=42976.31951625628\n",
      "t [[ 0.48816937]\n",
      " [-0.46497612]\n",
      " [-0.50784005]\n",
      " ...\n",
      " [ 0.29363589]\n",
      " [ 0.13064038]\n",
      " [-0.62142224]]\n",
      "t [[ 0.48816937]\n",
      " [-0.46497612]\n",
      " [-0.50784005]\n",
      " ...\n",
      " [ 0.29363589]\n",
      " [ 0.13064038]\n",
      " [-0.62142224]]\n",
      "t [[ 0.54726561]\n",
      " [-0.56032443]\n",
      " [-0.58326795]\n",
      " ...\n",
      " [ 0.30920595]\n",
      " [ 0.13801174]\n",
      " [-0.69220757]]\n",
      "t [[ 0.54726561]\n",
      " [-0.56032443]\n",
      " [-0.58326795]\n",
      " ...\n",
      " [ 0.30920595]\n",
      " [ 0.13801174]\n",
      " [-0.69220757]]\n",
      "Current iteration=6, loss=40499.68692304009\n",
      "t [[ 0.59853437]\n",
      " [-0.65150376]\n",
      " [-0.6530886 ]\n",
      " ...\n",
      " [ 0.318571  ]\n",
      " [ 0.14220486]\n",
      " [-0.75505829]]\n",
      "t [[ 0.59853437]\n",
      " [-0.65150376]\n",
      " [-0.6530886 ]\n",
      " ...\n",
      " [ 0.318571  ]\n",
      " [ 0.14220486]\n",
      " [-0.75505829]]\n",
      "t [[ 0.64327204]\n",
      " [-0.73820147]\n",
      " [-0.71806083]\n",
      " ...\n",
      " [ 0.323304  ]\n",
      " [ 0.14392072]\n",
      " [-0.81174698]]\n",
      "t [[ 0.64327204]\n",
      " [-0.73820147]\n",
      " [-0.71806083]\n",
      " ...\n",
      " [ 0.323304  ]\n",
      " [ 0.14392072]\n",
      " [-0.81174698]]\n",
      "Current iteration=8, loss=38674.08171287767\n",
      "t [[ 0.68252036]\n",
      " [-0.82039439]\n",
      " [-0.77879088]\n",
      " ...\n",
      " [ 0.32453662]\n",
      " [ 0.14369721]\n",
      " [-0.86351942]]\n",
      "t [[ 0.68252036]\n",
      " [-0.82039439]\n",
      " [-0.77879088]\n",
      " ...\n",
      " [ 0.32453662]\n",
      " [ 0.14369721]\n",
      " [-0.86351942]]\n",
      "t [[ 0.71712465]\n",
      " [-0.89821786]\n",
      " [-0.83577132]\n",
      " ...\n",
      " [ 0.32309616]\n",
      " [ 0.14195205]\n",
      " [-0.91126992]]\n",
      "loss=37277.88051697582\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.1332443 ]\n",
      " [-0.07926201]\n",
      " [-0.12526227]\n",
      " ...\n",
      " [ 0.19191369]\n",
      " [-0.12526227]\n",
      " [ 0.17975436]]\n",
      "t [[ 0.1332443 ]\n",
      " [-0.07926201]\n",
      " [-0.12526227]\n",
      " ...\n",
      " [ 0.19191369]\n",
      " [-0.12526227]\n",
      " [ 0.17975436]]\n",
      "t [[ 0.24487221]\n",
      " [-0.17389341]\n",
      " [-0.23601929]\n",
      " ...\n",
      " [ 0.34883215]\n",
      " [-0.23601929]\n",
      " [ 0.32106578]]\n",
      "t [[ 0.24487221]\n",
      " [-0.17389341]\n",
      " [-0.23601929]\n",
      " ...\n",
      " [ 0.34883215]\n",
      " [-0.23601929]\n",
      " [ 0.32106578]]\n",
      "Current iteration=2, loss=46513.62390442814\n",
      "t [[ 0.33899406]\n",
      " [-0.27505555]\n",
      " [-0.3351488 ]\n",
      " ...\n",
      " [ 0.47847543]\n",
      " [-0.3351488 ]\n",
      " [ 0.43356367]]\n",
      "t [[ 0.33899406]\n",
      " [-0.27505555]\n",
      " [-0.3351488 ]\n",
      " ...\n",
      " [ 0.47847543]\n",
      " [-0.3351488 ]\n",
      " [ 0.43356367]]\n",
      "t [[ 0.4189281 ]\n",
      " [-0.37740905]\n",
      " [-0.42483477]\n",
      " ...\n",
      " [ 0.58673503]\n",
      " [-0.42483477]\n",
      " [ 0.52431352]]\n",
      "t [[ 0.4189281 ]\n",
      " [-0.37740905]\n",
      " [-0.42483477]\n",
      " ...\n",
      " [ 0.58673503]\n",
      " [-0.42483477]\n",
      " [ 0.52431352]]\n",
      "Current iteration=4, loss=42975.01567879683\n",
      "t [[ 0.48729408]\n",
      " [-0.47791732]\n",
      " [-0.5067184 ]\n",
      " ...\n",
      " [ 0.67804112]\n",
      " [-0.5067184 ]\n",
      " [ 0.59842647]]\n",
      "t [[ 0.48729408]\n",
      " [-0.47791732]\n",
      " [-0.5067184 ]\n",
      " ...\n",
      " [ 0.67804112]\n",
      " [-0.5067184 ]\n",
      " [ 0.59842647]]\n",
      "t [[ 0.54615578]\n",
      " [-0.57495758]\n",
      " [-0.58204504]\n",
      " ...\n",
      " [ 0.75574034]\n",
      " [-0.58204504]\n",
      " [ 0.65962289]]\n",
      "t [[ 0.54615578]\n",
      " [-0.57495758]\n",
      " [-0.58204504]\n",
      " ...\n",
      " [ 0.75574034]\n",
      " [-0.58204504]\n",
      " [ 0.65962289]]\n",
      "Current iteration=6, loss=40491.413581157874\n",
      "t [[ 0.5971473 ]\n",
      " [-0.66774839]\n",
      " [-0.65177545]\n",
      " ...\n",
      " [ 0.82238938]\n",
      " [-0.65177545]\n",
      " [ 0.71064755]]\n",
      "t [[ 0.5971473 ]\n",
      " [-0.66774839]\n",
      " [-0.65177545]\n",
      " ...\n",
      " [ 0.82238938]\n",
      " [-0.65177545]\n",
      " [ 0.71064755]]\n",
      "t [[ 0.6415727 ]\n",
      " [-0.75599852]\n",
      " [-0.71666453]\n",
      " ...\n",
      " [ 0.87996733]\n",
      " [-0.71666453]\n",
      " [ 0.75355744]]\n",
      "t [[ 0.6415727 ]\n",
      " [-0.75599852]\n",
      " [-0.71666453]\n",
      " ...\n",
      " [ 0.87996733]\n",
      " [-0.71666453]\n",
      " [ 0.75355744]]\n",
      "Current iteration=8, loss=38659.82947798873\n",
      "t [[ 0.68048166]\n",
      " [-0.83969564]\n",
      " [-0.77731617]\n",
      " ...\n",
      " [ 0.93002669]\n",
      " [-0.77731617]\n",
      " [ 0.78991867]]\n",
      "t [[ 0.68048166]\n",
      " [-0.83969564]\n",
      " [-0.77731617]\n",
      " ...\n",
      " [ 0.93002669]\n",
      " [-0.77731617]\n",
      " [ 0.78991867]]\n",
      "t [[ 0.71472652]\n",
      " [-0.91898044]\n",
      " [-0.83422168]\n",
      " ...\n",
      " [ 0.9738008 ]\n",
      " [-0.83422168]\n",
      " [ 0.82094163]]\n",
      "loss=37258.68992323652\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04663378]\n",
      " [-0.22330573]\n",
      " [-0.13094905]\n",
      " ...\n",
      " [ 0.11197882]\n",
      " [ 0.04663378]\n",
      " [-0.1898171 ]]\n",
      "t [[ 0.04663378]\n",
      " [-0.22330573]\n",
      " [-0.13094905]\n",
      " ...\n",
      " [ 0.11197882]\n",
      " [ 0.04663378]\n",
      " [-0.1898171 ]]\n",
      "t [[ 0.08011981]\n",
      " [-0.42578311]\n",
      " [-0.24619233]\n",
      " ...\n",
      " [ 0.1890202 ]\n",
      " [ 0.08011981]\n",
      " [-0.33643814]]\n",
      "t [[ 0.08011981]\n",
      " [-0.42578311]\n",
      " [-0.24619233]\n",
      " ...\n",
      " [ 0.1890202 ]\n",
      " [ 0.08011981]\n",
      " [-0.33643814]]\n",
      "Current iteration=2, loss=46285.18314799472\n",
      "t [[ 0.10381461]\n",
      " [-0.60907516]\n",
      " [-0.34897294]\n",
      " ...\n",
      " [ 0.24202099]\n",
      " [ 0.10381461]\n",
      " [-0.45356437]]\n",
      "t [[ 0.10381461]\n",
      " [-0.60907516]\n",
      " [-0.34897294]\n",
      " ...\n",
      " [ 0.24202099]\n",
      " [ 0.10381461]\n",
      " [-0.45356437]]\n",
      "t [[ 0.12019633]\n",
      " [-0.77512239]\n",
      " [-0.44170996]\n",
      " ...\n",
      " [ 0.27826951]\n",
      " [ 0.12019633]\n",
      " [-0.55028054]]\n",
      "t [[ 0.12019633]\n",
      " [-0.77512239]\n",
      " [-0.44170996]\n",
      " ...\n",
      " [ 0.27826951]\n",
      " [ 0.12019633]\n",
      " [-0.55028054]]\n",
      "Current iteration=4, loss=42649.0576892088\n",
      "t [[ 0.1310676 ]\n",
      " [-0.9258711 ]\n",
      " [-0.52619688]\n",
      " ...\n",
      " [ 0.30265718]\n",
      " [ 0.1310676 ]\n",
      " [-0.63252784]]\n",
      "t [[ 0.1310676 ]\n",
      " [-0.9258711 ]\n",
      " [-0.52619688]\n",
      " ...\n",
      " [ 0.30265718]\n",
      " [ 0.1310676 ]\n",
      " [-0.63252784]]\n",
      "t [[ 0.13774684]\n",
      " [-1.06313336]\n",
      " [-0.60378066]\n",
      " ...\n",
      " [ 0.31851707]\n",
      " [ 0.13774684]\n",
      " [-0.70421863]]\n",
      "t [[ 0.13774684]\n",
      " [-1.06313336]\n",
      " [-0.60378066]\n",
      " ...\n",
      " [ 0.31851707]\n",
      " [ 0.13774684]\n",
      " [-0.70421863]]\n",
      "Current iteration=6, loss=40121.4767513959\n",
      "t [[ 0.14121028]\n",
      " [-1.1885332 ]\n",
      " [-0.67549211]\n",
      " ...\n",
      " [ 0.32814949]\n",
      " [ 0.14121028]\n",
      " [-0.76796633]]\n",
      "t [[ 0.14121028]\n",
      " [-1.1885332 ]\n",
      " [-0.67549211]\n",
      " ...\n",
      " [ 0.32814949]\n",
      " [ 0.14121028]\n",
      " [-0.76796633]]\n",
      "t [[ 0.14219093]\n",
      " [-1.30349553]\n",
      " [-0.74213613]\n",
      " ...\n",
      " [ 0.33316539]\n",
      " [ 0.14219093]\n",
      " [-0.82555045]]\n",
      "t [[ 0.14219093]\n",
      " [-1.30349553]\n",
      " [-0.74213613]\n",
      " ...\n",
      " [ 0.33316539]\n",
      " [ 0.14219093]\n",
      " [-0.82555045]]\n",
      "Current iteration=8, loss=38270.88777669312\n",
      "t [[ 0.14124686]\n",
      " [-1.40925594]\n",
      " [-0.80435363]\n",
      " ...\n",
      " [ 0.33471233]\n",
      " [ 0.14124686]\n",
      " [-0.87821011]]\n",
      "t [[ 0.14124686]\n",
      " [-1.40925594]\n",
      " [-0.80435363]\n",
      " ...\n",
      " [ 0.33471233]\n",
      " [ 0.14124686]\n",
      " [-0.87821011]]\n",
      "t [[ 0.13880832]\n",
      " [-1.50687978]\n",
      " [-0.86266418]\n",
      " ...\n",
      " [ 0.33362252]\n",
      " [ 0.13880832]\n",
      " [-0.92682939]]\n",
      "loss=36863.21889912531\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.14082361]\n",
      " [-0.08209452]\n",
      " [-0.13138473]\n",
      " ...\n",
      " [ 0.11033984]\n",
      " [ 0.04645069]\n",
      " [-0.18694844]]\n",
      "t [[ 0.14082361]\n",
      " [-0.08209452]\n",
      " [-0.13138473]\n",
      " ...\n",
      " [ 0.11033984]\n",
      " [ 0.04645069]\n",
      " [-0.18694844]]\n",
      "t [[ 0.25784853]\n",
      " [-0.18173666]\n",
      " [-0.24689675]\n",
      " ...\n",
      " [ 0.18599114]\n",
      " [ 0.07971049]\n",
      " [-0.3307255 ]]\n",
      "t [[ 0.25784853]\n",
      " [-0.18173666]\n",
      " [-0.24689675]\n",
      " ...\n",
      " [ 0.18599114]\n",
      " [ 0.07971049]\n",
      " [-0.3307255 ]]\n",
      "Current iteration=2, loss=46274.45325621127\n",
      "t [[ 0.35577916]\n",
      " [-0.28863814]\n",
      " [-0.3498297 ]\n",
      " ...\n",
      " [ 0.23782017]\n",
      " [ 0.1031569 ]\n",
      " [-0.44512943]]\n",
      "t [[ 0.35577916]\n",
      " [-0.28863814]\n",
      " [-0.3498297 ]\n",
      " ...\n",
      " [ 0.23782017]\n",
      " [ 0.1031569 ]\n",
      " [-0.44512943]]\n",
      "t [[ 0.43841649]\n",
      " [-0.39669167]\n",
      " [-0.44263903]\n",
      " ...\n",
      " [ 0.27308581]\n",
      " [ 0.11928141]\n",
      " [-0.53929352]]\n",
      "t [[ 0.43841649]\n",
      " [-0.39669167]\n",
      " [-0.44263903]\n",
      " ...\n",
      " [ 0.27308581]\n",
      " [ 0.11928141]\n",
      " [-0.53929352]]\n",
      "Current iteration=4, loss=42636.25300879593\n",
      "t [[ 0.50871723]\n",
      " [-0.50251713]\n",
      " [-0.52714337]\n",
      " ...\n",
      " [ 0.29665355]\n",
      " [ 0.12989475]\n",
      " [-0.61917725]]\n",
      "t [[ 0.50871723]\n",
      " [-0.50251713]\n",
      " [-0.52714337]\n",
      " ...\n",
      " [ 0.29665355]\n",
      " [ 0.12989475]\n",
      " [-0.61917725]]\n",
      "t [[ 0.56896989]\n",
      " [-0.60437555]\n",
      " [-0.60470689]\n",
      " ...\n",
      " [ 0.31183207]\n",
      " [ 0.13632024]\n",
      " [-0.68869351]]\n",
      "t [[ 0.56896989]\n",
      " [-0.60437555]\n",
      " [-0.60470689]\n",
      " ...\n",
      " [ 0.31183207]\n",
      " [ 0.13632024]\n",
      " [-0.68869351]]\n",
      "Current iteration=6, loss=40108.524706540185\n",
      "t [[ 0.6209615 ]\n",
      " [-0.70147693]\n",
      " [-0.67637228]\n",
      " ...\n",
      " [ 0.32089976]\n",
      " [ 0.13953706]\n",
      " [-0.75044704]]\n",
      "t [[ 0.6209615 ]\n",
      " [-0.70147693]\n",
      " [-0.67637228]\n",
      " ...\n",
      " [ 0.32089976]\n",
      " [ 0.13953706]\n",
      " [-0.75044704]]\n",
      "t [[ 0.66610339]\n",
      " [-0.793567  ]\n",
      " [-0.74295265]\n",
      " ...\n",
      " [ 0.32544885]\n",
      " [ 0.14028   ]\n",
      " [-0.80620436]]\n",
      "t [[ 0.66610339]\n",
      " [-0.793567  ]\n",
      " [-0.74295265]\n",
      " ...\n",
      " [ 0.32544885]\n",
      " [ 0.14028   ]\n",
      " [-0.80620436]]\n",
      "Current iteration=8, loss=38258.17832096909\n",
      "t [[ 0.70552242]\n",
      " [-0.88068592]\n",
      " [-0.80509466]\n",
      " ...\n",
      " [ 0.32661112]\n",
      " [ 0.13910808]\n",
      " [-0.85719018]]\n",
      "t [[ 0.70552242]\n",
      " [-0.88068592]\n",
      " [-0.80509466]\n",
      " ...\n",
      " [ 0.32661112]\n",
      " [ 0.13910808]\n",
      " [-0.85719018]]\n",
      "t [[ 0.74012741]\n",
      " [-0.96302866]\n",
      " [-0.86332193]\n",
      " ...\n",
      " [ 0.32520578]\n",
      " [ 0.13645206]\n",
      " [-0.90427416]]\n",
      "loss=36850.73675566114\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.13901868]\n",
      " [-0.07858805]\n",
      " [-0.13089268]\n",
      " ...\n",
      " [ 0.11204223]\n",
      " [ 0.04730682]\n",
      " [-0.19270552]]\n",
      "t [[ 0.13901868]\n",
      " [-0.07858805]\n",
      " [-0.13089268]\n",
      " ...\n",
      " [ 0.11204223]\n",
      " [ 0.04730682]\n",
      " [-0.19270552]]\n",
      "t [[ 0.25455407]\n",
      " [-0.1748035 ]\n",
      " [-0.24591383]\n",
      " ...\n",
      " [ 0.18845784]\n",
      " [ 0.08126484]\n",
      " [-0.34094159]]\n",
      "t [[ 0.25455407]\n",
      " [-0.1748035 ]\n",
      " [-0.24591383]\n",
      " ...\n",
      " [ 0.18845784]\n",
      " [ 0.08126484]\n",
      " [-0.34094159]]\n",
      "Current iteration=2, loss=46321.537469695\n",
      "t [[ 0.35125717]\n",
      " [-0.27831348]\n",
      " [-0.34837787]\n",
      " ...\n",
      " [ 0.24044302]\n",
      " [ 0.10529776]\n",
      " [-0.45889085]]\n",
      "t [[ 0.35125717]\n",
      " [-0.27831348]\n",
      " [-0.34837787]\n",
      " ...\n",
      " [ 0.24044302]\n",
      " [ 0.10529776]\n",
      " [-0.45889085]]\n",
      "t [[ 0.4328834 ]\n",
      " [-0.38300797]\n",
      " [-0.44074861]\n",
      " ...\n",
      " [ 0.27546753]\n",
      " [ 0.12192769]\n",
      " [-0.55594449]]\n",
      "t [[ 0.4328834 ]\n",
      " [-0.38300797]\n",
      " [-0.44074861]\n",
      " ...\n",
      " [ 0.27546753]\n",
      " [ 0.12192769]\n",
      " [-0.55594449]]\n",
      "Current iteration=4, loss=42725.08857334849\n",
      "t [[ 0.50235411]\n",
      " [-0.48551663]\n",
      " [-0.52484566]\n",
      " ...\n",
      " [ 0.2985179 ]\n",
      " [ 0.13298506]\n",
      " [-0.63822888]]\n",
      "t [[ 0.50235411]\n",
      " [-0.48551663]\n",
      " [-0.52484566]\n",
      " ...\n",
      " [ 0.2985179 ]\n",
      " [ 0.13298506]\n",
      " [-0.63822888]]\n",
      "t [[ 0.56192716]\n",
      " [-0.58411385]\n",
      " [-0.60203111]\n",
      " ...\n",
      " [ 0.31298074]\n",
      " [ 0.13980625]\n",
      " [-0.70977053]]\n",
      "t [[ 0.56192716]\n",
      " [-0.58411385]\n",
      " [-0.60203111]\n",
      " ...\n",
      " [ 0.31298074]\n",
      " [ 0.13980625]\n",
      " [-0.70977053]]\n",
      "Current iteration=6, loss=40230.20092307094\n",
      "t [[ 0.61336264]\n",
      " [-0.67802219]\n",
      " [-0.67334451]\n",
      " ...\n",
      " [ 0.32118954]\n",
      " [ 0.14337941]\n",
      " [-0.77325498]]\n",
      "t [[ 0.61336264]\n",
      " [-0.67802219]\n",
      " [-0.67334451]\n",
      " ...\n",
      " [ 0.32118954]\n",
      " [ 0.14337941]\n",
      " [-0.77325498]]\n",
      "t [[ 0.65804907]\n",
      " [-0.76699709]\n",
      " [-0.73959569]\n",
      " ...\n",
      " [ 0.32477668]\n",
      " [ 0.14444586]\n",
      " [-0.83050782]]\n",
      "t [[ 0.65804907]\n",
      " [-0.76699709]\n",
      " [-0.73959569]\n",
      " ...\n",
      " [ 0.32477668]\n",
      " [ 0.14444586]\n",
      " [-0.83050782]]\n",
      "Current iteration=8, loss=38405.5410628695\n",
      "t [[ 0.69709444]\n",
      " [-0.85108529]\n",
      " [-0.80142823]\n",
      " ...\n",
      " [ 0.32490352]\n",
      " [ 0.14356956]\n",
      " [-0.88279778]]\n",
      "t [[ 0.69709444]\n",
      " [-0.85108529]\n",
      " [-0.80142823]\n",
      " ...\n",
      " [ 0.32490352]\n",
      " [ 0.14356956]\n",
      " [-0.88279778]]\n",
      "t [[ 0.73139224]\n",
      " [-0.93048555]\n",
      " [-0.85936303]\n",
      " ...\n",
      " [ 0.32241117]\n",
      " [ 0.1411851 ]\n",
      " [-0.93102793]]\n",
      "loss=37018.50535295327\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.13879615]\n",
      " [-0.08256459]\n",
      " [-0.13048153]\n",
      " ...\n",
      " [ 0.1999101 ]\n",
      " [-0.13048153]\n",
      " [ 0.18724413]]\n",
      "t [[ 0.13879615]\n",
      " [-0.08256459]\n",
      " [-0.13048153]\n",
      " ...\n",
      " [ 0.1999101 ]\n",
      " [-0.13048153]\n",
      " [ 0.18724413]]\n",
      "t [[ 0.25414504]\n",
      " [-0.18179941]\n",
      " [-0.24522882]\n",
      " ...\n",
      " [ 0.36186102]\n",
      " [-0.24522882]\n",
      " [ 0.33278987]]\n",
      "t [[ 0.25414504]\n",
      " [-0.18179941]\n",
      " [-0.24522882]\n",
      " ...\n",
      " [ 0.36186102]\n",
      " [-0.24522882]\n",
      " [ 0.33278987]]\n",
      "Current iteration=2, loss=46326.35555341283\n",
      "t [[ 0.35069922]\n",
      " [-0.28772185]\n",
      " [-0.34749824]\n",
      " ...\n",
      " [ 0.49458734]\n",
      " [-0.34749824]\n",
      " [ 0.44752875]]\n",
      "t [[ 0.35069922]\n",
      " [-0.28772185]\n",
      " [-0.34749824]\n",
      " ...\n",
      " [ 0.49458734]\n",
      " [-0.34749824]\n",
      " [ 0.44752875]]\n",
      "t [[ 0.4321648 ]\n",
      " [-0.39447531]\n",
      " [-0.43972133]\n",
      " ...\n",
      " [ 0.60464922]\n",
      " [-0.43972133]\n",
      " [ 0.53931006]]\n",
      "t [[ 0.4321648 ]\n",
      " [-0.39447531]\n",
      " [-0.43972133]\n",
      " ...\n",
      " [ 0.60464922]\n",
      " [-0.43972133]\n",
      " [ 0.53931006]]\n",
      "Current iteration=4, loss=42723.19992960936\n",
      "t [[ 0.50143116]\n",
      " [-0.49884225]\n",
      " [-0.52369916]\n",
      " ...\n",
      " [ 0.69690817]\n",
      " [-0.52369916]\n",
      " [ 0.61371866]]\n",
      "t [[ 0.50143116]\n",
      " [-0.49884225]\n",
      " [-0.52369916]\n",
      " ...\n",
      " [ 0.69690817]\n",
      " [-0.52369916]\n",
      " [ 0.61371866]]\n",
      "t [[ 0.5607496 ]\n",
      " [-0.59917787]\n",
      " [-0.60078267]\n",
      " ...\n",
      " [ 0.77499152]\n",
      " [-0.60078267]\n",
      " [ 0.6747629 ]]\n",
      "t [[ 0.5607496 ]\n",
      " [-0.59917787]\n",
      " [-0.60078267]\n",
      " ...\n",
      " [ 0.77499152]\n",
      " [-0.60078267]\n",
      " [ 0.6747629 ]]\n",
      "Current iteration=6, loss=40221.11222193109\n",
      "t [[ 0.6118859 ]\n",
      " [-0.69474542]\n",
      " [-0.6720047 ]\n",
      " ...\n",
      " [ 0.84164193]\n",
      " [-0.6720047 ]\n",
      " [ 0.72536551]]\n",
      "t [[ 0.6118859 ]\n",
      " [-0.69474542]\n",
      " [-0.6720047 ]\n",
      " ...\n",
      " [ 0.84164193]\n",
      " [-0.6720047 ]\n",
      " [ 0.72536551]]\n",
      "t [[ 0.65623756]\n",
      " [-0.78532047]\n",
      " [-0.73817124]\n",
      " ...\n",
      " [ 0.89896408]\n",
      " [-0.73817124]\n",
      " [ 0.76769546]]\n",
      "t [[ 0.65623756]\n",
      " [-0.78532047]\n",
      " [-0.73817124]\n",
      " ...\n",
      " [ 0.89896408]\n",
      " [-0.73817124]\n",
      " [ 0.76769546]]\n",
      "Current iteration=8, loss=38390.37904828199\n",
      "t [[ 0.69492137]\n",
      " [-0.87095948]\n",
      " [-0.79992366]\n",
      " ...\n",
      " [ 0.94859685]\n",
      " [-0.79992366]\n",
      " [ 0.80339063]]\n",
      "t [[ 0.69492137]\n",
      " [-0.87095948]\n",
      " [-0.79992366]\n",
      " ...\n",
      " [ 0.94859685]\n",
      " [-0.79992366]\n",
      " [ 0.80339063]]\n",
      "t [[ 0.72883823]\n",
      " [-0.95186569]\n",
      " [-0.85778169]\n",
      " ...\n",
      " [ 0.99183397]\n",
      " [-0.85778169]\n",
      " [ 0.83370816]]\n",
      "loss=36998.377375382435\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04849913]\n",
      " [-0.23223796]\n",
      " [-0.13618701]\n",
      " ...\n",
      " [ 0.11645797]\n",
      " [ 0.04849913]\n",
      " [-0.19740978]]\n",
      "t [[ 0.04849913]\n",
      " [-0.23223796]\n",
      " [-0.13618701]\n",
      " ...\n",
      " [ 0.11645797]\n",
      " [ 0.04849913]\n",
      " [-0.19740978]]\n",
      "t [[ 0.08278294]\n",
      " [-0.44195333]\n",
      " [-0.25539218]\n",
      " ...\n",
      " [ 0.19514073]\n",
      " [ 0.08278294]\n",
      " [-0.34811622]]\n",
      "t [[ 0.08278294]\n",
      " [-0.44195333]\n",
      " [-0.25539218]\n",
      " ...\n",
      " [ 0.19514073]\n",
      " [ 0.08278294]\n",
      " [-0.34811622]]\n",
      "Current iteration=2, loss=46099.06558180521\n",
      "t [[ 0.10662966]\n",
      " [-0.63100381]\n",
      " [-0.36126725]\n",
      " ...\n",
      " [ 0.24830374]\n",
      " [ 0.10662966]\n",
      " [-0.46753234]]\n",
      "t [[ 0.10662966]\n",
      " [-0.63100381]\n",
      " [-0.36126725]\n",
      " ...\n",
      " [ 0.24830374]\n",
      " [ 0.10662966]\n",
      " [-0.46753234]]\n",
      "t [[ 0.12278153]\n",
      " [-0.80158399]\n",
      " [-0.45649151]\n",
      " ...\n",
      " [ 0.28397172]\n",
      " [ 0.12278153]\n",
      " [-0.56564955]]\n",
      "t [[ 0.12278153]\n",
      " [-0.80158399]\n",
      " [-0.45649151]\n",
      " ...\n",
      " [ 0.28397172]\n",
      " [ 0.12278153]\n",
      " [-0.56564955]]\n",
      "Current iteration=4, loss=42401.61558336635\n",
      "t [[ 0.13320238]\n",
      " [-0.95587786]\n",
      " [-0.54302321]\n",
      " ...\n",
      " [ 0.3074306 ]\n",
      " [ 0.13320238]\n",
      " [-0.64886424]]\n",
      "t [[ 0.13320238]\n",
      " [-0.95587786]\n",
      " [-0.54302321]\n",
      " ...\n",
      " [ 0.3074306 ]\n",
      " [ 0.13320238]\n",
      " [-0.64886424]]\n",
      "t [[ 0.1393096 ]\n",
      " [-1.09590002]\n",
      " [-0.6223165 ]\n",
      " ...\n",
      " [ 0.32222144]\n",
      " [ 0.1393096 ]\n",
      " [-0.72130914]]\n",
      "t [[ 0.1393096 ]\n",
      " [-1.09590002]\n",
      " [-0.6223165 ]\n",
      " ...\n",
      " [ 0.32222144]\n",
      " [ 0.1393096 ]\n",
      " [-0.72130914]]\n",
      "Current iteration=6, loss=39858.048486978\n",
      "t [[ 0.14214096]\n",
      " [-1.22343919]\n",
      " [-0.69547462]\n",
      " ...\n",
      " [ 0.33075122]\n",
      " [ 0.14214096]\n",
      " [-0.78570004]]\n",
      "t [[ 0.14214096]\n",
      " [-1.22343919]\n",
      " [-0.69547462]\n",
      " ...\n",
      " [ 0.33075122]\n",
      " [ 0.14214096]\n",
      " [-0.78570004]]\n",
      "t [[ 0.14246826]\n",
      " [-1.34005079]\n",
      " [-0.76335344]\n",
      " ...\n",
      " [ 0.33468401]\n",
      " [ 0.14246826]\n",
      " [-0.84386147]]\n",
      "t [[ 0.14246826]\n",
      " [-1.34005079]\n",
      " [-0.76335344]\n",
      " ...\n",
      " [ 0.33468401]\n",
      " [ 0.14246826]\n",
      " [-0.84386147]]\n",
      "Current iteration=8, loss=38009.951833771534\n",
      "t [[ 0.14087414]\n",
      " [-1.44707264]\n",
      " [-0.82663117]\n",
      " ...\n",
      " [ 0.33519266]\n",
      " [ 0.14087414]\n",
      " [-0.89705078]]\n",
      "t [[ 0.14087414]\n",
      " [-1.44707264]\n",
      " [-0.82663117]\n",
      " ...\n",
      " [ 0.33519266]\n",
      " [ 0.14087414]\n",
      " [-0.89705078]]\n",
      "t [[ 0.13780449]\n",
      " [-1.54564973]\n",
      " [-0.88585563]\n",
      " ...\n",
      " [ 0.3331204 ]\n",
      " [ 0.13780449]\n",
      " [-0.94615841]]\n",
      "loss=36612.40901336534\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.14645656]\n",
      " [-0.0853783 ]\n",
      " [-0.13664012]\n",
      " ...\n",
      " [ 0.11475343]\n",
      " [ 0.04830871]\n",
      " [-0.19442638]]\n",
      "t [[ 0.14645656]\n",
      " [-0.0853783 ]\n",
      " [-0.13664012]\n",
      " ...\n",
      " [ 0.11475343]\n",
      " [ 0.04830871]\n",
      " [-0.19442638]]\n",
      "t [[ 0.26718233]\n",
      " [-0.18972761]\n",
      " [-0.25611794]\n",
      " ...\n",
      " [ 0.19200068]\n",
      " [ 0.08235551]\n",
      " [-0.34217619]]\n",
      "t [[ 0.26718233]\n",
      " [-0.18972761]\n",
      " [-0.25611794]\n",
      " ...\n",
      " [ 0.19200068]\n",
      " [ 0.08235551]\n",
      " [-0.34217619]]\n",
      "Current iteration=2, loss=46088.125228068544\n",
      "t [[ 0.36747778]\n",
      " [-0.30148286]\n",
      " [-0.36214207]\n",
      " ...\n",
      " [ 0.24396281]\n",
      " [ 0.10594213]\n",
      " [-0.45877137]]\n",
      "t [[ 0.36747778]\n",
      " [-0.30148286]\n",
      " [-0.36214207]\n",
      " ...\n",
      " [ 0.24396281]\n",
      " [ 0.10594213]\n",
      " [-0.45877137]]\n",
      "t [[ 0.45156681]\n",
      " [-0.41397975]\n",
      " [-0.45743194]\n",
      " ...\n",
      " [ 0.27863172]\n",
      " [ 0.12182546]\n",
      " [-0.55425556]]\n",
      "t [[ 0.45156681]\n",
      " [-0.41397975]\n",
      " [-0.45743194]\n",
      " ...\n",
      " [ 0.27863172]\n",
      " [ 0.12182546]\n",
      " [-0.55425556]]\n",
      "Current iteration=4, loss=42388.74558072515\n",
      "t [[ 0.52269119]\n",
      " [-0.52365947]\n",
      " [-0.543973  ]\n",
      " ...\n",
      " [ 0.30126429]\n",
      " [ 0.13197796]\n",
      " [-0.635043  ]]\n",
      "t [[ 0.52269119]\n",
      " [-0.52365947]\n",
      " [-0.543973  ]\n",
      " ...\n",
      " [ 0.30126429]\n",
      " [ 0.13197796]\n",
      " [-0.635043  ]]\n",
      "t [[ 0.58333277]\n",
      " [-0.62877489]\n",
      " [-0.62323775]\n",
      " ...\n",
      " [ 0.31537439]\n",
      " [ 0.13782213]\n",
      " [-0.70526486]]\n",
      "t [[ 0.58333277]\n",
      " [-0.62877489]\n",
      " [-0.62323775]\n",
      " ...\n",
      " [ 0.31537439]\n",
      " [ 0.13782213]\n",
      " [-0.70526486]]\n",
      "Current iteration=6, loss=39845.115015725314\n",
      "t [[ 0.63541216]\n",
      " [-0.72859429]\n",
      " [-0.69634182]\n",
      " ...\n",
      " [ 0.32334494]\n",
      " [ 0.14039876]\n",
      " [-0.76762565]]\n",
      "t [[ 0.63541216]\n",
      " [-0.72859429]\n",
      " [-0.69634182]\n",
      " ...\n",
      " [ 0.32334494]\n",
      " [ 0.14039876]\n",
      " [-0.76762565]]\n",
      "t [[ 0.68043445]\n",
      " [-0.82294046]\n",
      " [-0.76414958]\n",
      " ...\n",
      " [ 0.32681967]\n",
      " [ 0.14048137]\n",
      " [-0.82393458]]\n",
      "t [[ 0.68043445]\n",
      " [-0.82294046]\n",
      " [-0.76414958]\n",
      " ...\n",
      " [ 0.32681967]\n",
      " [ 0.14048137]\n",
      " [-0.82393458]]\n",
      "Current iteration=8, loss=37997.287889085754\n",
      "t [[ 0.71959309]\n",
      " [-0.91192927]\n",
      " [-0.82734506]\n",
      " ...\n",
      " [ 0.32695457]\n",
      " [ 0.13865352]\n",
      " [-0.87543248]]\n",
      "t [[ 0.71959309]\n",
      " [-0.91192927]\n",
      " [-0.82734506]\n",
      " ...\n",
      " [ 0.32695457]\n",
      " [ 0.13865352]\n",
      " [-0.87543248]]\n",
      "t [[ 0.75384469]\n",
      " [-0.99582251]\n",
      " [-0.88648016]\n",
      " ...\n",
      " [ 0.32457904]\n",
      " [ 0.1353615 ]\n",
      " [-0.92299366]]\n",
      "loss=36599.96498976258\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.14457943]\n",
      " [-0.08173157]\n",
      " [-0.13612838]\n",
      " ...\n",
      " [ 0.11652392]\n",
      " [ 0.04919909]\n",
      " [-0.20041374]]\n",
      "t [[ 0.14457943]\n",
      " [-0.08173157]\n",
      " [-0.13612838]\n",
      " ...\n",
      " [ 0.11652392]\n",
      " [ 0.04919909]\n",
      " [-0.20041374]]\n",
      "t [[ 0.26376901]\n",
      " [-0.18252043]\n",
      " [-0.25509586]\n",
      " ...\n",
      " [ 0.19452834]\n",
      " [ 0.08396562]\n",
      " [-0.35274813]]\n",
      "t [[ 0.26376901]\n",
      " [-0.18252043]\n",
      " [-0.25509586]\n",
      " ...\n",
      " [ 0.19452834]\n",
      " [ 0.08396562]\n",
      " [-0.35274813]]\n",
      "Current iteration=2, loss=46137.08331484095\n",
      "t [[ 0.362809  ]\n",
      " [-0.29075152]\n",
      " [-0.3606343 ]\n",
      " ...\n",
      " [ 0.24660513]\n",
      " [ 0.10815328]\n",
      " [-0.47295746]]\n",
      "t [[ 0.362809  ]\n",
      " [-0.29075152]\n",
      " [-0.3606343 ]\n",
      " ...\n",
      " [ 0.24660513]\n",
      " [ 0.10815328]\n",
      " [-0.47295746]]\n",
      "t [[ 0.44587287]\n",
      " [-0.39975893]\n",
      " [-0.45547181]\n",
      " ...\n",
      " [ 0.28097551]\n",
      " [ 0.1245525 ]\n",
      " [-0.57136815]]\n",
      "t [[ 0.44587287]\n",
      " [-0.39975893]\n",
      " [-0.45547181]\n",
      " ...\n",
      " [ 0.28097551]\n",
      " [ 0.1245525 ]\n",
      " [-0.57136815]]\n",
      "Current iteration=4, loss=42480.61932036454\n",
      "t [[ 0.51616328]\n",
      " [-0.50599591]\n",
      " [-0.54159437]\n",
      " ...\n",
      " [ 0.30302526]\n",
      " [ 0.13515691]\n",
      " [-0.65457408]]\n",
      "t [[ 0.51616328]\n",
      " [-0.50599591]\n",
      " [-0.54159437]\n",
      " ...\n",
      " [ 0.30302526]\n",
      " [ 0.13515691]\n",
      " [-0.65457408]]\n",
      "t [[ 0.57612809]\n",
      " [-0.60773089]\n",
      " [-0.62047175]\n",
      " ...\n",
      " [ 0.31635099]\n",
      " [ 0.14140281]\n",
      " [-0.72682736]]\n",
      "t [[ 0.57612809]\n",
      " [-0.60773089]\n",
      " [-0.62047175]\n",
      " ...\n",
      " [ 0.31635099]\n",
      " [ 0.14140281]\n",
      " [-0.72682736]]\n",
      "Current iteration=6, loss=39970.33427601417\n",
      "t [[ 0.62765844]\n",
      " [-0.70424601]\n",
      " [-0.69321595]\n",
      " ...\n",
      " [ 0.32339374]\n",
      " [ 0.14434044]\n",
      " [-0.79091776]]\n",
      "t [[ 0.62765844]\n",
      " [-0.70424601]\n",
      " [-0.69321595]\n",
      " ...\n",
      " [ 0.32339374]\n",
      " [ 0.14434044]\n",
      " [-0.79091776]]\n",
      "t [[ 0.6722348 ]\n",
      " [-0.79537437]\n",
      " [-0.76068763]\n",
      " ...\n",
      " [ 0.32583916]\n",
      " [ 0.14475013]\n",
      " [-0.84871618]]\n",
      "t [[ 0.6722348 ]\n",
      " [-0.79537437]\n",
      " [-0.76068763]\n",
      " ...\n",
      " [ 0.32583916]\n",
      " [ 0.14475013]\n",
      " [-0.84871618]]\n",
      "Current iteration=8, loss=38148.35885630487\n",
      "t [[ 0.71103051]\n",
      " [-0.88123838]\n",
      " [-0.82356745]\n",
      " ...\n",
      " [ 0.32487373]\n",
      " [ 0.14322061]\n",
      " [-0.90150916]]\n",
      "t [[ 0.71103051]\n",
      " [-0.88123838]\n",
      " [-0.82356745]\n",
      " ...\n",
      " [ 0.32487373]\n",
      " [ 0.14322061]\n",
      " [-0.90150916]]\n",
      "t [[ 0.74498604]\n",
      " [-0.96210336]\n",
      " [-0.88240436]\n",
      " ...\n",
      " [ 0.32134922]\n",
      " [ 0.1402022 ]\n",
      " [-0.95020544]]\n",
      "loss=36771.438548724196\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.14434799]\n",
      " [-0.08586717]\n",
      " [-0.1357008 ]\n",
      " ...\n",
      " [ 0.2079065 ]\n",
      " [-0.1357008 ]\n",
      " [ 0.19473389]]\n",
      "t [[ 0.14434799]\n",
      " [-0.08586717]\n",
      " [-0.1357008 ]\n",
      " ...\n",
      " [ 0.2079065 ]\n",
      " [-0.1357008 ]\n",
      " [ 0.19473389]]\n",
      "t [[ 0.26334447]\n",
      " [-0.18975741]\n",
      " [-0.25438901]\n",
      " ...\n",
      " [ 0.37477109]\n",
      " [-0.25438901]\n",
      " [ 0.34438355]]\n",
      "t [[ 0.26334447]\n",
      " [-0.18975741]\n",
      " [-0.25438901]\n",
      " ...\n",
      " [ 0.37477109]\n",
      " [-0.25438901]\n",
      " [ 0.34438355]]\n",
      "Current iteration=2, loss=46141.774126530276\n",
      "t [[ 0.36222973]\n",
      " [-0.30045107]\n",
      " [-0.35973136]\n",
      " ...\n",
      " [ 0.51042753]\n",
      " [-0.35973136]\n",
      " [ 0.46120751]]\n",
      "t [[ 0.36222973]\n",
      " [-0.30045107]\n",
      " [-0.35973136]\n",
      " ...\n",
      " [ 0.51042753]\n",
      " [-0.35973136]\n",
      " [ 0.46120751]]\n",
      "t [[ 0.44512173]\n",
      " [-0.41156055]\n",
      " [-0.45442089]\n",
      " ...\n",
      " [ 0.62214261]\n",
      " [-0.45442089]\n",
      " [ 0.55388008]]\n",
      "t [[ 0.44512173]\n",
      " [-0.41156055]\n",
      " [-0.45442089]\n",
      " ...\n",
      " [ 0.62214261]\n",
      " [-0.45442089]\n",
      " [ 0.55388008]]\n",
      "Current iteration=4, loss=42478.14102346526\n",
      "t [[ 0.51519076]\n",
      " [-0.51970033]\n",
      " [-0.54042388]\n",
      " ...\n",
      " [ 0.71522334]\n",
      " [-0.54042388]\n",
      " [ 0.62847151]]\n",
      "t [[ 0.51519076]\n",
      " [-0.51970033]\n",
      " [-0.54042388]\n",
      " ...\n",
      " [ 0.71522334]\n",
      " [-0.54042388]\n",
      " [ 0.62847151]]\n",
      "t [[ 0.57488034]\n",
      " [-0.62322053]\n",
      " [-0.61919855]\n",
      " ...\n",
      " [ 0.79358154]\n",
      " [-0.61919855]\n",
      " [ 0.68927829]]\n",
      "t [[ 0.57488034]\n",
      " [-0.62322053]\n",
      " [-0.61919855]\n",
      " ...\n",
      " [ 0.79358154]\n",
      " [-0.61919855]\n",
      " [ 0.68927829]]\n",
      "Current iteration=6, loss=39960.44385370536\n",
      "t [[ 0.6260893 ]\n",
      " [-0.72144283]\n",
      " [-0.69185016]\n",
      " ...\n",
      " [ 0.86014528]\n",
      " [-0.69185016]\n",
      " [ 0.73939754]]\n",
      "t [[ 0.6260893 ]\n",
      " [-0.72144283]\n",
      " [-0.69185016]\n",
      " ...\n",
      " [ 0.86014528]\n",
      " [-0.69185016]\n",
      " [ 0.73939754]]\n",
      "t [[ 0.67030841]\n",
      " [-0.81421901]\n",
      " [-0.75923561]\n",
      " ...\n",
      " [ 0.91714276]\n",
      " [-0.75923561]\n",
      " [ 0.78110604]]\n",
      "t [[ 0.67030841]\n",
      " [-0.81421901]\n",
      " [-0.75923561]\n",
      " ...\n",
      " [ 0.91714276]\n",
      " [-0.75923561]\n",
      " [ 0.78110604]]\n",
      "Current iteration=8, loss=38132.313020797286\n",
      "t [[ 0.70872054]\n",
      " [-0.90168002]\n",
      " [-0.82203354]\n",
      " ...\n",
      " [ 0.96629682]\n",
      " [-0.82203354]\n",
      " [ 0.81610967]]\n",
      "t [[ 0.70872054]\n",
      " [-0.90168002]\n",
      " [-0.82203354]\n",
      " ...\n",
      " [ 0.96629682]\n",
      " [-0.82203354]\n",
      " [ 0.81610967]]\n",
      "t [[ 0.74227397]\n",
      " [-0.98409479]\n",
      " [-0.88079181]\n",
      " ...\n",
      " [ 1.00895922]\n",
      " [-0.88079181]\n",
      " [ 0.84570908]]\n",
      "loss=36750.40792506648\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05036448]\n",
      " [-0.24117019]\n",
      " [-0.14142497]\n",
      " ...\n",
      " [ 0.12093712]\n",
      " [ 0.05036448]\n",
      " [-0.20500247]]\n",
      "t [[ 0.05036448]\n",
      " [-0.24117019]\n",
      " [-0.14142497]\n",
      " ...\n",
      " [ 0.12093712]\n",
      " [ 0.05036448]\n",
      " [-0.20500247]]\n",
      "t [[ 0.08540505]\n",
      " [-0.45805795]\n",
      " [-0.26454287]\n",
      " ...\n",
      " [ 0.20115207]\n",
      " [ 0.08540505]\n",
      " [-0.35965953]]\n",
      "t [[ 0.08540505]\n",
      " [-0.45805795]\n",
      " [-0.26454287]\n",
      " ...\n",
      " [ 0.20115207]\n",
      " [ 0.08540505]\n",
      " [-0.35965953]]\n",
      "Current iteration=2, loss=45915.59940448812\n",
      "t [[ 0.10935526]\n",
      " [-0.65275407]\n",
      " [-0.37344684]\n",
      " ...\n",
      " [ 0.25436599]\n",
      " [ 0.10935526]\n",
      " [-0.48123128]]\n",
      "t [[ 0.10935526]\n",
      " [-0.65275407]\n",
      " [-0.37344684]\n",
      " ...\n",
      " [ 0.25436599]\n",
      " [ 0.10935526]\n",
      " [-0.48123128]]\n",
      "t [[ 0.12523489]\n",
      " [-0.82772833]\n",
      " [-0.47109002]\n",
      " ...\n",
      " [ 0.28937125]\n",
      " [ 0.12523489]\n",
      " [-0.58065129]]\n",
      "t [[ 0.12523489]\n",
      " [-0.82772833]\n",
      " [-0.47109002]\n",
      " ...\n",
      " [ 0.28937125]\n",
      " [ 0.12523489]\n",
      " [-0.58065129]]\n",
      "Current iteration=4, loss=42160.72828757731\n",
      "t [[ 0.13517299]\n",
      " [-0.98541897]\n",
      " [-0.55960029]\n",
      " ...\n",
      " [ 0.31185116]\n",
      " [ 0.13517299]\n",
      " [-0.66477007]]\n",
      "t [[ 0.13517299]\n",
      " [-0.98541897]\n",
      " [-0.55960029]\n",
      " ...\n",
      " [ 0.31185116]\n",
      " [ 0.13517299]\n",
      " [-0.66477007]]\n",
      "t [[ 0.14068613]\n",
      " [-1.12805406]\n",
      " [-0.64054036]\n",
      " ...\n",
      " [ 0.32554849]\n",
      " [ 0.14068613]\n",
      " [-0.73793031]]\n",
      "t [[ 0.14068613]\n",
      " [-1.12805406]\n",
      " [-0.64054036]\n",
      " ...\n",
      " [ 0.32554849]\n",
      " [ 0.14068613]\n",
      " [-0.73793031]]\n",
      "Current iteration=6, loss=39603.860289885415\n",
      "t [[ 0.14287258]\n",
      " [-1.25759341]\n",
      " [-0.71508605]\n",
      " ...\n",
      " [ 0.3329695 ]\n",
      " [ 0.14287258]\n",
      " [-0.80294011]]\n",
      "t [[ 0.14287258]\n",
      " [-1.25759341]\n",
      " [-0.71508605]\n",
      " ...\n",
      " [ 0.3329695 ]\n",
      " [ 0.14287258]\n",
      " [-0.80294011]]\n",
      "t [[ 0.14254153]\n",
      " [-1.37572616]\n",
      " [-0.784144  ]\n",
      " ...\n",
      " [ 0.33582538]\n",
      " [ 0.14254153]\n",
      " [-0.86166179]]\n",
      "t [[ 0.14254153]\n",
      " [-1.37572616]\n",
      " [-0.784144  ]\n",
      " ...\n",
      " [ 0.33582538]\n",
      " [ 0.14254153]\n",
      " [-0.86166179]]\n",
      "Current iteration=8, loss=37759.84213558206\n",
      "t [[ 0.1402989 ]\n",
      " [-1.48389348]\n",
      " [-0.84842942]\n",
      " ...\n",
      " [ 0.33531002]\n",
      " [ 0.1402989 ]\n",
      " [-0.9153664 ]]\n",
      "t [[ 0.1402989 ]\n",
      " [-1.48389348]\n",
      " [-0.84842942]\n",
      " ...\n",
      " [ 0.33531002]\n",
      " [ 0.1402989 ]\n",
      " [-0.9153664 ]]\n",
      "t [[ 0.13660505]\n",
      " [-1.58332008]\n",
      " [-0.90851819]\n",
      " ...\n",
      " [ 0.3322748 ]\n",
      " [ 0.13660505]\n",
      " [-0.96494817]]\n",
      "loss=36373.25708009602\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.1520895 ]\n",
      " [-0.08866208]\n",
      " [-0.14189551]\n",
      " ...\n",
      " [ 0.11916702]\n",
      " [ 0.05016674]\n",
      " [-0.20190432]]\n",
      "t [[ 0.1520895 ]\n",
      " [-0.08866208]\n",
      " [-0.14189551]\n",
      " ...\n",
      " [ 0.11916702]\n",
      " [ 0.05016674]\n",
      " [-0.20190432]]\n",
      "t [[ 0.27644193]\n",
      " [-0.19777304]\n",
      " [-0.26528945]\n",
      " ...\n",
      " [ 0.1979018 ]\n",
      " [ 0.08495936]\n",
      " [-0.35349222]]\n",
      "t [[ 0.27644193]\n",
      " [-0.19777304]\n",
      " [-0.26528945]\n",
      " ...\n",
      " [ 0.1979018 ]\n",
      " [ 0.08495936]\n",
      " [-0.35349222]]\n",
      "Current iteration=2, loss=45904.46069959242\n",
      "t [[ 0.3790019 ]\n",
      " [-0.31439134]\n",
      " [-0.37433868]\n",
      " ...\n",
      " [ 0.24988696]\n",
      " [ 0.10863772]\n",
      " [-0.47214547]]\n",
      "t [[ 0.3790019 ]\n",
      " [-0.31439134]\n",
      " [-0.37433868]\n",
      " ...\n",
      " [ 0.24988696]\n",
      " [ 0.10863772]\n",
      " [-0.47214547]]\n",
      "t [[ 0.46444056]\n",
      " [-0.43128368]\n",
      " [-0.47204039]\n",
      " ...\n",
      " [ 0.28387844]\n",
      " [ 0.12423752]\n",
      " [-0.56885356]]\n",
      "t [[ 0.46444056]\n",
      " [-0.43128368]\n",
      " [-0.47204039]\n",
      " ...\n",
      " [ 0.28387844]\n",
      " [ 0.12423752]\n",
      " [-0.56885356]]\n",
      "Current iteration=4, loss=42147.805163867175\n",
      "t [[ 0.53629517]\n",
      " [-0.54472825]\n",
      " [-0.56055179]\n",
      " ...\n",
      " [ 0.3055271 ]\n",
      " [ 0.13389697]\n",
      " [-0.65048402]]\n",
      "t [[ 0.53629517]\n",
      " [-0.54472825]\n",
      " [-0.56055179]\n",
      " ...\n",
      " [ 0.3055271 ]\n",
      " [ 0.13389697]\n",
      " [-0.65048402]]\n",
      "t [[ 0.59724497]\n",
      " [-0.65298816]\n",
      " [-0.64145498]\n",
      " ...\n",
      " [ 0.31854562]\n",
      " [ 0.13913797]\n",
      " [-0.72137552]]\n",
      "t [[ 0.59724497]\n",
      " [-0.65298816]\n",
      " [-0.64145498]\n",
      " ...\n",
      " [ 0.31854562]\n",
      " [ 0.13913797]\n",
      " [-0.72137552]]\n",
      "Current iteration=6, loss=39590.95116059354\n",
      "t [[ 0.64934513]\n",
      " [-0.75540402]\n",
      " [-0.71593872]\n",
      " ...\n",
      " [ 0.32541395]\n",
      " [ 0.14106185]\n",
      " [-0.78432211]]\n",
      "t [[ 0.64934513]\n",
      " [-0.75540402]\n",
      " [-0.71593872]\n",
      " ...\n",
      " [ 0.32541395]\n",
      " [ 0.14106185]\n",
      " [-0.78432211]]\n",
      "t [[ 0.69419404]\n",
      " [-0.85188485]\n",
      " [-0.78491833]\n",
      " ...\n",
      " [ 0.32782137]\n",
      " [ 0.14047945]\n",
      " [-0.84116824]]\n",
      "t [[ 0.69419404]\n",
      " [-0.85188485]\n",
      " [-0.78491833]\n",
      " ...\n",
      " [ 0.32782137]\n",
      " [ 0.14047945]\n",
      " [-0.84116824]]\n",
      "Current iteration=8, loss=37747.22339067304\n",
      "t [[ 0.73305062]\n",
      " [-0.94262749]\n",
      " [-0.84911495]\n",
      " ...\n",
      " [ 0.3269438 ]\n",
      " [ 0.13799754]\n",
      " [-0.89316629]]\n",
      "t [[ 0.73305062]\n",
      " [-0.94262749]\n",
      " [-0.84911495]\n",
      " ...\n",
      " [ 0.3269438 ]\n",
      " [ 0.13799754]\n",
      " [-0.89316629]]\n",
      "t [[ 0.76691792]\n",
      " [-1.02796322]\n",
      " [-0.90910848]\n",
      " ...\n",
      " [ 0.32361801]\n",
      " [ 0.13407676]\n",
      " [-0.94119262]]\n",
      "loss=36360.84639566728\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.15014017]\n",
      " [-0.08487509]\n",
      " [-0.14136409]\n",
      " ...\n",
      " [ 0.12100561]\n",
      " [ 0.05109136]\n",
      " [-0.20812196]]\n",
      "t [[ 0.15014017]\n",
      " [-0.08487509]\n",
      " [-0.14136409]\n",
      " ...\n",
      " [ 0.12100561]\n",
      " [ 0.05109136]\n",
      " [-0.20812196]]\n",
      "t [[ 0.27291072]\n",
      " [-0.19029208]\n",
      " [-0.26422822]\n",
      " ...\n",
      " [ 0.20048765]\n",
      " [ 0.08662476]\n",
      " [-0.36441608]]\n",
      "t [[ 0.27291072]\n",
      " [-0.19029208]\n",
      " [-0.26422822]\n",
      " ...\n",
      " [ 0.20048765]\n",
      " [ 0.08662476]\n",
      " [-0.36441608]]\n",
      "Current iteration=2, loss=45955.2867376193\n",
      "t [[ 0.37418873]\n",
      " [-0.30325367]\n",
      " [-0.3727752 ]\n",
      " ...\n",
      " [ 0.25254353]\n",
      " [ 0.11091817]\n",
      " [-0.48674817]]\n",
      "t [[ 0.37418873]\n",
      " [-0.30325367]\n",
      " [-0.3727752 ]\n",
      " ...\n",
      " [ 0.25254353]\n",
      " [ 0.11091817]\n",
      " [-0.48674817]]\n",
      "t [[ 0.45858974]\n",
      " [-0.41652629]\n",
      " [-0.47001114]\n",
      " ...\n",
      " [ 0.28617719]\n",
      " [ 0.12704389]\n",
      " [-0.58641618]]\n",
      "t [[ 0.45858974]\n",
      " [-0.41652629]\n",
      " [-0.47001114]\n",
      " ...\n",
      " [ 0.28617719]\n",
      " [ 0.12704389]\n",
      " [-0.58641618]]\n",
      "Current iteration=4, loss=42242.66579865062\n",
      "t [[ 0.52960789]\n",
      " [-0.52640288]\n",
      " [-0.55809317]\n",
      " ...\n",
      " [ 0.30717634]\n",
      " [ 0.13716273]\n",
      " [-0.67048013]]\n",
      "t [[ 0.52960789]\n",
      " [-0.52640288]\n",
      " [-0.55809317]\n",
      " ...\n",
      " [ 0.30717634]\n",
      " [ 0.13716273]\n",
      " [-0.67048013]]\n",
      "t [[ 0.58988496]\n",
      " [-0.63116434]\n",
      " [-0.63860006]\n",
      " ...\n",
      " [ 0.31934121]\n",
      " [ 0.14281114]\n",
      " [-0.74340693]]\n",
      "t [[ 0.58988496]\n",
      " [-0.63116434]\n",
      " [-0.63860006]\n",
      " ...\n",
      " [ 0.31934121]\n",
      " [ 0.14281114]\n",
      " [-0.74340693]]\n",
      "Current iteration=6, loss=39719.617657234965\n",
      "t [[ 0.64144406]\n",
      " [-0.73016649]\n",
      " [-0.71271633]\n",
      " ...\n",
      " [ 0.32521282]\n",
      " [ 0.14510032]\n",
      " [-0.80808016]]\n",
      "t [[ 0.64144406]\n",
      " [-0.73016649]\n",
      " [-0.71271633]\n",
      " ...\n",
      " [ 0.32521282]\n",
      " [ 0.14510032]\n",
      " [-0.80808016]]\n",
      "t [[ 0.68585712]\n",
      " [-0.82332907]\n",
      " [-0.78135321]\n",
      " ...\n",
      " [ 0.32652398]\n",
      " [ 0.14484829]\n",
      " [-0.86640854]]\n",
      "t [[ 0.68585712]\n",
      " [-0.82332907]\n",
      " [-0.78135321]\n",
      " ...\n",
      " [ 0.32652398]\n",
      " [ 0.14484829]\n",
      " [-0.86640854]]\n",
      "Current iteration=8, loss=37901.87544758065\n",
      "t [[ 0.72436181]\n",
      " [-0.91085539]\n",
      " [-0.84522817]\n",
      " ...\n",
      " [ 0.32448192]\n",
      " [ 0.14266716]\n",
      " [-0.91969169]]\n",
      "t [[ 0.72436181]\n",
      " [-0.91085539]\n",
      " [-0.84522817]\n",
      " ...\n",
      " [ 0.32448192]\n",
      " [ 0.14266716]\n",
      " [-0.91969169]]\n",
      "t [[ 0.75794424]\n",
      " [-0.99307984]\n",
      " [-0.90491793]\n",
      " ...\n",
      " [ 0.31994618]\n",
      " [ 0.13902179]\n",
      " [-0.96884135]]\n",
      "loss=36535.87739764879\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.14989984]\n",
      " [-0.08916976]\n",
      " [-0.14092006]\n",
      " ...\n",
      " [ 0.21590291]\n",
      " [-0.14092006]\n",
      " [ 0.20222366]]\n",
      "t [[ 0.14989984]\n",
      " [-0.08916976]\n",
      " [-0.14092006]\n",
      " ...\n",
      " [ 0.21590291]\n",
      " [-0.14092006]\n",
      " [ 0.20222366]]\n",
      "t [[ 0.27247066]\n",
      " [-0.19776729]\n",
      " [-0.26349995]\n",
      " ...\n",
      " [ 0.38756259]\n",
      " [-0.26349995]\n",
      " [ 0.35584709]]\n",
      "t [[ 0.27247066]\n",
      " [-0.19776729]\n",
      " [-0.26349995]\n",
      " ...\n",
      " [ 0.38756259]\n",
      " [-0.26349995]\n",
      " [ 0.35584709]]\n",
      "Current iteration=2, loss=45959.834498443\n",
      "t [[ 0.37358781]\n",
      " [-0.31323952]\n",
      " [-0.37184966]\n",
      " ...\n",
      " [ 0.52599996]\n",
      " [-0.37184966]\n",
      " [ 0.47460473]]\n",
      "t [[ 0.37358781]\n",
      " [-0.31323952]\n",
      " [-0.37184966]\n",
      " ...\n",
      " [ 0.52599996]\n",
      " [-0.37184966]\n",
      " [ 0.47460473]]\n",
      "t [[ 0.45780494]\n",
      " [-0.42865682]\n",
      " [-0.46893739]\n",
      " ...\n",
      " [ 0.63922562]\n",
      " [-0.46893739]\n",
      " [ 0.56803583]]\n",
      "t [[ 0.45780494]\n",
      " [-0.42865682]\n",
      " [-0.46893739]\n",
      " ...\n",
      " [ 0.63922562]\n",
      " [-0.46893739]\n",
      " [ 0.56803583]]\n",
      "Current iteration=4, loss=42239.59480231095\n",
      "t [[ 0.52858392]\n",
      " [-0.54048095]\n",
      " [-0.55689949]\n",
      " ...\n",
      " [ 0.73300498]\n",
      " [-0.55689949]\n",
      " [ 0.6427058 ]]\n",
      "t [[ 0.52858392]\n",
      " [-0.54048095]\n",
      " [-0.55689949]\n",
      " ...\n",
      " [ 0.73300498]\n",
      " [-0.55689949]\n",
      " [ 0.6427058 ]]\n",
      "t [[ 0.58856465]\n",
      " [-0.64707484]\n",
      " [-0.6373028 ]\n",
      " ...\n",
      " [ 0.81153704]\n",
      " [-0.6373028 ]\n",
      " [ 0.70319822]]\n",
      "t [[ 0.58856465]\n",
      " [-0.64707484]\n",
      " [-0.6373028 ]\n",
      " ...\n",
      " [ 0.81153704]\n",
      " [-0.6373028 ]\n",
      " [ 0.70319822]]\n",
      "Current iteration=6, loss=39708.93978217013\n",
      "t [[ 0.63977997]\n",
      " [-0.74783233]\n",
      " [-0.71132515]\n",
      " ...\n",
      " [ 0.87793417]\n",
      " [-0.71132515]\n",
      " [ 0.75278045]]\n",
      "t [[ 0.63977997]\n",
      " [-0.74783233]\n",
      " [-0.71132515]\n",
      " ...\n",
      " [ 0.87793417]\n",
      " [-0.71132515]\n",
      " [ 0.75278045]]\n",
      "t [[ 0.68381337]\n",
      " [-0.84269026]\n",
      " [-0.77987415]\n",
      " ...\n",
      " [ 0.93454569]\n",
      " [-0.77987415]\n",
      " [ 0.79383265]]\n",
      "t [[ 0.68381337]\n",
      " [-0.84269026]\n",
      " [-0.77987415]\n",
      " ...\n",
      " [ 0.93454569]\n",
      " [-0.77987415]\n",
      " [ 0.79383265]]\n",
      "Current iteration=8, loss=37884.97125936548\n",
      "t [[ 0.7219127 ]\n",
      " [-0.93185925]\n",
      " [-0.84366538]\n",
      " ...\n",
      " [ 0.9831758 ]\n",
      " [-0.84366538]\n",
      " [ 0.82812481]]\n",
      "t [[ 0.7219127 ]\n",
      " [-0.93185925]\n",
      " [-0.84366538]\n",
      " ...\n",
      " [ 0.9831758 ]\n",
      " [-0.84366538]\n",
      " [ 0.82812481]]\n",
      "t [[ 0.75507224]\n",
      " [-1.01567646]\n",
      " [-0.90327464]\n",
      " ...\n",
      " [ 1.02523188]\n",
      " [-0.90327464]\n",
      " [ 0.85699805]]\n",
      "loss=36513.977680084296\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05222983]\n",
      " [-0.25010242]\n",
      " [-0.14666293]\n",
      " ...\n",
      " [ 0.12541628]\n",
      " [ 0.05222983]\n",
      " [-0.21259515]]\n",
      "t [[ 0.05222983]\n",
      " [-0.25010242]\n",
      " [-0.14666293]\n",
      " ...\n",
      " [ 0.12541628]\n",
      " [ 0.05222983]\n",
      " [-0.21259515]]\n",
      "t [[ 0.08798623]\n",
      " [-0.47409708]\n",
      " [-0.27364449]\n",
      " ...\n",
      " [ 0.20705445]\n",
      " [ 0.08798623]\n",
      " [-0.3710684 ]]\n",
      "t [[ 0.08798623]\n",
      " [-0.47409708]\n",
      " [-0.27364449]\n",
      " ...\n",
      " [ 0.20705445]\n",
      " [ 0.08798623]\n",
      " [-0.3710684 ]]\n",
      "Current iteration=2, loss=45734.74013373561\n",
      "t [[ 0.1119929 ]\n",
      " [-0.674327  ]\n",
      " [-0.3855132 ]\n",
      " ...\n",
      " [ 0.26021221]\n",
      " [ 0.1119929 ]\n",
      " [-0.49466685]]\n",
      "t [[ 0.1119929 ]\n",
      " [-0.674327  ]\n",
      " [-0.3855132 ]\n",
      " ...\n",
      " [ 0.26021221]\n",
      " [ 0.1119929 ]\n",
      " [-0.49466685]]\n",
      "t [[ 0.12756022]\n",
      " [-0.85355911]\n",
      " [-0.48550934]\n",
      " ...\n",
      " [ 0.29447896]\n",
      " [ 0.12756022]\n",
      " [-0.59529909]]\n",
      "t [[ 0.12756022]\n",
      " [-0.85355911]\n",
      " [-0.48550934]\n",
      " ...\n",
      " [ 0.29447896]\n",
      " [ 0.12756022]\n",
      " [-0.59529909]]\n",
      "Current iteration=4, loss=41926.16203877263\n",
      "t [[ 0.13698587]\n",
      " [-1.01450253]\n",
      " [-0.57593479]\n",
      " ...\n",
      " [ 0.31593616]\n",
      " [ 0.13698587]\n",
      " [-0.68026597]]\n",
      "t [[ 0.13698587]\n",
      " [-1.01450253]\n",
      " [-0.57593479]\n",
      " ...\n",
      " [ 0.31593616]\n",
      " [ 0.13698587]\n",
      " [-0.68026597]]\n",
      "t [[ 0.14188551]\n",
      " [-1.15960952]\n",
      " [-0.65846191]\n",
      " ...\n",
      " [ 0.32852094]\n",
      " [ 0.14188551]\n",
      " [-0.75410845]]\n",
      "t [[ 0.14188551]\n",
      " [-1.15960952]\n",
      " [-0.65846191]\n",
      " ...\n",
      " [ 0.32852094]\n",
      " [ 0.14188551]\n",
      " [-0.75410845]]\n",
      "Current iteration=6, loss=39358.47010551549\n",
      "t [[ 0.14341665]\n",
      " [-1.29101685]\n",
      " [-0.73433906]\n",
      " ...\n",
      " [ 0.33483108]\n",
      " [ 0.14341665]\n",
      " [-0.81971671]]\n",
      "t [[ 0.14341665]\n",
      " [-1.29101685]\n",
      " [-0.73433906]\n",
      " ...\n",
      " [ 0.33483108]\n",
      " [ 0.14341665]\n",
      " [-0.81971671]]\n",
      "t [[ 0.14242438]\n",
      " [-1.41055014]\n",
      " [-0.80452336]\n",
      " ...\n",
      " [ 0.33661897]\n",
      " [ 0.14242438]\n",
      " [-0.8789839 ]]\n",
      "t [[ 0.14242438]\n",
      " [-1.41055014]\n",
      " [-0.80452336]\n",
      " ...\n",
      " [ 0.33661897]\n",
      " [ 0.14242438]\n",
      " [-0.8789839 ]]\n",
      "Current iteration=8, loss=37519.94268233995\n",
      "t [[ 0.13953655]\n",
      " [-1.51975476]\n",
      " [-0.86976679]\n",
      " ...\n",
      " [ 0.33509553]\n",
      " [ 0.13953655]\n",
      " [-0.93319067]]\n",
      "t [[ 0.13953655]\n",
      " [-1.51975476]\n",
      " [-0.86976679]\n",
      " ...\n",
      " [ 0.33509553]\n",
      " [ 0.13953655]\n",
      " [-0.93319067]]\n",
      "t [[ 0.13522678]\n",
      " [-1.61993488]\n",
      " [-0.930673  ]\n",
      " ...\n",
      " [ 0.33111773]\n",
      " [ 0.13522678]\n",
      " [-0.9832329 ]]\n",
      "loss=36145.019337579964\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.15772245]\n",
      " [-0.09194586]\n",
      " [-0.1471509 ]\n",
      " ...\n",
      " [ 0.12358062]\n",
      " [ 0.05202477]\n",
      " [-0.20938225]]\n",
      "t [[ 0.15772245]\n",
      " [-0.09194586]\n",
      " [-0.1471509 ]\n",
      " ...\n",
      " [ 0.12358062]\n",
      " [ 0.05202477]\n",
      " [-0.20938225]]\n",
      "t [[ 0.28562748]\n",
      " [-0.2058728 ]\n",
      " [-0.27441138]\n",
      " ...\n",
      " [ 0.20369473]\n",
      " [ 0.08752216]\n",
      " [-0.36467388]]\n",
      "t [[ 0.28562748]\n",
      " [-0.2058728 ]\n",
      " [-0.27441138]\n",
      " ...\n",
      " [ 0.20369473]\n",
      " [ 0.08752216]\n",
      " [-0.36467388]]\n",
      "Current iteration=2, loss=45723.41473670116\n",
      "t [[ 0.39035381]\n",
      " [-0.32735978]\n",
      " [-0.38642103]\n",
      " ...\n",
      " [ 0.25559706]\n",
      " [ 0.11124516]\n",
      " [-0.48525744]]\n",
      "t [[ 0.39035381]\n",
      " [-0.32735978]\n",
      " [-0.38642103]\n",
      " ...\n",
      " [ 0.25559706]\n",
      " [ 0.11124516]\n",
      " [-0.48525744]]\n",
      "t [[ 0.47704378]\n",
      " [-0.44859547]\n",
      " [-0.48646829]\n",
      " ...\n",
      " [ 0.28883674]\n",
      " [ 0.1265214 ]\n",
      " [-0.58310089]]\n",
      "t [[ 0.47704378]\n",
      " [-0.44859547]\n",
      " [-0.48646829]\n",
      " ...\n",
      " [ 0.28883674]\n",
      " [ 0.1265214 ]\n",
      " [-0.58310089]]\n",
      "Current iteration=4, loss=41913.19677995751\n",
      "t [[ 0.54954003]\n",
      " [-0.56571315]\n",
      " [-0.57688647]\n",
      " ...\n",
      " [ 0.30945916]\n",
      " [ 0.13565826]\n",
      " [-0.66552096]]\n",
      "t [[ 0.54954003]\n",
      " [-0.56571315]\n",
      " [-0.57688647]\n",
      " ...\n",
      " [ 0.30945916]\n",
      " [ 0.13565826]\n",
      " [-0.66552096]]\n",
      "t [[ 0.61072264]\n",
      " [-0.67700532]\n",
      " [-0.65936836]\n",
      " ...\n",
      " [ 0.32136826]\n",
      " [ 0.14027691]\n",
      " [-0.73705176]]\n",
      "t [[ 0.61072264]\n",
      " [-0.67700532]\n",
      " [-0.65936836]\n",
      " ...\n",
      " [ 0.32136826]\n",
      " [ 0.14027691]\n",
      " [-0.73705176]]\n",
      "Current iteration=6, loss=39345.58994808545\n",
      "t [[ 0.66278199]\n",
      " [-0.78189886]\n",
      " [-0.73517575]\n",
      " ...\n",
      " [ 0.32713326]\n",
      " [ 0.14153788]\n",
      " [-0.80056644]]\n",
      "t [[ 0.66278199]\n",
      " [-0.78189886]\n",
      " [-0.73517575]\n",
      " ...\n",
      " [ 0.32713326]\n",
      " [ 0.14153788]\n",
      " [-0.80056644]]\n",
      "t [[ 0.70740898]\n",
      " [-0.88039755]\n",
      " [-0.80527461]\n",
      " ...\n",
      " [ 0.32848305]\n",
      " [ 0.14028791]\n",
      " [-0.85793756]]\n",
      "t [[ 0.70740898]\n",
      " [-0.88039755]\n",
      " [-0.80527461]\n",
      " ...\n",
      " [ 0.32848305]\n",
      " [ 0.14028791]\n",
      " [-0.85793756]]\n",
      "Current iteration=8, loss=37507.36806543239\n",
      "t [[ 0.74592677]\n",
      " [-0.97278384]\n",
      " [-0.87042288]\n",
      " ...\n",
      " [ 0.32660949]\n",
      " [ 0.13715556]\n",
      " [-0.91042488]]\n",
      "t [[ 0.74592677]\n",
      " [-0.97278384]\n",
      " [-0.87042288]\n",
      " ...\n",
      " [ 0.32660949]\n",
      " [ 0.13715556]\n",
      " [-0.91042488]]\n",
      "t [[ 0.77938339]\n",
      " [-1.05946064]\n",
      " [-0.9312282 ]\n",
      " ...\n",
      " [ 0.32235418]\n",
      " [ 0.13261466]\n",
      " [-0.95890465]]\n",
      "loss=36132.63689949842\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.15570092]\n",
      " [-0.08801862]\n",
      " [-0.1465998 ]\n",
      " ...\n",
      " [ 0.1254873 ]\n",
      " [ 0.05298363]\n",
      " [-0.21583018]]\n",
      "t [[ 0.15570092]\n",
      " [-0.08801862]\n",
      " [-0.1465998 ]\n",
      " ...\n",
      " [ 0.1254873 ]\n",
      " [ 0.05298363]\n",
      " [-0.21583018]]\n",
      "t [[ 0.28197936]\n",
      " [-0.19811832]\n",
      " [-0.27331103]\n",
      " ...\n",
      " [ 0.20633603]\n",
      " [ 0.08924237]\n",
      " [-0.37594577]]\n",
      "t [[ 0.28197936]\n",
      " [-0.19811832]\n",
      " [-0.27331103]\n",
      " ...\n",
      " [ 0.20633603]\n",
      " [ 0.08924237]\n",
      " [-0.37594577]]\n",
      "Current iteration=2, loss=45776.10235108503\n",
      "t [[ 0.38539859]\n",
      " [-0.31581608]\n",
      " [-0.38480209]\n",
      " ...\n",
      " [ 0.2582628 ]\n",
      " [ 0.11359395]\n",
      " [-0.50026885]]\n",
      "t [[ 0.38539859]\n",
      " [-0.31581608]\n",
      " [-0.38480209]\n",
      " ...\n",
      " [ 0.2582628 ]\n",
      " [ 0.11359395]\n",
      " [-0.50026885]]\n",
      "t [[ 0.47103997]\n",
      " [-0.43330206]\n",
      " [-0.48437049]\n",
      " ...\n",
      " [ 0.29108363]\n",
      " [ 0.1294057 ]\n",
      " [-0.60110231]]\n",
      "t [[ 0.47103997]\n",
      " [-0.43330206]\n",
      " [-0.48437049]\n",
      " ...\n",
      " [ 0.29108363]\n",
      " [ 0.1294057 ]\n",
      " [-0.60110231]]\n",
      "Current iteration=4, loss=42010.9932747375\n",
      "t [[ 0.54269866]\n",
      " [-0.54672723]\n",
      " [-0.57434881]\n",
      " ...\n",
      " [ 0.31098873]\n",
      " [ 0.13900905]\n",
      " [-0.68596825]]\n",
      "t [[ 0.54269866]\n",
      " [-0.54672723]\n",
      " [-0.57434881]\n",
      " ...\n",
      " [ 0.31098873]\n",
      " [ 0.13900905]\n",
      " [-0.68596825]]\n",
      "t [[ 0.60321369]\n",
      " [-0.65440427]\n",
      " [-0.65642577]\n",
      " ...\n",
      " [ 0.32197445]\n",
      " [ 0.14404043]\n",
      " [-0.75953626]]\n",
      "t [[ 0.60321369]\n",
      " [-0.65440427]\n",
      " [-0.65642577]\n",
      " ...\n",
      " [ 0.32197445]\n",
      " [ 0.14404043]\n",
      " [-0.75953626]]\n",
      "Current iteration=6, loss=39477.61076127565\n",
      "t [[ 0.65474071]\n",
      " [-0.75577648]\n",
      " [-0.73185837]\n",
      " ...\n",
      " [ 0.3266739 ]\n",
      " [ 0.14567074]\n",
      " [-0.82477315]]\n",
      "t [[ 0.65474071]\n",
      " [-0.75577648]\n",
      " [-0.73185837]\n",
      " ...\n",
      " [ 0.3266739 ]\n",
      " [ 0.14567074]\n",
      " [-0.82477315]]\n",
      "t [[ 0.69894241]\n",
      " [-0.85085873]\n",
      " [-0.80160808]\n",
      " ...\n",
      " [ 0.32686098]\n",
      " [ 0.14475412]\n",
      " [-0.8836182 ]]\n",
      "t [[ 0.69894241]\n",
      " [-0.85085873]\n",
      " [-0.80160808]\n",
      " ...\n",
      " [ 0.32686098]\n",
      " [ 0.14475412]\n",
      " [-0.8836182 ]]\n",
      "Current iteration=8, loss=37665.47967943789\n",
      "t [[ 0.73711956]\n",
      " [-0.93993972]\n",
      " [-0.86642884]\n",
      " ...\n",
      " [ 0.3237596 ]\n",
      " [ 0.14192477]\n",
      " [-0.93737983]]\n",
      "t [[ 0.73711956]\n",
      " [-0.93993972]\n",
      " [-0.86642884]\n",
      " ...\n",
      " [ 0.3237596 ]\n",
      " [ 0.14192477]\n",
      " [-0.93737983]]\n",
      "t [[ 0.77030256]\n",
      " [-1.02342495]\n",
      " [-0.92692494]\n",
      " ...\n",
      " [ 0.3182344 ]\n",
      " [ 0.13766085]\n",
      " [-0.98697056]]\n",
      "loss=36311.085636607386\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.15545168]\n",
      " [-0.09247234]\n",
      " [-0.14613932]\n",
      " ...\n",
      " [ 0.22389931]\n",
      " [-0.14613932]\n",
      " [ 0.20971342]]\n",
      "t [[ 0.15545168]\n",
      " [-0.09247234]\n",
      " [-0.14613932]\n",
      " ...\n",
      " [ 0.22389931]\n",
      " [-0.14613932]\n",
      " [ 0.20971342]]\n",
      "t [[ 0.28152376]\n",
      " [-0.20582891]\n",
      " [-0.27256175]\n",
      " ...\n",
      " [ 0.40023579]\n",
      " [-0.27256175]\n",
      " [ 0.36718081]]\n",
      "t [[ 0.28152376]\n",
      " [-0.20582891]\n",
      " [-0.27256175]\n",
      " ...\n",
      " [ 0.40023579]\n",
      " [-0.27256175]\n",
      " [ 0.36718081]]\n",
      "Current iteration=2, loss=45780.49212201632\n",
      "t [[ 0.38477565]\n",
      " [-0.32608355]\n",
      " [-0.38385462]\n",
      " ...\n",
      " [ 0.5413086 ]\n",
      " [-0.38385462]\n",
      " [ 0.48772518]]\n",
      "t [[ 0.38477565]\n",
      " [-0.32608355]\n",
      " [-0.38385462]\n",
      " ...\n",
      " [ 0.5413086 ]\n",
      " [-0.38385462]\n",
      " [ 0.48772518]]\n",
      "t [[ 0.47022036]\n",
      " [-0.44575644]\n",
      " [-0.48327468]\n",
      " ...\n",
      " [ 0.65590848]\n",
      " [-0.48327468]\n",
      " [ 0.58178925]]\n",
      "t [[ 0.47022036]\n",
      " [-0.44575644]\n",
      " [-0.48327468]\n",
      " ...\n",
      " [ 0.65590848]\n",
      " [-0.48327468]\n",
      " [ 0.58178925]]\n",
      "Current iteration=4, loss=42007.328109727256\n",
      "t [[ 0.54162137]\n",
      " [-0.56117423]\n",
      " [-0.57313265]\n",
      " ...\n",
      " [ 0.75027079]\n",
      " [-0.57313265]\n",
      " [ 0.6564415 ]]\n",
      "t [[ 0.54162137]\n",
      " [-0.56117423]\n",
      " [-0.57313265]\n",
      " ...\n",
      " [ 0.75027079]\n",
      " [-0.57313265]\n",
      " [ 0.6564415 ]]\n",
      "t [[ 0.60181855]\n",
      " [-0.67073129]\n",
      " [-0.65510508]\n",
      " ...\n",
      " [ 0.82888349]\n",
      " [-0.65510508]\n",
      " [ 0.7165504 ]]\n",
      "t [[ 0.60181855]\n",
      " [-0.67073129]\n",
      " [-0.65510508]\n",
      " ...\n",
      " [ 0.82888349]\n",
      " [-0.65510508]\n",
      " [ 0.7165504 ]]\n",
      "Current iteration=6, loss=39466.16012545666\n",
      "t [[ 0.65297929]\n",
      " [-0.77390714]\n",
      " [-0.73044233]\n",
      " ...\n",
      " [ 0.8950415 ]\n",
      " [-0.73044233]\n",
      " [ 0.76554883]]\n",
      "t [[ 0.65297929]\n",
      " [-0.77390714]\n",
      " [-0.73044233]\n",
      " ...\n",
      " [ 0.8950415 ]\n",
      " [-0.73044233]\n",
      " [ 0.76554883]]\n",
      "t [[ 0.69677904]\n",
      " [-0.87073205]\n",
      " [-0.80010243]\n",
      " ...\n",
      " [ 0.95121262]\n",
      " [-0.80010243]\n",
      " [ 0.80591574]]\n",
      "t [[ 0.69677904]\n",
      " [-0.87073205]\n",
      " [-0.80010243]\n",
      " ...\n",
      " [ 0.95121262]\n",
      " [-0.80010243]\n",
      " [ 0.80591574]]\n",
      "Current iteration=8, loss=37647.74205075744\n",
      "t [[ 0.73452934]\n",
      " [-0.96150077]\n",
      " [-0.8648376 ]\n",
      " ...\n",
      " [ 0.99927969]\n",
      " [-0.8648376 ]\n",
      " [ 0.83948141]]\n",
      "t [[ 0.73452934]\n",
      " [-0.96150077]\n",
      " [-0.8648376 ]\n",
      " ...\n",
      " [ 0.99927969]\n",
      " [-0.8648376 ]\n",
      " [ 0.83948141]]\n",
      "t [[ 0.76726904]\n",
      " [-1.04662081]\n",
      " [-0.92525133]\n",
      " ...\n",
      " [ 1.0407033 ]\n",
      " [-0.92525133]\n",
      " [ 0.86762442]]\n",
      "loss=36288.349224996025\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05409518]\n",
      " [-0.25903465]\n",
      " [-0.15190089]\n",
      " ...\n",
      " [ 0.12989543]\n",
      " [ 0.05409518]\n",
      " [-0.22018784]]\n",
      "t [[ 0.05409518]\n",
      " [-0.25903465]\n",
      " [-0.15190089]\n",
      " ...\n",
      " [ 0.12989543]\n",
      " [ 0.05409518]\n",
      " [-0.22018784]]\n",
      "t [[ 0.09052656]\n",
      " [-0.49007082]\n",
      " [-0.28269714]\n",
      " ...\n",
      " [ 0.21284813]\n",
      " [ 0.09052656]\n",
      " [-0.38234313]]\n",
      "t [[ 0.09052656]\n",
      " [-0.49007082]\n",
      " [-0.28269714]\n",
      " ...\n",
      " [ 0.21284813]\n",
      " [ 0.09052656]\n",
      " [-0.38234313]]\n",
      "Current iteration=2, loss=45556.443867705064\n",
      "t [[ 0.11454405]\n",
      " [-0.69572368]\n",
      " [-0.39746781]\n",
      " ...\n",
      " [ 0.26584685]\n",
      " [ 0.11454405]\n",
      " [-0.50784469]]\n",
      "t [[ 0.11454405]\n",
      " [-0.69572368]\n",
      " [-0.39746781]\n",
      " ...\n",
      " [ 0.26584685]\n",
      " [ 0.11454405]\n",
      " [-0.50784469]]\n",
      "t [[ 0.1297612 ]\n",
      " [-0.87908001]\n",
      " [-0.49975322]\n",
      " ...\n",
      " [ 0.29930539]\n",
      " [ 0.1297612 ]\n",
      " [-0.60960589]]\n",
      "t [[ 0.1297612 ]\n",
      " [-0.87908001]\n",
      " [-0.49975322]\n",
      " ...\n",
      " [ 0.29930539]\n",
      " [ 0.1297612 ]\n",
      " [-0.60960589]]\n",
      "Current iteration=4, loss=41697.69350755517\n",
      "t [[ 0.13864725]\n",
      " [-1.04313657]\n",
      " [-0.59203315]\n",
      " ...\n",
      " [ 0.31970214]\n",
      " [ 0.13864725]\n",
      " [-0.69537156]]\n",
      "t [[ 0.13864725]\n",
      " [-1.04313657]\n",
      " [-0.59203315]\n",
      " ...\n",
      " [ 0.31970214]\n",
      " [ 0.13864725]\n",
      " [-0.69537156]]\n",
      "t [[ 0.14291644]\n",
      " [-1.19058013]\n",
      " [-0.67609036]\n",
      " ...\n",
      " [ 0.33116016]\n",
      " [ 0.14291644]\n",
      " [-0.76986819]]\n",
      "t [[ 0.14291644]\n",
      " [-1.19058013]\n",
      " [-0.67609036]\n",
      " ...\n",
      " [ 0.33116016]\n",
      " [ 0.14291644]\n",
      " [-0.76986819]]\n",
      "Current iteration=6, loss=39121.46258875138\n",
      "t [[ 0.14378405]\n",
      " [-1.32372991]\n",
      " [-0.75324563]\n",
      " ...\n",
      " [ 0.33636077]\n",
      " [ 0.14378405]\n",
      " [-0.8360577 ]]\n",
      "t [[ 0.14378405]\n",
      " [-1.32372991]\n",
      " [-0.75324563]\n",
      " ...\n",
      " [ 0.33636077]\n",
      " [ 0.14378405]\n",
      " [-0.8360577 ]]\n",
      "t [[ 0.14212956]\n",
      " [-1.44455028]\n",
      " [-0.82450622]\n",
      " ...\n",
      " [ 0.33709183]\n",
      " [ 0.14212956]\n",
      " [-0.89585749]]\n",
      "t [[ 0.14212956]\n",
      " [-1.44455028]\n",
      " [-0.82450622]\n",
      " ...\n",
      " [ 0.33709183]\n",
      " [ 0.14212956]\n",
      " [-0.89585749]]\n",
      "Current iteration=8, loss=37289.68179214828\n",
      "t [[ 0.13860136]\n",
      " [-1.55469129]\n",
      " [-0.89066058]\n",
      " ...\n",
      " [ 0.33457754]\n",
      " [ 0.13860136]\n",
      " [-0.95055411]]\n",
      "t [[ 0.13860136]\n",
      " [-1.55469129]\n",
      " [-0.89066058]\n",
      " ...\n",
      " [ 0.33457754]\n",
      " [ 0.13860136]\n",
      " [-0.95055411]]\n",
      "t [[ 0.1336852 ]\n",
      " [-1.65553608]\n",
      " [-0.95233989]\n",
      " ...\n",
      " [ 0.32967808]\n",
      " [ 0.1336852 ]\n",
      " [-1.0010434 ]]\n",
      "loss=35927.012258134695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.16335539]\n",
      " [-0.09522964]\n",
      " [-0.15240629]\n",
      " ...\n",
      " [ 0.12799421]\n",
      " [ 0.0538828 ]\n",
      " [-0.21686019]]\n",
      "t [[ 0.16335539]\n",
      " [-0.09522964]\n",
      " [-0.15240629]\n",
      " ...\n",
      " [ 0.12799421]\n",
      " [ 0.0538828 ]\n",
      " [-0.21686019]]\n",
      "t [[ 0.29473919]\n",
      " [-0.21402674]\n",
      " [-0.28348384]\n",
      " ...\n",
      " [ 0.2093797 ]\n",
      " [ 0.09004399]\n",
      " [-0.37572153]]\n",
      "t [[ 0.29473919]\n",
      " [-0.21402674]\n",
      " [-0.28348384]\n",
      " ...\n",
      " [ 0.2093797 ]\n",
      " [ 0.09004399]\n",
      " [-0.37572153]]\n",
      "Current iteration=2, loss=45544.942996440004\n",
      "t [[ 0.40153577]\n",
      " [-0.34038438]\n",
      " [-0.39839063]\n",
      " ...\n",
      " [ 0.26109756]\n",
      " [ 0.11376594]\n",
      " [-0.49811292]]\n",
      "t [[ 0.40153577]\n",
      " [-0.34038438]\n",
      " [-0.39839063]\n",
      " ...\n",
      " [ 0.26109756]\n",
      " [ 0.11376594]\n",
      " [-0.49811292]]\n",
      "t [[ 0.48938241]\n",
      " [-0.46590743]\n",
      " [-0.50071944]\n",
      " ...\n",
      " [ 0.29351714]\n",
      " [ 0.12868082]\n",
      " [-0.59701055]]\n",
      "t [[ 0.48938241]\n",
      " [-0.46590743]\n",
      " [-0.50071944]\n",
      " ...\n",
      " [ 0.29351714]\n",
      " [ 0.12868082]\n",
      " [-0.59701055]]\n",
      "Current iteration=4, loss=41684.695988946696\n",
      "t [[ 0.56243628]\n",
      " [-0.58660459]\n",
      " [-0.59298357]\n",
      " ...\n",
      " [ 0.3130769 ]\n",
      " [ 0.13726809]\n",
      " [-0.68017347]]\n",
      "t [[ 0.56243628]\n",
      " [-0.58660459]\n",
      " [-0.59298357]\n",
      " ...\n",
      " [ 0.3130769 ]\n",
      " [ 0.13726809]\n",
      " [-0.68017347]]\n",
      "t [[ 0.6237813 ]\n",
      " [-0.70081754]\n",
      " [-0.67698723]\n",
      " ...\n",
      " [ 0.3238635 ]\n",
      " [ 0.14124763]\n",
      " [-0.75231818]]\n",
      "t [[ 0.6237813 ]\n",
      " [-0.70081754]\n",
      " [-0.67698723]\n",
      " ...\n",
      " [ 0.3238635 ]\n",
      " [ 0.14124763]\n",
      " [-0.75231818]]\n",
      "Current iteration=6, loss=39108.615040842\n",
      "t [[ 0.67574327]\n",
      " [-0.80807299]\n",
      " [-0.75406502]\n",
      " ...\n",
      " [ 0.32852743]\n",
      " [ 0.14183777]\n",
      " [-0.81638632]]\n",
      "t [[ 0.67574327]\n",
      " [-0.80807299]\n",
      " [-0.75406502]\n",
      " ...\n",
      " [ 0.32852743]\n",
      " [ 0.14183777]\n",
      " [-0.81638632]]\n",
      "t [[ 0.72010465]\n",
      " [-0.90847747]\n",
      " [-0.82523324]\n",
      " ...\n",
      " [ 0.32883142]\n",
      " [ 0.13991952]\n",
      " [-0.8742719 ]]\n",
      "t [[ 0.72010465]\n",
      " [-0.90847747]\n",
      " [-0.82523324]\n",
      " ...\n",
      " [ 0.32883142]\n",
      " [ 0.13991952]\n",
      " [-0.8742719 ]]\n",
      "Current iteration=8, loss=37277.14960905647\n",
      "t [[ 0.75825141]\n",
      " [-1.00240304]\n",
      " [-0.89128627]\n",
      " ...\n",
      " [ 0.32597956]\n",
      " [ 0.13614188]\n",
      " [-0.9272383 ]]\n",
      "t [[ 0.75825141]\n",
      " [-1.00240304]\n",
      " [-0.89128627]\n",
      " ...\n",
      " [ 0.32597956]\n",
      " [ 0.13614188]\n",
      " [-0.9272383 ]]\n",
      "t [[ 0.79127507]\n",
      " [-1.09032581]\n",
      " [-0.9528593 ]\n",
      " ...\n",
      " [ 0.32081597]\n",
      " [ 0.13099071]\n",
      " [-0.97615993]]\n",
      "loss=35914.65277083995\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.16126167]\n",
      " [-0.09116214]\n",
      " [-0.1518355 ]\n",
      " ...\n",
      " [ 0.12996899]\n",
      " [ 0.05487591]\n",
      " [-0.2235384 ]]\n",
      "t [[ 0.16126167]\n",
      " [-0.09116214]\n",
      " [-0.1518355 ]\n",
      " ...\n",
      " [ 0.12996899]\n",
      " [ 0.05487591]\n",
      " [-0.2235384 ]]\n",
      "t [[ 0.29097512]\n",
      " [-0.20599897]\n",
      " [-0.2823444 ]\n",
      " ...\n",
      " [ 0.21207375]\n",
      " [ 0.09181855]\n",
      " [-0.38733756]]\n",
      "t [[ 0.29097512]\n",
      " [-0.20599897]\n",
      " [-0.2823444 ]\n",
      " ...\n",
      " [ 0.21207375]\n",
      " [ 0.09181855]\n",
      " [-0.38733756]]\n",
      "Current iteration=2, loss=45599.48537766008\n",
      "t [[ 0.39644085]\n",
      " [-0.32843497]\n",
      " [-0.3967165 ]\n",
      " ...\n",
      " [ 0.26376749]\n",
      " [ 0.11618212]\n",
      " [-0.51352532]]\n",
      "t [[ 0.39644085]\n",
      " [-0.32843497]\n",
      " [-0.3967165 ]\n",
      " ...\n",
      " [ 0.26376749]\n",
      " [ 0.11618212]\n",
      " [-0.51352532]]\n",
      "t [[ 0.48322942]\n",
      " [-0.45007856]\n",
      " [-0.49855369]\n",
      " ...\n",
      " [ 0.29570555]\n",
      " [ 0.13164169]\n",
      " [-0.61543988]]\n",
      "t [[ 0.48322942]\n",
      " [-0.45007856]\n",
      " [-0.49855369]\n",
      " ...\n",
      " [ 0.29570555]\n",
      " [ 0.13164169]\n",
      " [-0.61543988]]\n",
      "Current iteration=4, loss=41785.377655340846\n",
      "t [[ 0.55544595]\n",
      " [-0.56695943]\n",
      " [-0.5903678 ]\n",
      " ...\n",
      " [ 0.31447923]\n",
      " [ 0.14070219]\n",
      " [-0.70105865]]\n",
      "t [[ 0.55544595]\n",
      " [-0.56695943]\n",
      " [-0.5903678 ]\n",
      " ...\n",
      " [ 0.31447923]\n",
      " [ 0.14070219]\n",
      " [-0.70105865]]\n",
      "t [[ 0.61612953]\n",
      " [-0.6774419 ]\n",
      " [-0.6739582 ]\n",
      " ...\n",
      " [ 0.3242724 ]\n",
      " [ 0.14509948]\n",
      " [-0.77524068]]\n",
      "t [[ 0.61612953]\n",
      " [-0.6774419 ]\n",
      " [-0.6739582 ]\n",
      " ...\n",
      " [ 0.3242724 ]\n",
      " [ 0.14509948]\n",
      " [-0.77524068]]\n",
      "Current iteration=6, loss=39243.90011969231\n",
      "t [[ 0.66756857]\n",
      " [-0.78107027]\n",
      " [-0.75065414]\n",
      " ...\n",
      " [ 0.32780212]\n",
      " [ 0.14606268]\n",
      " [-0.8410253 ]]\n",
      "t [[ 0.66756857]\n",
      " [-0.78107027]\n",
      " [-0.75065414]\n",
      " ...\n",
      " [ 0.32780212]\n",
      " [ 0.14606268]\n",
      " [-0.8410253 ]]\n",
      "t [[ 0.71151563]\n",
      " [-0.87796238]\n",
      " [-0.82146698]\n",
      " ...\n",
      " [ 0.32687754]\n",
      " [ 0.14448053]\n",
      " [-0.90037552]]\n",
      "t [[ 0.71151563]\n",
      " [-0.87796238]\n",
      " [-0.82146698]\n",
      " ...\n",
      " [ 0.32687754]\n",
      " [ 0.14448053]\n",
      " [-0.90037552]]\n",
      "Current iteration=8, loss=37438.60459280245\n",
      "t [[ 0.74933315]\n",
      " [-0.96849619]\n",
      " [-0.88718682]\n",
      " ...\n",
      " [ 0.32273539]\n",
      " [ 0.14100786]\n",
      " [-0.95460474]]\n",
      "t [[ 0.74933315]\n",
      " [-0.96849619]\n",
      " [-0.88718682]\n",
      " ...\n",
      " [ 0.32273539]\n",
      " [ 0.14100786]\n",
      " [-0.95460474]]\n",
      "t [[ 0.78209438]\n",
      " [-1.0531498 ]\n",
      " [-0.94844526]\n",
      " ...\n",
      " [ 0.31624302]\n",
      " [ 0.13613504]\n",
      " [-1.00462443]]\n",
      "loss=36096.386847746224\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.16100353]\n",
      " [-0.09577492]\n",
      " [-0.15135858]\n",
      " ...\n",
      " [ 0.23189571]\n",
      " [-0.15135858]\n",
      " [ 0.21720319]]\n",
      "t [[ 0.16100353]\n",
      " [-0.09577492]\n",
      " [-0.15135858]\n",
      " ...\n",
      " [ 0.23189571]\n",
      " [-0.15135858]\n",
      " [ 0.21720319]]\n",
      "t [[ 0.29050395]\n",
      " [-0.21394213]\n",
      " [-0.2815745 ]\n",
      " ...\n",
      " [ 0.41279097]\n",
      " [-0.2815745 ]\n",
      " [ 0.378385  ]]\n",
      "t [[ 0.29050395]\n",
      " [-0.21394213]\n",
      " [-0.2815745 ]\n",
      " ...\n",
      " [ 0.41279097]\n",
      " [-0.2815745 ]\n",
      " [ 0.378385  ]]\n",
      "Current iteration=2, loss=45603.70303687158\n",
      "t [[ 0.39579548]\n",
      " [-0.33897955]\n",
      " [-0.39574774]\n",
      " ...\n",
      " [ 0.55635737]\n",
      " [-0.39574774]\n",
      " [ 0.5005736 ]]\n",
      "t [[ 0.39579548]\n",
      " [-0.33897955]\n",
      " [-0.39574774]\n",
      " ...\n",
      " [ 0.55635737]\n",
      " [-0.39574774]\n",
      " [ 0.5005736 ]]\n",
      "t [[ 0.48237381]\n",
      " [-0.46285208]\n",
      " [-0.49743652]\n",
      " ...\n",
      " [ 0.6722012 ]\n",
      " [-0.49743652]\n",
      " [ 0.59515199]]\n",
      "t [[ 0.48237381]\n",
      " [-0.46285208]\n",
      " [-0.49743652]\n",
      " ...\n",
      " [ 0.6722012 ]\n",
      " [-0.49743652]\n",
      " [ 0.59515199]]\n",
      "Current iteration=4, loss=41781.11822747265\n",
      "t [[ 0.55431351]\n",
      " [-0.58177102]\n",
      " [-0.58912983]\n",
      " ...\n",
      " [ 0.76703788]\n",
      " [-0.58912983]\n",
      " [ 0.66969782]]\n",
      "t [[ 0.55431351]\n",
      " [-0.58177102]\n",
      " [-0.58912983]\n",
      " ...\n",
      " [ 0.76703788]\n",
      " [-0.58912983]\n",
      " [ 0.66969782]]\n",
      "t [[ 0.61465739]\n",
      " [-0.69418148]\n",
      " [-0.67261465]\n",
      " ...\n",
      " [ 0.84564518]\n",
      " [-0.67261465]\n",
      " [ 0.72936115]]\n",
      "t [[ 0.61465739]\n",
      " [-0.69418148]\n",
      " [-0.67261465]\n",
      " ...\n",
      " [ 0.84564518]\n",
      " [-0.67261465]\n",
      " [ 0.72936115]]\n",
      "Current iteration=6, loss=39231.69167280288\n",
      "t [[ 0.66570762]\n",
      " [-0.79966188]\n",
      " [-0.74921372]\n",
      " ...\n",
      " [ 0.91149839]\n",
      " [-0.74921372]\n",
      " [ 0.77773522]]\n",
      "t [[ 0.66570762]\n",
      " [-0.79966188]\n",
      " [-0.74921372]\n",
      " ...\n",
      " [ 0.91149839]\n",
      " [-0.74921372]\n",
      " [ 0.77773522]]\n",
      "t [[ 0.70923058]\n",
      " [-0.89834364]\n",
      " [-0.81993515]\n",
      " ...\n",
      " [ 0.96718088]\n",
      " [-0.81993515]\n",
      " [ 0.81739307]]\n",
      "t [[ 0.70923058]\n",
      " [-0.89834364]\n",
      " [-0.81993515]\n",
      " ...\n",
      " [ 0.96718088]\n",
      " [-0.81993515]\n",
      " [ 0.81739307]]\n",
      "Current iteration=8, loss=37420.05784066068\n",
      "t [[ 0.74660008]\n",
      " [-0.99060956]\n",
      " [-0.88556751]\n",
      " ...\n",
      " [ 1.01465136]\n",
      " [-0.88556751]\n",
      " [ 0.85022149]]\n",
      "t [[ 0.74660008]\n",
      " [-0.99060956]\n",
      " [-0.88556751]\n",
      " ...\n",
      " [ 1.01465136]\n",
      " [-0.88556751]\n",
      " [ 0.85022149]]\n",
      "t [[ 0.77889803]\n",
      " [-1.076939  ]\n",
      " [-0.94674172]\n",
      " ...\n",
      " [ 1.0554212 ]\n",
      " [-0.94674172]\n",
      " [ 0.87763361]]\n",
      "loss=36072.845029377095\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05596053]\n",
      " [-0.26796688]\n",
      " [-0.15713886]\n",
      " ...\n",
      " [ 0.13437458]\n",
      " [ 0.05596053]\n",
      " [-0.22778052]]\n",
      "t [[ 0.05596053]\n",
      " [-0.26796688]\n",
      " [-0.15713886]\n",
      " ...\n",
      " [ 0.13437458]\n",
      " [ 0.05596053]\n",
      " [-0.22778052]]\n",
      "t [[ 0.09302617]\n",
      " [-0.50597927]\n",
      " [-0.29170094]\n",
      " ...\n",
      " [ 0.21853336]\n",
      " [ 0.09302617]\n",
      " [-0.39348408]]\n",
      "t [[ 0.09302617]\n",
      " [-0.50597927]\n",
      " [-0.29170094]\n",
      " ...\n",
      " [ 0.21853336]\n",
      " [ 0.09302617]\n",
      " [-0.39348408]]\n",
      "Current iteration=2, loss=45380.66729295403\n",
      "t [[ 0.11701018]\n",
      " [-0.71694522]\n",
      " [-0.40931214]\n",
      " ...\n",
      " [ 0.27127433]\n",
      " [ 0.11701018]\n",
      " [-0.52077038]]\n",
      "t [[ 0.11701018]\n",
      " [-0.71694522]\n",
      " [-0.40931214]\n",
      " ...\n",
      " [ 0.27127433]\n",
      " [ 0.11701018]\n",
      " [-0.52077038]]\n",
      "t [[ 0.13184147]\n",
      " [-0.90429472]\n",
      " [-0.51382534]\n",
      " ...\n",
      " [ 0.30386081]\n",
      " [ 0.13184147]\n",
      " [-0.62358426]]\n",
      "t [[ 0.13184147]\n",
      " [-0.90429472]\n",
      " [-0.51382534]\n",
      " ...\n",
      " [ 0.30386081]\n",
      " [ 0.13184147]\n",
      " [-0.62358426]]\n",
      "Current iteration=4, loss=41475.10926362185\n",
      "t [[ 0.14016312]\n",
      " [-1.07132902]\n",
      " [-0.60790161]\n",
      " ...\n",
      " [ 0.3231649 ]\n",
      " [ 0.14016312]\n",
      " [-0.71010555]]\n",
      "t [[ 0.14016312]\n",
      " [-1.07132902]\n",
      " [-0.60790161]\n",
      " ...\n",
      " [ 0.3231649 ]\n",
      " [ 0.14016312]\n",
      " [-0.71010555]]\n",
      "t [[ 0.14378715]\n",
      " [-1.22097938]\n",
      " [-0.69343458]\n",
      " ...\n",
      " [ 0.33348625]\n",
      " [ 0.14378715]\n",
      " [-0.78523261]]\n",
      "t [[ 0.14378715]\n",
      " [-1.22097938]\n",
      " [-0.69343458]\n",
      " ...\n",
      " [ 0.33348625]\n",
      " [ 0.14378715]\n",
      " [-0.78523261]]\n",
      "Current iteration=6, loss=38892.44714956466\n",
      "t [[ 0.14398502]\n",
      " [-1.35575242]\n",
      " [-0.77181717]\n",
      " ...\n",
      " [ 0.33758163]\n",
      " [ 0.14398502]\n",
      " [-0.85198881]]\n",
      "t [[ 0.14398502]\n",
      " [-1.35575242]\n",
      " [-0.77181717]\n",
      " ...\n",
      " [ 0.33758163]\n",
      " [ 0.14398502]\n",
      " [-0.85198881]]\n",
      "t [[ 0.14166898]\n",
      " [-1.47775313]\n",
      " [-0.84410647]\n",
      " ...\n",
      " [ 0.33726881]\n",
      " [ 0.14166898]\n",
      " [-0.91230963]]\n",
      "t [[ 0.14166898]\n",
      " [-1.47775313]\n",
      " [-0.84410647]\n",
      " ...\n",
      " [ 0.33726881]\n",
      " [ 0.14166898]\n",
      " [-0.91230963]]\n",
      "Current iteration=8, loss=37068.52827477375\n",
      "t [[ 0.13750659]\n",
      " [-1.58873641]\n",
      " [-0.91112706]\n",
      " ...\n",
      " [ 0.33378182]\n",
      " [ 0.13750659]\n",
      " [-0.96748436]]\n",
      "t [[ 0.13750659]\n",
      " [-1.58873641]\n",
      " [-0.91112706]\n",
      " ...\n",
      " [ 0.33378182]\n",
      " [ 0.13750659]\n",
      " [-0.96748436]]\n",
      "t [[ 0.13199457]\n",
      " [-1.69016363]\n",
      " [-0.97353752]\n",
      " ...\n",
      " [ 0.32798198]\n",
      " [ 0.13199457]\n",
      " [-1.01840738]]\n",
      "loss=35718.60669800975\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.16898834]\n",
      " [-0.09851342]\n",
      " [-0.15766168]\n",
      " ...\n",
      " [ 0.1324078 ]\n",
      " [ 0.05574082]\n",
      " [-0.22433813]]\n",
      "t [[ 0.16898834]\n",
      " [-0.09851342]\n",
      " [-0.15766168]\n",
      " ...\n",
      " [ 0.1324078 ]\n",
      " [ 0.05574082]\n",
      " [-0.22433813]]\n",
      "t [[ 0.30377722]\n",
      " [-0.2222347 ]\n",
      " [-0.29250693]\n",
      " ...\n",
      " [ 0.21495699]\n",
      " [ 0.09252496]\n",
      " [-0.38663548]]\n",
      "t [[ 0.30377722]\n",
      " [-0.2222347 ]\n",
      " [-0.29250693]\n",
      " ...\n",
      " [ 0.21495699]\n",
      " [ 0.09252496]\n",
      " [-0.38663548]]\n",
      "Current iteration=2, loss=45369.00173489221\n",
      "t [[ 0.41255003]\n",
      " [-0.35346142]\n",
      " [-0.41024899]\n",
      " ...\n",
      " [ 0.26639286]\n",
      " [ 0.11620153]\n",
      " [-0.51071756]]\n",
      "t [[ 0.41255003]\n",
      " [-0.35346142]\n",
      " [-0.41024899]\n",
      " ...\n",
      " [ 0.26639286]\n",
      " [ 0.11620153]\n",
      " [-0.51071756]]\n",
      "t [[ 0.50146225]\n",
      " [-0.48321222]\n",
      " [-0.51479757]\n",
      " ...\n",
      " [ 0.29792987]\n",
      " [ 0.13071942]\n",
      " [-0.61059514]]\n",
      "t [[ 0.50146225]\n",
      " [-0.48321222]\n",
      " [-0.51479757]\n",
      " ...\n",
      " [ 0.29792987]\n",
      " [ 0.13071942]\n",
      " [-0.61059514]]\n",
      "Current iteration=4, loss=41462.088345781245\n",
      "t [[ 0.57499409]\n",
      " [-0.60739371]\n",
      " [-0.6088494 ]\n",
      " ...\n",
      " [ 0.31639602]\n",
      " [ 0.13873246]\n",
      " [-0.69446027]]\n",
      "t [[ 0.57499409]\n",
      " [-0.60739371]\n",
      " [-0.6088494 ]\n",
      " ...\n",
      " [ 0.31639602]\n",
      " [ 0.13873246]\n",
      " [-0.69446027]]\n",
      "t [[ 0.63643584]\n",
      " [-0.72441701]\n",
      " [-0.69432052]\n",
      " ...\n",
      " [ 0.32605127]\n",
      " [ 0.14205842]\n",
      " [-0.76719778]]\n",
      "t [[ 0.63643584]\n",
      " [-0.72441701]\n",
      " [-0.69432052]\n",
      " ...\n",
      " [ 0.32605127]\n",
      " [ 0.14205842]\n",
      " [-0.76719778]]\n",
      "Current iteration=6, loss=38879.634984361524\n",
      "t [[ 0.68824853]\n",
      " [-0.83392189]\n",
      " [-0.77261806]\n",
      " ...\n",
      " [ 0.32961923]\n",
      " [ 0.14197178]\n",
      " [-0.83180732]]\n",
      "t [[ 0.68824853]\n",
      " [-0.83392189]\n",
      " [-0.77261806]\n",
      " ...\n",
      " [ 0.32961923]\n",
      " [ 0.14197178]\n",
      " [-0.83180732]]\n",
      "t [[ 0.73230507]\n",
      " [-0.93612486]\n",
      " [-0.84480822]\n",
      " ...\n",
      " [ 0.32889098]\n",
      " [ 0.13938622]\n",
      " [-0.89019804]]\n",
      "t [[ 0.73230507]\n",
      " [-0.93612486]\n",
      " [-0.84480822]\n",
      " ...\n",
      " [ 0.32889098]\n",
      " [ 0.13938622]\n",
      " [-0.89019804]]\n",
      "Current iteration=8, loss=37056.03632587605\n",
      "t [[ 0.77005264]\n",
      " [-1.03149098]\n",
      " [-0.91172156]\n",
      " ...\n",
      " [ 0.32507938]\n",
      " [ 0.13496979]\n",
      " [-0.94363375]]\n",
      "t [[ 0.77005264]\n",
      " [-1.03149098]\n",
      " [-0.91172156]\n",
      " ...\n",
      " [ 0.32507938]\n",
      " [ 0.13496979]\n",
      " [-0.94363375]]\n",
      "t [[ 0.80262474]\n",
      " [-1.12057066]\n",
      " [-0.97402052]\n",
      " ...\n",
      " [ 0.31902903]\n",
      " [ 0.12921919]\n",
      " [-0.99298561]]\n",
      "loss=35706.26475211246\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.16682241]\n",
      " [-0.09430566]\n",
      " [-0.15707121]\n",
      " ...\n",
      " [ 0.13445068]\n",
      " [ 0.05676818]\n",
      " [-0.23124662]]\n",
      "t [[ 0.16682241]\n",
      " [-0.09430566]\n",
      " [-0.15707121]\n",
      " ...\n",
      " [ 0.13445068]\n",
      " [ 0.05676818]\n",
      " [-0.23124662]]\n",
      "t [[ 0.29989817]\n",
      " [-0.2139339 ]\n",
      " [-0.29132842]\n",
      " ...\n",
      " [ 0.2177011 ]\n",
      " [ 0.0943534 ]\n",
      " [-0.39859181]]\n",
      "t [[ 0.29989817]\n",
      " [-0.2139339 ]\n",
      " [-0.29132842]\n",
      " ...\n",
      " [ 0.2177011 ]\n",
      " [ 0.0943534 ]\n",
      " [-0.39859181]]\n",
      "Current iteration=2, loss=45425.39165694879\n",
      "t [[ 0.40731772]\n",
      " [-0.34110658]\n",
      " [-0.40851991]\n",
      " ...\n",
      " [ 0.26906212]\n",
      " [ 0.11868418]\n",
      " [-0.52652338]]\n",
      "t [[ 0.40731772]\n",
      " [-0.34110658]\n",
      " [-0.40851991]\n",
      " ...\n",
      " [ 0.26906212]\n",
      " [ 0.11868418]\n",
      " [-0.52652338]]\n",
      "t [[ 0.49516383]\n",
      " [-0.46684842]\n",
      " [-0.51256446]\n",
      " ...\n",
      " [ 0.30005343]\n",
      " [ 0.13375555]\n",
      " [-0.62944184]]\n",
      "t [[ 0.49516383]\n",
      " [-0.46684842]\n",
      " [-0.51256446]\n",
      " ...\n",
      " [ 0.30005343]\n",
      " [ 0.13375555]\n",
      " [-0.62944184]]\n",
      "Current iteration=4, loss=41565.60493094822\n",
      "t [[ 0.56785979]\n",
      " [-0.58709063]\n",
      " [-0.60615644]\n",
      " ...\n",
      " [ 0.31766391]\n",
      " [ 0.14224821]\n",
      " [-0.71577055]]\n",
      "t [[ 0.56785979]\n",
      " [-0.58709063]\n",
      " [-0.60615644]\n",
      " ...\n",
      " [ 0.31766391]\n",
      " [ 0.14224821]\n",
      " [-0.71577055]]\n",
      "t [[ 0.62864714]\n",
      " [-0.70026951]\n",
      " [-0.69120626]\n",
      " ...\n",
      " [ 0.32625546]\n",
      " [ 0.14599663]\n",
      " [-0.79054386]]\n",
      "t [[ 0.62864714]\n",
      " [-0.70026951]\n",
      " [-0.69120626]\n",
      " ...\n",
      " [ 0.32625546]\n",
      " [ 0.14599663]\n",
      " [-0.79054386]]\n",
      "Current iteration=6, loss=39018.09711148596\n",
      "t [[ 0.67994691]\n",
      " [-0.80604346]\n",
      " [-0.76911512]\n",
      " ...\n",
      " [ 0.32862086]\n",
      " [ 0.1462865 ]\n",
      " [-0.85686298]]\n",
      "t [[ 0.67994691]\n",
      " [-0.80604346]\n",
      " [-0.76911512]\n",
      " ...\n",
      " [ 0.32862086]\n",
      " [ 0.1462865 ]\n",
      " [-0.85686298]]\n",
      "t [[ 0.72360037]\n",
      " [-0.90464037]\n",
      " [-0.84094385]\n",
      " ...\n",
      " [ 0.32659882]\n",
      " [ 0.14403954]\n",
      " [-0.91670821]]\n",
      "t [[ 0.72360037]\n",
      " [-0.90464037]\n",
      " [-0.84094385]\n",
      " ...\n",
      " [ 0.32659882]\n",
      " [ 0.14403954]\n",
      " [-0.91670821]]\n",
      "Current iteration=8, loss=37220.72358272434\n",
      "t [[ 0.76103021]\n",
      " [-0.99653078]\n",
      " [-0.90751843]\n",
      " ...\n",
      " [ 0.32143535]\n",
      " [ 0.13992984]\n",
      " [-0.97139461]]\n",
      "t [[ 0.76103021]\n",
      " [-0.99653078]\n",
      " [-0.90751843]\n",
      " ...\n",
      " [ 0.32143535]\n",
      " [ 0.13992984]\n",
      " [-0.97139461]]\n",
      "t [[ 0.79335099]\n",
      " [-1.08226635]\n",
      " [-0.96949754]\n",
      " ...\n",
      " [ 0.31399842]\n",
      " [ 0.13445879]\n",
      " [-1.02183117]]\n",
      "loss=35891.15861978383\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.16655537]\n",
      " [-0.09907751]\n",
      " [-0.15657784]\n",
      " ...\n",
      " [ 0.23989212]\n",
      " [-0.15657784]\n",
      " [ 0.22469295]]\n",
      "t [[ 0.16655537]\n",
      " [-0.09907751]\n",
      " [-0.15657784]\n",
      " ...\n",
      " [ 0.23989212]\n",
      " [-0.15657784]\n",
      " [ 0.22469295]]\n",
      "t [[ 0.29941138]\n",
      " [-0.2221068 ]\n",
      " [-0.29053831]\n",
      " ...\n",
      " [ 0.4252284 ]\n",
      " [-0.29053831]\n",
      " [ 0.38946   ]]\n",
      "t [[ 0.29941138]\n",
      " [-0.2221068 ]\n",
      " [-0.29053831]\n",
      " ...\n",
      " [ 0.4252284 ]\n",
      " [-0.29053831]\n",
      " [ 0.38946   ]]\n",
      "Current iteration=2, loss=45429.42387703841\n",
      "t [[ 0.40664947]\n",
      " [-0.35192393]\n",
      " [-0.4075305 ]\n",
      " ...\n",
      " [ 0.5711502 ]\n",
      " [-0.4075305 ]\n",
      " [ 0.51315471]]\n",
      "t [[ 0.40664947]\n",
      " [-0.35192393]\n",
      " [-0.4075305 ]\n",
      " ...\n",
      " [ 0.5711502 ]\n",
      " [-0.4075305 ]\n",
      " [ 0.51315471]]\n",
      "t [[ 0.49427101]\n",
      " [-0.47993666]\n",
      " [-0.51142661]\n",
      " ...\n",
      " [ 0.68811355]\n",
      " [-0.51142661]\n",
      " [ 0.60813543]]\n",
      "t [[ 0.49427101]\n",
      " [-0.47993666]\n",
      " [-0.51142661]\n",
      " ...\n",
      " [ 0.68811355]\n",
      " [-0.51142661]\n",
      " [ 0.60813543]]\n",
      "Current iteration=4, loss=41560.75233913994\n",
      "t [[ 0.5666704 ]\n",
      " [-0.60226285]\n",
      " [-0.60489726]\n",
      " ...\n",
      " [ 0.78332275]\n",
      " [-0.60489726]\n",
      " [ 0.68249322]]\n",
      "t [[ 0.5666704 ]\n",
      " [-0.60226285]\n",
      " [-0.60489726]\n",
      " ...\n",
      " [ 0.78332275]\n",
      " [-0.60489726]\n",
      " [ 0.68249322]]\n",
      "t [[ 0.62709595]\n",
      " [-0.71741804]\n",
      " [-0.68984036]\n",
      " ...\n",
      " [ 0.86184532]\n",
      " [-0.68984036]\n",
      " [ 0.74165544]]\n",
      "t [[ 0.62709595]\n",
      " [-0.71741804]\n",
      " [-0.68984036]\n",
      " ...\n",
      " [ 0.86184532]\n",
      " [-0.68984036]\n",
      " [ 0.74165544]]\n",
      "Current iteration=6, loss=39005.145928866164\n",
      "t [[ 0.67798438]\n",
      " [-0.8250924 ]\n",
      " [-0.76765073]\n",
      " ...\n",
      " [ 0.92733432]\n",
      " [-0.76765073]\n",
      " [ 0.7893702 ]]\n",
      "t [[ 0.67798438]\n",
      " [-0.8250924 ]\n",
      " [-0.76765073]\n",
      " ...\n",
      " [ 0.92733432]\n",
      " [-0.76765073]\n",
      " [ 0.7893702 ]]\n",
      "t [[ 0.72119183]\n",
      " [-0.92552559]\n",
      " [-0.8393862 ]\n",
      " ...\n",
      " [ 0.98248558]\n",
      " [-0.8393862 ]\n",
      " [ 0.82829982]]\n",
      "t [[ 0.72119183]\n",
      " [-0.92552559]\n",
      " [-0.8393862 ]\n",
      " ...\n",
      " [ 0.98248558]\n",
      " [-0.8393862 ]\n",
      " [ 0.82829982]]\n",
      "Current iteration=8, loss=37201.39140868415\n",
      "t [[ 0.7581528 ]\n",
      " [-1.01919171]\n",
      " [-0.9058714 ]\n",
      " ...\n",
      " [ 1.02933089]\n",
      " [-0.9058714 ]\n",
      " [ 0.86038396]]\n",
      "t [[ 0.7581528 ]\n",
      " [-1.01919171]\n",
      " [-0.9058714 ]\n",
      " ...\n",
      " [ 1.02933089]\n",
      " [-0.9058714 ]\n",
      " [ 0.86038396]]\n",
      "t [[ 0.78999072]\n",
      " [-1.10664307]\n",
      " [-0.96776444]\n",
      " ...\n",
      " [ 1.06942994]\n",
      " [-0.96776444]\n",
      " [ 0.88706748]]\n",
      "loss=35866.841612048695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05782588]\n",
      " [-0.27689911]\n",
      " [-0.16237682]\n",
      " ...\n",
      " [ 0.13885374]\n",
      " [ 0.05782588]\n",
      " [-0.2353732 ]]\n",
      "t [[ 0.05782588]\n",
      " [-0.27689911]\n",
      " [-0.16237682]\n",
      " ...\n",
      " [ 0.13885374]\n",
      " [ 0.05782588]\n",
      " [-0.2353732 ]]\n",
      "t [[ 0.09548514]\n",
      " [-0.52182253]\n",
      " [-0.30065598]\n",
      " ...\n",
      " [ 0.22411041]\n",
      " [ 0.09548514]\n",
      " [-0.40449158]]\n",
      "t [[ 0.09548514]\n",
      " [-0.52182253]\n",
      " [-0.30065598]\n",
      " ...\n",
      " [ 0.22411041]\n",
      " [ 0.09548514]\n",
      " [-0.40449158]]\n",
      "Current iteration=2, loss=45207.36769150634\n",
      "t [[ 0.11939276]\n",
      " [-0.73799271]\n",
      " [-0.42104767]\n",
      " ...\n",
      " [ 0.27649904]\n",
      " [ 0.11939276]\n",
      " [-0.53344949]]\n",
      "t [[ 0.11939276]\n",
      " [-0.73799271]\n",
      " [-0.42104767]\n",
      " ...\n",
      " [ 0.27649904]\n",
      " [ 0.11939276]\n",
      " [-0.53344949]]\n",
      "t [[ 0.13380454]\n",
      " [-0.92920692]\n",
      " [-0.52772931]\n",
      " ...\n",
      " [ 0.30815523]\n",
      " [ 0.13380454]\n",
      " [-0.6372464 ]]\n",
      "t [[ 0.13380454]\n",
      " [-0.92920692]\n",
      " [-0.52772931]\n",
      " ...\n",
      " [ 0.30815523]\n",
      " [ 0.13380454]\n",
      " [-0.6372464 ]]\n",
      "Current iteration=4, loss=41258.20526736646\n",
      "t [[ 0.14153926]\n",
      " [-1.09908772]\n",
      " [-0.62354617]\n",
      " ...\n",
      " [ 0.32633954]\n",
      " [ 0.14153926]\n",
      " [-0.72448574]]\n",
      "t [[ 0.14153926]\n",
      " [-1.09908772]\n",
      " [-0.62354617]\n",
      " ...\n",
      " [ 0.32633954]\n",
      " [ 0.14153926]\n",
      " [-0.72448574]]\n",
      "t [[ 0.14450551]\n",
      " [-1.25082046]\n",
      " [-0.710503  ]\n",
      " ...\n",
      " [ 0.33551809]\n",
      " [ 0.14450551]\n",
      " [-0.80022328]]\n",
      "t [[ 0.14450551]\n",
      " [-1.25082046]\n",
      " [-0.710503  ]\n",
      " ...\n",
      " [ 0.33551809]\n",
      " [ 0.14450551]\n",
      " [-0.80022328]]\n",
      "Current iteration=6, loss=38671.05616644838\n",
      "t [[ 0.14402922]\n",
      " [-1.38710363]\n",
      " [-0.79006451]\n",
      " ...\n",
      " [ 0.33851502]\n",
      " [ 0.14402922]\n",
      " [-0.86753378]]\n",
      "t [[ 0.14402922]\n",
      " [-1.38710363]\n",
      " [-0.79006451]\n",
      " ...\n",
      " [ 0.33851502]\n",
      " [ 0.14402922]\n",
      " [-0.86753378]]\n",
      "t [[ 0.14105379]\n",
      " [-1.5101843 ]\n",
      " [-0.86333722]\n",
      " ...\n",
      " [ 0.33717273]\n",
      " [ 0.14105379]\n",
      " [-0.92836508]]\n",
      "t [[ 0.14105379]\n",
      " [-1.5101843 ]\n",
      " [-0.86333722]\n",
      " ...\n",
      " [ 0.33717273]\n",
      " [ 0.14105379]\n",
      " [-0.92836508]]\n",
      "Current iteration=8, loss=36855.98799178415\n",
      "t [[ 0.13626456]\n",
      " [-1.62192204]\n",
      " [-0.93118161]\n",
      " ...\n",
      " [ 0.33273185]\n",
      " [ 0.13626456]\n",
      " [-0.98400647]]\n",
      "t [[ 0.13626456]\n",
      " [-1.62192204]\n",
      " [-0.93118161]\n",
      " ...\n",
      " [ 0.33273185]\n",
      " [ 0.13626456]\n",
      " [-0.98400647]]\n",
      "t [[ 0.13016809]\n",
      " [-1.7238556 ]\n",
      " [-0.99428339]\n",
      " ...\n",
      " [ 0.32605304]\n",
      " [ 0.13016809]\n",
      " [-1.03534988]]\n",
      "loss=35519.2227041723\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.17462128]\n",
      " [-0.10179721]\n",
      " [-0.16291707]\n",
      " ...\n",
      " [ 0.1368214 ]\n",
      " [ 0.05759885]\n",
      " [-0.23181607]]\n",
      "t [[ 0.17462128]\n",
      " [-0.10179721]\n",
      " [-0.16291707]\n",
      " ...\n",
      " [ 0.1368214 ]\n",
      " [ 0.05759885]\n",
      " [-0.23181607]]\n",
      "t [[ 0.31274178]\n",
      " [-0.23049651]\n",
      " [-0.30148077]\n",
      " ...\n",
      " [ 0.22042685]\n",
      " [ 0.09496518]\n",
      " [-0.3974161 ]]\n",
      "t [[ 0.31274178]\n",
      " [-0.23049651]\n",
      " [-0.30148077]\n",
      " ...\n",
      " [ 0.22042685]\n",
      " [ 0.09496518]\n",
      " [-0.3974161 ]]\n",
      "Current iteration=2, loss=45195.54781420421\n",
      "t [[ 0.42339884]\n",
      " [-0.3665872 ]\n",
      " [-0.42199758]\n",
      " ...\n",
      " [ 0.27148733]\n",
      " [ 0.11855341]\n",
      " [-0.52307693]]\n",
      "t [[ 0.42339884]\n",
      " [-0.3665872 ]\n",
      " [-0.42199758]\n",
      " ...\n",
      " [ 0.27148733]\n",
      " [ 0.11855341]\n",
      " [-0.52307693]]\n",
      "t [[ 0.51328898]\n",
      " [-0.50050278]\n",
      " [-0.52870634]\n",
      " ...\n",
      " [ 0.30208486]\n",
      " [ 0.13264074]\n",
      " [-0.62386691]]\n",
      "t [[ 0.51328898]\n",
      " [-0.50050278]\n",
      " [-0.52870634]\n",
      " ...\n",
      " [ 0.30208486]\n",
      " [ 0.13264074]\n",
      " [-0.62386691]]\n",
      "Current iteration=4, loss=41245.16888403236\n",
      "t [[ 0.58722335]\n",
      " [-0.62807231]\n",
      " [-0.62449005]\n",
      " ...\n",
      " [ 0.3194315 ]\n",
      " [ 0.14005717]\n",
      " [-0.70839916]]\n",
      "t [[ 0.58722335]\n",
      " [-0.62807231]\n",
      " [-0.62449005]\n",
      " ...\n",
      " [ 0.3194315 ]\n",
      " [ 0.14005717]\n",
      " [-0.70839916]]\n",
      "t [[ 0.64870054]\n",
      " [-0.74779697]\n",
      " [-0.71137679]\n",
      " ...\n",
      " [ 0.32795028]\n",
      " [ 0.14271717]\n",
      " [-0.78171207]]\n",
      "t [[ 0.64870054]\n",
      " [-0.74779697]\n",
      " [-0.71137679]\n",
      " ...\n",
      " [ 0.32795028]\n",
      " [ 0.14271717]\n",
      " [-0.78171207]]\n",
      "Current iteration=6, loss=38658.28140441564\n",
      "t [[ 0.70031644]\n",
      " [-0.85944221]\n",
      " [-0.7908458 ]\n",
      " ...\n",
      " [ 0.33042981]\n",
      " [ 0.14194959]\n",
      " [-0.846853  ]]\n",
      "t [[ 0.70031644]\n",
      " [-0.85944221]\n",
      " [-0.7908458 ]\n",
      " ...\n",
      " [ 0.33042981]\n",
      " [ 0.14194959]\n",
      " [-0.846853  ]]\n",
      "t [[ 0.74403296]\n",
      " [-0.96334114]\n",
      " [-0.86401281]\n",
      " ...\n",
      " [ 0.32868425]\n",
      " [ 0.13869917]\n",
      " [-0.90574043]]\n",
      "t [[ 0.74403296]\n",
      " [-0.96334114]\n",
      " [-0.86401281]\n",
      " ...\n",
      " [ 0.32868425]\n",
      " [ 0.13869917]\n",
      " [-0.90574043]]\n",
      "Current iteration=8, loss=36843.53367123207\n",
      "t [[ 0.78135694]\n",
      " [-1.06005456]\n",
      " [-0.93174419]\n",
      " ...\n",
      " [ 0.32393207]\n",
      " [ 0.1336516 ]\n",
      " [-0.95963582]]\n",
      "t [[ 0.78135694]\n",
      " [-1.06005456]\n",
      " [-0.93174419]\n",
      " ...\n",
      " [ 0.32393207]\n",
      " [ 0.1336516 ]\n",
      " [-0.95963582]]\n",
      "t [[ 0.81346218]\n",
      " [-1.15020782]\n",
      " [-0.99472949]\n",
      " ...\n",
      " [ 0.31701657]\n",
      " [ 0.12731331]\n",
      " [-1.00940611]]\n",
      "loss=35506.89284843219\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.17238316]\n",
      " [-0.09744918]\n",
      " [-0.16230692]\n",
      " ...\n",
      " [ 0.13893237]\n",
      " [ 0.05866045]\n",
      " [-0.23895485]]\n",
      "t [[ 0.17238316]\n",
      " [-0.09744918]\n",
      " [-0.16230692]\n",
      " ...\n",
      " [ 0.13893237]\n",
      " [ 0.05866045]\n",
      " [-0.23895485]]\n",
      "t [[ 0.3087487 ]\n",
      " [-0.22192292]\n",
      " [-0.30026322]\n",
      " ...\n",
      " [ 0.22321834]\n",
      " [ 0.09684703]\n",
      " [-0.40970888]]\n",
      "t [[ 0.3087487 ]\n",
      " [-0.22192292]\n",
      " [-0.30026322]\n",
      " ...\n",
      " [ 0.22321834]\n",
      " [ 0.09684703]\n",
      " [-0.40970888]]\n",
      "Current iteration=2, loss=45253.777652642384\n",
      "t [[ 0.41803143]\n",
      " [-0.35382718]\n",
      " [-0.42021384]\n",
      " ...\n",
      " [ 0.2741512 ]\n",
      " [ 0.12110161]\n",
      " [-0.53926876]]\n",
      "t [[ 0.41803143]\n",
      " [-0.35382718]\n",
      " [-0.42021384]\n",
      " ...\n",
      " [ 0.2741512 ]\n",
      " [ 0.12110161]\n",
      " [-0.53926876]]\n",
      "t [[ 0.50684881]\n",
      " [-0.48360457]\n",
      " [-0.52640646]\n",
      " ...\n",
      " [ 0.30413744]\n",
      " [ 0.13575084]\n",
      " [-0.64312077]]\n",
      "t [[ 0.50684881]\n",
      " [-0.48360457]\n",
      " [-0.52640646]\n",
      " ...\n",
      " [ 0.30413744]\n",
      " [ 0.13575084]\n",
      " [-0.64312077]]\n",
      "Current iteration=4, loss=41351.47064742744\n",
      "t [[ 0.5799499 ]\n",
      " [-0.6071127 ]\n",
      " [-0.6217208 ]\n",
      " ...\n",
      " [ 0.32055807]\n",
      " [ 0.14365296]\n",
      " [-0.73012226]]\n",
      "t [[ 0.5799499 ]\n",
      " [-0.6071127 ]\n",
      " [-0.6217208 ]\n",
      " ...\n",
      " [ 0.32055807]\n",
      " [ 0.14365296]\n",
      " [-0.73012226]]\n",
      "t [[ 0.64078058]\n",
      " [-0.7228804 ]\n",
      " [-0.70817848]\n",
      " ...\n",
      " [ 0.32794279]\n",
      " [ 0.14673983]\n",
      " [-0.80546798]]\n",
      "t [[ 0.64078058]\n",
      " [-0.7228804 ]\n",
      " [-0.70817848]\n",
      " ...\n",
      " [ 0.32794279]\n",
      " [ 0.14673983]\n",
      " [-0.80546798]]\n",
      "Current iteration=6, loss=38799.83614738311\n",
      "t [[ 0.69189406]\n",
      " [-0.83069278]\n",
      " [-0.78725219]\n",
      " ...\n",
      " [ 0.32915177]\n",
      " [ 0.14635197]\n",
      " [-0.87231052]]\n",
      "t [[ 0.69189406]\n",
      " [-0.83069278]\n",
      " [-0.78725219]\n",
      " ...\n",
      " [ 0.32915177]\n",
      " [ 0.14635197]\n",
      " [-0.87231052]]\n",
      "t [[ 0.73521901]\n",
      " [-0.93089424]\n",
      " [-0.86005187]\n",
      " ...\n",
      " [ 0.3260479 ]\n",
      " [ 0.14344242]\n",
      " [-0.93264158]]\n",
      "t [[ 0.73521901]\n",
      " [-0.93089424]\n",
      " [-0.86005187]\n",
      " ...\n",
      " [ 0.3260479 ]\n",
      " [ 0.14344242]\n",
      " [-0.93264158]]\n",
      "Current iteration=8, loss=37011.34694669655\n",
      "t [[ 0.77223678]\n",
      " [-1.02405047]\n",
      " [-0.92743905]\n",
      " ...\n",
      " [ 0.31988322]\n",
      " [ 0.13870313]\n",
      " [-0.987775  ]]\n",
      "t [[ 0.77223678]\n",
      " [-1.02405047]\n",
      " [-0.92743905]\n",
      " ...\n",
      " [ 0.31988322]\n",
      " [ 0.13870313]\n",
      " [-0.987775  ]]\n",
      "t [[ 0.80410169]\n",
      " [-1.11078728]\n",
      " [-0.9900993 ]\n",
      " ...\n",
      " [ 0.31152442]\n",
      " [ 0.13264542]\n",
      " [-1.03861622]]\n",
      "loss=35694.82736895585\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.17210722]\n",
      " [-0.10238009]\n",
      " [-0.1617971 ]\n",
      " ...\n",
      " [ 0.24788852]\n",
      " [-0.1617971 ]\n",
      " [ 0.23218272]]\n",
      "t [[ 0.17210722]\n",
      " [-0.10238009]\n",
      " [-0.1617971 ]\n",
      " ...\n",
      " [ 0.24788852]\n",
      " [-0.1617971 ]\n",
      " [ 0.23218272]]\n",
      "t [[ 0.30824626]\n",
      " [-0.23032279]\n",
      " [-0.2994533 ]\n",
      " ...\n",
      " [ 0.43754838]\n",
      " [-0.2994533 ]\n",
      " [ 0.40040612]]\n",
      "t [[ 0.30824626]\n",
      " [-0.23032279]\n",
      " [-0.2994533 ]\n",
      " ...\n",
      " [ 0.43754838]\n",
      " [-0.2994533 ]\n",
      " [ 0.40040612]]\n",
      "Current iteration=2, loss=45257.61187773725\n",
      "t [[ 0.4173398 ]\n",
      " [-0.36491315]\n",
      " [-0.41920437]\n",
      " ...\n",
      " [ 0.58569097]\n",
      " [-0.41920437]\n",
      " [ 0.52547321]]\n",
      "t [[ 0.4173398 ]\n",
      " [-0.36491315]\n",
      " [-0.41920437]\n",
      " ...\n",
      " [ 0.58569097]\n",
      " [-0.41920437]\n",
      " [ 0.52547321]]\n",
      "t [[ 0.50591755]\n",
      " [-0.49700342]\n",
      " [-0.52524855]\n",
      " ...\n",
      " [ 0.70365509]\n",
      " [-0.52524855]\n",
      " [ 0.62075066]]\n",
      "t [[ 0.50591755]\n",
      " [-0.49700342]\n",
      " [-0.52524855]\n",
      " ...\n",
      " [ 0.70365509]\n",
      " [-0.52524855]\n",
      " [ 0.62075066]]\n",
      "Current iteration=4, loss=41346.02702038176\n",
      "t [[ 0.57870183]\n",
      " [-0.62264189]\n",
      " [-0.62044098]\n",
      " ...\n",
      " [ 0.79914132]\n",
      " [-0.62044098]\n",
      " [ 0.69484544]]\n",
      "t [[ 0.57870183]\n",
      " [-0.62264189]\n",
      " [-0.62044098]\n",
      " ...\n",
      " [ 0.79914132]\n",
      " [-0.62044098]\n",
      " [ 0.69484544]]\n",
      "t [[ 0.63914839]\n",
      " [-0.74043456]\n",
      " [-0.70679068]\n",
      " ...\n",
      " [ 0.8775061 ]\n",
      " [-0.70679068]\n",
      " [ 0.753457  ]]\n",
      "t [[ 0.63914839]\n",
      " [-0.74043456]\n",
      " [-0.70679068]\n",
      " ...\n",
      " [ 0.8775061 ]\n",
      " [-0.70679068]\n",
      " [ 0.753457  ]]\n",
      "Current iteration=6, loss=38786.15732597865\n",
      "t [[ 0.68982806]\n",
      " [-0.85019571]\n",
      " [-0.78576422]\n",
      " ...\n",
      " [ 0.94257719]\n",
      " [-0.78576422]\n",
      " [ 0.80048251]]\n",
      "t [[ 0.68982806]\n",
      " [-0.85019571]\n",
      " [-0.78576422]\n",
      " ...\n",
      " [ 0.94257719]\n",
      " [-0.78576422]\n",
      " [ 0.80048251]]\n",
      "t [[ 0.73268531]\n",
      " [-0.95227959]\n",
      " [-0.85846874]\n",
      " ...\n",
      " [ 0.99715976]\n",
      " [-0.85846874]\n",
      " [ 0.83866885]]\n",
      "t [[ 0.73268531]\n",
      " [-0.95227959]\n",
      " [-0.85846874]\n",
      " ...\n",
      " [ 0.99715976]\n",
      " [-0.85846874]\n",
      " [ 0.83866885]]\n",
      "Current iteration=8, loss=36991.252430349035\n",
      "t [[ 0.76921373]\n",
      " [-1.0472543 ]\n",
      " [-0.92576463]\n",
      " ...\n",
      " [ 1.04335576]\n",
      " [-0.92576463]\n",
      " [ 0.87000491]]\n",
      "t [[ 0.76921373]\n",
      " [-1.0472543 ]\n",
      " [-0.92576463]\n",
      " ...\n",
      " [ 1.04335576]\n",
      " [-0.92576463]\n",
      " [ 0.87000491]]\n",
      "t [[ 0.80057662]\n",
      " [-1.13574571]\n",
      " [-0.98833702]\n",
      " ...\n",
      " [ 1.08277084]\n",
      " [-0.98833702]\n",
      " [ 0.89596464]]\n",
      "loss=35669.76436261829\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05969123]\n",
      " [-0.28583134]\n",
      " [-0.16761478]\n",
      " ...\n",
      " [ 0.14333289]\n",
      " [ 0.05969123]\n",
      " [-0.24296589]]\n",
      "t [[ 0.05969123]\n",
      " [-0.28583134]\n",
      " [-0.16761478]\n",
      " ...\n",
      " [ 0.14333289]\n",
      " [ 0.05969123]\n",
      " [-0.24296589]]\n",
      "t [[ 0.09790359]\n",
      " [-0.53760072]\n",
      " [-0.30956238]\n",
      " ...\n",
      " [ 0.22957955]\n",
      " [ 0.09790359]\n",
      " [-0.415366  ]]\n",
      "t [[ 0.09790359]\n",
      " [-0.53760072]\n",
      " [-0.30956238]\n",
      " ...\n",
      " [ 0.22957955]\n",
      " [ 0.09790359]\n",
      " [-0.415366  ]]\n",
      "Current iteration=2, loss=45036.50294707342\n",
      "t [[ 0.12169324]\n",
      " [-0.75886725]\n",
      " [-0.43267586]\n",
      " ...\n",
      " [ 0.28152536]\n",
      " [ 0.12169324]\n",
      " [-0.54588752]]\n",
      "t [[ 0.12169324]\n",
      " [-0.75886725]\n",
      " [-0.43267586]\n",
      " ...\n",
      " [ 0.28152536]\n",
      " [ 0.12169324]\n",
      " [-0.54588752]]\n",
      "t [[ 0.13565388]\n",
      " [-0.95382027]\n",
      " [-0.54146865]\n",
      " ...\n",
      " [ 0.31219837]\n",
      " [ 0.13565388]\n",
      " [-0.65060412]]\n",
      "t [[ 0.13565388]\n",
      " [-0.95382027]\n",
      " [-0.54146865]\n",
      " ...\n",
      " [ 0.31219837]\n",
      " [ 0.13565388]\n",
      " [-0.65060412]]\n",
      "Current iteration=4, loss=41046.78638706243\n",
      "t [[ 0.1427812 ]\n",
      " [-1.12642043]\n",
      " [-0.63897264]\n",
      " ...\n",
      " [ 0.32924044]\n",
      " [ 0.1427812 ]\n",
      " [-0.73852903]]\n",
      "t [[ 0.1427812 ]\n",
      " [-1.12642043]\n",
      " [-0.63897264]\n",
      " ...\n",
      " [ 0.32924044]\n",
      " [ 0.1427812 ]\n",
      " [-0.73852903]]\n",
      "t [[ 0.14507899]\n",
      " [-1.28011631]\n",
      " [-0.72730373]\n",
      " ...\n",
      " [ 0.33727345]\n",
      " [ 0.14507899]\n",
      " [-0.81486041]]\n",
      "t [[ 0.14507899]\n",
      " [-1.28011631]\n",
      " [-0.72730373]\n",
      " ...\n",
      " [ 0.33727345]\n",
      " [ 0.14507899]\n",
      " [-0.81486041]]\n",
      "Current iteration=6, loss=38456.94335165662\n",
      "t [[ 0.14392577]\n",
      " [-1.41780226]\n",
      " [-0.80799796]\n",
      " ...\n",
      " [ 0.33918082]\n",
      " [ 0.14392577]\n",
      " [-0.88271454]]\n",
      "t [[ 0.14392577]\n",
      " [-1.41780226]\n",
      " [-0.80799796]\n",
      " ...\n",
      " [ 0.33918082]\n",
      " [ 0.14392577]\n",
      " [-0.88271454]]\n",
      "t [[ 0.14029442]\n",
      " [-1.54186848]\n",
      " [-0.88221092]\n",
      " ...\n",
      " [ 0.33682455]\n",
      " [ 0.14029442]\n",
      " [-0.94404645]]\n",
      "t [[ 0.14029442]\n",
      " [-1.54186848]\n",
      " [-0.88221092]\n",
      " ...\n",
      " [ 0.33682455]\n",
      " [ 0.14029442]\n",
      " [-0.94404645]]\n",
      "Current iteration=8, loss=36651.60075800115\n",
      "t [[ 0.13488669]\n",
      " [-1.6542788 ]\n",
      " [-0.95083871]\n",
      " ...\n",
      " [ 0.33144906]\n",
      " [ 0.13488669]\n",
      " [-1.00014319]]\n",
      "t [[ 0.13488669]\n",
      " [-1.6542788 ]\n",
      " [-0.95083871]\n",
      " ...\n",
      " [ 0.33144906]\n",
      " [ 0.13488669]\n",
      " [-1.00014319]]\n",
      "t [[ 0.12821794]\n",
      " [-1.75664824]\n",
      " [-1.014594  ]\n",
      " ...\n",
      " [ 0.32391268]\n",
      " [ 0.12821794]\n",
      " [-1.05189353]]\n",
      "loss=35328.32489409494\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.18025423]\n",
      " [-0.10508099]\n",
      " [-0.16817245]\n",
      " ...\n",
      " [ 0.14123499]\n",
      " [ 0.05945688]\n",
      " [-0.239294  ]]\n",
      "t [[ 0.18025423]\n",
      " [-0.10508099]\n",
      " [-0.16817245]\n",
      " ...\n",
      " [ 0.14123499]\n",
      " [ 0.05945688]\n",
      " [-0.239294  ]]\n",
      "t [[ 0.32163306]\n",
      " [-0.238812  ]\n",
      " [-0.31040546]\n",
      " ...\n",
      " [ 0.22578955]\n",
      " [ 0.09736475]\n",
      " [-0.40806373]]\n",
      "t [[ 0.32163306]\n",
      " [-0.238812  ]\n",
      " [-0.31040546]\n",
      " ...\n",
      " [ 0.22578955]\n",
      " [ 0.09736475]\n",
      " [-0.40806373]]\n",
      "Current iteration=2, loss=45024.538708757376\n",
      "t [[ 0.43408442]\n",
      " [-0.37975806]\n",
      " [-0.43363791]\n",
      " ...\n",
      " [ 0.27638534]\n",
      " [ 0.12082302]\n",
      " [-0.53519658]]\n",
      "t [[ 0.43408442]\n",
      " [-0.37975806]\n",
      " [-0.43363791]\n",
      " ...\n",
      " [ 0.27638534]\n",
      " [ 0.12082302]\n",
      " [-0.53519658]]\n",
      "t [[ 0.52486819]\n",
      " [-0.51777234]\n",
      " [-0.54244931]\n",
      " ...\n",
      " [ 0.3059918 ]\n",
      " [ 0.13444825]\n",
      " [-0.6368377 ]]\n",
      "t [[ 0.52486819]\n",
      " [-0.51777234]\n",
      " [-0.54244931]\n",
      " ...\n",
      " [ 0.3059918 ]\n",
      " [ 0.13444825]\n",
      " [-0.6368377 ]]\n",
      "Current iteration=4, loss=41033.7416257295\n",
      "t [[ 0.59913361]\n",
      " [-0.64863286]\n",
      " [-0.6399114 ]\n",
      " ...\n",
      " [ 0.32219764]\n",
      " [ 0.14124778]\n",
      " [-0.72200706]]\n",
      "t [[ 0.59913361]\n",
      " [-0.64863286]\n",
      " [-0.6399114 ]\n",
      " ...\n",
      " [ 0.32219764]\n",
      " [ 0.14124778]\n",
      " [-0.72200706]]\n",
      "t [[ 0.66058914]\n",
      " [-0.77095154]\n",
      " [-0.72816421]\n",
      " ...\n",
      " [ 0.32957811]\n",
      " [ 0.14323135]\n",
      " [-0.79588117]]\n",
      "t [[ 0.66058914]\n",
      " [-0.77095154]\n",
      " [-0.72816421]\n",
      " ...\n",
      " [ 0.32957811]\n",
      " [ 0.14323135]\n",
      " [-0.79588117]]\n",
      "Current iteration=6, loss=38444.20735837955\n",
      "t [[ 0.71196476]\n",
      " [-0.88463167]\n",
      " [-0.80875865]\n",
      " ...\n",
      " [ 0.33097879]\n",
      " [ 0.14178035]\n",
      " [-0.8615451 ]]\n",
      "t [[ 0.71196476]\n",
      " [-0.88463167]\n",
      " [-0.80875865]\n",
      " ...\n",
      " [ 0.33097879]\n",
      " [ 0.14178035]\n",
      " [-0.8615451 ]]\n",
      "t [[ 0.75530988]\n",
      " [-0.99012876]\n",
      " [-0.88285952]\n",
      " ...\n",
      " [ 0.32823188]\n",
      " [ 0.13786883]\n",
      " [-0.92092137]]\n",
      "t [[ 0.75530988]\n",
      " [-0.99012876]\n",
      " [-0.88285952]\n",
      " ...\n",
      " [ 0.32823188]\n",
      " [ 0.13786883]\n",
      " [-0.92092137]]\n",
      "Current iteration=8, loss=36639.18113794837\n",
      "t [[ 0.79218924]\n",
      " [-1.08810149]\n",
      " [-0.95136878]\n",
      " ...\n",
      " [ 0.32255868]\n",
      " [ 0.13219877]\n",
      " [-0.97526682]]\n",
      "t [[ 0.79218924]\n",
      " [-1.08810149]\n",
      " [-0.95136878]\n",
      " ...\n",
      " [ 0.32255868]\n",
      " [ 0.13219877]\n",
      " [-0.97526682]]\n",
      "t [[ 0.82381531]\n",
      " [-1.1792504 ]\n",
      " [-1.01500282]\n",
      " ...\n",
      " [ 0.31479957]\n",
      " [ 0.12528524]\n",
      " [-1.0254435 ]]\n",
      "loss=35316.00169337884\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.17794391]\n",
      " [-0.1005927 ]\n",
      " [-0.16754262]\n",
      " ...\n",
      " [ 0.14341406]\n",
      " [ 0.06055273]\n",
      " [-0.24666307]]\n",
      "t [[ 0.17794391]\n",
      " [-0.1005927 ]\n",
      " [-0.16754262]\n",
      " ...\n",
      " [ 0.14341406]\n",
      " [ 0.06055273]\n",
      " [-0.24666307]]\n",
      "t [[ 0.31752691]\n",
      " [-0.22996586]\n",
      " [-0.30914891]\n",
      " ...\n",
      " [ 0.22862578]\n",
      " [ 0.09929956]\n",
      " [-0.42068915]]\n",
      "t [[ 0.31752691]\n",
      " [-0.22996586]\n",
      " [-0.30914891]\n",
      " ...\n",
      " [ 0.22862578]\n",
      " [ 0.09929956]\n",
      " [-0.42068915]]\n",
      "Current iteration=2, loss=45084.6004585434\n",
      "t [[ 0.42858419]\n",
      " [-0.36659313]\n",
      " [-0.43179977]\n",
      " ...\n",
      " [ 0.27903918]\n",
      " [ 0.1234359 ]\n",
      " [-0.55176716]]\n",
      "t [[ 0.42858419]\n",
      " [-0.36659313]\n",
      " [-0.43179977]\n",
      " ...\n",
      " [ 0.27903918]\n",
      " [ 0.1234359 ]\n",
      " [-0.55176716]]\n",
      "t [[ 0.51828987]\n",
      " [-0.50034025]\n",
      " [-0.54008325]\n",
      " ...\n",
      " [ 0.30796744]\n",
      " [ 0.13763108]\n",
      " [-0.65648882]]\n",
      "t [[ 0.51828987]\n",
      " [-0.50034025]\n",
      " [-0.54008325]\n",
      " ...\n",
      " [ 0.30796744]\n",
      " [ 0.13763108]\n",
      " [-0.65648882]]\n",
      "Current iteration=4, loss=41142.77940494337\n",
      "t [[ 0.59172571]\n",
      " [-0.6270181 ]\n",
      " [-0.63706675]\n",
      " ...\n",
      " [ 0.32317636]\n",
      " [ 0.14492206]\n",
      " [-0.74413117]]\n",
      "t [[ 0.59172571]\n",
      " [-0.6270181 ]\n",
      " [-0.63706675]\n",
      " ...\n",
      " [ 0.32317636]\n",
      " [ 0.14492206]\n",
      " [-0.74413117]]\n",
      "t [[ 0.65254335]\n",
      " [-0.74526879]\n",
      " [-0.724883  ]\n",
      " ...\n",
      " [ 0.3293524 ]\n",
      " [ 0.14733665]\n",
      " [-0.82003375]]\n",
      "t [[ 0.65254335]\n",
      " [-0.74526879]\n",
      " [-0.724883  ]\n",
      " ...\n",
      " [ 0.3293524 ]\n",
      " [ 0.14733665]\n",
      " [-0.82003375]]\n",
      "Current iteration=6, loss=38588.773011027064\n",
      "t [[ 0.70342749]\n",
      " [-0.85501606]\n",
      " [-0.80507572]\n",
      " ...\n",
      " [ 0.32941497]\n",
      " [ 0.1462683 ]\n",
      " [-0.88739039]]\n",
      "t [[ 0.70342749]\n",
      " [-0.85501606]\n",
      " [-0.80507572]\n",
      " ...\n",
      " [ 0.32941497]\n",
      " [ 0.1462683 ]\n",
      " [-0.88739039]]\n",
      "t [[ 0.74639271]\n",
      " [-0.95672652]\n",
      " [-0.87880351]\n",
      " ...\n",
      " [ 0.325246  ]\n",
      " [ 0.14269971]\n",
      " [-0.94819873]]\n",
      "t [[ 0.74639271]\n",
      " [-0.95672652]\n",
      " [-0.87880351]\n",
      " ...\n",
      " [ 0.325246  ]\n",
      " [ 0.14269971]\n",
      " [-0.94819873]]\n",
      "Current iteration=8, loss=36810.01877893252\n",
      "t [[ 0.78297738]\n",
      " [-1.05106304]\n",
      " [-0.9469632 ]\n",
      " ...\n",
      " [ 0.31810062]\n",
      " [ 0.13733932]\n",
      " [-1.00376906]]\n",
      "t [[ 0.78297738]\n",
      " [-1.05106304]\n",
      " [-0.9469632 ]\n",
      " ...\n",
      " [ 0.31810062]\n",
      " [ 0.13733932]\n",
      " [-1.00376906]]\n",
      "t [[ 0.81437395]\n",
      " [-1.13872571]\n",
      " [-1.01026705]\n",
      " ...\n",
      " [ 0.30884258]\n",
      " [ 0.13070723]\n",
      " [-1.05500256]]\n",
      "loss=35506.86373460191\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.17765907]\n",
      " [-0.10568267]\n",
      " [-0.16701636]\n",
      " ...\n",
      " [ 0.25588492]\n",
      " [-0.16701636]\n",
      " [ 0.23967248]]\n",
      "t [[ 0.17765907]\n",
      " [-0.10568267]\n",
      " [-0.16701636]\n",
      " ...\n",
      " [ 0.25588492]\n",
      " [-0.16701636]\n",
      " [ 0.23967248]]\n",
      "t [[ 0.31700875]\n",
      " [-0.23858995]\n",
      " [-0.30831958]\n",
      " ...\n",
      " [ 0.44975121]\n",
      " [-0.30831958]\n",
      " [ 0.41122371]]\n",
      "t [[ 0.31700875]\n",
      " [-0.23858995]\n",
      " [-0.30831958]\n",
      " ...\n",
      " [ 0.44975121]\n",
      " [-0.30831958]\n",
      " [ 0.41122371]]\n",
      "Current iteration=2, loss=45088.22488132517\n",
      "t [[ 0.42786867]\n",
      " [-0.37794372]\n",
      " [-0.43077083]\n",
      " ...\n",
      " [ 0.59998357]\n",
      " [-0.43077083]\n",
      " [ 0.53753374]]\n",
      "t [[ 0.42786867]\n",
      " [-0.37794372]\n",
      " [-0.43077083]\n",
      " ...\n",
      " [ 0.59998357]\n",
      " [-0.43077083]\n",
      " [ 0.53753374]]\n",
      "t [[ 0.51731895]\n",
      " [-0.51404589]\n",
      " [-0.53890589]\n",
      " ...\n",
      " [ 0.71883519]\n",
      " [-0.53890589]\n",
      " [ 0.63300851]]\n",
      "t [[ 0.51731895]\n",
      " [-0.51404589]\n",
      " [-0.53890589]\n",
      " ...\n",
      " [ 0.71883519]\n",
      " [-0.53890589]\n",
      " [ 0.63300851]]\n",
      "Current iteration=4, loss=41136.74775502001\n",
      "t [[ 0.59041727]\n",
      " [-0.64290094]\n",
      " [-0.63576679]\n",
      " ...\n",
      " [ 0.81450896]\n",
      " [-0.63576679]\n",
      " [ 0.70677151]]\n",
      "t [[ 0.59041727]\n",
      " [-0.64290094]\n",
      " [-0.63576679]\n",
      " ...\n",
      " [ 0.81450896]\n",
      " [-0.63576679]\n",
      " [ 0.70677151]]\n",
      "t [[ 0.65082833]\n",
      " [-0.76322553]\n",
      " [-0.7234737 ]\n",
      " ...\n",
      " [ 0.89264866]\n",
      " [-0.7234737 ]\n",
      " [ 0.76478835]]\n",
      "t [[ 0.65082833]\n",
      " [-0.76322553]\n",
      " [-0.7234737 ]\n",
      " ...\n",
      " [ 0.89264866]\n",
      " [-0.7234737 ]\n",
      " [ 0.76478835]]\n",
      "Current iteration=6, loss=38574.38158832221\n",
      "t [[ 0.7012563 ]\n",
      " [-0.87496981]\n",
      " [-0.80356449]\n",
      " ...\n",
      " [ 0.95725346]\n",
      " [-0.80356449]\n",
      " [ 0.81109923]]\n",
      "t [[ 0.7012563 ]\n",
      " [-0.87496981]\n",
      " [-0.80356449]\n",
      " ...\n",
      " [ 0.95725346]\n",
      " [-0.80356449]\n",
      " [ 0.81109923]]\n",
      "t [[ 0.74373241]\n",
      " [-0.97860828]\n",
      " [-0.87719521]\n",
      " ...\n",
      " [ 1.01123448]\n",
      " [-0.87719521]\n",
      " [ 0.84853083]]\n",
      "t [[ 0.74373241]\n",
      " [-0.97860828]\n",
      " [-0.87719521]\n",
      " ...\n",
      " [ 1.01123448]\n",
      " [-0.87719521]\n",
      " [ 0.84853083]]\n",
      "Current iteration=8, loss=36789.18438096184\n",
      "t [[ 0.77980759]\n",
      " [-1.07480515]\n",
      " [-0.94526171]\n",
      " ...\n",
      " [ 1.05676106]\n",
      " [-0.94526171]\n",
      " [ 0.87911785]]\n",
      "t [[ 0.77980759]\n",
      " [-1.07480515]\n",
      " [-0.94526171]\n",
      " ...\n",
      " [ 1.05676106]\n",
      " [-0.94526171]\n",
      " [ 0.87911785]]\n",
      "t [[ 0.81068341]\n",
      " [-1.16426007]\n",
      " [-1.00847593]\n",
      " ...\n",
      " [ 1.09548237]\n",
      " [-1.00847593]\n",
      " [ 0.90436077]]\n",
      "loss=35481.08293621003\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06155658]\n",
      " [-0.29476357]\n",
      " [-0.17285274]\n",
      " ...\n",
      " [ 0.14781204]\n",
      " [ 0.06155658]\n",
      " [-0.25055857]]\n",
      "t [[ 0.06155658]\n",
      " [-0.29476357]\n",
      " [-0.17285274]\n",
      " ...\n",
      " [ 0.14781204]\n",
      " [ 0.06155658]\n",
      " [-0.25055857]]\n",
      "t [[ 0.10028162]\n",
      " [-0.55331395]\n",
      " [-0.31842027]\n",
      " ...\n",
      " [ 0.23494106]\n",
      " [ 0.10028162]\n",
      " [-0.42610769]]\n",
      "t [[ 0.10028162]\n",
      " [-0.55331395]\n",
      " [-0.31842027]\n",
      " ...\n",
      " [ 0.23494106]\n",
      " [ 0.10028162]\n",
      " [-0.42610769]]\n",
      "Current iteration=2, loss=44868.03155046554\n",
      "t [[ 0.12391306]\n",
      " [-0.77956996]\n",
      " [-0.44419818]\n",
      " ...\n",
      " [ 0.28635761]\n",
      " [ 0.12391306]\n",
      " [-0.55808995]]\n",
      "t [[ 0.12391306]\n",
      " [-0.77956996]\n",
      " [-0.44419818]\n",
      " ...\n",
      " [ 0.28635761]\n",
      " [ 0.12391306]\n",
      " [-0.55808995]]\n",
      "t [[ 0.13739285]\n",
      " [-0.97813842]\n",
      " [-0.5550468 ]\n",
      " ...\n",
      " [ 0.31599966]\n",
      " [ 0.13739285]\n",
      " [-0.66366888]]\n",
      "t [[ 0.13739285]\n",
      " [-0.97813842]\n",
      " [-0.5550468 ]\n",
      " ...\n",
      " [ 0.31599966]\n",
      " [ 0.13739285]\n",
      " [-0.66366888]]\n",
      "Current iteration=4, loss=40840.665940907675\n",
      "t [[ 0.1438943 ]\n",
      " [-1.15333478]\n",
      " [-0.65418664]\n",
      " ...\n",
      " [ 0.33188136]\n",
      " [ 0.1438943 ]\n",
      " [-0.75225152]]\n",
      "t [[ 0.1438943 ]\n",
      " [-1.15333478]\n",
      " [-0.65418664]\n",
      " ...\n",
      " [ 0.33188136]\n",
      " [ 0.1438943 ]\n",
      " [-0.75225152]]\n",
      "t [[ 0.14551471]\n",
      " [-1.30887957]\n",
      " [-0.7438445 ]\n",
      " ...\n",
      " [ 0.338769  ]\n",
      " [ 0.14551471]\n",
      " [-0.82916286]]\n",
      "t [[ 0.14551471]\n",
      " [-1.30887957]\n",
      " [-0.7438445 ]\n",
      " ...\n",
      " [ 0.338769  ]\n",
      " [ 0.14551471]\n",
      " [-0.82916286]]\n",
      "Current iteration=6, loss=38249.78225377886\n",
      "t [[ 0.14368325]\n",
      " [-1.44786646]\n",
      " [-0.82562732]\n",
      " ...\n",
      " [ 0.33959742]\n",
      " [ 0.14368325]\n",
      " [-0.89755132]]\n",
      "t [[ 0.14368325]\n",
      " [-1.44786646]\n",
      " [-0.82562732]\n",
      " ...\n",
      " [ 0.33959742]\n",
      " [ 0.14368325]\n",
      " [-0.89755132]]\n",
      "t [[ 0.13940062]\n",
      " [-1.57282946]\n",
      " [-0.90073935]\n",
      " ...\n",
      " [ 0.33624355]\n",
      " [ 0.13940062]\n",
      " [-0.95937444]]\n",
      "t [[ 0.13940062]\n",
      " [-1.57282946]\n",
      " [-0.90073935]\n",
      " ...\n",
      " [ 0.33624355]\n",
      " [ 0.13940062]\n",
      " [-0.95937444]]\n",
      "Current iteration=8, loss=36454.93754517238\n",
      "t [[ 0.13338364]\n",
      " [-1.68583598]\n",
      " [-0.97011206]\n",
      " ...\n",
      " [ 0.32995297]\n",
      " [ 0.13338364]\n",
      " [-1.01591517]]\n",
      "t [[ 0.13338364]\n",
      " [-1.68583598]\n",
      " [-0.97011206]\n",
      " ...\n",
      " [ 0.32995297]\n",
      " [ 0.13338364]\n",
      " [-1.01591517]]\n",
      "t [[ 0.12615537]\n",
      " [-1.7885761 ]\n",
      " [-1.03448493]\n",
      " ...\n",
      " [ 0.32158029]\n",
      " [ 0.12615537]\n",
      " [-1.06805883]]\n",
      "loss=35145.41833675407\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.18588717]\n",
      " [-0.10836477]\n",
      " [-0.17342784]\n",
      " ...\n",
      " [ 0.14564858]\n",
      " [ 0.06131491]\n",
      " [-0.24677194]]\n",
      "t [[ 0.18588717]\n",
      " [-0.10836477]\n",
      " [-0.17342784]\n",
      " ...\n",
      " [ 0.14564858]\n",
      " [ 0.06131491]\n",
      " [-0.24677194]]\n",
      "t [[ 0.33045126]\n",
      " [-0.24718101]\n",
      " [-0.31928113]\n",
      " ...\n",
      " [ 0.23104537]\n",
      " [ 0.09972378]\n",
      " [-0.41857876]]\n",
      "t [[ 0.33045126]\n",
      " [-0.24718101]\n",
      " [-0.31928113]\n",
      " ...\n",
      " [ 0.23104537]\n",
      " [ 0.09972378]\n",
      " [-0.41857876]]\n",
      "Current iteration=2, loss=44855.93251051697\n",
      "t [[ 0.444609  ]\n",
      " [-0.39297039]\n",
      " [-0.44517144]\n",
      " ...\n",
      " [ 0.28109119]\n",
      " [ 0.12301182]\n",
      " [-0.547082  ]]\n",
      "t [[ 0.444609  ]\n",
      " [-0.39297039]\n",
      " [-0.44517144]\n",
      " ...\n",
      " [ 0.28109119]\n",
      " [ 0.12301182]\n",
      " [-0.547082  ]]\n",
      "t [[ 0.53620531]\n",
      " [-0.53501443]\n",
      " [-0.55602997]\n",
      " ...\n",
      " [ 0.30966006]\n",
      " [ 0.13614532]\n",
      " [-0.64951902]]\n",
      "t [[ 0.53620531]\n",
      " [-0.53501443]\n",
      " [-0.55602997]\n",
      " ...\n",
      " [ 0.30966006]\n",
      " [ 0.13614532]\n",
      " [-0.64951902]]\n",
      "Current iteration=4, loss=40827.61911663659\n",
      "t [[ 0.61073414]\n",
      " [-0.66906841]\n",
      " [-0.65511911]\n",
      " ...\n",
      " [ 0.32470806]\n",
      " [ 0.14230966]\n",
      " [-0.73530003]]\n",
      "t [[ 0.61073414]\n",
      " [-0.66906841]\n",
      " [-0.65511911]\n",
      " ...\n",
      " [ 0.32470806]\n",
      " [ 0.14230966]\n",
      " [-0.73530003]]\n",
      "t [[ 0.67211481]\n",
      " [-0.79387573]\n",
      " [-0.74469062]\n",
      " ...\n",
      " [ 0.33095129]\n",
      " [ 0.14360812]\n",
      " [-0.80972388]]\n",
      "t [[ 0.67211481]\n",
      " [-0.79387573]\n",
      " [-0.74469062]\n",
      " ...\n",
      " [ 0.33095129]\n",
      " [ 0.14360812]\n",
      " [-0.80972388]]\n",
      "Current iteration=6, loss=38237.085825713664\n",
      "t [[ 0.72321045]\n",
      " [-0.90948892]\n",
      " [-0.8263665 ]\n",
      " ...\n",
      " [ 0.33128434]\n",
      " [ 0.14147265]\n",
      " [-0.87590366]]\n",
      "t [[ 0.72321045]\n",
      " [-0.90948892]\n",
      " [-0.8263665 ]\n",
      " ...\n",
      " [ 0.33128434]\n",
      " [ 0.14147265]\n",
      " [-0.87590366]]\n",
      "t [[ 0.76615624]\n",
      " [-1.01649101]\n",
      " [-0.90136025]\n",
      " ...\n",
      " [ 0.32755287]\n",
      " [ 0.13690495]\n",
      " [-0.93576124]]\n",
      "t [[ 0.76615624]\n",
      " [-1.01649101]\n",
      " [-0.90136025]\n",
      " ...\n",
      " [ 0.32755287]\n",
      " [ 0.13690495]\n",
      " [-0.93576124]]\n",
      "Current iteration=8, loss=36442.54944732863\n",
      "t [[ 0.80257305]\n",
      " [-1.11564016]\n",
      " [-0.9706091 ]\n",
      " ...\n",
      " [ 0.3209784 ]\n",
      " [ 0.13062195]\n",
      " [-0.99054698]]\n",
      "t [[ 0.80257305]\n",
      " [-1.11564016]\n",
      " [-0.9706091 ]\n",
      " ...\n",
      " [ 0.3209784 ]\n",
      " [ 0.13062195]\n",
      " [-0.99054698]]\n",
      "t [[ 0.83371035]\n",
      " [-1.20771192]\n",
      " [-1.03485615]\n",
      " ...\n",
      " [ 0.31239707]\n",
      " [ 0.12314624]\n",
      " [-1.04111773]]\n",
      "loss=35133.09641835651\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.18350466]\n",
      " [-0.10373623]\n",
      " [-0.17277833]\n",
      " ...\n",
      " [ 0.14789575]\n",
      " [ 0.062445  ]\n",
      " [-0.25437129]]\n",
      "t [[ 0.18350466]\n",
      " [-0.10373623]\n",
      " [-0.17277833]\n",
      " ...\n",
      " [ 0.14789575]\n",
      " [ 0.062445  ]\n",
      " [-0.25437129]]\n",
      "t [[ 0.326233  ]\n",
      " [-0.23806257]\n",
      " [-0.3179856 ]\n",
      " ...\n",
      " [ 0.23392371]\n",
      " [ 0.10171109]\n",
      " [-0.43153303]]\n",
      "t [[ 0.326233  ]\n",
      " [-0.23806257]\n",
      " [-0.3179856 ]\n",
      " ...\n",
      " [ 0.23392371]\n",
      " [ 0.10171109]\n",
      " [-0.43153303]]\n",
      "Current iteration=2, loss=44917.81780374318\n",
      "t [[ 0.43897818]\n",
      " [-0.37940077]\n",
      " [-0.44327918]\n",
      " ...\n",
      " [ 0.2837305 ]\n",
      " [ 0.12568851]\n",
      " [-0.56402424]]\n",
      "t [[ 0.43897818]\n",
      " [-0.37940077]\n",
      " [-0.44327918]\n",
      " ...\n",
      " [ 0.2837305 ]\n",
      " [ 0.12568851]\n",
      " [-0.56402424]]\n",
      "t [[ 0.52949239]\n",
      " [-0.517049  ]\n",
      " [-0.55359832]\n",
      " ...\n",
      " [ 0.31155306]\n",
      " [ 0.13939968]\n",
      " [-0.66955782]]\n",
      "t [[ 0.52949239]\n",
      " [-0.517049  ]\n",
      " [-0.55359832]\n",
      " ...\n",
      " [ 0.31155306]\n",
      " [ 0.13939968]\n",
      " [-0.66955782]]\n",
      "Current iteration=4, loss=40939.3443833488\n",
      "t [[ 0.60319633]\n",
      " [-0.64679995]\n",
      " [-0.65219995]\n",
      " ...\n",
      " [ 0.3255327 ]\n",
      " [ 0.14606091]\n",
      " [-0.7578138 ]]\n",
      "t [[ 0.60319633]\n",
      " [-0.64679995]\n",
      " [-0.65219995]\n",
      " ...\n",
      " [ 0.3255327 ]\n",
      " [ 0.14606091]\n",
      " [-0.7578138 ]]\n",
      "t [[ 0.66394842]\n",
      " [-0.76742975]\n",
      " [-0.74132762]\n",
      " ...\n",
      " [ 0.33050121]\n",
      " [ 0.14779428]\n",
      " [-0.83426055]]\n",
      "t [[ 0.66394842]\n",
      " [-0.76742975]\n",
      " [-0.74132762]\n",
      " ...\n",
      " [ 0.33050121]\n",
      " [ 0.14779428]\n",
      " [-0.83426055]]\n",
      "Current iteration=6, loss=38384.58334182034\n",
      "t [[ 0.71456387]\n",
      " [-0.87901204]\n",
      " [-0.82259554]\n",
      " ...\n",
      " [ 0.32942911]\n",
      " [ 0.14604418]\n",
      " [-0.90212329]]\n",
      "t [[ 0.71456387]\n",
      " [-0.87901204]\n",
      " [-0.82259554]\n",
      " ...\n",
      " [ 0.32942911]\n",
      " [ 0.14604418]\n",
      " [-0.90212329]]\n",
      "t [[ 0.75714157]\n",
      " [-0.98214059]\n",
      " [-0.89721058]\n",
      " ...\n",
      " [ 0.3242126 ]\n",
      " [ 0.14182127]\n",
      " [-0.96340079]]\n",
      "t [[ 0.75714157]\n",
      " [-0.98214059]\n",
      " [-0.89721058]\n",
      " ...\n",
      " [ 0.3242126 ]\n",
      " [ 0.14182127]\n",
      " [-0.96340079]]\n",
      "Current iteration=8, loss=36616.31417041226\n",
      "t [[ 0.79327513]\n",
      " [-1.07757692]\n",
      " [-0.96610459]\n",
      " ...\n",
      " [ 0.31610724]\n",
      " [ 0.13584914]\n",
      " [-1.01939785]]\n",
      "t [[ 0.79327513]\n",
      " [-1.07757692]\n",
      " [-0.96610459]\n",
      " ...\n",
      " [ 0.31610724]\n",
      " [ 0.13584914]\n",
      " [-1.01939785]]\n",
      "t [[ 0.82419357]\n",
      " [-1.16609515]\n",
      " [-1.03001634]\n",
      " ...\n",
      " [ 0.30597246]\n",
      " [ 0.12865559]\n",
      " [-1.07101098]]\n",
      "loss=35326.7784768146\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.18321091]\n",
      " [-0.10898526]\n",
      " [-0.17223562]\n",
      " ...\n",
      " [ 0.26388133]\n",
      " [-0.17223562]\n",
      " [ 0.24716225]]\n",
      "t [[ 0.18321091]\n",
      " [-0.10898526]\n",
      " [-0.17223562]\n",
      " ...\n",
      " [ 0.26388133]\n",
      " [-0.17223562]\n",
      " [ 0.24716225]]\n",
      "t [[ 0.32569904]\n",
      " [-0.24690811]\n",
      " [-0.31713726]\n",
      " ...\n",
      " [ 0.46183719]\n",
      " [-0.31713726]\n",
      " [ 0.42191312]]\n",
      "t [[ 0.32569904]\n",
      " [-0.24690811]\n",
      " [-0.31713726]\n",
      " ...\n",
      " [ 0.46183719]\n",
      " [-0.31713726]\n",
      " [ 0.42191312]]\n",
      "Current iteration=2, loss=44921.22134243199\n",
      "t [[ 0.43823821]\n",
      " [-0.39101216]\n",
      " [-0.44223134]\n",
      " ...\n",
      " [ 0.61403186]\n",
      " [-0.44223134]\n",
      " [ 0.54934095]]\n",
      "t [[ 0.43823821]\n",
      " [-0.39101216]\n",
      " [-0.44223134]\n",
      " ...\n",
      " [ 0.61403186]\n",
      " [-0.44223134]\n",
      " [ 0.54934095]]\n",
      "t [[ 0.52848057]\n",
      " [-0.53105784]\n",
      " [-0.55240206]\n",
      " ...\n",
      " [ 0.73366297]\n",
      " [-0.55240206]\n",
      " [ 0.64491952]]\n",
      "t [[ 0.52848057]\n",
      " [-0.53105784]\n",
      " [-0.55240206]\n",
      " ...\n",
      " [ 0.73366297]\n",
      " [-0.55240206]\n",
      " [ 0.64491952]]\n",
      "Current iteration=4, loss=40932.728476254895\n",
      "t [[ 0.60182587]\n",
      " [-0.66303337]\n",
      " [-0.65088032]\n",
      " ...\n",
      " [ 0.82944048]\n",
      " [-0.65088032]\n",
      " [ 0.71828781]]\n",
      "t [[ 0.60182587]\n",
      " [-0.66303337]\n",
      " [-0.65088032]\n",
      " ...\n",
      " [ 0.82944048]\n",
      " [-0.65088032]\n",
      " [ 0.71828781]]\n",
      "t [[ 0.66214885]\n",
      " [-0.78578625]\n",
      " [-0.7398972 ]\n",
      " ...\n",
      " [ 0.90729323]\n",
      " [-0.7398972 ]\n",
      " [ 0.77567089]]\n",
      "t [[ 0.66214885]\n",
      " [-0.78578625]\n",
      " [-0.7398972 ]\n",
      " ...\n",
      " [ 0.90729323]\n",
      " [-0.7398972 ]\n",
      " [ 0.77567089]]\n",
      "Current iteration=6, loss=38369.49423354805\n",
      "t [[ 0.7122859 ]\n",
      " [-0.89941359]\n",
      " [-0.82106135]\n",
      " ...\n",
      " [ 0.97138818]\n",
      " [-0.82106135]\n",
      " [ 0.8212458 ]]\n",
      "t [[ 0.7122859 ]\n",
      " [-0.89941359]\n",
      " [-0.82106135]\n",
      " ...\n",
      " [ 0.97138818]\n",
      " [-0.82106135]\n",
      " [ 0.8212458 ]]\n",
      "t [[ 0.75435335]\n",
      " [-1.00451515]\n",
      " [-0.89557738]\n",
      " ...\n",
      " [ 1.02473904]\n",
      " [-0.89557738]\n",
      " [ 0.85791442]]\n",
      "t [[ 0.75435335]\n",
      " [-1.00451515]\n",
      " [-0.89557738]\n",
      " ...\n",
      " [ 1.02473904]\n",
      " [-0.89557738]\n",
      " [ 0.85791442]]\n",
      "Current iteration=8, loss=36594.76174220145\n",
      "t [[ 0.78995769]\n",
      " [-1.10185273]\n",
      " [-0.96437633]\n",
      " ...\n",
      " [ 1.06957966]\n",
      " [-0.96437633]\n",
      " [ 0.8877539 ]]\n",
      "t [[ 0.78995769]\n",
      " [-1.10185273]\n",
      " [-0.96437633]\n",
      " ...\n",
      " [ 1.06957966]\n",
      " [-0.96437633]\n",
      " [ 0.8877539 ]]\n",
      "t [[ 0.82033705]\n",
      " [-1.19219965]\n",
      " [-1.02819674]\n",
      " ...\n",
      " [ 1.10760047]\n",
      " [-1.02819674]\n",
      " [ 0.91228884]]\n",
      "loss=35300.30714940682\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06342193]\n",
      " [-0.3036958 ]\n",
      " [-0.1780907 ]\n",
      " ...\n",
      " [ 0.15229119]\n",
      " [ 0.06342193]\n",
      " [-0.25815126]]\n",
      "t [[ 0.06342193]\n",
      " [-0.3036958 ]\n",
      " [-0.1780907 ]\n",
      " ...\n",
      " [ 0.15229119]\n",
      " [ 0.06342193]\n",
      " [-0.25815126]]\n",
      "t [[ 0.10261936]\n",
      " [-0.56896233]\n",
      " [-0.32722975]\n",
      " ...\n",
      " [ 0.24019524]\n",
      " [ 0.10261936]\n",
      " [-0.43671704]]\n",
      "t [[ 0.10261936]\n",
      " [-0.56896233]\n",
      " [-0.32722975]\n",
      " ...\n",
      " [ 0.24019524]\n",
      " [ 0.10261936]\n",
      " [-0.43671704]]\n",
      "Current iteration=2, loss=44701.912604221114\n",
      "t [[ 0.12605365]\n",
      " [-0.80010194]\n",
      " [-0.45561606]\n",
      " ...\n",
      " [ 0.2910001 ]\n",
      " [ 0.12605365]\n",
      " [-0.57006219]]\n",
      "t [[ 0.12605365]\n",
      " [-0.80010194]\n",
      " [-0.45561606]\n",
      " ...\n",
      " [ 0.2910001 ]\n",
      " [ 0.12605365]\n",
      " [-0.57006219]]\n",
      "t [[ 0.13902473]\n",
      " [-1.00216501]\n",
      " [-0.56846713]\n",
      " ...\n",
      " [ 0.31956829]\n",
      " [ 0.13902473]\n",
      " [-0.67645181]]\n",
      "t [[ 0.13902473]\n",
      " [-1.00216501]\n",
      " [-0.56846713]\n",
      " ...\n",
      " [ 0.31956829]\n",
      " [ 0.13902473]\n",
      " [-0.67645181]]\n",
      "Current iteration=4, loss=40639.66526311698\n",
      "t [[ 0.14488369]\n",
      " [-1.17983834]\n",
      " [-0.66919357]\n",
      " ...\n",
      " [ 0.33427538]\n",
      " [ 0.14488369]\n",
      " [-0.76566847]]\n",
      "t [[ 0.14488369]\n",
      " [-1.17983834]\n",
      " [-0.66919357]\n",
      " ...\n",
      " [ 0.33427538]\n",
      " [ 0.14488369]\n",
      " [-0.76566847]]\n",
      "t [[ 0.14581944]\n",
      " [-1.33712261]\n",
      " [-0.76013274]\n",
      " ...\n",
      " [ 0.34002043]\n",
      " [ 0.14581944]\n",
      " [-0.8431483 ]]\n",
      "t [[ 0.14581944]\n",
      " [-1.33712261]\n",
      " [-0.76013274]\n",
      " ...\n",
      " [ 0.34002043]\n",
      " [ 0.14581944]\n",
      " [-0.8431483 ]]\n",
      "Current iteration=6, loss=38049.26488459002\n",
      "t [[ 0.14330977]\n",
      " [-1.47731385]\n",
      " [-0.84296192]\n",
      " ...\n",
      " [ 0.33978195]\n",
      " [ 0.14330977]\n",
      " [-0.91206282]]\n",
      "t [[ 0.14330977]\n",
      " [-1.47731385]\n",
      " [-0.84296192]\n",
      " ...\n",
      " [ 0.33978195]\n",
      " [ 0.14330977]\n",
      " [-0.91206282]]\n",
      "t [[ 0.13838153]\n",
      " [-1.60309017]\n",
      " [-0.91893367]\n",
      " ...\n",
      " [ 0.33544746]\n",
      " [ 0.13838153]\n",
      " [-0.97436797]]\n",
      "t [[ 0.13838153]\n",
      " [-1.60309017]\n",
      " [-0.91893367]\n",
      " ...\n",
      " [ 0.33544746]\n",
      " [ 0.13838153]\n",
      " [-0.97436797]]\n",
      "Current iteration=8, loss=36265.59795384726\n",
      "t [[ 0.13176531]\n",
      " [-1.71662166]\n",
      " [-0.98901461]\n",
      " ...\n",
      " [ 0.32826142]\n",
      " [ 0.13176531]\n",
      " [-1.03134125]]\n",
      "t [[ 0.13176531]\n",
      " [-1.71662166]\n",
      " [-0.98901461]\n",
      " ...\n",
      " [ 0.32826142]\n",
      " [ 0.13176531]\n",
      " [-1.03134125]]\n",
      "t [[ 0.1239908 ]\n",
      " [-1.81967208]\n",
      " [-1.05397085]\n",
      " ...\n",
      " [ 0.31907352]\n",
      " [ 0.1239908 ]\n",
      " [-1.0838644 ]]\n",
      "loss=34970.04487319554\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.19152011]\n",
      " [-0.11164855]\n",
      " [-0.17868323]\n",
      " ...\n",
      " [ 0.15006218]\n",
      " [ 0.06317293]\n",
      " [-0.25424988]]\n",
      "t [[ 0.19152011]\n",
      " [-0.11164855]\n",
      " [-0.17868323]\n",
      " ...\n",
      " [ 0.15006218]\n",
      " [ 0.06317293]\n",
      " [-0.25424988]]\n",
      "t [[ 0.33919659]\n",
      " [-0.25560335]\n",
      " [-0.3281079 ]\n",
      " ...\n",
      " [ 0.23619461]\n",
      " [ 0.10204238]\n",
      " [-0.42896155]]\n",
      "t [[ 0.33919659]\n",
      " [-0.25560335]\n",
      " [-0.3281079 ]\n",
      " ...\n",
      " [ 0.23619461]\n",
      " [ 0.10204238]\n",
      " [-0.42896155]]\n",
      "Current iteration=2, loss=44689.68793358778\n",
      "t [[ 0.45497476]\n",
      " [-0.40622064]\n",
      " [-0.45659965]\n",
      " ...\n",
      " [ 0.2856092 ]\n",
      " [ 0.12512126]\n",
      " [-0.55873865]]\n",
      "t [[ 0.45497476]\n",
      " [-0.40622064]\n",
      " [-0.45659965]\n",
      " ...\n",
      " [ 0.2856092 ]\n",
      " [ 0.12512126]\n",
      " [-0.55873865]]\n",
      "t [[ 0.5473057 ]\n",
      " [-0.55222288]\n",
      " [-0.56945174]\n",
      " ...\n",
      " [ 0.31309878]\n",
      " [ 0.13773526]\n",
      " [-0.661922  ]]\n",
      "t [[ 0.5473057 ]\n",
      " [-0.55222288]\n",
      " [-0.56945174]\n",
      " ...\n",
      " [ 0.31309878]\n",
      " [ 0.13773526]\n",
      " [-0.661922  ]]\n",
      "Current iteration=4, loss=40626.62198611691\n",
      "t [[ 0.62203389]\n",
      " [-0.68937261]\n",
      " [-0.67011868]\n",
      " ...\n",
      " [ 0.32697577]\n",
      " [ 0.14324796]\n",
      " [-0.74829336]]\n",
      "t [[ 0.62203389]\n",
      " [-0.68937261]\n",
      " [-0.67011868]\n",
      " ...\n",
      " [ 0.32697577]\n",
      " [ 0.14324796]\n",
      " [-0.74829336]]\n",
      "t [[ 0.68329023]\n",
      " [-0.81656531]\n",
      " [-0.76096351]\n",
      " ...\n",
      " [ 0.33208532]\n",
      " [ 0.14385427]\n",
      " [-0.82325776]]\n",
      "t [[ 0.68329023]\n",
      " [-0.81656531]\n",
      " [-0.76096351]\n",
      " ...\n",
      " [ 0.33208532]\n",
      " [ 0.14385427]\n",
      " [-0.82325776]]\n",
      "Current iteration=6, loss=38036.60832430019\n",
      "t [[ 0.73406966]\n",
      " [-0.93401346]\n",
      " [-0.84367877]\n",
      " ...\n",
      " [ 0.33136336]\n",
      " [ 0.14103462]\n",
      " [-0.88994719]]\n",
      "t [[ 0.73406966]\n",
      " [-0.93401346]\n",
      " [-0.84367877]\n",
      " ...\n",
      " [ 0.33136336]\n",
      " [ 0.14103462]\n",
      " [-0.88994719]]\n",
      "t [[ 0.77659139]\n",
      " [-1.04243197]\n",
      " [-0.91952624]\n",
      " ...\n",
      " [ 0.32666466]\n",
      " [ 0.13581671]\n",
      " [-0.95027866]]\n",
      "t [[ 0.77659139]\n",
      " [-1.04243197]\n",
      " [-0.91952624]\n",
      " ...\n",
      " [ 0.32666466]\n",
      " [ 0.13581671]\n",
      " [-0.95027866]]\n",
      "Current iteration=8, loss=36253.238010275636\n",
      "t [[ 0.81253054]\n",
      " [-1.14267949]\n",
      " [-0.9894782 ]\n",
      " ...\n",
      " [ 0.31920876]\n",
      " [ 0.12893105]\n",
      " [-1.00549471]]\n",
      "t [[ 0.81253054]\n",
      " [-1.14267949]\n",
      " [-0.9894782 ]\n",
      " ...\n",
      " [ 0.31920876]\n",
      " [ 0.12893105]\n",
      " [-1.00549471]]\n",
      "t [[ 0.84317188]\n",
      " [-1.23560607]\n",
      " [-1.05430424]\n",
      " ...\n",
      " [ 0.30982635]\n",
      " [ 0.12090672]\n",
      " [-1.05644689]]\n",
      "loss=34957.71896348053\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.1890654 ]\n",
      " [-0.10687975]\n",
      " [-0.17801404]\n",
      " ...\n",
      " [ 0.15237744]\n",
      " [ 0.06433727]\n",
      " [-0.26207951]]\n",
      "t [[ 0.1890654 ]\n",
      " [-0.10687975]\n",
      " [-0.17801404]\n",
      " ...\n",
      " [ 0.15237744]\n",
      " [ 0.06433727]\n",
      " [-0.26207951]]\n",
      "t [[ 0.33486717]\n",
      " [-0.24621284]\n",
      " [-0.32677344]\n",
      " ...\n",
      " [ 0.23911245]\n",
      " [ 0.10408174]\n",
      " [-0.44224091]]\n",
      "t [[ 0.33486717]\n",
      " [-0.24621284]\n",
      " [-0.32677344]\n",
      " ...\n",
      " [ 0.23911245]\n",
      " [ 0.10408174]\n",
      " [-0.44224091]]\n",
      "Current iteration=2, loss=44753.38805698598\n",
      "t [[ 0.4492156 ]\n",
      " [-0.39224654]\n",
      " [-0.45465356]\n",
      " ...\n",
      " [ 0.28822956]\n",
      " [ 0.12786089]\n",
      " [-0.5760456 ]]\n",
      "t [[ 0.4492156 ]\n",
      " [-0.39224654]\n",
      " [-0.45465356]\n",
      " ...\n",
      " [ 0.28822956]\n",
      " [ 0.12786089]\n",
      " [-0.5760456 ]]\n",
      "t [[ 0.54046163]\n",
      " [-0.53372463]\n",
      " [-0.56695509]\n",
      " ...\n",
      " [ 0.31490362]\n",
      " [ 0.14105995]\n",
      " [-0.68233918]]\n",
      "t [[ 0.54046163]\n",
      " [-0.53372463]\n",
      " [-0.56695509]\n",
      " ...\n",
      " [ 0.31490362]\n",
      " [ 0.14105995]\n",
      " [-0.68233918]]\n",
      "Current iteration=4, loss=40740.98689312955\n",
      "t [[ 0.6143706 ]\n",
      " [-0.66645193]\n",
      " [-0.66712587]\n",
      " ...\n",
      " [ 0.32764041]\n",
      " [ 0.14707473]\n",
      " [-0.77118586]]\n",
      "t [[ 0.6143706 ]\n",
      " [-0.66645193]\n",
      " [-0.66712587]\n",
      " ...\n",
      " [ 0.32764041]\n",
      " [ 0.14707473]\n",
      " [-0.77118586]]\n",
      "t [[ 0.67500824]\n",
      " [-0.78935913]\n",
      " [-0.75751981]\n",
      " ...\n",
      " [ 0.3314051 ]\n",
      " [ 0.14811957]\n",
      " [-0.84816648]]\n",
      "t [[ 0.67500824]\n",
      " [-0.78935913]\n",
      " [-0.75751981]\n",
      " ...\n",
      " [ 0.3314051 ]\n",
      " [ 0.14811957]\n",
      " [-0.84816648]]\n",
      "Current iteration=6, loss=38186.96124568981\n",
      "t [[ 0.7253191 ]\n",
      " [-0.9026803 ]\n",
      " [-0.83982103]\n",
      " ...\n",
      " [ 0.32921151]\n",
      " [ 0.14568781]\n",
      " [-0.91652836]]\n",
      "t [[ 0.7253191 ]\n",
      " [-0.9026803 ]\n",
      " [-0.83982103]\n",
      " ...\n",
      " [ 0.32921151]\n",
      " [ 0.14568781]\n",
      " [-0.91652836]]\n",
      "t [[ 0.76748461]\n",
      " [-1.00714059]\n",
      " [-0.91528428]\n",
      " ...\n",
      " [ 0.32296562]\n",
      " [ 0.14081635]\n",
      " [-0.97826708]]\n",
      "t [[ 0.76748461]\n",
      " [-1.00714059]\n",
      " [-0.91528428]\n",
      " ...\n",
      " [ 0.32296562]\n",
      " [ 0.14081635]\n",
      " [-0.97826708]]\n",
      "Current iteration=8, loss=36429.83667982539\n",
      "t [[ 0.80315186]\n",
      " [-1.10360107]\n",
      " [-0.98487618]\n",
      " ...\n",
      " [ 0.31392111]\n",
      " [ 0.1342426 ]\n",
      " [-1.03468049]]\n",
      "t [[ 0.80315186]\n",
      " [-1.10360107]\n",
      " [-0.98487618]\n",
      " ...\n",
      " [ 0.31392111]\n",
      " [ 0.1342426 ]\n",
      " [-1.03468049]]\n",
      "t [[ 0.83358475]\n",
      " [-1.19290929]\n",
      " [-1.04936188]\n",
      " ...\n",
      " [ 0.30293181]\n",
      " [ 0.12650102]\n",
      " [-1.08666035]]\n",
      "loss=35154.11881395059\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.18876276]\n",
      " [-0.11228784]\n",
      " [-0.17745489]\n",
      " ...\n",
      " [ 0.27187773]\n",
      " [-0.17745489]\n",
      " [ 0.25465201]]\n",
      "t [[ 0.18876276]\n",
      " [-0.11228784]\n",
      " [-0.17745489]\n",
      " ...\n",
      " [ 0.27187773]\n",
      " [-0.17745489]\n",
      " [ 0.25465201]]\n",
      "t [[ 0.33431734]\n",
      " [-0.25527711]\n",
      " [-0.32590647]\n",
      " ...\n",
      " [ 0.47380665]\n",
      " [-0.32590647]\n",
      " [ 0.43247471]]\n",
      "t [[ 0.33431734]\n",
      " [-0.25527711]\n",
      " [-0.32590647]\n",
      " ...\n",
      " [ 0.47380665]\n",
      " [-0.32590647]\n",
      " [ 0.43247471]]\n",
      "Current iteration=2, loss=44756.56033232175\n",
      "t [[ 0.44845059]\n",
      " [-0.40411506]\n",
      " [-0.45358735]\n",
      " ...\n",
      " [ 0.62783965]\n",
      " [-0.45358735]\n",
      " [ 0.56089941]]\n",
      "t [[ 0.44845059]\n",
      " [-0.40411506]\n",
      " [-0.45358735]\n",
      " ...\n",
      " [ 0.62783965]\n",
      " [-0.45358735]\n",
      " [ 0.56089941]]\n",
      "t [[ 0.53940769]\n",
      " [-0.54803335]\n",
      " [-0.56574044]\n",
      " ...\n",
      " [ 0.74814736]\n",
      " [-0.56574044]\n",
      " [ 0.65649395]]\n",
      "t [[ 0.53940769]\n",
      " [-0.54803335]\n",
      " [-0.56574044]\n",
      " ...\n",
      " [ 0.74814736]\n",
      " [-0.56574044]\n",
      " [ 0.65649395]]\n",
      "Current iteration=4, loss=40733.79113210223\n",
      "t [[ 0.61293655]\n",
      " [-0.68303312]\n",
      " [-0.66578698]\n",
      " ...\n",
      " [ 0.84395018]\n",
      " [-0.66578698]\n",
      " [ 0.72941005]]\n",
      "t [[ 0.61293655]\n",
      " [-0.68303312]\n",
      " [-0.66578698]\n",
      " ...\n",
      " [ 0.84395018]\n",
      " [-0.66578698]\n",
      " [ 0.72941005]]\n",
      "t [[ 0.67312249]\n",
      " [-0.80811278]\n",
      " [-0.75606859]\n",
      " ...\n",
      " [ 0.92145913]\n",
      " [-0.75606859]\n",
      " [ 0.78612492]]\n",
      "t [[ 0.67312249]\n",
      " [-0.80811278]\n",
      " [-0.75606859]\n",
      " ...\n",
      " [ 0.92145913]\n",
      " [-0.75606859]\n",
      " [ 0.78612492]]\n",
      "Current iteration=6, loss=38171.18919879812\n",
      "t [[ 0.7229329 ]\n",
      " [-0.9235268 ]\n",
      " [-0.83826416]\n",
      " ...\n",
      " [ 0.9850051 ]\n",
      " [-0.83826416]\n",
      " [ 0.8309462 ]]\n",
      "t [[ 0.7229329 ]\n",
      " [-0.9235268 ]\n",
      " [-0.83826416]\n",
      " ...\n",
      " [ 0.9850051 ]\n",
      " [-0.83826416]\n",
      " [ 0.8309462 ]]\n",
      "t [[ 0.76456735]\n",
      " [-1.03000439]\n",
      " [-0.91362644]\n",
      " ...\n",
      " [ 1.03770101]\n",
      " [-0.91362644]\n",
      " [ 0.86684642]]\n",
      "t [[ 0.76456735]\n",
      " [-1.03000439]\n",
      " [-0.91362644]\n",
      " ...\n",
      " [ 1.03770101]\n",
      " [-0.91362644]\n",
      " [ 0.86684642]]\n",
      "Current iteration=8, loss=36407.587477334004\n",
      "t [[ 0.79968601]\n",
      " [-1.12840601]\n",
      " [-0.98312143]\n",
      " ...\n",
      " [ 1.0818424 ]\n",
      " [-0.98312143]\n",
      " [ 0.89594199]]\n",
      "t [[ 0.79968601]\n",
      " [-1.12840601]\n",
      " [-0.98312143]\n",
      " ...\n",
      " [ 1.0818424 ]\n",
      " [-0.98312143]\n",
      " [ 0.89594199]]\n",
      "t [[ 0.82956192]\n",
      " [-1.21957814]\n",
      " [-1.04751412]\n",
      " ...\n",
      " [ 1.11915865]\n",
      " [-1.04751412]\n",
      " [ 0.91977937]]\n",
      "loss=35126.98331561001\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06528729]\n",
      " [-0.31262803]\n",
      " [-0.18332866]\n",
      " ...\n",
      " [ 0.15677035]\n",
      " [ 0.06528729]\n",
      " [-0.26574394]]\n",
      "t [[ 0.06528729]\n",
      " [-0.31262803]\n",
      " [-0.18332866]\n",
      " ...\n",
      " [ 0.15677035]\n",
      " [ 0.06528729]\n",
      " [-0.26574394]]\n",
      "t [[ 0.10491691]\n",
      " [-0.58454599]\n",
      " [-0.33599094]\n",
      " ...\n",
      " [ 0.24534239]\n",
      " [ 0.10491691]\n",
      " [-0.44719444]]\n",
      "t [[ 0.10491691]\n",
      " [-0.58454599]\n",
      " [-0.33599094]\n",
      " ...\n",
      " [ 0.24534239]\n",
      " [ 0.10491691]\n",
      " [-0.44719444]]\n",
      "Current iteration=2, loss=44538.10582648419\n",
      "t [[ 0.12811643]\n",
      " [-0.82046431]\n",
      " [-0.46693095]\n",
      " ...\n",
      " [ 0.29545712]\n",
      " [ 0.12811643]\n",
      " [-0.58180963]]\n",
      "t [[ 0.12811643]\n",
      " [-0.82046431]\n",
      " [-0.46693095]\n",
      " ...\n",
      " [ 0.29545712]\n",
      " [ 0.12811643]\n",
      " [-0.58180963]]\n",
      "t [[ 0.14055272]\n",
      " [-1.02590365]\n",
      " [-0.58173293]\n",
      " ...\n",
      " [ 0.32291315]\n",
      " [ 0.14055272]\n",
      " [-0.68896364]]\n",
      "t [[ 0.14055272]\n",
      " [-1.02590365]\n",
      " [-0.58173293]\n",
      " ...\n",
      " [ 0.32291315]\n",
      " [ 0.14055272]\n",
      " [-0.68896364]]\n",
      "Current iteration=4, loss=40443.6132931843\n",
      "t [[ 0.14575432]\n",
      " [-1.20593855]\n",
      " [-0.68399867]\n",
      " ...\n",
      " [ 0.336435  ]\n",
      " [ 0.14575432]\n",
      " [-0.77879439]]\n",
      "t [[ 0.14575432]\n",
      " [-1.20593855]\n",
      " [-0.68399867]\n",
      " ...\n",
      " [ 0.336435  ]\n",
      " [ 0.14575432]\n",
      " [-0.77879439]]\n",
      "t [[ 0.14599962]\n",
      " [-1.36485752]\n",
      " [-0.77617554]\n",
      " ...\n",
      " [ 0.34104245]\n",
      " [ 0.14599962]\n",
      " [-0.85683322]]\n",
      "t [[ 0.14599962]\n",
      " [-1.36485752]\n",
      " [-0.77617554]\n",
      " ...\n",
      " [ 0.34104245]\n",
      " [ 0.14599962]\n",
      " [-0.85683322]]\n",
      "Current iteration=6, loss=37855.10045842835\n",
      "t [[ 0.14281299]\n",
      " [-1.50616152]\n",
      " [-0.86001064]\n",
      " ...\n",
      " [ 0.33975028]\n",
      " [ 0.14281299]\n",
      " [-0.92626628]]\n",
      "t [[ 0.14281299]\n",
      " [-1.50616152]\n",
      " [-0.86001064]\n",
      " ...\n",
      " [ 0.33975028]\n",
      " [ 0.14281299]\n",
      " [-0.92626628]]\n",
      "t [[ 0.13724573]\n",
      " [-1.63267271]\n",
      " [-0.93680448]\n",
      " ...\n",
      " [ 0.3344526 ]\n",
      " [ 0.13724573]\n",
      " [-0.98904437]]\n",
      "t [[ 0.13724573]\n",
      " [-1.63267271]\n",
      " [-0.93680448]\n",
      " ...\n",
      " [ 0.3344526 ]\n",
      " [ 0.13724573]\n",
      " [-0.98904437]]\n",
      "Current iteration=8, loss=36083.207923799724\n",
      "t [[ 0.13004092]\n",
      " [-1.74666273]\n",
      " [-1.00755863]\n",
      " ...\n",
      " [ 0.32639072]\n",
      " [ 0.13004092]\n",
      " [-1.04643857]]\n",
      "t [[ 0.13004092]\n",
      " [-1.74666273]\n",
      " [-1.00755863]\n",
      " ...\n",
      " [ 0.32639072]\n",
      " [ 0.13004092]\n",
      " [-1.04643857]]\n",
      "t [[ 0.12173388]\n",
      " [-1.84996756]\n",
      " [-1.07306566]\n",
      " ...\n",
      " [ 0.31640838]\n",
      " [ 0.12173388]\n",
      " [-1.09932719]]\n",
      "loss=34801.7798235685\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.19715306]\n",
      " [-0.11493233]\n",
      " [-0.18393862]\n",
      " ...\n",
      " [ 0.15447577]\n",
      " [ 0.06503096]\n",
      " [-0.26172782]]\n",
      "t [[ 0.19715306]\n",
      " [-0.11493233]\n",
      " [-0.18393862]\n",
      " ...\n",
      " [ 0.15447577]\n",
      " [ 0.06503096]\n",
      " [-0.26172782]]\n",
      "t [[ 0.34786926]\n",
      " [-0.26407884]\n",
      " [-0.33688589]\n",
      " ...\n",
      " [ 0.24123755]\n",
      " [ 0.10432068]\n",
      " [-0.43921249]]\n",
      "t [[ 0.34786926]\n",
      " [-0.26407884]\n",
      " [-0.33688589]\n",
      " ...\n",
      " [ 0.24123755]\n",
      " [ 0.10432068]\n",
      " [-0.43921249]]\n",
      "Current iteration=2, loss=44525.76431801008\n",
      "t [[ 0.46518391]\n",
      " [-0.41950527]\n",
      " [-0.46792399]\n",
      " ...\n",
      " [ 0.28994362]\n",
      " [ 0.12715275]\n",
      " [-0.57017193]]\n",
      "t [[ 0.46518391]\n",
      " [-0.41950527]\n",
      " [-0.46792399]\n",
      " ...\n",
      " [ 0.28994362]\n",
      " [ 0.12715275]\n",
      " [-0.57017193]]\n",
      "t [[ 0.55817457]\n",
      " [-0.56939177]\n",
      " [-0.58271795]\n",
      " ...\n",
      " [ 0.31631682]\n",
      " [ 0.13922127]\n",
      " [-0.67405743]]\n",
      "t [[ 0.55817457]\n",
      " [-0.56939177]\n",
      " [-0.58271795]\n",
      " ...\n",
      " [ 0.31631682]\n",
      " [ 0.13922127]\n",
      " [-0.67405743]]\n",
      "Current iteration=4, loss=40430.57853066841\n",
      "t [[ 0.63304156]\n",
      " [-0.70953964]\n",
      " [-0.68491539]\n",
      " ...\n",
      " [ 0.32901315]\n",
      " [ 0.14406765]\n",
      " [-0.76100152]]\n",
      "t [[ 0.63304156]\n",
      " [-0.70953964]\n",
      " [-0.68491539]\n",
      " ...\n",
      " [ 0.32901315]\n",
      " [ 0.14406765]\n",
      " [-0.76100152]]\n",
      "t [[ 0.69412754]\n",
      " [-0.83901682]\n",
      " [-0.77699004]\n",
      " ...\n",
      " [ 0.33299477]\n",
      " [ 0.14397625]\n",
      " [-0.83649921]]\n",
      "t [[ 0.69412754]\n",
      " [-0.83901682]\n",
      " [-0.77699004]\n",
      " ...\n",
      " [ 0.33299477]\n",
      " [ 0.14397625]\n",
      " [-0.83649921]]\n",
      "Current iteration=6, loss=37842.48364066772\n",
      "t [[ 0.74455783]\n",
      " [-0.95820557]\n",
      " [-0.86070442]\n",
      " ...\n",
      " [ 0.33123151]\n",
      " [ 0.14047393]\n",
      " [-0.90369273]]\n",
      "t [[ 0.74455783]\n",
      " [-0.95820557]\n",
      " [-0.86070442]\n",
      " ...\n",
      " [ 0.33123151]\n",
      " [ 0.14047393]\n",
      " [-0.90369273]]\n",
      "t [[ 0.7866337 ]\n",
      " [-1.06795633]\n",
      " [-0.93736817]\n",
      " ...\n",
      " [ 0.32558332]\n",
      " [ 0.13461268]\n",
      " [-0.96449066]]\n",
      "t [[ 0.7866337 ]\n",
      " [-1.06795633]\n",
      " [-0.93736817]\n",
      " ...\n",
      " [ 0.32558332]\n",
      " [ 0.13461268]\n",
      " [-0.96449066]]\n",
      "Current iteration=8, loss=36070.8726285586\n",
      "t [[ 0.82208265]\n",
      " [-1.1692288 ]\n",
      " [-1.00798841]\n",
      " ...\n",
      " [ 0.31726574]\n",
      " [ 0.12713529]\n",
      " [-1.02012672]]\n",
      "t [[ 0.82208265]\n",
      " [-1.1692288 ]\n",
      " [-1.00798841]\n",
      " ...\n",
      " [ 0.31726574]\n",
      " [ 0.12713529]\n",
      " [-1.02012672]]\n",
      "t [[ 0.85222302]\n",
      " [-1.26294671]\n",
      " [-1.07336107]\n",
      " ...\n",
      " [ 0.30710308]\n",
      " [ 0.11857632]\n",
      " [-1.07144741]]\n",
      "loss=34789.44477655421\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.19462615]\n",
      " [-0.11002327]\n",
      " [-0.18324975]\n",
      " ...\n",
      " [ 0.15685913]\n",
      " [ 0.06622954]\n",
      " [-0.26978773]]\n",
      "t [[ 0.19462615]\n",
      " [-0.11002327]\n",
      " [-0.18324975]\n",
      " ...\n",
      " [ 0.15685913]\n",
      " [ 0.06622954]\n",
      " [-0.26978773]]\n",
      "t [[ 0.34342964]\n",
      " [-0.25441651]\n",
      " [-0.33551253]\n",
      " ...\n",
      " [ 0.24419232]\n",
      " [ 0.10641164]\n",
      " [-0.4528132 ]]\n",
      "t [[ 0.34342964]\n",
      " [-0.25441651]\n",
      " [-0.33551253]\n",
      " ...\n",
      " [ 0.24419232]\n",
      " [ 0.10641164]\n",
      " [-0.4528132 ]]\n",
      "Current iteration=2, loss=44591.27023025162\n",
      "t [[ 0.4592986 ]\n",
      " [-0.40512688]\n",
      " [-0.46592435]\n",
      " ...\n",
      " [ 0.29254071]\n",
      " [ 0.12995451]\n",
      " [-0.58783678]]\n",
      "t [[ 0.4592986 ]\n",
      " [-0.40512688]\n",
      " [-0.46592435]\n",
      " ...\n",
      " [ 0.29254071]\n",
      " [ 0.12995451]\n",
      " [-0.58783678]]\n",
      "t [[ 0.55120276]\n",
      " [-0.55036124]\n",
      " [-0.58015688]\n",
      " ...\n",
      " [ 0.31802817]\n",
      " [ 0.14261515]\n",
      " [-0.69484398]]\n",
      "t [[ 0.55120276]\n",
      " [-0.55036124]\n",
      " [-0.58015688]\n",
      " ...\n",
      " [ 0.31802817]\n",
      " [ 0.14261515]\n",
      " [-0.69484398]]\n",
      "Current iteration=4, loss=40547.53595090987\n",
      "t [[ 0.62525705]\n",
      " [-0.68596827]\n",
      " [-0.68184978]\n",
      " ...\n",
      " [ 0.32951214]\n",
      " [ 0.14796852]\n",
      " [-0.78426225]]\n",
      "t [[ 0.62525705]\n",
      " [-0.68596827]\n",
      " [-0.68184978]\n",
      " ...\n",
      " [ 0.32951214]\n",
      " [ 0.14796852]\n",
      " [-0.78426225]]\n",
      "t [[ 0.68573476]\n",
      " [-0.81105351]\n",
      " [-0.77346671]\n",
      " ...\n",
      " [ 0.33207901]\n",
      " [ 0.14831904]\n",
      " [-0.86176847]]\n",
      "t [[ 0.68573476]\n",
      " [-0.81105351]\n",
      " [-0.77346671]\n",
      " ...\n",
      " [ 0.33207901]\n",
      " [ 0.14831904]\n",
      " [-0.86176847]]\n",
      "Current iteration=6, loss=37995.61802144657\n",
      "t [[ 0.73570834]\n",
      " [-0.92602119]\n",
      " [-0.8567611 ]\n",
      " ...\n",
      " [ 0.32877824]\n",
      " [ 0.14520691]\n",
      " [-0.93062324]]\n",
      "t [[ 0.73570834]\n",
      " [-0.92602119]\n",
      " [-0.8567611 ]\n",
      " ...\n",
      " [ 0.32877824]\n",
      " [ 0.14520691]\n",
      " [-0.93062324]]\n",
      "t [[ 0.7774399 ]\n",
      " [-1.0317313 ]\n",
      " [-0.93303522]\n",
      " ...\n",
      " [ 0.32152154]\n",
      " [ 0.1396936 ]\n",
      " [-0.99281526]]\n",
      "t [[ 0.7774399 ]\n",
      " [-1.0317313 ]\n",
      " [-0.93303522]\n",
      " ...\n",
      " [ 0.32152154]\n",
      " [ 0.1396936 ]\n",
      " [-0.99281526]]\n",
      "Current iteration=8, loss=36250.21604507955\n",
      "t [[ 0.81262817]\n",
      " [-1.12914486]\n",
      " [-1.00329025]\n",
      " ...\n",
      " [ 0.31155864]\n",
      " [ 0.13252903]\n",
      " [-1.04963441]]\n",
      "t [[ 0.81262817]\n",
      " [-1.12914486]\n",
      " [-1.00329025]\n",
      " ...\n",
      " [ 0.31155864]\n",
      " [ 0.13252903]\n",
      " [-1.04963441]]\n",
      "t [[ 0.84257027]\n",
      " [-1.21918195]\n",
      " [-1.06831752]\n",
      " ...\n",
      " [ 0.29973675]\n",
      " [ 0.12425327]\n",
      " [-1.10196782]]\n",
      "loss=34988.46514651415\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.1943146 ]\n",
      " [-0.11559042]\n",
      " [-0.18267415]\n",
      " ...\n",
      " [ 0.27987414]\n",
      " [-0.18267415]\n",
      " [ 0.26214178]]\n",
      "t [[ 0.1943146 ]\n",
      " [-0.11559042]\n",
      " [-0.18267415]\n",
      " ...\n",
      " [ 0.27987414]\n",
      " [-0.18267415]\n",
      " [ 0.26214178]]\n",
      "t [[ 0.34286383]\n",
      " [-0.2636968 ]\n",
      " [-0.33462732]\n",
      " ...\n",
      " [ 0.48565989]\n",
      " [-0.33462732]\n",
      " [ 0.44290884]]\n",
      "t [[ 0.34286383]\n",
      " [-0.2636968 ]\n",
      " [-0.33462732]\n",
      " ...\n",
      " [ 0.48565989]\n",
      " [-0.33462732]\n",
      " [ 0.44290884]]\n",
      "Current iteration=2, loss=44594.201542509065\n",
      "t [[ 0.45850795]\n",
      " [-0.41724904]\n",
      " [-0.4648403 ]\n",
      " ...\n",
      " [ 0.64141076]\n",
      " [-0.4648403 ]\n",
      " [ 0.5722137 ]]\n",
      "t [[ 0.45850795]\n",
      " [-0.41724904]\n",
      " [-0.4648403 ]\n",
      " ...\n",
      " [ 0.64141076]\n",
      " [-0.4648403 ]\n",
      " [ 0.5722137 ]]\n",
      "t [[ 0.55010547]\n",
      " [-0.56496675]\n",
      " [-0.57892433]\n",
      " ...\n",
      " [ 0.76229708]\n",
      " [-0.57892433]\n",
      " [ 0.66774182]]\n",
      "t [[ 0.55010547]\n",
      " [-0.56496675]\n",
      " [-0.57892433]\n",
      " ...\n",
      " [ 0.76229708]\n",
      " [-0.57892433]\n",
      " [ 0.66774182]]\n",
      "Current iteration=4, loss=40539.76527413768\n",
      "t [[ 0.62375789]\n",
      " [-0.70289467]\n",
      " [-0.68049202]\n",
      " ...\n",
      " [ 0.85805184]\n",
      " [-0.68049202]\n",
      " [ 0.74015332]]\n",
      "t [[ 0.62375789]\n",
      " [-0.70289467]\n",
      " [-0.68049202]\n",
      " ...\n",
      " [ 0.85805184]\n",
      " [-0.68049202]\n",
      " [ 0.74015332]]\n",
      "t [[ 0.68376133]\n",
      " [-0.8302019 ]\n",
      " [-0.77199499]\n",
      " ...\n",
      " [ 0.93516478]\n",
      " [-0.77199499]\n",
      " [ 0.79616972]]\n",
      "t [[ 0.68376133]\n",
      " [-0.8302019 ]\n",
      " [-0.77199499]\n",
      " ...\n",
      " [ 0.93516478]\n",
      " [-0.77199499]\n",
      " [ 0.79616972]]\n",
      "Current iteration=6, loss=37979.17757937885\n",
      "t [[ 0.73321261]\n",
      " [-0.94730988]\n",
      " [-0.8551818 ]\n",
      " ...\n",
      " [ 0.99812676]\n",
      " [-0.8551818 ]\n",
      " [ 0.84022299]]\n",
      "t [[ 0.73321261]\n",
      " [-0.94730988]\n",
      " [-0.8551818 ]\n",
      " ...\n",
      " [ 0.99812676]\n",
      " [-0.8551818 ]\n",
      " [ 0.84022299]]\n",
      "t [[ 0.77439259]\n",
      " [-1.05508083]\n",
      " [-0.931353  ]\n",
      " ...\n",
      " [ 1.0501464 ]\n",
      " [-0.931353  ]\n",
      " [ 0.87535189]]\n",
      "t [[ 0.77439259]\n",
      " [-1.05508083]\n",
      " [-0.931353  ]\n",
      " ...\n",
      " [ 1.0501464 ]\n",
      " [-0.931353  ]\n",
      " [ 0.87535189]]\n",
      "Current iteration=8, loss=36227.29074533172\n",
      "t [[ 0.80901329]\n",
      " [-1.15447435]\n",
      " [-1.00150929]\n",
      " ...\n",
      " [ 1.09357821]\n",
      " [-1.00150929]\n",
      " [ 0.90370908]]\n",
      "t [[ 0.80901329]\n",
      " [-1.15447435]\n",
      " [-1.00150929]\n",
      " ...\n",
      " [ 1.09357821]\n",
      " [-1.00150929]\n",
      " [ 0.90370908]]\n",
      "t [[ 0.83838092]\n",
      " [-1.24640934]\n",
      " [-1.06644195]\n",
      " ...\n",
      " [ 1.13018825]\n",
      " [-1.06644195]\n",
      " [ 0.92686065]]\n",
      "loss=34960.69096659441\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06715264]\n",
      " [-0.32156026]\n",
      " [-0.18856663]\n",
      " ...\n",
      " [ 0.1612495 ]\n",
      " [ 0.06715264]\n",
      " [-0.27333663]]\n",
      "t [[ 0.06715264]\n",
      " [-0.32156026]\n",
      " [-0.18856663]\n",
      " ...\n",
      " [ 0.1612495 ]\n",
      " [ 0.06715264]\n",
      " [-0.27333663]]\n",
      "t [[ 0.1071744 ]\n",
      " [-0.60006505]\n",
      " [-0.34470399]\n",
      " ...\n",
      " [ 0.25038281]\n",
      " [ 0.1071744 ]\n",
      " [-0.45754028]]\n",
      "t [[ 0.1071744 ]\n",
      " [-0.60006505]\n",
      " [-0.34470399]\n",
      " ...\n",
      " [ 0.25038281]\n",
      " [ 0.1071744 ]\n",
      " [-0.45754028]]\n",
      "Current iteration=2, loss=44376.57155416108\n",
      "t [[ 0.13010282]\n",
      " [-0.84065821]\n",
      " [-0.47814427]\n",
      " ...\n",
      " [ 0.29973291]\n",
      " [ 0.13010282]\n",
      " [-0.59333759]]\n",
      "t [[ 0.13010282]\n",
      " [-0.84065821]\n",
      " [-0.47814427]\n",
      " ...\n",
      " [ 0.29973291]\n",
      " [ 0.13010282]\n",
      " [-0.59333759]]\n",
      "t [[ 0.14197993]\n",
      " [-1.04935795]\n",
      " [-0.59484741]\n",
      " ...\n",
      " [ 0.32604289]\n",
      " [ 0.14197993]\n",
      " [-0.70121478]]\n",
      "t [[ 0.14197993]\n",
      " [-1.04935795]\n",
      " [-0.59484741]\n",
      " ...\n",
      " [ 0.32604289]\n",
      " [ 0.14197993]\n",
      " [-0.70121478]]\n",
      "Current iteration=4, loss=40252.34618739018\n",
      "t [[ 0.14651095]\n",
      " [-1.23164273]\n",
      " [-0.69860697]\n",
      " ...\n",
      " [ 0.33837209]\n",
      " [ 0.14651095]\n",
      " [-0.79164306]]\n",
      "t [[ 0.14651095]\n",
      " [-1.23164273]\n",
      " [-0.69860697]\n",
      " ...\n",
      " [ 0.33837209]\n",
      " [ 0.14651095]\n",
      " [-0.79164306]]\n",
      "t [[ 0.14606139]\n",
      " [-1.39209612]\n",
      " [-0.79197971]\n",
      " ...\n",
      " [ 0.34184887]\n",
      " [ 0.14606139]\n",
      " [-0.87023304]]\n",
      "t [[ 0.14606139]\n",
      " [-1.39209612]\n",
      " [-0.79197971]\n",
      " ...\n",
      " [ 0.34184887]\n",
      " [ 0.14606139]\n",
      " [-0.87023304]]\n",
      "Current iteration=6, loss=37667.01423352606\n",
      "t [[ 0.14220011]\n",
      " [-1.53442603]\n",
      " [-0.87678196]\n",
      " ...\n",
      " [ 0.33951714]\n",
      " [ 0.14220011]\n",
      " [-0.94017763]]\n",
      "t [[ 0.14220011]\n",
      " [-1.53442603]\n",
      " [-0.87678196]\n",
      " ...\n",
      " [ 0.33951714]\n",
      " [ 0.14220011]\n",
      " [-0.94017763]]\n",
      "t [[ 0.13600125]\n",
      " [-1.66159835]\n",
      " [-0.95436187]\n",
      " ...\n",
      " [ 0.33327399]\n",
      " [ 0.13600125]\n",
      " [-1.00341954]]\n",
      "t [[ 0.13600125]\n",
      " [-1.66159835]\n",
      " [-0.95436187]\n",
      " ...\n",
      " [ 0.33327399]\n",
      " [ 0.13600125]\n",
      " [-1.00341954]]\n",
      "Current iteration=8, loss=35907.417657085025\n",
      "t [[ 0.12821907]\n",
      " [-1.77598494]\n",
      " [-1.02575575]\n",
      " ...\n",
      " [ 0.32435578]\n",
      " [ 0.12821907]\n",
      " [-1.06122278]]\n",
      "t [[ 0.12821907]\n",
      " [-1.77598494]\n",
      " [-1.02575575]\n",
      " ...\n",
      " [ 0.32435578]\n",
      " [ 0.12821907]\n",
      " [-1.06122278]]\n",
      "t [[ 0.11939353]\n",
      " [-1.87949244]\n",
      " [-1.09178252]\n",
      " ...\n",
      " [ 0.31359947]\n",
      " [ 0.11939353]\n",
      " [-1.11446265]]\n",
      "loss=34640.229034741526\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.202786  ]\n",
      " [-0.11821611]\n",
      " [-0.18919401]\n",
      " ...\n",
      " [ 0.15888936]\n",
      " [ 0.06688899]\n",
      " [-0.26920575]]\n",
      "t [[ 0.202786  ]\n",
      " [-0.11821611]\n",
      " [-0.18919401]\n",
      " ...\n",
      " [ 0.15888936]\n",
      " [ 0.06688899]\n",
      " [-0.26920575]]\n",
      "t [[ 0.3564695 ]\n",
      " [-0.2726073 ]\n",
      " [-0.34561524]\n",
      " ...\n",
      " [ 0.24617451]\n",
      " [ 0.1065588 ]\n",
      " [-0.449332  ]]\n",
      "t [[ 0.3564695 ]\n",
      " [-0.2726073 ]\n",
      " [-0.34561524]\n",
      " ...\n",
      " [ 0.24617451]\n",
      " [ 0.1065588 ]\n",
      " [-0.449332  ]]\n",
      "Current iteration=2, loss=44364.121632824244\n",
      "t [[ 0.47523861]\n",
      " [-0.4328208 ]\n",
      " [-0.47914593]\n",
      " ...\n",
      " [ 0.29409867]\n",
      " [ 0.12910773]\n",
      " [-0.58138719]]\n",
      "t [[ 0.47523861]\n",
      " [-0.4328208 ]\n",
      " [-0.47914593]\n",
      " ...\n",
      " [ 0.29409867]\n",
      " [ 0.12910773]\n",
      " [-0.58138719]]\n",
      "t [[ 0.56881705]\n",
      " [-0.58651547]\n",
      " [-0.59583184]\n",
      " ...\n",
      " [ 0.31932276]\n",
      " [ 0.14060648]\n",
      " [-0.68593573]]\n",
      "t [[ 0.56881705]\n",
      " [-0.58651547]\n",
      " [-0.59583184]\n",
      " ...\n",
      " [ 0.31932276]\n",
      " [ 0.14060648]\n",
      " [-0.68593573]]\n",
      "Current iteration=4, loss=40239.32432016853\n",
      "t [[ 0.64376556]\n",
      " [-0.72956423]\n",
      " [-0.69951434]\n",
      " ...\n",
      " [ 0.33083198]\n",
      " [ 0.1447735 ]\n",
      " [-0.77343827]]\n",
      "t [[ 0.64376556]\n",
      " [-0.72956423]\n",
      " [-0.69951434]\n",
      " ...\n",
      " [ 0.33083198]\n",
      " [ 0.1447735 ]\n",
      " [-0.77343827]]\n",
      "t [[ 0.70463843]\n",
      " [-0.86122742]\n",
      " [-0.79277711]\n",
      " ...\n",
      " [ 0.33369329]\n",
      " [ 0.14398022]\n",
      " [-0.84946355]]\n",
      "t [[ 0.70463843]\n",
      " [-0.86122742]\n",
      " [-0.79277711]\n",
      " ...\n",
      " [ 0.33369329]\n",
      " [ 0.14398022]\n",
      " [-0.84946355]]\n",
      "Current iteration=6, loss=37654.43666334627\n",
      "t [[ 0.75468967]\n",
      " [-0.98206618]\n",
      " [-0.87745198]\n",
      " ...\n",
      " [ 0.33090332]\n",
      " [ 0.13979781]\n",
      " [-0.91715601]]\n",
      "t [[ 0.75468967]\n",
      " [-0.98206618]\n",
      " [-0.87745198]\n",
      " ...\n",
      " [ 0.33090332]\n",
      " [ 0.13979781]\n",
      " [-0.91715601]]\n",
      "t [[ 0.79630059]\n",
      " [-1.09306936]\n",
      " [-0.95489619]\n",
      " ...\n",
      " [ 0.3243236 ]\n",
      " [ 0.1333009 ]\n",
      " [-0.97841281]]\n",
      "t [[ 0.79630059]\n",
      " [-1.09306936]\n",
      " [-0.95489619]\n",
      " ...\n",
      " [ 0.3243236 ]\n",
      " [ 0.1333009 ]\n",
      " [-0.97841281]]\n",
      "Current iteration=8, loss=35895.103410051626\n",
      "t [[ 0.83124915]\n",
      " [-1.19529774]\n",
      " [-1.02615142]\n",
      " ...\n",
      " [ 0.31516399]\n",
      " [ 0.12524327]\n",
      " [-1.03445828]]\n",
      "t [[ 0.83124915]\n",
      " [-1.19529774]\n",
      " [-1.02615142]\n",
      " ...\n",
      " [ 0.31516399]\n",
      " [ 0.12524327]\n",
      " [-1.03445828]]\n",
      "t [[ 0.86088551]\n",
      " [-1.28974769]\n",
      " [-1.09203983]\n",
      " ...\n",
      " [ 0.30424156]\n",
      " [ 0.11616396]\n",
      " [-1.08613424]]\n",
      "loss=34627.87985397489\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.2001869 ]\n",
      " [-0.11316679]\n",
      " [-0.18848545]\n",
      " ...\n",
      " [ 0.16134082]\n",
      " [ 0.06812182]\n",
      " [-0.27749595]]\n",
      "t [[ 0.2001869 ]\n",
      " [-0.11316679]\n",
      " [-0.18848545]\n",
      " ...\n",
      " [ 0.16134082]\n",
      " [ 0.06812182]\n",
      " [-0.27749595]]\n",
      "t [[ 0.35192061]\n",
      " [-0.26267338]\n",
      " [-0.34420301]\n",
      " ...\n",
      " [ 0.24916365]\n",
      " [ 0.1087009 ]\n",
      " [-0.46325034]]\n",
      "t [[ 0.35192061]\n",
      " [-0.26267338]\n",
      " [-0.34420301]\n",
      " ...\n",
      " [ 0.24916365]\n",
      " [ 0.1087009 ]\n",
      " [-0.46325034]]\n",
      "Current iteration=2, loss=44431.42398158802\n",
      "t [[ 0.46922932]\n",
      " [-0.41803832]\n",
      " [-0.47709303]\n",
      " ...\n",
      " [ 0.29666829]\n",
      " [ 0.13197078]\n",
      " [-0.59940329]]\n",
      "t [[ 0.46922932]\n",
      " [-0.41803832]\n",
      " [-0.47709303]\n",
      " ...\n",
      " [ 0.29666829]\n",
      " [ 0.13197078]\n",
      " [-0.59940329]]\n",
      "t [[ 0.56172082]\n",
      " [-0.56695318]\n",
      " [-0.59320694]\n",
      " ...\n",
      " [ 0.32093548]\n",
      " [ 0.14406845]\n",
      " [-0.70708292]]\n",
      "t [[ 0.56172082]\n",
      " [-0.56695318]\n",
      " [-0.59320694]\n",
      " ...\n",
      " [ 0.32093548]\n",
      " [ 0.14406845]\n",
      " [-0.70708292]]\n",
      "Current iteration=4, loss=40358.82787849893\n",
      "t [[ 0.63586398]\n",
      " [-0.70534373]\n",
      " [-0.69637677]\n",
      " ...\n",
      " [ 0.33115997]\n",
      " [ 0.1487471 ]\n",
      " [-0.7970571 ]]\n",
      "t [[ 0.63586398]\n",
      " [-0.70534373]\n",
      " [-0.69637677]\n",
      " ...\n",
      " [ 0.33115997]\n",
      " [ 0.1487471 ]\n",
      " [-0.7970571 ]]\n",
      "t [[ 0.69613949]\n",
      " [-0.83251016]\n",
      " [-0.78917516]\n",
      " ...\n",
      " [ 0.33253693]\n",
      " [ 0.14839891]\n",
      " [-0.87508231]]\n",
      "t [[ 0.69613949]\n",
      " [-0.83251016]\n",
      " [-0.78917516]\n",
      " ...\n",
      " [ 0.33253693]\n",
      " [ 0.14839891]\n",
      " [-0.87508231]]\n",
      "Current iteration=6, loss=37810.28099167271\n",
      "t [[ 0.74574607]\n",
      " [-0.94903573]\n",
      " [-0.87342426]\n",
      " ...\n",
      " [ 0.32814421]\n",
      " [ 0.14460878]\n",
      " [-0.94442422]]\n",
      "t [[ 0.74574607]\n",
      " [-0.94903573]\n",
      " [-0.87342426]\n",
      " ...\n",
      " [ 0.32814421]\n",
      " [ 0.14460878]\n",
      " [-0.94442422]]\n",
      "t [[ 0.78702457]\n",
      " [-1.05591801]\n",
      " [-0.9504735 ]\n",
      " ...\n",
      " [ 0.31989554]\n",
      " [ 0.13846114]\n",
      " [-1.00706153]]\n",
      "t [[ 0.78702457]\n",
      " [-1.05591801]\n",
      " [-0.9504735 ]\n",
      " ...\n",
      " [ 0.31989554]\n",
      " [ 0.13846114]\n",
      " [-1.00706153]]\n",
      "Current iteration=8, loss=36077.10610893248\n",
      "t [[ 0.8217235 ]\n",
      " [-1.15421793]\n",
      " [-1.02135842]\n",
      " ...\n",
      " [ 0.30903488]\n",
      " [ 0.13071709]\n",
      " [-1.06427549]]\n",
      "t [[ 0.8217235 ]\n",
      " [-1.15421793]\n",
      " [-1.02135842]\n",
      " ...\n",
      " [ 0.30903488]\n",
      " [ 0.13071709]\n",
      " [-1.06427549]]\n",
      "t [[ 0.85117152]\n",
      " [-1.24492696]\n",
      " [-1.08689642]\n",
      " ...\n",
      " [ 0.29640196]\n",
      " [ 0.12192136]\n",
      " [-1.11694902]]\n",
      "loss=34829.42812125769\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.19986645]\n",
      " [-0.11889301]\n",
      " [-0.18789341]\n",
      " ...\n",
      " [ 0.28787054]\n",
      " [-0.18789341]\n",
      " [ 0.26963154]]\n",
      "t [[ 0.19986645]\n",
      " [-0.11889301]\n",
      " [-0.18789341]\n",
      " ...\n",
      " [ 0.28787054]\n",
      " [-0.18789341]\n",
      " [ 0.26963154]]\n",
      "t [[ 0.35133872]\n",
      " [-0.27216701]\n",
      " [-0.34329995]\n",
      " ...\n",
      " [ 0.49739726]\n",
      " [-0.34329995]\n",
      " [ 0.4532159 ]]\n",
      "t [[ 0.35133872]\n",
      " [-0.27216701]\n",
      " [-0.34329995]\n",
      " ...\n",
      " [ 0.49739726]\n",
      " [-0.34329995]\n",
      " [ 0.4532159 ]]\n",
      "Current iteration=2, loss=44434.105287660066\n",
      "t [[ 0.4684124 ]\n",
      " [-0.43041075]\n",
      " [-0.47599164]\n",
      " ...\n",
      " [ 0.65474896]\n",
      " [-0.47599164]\n",
      " [ 0.58328833]]\n",
      "t [[ 0.4684124 ]\n",
      " [-0.43041075]\n",
      " [-0.47599164]\n",
      " ...\n",
      " [ 0.65474896]\n",
      " [-0.47599164]\n",
      " [ 0.58328833]]\n",
      "t [[ 0.56057898]\n",
      " [-0.58185262]\n",
      " [-0.59195694]\n",
      " ...\n",
      " [ 0.77612062]\n",
      " [-0.59195694]\n",
      " [ 0.67867286]]\n",
      "t [[ 0.56057898]\n",
      " [-0.58185262]\n",
      " [-0.59195694]\n",
      " ...\n",
      " [ 0.77612062]\n",
      " [-0.59195694]\n",
      " [ 0.67867286]]\n",
      "Current iteration=4, loss=40350.487668584654\n",
      "t [[ 0.63429826]\n",
      " [-0.72261297]\n",
      " [-0.69500049]\n",
      " ...\n",
      " [ 0.87175875]\n",
      " [-0.69500049]\n",
      " [ 0.75053209]]\n",
      "t [[ 0.63429826]\n",
      " [-0.72261297]\n",
      " [-0.69500049]\n",
      " ...\n",
      " [ 0.87175875]\n",
      " [-0.69500049]\n",
      " [ 0.75053209]]\n",
      "t [[ 0.69407696]\n",
      " [-0.85205102]\n",
      " [-0.7876832 ]\n",
      " ...\n",
      " [ 0.94842783]\n",
      " [-0.7876832 ]\n",
      " [ 0.80582361]]\n",
      "t [[ 0.69407696]\n",
      " [-0.85205102]\n",
      " [-0.7876832 ]\n",
      " ...\n",
      " [ 0.94842783]\n",
      " [-0.7876832 ]\n",
      " [ 0.80582361]]\n",
      "Current iteration=6, loss=37793.18646952115\n",
      "t [[ 0.74313961]\n",
      " [-0.97076394]\n",
      " [-0.87182276]\n",
      " ...\n",
      " [ 1.01077451]\n",
      " [-0.87182276]\n",
      " [ 0.84909742]]\n",
      "t [[ 0.74313961]\n",
      " [-0.97076394]\n",
      " [-0.87182276]\n",
      " ...\n",
      " [ 1.01077451]\n",
      " [-0.87182276]\n",
      " [ 0.84909742]]\n",
      "t [[ 0.78384637]\n",
      " [-1.0797498 ]\n",
      " [-0.94876712]\n",
      " ...\n",
      " [ 1.06209977]\n",
      " [-0.94876712]\n",
      " [ 0.8834543 ]]\n",
      "t [[ 0.78384637]\n",
      " [-1.0797498 ]\n",
      " [-0.94876712]\n",
      " ...\n",
      " [ 1.06209977]\n",
      " [-0.94876712]\n",
      " [ 0.8834543 ]]\n",
      "Current iteration=8, loss=36053.52482791373\n",
      "t [[ 0.81795912]\n",
      " [-1.18006742]\n",
      " [-1.01955153]\n",
      " ...\n",
      " [ 1.10481427]\n",
      " [-1.01955153]\n",
      " [ 0.91108024]]\n",
      "t [[ 0.81795912]\n",
      " [-1.18006742]\n",
      " [-1.01955153]\n",
      " ...\n",
      " [ 1.10481427]\n",
      " [-1.01955153]\n",
      " [ 0.91108024]]\n",
      "t [[ 0.84681559]\n",
      " [-1.27270705]\n",
      " [-1.08499338]\n",
      " ...\n",
      " [ 1.14071859]\n",
      " [-1.08499338]\n",
      " [ 0.93355893]]\n",
      "loss=34801.03991427561\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06901799]\n",
      " [-0.33049249]\n",
      " [-0.19380459]\n",
      " ...\n",
      " [ 0.16572865]\n",
      " [ 0.06901799]\n",
      " [-0.28092931]]\n",
      "t [[ 0.06901799]\n",
      " [-0.33049249]\n",
      " [-0.19380459]\n",
      " ...\n",
      " [ 0.16572865]\n",
      " [ 0.06901799]\n",
      " [-0.28092931]]\n",
      "t [[ 0.10939194]\n",
      " [-0.61551963]\n",
      " [-0.353369  ]\n",
      " ...\n",
      " [ 0.25531682]\n",
      " [ 0.10939194]\n",
      " [-0.46775496]]\n",
      "t [[ 0.10939194]\n",
      " [-0.61551963]\n",
      " [-0.353369  ]\n",
      " ...\n",
      " [ 0.25531682]\n",
      " [ 0.10939194]\n",
      " [-0.46775496]]\n",
      "Current iteration=2, loss=44217.270745385984\n",
      "t [[ 0.13201422]\n",
      " [-0.86068474]\n",
      " [-0.48925746]\n",
      " ...\n",
      " [ 0.30383168]\n",
      " [ 0.13201422]\n",
      " [-0.60465135]]\n",
      "t [[ 0.13201422]\n",
      " [-0.86068474]\n",
      " [-0.48925746]\n",
      " ...\n",
      " [ 0.30383168]\n",
      " [ 0.13201422]\n",
      " [-0.60465135]]\n",
      "t [[ 0.14330942]\n",
      " [-1.07253148]\n",
      " [-0.6078137 ]\n",
      " ...\n",
      " [ 0.32896588]\n",
      " [ 0.14330942]\n",
      " [-0.71321531]]\n",
      "t [[ 0.14330942]\n",
      " [-1.07253148]\n",
      " [-0.6078137 ]\n",
      " ...\n",
      " [ 0.32896588]\n",
      " [ 0.14330942]\n",
      " [-0.71321531]]\n",
      "Current iteration=4, loss=40065.70695159934\n",
      "t [[ 0.14715815]\n",
      " [-1.25695814]\n",
      " [-0.71302336]\n",
      " ...\n",
      " [ 0.34009798]\n",
      " [ 0.14715815]\n",
      " [-0.80422751]]\n",
      "t [[ 0.14715815]\n",
      " [-1.25695814]\n",
      " [-0.71302336]\n",
      " ...\n",
      " [ 0.34009798]\n",
      " [ 0.14715815]\n",
      " [-0.80422751]]\n",
      "t [[ 0.1460106 ]\n",
      " [-1.41884996]\n",
      " [-0.80755176]\n",
      " ...\n",
      " [ 0.34245266]\n",
      " [ 0.1460106 ]\n",
      " [-0.88336215]]\n",
      "t [[ 0.1460106 ]\n",
      " [-1.41884996]\n",
      " [-0.80755176]\n",
      " ...\n",
      " [ 0.34245266]\n",
      " [ 0.1460106 ]\n",
      " [-0.88336215]]\n",
      "Current iteration=6, loss=37484.74644578734\n",
      "t [[ 0.14147797]\n",
      " [-1.56212346]\n",
      " [-0.89328394]\n",
      " ...\n",
      " [ 0.33909624]\n",
      " [ 0.14147797]\n",
      " [-0.9538116 ]]\n",
      "t [[ 0.14147797]\n",
      " [-1.56212346]\n",
      " [-0.89328394]\n",
      " ...\n",
      " [ 0.33909624]\n",
      " [ 0.14147797]\n",
      " [-0.9538116 ]]\n",
      "t [[ 0.13465562]\n",
      " [-1.68988759]\n",
      " [-0.9716154 ]\n",
      " ...\n",
      " [ 0.33192546]\n",
      " [ 0.13465562]\n",
      " [-1.01750803]]\n",
      "t [[ 0.13465562]\n",
      " [-1.68988759]\n",
      " [-0.9716154 ]\n",
      " ...\n",
      " [ 0.33192546]\n",
      " [ 0.13465562]\n",
      " [-1.01750803]]\n",
      "Current iteration=8, loss=35737.8997310289\n",
      "t [[ 0.12630776]\n",
      " [-1.80461297]\n",
      " [-1.04361701]\n",
      " ...\n",
      " [ 0.32217027]\n",
      " [ 0.12630776]\n",
      " [-1.07570821]]\n",
      "t [[ 0.12630776]\n",
      " [-1.80461297]\n",
      " [-1.04361701]\n",
      " ...\n",
      " [ 0.32217027]\n",
      " [ 0.12630776]\n",
      " [-1.07570821]]\n",
      "t [[ 0.11697803]\n",
      " [-1.90827522]\n",
      " [-1.11013389]\n",
      " ...\n",
      " [ 0.31066009]\n",
      " [ 0.11697803]\n",
      " [-1.12928493]]\n",
      "loss=34485.026228725794\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.20841895]\n",
      " [-0.12149989]\n",
      " [-0.1944494 ]\n",
      " ...\n",
      " [ 0.16330296]\n",
      " [ 0.06874702]\n",
      " [-0.27668369]]\n",
      "t [[ 0.20841895]\n",
      " [-0.12149989]\n",
      " [-0.1944494 ]\n",
      " ...\n",
      " [ 0.16330296]\n",
      " [ 0.06874702]\n",
      " [-0.27668369]]\n",
      "t [[ 0.36499751]\n",
      " [-0.28118854]\n",
      " [-0.35429606]\n",
      " ...\n",
      " [ 0.25100579]\n",
      " [ 0.10875685]\n",
      " [-0.45932047]]\n",
      "t [[ 0.36499751]\n",
      " [-0.28118854]\n",
      " [-0.35429606]\n",
      " ...\n",
      " [ 0.25100579]\n",
      " [ 0.10875685]\n",
      " [-0.45932047]]\n",
      "Current iteration=2, loss=44204.720478437994\n",
      "t [[ 0.48514101]\n",
      " [-0.44616381]\n",
      " [-0.49026689]\n",
      " ...\n",
      " [ 0.29807857]\n",
      " [ 0.13098759]\n",
      " [-0.59238975]]\n",
      "t [[ 0.48514101]\n",
      " [-0.44616381]\n",
      " [-0.49026689]\n",
      " ...\n",
      " [ 0.29807857]\n",
      " [ 0.13098759]\n",
      " [-0.59238975]]\n",
      "t [[ 0.57923813]\n",
      " [-0.60358859]\n",
      " [-0.6087966 ]\n",
      " ...\n",
      " [ 0.32212493]\n",
      " [ 0.14189396]\n",
      " [-0.69756699]]\n",
      "t [[ 0.57923813]\n",
      " [-0.60358859]\n",
      " [-0.6087966 ]\n",
      " ...\n",
      " [ 0.32212493]\n",
      " [ 0.14189396]\n",
      " [-0.69756699]]\n",
      "Current iteration=4, loss=40052.70182583994\n",
      "t [[ 0.65421403]\n",
      " [-0.74944158]\n",
      " [-0.71392046]\n",
      " ...\n",
      " [ 0.33244349]\n",
      " [ 0.1453701 ]\n",
      " [-0.78561663]]\n",
      "t [[ 0.65421403]\n",
      " [-0.74944158]\n",
      " [-0.71392046]\n",
      " ...\n",
      " [ 0.33244349]\n",
      " [ 0.1453701 ]\n",
      " [-0.78561663]]\n",
      "t [[ 0.71483412]\n",
      " [-0.88319494]\n",
      " [-0.80833127]\n",
      " ...\n",
      " [ 0.33419371]\n",
      " [ 0.14387204]\n",
      " [-0.86216506]]\n",
      "t [[ 0.71483412]\n",
      " [-0.88319494]\n",
      " [-0.80833127]\n",
      " ...\n",
      " [ 0.33419371]\n",
      " [ 0.14387204]\n",
      " [-0.86216506]]\n",
      "Current iteration=6, loss=37472.20730969154\n",
      "t [[ 0.76447924]\n",
      " [-1.00559682]\n",
      " [-0.89392961]\n",
      " ...\n",
      " [ 0.33039229]\n",
      " [ 0.1390131 ]\n",
      " [-0.93035156]]\n",
      "t [[ 0.76447924]\n",
      " [-1.00559682]\n",
      " [-0.89392961]\n",
      " ...\n",
      " [ 0.33039229]\n",
      " [ 0.1390131 ]\n",
      " [-0.93035156]]\n",
      "t [[ 0.80560862]\n",
      " [-1.11777675]\n",
      " [-0.97211994]\n",
      " ...\n",
      " [ 0.32289912]\n",
      " [ 0.1318889 ]\n",
      " [-0.99205938]]\n",
      "t [[ 0.80560862]\n",
      " [-1.11777675]\n",
      " [-0.97211994]\n",
      " ...\n",
      " [ 0.32289912]\n",
      " [ 0.1318889 ]\n",
      " [-0.99205938]]\n",
      "Current iteration=8, loss=35725.60287503002\n",
      "t [[ 0.84004871]\n",
      " [-1.22089618]\n",
      " [-1.04397834]\n",
      " ...\n",
      " [ 0.31291688]\n",
      " [ 0.123263  ]\n",
      " [-1.04850328]]\n",
      "t [[ 0.84004871]\n",
      " [-1.22089618]\n",
      " [-1.04397834]\n",
      " ...\n",
      " [ 0.31291688]\n",
      " [ 0.123263  ]\n",
      " [-1.04850328]]\n",
      "t [[ 0.86917981]\n",
      " [-1.31602284]\n",
      " [-1.11035305]\n",
      " ...\n",
      " [ 0.30125478]\n",
      " [ 0.11367793]\n",
      " [-1.10052104]]\n",
      "loss=34472.65808357359\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.20574765]\n",
      " [-0.11631031]\n",
      " [-0.19372116]\n",
      " ...\n",
      " [ 0.16582251]\n",
      " [ 0.07001409]\n",
      " [-0.28520417]]\n",
      "t [[ 0.20574765]\n",
      " [-0.11631031]\n",
      " [-0.19372116]\n",
      " ...\n",
      " [ 0.16582251]\n",
      " [ 0.07001409]\n",
      " [-0.28520417]]\n",
      "t [[ 0.36034032]\n",
      " [-0.27098326]\n",
      " [-0.35284501]\n",
      " ...\n",
      " [ 0.25402676]\n",
      " [ 0.11094966]\n",
      " [-0.47355274]]\n",
      "t [[ 0.36034032]\n",
      " [-0.27098326]\n",
      " [-0.35284501]\n",
      " ...\n",
      " [ 0.25402676]\n",
      " [ 0.11094966]\n",
      " [-0.47355274]]\n",
      "Current iteration=2, loss=44273.809617228515\n",
      "t [[ 0.47900991]\n",
      " [-0.43097738]\n",
      " [-0.48816103]\n",
      " ...\n",
      " [ 0.30061659]\n",
      " [ 0.13391114]\n",
      " [-0.61075058]]\n",
      "t [[ 0.47900991]\n",
      " [-0.43097738]\n",
      " [-0.48816103]\n",
      " ...\n",
      " [ 0.30061659]\n",
      " [ 0.13391114]\n",
      " [-0.61075058]]\n",
      "t [[ 0.57202073]\n",
      " [-0.58349509]\n",
      " [-0.60610845]\n",
      " ...\n",
      " [ 0.32363407]\n",
      " [ 0.14542291]\n",
      " [-0.71906637]]\n",
      "t [[ 0.57202073]\n",
      " [-0.58349509]\n",
      " [-0.60610845]\n",
      " ...\n",
      " [ 0.32363407]\n",
      " [ 0.14542291]\n",
      " [-0.71906637]]\n",
      "Current iteration=4, loss=40174.70592441565\n",
      "t [[ 0.64619939]\n",
      " [-0.72457356]\n",
      " [-0.71071176]\n",
      " ...\n",
      " [ 0.33259538]\n",
      " [ 0.14941508]\n",
      " [-0.80958382]]\n",
      "t [[ 0.64619939]\n",
      " [-0.72457356]\n",
      " [-0.71071176]\n",
      " ...\n",
      " [ 0.33259538]\n",
      " [ 0.14941508]\n",
      " [-0.80958382]]\n",
      "t [[ 0.70623344]\n",
      " [-0.85372695]\n",
      " [-0.80465171]\n",
      " ...\n",
      " [ 0.332792  ]\n",
      " [ 0.14836506]\n",
      " [-0.88812275]]\n",
      "t [[ 0.70623344]\n",
      " [-0.85372695]\n",
      " [-0.80465171]\n",
      " ...\n",
      " [ 0.332792  ]\n",
      " [ 0.14836506]\n",
      " [-0.88812275]]\n",
      "Current iteration=6, loss=37630.69242821326\n",
      "t [[ 0.7554461 ]\n",
      " [-0.97172551]\n",
      " [-0.8898186 ]\n",
      " ...\n",
      " [ 0.32732327]\n",
      " [ 0.14390033]\n",
      " [-0.95794634]]\n",
      "t [[ 0.7554461 ]\n",
      " [-0.97172551]\n",
      " [-0.8898186 ]\n",
      " ...\n",
      " [ 0.32732327]\n",
      " [ 0.14390033]\n",
      " [-0.95794634]]\n",
      "t [[ 0.79625491]\n",
      " [-1.07970648]\n",
      " [-0.96760869]\n",
      " ...\n",
      " [ 0.31810159]\n",
      " [ 0.13712657]\n",
      " [-1.02102072]]\n",
      "t [[ 0.79625491]\n",
      " [-1.07970648]\n",
      " [-0.96760869]\n",
      " ...\n",
      " [ 0.31810159]\n",
      " [ 0.13712657]\n",
      " [-1.02102072]]\n",
      "Current iteration=8, loss=35910.18293563311\n",
      "t [[ 0.83045625]\n",
      " [-1.17883018]\n",
      " [-1.03909172]\n",
      " ...\n",
      " [ 0.30636358]\n",
      " [ 0.12881489]\n",
      " [-1.07861826]]\n",
      "t [[ 0.83045625]\n",
      " [-1.17883018]\n",
      " [-1.03909172]\n",
      " ...\n",
      " [ 0.30636358]\n",
      " [ 0.12881489]\n",
      " [-1.07861826]]\n",
      "t [[ 0.85940865]\n",
      " [-1.27015807]\n",
      " [-1.10511101]\n",
      " ...\n",
      " [ 0.2929408 ]\n",
      " [ 0.11951365]\n",
      " [-1.13161822]]\n",
      "loss=34676.64599555236\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.2054183 ]\n",
      " [-0.12219559]\n",
      " [-0.19311267]\n",
      " ...\n",
      " [ 0.29586694]\n",
      " [-0.19311267]\n",
      " [ 0.27712131]]\n",
      "t [[ 0.2054183 ]\n",
      " [-0.12219559]\n",
      " [-0.19311267]\n",
      " ...\n",
      " [ 0.29586694]\n",
      " [-0.19311267]\n",
      " [ 0.27712131]]\n",
      "t [[ 0.35974223]\n",
      " [-0.28068756]\n",
      " [-0.35192448]\n",
      " ...\n",
      " [ 0.5090191 ]\n",
      " [-0.35192448]\n",
      " [ 0.46339626]]\n",
      "t [[ 0.35974223]\n",
      " [-0.28068756]\n",
      " [-0.35192448]\n",
      " ...\n",
      " [ 0.5090191 ]\n",
      " [-0.35192448]\n",
      " [ 0.46339626]]\n",
      "Current iteration=2, loss=44276.23250781192\n",
      "t [[ 0.47816606]\n",
      " [-0.4435969 ]\n",
      " [-0.48704277]\n",
      " ...\n",
      " [ 0.66785799]\n",
      " [-0.48704277]\n",
      " [ 0.5941278 ]]\n",
      "t [[ 0.47816606]\n",
      " [-0.4435969 ]\n",
      " [-0.48704277]\n",
      " ...\n",
      " [ 0.66785799]\n",
      " [-0.48704277]\n",
      " [ 0.5941278 ]]\n",
      "t [[ 0.57083315]\n",
      " [-0.59868581]\n",
      " [-0.60484142]\n",
      " ...\n",
      " [ 0.7896263 ]\n",
      " [-0.60484142]\n",
      " [ 0.68929655]]\n",
      "t [[ 0.57083315]\n",
      " [-0.59868581]\n",
      " [-0.60484142]\n",
      " ...\n",
      " [ 0.7896263 ]\n",
      " [-0.60484142]\n",
      " [ 0.68929655]]\n",
      "Current iteration=4, loss=40165.80192876677\n",
      "t [[ 0.64456572]\n",
      " [-0.74218348]\n",
      " [-0.70931727]\n",
      " ...\n",
      " [ 0.88508371]\n",
      " [-0.70931727]\n",
      " [ 0.76056027]]\n",
      "t [[ 0.64456572]\n",
      " [-0.74218348]\n",
      " [-0.70931727]\n",
      " ...\n",
      " [ 0.88508371]\n",
      " [-0.70931727]\n",
      " [ 0.76056027]]\n",
      "t [[ 0.7040805 ]\n",
      " [-0.87365814]\n",
      " [-0.80313976]\n",
      " ...\n",
      " [ 0.96126509]\n",
      " [-0.80313976]\n",
      " [ 0.81510397]]\n",
      "t [[ 0.7040805 ]\n",
      " [-0.87365814]\n",
      " [-0.80313976]\n",
      " ...\n",
      " [ 0.96126509]\n",
      " [-0.80313976]\n",
      " [ 0.81510397]]\n",
      "Current iteration=6, loss=37612.95789572327\n",
      "t [[ 0.75272785]\n",
      " [-0.99389065]\n",
      " [-0.8881951 ]\n",
      " ...\n",
      " [ 1.02296865]\n",
      " [-0.8881951 ]\n",
      " [ 0.85758953]]\n",
      "t [[ 0.75272785]\n",
      " [-0.99389065]\n",
      " [-0.8881951 ]\n",
      " ...\n",
      " [ 1.02296865]\n",
      " [-0.8881951 ]\n",
      " [ 0.85758953]]\n",
      "t [[ 0.79294508]\n",
      " [-1.10401707]\n",
      " [-0.96587839]\n",
      " ...\n",
      " [ 1.07358426]\n",
      " [-0.96587839]\n",
      " [ 0.89117564]]\n",
      "t [[ 0.79294508]\n",
      " [-1.10401707]\n",
      " [-0.96587839]\n",
      " ...\n",
      " [ 1.07358426]\n",
      " [-0.96587839]\n",
      " [ 0.89117564]]\n",
      "Current iteration=8, loss=35885.965246746244\n",
      "t [[ 0.82654203]\n",
      " [-1.20519506]\n",
      " [-1.03725917]\n",
      " ...\n",
      " [ 1.11557612]\n",
      " [-1.03725917]\n",
      " [ 0.91807888]]\n",
      "t [[ 0.82654203]\n",
      " [-1.20519506]\n",
      " [-1.03725917]\n",
      " ...\n",
      " [ 1.11557612]\n",
      " [-1.03725917]\n",
      " [ 0.91807888]]\n",
      "t [[ 0.85488618]\n",
      " [-1.29848502]\n",
      " [-1.10318084]\n",
      " ...\n",
      " [ 1.15077713]\n",
      " [-1.10318084]\n",
      " [ 0.93989856]]\n",
      "loss=34647.66761284361\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07088334]\n",
      " [-0.33942471]\n",
      " [-0.19904255]\n",
      " ...\n",
      " [ 0.17020781]\n",
      " [ 0.07088334]\n",
      " [-0.28852199]]\n",
      "t [[ 0.07088334]\n",
      " [-0.33942471]\n",
      " [-0.19904255]\n",
      " ...\n",
      " [ 0.17020781]\n",
      " [ 0.07088334]\n",
      " [-0.28852199]]\n",
      "t [[ 0.11156967]\n",
      " [-0.63090987]\n",
      " [-0.36198612]\n",
      " ...\n",
      " [ 0.26014474]\n",
      " [ 0.11156967]\n",
      " [-0.47783892]]\n",
      "t [[ 0.11156967]\n",
      " [-0.63090987]\n",
      " [-0.36198612]\n",
      " ...\n",
      " [ 0.26014474]\n",
      " [ 0.11156967]\n",
      " [-0.47783892]]\n",
      "Current iteration=2, loss=44060.16498132691\n",
      "t [[ 0.13385202]\n",
      " [-0.88054504]\n",
      " [-0.50027192]\n",
      " ...\n",
      " [ 0.30775761]\n",
      " [ 0.13385202]\n",
      " [-0.61575613]]\n",
      "t [[ 0.13385202]\n",
      " [-0.88054504]\n",
      " [-0.50027192]\n",
      " ...\n",
      " [ 0.30775761]\n",
      " [ 0.13385202]\n",
      " [-0.61575613]]\n",
      "t [[ 0.14454415]\n",
      " [-1.09542781]\n",
      " [-0.62063488]\n",
      " ...\n",
      " [ 0.33169025]\n",
      " [ 0.14454415]\n",
      " [-0.72497496]]\n",
      "t [[ 0.14454415]\n",
      " [-1.09542781]\n",
      " [-0.62063488]\n",
      " ...\n",
      " [ 0.33169025]\n",
      " [ 0.14454415]\n",
      " [-0.72497496]]\n",
      "Current iteration=4, loss=39883.54509438077\n",
      "t [[ 0.14770032]\n",
      " [-1.28189187]\n",
      " [-0.72725252]\n",
      " ...\n",
      " [ 0.34162345]\n",
      " [ 0.14770032]\n",
      " [-0.81656015]]\n",
      "t [[ 0.14770032]\n",
      " [-1.28189187]\n",
      " [-0.72725252]\n",
      " ...\n",
      " [ 0.34162345]\n",
      " [ 0.14770032]\n",
      " [-0.81656015]]\n",
      "t [[ 0.14585279]\n",
      " [-1.44513032]\n",
      " [-0.82289792]\n",
      " ...\n",
      " [ 0.34286601]\n",
      " [ 0.14585279]\n",
      " [-0.89623399]]\n",
      "t [[ 0.14585279]\n",
      " [-1.44513032]\n",
      " [-0.82289792]\n",
      " ...\n",
      " [ 0.34286601]\n",
      " [ 0.14585279]\n",
      " [-0.89623399]]\n",
      "Current iteration=6, loss=37308.05132647411\n",
      "t [[ 0.140653  ]\n",
      " [-1.58926939]\n",
      " [-0.90952429]\n",
      " ...\n",
      " [ 0.33850028]\n",
      " [ 0.140653  ]\n",
      " [-0.96718177]]\n",
      "t [[ 0.140653  ]\n",
      " [-1.58926939]\n",
      " [-0.90952429]\n",
      " ...\n",
      " [ 0.33850028]\n",
      " [ 0.140653  ]\n",
      " [-0.96718177]]\n",
      "t [[ 0.13321591]\n",
      " [-1.71756018]\n",
      " [-0.98857418]\n",
      " ...\n",
      " [ 0.33041977]\n",
      " [ 0.13321591]\n",
      " [-1.03132323]]\n",
      "t [[ 0.13321591]\n",
      " [-1.71756018]\n",
      " [-0.98857418]\n",
      " ...\n",
      " [ 0.33041977]\n",
      " [ 0.13321591]\n",
      " [-1.03132323]]\n",
      "Current iteration=8, loss=35574.34738122067\n",
      "t [[ 0.12431448]\n",
      " [-1.83257046]\n",
      " [-1.06115288]\n",
      " ...\n",
      " [ 0.31984671]\n",
      " [ 0.12431448]\n",
      " [-1.08990795]]\n",
      "t [[ 0.12431448]\n",
      " [-1.83257046]\n",
      " [-1.06115288]\n",
      " ...\n",
      " [ 0.31984671]\n",
      " [ 0.12431448]\n",
      " [-1.08990795]]\n",
      "t [[ 0.11449509]\n",
      " [-1.93634311]\n",
      " [-1.12813158]\n",
      " ...\n",
      " [ 0.30760238]\n",
      " [ 0.11449509]\n",
      " [-1.14380699]]\n",
      "loss=34335.83061733999\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.21405189]\n",
      " [-0.12478367]\n",
      " [-0.19970479]\n",
      " ...\n",
      " [ 0.16771655]\n",
      " [ 0.07060504]\n",
      " [-0.28416163]]\n",
      "t [[ 0.21405189]\n",
      " [-0.12478367]\n",
      " [-0.19970479]\n",
      " ...\n",
      " [ 0.16771655]\n",
      " [ 0.07060504]\n",
      " [-0.28416163]]\n",
      "t [[ 0.37345354]\n",
      " [-0.28982236]\n",
      " [-0.3629285 ]\n",
      " ...\n",
      " [ 0.25573172]\n",
      " [ 0.11091497]\n",
      " [-0.46917833]]\n",
      "t [[ 0.37345354]\n",
      " [-0.28982236]\n",
      " [-0.3629285 ]\n",
      " ...\n",
      " [ 0.25573172]\n",
      " [ 0.11091497]\n",
      " [-0.46917833]]\n",
      "Current iteration=2, loss=44047.522088323596\n",
      "t [[ 0.49489324]\n",
      " [-0.45953089]\n",
      " [-0.50128831]\n",
      " ...\n",
      " [ 0.30188746]\n",
      " [ 0.13279372]\n",
      " [-0.60318484]]\n",
      "t [[ 0.49489324]\n",
      " [-0.45953089]\n",
      " [-0.50128831]\n",
      " ...\n",
      " [ 0.30188746]\n",
      " [ 0.13279372]\n",
      " [-0.60318484]]\n",
      "t [[ 0.5894427 ]\n",
      " [-0.62060601]\n",
      " [-0.62161533]\n",
      " ...\n",
      " [ 0.3247314 ]\n",
      " [ 0.14308668]\n",
      " [-0.70896097]]\n",
      "t [[ 0.5894427 ]\n",
      " [-0.62060601]\n",
      " [-0.62161533]\n",
      " ...\n",
      " [ 0.3247314 ]\n",
      " [ 0.14308668]\n",
      " [-0.70896097]]\n",
      "Current iteration=4, loss=39870.56006893774\n",
      "t [[ 0.66439487]\n",
      " [-0.76916737]\n",
      " [-0.7281385 ]\n",
      " ...\n",
      " [ 0.33385837]\n",
      " [ 0.14586186]\n",
      " [-0.79754896]]\n",
      "t [[ 0.66439487]\n",
      " [-0.76916737]\n",
      " [-0.7281385 ]\n",
      " ...\n",
      " [ 0.33385837]\n",
      " [ 0.14586186]\n",
      " [-0.79754896]]\n",
      "t [[ 0.72472541]\n",
      " [-0.90491775]\n",
      " [-0.82365883]\n",
      " ...\n",
      " [ 0.33450805]\n",
      " [ 0.14365727]\n",
      " [-0.87461708]]\n",
      "t [[ 0.72472541]\n",
      " [-0.90491775]\n",
      " [-0.82365883]\n",
      " ...\n",
      " [ 0.33450805]\n",
      " [ 0.14365727]\n",
      " [-0.87461708]]\n",
      "Current iteration=6, loss=37295.54953749478\n",
      "t [[ 0.77393995]\n",
      " [-1.02879958]\n",
      " [-0.91014507]\n",
      " ...\n",
      " [ 0.32971093]\n",
      " [ 0.13812625]\n",
      " [-0.94329275]]\n",
      "t [[ 0.77393995]\n",
      " [-1.02879958]\n",
      " [-0.91014507]\n",
      " ...\n",
      " [ 0.32971093]\n",
      " [ 0.13812625]\n",
      " [-0.94329275]]\n",
      "t [[ 0.81457351]\n",
      " [-1.14208459]\n",
      " [-0.98904858]\n",
      " ...\n",
      " [ 0.32132238]\n",
      " [ 0.13038377]\n",
      " [-1.00544345]]\n",
      "t [[ 0.81457351]\n",
      " [-1.14208459]\n",
      " [-0.98904858]\n",
      " ...\n",
      " [ 0.32132238]\n",
      " [ 0.13038377]\n",
      " [-1.00544345]]\n",
      "Current iteration=8, loss=35562.06423341926\n",
      "t [[ 0.84849901]\n",
      " [-1.24603416]\n",
      " [-1.0614797 ]\n",
      " ...\n",
      " [ 0.31053669]\n",
      " [ 0.12120195]\n",
      " [-1.06227447]]\n",
      "t [[ 0.84849901]\n",
      " [-1.24603416]\n",
      " [-1.0614797 ]\n",
      " ...\n",
      " [ 0.31053669]\n",
      " [ 0.12120195]\n",
      " [-1.06227447]]\n",
      "t [[ 0.87712517]\n",
      " [-1.34178584]\n",
      " [-1.1283126 ]\n",
      " ...\n",
      " [ 0.29815461]\n",
      " [ 0.11112589]\n",
      " [-1.11462031]]\n",
      "loss=34323.43885463796\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.21130839]\n",
      " [-0.11945383]\n",
      " [-0.19895687]\n",
      " ...\n",
      " [ 0.1703042 ]\n",
      " [ 0.07190636]\n",
      " [-0.29291239]]\n",
      "t [[ 0.21130839]\n",
      " [-0.11945383]\n",
      " [-0.19895687]\n",
      " ...\n",
      " [ 0.1703042 ]\n",
      " [ 0.07190636]\n",
      " [-0.29291239]]\n",
      "t [[ 0.36868897]\n",
      " [-0.27934595]\n",
      " [-0.36143867]\n",
      " ...\n",
      " [ 0.25878201]\n",
      " [ 0.11315804]\n",
      " [-0.48372087]]\n",
      "t [[ 0.36868897]\n",
      " [-0.27934595]\n",
      " [-0.36143867]\n",
      " ...\n",
      " [ 0.25878201]\n",
      " [ 0.11315804]\n",
      " [-0.48372087]]\n",
      "Current iteration=2, loss=44118.3880930243\n",
      "t [[ 0.48864247]\n",
      " [-0.44394067]\n",
      " [-0.49912979]\n",
      " ...\n",
      " [ 0.30438988]\n",
      " [ 0.135777  ]\n",
      " [-0.62188403]]\n",
      "t [[ 0.48864247]\n",
      " [-0.44394067]\n",
      " [-0.49912979]\n",
      " ...\n",
      " [ 0.30438988]\n",
      " [ 0.135777  ]\n",
      " [-0.62188403]]\n",
      "t [[ 0.58210733]\n",
      " [-0.59998186]\n",
      " [-0.61886451]\n",
      " ...\n",
      " [ 0.32613219]\n",
      " [ 0.14668155]\n",
      " [-0.73080434]]\n",
      "t [[ 0.58210733]\n",
      " [-0.59998186]\n",
      " [-0.61886451]\n",
      " ...\n",
      " [ 0.32613219]\n",
      " [ 0.14668155]\n",
      " [-0.73080434]]\n",
      "Current iteration=4, loss=39995.01990683925\n",
      "t [[ 0.65627104]\n",
      " [-0.74365348]\n",
      " [-0.72485947]\n",
      " ...\n",
      " [ 0.33382931]\n",
      " [ 0.14997693]\n",
      " [-0.82185514]]\n",
      "t [[ 0.65627104]\n",
      " [-0.74365348]\n",
      " [-0.72485947]\n",
      " ...\n",
      " [ 0.33382931]\n",
      " [ 0.14997693]\n",
      " [-0.82185514]]\n",
      "t [[ 0.71602724]\n",
      " [-0.87470232]\n",
      " [-0.81990264]\n",
      " ...\n",
      " [ 0.33285656]\n",
      " [ 0.14822313]\n",
      " [-0.90090356]]\n",
      "t [[ 0.71602724]\n",
      " [-0.87470232]\n",
      " [-0.81990264]\n",
      " ...\n",
      " [ 0.33285656]\n",
      " [ 0.14822313]\n",
      " [-0.90090356]]\n",
      "Current iteration=6, loss=37456.60856337327\n",
      "t [[ 0.76482162]\n",
      " [-0.99409267]\n",
      " [-0.90595185]\n",
      " ...\n",
      " [ 0.32632828]\n",
      " [ 0.14308806]\n",
      " [-0.97120347]]\n",
      "t [[ 0.76482162]\n",
      " [-0.99409267]\n",
      " [-0.90595185]\n",
      " ...\n",
      " [ 0.32632828]\n",
      " [ 0.14308806]\n",
      " [-0.97120347]]\n",
      "t [[ 0.80514638]\n",
      " [-1.10310284]\n",
      " [-0.98444993]\n",
      " ...\n",
      " [ 0.31615254]\n",
      " [ 0.13569705]\n",
      " [-1.03470643]]\n",
      "t [[ 0.80514638]\n",
      " [-1.10310284]\n",
      " [-0.98444993]\n",
      " ...\n",
      " [ 0.31615254]\n",
      " [ 0.13569705]\n",
      " [-1.03470643]]\n",
      "Current iteration=8, loss=35749.14309832365\n",
      "t [[ 0.83884382]\n",
      " [-1.20299162]\n",
      " [-1.05650064]\n",
      " ...\n",
      " [ 0.30355736]\n",
      " [ 0.12682997]\n",
      " [-1.09267599]]\n",
      "t [[ 0.83884382]\n",
      " [-1.20299162]\n",
      " [-1.05650064]\n",
      " ...\n",
      " [ 0.30355736]\n",
      " [ 0.12682997]\n",
      " [-1.09267599]]\n",
      "t [[ 0.86730063]\n",
      " [-1.29488894]\n",
      " [-1.12297312]\n",
      " ...\n",
      " [ 0.28936545]\n",
      " [ 0.11703791]\n",
      " [-1.1459885 ]]\n",
      "loss=34529.78226734584\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.21097014]\n",
      " [-0.12549818]\n",
      " [-0.19833193]\n",
      " ...\n",
      " [ 0.30386335]\n",
      " [-0.19833193]\n",
      " [ 0.28461107]]\n",
      "t [[ 0.21097014]\n",
      " [-0.12549818]\n",
      " [-0.19833193]\n",
      " ...\n",
      " [ 0.30386335]\n",
      " [-0.19833193]\n",
      " [ 0.28461107]]\n",
      "t [[ 0.36807456]\n",
      " [-0.28925828]\n",
      " [-0.36050105]\n",
      " ...\n",
      " [ 0.52052575]\n",
      " [-0.36050105]\n",
      " [ 0.47345033]]\n",
      "t [[ 0.36807456]\n",
      " [-0.28925828]\n",
      " [-0.36050105]\n",
      " ...\n",
      " [ 0.52052575]\n",
      " [-0.36050105]\n",
      " [ 0.47345033]]\n",
      "Current iteration=2, loss=44120.544769936285\n",
      "t [[ 0.48777102]\n",
      " [-0.45680422]\n",
      " [-0.49799513]\n",
      " ...\n",
      " [ 0.68074158]\n",
      " [-0.49799513]\n",
      " [ 0.60473655]]\n",
      "t [[ 0.48777102]\n",
      " [-0.45680422]\n",
      " [-0.49799513]\n",
      " ...\n",
      " [ 0.68074158]\n",
      " [-0.49799513]\n",
      " [ 0.60473655]]\n",
      "t [[ 0.58087284]\n",
      " [-0.6154614 ]\n",
      " [-0.61758084]\n",
      " ...\n",
      " [ 0.80282221]\n",
      " [-0.61758084]\n",
      " [ 0.69962211]]\n",
      "t [[ 0.58087284]\n",
      " [-0.6154614 ]\n",
      " [-0.61758084]\n",
      " ...\n",
      " [ 0.80282221]\n",
      " [-0.61758084]\n",
      " [ 0.69962211]]\n",
      "Current iteration=4, loss=39985.55816793359\n",
      "t [[ 0.65456811]\n",
      " [-0.7616021 ]\n",
      " [-0.72344706]\n",
      " ...\n",
      " [ 0.89803909]\n",
      " [-0.72344706]\n",
      " [ 0.7702512 ]]\n",
      "t [[ 0.65456811]\n",
      " [-0.7616021 ]\n",
      " [-0.72344706]\n",
      " ...\n",
      " [ 0.89803909]\n",
      " [-0.72344706]\n",
      " [ 0.7702512 ]]\n",
      "t [[ 0.71378267]\n",
      " [-0.89502183]\n",
      " [-0.8183709 ]\n",
      " ...\n",
      " [ 0.97369267]\n",
      " [-0.8183709 ]\n",
      " [ 0.82402733]]\n",
      "t [[ 0.71378267]\n",
      " [-0.89502183]\n",
      " [-0.8183709 ]\n",
      " ...\n",
      " [ 0.97369267]\n",
      " [-0.8183709 ]\n",
      " [ 0.82402733]]\n",
      "Current iteration=6, loss=37438.247834137466\n",
      "t [[ 0.76199063]\n",
      " [-1.01669218]\n",
      " [-0.90430655]\n",
      " ...\n",
      " [ 1.03472843]\n",
      " [-0.90430655]\n",
      " [ 0.86571819]]\n",
      "t [[ 0.76199063]\n",
      " [-1.01669218]\n",
      " [-0.90430655]\n",
      " ...\n",
      " [ 1.03472843]\n",
      " [-0.90430655]\n",
      " [ 0.86571819]]\n",
      "t [[ 0.80170431]\n",
      " [-1.12788875]\n",
      " [-0.98269592]\n",
      " ...\n",
      " [ 1.08462179]\n",
      " [-0.98269592]\n",
      " [ 0.89853651]]\n",
      "t [[ 0.80170431]\n",
      " [-1.12788875]\n",
      " [-0.98269592]\n",
      " ...\n",
      " [ 1.08462179]\n",
      " [-0.98269592]\n",
      " [ 0.89853651]]\n",
      "Current iteration=8, loss=35724.30805081638\n",
      "t [[ 0.83477951]\n",
      " [-1.2298673 ]\n",
      " [-1.0546427 ]\n",
      " ...\n",
      " [ 1.12588778]\n",
      " [-1.0546427 ]\n",
      " [ 0.92472681]]\n",
      "t [[ 0.83477951]\n",
      " [-1.2298673 ]\n",
      " [-1.0546427 ]\n",
      " ...\n",
      " [ 1.12588778]\n",
      " [-1.0546427 ]\n",
      " [ 0.92472681]]\n",
      "t [[ 0.86261177]\n",
      " [-1.32375687]\n",
      " [-1.12101614]\n",
      " ...\n",
      " [ 1.16038959]\n",
      " [-1.12101614]\n",
      " [ 0.94590221]]\n",
      "loss=34500.236786639485\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07274869]\n",
      " [-0.34835694]\n",
      " [-0.20428051]\n",
      " ...\n",
      " [ 0.17468696]\n",
      " [ 0.07274869]\n",
      " [-0.29611468]]\n",
      "t [[ 0.07274869]\n",
      " [-0.34835694]\n",
      " [-0.20428051]\n",
      " ...\n",
      " [ 0.17468696]\n",
      " [ 0.07274869]\n",
      " [-0.29611468]]\n",
      "t [[ 0.1137077 ]\n",
      " [-0.64623589]\n",
      " [-0.37055547]\n",
      " ...\n",
      " [ 0.26486691]\n",
      " [ 0.1137077 ]\n",
      " [-0.48779256]]\n",
      "t [[ 0.1137077 ]\n",
      " [-0.64623589]\n",
      " [-0.37055547]\n",
      " ...\n",
      " [ 0.26486691]\n",
      " [ 0.1137077 ]\n",
      " [-0.48779256]]\n",
      "Current iteration=2, loss=43905.21646735724\n",
      "t [[ 0.13561758]\n",
      " [-0.90024024]\n",
      " [-0.51118905]\n",
      " ...\n",
      " [ 0.31151484]\n",
      " [ 0.13561758]\n",
      " [-0.62665712]]\n",
      "t [[ 0.13561758]\n",
      " [-0.90024024]\n",
      " [-0.51118905]\n",
      " ...\n",
      " [ 0.31151484]\n",
      " [ 0.13561758]\n",
      " [-0.62665712]]\n",
      "t [[ 0.14568699]\n",
      " [-1.11805046]\n",
      " [-0.63331392]\n",
      " ...\n",
      " [ 0.33422387]\n",
      " [ 0.14568699]\n",
      " [-0.73650314]]\n",
      "t [[ 0.14568699]\n",
      " [-1.11805046]\n",
      " [-0.63331392]\n",
      " ...\n",
      " [ 0.33422387]\n",
      " [ 0.14568699]\n",
      " [-0.73650314]]\n",
      "Current iteration=4, loss=39705.71629948238\n",
      "t [[ 0.14814168]\n",
      " [-1.30645096]\n",
      " [-0.74129901]\n",
      " ...\n",
      " [ 0.34295876]\n",
      " [ 0.14814168]\n",
      " [-0.8286527 ]]\n",
      "t [[ 0.14814168]\n",
      " [-1.30645096]\n",
      " [-0.74129901]\n",
      " ...\n",
      " [ 0.34295876]\n",
      " [ 0.14814168]\n",
      " [-0.8286527 ]]\n",
      "t [[ 0.14559327]\n",
      " [-1.47094819]\n",
      " [-0.8380242 ]\n",
      " ...\n",
      " [ 0.34310033]\n",
      " [ 0.14559327]\n",
      " [-0.90886111]]\n",
      "t [[ 0.14559327]\n",
      " [-1.47094819]\n",
      " [-0.8380242 ]\n",
      " ...\n",
      " [ 0.34310033]\n",
      " [ 0.14559327]\n",
      " [-0.90886111]]\n",
      "Current iteration=6, loss=37136.696196123565\n",
      "t [[ 0.13973129]\n",
      " [-1.6158789 ]\n",
      " [-0.92551036]\n",
      " ...\n",
      " [ 0.33774107]\n",
      " [ 0.13973129]\n",
      " [-0.9803007 ]]\n",
      "t [[ 0.13973129]\n",
      " [-1.6158789 ]\n",
      " [-0.92551036]\n",
      " ...\n",
      " [ 0.33774107]\n",
      " [ 0.13973129]\n",
      " [-0.9803007 ]]\n",
      "t [[ 0.13168875]\n",
      " [-1.74463515]\n",
      " [-1.0052469 ]\n",
      " ...\n",
      " [ 0.32876869]\n",
      " [ 0.13168875]\n",
      " [-1.04487742]]\n",
      "t [[ 0.13168875]\n",
      " [-1.74463515]\n",
      " [-1.0052469 ]\n",
      " ...\n",
      " [ 0.32876869]\n",
      " [ 0.13168875]\n",
      " [-1.04487742]]\n",
      "Current iteration=8, loss=35416.47293696778\n",
      "t [[ 0.12224619]\n",
      " [-1.85988007]\n",
      " [-1.07837333]\n",
      " ...\n",
      " [ 0.31739658]\n",
      " [ 0.12224619]\n",
      " [-1.10383405]]\n",
      "t [[ 0.12224619]\n",
      " [-1.85988007]\n",
      " [-1.07837333]\n",
      " ...\n",
      " [ 0.31739658]\n",
      " [ 0.12224619]\n",
      " [-1.10383405]]\n",
      "t [[ 0.11195181]\n",
      " [-1.96372203]\n",
      " [-1.14578685]\n",
      " ...\n",
      " [ 0.30443743]\n",
      " [ 0.11195181]\n",
      " [-1.15804073]]\n",
      "loss=34192.32475298665\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.21968484]\n",
      " [-0.12806745]\n",
      " [-0.20496018]\n",
      " ...\n",
      " [ 0.17213014]\n",
      " [ 0.07246307]\n",
      " [-0.29163957]]\n",
      "t [[ 0.21968484]\n",
      " [-0.12806745]\n",
      " [-0.20496018]\n",
      " ...\n",
      " [ 0.17213014]\n",
      " [ 0.07246307]\n",
      " [-0.29163957]]\n",
      "t [[ 0.38183782]\n",
      " [-0.29850856]\n",
      " [-0.37151269]\n",
      " ...\n",
      " [ 0.26035263]\n",
      " [ 0.11303328]\n",
      " [-0.478906  ]]\n",
      "t [[ 0.38183782]\n",
      " [-0.29850856]\n",
      " [-0.37151269]\n",
      " ...\n",
      " [ 0.26035263]\n",
      " [ 0.11303328]\n",
      " [-0.478906  ]]\n",
      "Current iteration=2, loss=43892.48833007799\n",
      "t [[ 0.50449743]\n",
      " [-0.47291871]\n",
      " [-0.51221161]\n",
      " ...\n",
      " [ 0.30552948]\n",
      " [ 0.13452752]\n",
      " [-0.61377768]]\n",
      "t [[ 0.50449743]\n",
      " [-0.47291871]\n",
      " [-0.51221161]\n",
      " ...\n",
      " [ 0.30552948]\n",
      " [ 0.13452752]\n",
      " [-0.61377768]]\n",
      "t [[ 0.59943556]\n",
      " [-0.63756286]\n",
      " [-0.63429105]\n",
      " ...\n",
      " [ 0.32715001]\n",
      " [ 0.14418752]\n",
      " [-0.7201271 ]]\n",
      "t [[ 0.59943556]\n",
      " [-0.63756286]\n",
      " [-0.63429105]\n",
      " ...\n",
      " [ 0.32715001]\n",
      " [ 0.14418752]\n",
      " [-0.7201271 ]]\n",
      "Current iteration=4, loss=39692.75428915722\n",
      "t [[ 0.67431571]\n",
      " [-0.78873774]\n",
      " [-0.74217306]\n",
      " ...\n",
      " [ 0.33508678]\n",
      " [ 0.14625302]\n",
      " [-0.80924695]]\n",
      "t [[ 0.67431571]\n",
      " [-0.78873774]\n",
      " [-0.74217306]\n",
      " ...\n",
      " [ 0.33508678]\n",
      " [ 0.14625302]\n",
      " [-0.80924695]]\n",
      "t [[ 0.73432264]\n",
      " [-0.92639476]\n",
      " [-0.83876584]\n",
      " ...\n",
      " [ 0.33464761]\n",
      " [ 0.14334124]\n",
      " [-0.88683204]]\n",
      "t [[ 0.73432264]\n",
      " [-0.92639476]\n",
      " [-0.83876584]\n",
      " ...\n",
      " [ 0.33464761]\n",
      " [ 0.14334124]\n",
      " [-0.88683204]]\n",
      "Current iteration=6, loss=37124.23043358894\n",
      "t [[ 0.78308462]\n",
      " [-1.05167698]\n",
      " [-0.92610577]\n",
      " ...\n",
      " [ 0.32887088]\n",
      " [ 0.13714334]\n",
      " [-0.95599195]]\n",
      "t [[ 0.78308462]\n",
      " [-1.05167698]\n",
      " [-0.92610577]\n",
      " ...\n",
      " [ 0.32887088]\n",
      " [ 0.13714334]\n",
      " [-0.95599195]]\n",
      "t [[ 0.82321019]\n",
      " [-1.16599927]\n",
      " [-1.00569085]\n",
      " ...\n",
      " [ 0.31960495]\n",
      " [ 0.12879214]\n",
      " [-1.01857702]]\n",
      "t [[ 0.82321019]\n",
      " [-1.16599927]\n",
      " [-1.00569085]\n",
      " ...\n",
      " [ 0.31960495]\n",
      " [ 0.12879214]\n",
      " [-1.01857702]]\n",
      "Current iteration=8, loss=35404.199815307176\n",
      "t [[ 0.85661676]\n",
      " [-1.27072178]\n",
      " [-1.07866552]\n",
      " ...\n",
      " [ 0.30803466]\n",
      " [ 0.11906711]\n",
      " [-1.0757835 ]]\n",
      "t [[ 0.85661676]\n",
      " [-1.27072178]\n",
      " [-1.07866552]\n",
      " ...\n",
      " [ 0.30803466]\n",
      " [ 0.11906711]\n",
      " [-1.0757835 ]]\n",
      "t [[ 0.88473975]\n",
      " [-1.36705025]\n",
      " [-1.14592976]\n",
      " ...\n",
      " [ 0.29495188]\n",
      " [ 0.10851496]\n",
      " [-1.12844351]]\n",
      "loss=34179.90490484054\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.21686914]\n",
      " [-0.12259736]\n",
      " [-0.20419257]\n",
      " ...\n",
      " [ 0.17478589]\n",
      " [ 0.07379863]\n",
      " [-0.30062061]]\n",
      "t [[ 0.21686914]\n",
      " [-0.12259736]\n",
      " [-0.20419257]\n",
      " ...\n",
      " [ 0.17478589]\n",
      " [ 0.07379863]\n",
      " [-0.30062061]]\n",
      "t [[ 0.37696681]\n",
      " [-0.28776126]\n",
      " [-0.36998412]\n",
      " ...\n",
      " [ 0.26342975]\n",
      " [ 0.11532618]\n",
      " [-0.49375517]]\n",
      "t [[ 0.37696681]\n",
      " [-0.28776126]\n",
      " [-0.36998412]\n",
      " ...\n",
      " [ 0.26342975]\n",
      " [ 0.11532618]\n",
      " [-0.49375517]]\n",
      "Current iteration=2, loss=43965.12101522318\n",
      "t [[ 0.4981291 ]\n",
      " [-0.45692483]\n",
      " [-0.51000073]\n",
      " ...\n",
      " [ 0.30799236]\n",
      " [ 0.13756975]\n",
      " [-0.63280899]]\n",
      "t [[ 0.4981291 ]\n",
      " [-0.45692483]\n",
      " [-0.51000073]\n",
      " ...\n",
      " [ 0.30799236]\n",
      " [ 0.13756975]\n",
      " [-0.63280899]]\n",
      "t [[ 0.59198532]\n",
      " [-0.61640861]\n",
      " [-0.63147812]\n",
      " ...\n",
      " [ 0.32843784]\n",
      " [ 0.14784729]\n",
      " [-0.74230649]]\n",
      "t [[ 0.59198532]\n",
      " [-0.61640861]\n",
      " [-0.63147812]\n",
      " ...\n",
      " [ 0.32843784]\n",
      " [ 0.14784729]\n",
      " [-0.74230649]]\n",
      "Current iteration=4, loss=39819.625876920494\n",
      "t [[ 0.66608646]\n",
      " [-0.76257966]\n",
      " [-0.73882448]\n",
      " ...\n",
      " [ 0.33487216]\n",
      " [ 0.15043692]\n",
      " [-0.83388308]]\n",
      "t [[ 0.66608646]\n",
      " [-0.76257966]\n",
      " [-0.73882448]\n",
      " ...\n",
      " [ 0.33487216]\n",
      " [ 0.15043692]\n",
      " [-0.83388308]]\n",
      "t [[ 0.72553106]\n",
      " [-0.89543522]\n",
      " [-0.83493395]\n",
      " ...\n",
      " [ 0.33274217]\n",
      " [ 0.14797847]\n",
      " [-0.9134376 ]]\n",
      "t [[ 0.72553106]\n",
      " [-0.89543522]\n",
      " [-0.83493395]\n",
      " ...\n",
      " [ 0.33274217]\n",
      " [ 0.14797847]\n",
      " [-0.9134376 ]]\n",
      "Current iteration=6, loss=37287.79867884629\n",
      "t [[ 0.77388525]\n",
      " [-1.0161398 ]\n",
      " [-0.92183138]\n",
      " ...\n",
      " [ 0.32517117]\n",
      " [ 0.14217811]\n",
      " [-0.98420844]]\n",
      "t [[ 0.77388525]\n",
      " [-1.0161398 ]\n",
      " [-0.92183138]\n",
      " ...\n",
      " [ 0.32517117]\n",
      " [ 0.14217811]\n",
      " [-0.98420844]]\n",
      "t [[ 0.81371368]\n",
      " [-1.1261135 ]\n",
      " [-1.00100589]\n",
      " ...\n",
      " [ 0.31406027]\n",
      " [ 0.13417928]\n",
      " [-1.04813116]]\n",
      "t [[ 0.81371368]\n",
      " [-1.1261135 ]\n",
      " [-1.00100589]\n",
      " ...\n",
      " [ 0.31406027]\n",
      " [ 0.13417928]\n",
      " [-1.04813116]]\n",
      "Current iteration=8, loss=35593.70211941295\n",
      "t [[ 0.84690265]\n",
      " [-1.22671235]\n",
      " [-1.07359513]\n",
      " ...\n",
      " [ 0.30062776]\n",
      " [ 0.12476939]\n",
      " [-1.10646087]]\n",
      "t [[ 0.84690265]\n",
      " [-1.22671235]\n",
      " [-1.07359513]\n",
      " ...\n",
      " [ 0.30062776]\n",
      " [ 0.12476939]\n",
      " [-1.10646087]]\n",
      "t [[ 0.87486535]\n",
      " [-1.31913302]\n",
      " [-1.14049394]\n",
      " ...\n",
      " [ 0.28568707]\n",
      " [ 0.11450133]\n",
      " [-1.16007186]]\n",
      "loss=34388.52354051628\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.21652199]\n",
      " [-0.12880076]\n",
      " [-0.20355119]\n",
      " ...\n",
      " [ 0.31185975]\n",
      " [-0.20355119]\n",
      " [ 0.29210084]]\n",
      "t [[ 0.21652199]\n",
      " [-0.12880076]\n",
      " [-0.20355119]\n",
      " ...\n",
      " [ 0.31185975]\n",
      " [-0.20355119]\n",
      " [ 0.29210084]]\n",
      "t [[ 0.37633594]\n",
      " [-0.29787899]\n",
      " [-0.36902979]\n",
      " ...\n",
      " [ 0.53191758]\n",
      " [-0.36902979]\n",
      " [ 0.48337851]]\n",
      "t [[ 0.37633594]\n",
      " [-0.29787899]\n",
      " [-0.36902979]\n",
      " ...\n",
      " [ 0.53191758]\n",
      " [-0.36902979]\n",
      " [ 0.48337851]]\n",
      "Current iteration=2, loss=43967.00426888201\n",
      "t [[ 0.49722936]\n",
      " [-0.47002951]\n",
      " [-0.50885011]\n",
      " ...\n",
      " [ 0.69340342]\n",
      " [-0.50885011]\n",
      " [ 0.61511899]]\n",
      "t [[ 0.49722936]\n",
      " [-0.47002951]\n",
      " [-0.50885011]\n",
      " ...\n",
      " [ 0.69340342]\n",
      " [-0.50885011]\n",
      " [ 0.61511899]]\n",
      "t [[ 0.59070278]\n",
      " [-0.63217471]\n",
      " [-0.63017818]\n",
      " ...\n",
      " [ 0.81571624]\n",
      " [-0.63017818]\n",
      " [ 0.70965852]]\n",
      "t [[ 0.59070278]\n",
      " [-0.63217471]\n",
      " [-0.63017818]\n",
      " ...\n",
      " [ 0.81571624]\n",
      " [-0.63017818]\n",
      " [ 0.70965852]]\n",
      "Current iteration=4, loss=39809.61267147521\n",
      "t [[ 0.664313  ]\n",
      " [-0.78086515]\n",
      " [-0.73739441]\n",
      " ...\n",
      " [ 0.91063678]\n",
      " [-0.73739441]\n",
      " [ 0.77961767]]\n",
      "t [[ 0.664313  ]\n",
      " [-0.78086515]\n",
      " [-0.73739441]\n",
      " ...\n",
      " [ 0.91063678]\n",
      " [-0.73739441]\n",
      " [ 0.77961767]]\n",
      "t [[ 0.72319374]\n",
      " [-0.91614114]\n",
      " [-0.83338262]\n",
      " ...\n",
      " [ 0.98572593]\n",
      " [-0.83338262]\n",
      " [ 0.83260936]]\n",
      "t [[ 0.72319374]\n",
      " [-0.91614114]\n",
      " [-0.83338262]\n",
      " ...\n",
      " [ 0.98572593]\n",
      " [-0.83338262]\n",
      " [ 0.83260936]]\n",
      "Current iteration=6, loss=37268.82530432234\n",
      "t [[ 0.77094067]\n",
      " [-1.03917118]\n",
      " [-0.92016446]\n",
      " ...\n",
      " [ 1.04607215]\n",
      " [-0.92016446]\n",
      " [ 0.87350121]]\n",
      "t [[ 0.77094067]\n",
      " [-1.03917118]\n",
      " [-0.92016446]\n",
      " ...\n",
      " [ 1.04607215]\n",
      " [-0.92016446]\n",
      " [ 0.87350121]]\n",
      "t [[ 0.81013887]\n",
      " [-1.15137128]\n",
      " [-0.99922838]\n",
      " ...\n",
      " [ 1.09523302]\n",
      " [-0.99922838]\n",
      " [ 0.90555623]]\n",
      "t [[ 0.81013887]\n",
      " [-1.15137128]\n",
      " [-0.99922838]\n",
      " ...\n",
      " [ 1.09523302]\n",
      " [-0.99922838]\n",
      " [ 0.90555623]]\n",
      "Current iteration=8, loss=35568.26825638566\n",
      "t [[ 0.84268812]\n",
      " [-1.2540942 ]\n",
      " [-1.07171208]\n",
      " ...\n",
      " [ 1.13577189]\n",
      " [-1.07171208]\n",
      " [ 0.93104445]]\n",
      "t [[ 0.84268812]\n",
      " [-1.2540942 ]\n",
      " [-1.07171208]\n",
      " ...\n",
      " [ 1.13577189]\n",
      " [-1.07171208]\n",
      " [ 0.93104445]]\n",
      "t [[ 0.87001033]\n",
      " [-1.34853607]\n",
      " [-1.1385105 ]\n",
      " ...\n",
      " [ 1.1695801 ]\n",
      " [-1.1385105 ]\n",
      " [ 0.95159095]]\n",
      "loss=34358.433293611604\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07461404]\n",
      " [-0.35728917]\n",
      " [-0.20951847]\n",
      " ...\n",
      " [ 0.17916611]\n",
      " [ 0.07461404]\n",
      " [-0.30370736]]\n",
      "t [[ 0.07461404]\n",
      " [-0.35728917]\n",
      " [-0.20951847]\n",
      " ...\n",
      " [ 0.17916611]\n",
      " [ 0.07461404]\n",
      " [-0.30370736]]\n",
      "t [[ 0.11580618]\n",
      " [-0.66149784]\n",
      " [-0.3790772 ]\n",
      " ...\n",
      " [ 0.26948366]\n",
      " [ 0.11580618]\n",
      " [-0.49761633]]\n",
      "t [[ 0.11580618]\n",
      " [-0.66149784]\n",
      " [-0.3790772 ]\n",
      " ...\n",
      " [ 0.26948366]\n",
      " [ 0.11580618]\n",
      " [-0.49761633]]\n",
      "Current iteration=2, loss=43752.388033628726\n",
      "t [[ 0.13731229]\n",
      " [-0.91977147]\n",
      " [-0.52201025]\n",
      " ...\n",
      " [ 0.31510747]\n",
      " [ 0.13731229]\n",
      " [-0.63735943]]\n",
      "t [[ 0.13731229]\n",
      " [-0.91977147]\n",
      " [-0.52201025]\n",
      " ...\n",
      " [ 0.31510747]\n",
      " [ 0.13731229]\n",
      " [-0.63735943]]\n",
      "t [[ 0.14674076]\n",
      " [-1.14040296]\n",
      " [-0.64585374]\n",
      " ...\n",
      " [ 0.33657434]\n",
      " [ 0.14674076]\n",
      " [-0.74780895]]\n",
      "t [[ 0.14674076]\n",
      " [-1.14040296]\n",
      " [-0.64585374]\n",
      " ...\n",
      " [ 0.33657434]\n",
      " [ 0.14674076]\n",
      " [-0.74780895]]\n",
      "Current iteration=4, loss=39532.08211669788\n",
      "t [[ 0.14848631]\n",
      " [-1.33064229]\n",
      " [-0.75516719]\n",
      " ...\n",
      " [ 0.34411364]\n",
      " [ 0.14848631]\n",
      " [-0.84051628]]\n",
      "t [[ 0.14848631]\n",
      " [-1.33064229]\n",
      " [-0.75516719]\n",
      " ...\n",
      " [ 0.34411364]\n",
      " [ 0.14848631]\n",
      " [-0.84051628]]\n",
      "t [[ 0.14523707]\n",
      " [-1.49631433]\n",
      " [-0.85293631]\n",
      " ...\n",
      " [ 0.34316634]\n",
      " [ 0.14523707]\n",
      " [-0.92125523]]\n",
      "t [[ 0.14523707]\n",
      " [-1.49631433]\n",
      " [-0.85293631]\n",
      " ...\n",
      " [ 0.34316634]\n",
      " [ 0.14523707]\n",
      " [-0.92125523]]\n",
      "Current iteration=6, loss=36970.4606278001\n",
      "t [[ 0.13871859]\n",
      " [-1.64196664]\n",
      " [-0.94124919]\n",
      " ...\n",
      " [ 0.3368296 ]\n",
      " [ 0.13871859]\n",
      " [-0.99318002]]\n",
      "t [[ 0.13871859]\n",
      " [-1.64196664]\n",
      " [-0.94124919]\n",
      " ...\n",
      " [ 0.3368296 ]\n",
      " [ 0.13871859]\n",
      " [-0.99318002]]\n",
      "t [[ 0.13008038]\n",
      " [-1.77113081]\n",
      " [-1.02164182]\n",
      " ...\n",
      " [ 0.32698306]\n",
      " [ 0.13008038]\n",
      " [-1.05818193]]\n",
      "t [[ 0.13008038]\n",
      " [-1.77113081]\n",
      " [-1.02164182]\n",
      " ...\n",
      " [ 0.32698306]\n",
      " [ 0.13008038]\n",
      " [-1.05818193]]\n",
      "Current iteration=8, loss=35264.00639374067\n",
      "t [[ 0.12010942]\n",
      " [-1.8865635 ]\n",
      " [-1.09528787]\n",
      " ...\n",
      " [ 0.31483044]\n",
      " [ 0.12010942]\n",
      " [-1.11749757]]\n",
      "t [[ 0.12010942]\n",
      " [-1.8865635 ]\n",
      " [-1.09528787]\n",
      " ...\n",
      " [ 0.31483044]\n",
      " [ 0.12010942]\n",
      " [-1.11749757]]\n",
      "t [[ 0.10935484]\n",
      " [-1.99043675]\n",
      " [-1.16311037]\n",
      " ...\n",
      " [ 0.3011754 ]\n",
      " [ 0.10935484]\n",
      " [-1.17199715]]\n",
      "loss=34054.21258921692\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.22531778]\n",
      " [-0.13135123]\n",
      " [-0.21021557]\n",
      " ...\n",
      " [ 0.17654374]\n",
      " [ 0.0743211 ]\n",
      " [-0.2991175 ]]\n",
      "t [[ 0.22531778]\n",
      " [-0.13135123]\n",
      " [-0.21021557]\n",
      " ...\n",
      " [ 0.17654374]\n",
      " [ 0.0743211 ]\n",
      " [-0.2991175 ]]\n",
      "t [[ 0.39015057]\n",
      " [-0.30724695]\n",
      " [-0.38004877]\n",
      " ...\n",
      " [ 0.26486886]\n",
      " [ 0.11511191]\n",
      " [-0.48850392]]\n",
      "t [[ 0.39015057]\n",
      " [-0.30724695]\n",
      " [-0.38004877]\n",
      " ...\n",
      " [ 0.26486886]\n",
      " [ 0.11511191]\n",
      " [-0.48850392]]\n",
      "Current iteration=2, loss=43739.58170587395\n",
      "t [[ 0.51395567]\n",
      " [-0.48632396]\n",
      " [-0.5230382 ]\n",
      " ...\n",
      " [ 0.30900872]\n",
      " [ 0.13619035]\n",
      " [-0.62417341]]\n",
      "t [[ 0.51395567]\n",
      " [-0.48632396]\n",
      " [-0.5230382 ]\n",
      " ...\n",
      " [ 0.30900872]\n",
      " [ 0.13619035]\n",
      " [-0.62417341]]\n",
      "t [[ 0.60922137]\n",
      " [-0.65445451]\n",
      " [-0.64682672]\n",
      " ...\n",
      " [ 0.32938831]\n",
      " [ 0.14519931]\n",
      " [-0.73107447]]\n",
      "t [[ 0.60922137]\n",
      " [-0.65445451]\n",
      " [-0.64682672]\n",
      " ...\n",
      " [ 0.32938831]\n",
      " [ 0.14519931]\n",
      " [-0.73107447]]\n",
      "Current iteration=4, loss=39519.1456317745\n",
      "t [[ 0.68398395]\n",
      " [-0.80814922]\n",
      " [-0.75602855]\n",
      " ...\n",
      " [ 0.33613838]\n",
      " [ 0.14654767]\n",
      " [-0.82072169]]\n",
      "t [[ 0.68398395]\n",
      " [-0.80814922]\n",
      " [-0.75602855]\n",
      " ...\n",
      " [ 0.33613838]\n",
      " [ 0.14654767]\n",
      " [-0.82072169]]\n",
      "t [[ 0.7436358 ]\n",
      " [-0.94762534]\n",
      " [-0.85365809]\n",
      " ...\n",
      " [ 0.33462296]\n",
      " [ 0.14292897]\n",
      " [-0.89882155]]\n",
      "t [[ 0.7436358 ]\n",
      " [-0.94762534]\n",
      " [-0.85365809]\n",
      " ...\n",
      " [ 0.33462296]\n",
      " [ 0.14292897]\n",
      " [-0.89882155]]\n",
      "Current iteration=6, loss=36958.029372447796\n",
      "t [[ 0.79192551]\n",
      " [-1.07423198]\n",
      " [-0.94181879]\n",
      " ...\n",
      " [ 0.32788294]\n",
      " [ 0.13607015]\n",
      " [-0.96846058]]\n",
      "t [[ 0.79192551]\n",
      " [-1.07423198]\n",
      " [-0.94181879]\n",
      " ...\n",
      " [ 0.32788294]\n",
      " [ 0.13607015]\n",
      " [-0.96846058]]\n",
      "t [[ 0.83153287]\n",
      " [-1.18952744]\n",
      " [-1.02205508]\n",
      " ...\n",
      " [ 0.31775746]\n",
      " [ 0.12712026]\n",
      " [-1.03147111]]\n",
      "t [[ 0.83153287]\n",
      " [-1.18952744]\n",
      " [-1.02205508]\n",
      " ...\n",
      " [ 0.31775746]\n",
      " [ 0.12712026]\n",
      " [-1.03147111]]\n",
      "Current iteration=8, loss=35251.73963912837\n",
      "t [[ 0.86441778]\n",
      " [-1.29496918]\n",
      " [-1.09554534]\n",
      " ...\n",
      " [ 0.30542112]\n",
      " [ 0.11686497]\n",
      " [-1.08904107]]\n",
      "t [[ 0.86441778]\n",
      " [-1.29496918]\n",
      " [-1.09554534]\n",
      " ...\n",
      " [ 0.30542112]\n",
      " [ 0.11686497]\n",
      " [-1.08904107]]\n",
      "t [[ 0.89204063]\n",
      " [-1.39182938]\n",
      " [-1.16321526]\n",
      " ...\n",
      " [ 0.2916565 ]\n",
      " [ 0.10585177]\n",
      " [-1.14200118]]\n",
      "loss=34041.76037762391\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.22242989]\n",
      " [-0.12574088]\n",
      " [-0.20942828]\n",
      " ...\n",
      " [ 0.17926757]\n",
      " [ 0.07569091]\n",
      " [-0.30832883]]\n",
      "t [[ 0.22242989]\n",
      " [-0.12574088]\n",
      " [-0.20942828]\n",
      " ...\n",
      " [ 0.17926757]\n",
      " [ 0.07569091]\n",
      " [-0.30832883]]\n",
      "t [[ 0.38517406]\n",
      " [-0.29622897]\n",
      " [-0.37848152]\n",
      " ...\n",
      " [ 0.26797034]\n",
      " [ 0.11745421]\n",
      " [-0.5036561 ]]\n",
      "t [[ 0.38517406]\n",
      " [-0.29622897]\n",
      " [-0.37848152]\n",
      " ...\n",
      " [ 0.26797034]\n",
      " [ 0.11745421]\n",
      " [-0.5036561 ]]\n",
      "Current iteration=2, loss=43813.97064062707\n",
      "t [[ 0.50747187]\n",
      " [-0.46992654]\n",
      " [-0.52077526]\n",
      " ...\n",
      " [ 0.31142821]\n",
      " [ 0.13929079]\n",
      " [-0.64353074]]\n",
      "t [[ 0.50747187]\n",
      " [-0.46992654]\n",
      " [-0.52077526]\n",
      " ...\n",
      " [ 0.31142821]\n",
      " [ 0.13929079]\n",
      " [-0.64353074]]\n",
      "t [[ 0.60165931]\n",
      " [-0.63277073]\n",
      " [-0.64395225]\n",
      " ...\n",
      " [ 0.33055875]\n",
      " [ 0.14892297]\n",
      " [-0.75358219]]\n",
      "t [[ 0.60165931]\n",
      " [-0.63277073]\n",
      " [-0.64395225]\n",
      " ...\n",
      " [ 0.33055875]\n",
      " [ 0.14892297]\n",
      " [-0.75358219]]\n",
      "Current iteration=4, loss=39648.38580141174\n",
      "t [[ 0.67565291]\n",
      " [-0.78134869]\n",
      " [-0.7526112 ]\n",
      " ...\n",
      " [ 0.33573382]\n",
      " [ 0.15079915]\n",
      " [-0.84567907]]\n",
      "t [[ 0.67565291]\n",
      " [-0.78134869]\n",
      " [-0.7526112 ]\n",
      " ...\n",
      " [ 0.33573382]\n",
      " [ 0.15079915]\n",
      " [-0.84567907]]\n",
      "t [[ 0.7347547 ]\n",
      " [-0.91592511]\n",
      " [-0.84975141]\n",
      " ...\n",
      " [ 0.3324597 ]\n",
      " [ 0.14763617]\n",
      " [-0.92573685]]\n",
      "t [[ 0.7347547 ]\n",
      " [-0.91592511]\n",
      " [-0.84975141]\n",
      " ...\n",
      " [ 0.3324597 ]\n",
      " [ 0.14763617]\n",
      " [-0.92573685]]\n",
      "Current iteration=6, loss=37124.04426521714\n",
      "t [[ 0.78264904]\n",
      " [-1.03786991]\n",
      " [-0.93746423]\n",
      " ...\n",
      " [ 0.32386305]\n",
      " [ 0.14117631]\n",
      " [-0.99697309]]\n",
      "t [[ 0.78264904]\n",
      " [-1.03786991]\n",
      " [-0.93746423]\n",
      " ...\n",
      " [ 0.32386305]\n",
      " [ 0.14117631]\n",
      " [-0.99697309]]\n",
      "t [[ 0.8219708 ]\n",
      " [-1.14874513]\n",
      " [-1.01728484]\n",
      " ...\n",
      " [ 0.31183572]\n",
      " [ 0.13257955]\n",
      " [-1.06130639]]\n",
      "t [[ 0.8219708 ]\n",
      " [-1.14874513]\n",
      " [-1.01728484]\n",
      " ...\n",
      " [ 0.31183572]\n",
      " [ 0.13257955]\n",
      " [-1.06130639]]\n",
      "Current iteration=8, loss=35443.59304825749\n",
      "t [[ 0.85464833]\n",
      " [-1.2500025 ]\n",
      " [-1.09038467]\n",
      " ...\n",
      " [ 0.29758541]\n",
      " [ 0.12263973]\n",
      " [-1.11998407]]\n",
      "t [[ 0.85464833]\n",
      " [-1.2500025 ]\n",
      " [-1.09038467]\n",
      " ...\n",
      " [ 0.29758541]\n",
      " [ 0.12263973]\n",
      " [-1.11998407]]\n",
      "t [[ 0.88211966]\n",
      " [-1.34290358]\n",
      " [-1.15768416]\n",
      " ...\n",
      " [ 0.28191582]\n",
      " [ 0.1119106 ]\n",
      " [-1.17387935]]\n",
      "loss=34252.577599269214\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.22207383]\n",
      " [-0.13210334]\n",
      " [-0.20877045]\n",
      " ...\n",
      " [ 0.31985616]\n",
      " [-0.20877045]\n",
      " [ 0.2995906 ]]\n",
      "t [[ 0.22207383]\n",
      " [-0.13210334]\n",
      " [-0.20877045]\n",
      " ...\n",
      " [ 0.31985616]\n",
      " [-0.20877045]\n",
      " [ 0.2995906 ]]\n",
      "t [[ 0.38452658]\n",
      " [-0.30654951]\n",
      " [-0.37751084]\n",
      " ...\n",
      " [ 0.54319494]\n",
      " [-0.37751084]\n",
      " [ 0.49318121]]\n",
      "t [[ 0.38452658]\n",
      " [-0.30654951]\n",
      " [-0.37751084]\n",
      " ...\n",
      " [ 0.54319494]\n",
      " [-0.37751084]\n",
      " [ 0.49318121]]\n",
      "Current iteration=2, loss=43815.573827721564\n",
      "t [[ 0.50654314]\n",
      " [-0.48326957]\n",
      " [-0.51960911]\n",
      " ...\n",
      " [ 0.70584715]\n",
      " [-0.51960911]\n",
      " [ 0.6252795 ]]\n",
      "t [[ 0.50654314]\n",
      " [-0.48326957]\n",
      " [-0.51960911]\n",
      " ...\n",
      " [ 0.70584715]\n",
      " [-0.51960911]\n",
      " [ 0.6252795 ]]\n",
      "t [[ 0.60032761]\n",
      " [-0.64882131]\n",
      " [-0.64263636]\n",
      " ...\n",
      " [ 0.82831612]\n",
      " [-0.64263636]\n",
      " [ 0.71941451]]\n",
      "t [[ 0.60032761]\n",
      " [-0.64882131]\n",
      " [-0.64263636]\n",
      " ...\n",
      " [ 0.82831612]\n",
      " [-0.64263636]\n",
      " [ 0.71941451]]\n",
      "Current iteration=4, loss=39637.82758755021\n",
      "t [[ 0.67380773]\n",
      " [-0.79996937]\n",
      " [-0.7511637 ]\n",
      " ...\n",
      " [ 0.92288826]\n",
      " [-0.7511637 ]\n",
      " [ 0.78867195]]\n",
      "t [[ 0.67380773]\n",
      " [-0.79996937]\n",
      " [-0.7511637 ]\n",
      " ...\n",
      " [ 0.92288826]\n",
      " [-0.7511637 ]\n",
      " [ 0.78867195]]\n",
      "t [[ 0.73232359]\n",
      " [-0.93701559]\n",
      " [-0.84818067]\n",
      " ...\n",
      " [ 0.99737959]\n",
      " [-0.84818067]\n",
      " [ 0.84086499]]\n",
      "t [[ 0.73232359]\n",
      " [-0.93701559]\n",
      " [-0.84818067]\n",
      " ...\n",
      " [ 0.99737959]\n",
      " [-0.84818067]\n",
      " [ 0.84086499]]\n",
      "Current iteration=6, loss=37104.47153246126\n",
      "t [[ 0.7795901 ]\n",
      " [-1.06133066]\n",
      " [-0.93577587]\n",
      " ...\n",
      " [ 1.05701717]\n",
      " [-0.93577587]\n",
      " [ 0.88095539]]\n",
      "t [[ 0.7795901 ]\n",
      " [-1.06133066]\n",
      " [-0.93577587]\n",
      " ...\n",
      " [ 1.05701717]\n",
      " [-0.93577587]\n",
      " [ 0.88095539]]\n",
      "t [[ 0.81826284]\n",
      " [-1.17447129]\n",
      " [-1.01548405]\n",
      " ...\n",
      " [ 1.10543752]\n",
      " [-1.01548405]\n",
      " [ 0.91225294]]\n",
      "t [[ 0.81826284]\n",
      " [-1.17447129]\n",
      " [-1.01548405]\n",
      " ...\n",
      " [ 1.10543752]\n",
      " [-1.01548405]\n",
      " [ 0.91225294]]\n",
      "Current iteration=8, loss=35417.578424002575\n",
      "t [[ 0.85028355]\n",
      " [-1.27788586]\n",
      " [-1.08847678]\n",
      " ...\n",
      " [ 1.14524976]\n",
      " [-1.08847678]\n",
      " [ 0.93705083]]\n",
      "t [[ 0.85028355]\n",
      " [-1.27788586]\n",
      " [-1.08847678]\n",
      " ...\n",
      " [ 1.14524976]\n",
      " [-1.08847678]\n",
      " [ 0.93705083]]\n",
      "t [[ 0.8770988 ]\n",
      " [-1.37283585]\n",
      " [-1.15567459]\n",
      " ...\n",
      " [ 1.17837133]\n",
      " [-1.15567459]\n",
      " [ 0.95698443]]\n",
      "loss=34221.96419800151\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07647939]\n",
      " [-0.3662214 ]\n",
      " [-0.21475644]\n",
      " ...\n",
      " [ 0.18364526]\n",
      " [ 0.07647939]\n",
      " [-0.31130005]]\n",
      "t [[ 0.07647939]\n",
      " [-0.3662214 ]\n",
      " [-0.21475644]\n",
      " ...\n",
      " [ 0.18364526]\n",
      " [ 0.07647939]\n",
      " [-0.31130005]]\n",
      "t [[ 0.11786523]\n",
      " [-0.67669585]\n",
      " [-0.38755145]\n",
      " ...\n",
      " [ 0.27399535]\n",
      " [ 0.11786523]\n",
      " [-0.50731067]]\n",
      "t [[ 0.11786523]\n",
      " [-0.67669585]\n",
      " [-0.38755145]\n",
      " ...\n",
      " [ 0.27399535]\n",
      " [ 0.11786523]\n",
      " [-0.50731067]]\n",
      "Current iteration=2, loss=43601.64313506841\n",
      "t [[ 0.13893748]\n",
      " [-0.93913986]\n",
      " [-0.53273689]\n",
      " ...\n",
      " [ 0.31853956]\n",
      " [ 0.13893748]\n",
      " [-0.64786815]]\n",
      "t [[ 0.13893748]\n",
      " [-0.93913986]\n",
      " [-0.53273689]\n",
      " ...\n",
      " [ 0.31853956]\n",
      " [ 0.13893748]\n",
      " [-0.64786815]]\n",
      "t [[ 0.1477082 ]\n",
      " [-1.16248878]\n",
      " [-0.65825719]\n",
      " ...\n",
      " [ 0.33874907]\n",
      " [ 0.1477082 ]\n",
      " [-0.75890115]]\n",
      "t [[ 0.1477082 ]\n",
      " [-1.16248878]\n",
      " [-0.65825719]\n",
      " ...\n",
      " [ 0.33874907]\n",
      " [ 0.1477082 ]\n",
      " [-0.75890115]]\n",
      "Current iteration=4, loss=39362.50967017753\n",
      "t [[ 0.1487381 ]\n",
      " [-1.35447266]\n",
      " [-0.76886128]\n",
      " ...\n",
      " [ 0.34509738]\n",
      " [ 0.1487381 ]\n",
      " [-0.85216141]]\n",
      "t [[ 0.1487381 ]\n",
      " [-1.35447266]\n",
      " [-0.76886128]\n",
      " ...\n",
      " [ 0.34509738]\n",
      " [ 0.1487381 ]\n",
      " [-0.85216141]]\n",
      "t [[ 0.144789  ]\n",
      " [-1.52123921]\n",
      " [-0.86763978]\n",
      " ...\n",
      " [ 0.3430741 ]\n",
      " [ 0.144789  ]\n",
      " [-0.93342727]]\n",
      "t [[ 0.144789  ]\n",
      " [-1.52123921]\n",
      " [-0.86763978]\n",
      " ...\n",
      " [ 0.3430741 ]\n",
      " [ 0.144789  ]\n",
      " [-0.93342727]]\n",
      "Current iteration=6, loss=36809.135673481476\n",
      "t [[ 0.13762034]\n",
      " [-1.66754676]\n",
      " [-0.95674748]\n",
      " ...\n",
      " [ 0.33577605]\n",
      " [ 0.13762034]\n",
      " [-1.00583044]]\n",
      "t [[ 0.13762034]\n",
      " [-1.66754676]\n",
      " [-0.95674748]\n",
      " ...\n",
      " [ 0.33577605]\n",
      " [ 0.13762034]\n",
      " [-1.00583044]]\n",
      "t [[ 0.12839667]\n",
      " [-1.79706481]\n",
      " [-1.03776684]\n",
      " ...\n",
      " [ 0.32507293]\n",
      " [ 0.12839667]\n",
      " [-1.07124715]]\n",
      "t [[ 0.12839667]\n",
      " [-1.79706481]\n",
      " [-1.03776684]\n",
      " ...\n",
      " [ 0.32507293]\n",
      " [ 0.12839667]\n",
      " [-1.07124715]]\n",
      "Current iteration=8, loss=35116.69410892404\n",
      "t [[ 0.11791027]\n",
      " [-1.91264156]\n",
      " [-1.11190553]\n",
      " ...\n",
      " [ 0.312158  ]\n",
      " [ 0.11791027]\n",
      " [-1.1309087 ]]\n",
      "t [[ 0.11791027]\n",
      " [-1.91264156]\n",
      " [-1.11190553]\n",
      " ...\n",
      " [ 0.312158  ]\n",
      " [ 0.11791027]\n",
      " [-1.1309087 ]]\n",
      "t [[ 0.10671032]\n",
      " [-2.01651089]\n",
      " [-1.18011232]\n",
      " ...\n",
      " [ 0.29782559]\n",
      " [ 0.10671032]\n",
      " [-1.18568639]]\n",
      "loss=33921.217728031195\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.23095073]\n",
      " [-0.13463501]\n",
      " [-0.21547096]\n",
      " ...\n",
      " [ 0.18095733]\n",
      " [ 0.07617913]\n",
      " [-0.30659544]]\n",
      "t [[ 0.23095073]\n",
      " [-0.13463501]\n",
      " [-0.21547096]\n",
      " ...\n",
      " [ 0.18095733]\n",
      " [ 0.07617913]\n",
      " [-0.30659544]]\n",
      "t [[ 0.39839205]\n",
      " [-0.31603732]\n",
      " [-0.38853688]\n",
      " ...\n",
      " [ 0.26928074]\n",
      " [ 0.117151  ]\n",
      " [-0.49797254]]\n",
      "t [[ 0.39839205]\n",
      " [-0.31603732]\n",
      " [-0.38853688]\n",
      " ...\n",
      " [ 0.26928074]\n",
      " [ 0.117151  ]\n",
      " [-0.49797254]]\n",
      "Current iteration=2, loss=43588.76535233288\n",
      "t [[ 0.52327004]\n",
      " [-0.49974339]\n",
      " [-0.53376948]\n",
      " ...\n",
      " [ 0.31232922]\n",
      " [ 0.13778358]\n",
      " [-0.63437713]]\n",
      "t [[ 0.52327004]\n",
      " [-0.49974339]\n",
      " [-0.53376948]\n",
      " ...\n",
      " [ 0.31232922]\n",
      " [ 0.13778358]\n",
      " [-0.63437713]]\n",
      "t [[ 0.61880472]\n",
      " [-0.67127656]\n",
      " [-0.65922521]\n",
      " ...\n",
      " [ 0.33145364]\n",
      " [ 0.14612481]\n",
      " [-0.74181189]]\n",
      "t [[ 0.61880472]\n",
      " [-0.67127656]\n",
      " [-0.65922521]\n",
      " ...\n",
      " [ 0.33145364]\n",
      " [ 0.14612481]\n",
      " [-0.74181189]]\n",
      "Current iteration=4, loss=39349.600852544136\n",
      "t [[ 0.69340675]\n",
      " [-0.82739877]\n",
      " [-0.76970925]\n",
      " ...\n",
      " [ 0.33702234]\n",
      " [ 0.14674971]\n",
      " [-0.83198366]]\n",
      "t [[ 0.69340675]\n",
      " [-0.82739877]\n",
      " [-0.76970925]\n",
      " ...\n",
      " [ 0.33702234]\n",
      " [ 0.14674971]\n",
      " [-0.83198366]]\n",
      "t [[ 0.75267446]\n",
      " [-0.96860931]\n",
      " [-0.86834113]\n",
      " ...\n",
      " [ 0.33444403]\n",
      " [ 0.1424253 ]\n",
      " [-0.91059641]]\n",
      "t [[ 0.75267446]\n",
      " [-0.96860931]\n",
      " [-0.86834113]\n",
      " ...\n",
      " [ 0.33444403]\n",
      " [ 0.1424253 ]\n",
      " [-0.91059641]]\n",
      "Current iteration=6, loss=36796.73723849101\n",
      "t [[ 0.80047432]\n",
      " [-1.09646788]\n",
      " [-0.95729089]\n",
      " ...\n",
      " [ 0.32675713]\n",
      " [ 0.13491212]\n",
      " [-0.98070915]]\n",
      "t [[ 0.80047432]\n",
      " [-1.09646788]\n",
      " [-0.95729089]\n",
      " ...\n",
      " [ 0.32675713]\n",
      " [ 0.13491212]\n",
      " [-0.98070915]]\n",
      "t [[ 0.83955507]\n",
      " [-1.21267591]\n",
      " [-1.03814919]\n",
      " ...\n",
      " [ 0.31578974]\n",
      " [ 0.12537398]\n",
      " [-1.04413585]]\n",
      "t [[ 0.83955507]\n",
      " [-1.21267591]\n",
      " [-1.03814919]\n",
      " ...\n",
      " [ 0.31578974]\n",
      " [ 0.12537398]\n",
      " [-1.04413585]]\n",
      "Current iteration=8, loss=35104.43010373844\n",
      "t [[ 0.87191707]\n",
      " [-1.31878649]\n",
      " [-1.11212825]\n",
      " ...\n",
      " [ 0.30270554]\n",
      " [ 0.11460164]\n",
      " [-1.10205701]]\n",
      "t [[ 0.87191707]\n",
      " [-1.31878649]\n",
      " [-1.11212825]\n",
      " ...\n",
      " [ 0.30270554]\n",
      " [ 0.11460164]\n",
      " [-1.10205701]]\n",
      "t [[ 0.89904397]\n",
      " [-1.41613633]\n",
      " [-1.1801793 ]\n",
      " ...\n",
      " [ 0.28827756]\n",
      " [ 0.10314243]\n",
      " [-1.15530306]]\n",
      "loss=33908.72906688586\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.22799063]\n",
      " [-0.1288844 ]\n",
      " [-0.21466399]\n",
      " ...\n",
      " [ 0.18374926]\n",
      " [ 0.07758318]\n",
      " [-0.31603705]]\n",
      "t [[ 0.22799063]\n",
      " [-0.1288844 ]\n",
      " [-0.21466399]\n",
      " ...\n",
      " [ 0.18374926]\n",
      " [ 0.07758318]\n",
      " [-0.31603705]]\n",
      "t [[ 0.39331097]\n",
      " [-0.30474889]\n",
      " [-0.386931  ]\n",
      " ...\n",
      " [ 0.27240414]\n",
      " [ 0.11954227]\n",
      " [-0.51342415]]\n",
      "t [[ 0.39331097]\n",
      " [-0.30474889]\n",
      " [-0.386931  ]\n",
      " ...\n",
      " [ 0.27240414]\n",
      " [ 0.11954227]\n",
      " [-0.51342415]]\n",
      "Current iteration=2, loss=43664.89987615877\n",
      "t [[ 0.51667285]\n",
      " [-0.48294252]\n",
      " [-0.53145479]\n",
      " ...\n",
      " [ 0.31470159]\n",
      " [ 0.14094147]\n",
      " [-0.65405451]]\n",
      "t [[ 0.51667285]\n",
      " [-0.48294252]\n",
      " [-0.53145479]\n",
      " ...\n",
      " [ 0.31470159]\n",
      " [ 0.14094147]\n",
      " [-0.65405451]]\n",
      "t [[ 0.61113382]\n",
      " [-0.64906384]\n",
      " [-0.65628977]\n",
      " ...\n",
      " [ 0.33250242]\n",
      " [ 0.14991136]\n",
      " [-0.76464045]]\n",
      "t [[ 0.61113382]\n",
      " [-0.64906384]\n",
      " [-0.65628977]\n",
      " ...\n",
      " [ 0.33250242]\n",
      " [ 0.14991136]\n",
      " [-0.76464045]]\n",
      "Current iteration=4, loss=39481.16726358904\n",
      "t [[ 0.68497745]\n",
      " [-0.79995755]\n",
      " [-0.76622389]\n",
      " ...\n",
      " [ 0.33642369]\n",
      " [ 0.1510676 ]\n",
      " [-0.85725389]]\n",
      "t [[ 0.68497745]\n",
      " [-0.79995755]\n",
      " [-0.76622389]\n",
      " ...\n",
      " [ 0.33642369]\n",
      " [ 0.1510676 ]\n",
      " [-0.85725389]]\n",
      "t [[ 0.74370759]\n",
      " [-0.93617186]\n",
      " [-0.86436055]\n",
      " ...\n",
      " [ 0.33201932]\n",
      " [ 0.14720109]\n",
      " [-0.93781249]]\n",
      "t [[ 0.74370759]\n",
      " [-0.93617186]\n",
      " [-0.86436055]\n",
      " ...\n",
      " [ 0.33201932]\n",
      " [ 0.14720109]\n",
      " [-0.93781249]]\n",
      "Current iteration=6, loss=36965.138245626884\n",
      "t [[ 0.79112449]\n",
      " [-1.05928635]\n",
      " [-0.95285712]\n",
      " ...\n",
      " [ 0.3224142 ]\n",
      " [ 0.14008815]\n",
      " [-1.00950836]]\n",
      "t [[ 0.79112449]\n",
      " [-1.05928635]\n",
      " [-0.95285712]\n",
      " ...\n",
      " [ 0.3224142 ]\n",
      " [ 0.14008815]\n",
      " [-1.00950836]]\n",
      "t [[ 0.82993103]\n",
      " [-1.17100458]\n",
      " [-1.03329469]\n",
      " ...\n",
      " [ 0.309489  ]\n",
      " [ 0.13090379]\n",
      " [-1.0742427 ]]\n",
      "t [[ 0.82993103]\n",
      " [-1.17100458]\n",
      " [-1.03329469]\n",
      " ...\n",
      " [ 0.309489  ]\n",
      " [ 0.13090379]\n",
      " [-1.0742427 ]]\n",
      "Current iteration=8, loss=35298.565162323306\n",
      "t [[ 0.86209562]\n",
      " [-1.27287218]\n",
      " [-1.10687831]\n",
      " ...\n",
      " [ 0.29444005]\n",
      " [ 0.12044714]\n",
      " [-1.13325588]]\n",
      "t [[ 0.86209562]\n",
      " [-1.27287218]\n",
      " [-1.10687831]\n",
      " ...\n",
      " [ 0.29444005]\n",
      " [ 0.12044714]\n",
      " [-1.13325588]]\n",
      "t [[ 0.88907947]\n",
      " [-1.36621362]\n",
      " [-1.17455391]\n",
      " ...\n",
      " [ 0.27806105]\n",
      " [ 0.10927195]\n",
      " [-1.18742117]]\n",
      "loss=34121.67166851426\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.22762568]\n",
      " [-0.13540593]\n",
      " [-0.21398972]\n",
      " ...\n",
      " [ 0.32785256]\n",
      " [-0.21398972]\n",
      " [ 0.30708037]]\n",
      "t [[ 0.22762568]\n",
      " [-0.13540593]\n",
      " [-0.21398972]\n",
      " ...\n",
      " [ 0.32785256]\n",
      " [-0.21398972]\n",
      " [ 0.30708037]]\n",
      "t [[ 0.39264671]\n",
      " [-0.31526964]\n",
      " [-0.38594434]\n",
      " ...\n",
      " [ 0.55435821]\n",
      " [-0.38594434]\n",
      " [ 0.50285886]]\n",
      "t [[ 0.39264671]\n",
      " [-0.31526964]\n",
      " [-0.38594434]\n",
      " ...\n",
      " [ 0.55435821]\n",
      " [-0.38594434]\n",
      " [ 0.50285886]]\n",
      "Current iteration=2, loss=43666.21689753272\n",
      "t [[ 0.51571441]\n",
      " [-0.49652128]\n",
      " [-0.53027351]\n",
      " ...\n",
      " [ 0.71807642]\n",
      " [-0.53027351]\n",
      " [ 0.6352224 ]]\n",
      "t [[ 0.51571441]\n",
      " [-0.49652128]\n",
      " [-0.53027351]\n",
      " ...\n",
      " [ 0.71807642]\n",
      " [-0.53027351]\n",
      " [ 0.6352224 ]]\n",
      "t [[ 0.60975186]\n",
      " [-0.66539696]\n",
      " [-0.65495824]\n",
      " ...\n",
      " [ 0.84062933]\n",
      " [-0.65495824]\n",
      " [ 0.72889855]]\n",
      "t [[ 0.60975186]\n",
      " [-0.66539696]\n",
      " [-0.65495824]\n",
      " ...\n",
      " [ 0.84062933]\n",
      " [-0.65495824]\n",
      " [ 0.72889855]]\n",
      "Current iteration=4, loss=39470.07063517169\n",
      "t [[ 0.68305942]\n",
      " [-0.81891186]\n",
      " [-0.76475916]\n",
      " ...\n",
      " [ 0.93480459]\n",
      " [-0.76475916]\n",
      " [ 0.79742581]]\n",
      "t [[ 0.68305942]\n",
      " [-0.81891186]\n",
      " [-0.76475916]\n",
      " ...\n",
      " [ 0.93480459]\n",
      " [-0.76475916]\n",
      " [ 0.79742581]]\n",
      "t [[ 0.74118175]\n",
      " [-0.95764512]\n",
      " [-0.86277056]\n",
      " ...\n",
      " [ 1.00866769]\n",
      " [-0.86277056]\n",
      " [ 0.84880835]]\n",
      "t [[ 0.74118175]\n",
      " [-0.95764512]\n",
      " [-0.86277056]\n",
      " ...\n",
      " [ 1.00866769]\n",
      " [-0.86277056]\n",
      " [ 0.84880835]]\n",
      "Current iteration=6, loss=36944.9791778403\n",
      "t [[ 0.78795055]\n",
      " [-1.08317399]\n",
      " [-0.95114748]\n",
      " ...\n",
      " [ 1.06758002]\n",
      " [-0.95114748]\n",
      " [ 0.88809658]]\n",
      "t [[ 0.78795055]\n",
      " [-1.08317399]\n",
      " [-0.95114748]\n",
      " ...\n",
      " [ 1.06758002]\n",
      " [-0.95114748]\n",
      " [ 0.88809658]]\n",
      "t [[ 0.82608961]\n",
      " [-1.19719561]\n",
      " [-1.03147082]\n",
      " ...\n",
      " [ 1.11525383]\n",
      " [-1.03147082]\n",
      " [ 0.9186437 ]]\n",
      "t [[ 0.82608961]\n",
      " [-1.19719561]\n",
      " [-1.03147082]\n",
      " ...\n",
      " [ 1.11525383]\n",
      " [-1.03147082]\n",
      " [ 0.9186437 ]]\n",
      "Current iteration=8, loss=35271.98735885473\n",
      "t [[ 0.85758062]\n",
      " [-1.30125235]\n",
      " [-1.10494584]\n",
      " ...\n",
      " [ 1.15434147]\n",
      " [-1.10494584]\n",
      " [ 0.94276381]]\n",
      "t [[ 0.85758062]\n",
      " [-1.30125235]\n",
      " [-1.10494584]\n",
      " ...\n",
      " [ 1.15434147]\n",
      " [-1.10494584]\n",
      " [ 0.94276381]]\n",
      "t [[ 0.88389315]\n",
      " [-1.39666922]\n",
      " [-1.17251855]\n",
      " ...\n",
      " [ 1.18678456]\n",
      " [-1.17251855]\n",
      " [ 0.96210095]]\n",
      "loss=34090.556029196465\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07834474]\n",
      " [-0.37515363]\n",
      " [-0.2199944 ]\n",
      " ...\n",
      " [ 0.18812442]\n",
      " [ 0.07834474]\n",
      " [-0.31889273]]\n",
      "t [[ 0.07834474]\n",
      " [-0.37515363]\n",
      " [-0.2199944 ]\n",
      " ...\n",
      " [ 0.18812442]\n",
      " [ 0.07834474]\n",
      " [-0.31889273]]\n",
      "t [[ 0.11988499]\n",
      " [-0.69183006]\n",
      " [-0.39597835]\n",
      " ...\n",
      " [ 0.27840232]\n",
      " [ 0.11988499]\n",
      " [-0.51687604]]\n",
      "t [[ 0.11988499]\n",
      " [-0.69183006]\n",
      " [-0.39597835]\n",
      " ...\n",
      " [ 0.27840232]\n",
      " [ 0.11988499]\n",
      " [-0.51687604]]\n",
      "Current iteration=2, loss=43452.9458508316\n",
      "t [[ 0.1404945 ]\n",
      " [-0.95834654]\n",
      " [-0.54337034]\n",
      " ...\n",
      " [ 0.32181516]\n",
      " [ 0.1404945 ]\n",
      " [-0.65818827]]\n",
      "t [[ 0.1404945 ]\n",
      " [-0.95834654]\n",
      " [-0.54337034]\n",
      " ...\n",
      " [ 0.32181516]\n",
      " [ 0.1404945 ]\n",
      " [-0.65818827]]\n",
      "t [[ 0.14859197]\n",
      " [-1.18431139]\n",
      " [-0.67052704]\n",
      " ...\n",
      " [ 0.34075517]\n",
      " [ 0.14859197]\n",
      " [-0.76978822]]\n",
      "t [[ 0.14859197]\n",
      " [-1.18431139]\n",
      " [-0.67052704]\n",
      " ...\n",
      " [ 0.34075517]\n",
      " [ 0.14859197]\n",
      " [-0.76978822]]\n",
      "Current iteration=4, loss=39196.87138326137\n",
      "t [[ 0.14890082]\n",
      " [-1.37794875]\n",
      " [-0.78238538]\n",
      " ...\n",
      " [ 0.34591879]\n",
      " [ 0.14890082]\n",
      " [-0.86359805]]\n",
      "t [[ 0.14890082]\n",
      " [-1.37794875]\n",
      " [-0.78238538]\n",
      " ...\n",
      " [ 0.34591879]\n",
      " [ 0.14890082]\n",
      " [-0.86359805]]\n",
      "t [[ 0.14425362]\n",
      " [-1.54573306]\n",
      " [-0.88213986]\n",
      " ...\n",
      " [ 0.34283304]\n",
      " [ 0.14425362]\n",
      " [-0.94538743]]\n",
      "t [[ 0.14425362]\n",
      " [-1.54573306]\n",
      " [-0.88213986]\n",
      " ...\n",
      " [ 0.34283304]\n",
      " [ 0.14425362]\n",
      " [-0.94538743]]\n",
      "Current iteration=6, loss=36652.52314800257\n",
      "t [[ 0.13644168]\n",
      " [-1.692633  ]\n",
      " [-0.97201166]\n",
      " ...\n",
      " [ 0.33458992]\n",
      " [ 0.13644168]\n",
      " [-1.01826191]]\n",
      "t [[ 0.13644168]\n",
      " [-1.692633  ]\n",
      " [-0.97201166]\n",
      " ...\n",
      " [ 0.33458992]\n",
      " [ 0.13644168]\n",
      " [-1.01826191]]\n",
      "t [[ 0.12664311]\n",
      " [-1.82245416]\n",
      " [-1.05362949]\n",
      " ...\n",
      " [ 0.32304755]\n",
      " [ 0.12664311]\n",
      " [-1.0840827 ]]\n",
      "t [[ 0.12664311]\n",
      " [-1.82245416]\n",
      " [-1.05362949]\n",
      " ...\n",
      " [ 0.32304755]\n",
      " [ 0.12664311]\n",
      " [-1.0840827 ]]\n",
      "Current iteration=8, loss=34974.29760875091\n",
      "t [[ 0.11565442]\n",
      " [-1.93813421]\n",
      " [-1.12823494]\n",
      " ...\n",
      " [ 0.30938819]\n",
      " [ 0.11565442]\n",
      " [-1.14407685]]\n",
      "t [[ 0.11565442]\n",
      " [-1.93813421]\n",
      " [-1.12823494]\n",
      " ...\n",
      " [ 0.30938819]\n",
      " [ 0.11565442]\n",
      " [-1.14407685]]\n",
      "t [[ 0.10402397]\n",
      " [-2.04196701]\n",
      " [-1.19680239]\n",
      " ...\n",
      " [ 0.29439654]\n",
      " [ 0.10402397]\n",
      " [-1.19911784]]\n",
      "loss=33793.08183367807\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.23658367]\n",
      " [-0.13791879]\n",
      " [-0.22072635]\n",
      " ...\n",
      " [ 0.18537093]\n",
      " [ 0.07803715]\n",
      " [-0.31407338]]\n",
      "t [[ 0.23658367]\n",
      " [-0.13791879]\n",
      " [-0.22072635]\n",
      " ...\n",
      " [ 0.18537093]\n",
      " [ 0.07803715]\n",
      " [-0.31407338]]\n",
      "t [[ 0.40656249]\n",
      " [-0.32487945]\n",
      " [-0.39697718]\n",
      " ...\n",
      " [ 0.27358863]\n",
      " [ 0.11915069]\n",
      " [-0.50731231]]\n",
      "t [[ 0.40656249]\n",
      " [-0.32487945]\n",
      " [-0.39697718]\n",
      " ...\n",
      " [ 0.27358863]\n",
      " [ 0.11915069]\n",
      " [-0.50731231]]\n",
      "Current iteration=2, loss=43440.0030398462\n",
      "t [[ 0.5324426 ]\n",
      " [-0.51317377]\n",
      " [-0.54440683]\n",
      " ...\n",
      " [ 0.315495  ]\n",
      " [ 0.13930854]\n",
      " [-0.64439387]]\n",
      "t [[ 0.5324426 ]\n",
      " [-0.51317377]\n",
      " [-0.54440683]\n",
      " ...\n",
      " [ 0.315495  ]\n",
      " [ 0.13930854]\n",
      " [-0.64439387]]\n",
      "t [[ 0.62819007]\n",
      " [-0.68802485]\n",
      " [-0.67148934]\n",
      " ...\n",
      " [ 0.33335309]\n",
      " [ 0.14696667]\n",
      " [-0.75234782]]\n",
      "t [[ 0.62819007]\n",
      " [-0.68802485]\n",
      " [-0.67148934]\n",
      " ...\n",
      " [ 0.33335309]\n",
      " [ 0.14696667]\n",
      " [-0.75234782]]\n",
      "Current iteration=4, loss=39183.99203940788\n",
      "t [[ 0.70259106]\n",
      " [-0.8464837 ]\n",
      " [-0.78321929]\n",
      " ...\n",
      " [ 0.33774739]\n",
      " [ 0.14686292]\n",
      " [-0.84304277]]\n",
      "t [[ 0.70259106]\n",
      " [-0.8464837 ]\n",
      " [-0.78321929]\n",
      " ...\n",
      " [ 0.33774739]\n",
      " [ 0.14686292]\n",
      " [-0.84304277]]\n",
      "t [[ 0.76144783]\n",
      " [-0.9893469 ]\n",
      " [-0.88282028]\n",
      " ...\n",
      " [ 0.33412012]\n",
      " [ 0.14183479]\n",
      " [-0.92216671]]\n",
      "t [[ 0.76144783]\n",
      " [-0.9893469 ]\n",
      " [-0.88282028]\n",
      " ...\n",
      " [ 0.33412012]\n",
      " [ 0.14183479]\n",
      " [-0.92216671]]\n",
      "Current iteration=6, loss=36640.155706440484\n",
      "t [[ 0.80874225]\n",
      " [-1.1183883 ]\n",
      " [-0.97252854]\n",
      " ...\n",
      " [ 0.32550278]\n",
      " [ 0.13367439]\n",
      " [-0.99274742]]\n",
      "t [[ 0.80874225]\n",
      " [-1.1183883 ]\n",
      " [-0.97252854]\n",
      " ...\n",
      " [ 0.32550278]\n",
      " [ 0.13367439]\n",
      " [-0.99274742]]\n",
      "t [[ 0.84728966]\n",
      " [-1.23545167]\n",
      " [-1.05398077]\n",
      " ...\n",
      " [ 0.31371089]\n",
      " [ 0.12355881]\n",
      " [-1.05658058]]\n",
      "t [[ 0.84728966]\n",
      " [-1.23545167]\n",
      " [-1.05398077]\n",
      " ...\n",
      " [ 0.31371089]\n",
      " [ 0.12355881]\n",
      " [-1.05658058]]\n",
      "Current iteration=8, loss=34962.03279216636\n",
      "t [[ 0.87912882]\n",
      " [-1.34218377]\n",
      " [-1.1284229 ]\n",
      " ...\n",
      " [ 0.29989667]\n",
      " [ 0.1122828 ]\n",
      " [-1.11484039]]\n",
      "t [[ 0.87912882]\n",
      " [-1.34218377]\n",
      " [-1.1284229 ]\n",
      " ...\n",
      " [ 0.29989667]\n",
      " [ 0.1122828 ]\n",
      " [-1.11484039]]\n",
      "t [[ 0.90576499]\n",
      " [-1.43998391]\n",
      " [-1.19683161]\n",
      " ...\n",
      " [ 0.28482336]\n",
      " [ 0.10039268]\n",
      " [-1.16835814]]\n",
      "loss=33780.55282864028\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.23355138]\n",
      " [-0.13202792]\n",
      " [-0.2198997 ]\n",
      " ...\n",
      " [ 0.18823095]\n",
      " [ 0.07947545]\n",
      " [-0.32374527]]\n",
      "t [[ 0.23355138]\n",
      " [-0.13202792]\n",
      " [-0.2198997 ]\n",
      " ...\n",
      " [ 0.18823095]\n",
      " [ 0.07947545]\n",
      " [-0.32374527]]\n",
      "t [[ 0.40137778]\n",
      " [-0.31332079]\n",
      " [-0.39533271]\n",
      " ...\n",
      " [ 0.27673153]\n",
      " [ 0.1215905 ]\n",
      " [-0.52305979]]\n",
      "t [[ 0.40137778]\n",
      " [-0.31332079]\n",
      " [-0.39533271]\n",
      " ...\n",
      " [ 0.27673153]\n",
      " [ 0.1215905 ]\n",
      " [-0.52305979]]\n",
      "Current iteration=2, loss=43517.87227786552\n",
      "t [[ 0.52573405]\n",
      " [-0.49596955]\n",
      " [-0.5420407 ]\n",
      " ...\n",
      " [ 0.31781658]\n",
      " [ 0.14252317]\n",
      " [-0.66438546]]\n",
      "t [[ 0.52573405]\n",
      " [-0.49596955]\n",
      " [-0.5420407 ]\n",
      " ...\n",
      " [ 0.31781658]\n",
      " [ 0.14252317]\n",
      " [-0.66438546]]\n",
      "t [[ 0.62041324]\n",
      " [-0.66528377]\n",
      " [-0.66849348]\n",
      " ...\n",
      " [ 0.33427609]\n",
      " [ 0.15081516]\n",
      " [-0.77548998]]\n",
      "t [[ 0.62041324]\n",
      " [-0.66528377]\n",
      " [-0.66849348]\n",
      " ...\n",
      " [ 0.33427609]\n",
      " [ 0.15081516]\n",
      " [-0.77548998]]\n",
      "Current iteration=4, loss=39317.843181468685\n",
      "t [[ 0.69406689]\n",
      " [-0.81840361]\n",
      " [-0.77966665]\n",
      " ...\n",
      " [ 0.33695072]\n",
      " [ 0.15124605]\n",
      " [-0.86861775]]\n",
      "t [[ 0.69406689]\n",
      " [-0.81840361]\n",
      " [-0.77966665]\n",
      " ...\n",
      " [ 0.33695072]\n",
      " [ 0.15124605]\n",
      " [-0.86861775]]\n",
      "t [[ 0.75239878]\n",
      " [-0.95617573]\n",
      " [-0.87876667]\n",
      " ...\n",
      " [ 0.33143059]\n",
      " [ 0.14667785]\n",
      " [-0.94967496]]\n",
      "t [[ 0.75239878]\n",
      " [-0.95617573]\n",
      " [-0.87876667]\n",
      " ...\n",
      " [ 0.33143059]\n",
      " [ 0.14667785]\n",
      " [-0.94967496]]\n",
      "Current iteration=6, loss=36810.88425783749\n",
      "t [[ 0.79932262]\n",
      " [-1.08039277]\n",
      " [-0.9680165 ]\n",
      " ...\n",
      " [ 0.32083422]\n",
      " [ 0.13891882]\n",
      " [-1.02182437]]\n",
      "t [[ 0.79932262]\n",
      " [-1.08039277]\n",
      " [-0.9680165 ]\n",
      " ...\n",
      " [ 0.32083422]\n",
      " [ 0.13891882]\n",
      " [-1.02182437]]\n",
      "t [[ 0.83760704]\n",
      " [-1.19289885]\n",
      " [-1.04904295]\n",
      " ...\n",
      " [ 0.30702945]\n",
      " [ 0.12915757]\n",
      " [-1.08694981]]\n",
      "t [[ 0.83760704]\n",
      " [-1.19289885]\n",
      " [-1.04904295]\n",
      " ...\n",
      " [ 0.30702945]\n",
      " [ 0.12915757]\n",
      " [-1.08694981]]\n",
      "Current iteration=8, loss=35158.38277959513\n",
      "t [[ 0.86925852]\n",
      " [-1.2953314 ]\n",
      " [-1.12308465]\n",
      " ...\n",
      " [ 0.29120067]\n",
      " [ 0.11819738]\n",
      " [-1.1462858 ]]\n",
      "t [[ 0.86925852]\n",
      " [-1.2953314 ]\n",
      " [-1.12308465]\n",
      " ...\n",
      " [ 0.29120067]\n",
      " [ 0.11819738]\n",
      " [-1.1462858 ]]\n",
      "t [[ 0.89575979]\n",
      " [-1.38907587]\n",
      " [-1.19111288]\n",
      " ...\n",
      " [ 0.27413129]\n",
      " [ 0.10659115]\n",
      " [-1.20070676]]\n",
      "loss=33995.550839997995\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.23317752]\n",
      " [-0.13870851]\n",
      " [-0.21920898]\n",
      " ...\n",
      " [ 0.33584896]\n",
      " [-0.21920898]\n",
      " [ 0.31457013]]\n",
      "t [[ 0.23317752]\n",
      " [-0.13870851]\n",
      " [-0.21920898]\n",
      " ...\n",
      " [ 0.33584896]\n",
      " [-0.21920898]\n",
      " [ 0.31457013]]\n",
      "t [[ 0.40069658]\n",
      " [-0.3240392 ]\n",
      " [-0.39433044]\n",
      " ...\n",
      " [ 0.56540776]\n",
      " [-0.39433044]\n",
      " [ 0.51241188]]\n",
      "t [[ 0.40069658]\n",
      " [-0.3240392 ]\n",
      " [-0.39433044]\n",
      " ...\n",
      " [ 0.56540776]\n",
      " [-0.39433044]\n",
      " [ 0.51241188]]\n",
      "Current iteration=2, loss=43518.89755664398\n",
      "t [[ 0.5247452 ]\n",
      " [-0.50978154]\n",
      " [-0.54084468]\n",
      " ...\n",
      " [ 0.73009481]\n",
      " [-0.54084468]\n",
      " [ 0.64495197]]\n",
      "t [[ 0.5247452 ]\n",
      " [-0.50978154]\n",
      " [-0.54084468]\n",
      " ...\n",
      " [ 0.73009481]\n",
      " [-0.54084468]\n",
      " [ 0.64495197]]\n",
      "t [[ 0.61897996]\n",
      " [-0.68189769]\n",
      " [-0.6671466 ]\n",
      " ...\n",
      " [ 0.85266322]\n",
      " [-0.6671466 ]\n",
      " [ 0.73811889]]\n",
      "t [[ 0.61897996]\n",
      " [-0.68189769]\n",
      " [-0.6671466 ]\n",
      " ...\n",
      " [ 0.85266322]\n",
      " [-0.6671466 ]\n",
      " [ 0.73811889]]\n",
      "Current iteration=4, loss=39306.214828820885\n",
      "t [[ 0.69207494]\n",
      " [-0.83769011]\n",
      " [-0.77818489]\n",
      " ...\n",
      " [ 0.94639643]\n",
      " [-0.77818489]\n",
      " [ 0.80589056]]\n",
      "t [[ 0.69207494]\n",
      " [-0.83769011]\n",
      " [-0.77818489]\n",
      " ...\n",
      " [ 0.94639643]\n",
      " [-0.77818489]\n",
      " [ 0.80589056]]\n",
      "t [[ 0.74977733]\n",
      " [-0.97803007]\n",
      " [-0.87715757]\n",
      " ...\n",
      " [ 1.01960368]\n",
      " [-0.87715757]\n",
      " [ 0.85645292]]\n",
      "t [[ 0.74977733]\n",
      " [-0.97803007]\n",
      " [-0.87715757]\n",
      " ...\n",
      " [ 1.01960368]\n",
      " [-0.87715757]\n",
      " [ 0.85645292]]\n",
      "Current iteration=6, loss=36790.151617002106\n",
      "t [[ 0.79603312]\n",
      " [-1.10470482]\n",
      " [-0.96628574]\n",
      " ...\n",
      " [ 1.07777642]\n",
      " [-0.96628574]\n",
      " [ 0.89493975]]\n",
      "t [[ 0.79603312]\n",
      " [-1.10470482]\n",
      " [-0.96628574]\n",
      " ...\n",
      " [ 1.07777642]\n",
      " [-0.96628574]\n",
      " [ 0.89493975]]\n",
      "t [[ 0.83363192]\n",
      " [-1.21955122]\n",
      " [-1.04719622]\n",
      " ...\n",
      " [ 1.12469947]\n",
      " [-1.04719622]\n",
      " [ 0.9247445 ]]\n",
      "t [[ 0.83363192]\n",
      " [-1.21955122]\n",
      " [-1.04719622]\n",
      " ...\n",
      " [ 1.12469947]\n",
      " [-1.04719622]\n",
      " [ 0.9247445 ]]\n",
      "Current iteration=8, loss=35131.258922297864\n",
      "t [[ 0.86459342]\n",
      " [-1.32420369]\n",
      " [-1.12112787]\n",
      " ...\n",
      " [ 1.163066  ]\n",
      " [-1.12112787]\n",
      " [ 0.94820009]]\n",
      "t [[ 0.86459342]\n",
      " [-1.32420369]\n",
      " [-1.12112787]\n",
      " ...\n",
      " [ 1.163066  ]\n",
      " [-1.12112787]\n",
      " [ 0.94820009]]\n",
      "t [[ 0.89040848]\n",
      " [-1.42004889]\n",
      " [-1.18905207]\n",
      " ...\n",
      " [ 1.1948398 ]\n",
      " [-1.18905207]\n",
      " [ 0.9669576 ]]\n",
      "loss=33963.9532065072\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08021009]\n",
      " [-0.38408586]\n",
      " [-0.22523236]\n",
      " ...\n",
      " [ 0.19260357]\n",
      " [ 0.08021009]\n",
      " [-0.32648541]]\n",
      "t [[ 0.08021009]\n",
      " [-0.38408586]\n",
      " [-0.22523236]\n",
      " ...\n",
      " [ 0.19260357]\n",
      " [ 0.08021009]\n",
      " [-0.32648541]]\n",
      "t [[ 0.12186559]\n",
      " [-0.70690062]\n",
      " [-0.40435806]\n",
      " ...\n",
      " [ 0.28270494]\n",
      " [ 0.12186559]\n",
      " [-0.52631289]]\n",
      "t [[ 0.12186559]\n",
      " [-0.70690062]\n",
      " [-0.40435806]\n",
      " ...\n",
      " [ 0.28270494]\n",
      " [ 0.12186559]\n",
      " [-0.52631289]]\n",
      "Current iteration=2, loss=43306.260883237\n",
      "t [[ 0.14198467]\n",
      " [-0.97739265]\n",
      " [-0.55391195]\n",
      " ...\n",
      " [ 0.32493824]\n",
      " [ 0.14198467]\n",
      " [-0.66832478]]\n",
      "t [[ 0.14198467]\n",
      " [-0.97739265]\n",
      " [-0.55391195]\n",
      " ...\n",
      " [ 0.32493824]\n",
      " [ 0.14198467]\n",
      " [-0.66832478]]\n",
      "t [[ 0.14939465]\n",
      " [-1.20587421]\n",
      " [-0.682666  ]\n",
      " ...\n",
      " [ 0.34259955]\n",
      " [ 0.14939465]\n",
      " [-0.78047833]]\n",
      "t [[ 0.14939465]\n",
      " [-1.20587421]\n",
      " [-0.682666  ]\n",
      " ...\n",
      " [ 0.34259955]\n",
      " [ 0.14939465]\n",
      " [-0.78047833]]\n",
      "Current iteration=4, loss=39035.044718934136\n",
      "t [[ 0.14897806]\n",
      " [-1.40107711]\n",
      " [-0.79574342]\n",
      " ...\n",
      " [ 0.34658623]\n",
      " [ 0.14897806]\n",
      " [-0.87483561]]\n",
      "t [[ 0.14897806]\n",
      " [-1.40107711]\n",
      " [-0.79574342]\n",
      " ...\n",
      " [ 0.34658623]\n",
      " [ 0.14897806]\n",
      " [-0.87483561]]\n",
      "t [[ 0.14363529]\n",
      " [-1.56980586]\n",
      " [-0.89644165]\n",
      " ...\n",
      " [ 0.34245201]\n",
      " [ 0.14363529]\n",
      " [-0.95714522]]\n",
      "t [[ 0.14363529]\n",
      " [-1.56980586]\n",
      " [-0.89644165]\n",
      " ...\n",
      " [ 0.34245201]\n",
      " [ 0.14363529]\n",
      " [-0.95714522]]\n",
      "Current iteration=6, loss=36500.43496553681\n",
      "t [[ 0.13518749]\n",
      " [-1.71723867]\n",
      " [-0.98704788]\n",
      " ...\n",
      " [ 0.33328003]\n",
      " [ 0.13518749]\n",
      " [-1.03048362]]\n",
      "t [[ 0.13518749]\n",
      " [-1.71723867]\n",
      " [-0.98704788]\n",
      " ...\n",
      " [ 0.33328003]\n",
      " [ 0.13518749]\n",
      " [-1.03048362]]\n",
      "t [[ 0.12482491]\n",
      " [-1.84731523]\n",
      " [-1.06923697]\n",
      " ...\n",
      " [ 0.32091551]\n",
      " [ 0.12482491]\n",
      " [-1.09669744]]\n",
      "t [[ 0.12482491]\n",
      " [-1.84731523]\n",
      " [-1.06923697]\n",
      " ...\n",
      " [ 0.32091551]\n",
      " [ 0.12482491]\n",
      " [-1.09669744]]\n",
      "Current iteration=8, loss=34836.59249564975\n",
      "t [[ 0.11334721]\n",
      " [-1.96306058]\n",
      " [-1.14428434]\n",
      " ...\n",
      " [ 0.30652925]\n",
      " [ 0.11334721]\n",
      " [-1.15701072]]\n",
      "t [[ 0.11334721]\n",
      " [-1.96306058]\n",
      " [-1.14428434]\n",
      " ...\n",
      " [ 0.30652925]\n",
      " [ 0.11334721]\n",
      " [-1.15701072]]\n",
      "t [[ 0.10130113]\n",
      " [-2.06682665]\n",
      " [-1.21318983]\n",
      " ...\n",
      " [ 0.2908961 ]\n",
      " [ 0.10130113]\n",
      " [-1.21230025]]\n",
      "loss=33669.56319515154\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.24221662]\n",
      " [-0.14120257]\n",
      " [-0.22598174]\n",
      " ...\n",
      " [ 0.18978452]\n",
      " [ 0.07989518]\n",
      " [-0.32155132]]\n",
      "t [[ 0.24221662]\n",
      " [-0.14120257]\n",
      " [-0.22598174]\n",
      " ...\n",
      " [ 0.18978452]\n",
      " [ 0.07989518]\n",
      " [-0.32155132]]\n",
      "t [[ 0.41466216]\n",
      " [-0.33377313]\n",
      " [-0.40536981]\n",
      " ...\n",
      " [ 0.2777929 ]\n",
      " [ 0.1211111 ]\n",
      " [-0.5165237 ]]\n",
      "t [[ 0.41466216]\n",
      " [-0.33377313]\n",
      " [-0.40536981]\n",
      " ...\n",
      " [ 0.2777929 ]\n",
      " [ 0.1211111 ]\n",
      " [-0.5165237 ]]\n",
      "Current iteration=2, loss=43293.25917137626\n",
      "t [[ 0.5414754 ]\n",
      " [-0.52661194]\n",
      " [-0.55495164]\n",
      " ...\n",
      " [ 0.31851004]\n",
      " [ 0.14076656]\n",
      " [-0.65422863]]\n",
      "t [[ 0.5414754 ]\n",
      " [-0.52661194]\n",
      " [-0.55495164]\n",
      " ...\n",
      " [ 0.31851004]\n",
      " [ 0.14076656]\n",
      " [-0.65422863]]\n",
      "t [[ 0.6373818 ]\n",
      " [-0.70469542]\n",
      " [-0.68362183]\n",
      " ...\n",
      " [ 0.33509352]\n",
      " [ 0.1477275 ]\n",
      " [-0.76269045]]\n",
      "t [[ 0.6373818 ]\n",
      " [-0.70469542]\n",
      " [-0.68362183]\n",
      " ...\n",
      " [ 0.33509352]\n",
      " [ 0.1477275 ]\n",
      " [-0.76269045]]\n",
      "Current iteration=4, loss=39022.19635009084\n",
      "t [[ 0.71154359]\n",
      " [-0.86540168]\n",
      " [-0.79656264]\n",
      " ...\n",
      " [ 0.33832182]\n",
      " [ 0.14689091]\n",
      " [-0.8539084 ]]\n",
      "t [[ 0.71154359]\n",
      " [-0.86540168]\n",
      " [-0.79656264]\n",
      " ...\n",
      " [ 0.33832182]\n",
      " [ 0.14689091]\n",
      " [-0.8539084 ]]\n",
      "t [[ 0.76996478]\n",
      " [-1.00983869]\n",
      " [-0.89710066]\n",
      " ...\n",
      " [ 0.33365995]\n",
      " [ 0.14116182]\n",
      " [-0.93354184]]\n",
      "t [[ 0.76996478]\n",
      " [-1.00983869]\n",
      " [-0.89710066]\n",
      " ...\n",
      " [ 0.33365995]\n",
      " [ 0.14116182]\n",
      " [-0.93354184]]\n",
      "Current iteration=6, loss=36488.09657464461\n",
      "t [[ 0.81674002]\n",
      " [-1.13999711]\n",
      " [-0.98753794]\n",
      " ...\n",
      " [ 0.32412855]\n",
      " [ 0.13236184]\n",
      " [-1.00458436]]\n",
      "t [[ 0.81674002]\n",
      " [-1.13999711]\n",
      " [-0.98753794]\n",
      " ...\n",
      " [ 0.32412855]\n",
      " [ 0.13236184]\n",
      " [-1.00458436]]\n",
      "t [[ 0.85474887]\n",
      " [-1.2578618 ]\n",
      " [-1.06955706]\n",
      " ...\n",
      " [ 0.31152929]\n",
      " [ 0.12167994]\n",
      " [-1.06881389]]\n",
      "t [[ 0.85474887]\n",
      " [-1.2578618 ]\n",
      " [-1.06955706]\n",
      " ...\n",
      " [ 0.31152929]\n",
      " [ 0.12167994]\n",
      " [-1.06881389]]\n",
      "Current iteration=8, loss=34824.32337621156\n",
      "t [[ 0.88606651]\n",
      " [-1.36517101]\n",
      " [-1.14443756]\n",
      " ...\n",
      " [ 0.29700254]\n",
      " [ 0.10991377]\n",
      " [-1.12739958]]\n",
      "t [[ 0.88606651]\n",
      " [-1.36517101]\n",
      " [-1.14443756]\n",
      " ...\n",
      " [ 0.29700254]\n",
      " [ 0.10991377]\n",
      " [-1.12739958]]\n",
      "t [[ 0.91221808]\n",
      " [-1.46338465]\n",
      " [-1.21318146]\n",
      " ...\n",
      " [ 0.28130156]\n",
      " [ 0.09760781]\n",
      " [-1.18117479]]\n",
      "loss=33656.990141780945\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.23911213]\n",
      " [-0.13517144]\n",
      " [-0.2251354 ]\n",
      " ...\n",
      " [ 0.19271264]\n",
      " [ 0.08136772]\n",
      " [-0.33145349]]\n",
      "t [[ 0.23911213]\n",
      " [-0.13517144]\n",
      " [-0.2251354 ]\n",
      " ...\n",
      " [ 0.19271264]\n",
      " [ 0.08136772]\n",
      " [-0.33145349]]\n",
      "t [[ 0.40937474]\n",
      " [-0.32194447]\n",
      " [-0.4036868 ]\n",
      " ...\n",
      " [ 0.2809529 ]\n",
      " [ 0.12359904]\n",
      " [-0.53256351]]\n",
      "t [[ 0.40937474]\n",
      " [-0.32194447]\n",
      " [-0.4036868 ]\n",
      " ...\n",
      " [ 0.2809529 ]\n",
      " [ 0.12359904]\n",
      " [-0.53256351]]\n",
      "Current iteration=2, loss=43372.852049392866\n",
      "t [[ 0.53465751]\n",
      " [-0.50900444]\n",
      " [-0.55253437]\n",
      " ...\n",
      " [ 0.32077724]\n",
      " [ 0.14403724]\n",
      " [-0.6745287 ]]\n",
      "t [[ 0.53465751]\n",
      " [-0.50900444]\n",
      " [-0.55253437]\n",
      " ...\n",
      " [ 0.32077724]\n",
      " [ 0.14403724]\n",
      " [-0.6745287 ]]\n",
      "t [[ 0.62950188]\n",
      " [-0.6814266 ]\n",
      " [-0.68056611]\n",
      " ...\n",
      " [ 0.33588677]\n",
      " [ 0.15163699]\n",
      " [-0.78613918]]\n",
      "t [[ 0.62950188]\n",
      " [-0.6814266 ]\n",
      " [-0.68056611]\n",
      " ...\n",
      " [ 0.33588677]\n",
      " [ 0.15163699]\n",
      " [-0.78613918]]\n",
      "Current iteration=4, loss=39158.29154235245\n",
      "t [[ 0.70292783]\n",
      " [-0.83668459]\n",
      " [-0.79294345]\n",
      " ...\n",
      " [ 0.33732337]\n",
      " [ 0.15133815]\n",
      " [-0.87978033]]\n",
      "t [[ 0.70292783]\n",
      " [-0.83668459]\n",
      " [-0.79294345]\n",
      " ...\n",
      " [ 0.33732337]\n",
      " [ 0.15133815]\n",
      " [-0.87978033]]\n",
      "t [[ 0.76083699]\n",
      " [-0.97593737]\n",
      " [-0.89297484]\n",
      " ...\n",
      " [ 0.33070244]\n",
      " [ 0.14607084]\n",
      " [-0.96133397]]\n",
      "t [[ 0.76083699]\n",
      " [-0.97593737]\n",
      " [-0.89297484]\n",
      " ...\n",
      " [ 0.33070244]\n",
      " [ 0.14607084]\n",
      " [-0.96133397]]\n",
      "Current iteration=6, loss=36661.095989527035\n",
      "t [[ 0.80725398]\n",
      " [-1.1011931 ]\n",
      " [-0.9829485 ]\n",
      " ...\n",
      " [ 0.319132  ]\n",
      " [ 0.13767325]\n",
      " [-1.03393048]]\n",
      "t [[ 0.80725398]\n",
      " [-1.1011931 ]\n",
      " [-0.9829485 ]\n",
      " ...\n",
      " [ 0.319132  ]\n",
      " [ 0.13767325]\n",
      " [-1.03393048]]\n",
      "t [[ 0.84501088]\n",
      " [-1.21443501]\n",
      " [-1.06453684]\n",
      " ...\n",
      " [ 0.30446569]\n",
      " [ 0.12734611]\n",
      " [-1.09943671]]\n",
      "t [[ 0.84501088]\n",
      " [-1.21443501]\n",
      " [-1.06453684]\n",
      " ...\n",
      " [ 0.30446569]\n",
      " [ 0.12734611]\n",
      " [-1.09943671]]\n",
      "Current iteration=8, loss=35022.824171379616\n",
      "t [[ 0.87615029]\n",
      " [-1.31739011]\n",
      " [-1.13901192]\n",
      " ...\n",
      " [ 0.28787552]\n",
      " [ 0.11589582]\n",
      " [-1.1590826 ]]\n",
      "t [[ 0.87615029]\n",
      " [-1.31739011]\n",
      " [-1.13901192]\n",
      " ...\n",
      " [ 0.28787552]\n",
      " [ 0.11589582]\n",
      " [-1.1590826 ]]\n",
      "t [[ 0.9021748 ]\n",
      " [-1.41150277]\n",
      " [-1.20737028]\n",
      " ...\n",
      " [ 0.2701344 ]\n",
      " [ 0.10387358]\n",
      " [-1.21374487]]\n",
      "loss=33873.97664640848\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.23872937]\n",
      " [-0.14201109]\n",
      " [-0.22442824]\n",
      " ...\n",
      " [ 0.34384537]\n",
      " [-0.22442824]\n",
      " [ 0.3220599 ]]\n",
      "t [[ 0.23872937]\n",
      " [-0.14201109]\n",
      " [-0.22442824]\n",
      " ...\n",
      " [ 0.34384537]\n",
      " [-0.22442824]\n",
      " [ 0.3220599 ]]\n",
      "t [[ 0.4086764 ]\n",
      " [-0.33285799]\n",
      " [-0.40266929]\n",
      " ...\n",
      " [ 0.57634399]\n",
      " [-0.40266929]\n",
      " [ 0.52184071]]\n",
      "t [[ 0.4086764 ]\n",
      " [-0.33285799]\n",
      " [-0.40266929]\n",
      " ...\n",
      " [ 0.57634399]\n",
      " [-0.40266929]\n",
      " [ 0.52184071]]\n",
      "Current iteration=2, loss=43373.580509367835\n",
      "t [[ 0.53363752]\n",
      " [-0.5230473 ]\n",
      " [-0.55132397]\n",
      " ...\n",
      " [ 0.74190588]\n",
      " [-0.55132397]\n",
      " [ 0.65447248]]\n",
      "t [[ 0.53363752]\n",
      " [-0.5230473 ]\n",
      " [-0.55132397]\n",
      " ...\n",
      " [ 0.74190588]\n",
      " [-0.55132397]\n",
      " [ 0.65447248]]\n",
      "t [[ 0.62801626]\n",
      " [-0.69831969]\n",
      " [-0.67920413]\n",
      " ...\n",
      " [ 0.86442491]\n",
      " [-0.67920413]\n",
      " [ 0.74708354]]\n",
      "t [[ 0.62801626]\n",
      " [-0.69831969]\n",
      " [-0.67920413]\n",
      " ...\n",
      " [ 0.86442491]\n",
      " [-0.67920413]\n",
      " [ 0.74708354]]\n",
      "Current iteration=4, loss=39146.13821868367\n",
      "t [[ 0.70086097]\n",
      " [-0.85630195]\n",
      " [-0.79144482]\n",
      " ...\n",
      " [ 0.95767404]\n",
      " [-0.79144482]\n",
      " [ 0.814077  ]]\n",
      "t [[ 0.70086097]\n",
      " [-0.85630195]\n",
      " [-0.79144482]\n",
      " ...\n",
      " [ 0.95767404]\n",
      " [-0.79144482]\n",
      " [ 0.814077  ]]\n",
      "t [[ 0.75811914]\n",
      " [-0.99817111]\n",
      " [-0.89134678]\n",
      " ...\n",
      " [ 1.03020042]\n",
      " [-0.89134678]\n",
      " [ 0.86381148]]\n",
      "t [[ 0.75811914]\n",
      " [-0.99817111]\n",
      " [-0.89134678]\n",
      " ...\n",
      " [ 1.03020042]\n",
      " [-0.89134678]\n",
      " [ 0.86381148]]\n",
      "Current iteration=6, loss=36639.802280549855\n",
      "t [[ 0.80384842]\n",
      " [-1.12592708]\n",
      " [-0.98119679]\n",
      " ...\n",
      " [ 1.08762133]\n",
      " [-0.98119679]\n",
      " [ 0.90149905]]\n",
      "t [[ 0.80384842]\n",
      " [-1.12592708]\n",
      " [-0.98119679]\n",
      " ...\n",
      " [ 1.08762133]\n",
      " [-0.98119679]\n",
      " [ 0.90149905]]\n",
      "t [[ 0.84090192]\n",
      " [-1.24154517]\n",
      " [-1.06266745]\n",
      " ...\n",
      " [ 1.13379107]\n",
      " [-1.06266745]\n",
      " [ 0.93057043]]\n",
      "t [[ 0.84090192]\n",
      " [-1.24154517]\n",
      " [-1.06266745]\n",
      " ...\n",
      " [ 1.13379107]\n",
      " [-1.06266745]\n",
      " [ 0.93057043]]\n",
      "Current iteration=8, loss=34995.1709437637\n",
      "t [[ 0.87133528]\n",
      " [-1.3467498 ]\n",
      " [-1.13703111]\n",
      " ...\n",
      " [ 1.17144124]\n",
      " [-1.13703111]\n",
      " [ 0.95337531]]\n",
      "t [[ 0.87133528]\n",
      " [-1.3467498 ]\n",
      " [-1.13703111]\n",
      " ...\n",
      " [ 1.17144124]\n",
      " [-1.13703111]\n",
      " [ 0.95337531]]\n",
      "t [[ 0.89665902]\n",
      " [-1.44298732]\n",
      " [-1.20528437]\n",
      " ...\n",
      " [ 1.20255589]\n",
      " [-1.20528437]\n",
      " [ 0.97157035]]\n",
      "loss=33841.91661207479\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08207544]\n",
      " [-0.39301809]\n",
      " [-0.23047032]\n",
      " ...\n",
      " [ 0.19708272]\n",
      " [ 0.08207544]\n",
      " [-0.3340781 ]]\n",
      "t [[ 0.08207544]\n",
      " [-0.39301809]\n",
      " [-0.23047032]\n",
      " ...\n",
      " [ 0.19708272]\n",
      " [ 0.08207544]\n",
      " [-0.3340781 ]]\n",
      "t [[ 0.12380718]\n",
      " [-0.72190767]\n",
      " [-0.41269072]\n",
      " ...\n",
      " [ 0.28690359]\n",
      " [ 0.12380718]\n",
      " [-0.5356217 ]]\n",
      "t [[ 0.12380718]\n",
      " [-0.72190767]\n",
      " [-0.41269072]\n",
      " ...\n",
      " [ 0.28690359]\n",
      " [ 0.12380718]\n",
      " [-0.5356217 ]]\n",
      "Current iteration=2, loss=43161.55355621166\n",
      "t [[ 0.14340932]\n",
      " [-0.99627931]\n",
      " [-0.56436308]\n",
      " ...\n",
      " [ 0.32791276]\n",
      " [ 0.14340932]\n",
      " [-0.67828256]]\n",
      "t [[ 0.14340932]\n",
      " [-0.99627931]\n",
      " [-0.56436308]\n",
      " ...\n",
      " [ 0.32791276]\n",
      " [ 0.14340932]\n",
      " [-0.67828256]]\n",
      "t [[ 0.15011877]\n",
      " [-1.22718067]\n",
      " [-0.69467669]\n",
      " ...\n",
      " [ 0.34428888]\n",
      " [ 0.15011877]\n",
      " [-0.79097936]]\n",
      "t [[ 0.15011877]\n",
      " [-1.22718067]\n",
      " [-0.69467669]\n",
      " ...\n",
      " [ 0.34428888]\n",
      " [ 0.15011877]\n",
      " [-0.79097936]]\n",
      "Current iteration=4, loss=38876.91193503366\n",
      "t [[ 0.14897331]\n",
      " [-1.42386422]\n",
      " [-0.80893919]\n",
      " ...\n",
      " [ 0.34710765]\n",
      " [ 0.14897331]\n",
      " [-0.885883  ]]\n",
      "t [[ 0.14897331]\n",
      " [-1.42386422]\n",
      " [-0.80893919]\n",
      " ...\n",
      " [ 0.34710765]\n",
      " [ 0.14897331]\n",
      " [-0.885883  ]]\n",
      "t [[ 0.14293816]\n",
      " [-1.59346732]\n",
      " [-0.91054999]\n",
      " ...\n",
      " [ 0.3419393 ]\n",
      " [ 0.14293816]\n",
      " [-0.96870952]]\n",
      "t [[ 0.14293816]\n",
      " [-1.59346732]\n",
      " [-0.91054999]\n",
      " ...\n",
      " [ 0.3419393 ]\n",
      " [ 0.14293816]\n",
      " [-0.96870952]]\n",
      "Current iteration=6, loss=36352.69252409726\n",
      "t [[ 0.13386236]\n",
      " [-1.74137665]\n",
      " [-1.00186202]\n",
      " ...\n",
      " [ 0.33185459]\n",
      " [ 0.13386236]\n",
      " [-1.04250408]]\n",
      "t [[ 0.13386236]\n",
      " [-1.74137665]\n",
      " [-1.00186202]\n",
      " ...\n",
      " [ 0.33185459]\n",
      " [ 0.13386236]\n",
      " [-1.04250408]]\n",
      "t [[ 0.12294692]\n",
      " [-1.8716638 ]\n",
      " [-1.08459618]\n",
      " ...\n",
      " [ 0.31868474]\n",
      " [ 0.12294692]\n",
      " [-1.10909958]]\n",
      "t [[ 0.12294692]\n",
      " [-1.8716638 ]\n",
      " [-1.08459618]\n",
      " ...\n",
      " [ 0.31868474]\n",
      " [ 0.12294692]\n",
      " [-1.10909958]]\n",
      "Current iteration=8, loss=34703.36744642074\n",
      "t [[ 0.11099363]\n",
      " [-1.98743902]\n",
      " [-1.1600616 ]\n",
      " ...\n",
      " [ 0.30358877]\n",
      " [ 0.11099363]\n",
      " [-1.16971839]]\n",
      "t [[ 0.11099363]\n",
      " [-1.98743902]\n",
      " [-1.1600616 ]\n",
      " ...\n",
      " [ 0.30358877]\n",
      " [ 0.11099363]\n",
      " [-1.16971839]]\n",
      "t [[ 0.09854674]\n",
      " [-2.09111041]\n",
      " [-1.22928348]\n",
      " ...\n",
      " [ 0.28733147]\n",
      " [ 0.09854674]\n",
      " [-1.22524174]]\n",
      "loss=33550.43542169759\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.24784956]\n",
      " [-0.14448636]\n",
      " [-0.23123713]\n",
      " ...\n",
      " [ 0.19419811]\n",
      " [ 0.08175321]\n",
      " [-0.32902925]]\n",
      "t [[ 0.24784956]\n",
      " [-0.14448636]\n",
      " [-0.23123713]\n",
      " ...\n",
      " [ 0.19419811]\n",
      " [ 0.08175321]\n",
      " [-0.32902925]]\n",
      "t [[ 0.4226913 ]\n",
      " [-0.34271815]\n",
      " [-0.41371492]\n",
      " ...\n",
      " [ 0.28189391]\n",
      " [ 0.1230324 ]\n",
      " [-0.52560719]]\n",
      "t [[ 0.4226913 ]\n",
      " [-0.34271815]\n",
      " [-0.41371492]\n",
      " ...\n",
      " [ 0.28189391]\n",
      " [ 0.1230324 ]\n",
      " [-0.52560719]]\n",
      "Current iteration=2, loss=43148.49878076154\n",
      "t [[ 0.55037045]\n",
      " [-0.54005476]\n",
      " [-0.56540526]\n",
      " ...\n",
      " [ 0.32137826]\n",
      " [ 0.14215898]\n",
      " [-0.66388632]]\n",
      "t [[ 0.55037045]\n",
      " [-0.54005476]\n",
      " [-0.56540526]\n",
      " ...\n",
      " [ 0.32137826]\n",
      " [ 0.14215898]\n",
      " [-0.66388632]]\n",
      "t [[ 0.64638418]\n",
      " [-0.72128456]\n",
      " [-0.69562537]\n",
      " ...\n",
      " [ 0.33668155]\n",
      " [ 0.14840983]\n",
      " [-0.77284766]]\n",
      "t [[ 0.64638418]\n",
      " [-0.72128456]\n",
      " [-0.69562537]\n",
      " ...\n",
      " [ 0.33668155]\n",
      " [ 0.14840983]\n",
      " [-0.77284766]]\n",
      "Current iteration=4, loss=38864.09576469685\n",
      "t [[ 0.72027084]\n",
      " [-0.88415074]\n",
      " [-0.80974314]\n",
      " ...\n",
      " [ 0.33875348]\n",
      " [ 0.14683717]\n",
      " [-0.86458939]]\n",
      "t [[ 0.72027084]\n",
      " [-0.88415074]\n",
      " [-0.80974314]\n",
      " ...\n",
      " [ 0.33875348]\n",
      " [ 0.14683717]\n",
      " [-0.86458939]]\n",
      "t [[ 0.77823382]\n",
      " [-1.0300856 ]\n",
      " [-0.91118718]\n",
      " ...\n",
      " [ 0.3330717 ]\n",
      " [ 0.14041054]\n",
      " [-0.94473054]]\n",
      "t [[ 0.77823382]\n",
      " [-1.0300856 ]\n",
      " [-0.91118718]\n",
      " ...\n",
      " [ 0.3330717 ]\n",
      " [ 0.14041054]\n",
      " [-0.94473054]]\n",
      "Current iteration=6, loss=36340.381146793356\n",
      "t [[ 0.82447787]\n",
      " [-1.16129844]\n",
      " [-1.00232501]\n",
      " ...\n",
      " [ 0.3226425 ]\n",
      " [ 0.13097909]\n",
      " [-1.01622831]]\n",
      "t [[ 0.82447787]\n",
      " [-1.16129844]\n",
      " [-1.00232501]\n",
      " ...\n",
      " [ 0.3226425 ]\n",
      " [ 0.13097909]\n",
      " [-1.01622831]]\n",
      "t [[ 0.86194438]\n",
      " [-1.27991343]\n",
      " [-1.08488498]\n",
      " ...\n",
      " [ 0.3092527 ]\n",
      " [ 0.11974224]\n",
      " [-1.08084373]]\n",
      "t [[ 0.86194438]\n",
      " [-1.27991343]\n",
      " [-1.08488498]\n",
      " ...\n",
      " [ 0.3092527 ]\n",
      " [ 0.11974224]\n",
      " [-1.08084373]]\n",
      "Current iteration=8, loss=34691.09061223772\n",
      "t [[ 0.8927429 ]\n",
      " [-1.38775804]\n",
      " [-1.16018015]\n",
      " ...\n",
      " [ 0.29403055]\n",
      " [ 0.10749953]\n",
      " [-1.13974235]]\n",
      "t [[ 0.8927429 ]\n",
      " [-1.38775804]\n",
      " [-1.16018015]\n",
      " ...\n",
      " [ 0.29403055]\n",
      " [ 0.10749953]\n",
      " [-1.13974235]]\n",
      "t [[ 0.91841682]\n",
      " [-1.48635078]\n",
      " [-1.22923772]\n",
      " ...\n",
      " [ 0.27771917]\n",
      " [ 0.09479276]\n",
      " [-1.19376075]]\n",
      "loss=33537.81480219631\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.24467288]\n",
      " [-0.13831497]\n",
      " [-0.23037111]\n",
      " ...\n",
      " [ 0.19719433]\n",
      " [ 0.08326   ]\n",
      " [-0.33916172]]\n",
      "t [[ 0.24467288]\n",
      " [-0.13831497]\n",
      " [-0.23037111]\n",
      " ...\n",
      " [ 0.19719433]\n",
      " [ 0.08326   ]\n",
      " [-0.33916172]]\n",
      "t [[ 0.4173021 ]\n",
      " [-0.33061969]\n",
      " [-0.41199343]\n",
      " ...\n",
      " [ 0.28506864]\n",
      " [ 0.12556804]\n",
      " [-0.54193582]]\n",
      "t [[ 0.4173021 ]\n",
      " [-0.33061969]\n",
      " [-0.41199343]\n",
      " ...\n",
      " [ 0.28506864]\n",
      " [ 0.12556804]\n",
      " [-0.54193582]]\n",
      "Current iteration=2, loss=43229.804039953466\n",
      "t [[ 0.54344523]\n",
      " [-0.52204405]\n",
      " [-0.56293716]\n",
      " ...\n",
      " [ 0.3235876 ]\n",
      " [ 0.14548499]\n",
      " [-0.6844893 ]]\n",
      "t [[ 0.54344523]\n",
      " [-0.52204405]\n",
      " [-0.56293716]\n",
      " ...\n",
      " [ 0.3235876 ]\n",
      " [ 0.14548499]\n",
      " [-0.6844893 ]]\n",
      "t [[ 0.63840394]\n",
      " [-0.69748862]\n",
      " [-0.69251031]\n",
      " ...\n",
      " [ 0.33734123]\n",
      " [ 0.15237941]\n",
      " [-0.79659614]]\n",
      "t [[ 0.63840394]\n",
      " [-0.69748862]\n",
      " [-0.69251031]\n",
      " ...\n",
      " [ 0.33734123]\n",
      " [ 0.15237941]\n",
      " [-0.79659614]]\n",
      "Current iteration=4, loss=39002.39515276726\n",
      "t [[ 0.71156669]\n",
      " [-0.85479853]\n",
      " [-0.8060581 ]\n",
      " ...\n",
      " [ 0.33754972]\n",
      " [ 0.15134741]\n",
      " [-0.89075074]]\n",
      "t [[ 0.71156669]\n",
      " [-0.85479853]\n",
      " [-0.8060581 ]\n",
      " ...\n",
      " [ 0.33754972]\n",
      " [ 0.15134741]\n",
      " [-0.89075074]]\n",
      "t [[ 0.76903059]\n",
      " [-0.99545774]\n",
      " [-0.90698996]\n",
      " ...\n",
      " [ 0.32984329]\n",
      " [ 0.14538427]\n",
      " [-0.9727986 ]]\n",
      "t [[ 0.76903059]\n",
      " [-0.99545774]\n",
      " [-0.90698996]\n",
      " ...\n",
      " [ 0.32984329]\n",
      " [ 0.14538427]\n",
      " [-0.9727986 ]]\n",
      "Current iteration=6, loss=36515.59656216533\n",
      "t [[ 0.81492865]\n",
      " [-1.12169148]\n",
      " [-0.99765903]\n",
      " ...\n",
      " [ 0.31731583]\n",
      " [ 0.13635609]\n",
      " [-1.04583535]]\n",
      "t [[ 0.81492865]\n",
      " [-1.12169148]\n",
      " [-0.99765903]\n",
      " ...\n",
      " [ 0.31731583]\n",
      " [ 0.13635609]\n",
      " [-1.04583535]]\n",
      "t [[ 0.85215404]\n",
      " [-1.23562021]\n",
      " [-1.07978323]\n",
      " ...\n",
      " [ 0.30180571]\n",
      " [ 0.12547436]\n",
      " [-1.1117117 ]]\n",
      "t [[ 0.85215404]\n",
      " [-1.23562021]\n",
      " [-1.07978323]\n",
      " ...\n",
      " [ 0.30180571]\n",
      " [ 0.12547436]\n",
      " [-1.1117117 ]]\n",
      "Current iteration=8, loss=34891.68056586047\n",
      "t [[ 0.88278352]\n",
      " [-1.33905812]\n",
      " [-1.15466797]\n",
      " ...\n",
      " [ 0.28447223]\n",
      " [ 0.11354752]\n",
      " [-1.1716544 ]]\n",
      "t [[ 0.88278352]\n",
      " [-1.33905812]\n",
      " [-1.15466797]\n",
      " ...\n",
      " [ 0.28447223]\n",
      " [ 0.11354752]\n",
      " [-1.1716544 ]]\n",
      "t [[ 0.90833792]\n",
      " [-1.43350644]\n",
      " [-1.22333494]\n",
      " ...\n",
      " [ 0.2660776 ]\n",
      " [ 0.10112424]\n",
      " [-1.22654366]]\n",
      "loss=33756.72576779485\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.24428122]\n",
      " [-0.14531368]\n",
      " [-0.2296475 ]\n",
      " ...\n",
      " [ 0.35184177]\n",
      " [-0.2296475 ]\n",
      " [ 0.32954966]]\n",
      "t [[ 0.24428122]\n",
      " [-0.14531368]\n",
      " [-0.2296475 ]\n",
      " ...\n",
      " [ 0.35184177]\n",
      " [-0.2296475 ]\n",
      " [ 0.32954966]]\n",
      "t [[ 0.41658644]\n",
      " [-0.34172582]\n",
      " [-0.41096103]\n",
      " ...\n",
      " [ 0.58716728]\n",
      " [-0.41096103]\n",
      " [ 0.53114581]]\n",
      "t [[ 0.41658644]\n",
      " [-0.34172582]\n",
      " [-0.41096103]\n",
      " ...\n",
      " [ 0.58716728]\n",
      " [-0.41096103]\n",
      " [ 0.53114581]]\n",
      "Current iteration=2, loss=43230.23108425431\n",
      "t [[ 0.54239337]\n",
      " [-0.53631554]\n",
      " [-0.56171274]\n",
      " ...\n",
      " [ 0.75351317]\n",
      " [-0.56171274]\n",
      " [ 0.66378812]]\n",
      "t [[ 0.54239337]\n",
      " [-0.53631554]\n",
      " [-0.56171274]\n",
      " ...\n",
      " [ 0.75351317]\n",
      " [-0.56171274]\n",
      " [ 0.66378812]]\n",
      "t [[ 0.63686498]\n",
      " [-0.71465941]\n",
      " [-0.69113348]\n",
      " ...\n",
      " [ 0.87592135]\n",
      " [-0.69113348]\n",
      " [ 0.75580028]]\n",
      "t [[ 0.63686498]\n",
      " [-0.71465941]\n",
      " [-0.69113348]\n",
      " ...\n",
      " [ 0.87592135]\n",
      " [-0.69113348]\n",
      " [ 0.75580028]]\n",
      "Current iteration=4, loss=38989.72364564134\n",
      "t [[ 0.70942396]\n",
      " [-0.8747455 ]\n",
      " [-0.80454276]\n",
      " ...\n",
      " [ 0.96864734]\n",
      " [-0.80454276]\n",
      " [ 0.82199553]]\n",
      "t [[ 0.70942396]\n",
      " [-0.8747455 ]\n",
      " [-0.80454276]\n",
      " ...\n",
      " [ 0.96864734]\n",
      " [-0.80454276]\n",
      " [ 0.82199553]]\n",
      "t [[ 0.76621563]\n",
      " [-1.01806923]\n",
      " [-0.90534306]\n",
      " ...\n",
      " [ 1.04047022]\n",
      " [-0.90534306]\n",
      " [ 0.87089618]]\n",
      "t [[ 0.76621563]\n",
      " [-1.01806923]\n",
      " [-0.90534306]\n",
      " ...\n",
      " [ 1.04047022]\n",
      " [-0.90534306]\n",
      " [ 0.87089618]]\n",
      "Current iteration=6, loss=36493.754038072344\n",
      "t [[ 0.81140662]\n",
      " [-1.14684488]\n",
      " [-0.99588652]\n",
      " ...\n",
      " [ 1.09712897]\n",
      " [-0.99588652]\n",
      " [ 0.90778784]]\n",
      "t [[ 0.81140662]\n",
      " [-1.14684488]\n",
      " [-0.99588652]\n",
      " ...\n",
      " [ 1.09712897]\n",
      " [-0.99588652]\n",
      " [ 0.90778784]]\n",
      "t [[ 0.84791116]\n",
      " [-1.26318458]\n",
      " [-1.07789138]\n",
      " ...\n",
      " [ 1.14254437]\n",
      " [-1.07789138]\n",
      " [ 0.93613566]]\n",
      "t [[ 0.84791116]\n",
      " [-1.26318458]\n",
      " [-1.07789138]\n",
      " ...\n",
      " [ 1.14254437]\n",
      " [-1.07789138]\n",
      " [ 0.93613566]]\n",
      "Current iteration=8, loss=34863.514223437276\n",
      "t [[ 0.87781885]\n",
      " [-1.36890045]\n",
      " [-1.1526634 ]\n",
      " ...\n",
      " [ 1.17948411]\n",
      " [-1.1526634 ]\n",
      " [ 0.95830417]]\n",
      "t [[ 0.87781885]\n",
      " [-1.36890045]\n",
      " [-1.1526634 ]\n",
      " ...\n",
      " [ 1.17948411]\n",
      " [-1.1526634 ]\n",
      " [ 0.95830417]]\n",
      "t [[ 0.90265822]\n",
      " [-1.46549663]\n",
      " [-1.22122425]\n",
      " ...\n",
      " [ 1.20995058]\n",
      " [-1.22122425]\n",
      " [ 0.97595412]]\n",
      "loss=33724.22229622849\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0839408 ]\n",
      " [-0.40195032]\n",
      " [-0.23570828]\n",
      " ...\n",
      " [ 0.20156187]\n",
      " [ 0.0839408 ]\n",
      " [-0.34167078]]\n",
      "t [[ 0.0839408 ]\n",
      " [-0.40195032]\n",
      " [-0.23570828]\n",
      " ...\n",
      " [ 0.20156187]\n",
      " [ 0.0839408 ]\n",
      " [-0.34167078]]\n",
      "t [[ 0.1257099 ]\n",
      " [-0.73685137]\n",
      " [-0.42097649]\n",
      " ...\n",
      " [ 0.29099863]\n",
      " [ 0.1257099 ]\n",
      " [-0.54480295]]\n",
      "t [[ 0.1257099 ]\n",
      " [-0.73685137]\n",
      " [-0.42097649]\n",
      " ...\n",
      " [ 0.29099863]\n",
      " [ 0.1257099 ]\n",
      " [-0.54480295]]\n",
      "Current iteration=2, loss=43018.789813270356\n",
      "t [[ 0.14476974]\n",
      " [-1.01500766]\n",
      " [-0.57472506]\n",
      " ...\n",
      " [ 0.33074262]\n",
      " [ 0.14476974]\n",
      " [-0.68806649]]\n",
      "t [[ 0.14476974]\n",
      " [-1.01500766]\n",
      " [-0.57472506]\n",
      " ...\n",
      " [ 0.33074262]\n",
      " [ 0.14476974]\n",
      " [-0.68806649]]\n",
      "t [[ 0.15076679]\n",
      " [-1.24823413]\n",
      " [-0.7065617 ]\n",
      " ...\n",
      " [ 0.34582962]\n",
      " [ 0.15076679]\n",
      " [-0.80129892]]\n",
      "t [[ 0.15076679]\n",
      " [-1.24823413]\n",
      " [-0.7065617 ]\n",
      " ...\n",
      " [ 0.34582962]\n",
      " [ 0.15076679]\n",
      " [-0.80129892]]\n",
      "Current iteration=4, loss=38722.35985337627\n",
      "t [[ 0.14888989]\n",
      " [-1.4463164 ]\n",
      " [-0.82197636]\n",
      " ...\n",
      " [ 0.3474906 ]\n",
      " [ 0.14888989]\n",
      " [-0.89674863]]\n",
      "t [[ 0.14888989]\n",
      " [-1.4463164 ]\n",
      " [-0.82197636]\n",
      " ...\n",
      " [ 0.3474906 ]\n",
      " [ 0.14888989]\n",
      " [-0.89674863]]\n",
      "t [[ 0.14216618]\n",
      " [-1.61672694]\n",
      " [-0.92446957]\n",
      " ...\n",
      " [ 0.34130271]\n",
      " [ 0.14216618]\n",
      " [-0.98008859]]\n",
      "t [[ 0.14216618]\n",
      " [-1.61672694]\n",
      " [-0.92446957]\n",
      " ...\n",
      " [ 0.34130271]\n",
      " [ 0.14216618]\n",
      " [-0.98008859]]\n",
      "Current iteration=6, loss=36209.126133978796\n",
      "t [[ 0.13247065]\n",
      " [-1.76505942]\n",
      " [-1.01645973]\n",
      " ...\n",
      " [ 0.33032123]\n",
      " [ 0.13247065]\n",
      " [-1.05433118]]\n",
      "t [[ 0.13247065]\n",
      " [-1.76505942]\n",
      " [-1.01645973]\n",
      " ...\n",
      " [ 0.33032123]\n",
      " [ 0.13247065]\n",
      " [-1.05433118]]\n",
      "t [[ 0.12101376]\n",
      " [-1.89551508]\n",
      " [-1.0997137 ]\n",
      " ...\n",
      " [ 0.31636259]\n",
      " [ 0.12101376]\n",
      " [-1.12129672]]\n",
      "t [[ 0.12101376]\n",
      " [-1.89551508]\n",
      " [-1.0997137 ]\n",
      " ...\n",
      " [ 0.31636259]\n",
      " [ 0.12101376]\n",
      " [-1.12129672]]\n",
      "Current iteration=8, loss=34574.42329269154\n",
      "t [[ 0.10859834]\n",
      " [-2.01128711]\n",
      " [-1.17557425]\n",
      " ...\n",
      " [ 0.30057376]\n",
      " [ 0.10859834]\n",
      " [-1.18220736]]\n",
      "t [[ 0.10859834]\n",
      " [-2.01128711]\n",
      " [-1.17557425]\n",
      " ...\n",
      " [ 0.30057376]\n",
      " [ 0.10859834]\n",
      " [-1.18220736]]\n",
      "t [[ 0.09576541]\n",
      " [-2.11483795]\n",
      " [-1.24509178]\n",
      " ...\n",
      " [ 0.2837093 ]\n",
      " [ 0.09576541]\n",
      " [-1.23794992]]\n",
      "loss=33435.48625747453\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.2534825 ]\n",
      " [-0.14777014]\n",
      " [-0.23649251]\n",
      " ...\n",
      " [ 0.19861171]\n",
      " [ 0.08361123]\n",
      " [-0.33650719]]\n",
      "t [[ 0.2534825 ]\n",
      " [-0.14777014]\n",
      " [-0.23649251]\n",
      " ...\n",
      " [ 0.19861171]\n",
      " [ 0.08361123]\n",
      " [-0.33650719]]\n",
      "t [[ 0.43065017]\n",
      " [-0.35171429]\n",
      " [-0.42201266]\n",
      " ...\n",
      " [ 0.28589203]\n",
      " [ 0.12491471]\n",
      " [-0.53456326]]\n",
      "t [[ 0.43065017]\n",
      " [-0.35171429]\n",
      " [-0.42201266]\n",
      " ...\n",
      " [ 0.28589203]\n",
      " [ 0.12491471]\n",
      " [-0.53456326]]\n",
      "Current iteration=2, loss=43005.687530553434\n",
      "t [[ 0.55912976]\n",
      " [-0.55349916]\n",
      " [-0.57576904]\n",
      " ...\n",
      " [ 0.32410356]\n",
      " [ 0.14348709]\n",
      " [-0.67337182]]\n",
      "t [[ 0.55912976]\n",
      " [-0.55349916]\n",
      " [-0.57576904]\n",
      " ...\n",
      " [ 0.32410356]\n",
      " [ 0.14348709]\n",
      " [-0.67337182]]\n",
      "t [[ 0.65520137]\n",
      " [-0.73778875]\n",
      " [-0.70750254]\n",
      " ...\n",
      " [ 0.33812358]\n",
      " [ 0.14901612]\n",
      " [-0.78282705]]\n",
      "t [[ 0.65520137]\n",
      " [-0.73778875]\n",
      " [-0.70750254]\n",
      " ...\n",
      " [ 0.33812358]\n",
      " [ 0.14901612]\n",
      " [-0.78282705]]\n",
      "Current iteration=4, loss=38709.576852444516\n",
      "t [[ 0.72877913]\n",
      " [-0.90272921]\n",
      " [-0.82276449]\n",
      " ...\n",
      " [ 0.33904984]\n",
      " [ 0.14670503]\n",
      " [-0.8750941 ]]\n",
      "t [[ 0.72877913]\n",
      " [-0.90272921]\n",
      " [-0.82276449]\n",
      " ...\n",
      " [ 0.33904984]\n",
      " [ 0.14670503]\n",
      " [-0.8750941 ]]\n",
      "t [[ 0.78626317]\n",
      " [-1.05008886]\n",
      " [-0.92508455]\n",
      " ...\n",
      " [ 0.33236304]\n",
      " [ 0.1395849 ]\n",
      " [-0.95574096]]\n",
      "t [[ 0.78626317]\n",
      " [-1.05008886]\n",
      " [-0.92508455]\n",
      " ...\n",
      " [ 0.33236304]\n",
      " [ 0.1395849 ]\n",
      " [-0.95574096]]\n",
      "Current iteration=6, loss=36196.839657899676\n",
      "t [[ 0.83196563]\n",
      " [-1.18229657]\n",
      " [-1.01689542]\n",
      " ...\n",
      " [ 0.32105212]\n",
      " [ 0.12953048]\n",
      " [-1.02768695]]\n",
      "t [[ 0.83196563]\n",
      " [-1.18229657]\n",
      " [-1.01689542]\n",
      " ...\n",
      " [ 0.32105212]\n",
      " [ 0.12953048]\n",
      " [-1.02768695]]\n",
      "t [[ 0.86888731]\n",
      " [-1.30161374]\n",
      " [-1.09997116]\n",
      " ...\n",
      " [ 0.30688833]\n",
      " [ 0.11775032]\n",
      " [-1.09267743]]\n",
      "t [[ 0.86888731]\n",
      " [-1.30161374]\n",
      " [-1.09997116]\n",
      " ...\n",
      " [ 0.30688833]\n",
      " [ 0.11775032]\n",
      " [-1.09267743]]\n",
      "Current iteration=8, loss=34562.135419566395\n",
      "t [[ 0.89917013]\n",
      " [-1.40995459]\n",
      " [-1.17565821]\n",
      " ...\n",
      " [ 0.29098757]\n",
      " [ 0.10504475]\n",
      " [-1.15187587]]\n",
      "t [[ 0.89917013]\n",
      " [-1.40995459]\n",
      " [-1.17565821]\n",
      " ...\n",
      " [ 0.29098757]\n",
      " [ 0.10504475]\n",
      " [-1.15187587]]\n",
      "t [[ 0.92437407]\n",
      " [-1.5088942 ]\n",
      " [-1.24500884]\n",
      " ...\n",
      " [ 0.27408267]\n",
      " [ 0.09195213]\n",
      " [-1.20612329]]\n",
      "loss=33422.8147363282\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.25023362]\n",
      " [-0.14145849]\n",
      " [-0.23560682]\n",
      " ...\n",
      " [ 0.20167602]\n",
      " [ 0.08515227]\n",
      " [-0.34686994]]\n",
      "t [[ 0.25023362]\n",
      " [-0.14145849]\n",
      " [-0.23560682]\n",
      " ...\n",
      " [ 0.20167602]\n",
      " [ 0.08515227]\n",
      " [-0.34686994]]\n",
      "t [[ 0.42516012]\n",
      " [-0.33934625]\n",
      " [-0.42025275]\n",
      " ...\n",
      " [ 0.28907914]\n",
      " [ 0.12749764]\n",
      " [-0.55117723]]\n",
      "t [[ 0.42516012]\n",
      " [-0.33934625]\n",
      " [-0.42025275]\n",
      " ...\n",
      " [ 0.28907914]\n",
      " [ 0.12749764]\n",
      " [-0.55117723]]\n",
      "Current iteration=2, loss=43088.69374182164\n",
      "t [[ 0.55209917]\n",
      " [-0.53508529]\n",
      " [-0.57325043]\n",
      " ...\n",
      " [ 0.32625163]\n",
      " [ 0.14686776]\n",
      " [-0.69427224]]\n",
      "t [[ 0.55209917]\n",
      " [-0.53508529]\n",
      " [-0.57325043]\n",
      " ...\n",
      " [ 0.32625163]\n",
      " [ 0.14686776]\n",
      " [-0.69427224]]\n",
      "t [[ 0.64712353]\n",
      " [-0.71346632]\n",
      " [-0.7043287 ]\n",
      " ...\n",
      " [ 0.33864602]\n",
      " [ 0.15304489]\n",
      " [-0.80686866]]\n",
      "t [[ 0.64712353]\n",
      " [-0.71346632]\n",
      " [-0.7043287 ]\n",
      " ...\n",
      " [ 0.33864602]\n",
      " [ 0.15304489]\n",
      " [-0.80686866]]\n",
      "Current iteration=4, loss=38850.04140290593\n",
      "t [[ 0.71998965]\n",
      " [-0.87274381]\n",
      " [-0.8190143 ]\n",
      " ...\n",
      " [ 0.3376374 ]\n",
      " [ 0.1512772 ]\n",
      " [-0.9015376 ]]\n",
      "t [[ 0.71998965]\n",
      " [-0.87274381]\n",
      " [-0.8190143 ]\n",
      " ...\n",
      " [ 0.3376374 ]\n",
      " [ 0.1512772 ]\n",
      " [-0.9015376 ]]\n",
      "t [[ 0.77698765]\n",
      " [-1.01473812]\n",
      " [-0.92081671]\n",
      " ...\n",
      " [ 0.32886099]\n",
      " [ 0.14462212]\n",
      " [-0.98407728]]\n",
      "t [[ 0.77698765]\n",
      " [-1.01473812]\n",
      " [-0.92081671]\n",
      " ...\n",
      " [ 0.32886099]\n",
      " [ 0.14462212]\n",
      " [-0.98407728]]\n",
      "Current iteration=6, loss=36374.21795928698\n",
      "t [[ 0.82235628]\n",
      " [-1.14189224]\n",
      " [-1.01215374]\n",
      " ...\n",
      " [ 0.31539341]\n",
      " [ 0.13497176]\n",
      " [-1.05754699]]\n",
      "t [[ 0.82235628]\n",
      " [-1.14189224]\n",
      " [-1.01215374]\n",
      " ...\n",
      " [ 0.31539341]\n",
      " [ 0.13497176]\n",
      " [-1.05754699]]\n",
      "t [[ 0.85904747]\n",
      " [-1.25646161]\n",
      " [-1.0947887 ]\n",
      " ...\n",
      " [ 0.29905691]\n",
      " [ 0.12354695]\n",
      " [-1.12378246]]\n",
      "t [[ 0.85904747]\n",
      " [-1.25646161]\n",
      " [-1.0947887 ]\n",
      " ...\n",
      " [ 0.29905691]\n",
      " [ 0.12354695]\n",
      " [-1.12378246]]\n",
      "Current iteration=8, loss=34764.755233811455\n",
      "t [[ 0.88917016]\n",
      " [-1.3603451 ]\n",
      " [-1.17006032]\n",
      " ...\n",
      " [ 0.28099784]\n",
      " [ 0.11115718]\n",
      " [-1.18400874]]\n",
      "t [[ 0.88917016]\n",
      " [-1.3603451 ]\n",
      " [-1.17006032]\n",
      " ...\n",
      " [ 0.28099784]\n",
      " [ 0.11115718]\n",
      " [-1.18400874]]\n",
      "t [[ 0.9142618 ]\n",
      " [-1.45509868]\n",
      " [-1.23901526]\n",
      " ...\n",
      " [ 0.26196752]\n",
      " [ 0.09834778]\n",
      " [-1.23911073]]\n",
      "loss=33643.58885647325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.24983306]\n",
      " [-0.14861626]\n",
      " [-0.23486676]\n",
      " ...\n",
      " [ 0.35983818]\n",
      " [-0.23486676]\n",
      " [ 0.33703943]]\n",
      "t [[ 0.24983306]\n",
      " [-0.14861626]\n",
      " [-0.23486676]\n",
      " ...\n",
      " [ 0.35983818]\n",
      " [-0.23486676]\n",
      " [ 0.33703943]]\n",
      "t [[ 0.42442692]\n",
      " [-0.35064249]\n",
      " [-0.41920581]\n",
      " ...\n",
      " [ 0.59787805]\n",
      " [-0.41920581]\n",
      " [ 0.54032762]]\n",
      "t [[ 0.42442692]\n",
      " [-0.35064249]\n",
      " [-0.41920581]\n",
      " ...\n",
      " [ 0.59787805]\n",
      " [-0.41920581]\n",
      " [ 0.54032762]]\n",
      "Current iteration=2, loss=43088.81523188576\n",
      "t [[ 0.55101471]\n",
      " [-0.5495833 ]\n",
      " [-0.57201232]\n",
      " ...\n",
      " [ 0.76492017]\n",
      " [-0.57201232]\n",
      " [ 0.67290305]]\n",
      "t [[ 0.55101471]\n",
      " [-0.5495833 ]\n",
      " [-0.57201232]\n",
      " ...\n",
      " [ 0.76492017]\n",
      " [-0.57201232]\n",
      " [ 0.67290305]]\n",
      "t [[ 0.64553027]\n",
      " [-0.73091346]\n",
      " [-0.70293723]\n",
      " ...\n",
      " [ 0.88715932]\n",
      " [-0.70293723]\n",
      " [ 0.76427668]]\n",
      "t [[ 0.64553027]\n",
      " [-0.73091346]\n",
      " [-0.70293723]\n",
      " ...\n",
      " [ 0.88715932]\n",
      " [-0.70293723]\n",
      " [ 0.76427668]]\n",
      "Current iteration=4, loss=38836.85851017337\n",
      "t [[ 0.71777017]\n",
      " [-0.89301923]\n",
      " [-0.81748239]\n",
      " ...\n",
      " [ 0.97932585]\n",
      " [-0.81748239]\n",
      " [ 0.8296561 ]]\n",
      "t [[ 0.71777017]\n",
      " [-0.89301923]\n",
      " [-0.81748239]\n",
      " ...\n",
      " [ 0.97932585]\n",
      " [-0.81748239]\n",
      " [ 0.8296561 ]]\n",
      "t [[ 0.77407492]\n",
      " [-1.03772572]\n",
      " [-0.9191511 ]\n",
      " ...\n",
      " [ 1.05042483]\n",
      " [-0.9191511 ]\n",
      " [ 0.87771859]]\n",
      "t [[ 0.77407492]\n",
      " [-1.03772572]\n",
      " [-0.9191511 ]\n",
      " ...\n",
      " [ 1.05042483]\n",
      " [-0.9191511 ]\n",
      " [ 0.87771859]]\n",
      "Current iteration=6, loss=36351.83862710601\n",
      "t [[ 0.81871744]\n",
      " [-1.16746255]\n",
      " [-1.01036057]\n",
      " ...\n",
      " [ 1.1063129 ]\n",
      " [-1.01036057]\n",
      " [ 0.91381877]]\n",
      "t [[ 0.81871744]\n",
      " [-1.16746255]\n",
      " [-1.01036057]\n",
      " ...\n",
      " [ 1.1063129 ]\n",
      " [-1.01036057]\n",
      " [ 0.91381877]]\n",
      "t [[ 0.85467065]\n",
      " [-1.28447657]\n",
      " [-1.09287462]\n",
      " ...\n",
      " [ 1.15097435]\n",
      " [-1.09287462]\n",
      " [ 0.94145355]]\n",
      "t [[ 0.85467065]\n",
      " [-1.28447657]\n",
      " [-1.09287462]\n",
      " ...\n",
      " [ 1.15097435]\n",
      " [-1.09287462]\n",
      " [ 0.94145355]]\n",
      "Current iteration=8, loss=34736.0916171326\n",
      "t [[ 0.88405614]\n",
      " [-1.3906653 ]\n",
      " [-1.16803227]\n",
      " ...\n",
      " [ 1.1872106 ]\n",
      " [-1.16803227]\n",
      " [ 0.96300046]]\n",
      "t [[ 0.88405614]\n",
      " [-1.3906653 ]\n",
      " [-1.16803227]\n",
      " ...\n",
      " [ 1.1872106 ]\n",
      " [-1.16803227]\n",
      " [ 0.96300046]]\n",
      "t [[ 0.9084188 ]\n",
      " [-1.48758864]\n",
      " [-1.23688015]\n",
      " ...\n",
      " [ 1.21704057]\n",
      " [-1.23688015]\n",
      " [ 0.98012288]]\n",
      "loss=33610.66030145165\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08580615]\n",
      " [-0.41088255]\n",
      " [-0.24094625]\n",
      " ...\n",
      " [ 0.20604103]\n",
      " [ 0.08580615]\n",
      " [-0.34926347]]\n",
      "t [[ 0.08580615]\n",
      " [-0.41088255]\n",
      " [-0.24094625]\n",
      " ...\n",
      " [ 0.20604103]\n",
      " [ 0.08580615]\n",
      " [-0.34926347]]\n",
      "t [[ 0.1275739 ]\n",
      " [-0.75173186]\n",
      " [-0.42921552]\n",
      " ...\n",
      " [ 0.29499046]\n",
      " [ 0.1275739 ]\n",
      " [-0.55385712]]\n",
      "t [[ 0.1275739 ]\n",
      " [-0.75173186]\n",
      " [-0.42921552]\n",
      " ...\n",
      " [ 0.29499046]\n",
      " [ 0.1275739 ]\n",
      " [-0.55385712]]\n",
      "Current iteration=2, loss=42877.93621505919\n",
      "t [[ 0.14606721]\n",
      " [-1.03357883]\n",
      " [-0.5849992 ]\n",
      " ...\n",
      " [ 0.3334317 ]\n",
      " [ 0.14606721]\n",
      " [-0.69768135]]\n",
      "t [[ 0.14606721]\n",
      " [-1.03357883]\n",
      " [-0.5849992 ]\n",
      " ...\n",
      " [ 0.3334317 ]\n",
      " [ 0.14606721]\n",
      " [-0.69768135]]\n",
      "t [[ 0.15134107]\n",
      " [-1.26903793]\n",
      " [-0.71832351]\n",
      " ...\n",
      " [ 0.34722798]\n",
      " [ 0.15134107]\n",
      " [-0.8114443 ]]\n",
      "t [[ 0.15134107]\n",
      " [-1.26903793]\n",
      " [-0.71832351]\n",
      " ...\n",
      " [ 0.34722798]\n",
      " [ 0.15134107]\n",
      " [-0.8114443 ]]\n",
      "Current iteration=4, loss=38571.27964199665\n",
      "t [[ 0.14873101]\n",
      " [-1.46843989]\n",
      " [-0.83485848]\n",
      " ...\n",
      " [ 0.34774223]\n",
      " [ 0.14873101]\n",
      " [-0.90744044]]\n",
      "t [[ 0.14873101]\n",
      " [-1.46843989]\n",
      " [-0.83485848]\n",
      " ...\n",
      " [ 0.34774223]\n",
      " [ 0.14873101]\n",
      " [-0.90744044]]\n",
      "t [[ 0.14132312]\n",
      " [-1.63959395]\n",
      " [-0.93820488]\n",
      " ...\n",
      " [ 0.34054952]\n",
      " [ 0.14132312]\n",
      " [-0.99129014]]\n",
      "t [[ 0.14132312]\n",
      " [-1.63959395]\n",
      " [-0.93820488]\n",
      " ...\n",
      " [ 0.34054952]\n",
      " [ 0.14132312]\n",
      " [-0.99129014]]\n",
      "Current iteration=6, loss=36069.574486468046\n",
      "t [[ 0.13101651]\n",
      " [-1.78829909]\n",
      " [-1.03084642]\n",
      " ...\n",
      " [ 0.32868708]\n",
      " [ 0.13101651]\n",
      " [-1.06597224]]\n",
      "t [[ 0.13101651]\n",
      " [-1.78829909]\n",
      " [-1.03084642]\n",
      " ...\n",
      " [ 0.32868708]\n",
      " [ 0.13101651]\n",
      " [-1.06597224]]\n",
      "t [[ 0.11902974]\n",
      " [-1.91888371]\n",
      " [-1.11459586]\n",
      " ...\n",
      " [ 0.31395589]\n",
      " [ 0.11902974]\n",
      " [-1.1332959 ]]\n",
      "t [[ 0.11902974]\n",
      " [-1.91888371]\n",
      " [-1.11459586]\n",
      " ...\n",
      " [ 0.31395589]\n",
      " [ 0.11902974]\n",
      " [-1.1332959 ]]\n",
      "Current iteration=8, loss=34449.57217601515\n",
      "t [[ 0.10616574]\n",
      " [-2.03462177]\n",
      " [-1.1908295 ]\n",
      " ...\n",
      " [ 0.29749072]\n",
      " [ 0.10616574]\n",
      " [-1.19448461]]\n",
      "t [[ 0.10616574]\n",
      " [-2.03462177]\n",
      " [-1.1908295 ]\n",
      " ...\n",
      " [ 0.29749072]\n",
      " [ 0.10616574]\n",
      " [-1.19448461]]\n",
      "t [[ 0.09296144]\n",
      " [-2.1380281 ]\n",
      " [-1.26062281]\n",
      " ...\n",
      " [ 0.28003572]\n",
      " [ 0.09296144]\n",
      " [-1.2504319 ]]\n",
      "loss=33324.51650310999\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.25911545]\n",
      " [-0.15105392]\n",
      " [-0.2417479 ]\n",
      " ...\n",
      " [ 0.2030253 ]\n",
      " [ 0.08546926]\n",
      " [-0.34398513]]\n",
      "t [[ 0.25911545]\n",
      " [-0.15105392]\n",
      " [-0.2417479 ]\n",
      " ...\n",
      " [ 0.2030253 ]\n",
      " [ 0.08546926]\n",
      " [-0.34398513]]\n",
      "t [[ 0.43853904]\n",
      " [-0.36076133]\n",
      " [-0.43026319]\n",
      " ...\n",
      " [ 0.28978766]\n",
      " [ 0.1267582 ]\n",
      " [-0.54339239]]\n",
      "t [[ 0.43853904]\n",
      " [-0.36076133]\n",
      " [-0.43026319]\n",
      " ...\n",
      " [ 0.28978766]\n",
      " [ 0.1267582 ]\n",
      " [-0.54339239]]\n",
      "Current iteration=2, loss=42864.791709412806\n",
      "t [[ 0.56775531]\n",
      " [-0.56694209]\n",
      " [-0.58604432]\n",
      " ...\n",
      " [ 0.3266898 ]\n",
      " [ 0.14475219]\n",
      " [-0.68268996]]\n",
      "t [[ 0.56775531]\n",
      " [-0.56694209]\n",
      " [-0.58604432]\n",
      " ...\n",
      " [ 0.3266898 ]\n",
      " [ 0.14475219]\n",
      " [-0.68268996]]\n",
      "t [[ 0.66383747]\n",
      " [-0.75420468]\n",
      " [-0.71925589]\n",
      " ...\n",
      " [ 0.33942579]\n",
      " [ 0.14954876]\n",
      " [-0.79263593]]\n",
      "t [[ 0.66383747]\n",
      " [-0.75420468]\n",
      " [-0.71925589]\n",
      " ...\n",
      " [ 0.33942579]\n",
      " [ 0.14954876]\n",
      " [-0.79263593]]\n",
      "Current iteration=4, loss=38558.53055172592\n",
      "t [[ 0.73707455]\n",
      " [-0.9211357 ]\n",
      " [-0.83563028]\n",
      " ...\n",
      " [ 0.33921798]\n",
      " [ 0.14649771]\n",
      " [-0.88543044]]\n",
      "t [[ 0.73707455]\n",
      " [-0.9211357 ]\n",
      " [-0.83563028]\n",
      " ...\n",
      " [ 0.33921798]\n",
      " [ 0.14649771]\n",
      " [-0.88543044]]\n",
      "t [[ 0.79406071]\n",
      " [-1.06984995]\n",
      " [-0.9387973 ]\n",
      " ...\n",
      " [ 0.33154114]\n",
      " [ 0.13868869]\n",
      " [-0.96658069]]\n",
      "t [[ 0.79406071]\n",
      " [-1.06984995]\n",
      " [-0.9387973 ]\n",
      " ...\n",
      " [ 0.33154114]\n",
      " [ 0.13868869]\n",
      " [-0.96658069]]\n",
      "Current iteration=6, loss=36057.310740828034\n",
      "t [[ 0.83921268]\n",
      " [-1.20299597]\n",
      " [-1.03125463]\n",
      " ...\n",
      " [ 0.3193644 ]\n",
      " [ 0.12802017]\n",
      " [-1.03896741]]\n",
      "t [[ 0.83921268]\n",
      " [-1.20299597]\n",
      " [-1.03125463]\n",
      " ...\n",
      " [ 0.3193644 ]\n",
      " [ 0.12802017]\n",
      " [-1.03896741]]\n",
      "t [[ 0.87558826]\n",
      " [-1.3229699 ]\n",
      " [-1.11482194]\n",
      " ...\n",
      " [ 0.30444283]\n",
      " [ 0.11570848]\n",
      " [-1.10432179]]\n",
      "t [[ 0.87558826]\n",
      " [-1.3229699 ]\n",
      " [-1.11482194]\n",
      " ...\n",
      " [ 0.30444283]\n",
      " [ 0.11570848]\n",
      " [-1.10432179]]\n",
      "Current iteration=8, loss=34437.27003379026\n",
      "t [[ 0.90535971]\n",
      " [-1.43177019]\n",
      " [-1.19087897]\n",
      " ...\n",
      " [ 0.2878799 ]\n",
      " [ 0.1025538 ]\n",
      " [-1.16380683]]\n",
      "t [[ 0.90535971]\n",
      " [-1.43177019]\n",
      " [-1.19087897]\n",
      " ...\n",
      " [ 0.2878799 ]\n",
      " [ 0.1025538 ]\n",
      " [-1.16380683]]\n",
      "t [[ 0.93010198]\n",
      " [-1.5310265 ]\n",
      " [-1.26050292]\n",
      " ...\n",
      " [ 0.27039799]\n",
      " [ 0.08909019]\n",
      " [-1.21826916]]\n",
      "loss=33311.79092187097\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.25579437]\n",
      " [-0.14460201]\n",
      " [-0.24084252]\n",
      " ...\n",
      " [ 0.20615771]\n",
      " [ 0.08704454]\n",
      " [-0.35457816]]\n",
      "t [[ 0.25579437]\n",
      " [-0.14460201]\n",
      " [-0.24084252]\n",
      " ...\n",
      " [ 0.20615771]\n",
      " [ 0.08704454]\n",
      " [-0.35457816]]\n",
      "t [[ 0.43294905]\n",
      " [-0.3481239 ]\n",
      " [-0.42846492]\n",
      " ...\n",
      " [ 0.29298482]\n",
      " [ 0.12938801]\n",
      " [-0.56028826]]\n",
      "t [[ 0.43294905]\n",
      " [-0.3481239 ]\n",
      " [-0.42846492]\n",
      " ...\n",
      " [ 0.29298482]\n",
      " [ 0.12938801]\n",
      " [-0.56028826]]\n",
      "Current iteration=2, loss=42949.48728737853\n",
      "t [[ 0.56062129]\n",
      " [-0.5481251 ]\n",
      " [-0.58347552]\n",
      " ...\n",
      " [ 0.32877327]\n",
      " [ 0.14818685]\n",
      " [-0.70388246]]\n",
      "t [[ 0.56062129]\n",
      " [-0.5481251 ]\n",
      " [-0.58347552]\n",
      " ...\n",
      " [ 0.32877327]\n",
      " [ 0.14818685]\n",
      " [-0.70388246]]\n",
      "t [[ 0.65566467]\n",
      " [-0.72935642]\n",
      " [-0.71602379]\n",
      " ...\n",
      " [ 0.33980747]\n",
      " [ 0.15363584]\n",
      " [-0.81696425]]\n",
      "t [[ 0.65566467]\n",
      " [-0.72935642]\n",
      " [-0.71602379]\n",
      " ...\n",
      " [ 0.33980747]\n",
      " [ 0.15363584]\n",
      " [-0.81696425]]\n",
      "Current iteration=4, loss=38701.1220447103\n",
      "t [[ 0.72820271]\n",
      " [-0.89051909]\n",
      " [-0.8318156 ]\n",
      " ...\n",
      " [ 0.33759367]\n",
      " [ 0.15113076]\n",
      " [-0.91214906]]\n",
      "t [[ 0.72820271]\n",
      " [-0.89051909]\n",
      " [-0.8318156 ]\n",
      " ...\n",
      " [ 0.33759367]\n",
      " [ 0.15113076]\n",
      " [-0.91214906]]\n",
      "t [[ 0.78471594]\n",
      " [-1.03378003]\n",
      " [-0.9344596 ]\n",
      " ...\n",
      " [ 0.32776293]\n",
      " [ 0.14378821]\n",
      " [-0.9951779 ]]\n",
      "t [[ 0.78471594]\n",
      " [-1.03378003]\n",
      " [-0.9344596 ]\n",
      " ...\n",
      " [ 0.32776293]\n",
      " [ 0.14378821]\n",
      " [-0.9951779 ]]\n",
      "Current iteration=6, loss=36236.80049539655\n",
      "t [[ 0.82954612]\n",
      " [-1.16179987]\n",
      " [-1.02643803]\n",
      " ...\n",
      " [ 0.31337193]\n",
      " [ 0.13352441]\n",
      " [-1.06907284]]\n",
      "t [[ 0.82954612]\n",
      " [-1.16179987]\n",
      " [-1.02643803]\n",
      " ...\n",
      " [ 0.31337193]\n",
      " [ 0.13352441]\n",
      " [-1.06907284]]\n",
      "t [[ 0.86570161]\n",
      " [-1.27696639]\n",
      " [-1.10955958]\n",
      " ...\n",
      " [ 0.29622614]\n",
      " [ 0.12156826]\n",
      " [-1.1356561 ]]\n",
      "t [[ 0.86570161]\n",
      " [-1.27696639]\n",
      " [-1.10955958]\n",
      " ...\n",
      " [ 0.29622614]\n",
      " [ 0.12156826]\n",
      " [-1.1356561 ]]\n",
      "Current iteration=8, loss=34641.86264879927\n",
      "t [[ 0.89532157]\n",
      " [-1.38126052]\n",
      " [-1.18519616]\n",
      " ...\n",
      " [ 0.27745883]\n",
      " [ 0.10872923]\n",
      " [-1.19615264]]\n",
      "t [[ 0.89532157]\n",
      " [-1.38126052]\n",
      " [-1.18519616]\n",
      " ...\n",
      " [ 0.27745883]\n",
      " [ 0.10872923]\n",
      " [-1.19615264]]\n",
      "t [[ 0.91995846]\n",
      " [-1.47629098]\n",
      " [-1.25441931]\n",
      " ...\n",
      " [ 0.25781028]\n",
      " [ 0.09554853]\n",
      " [-1.2514532 ]]\n",
      "loss=33534.36946819996\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.25538491]\n",
      " [-0.15191884]\n",
      " [-0.24008602]\n",
      " ...\n",
      " [ 0.36783458]\n",
      " [-0.24008602]\n",
      " [ 0.34452919]]\n",
      "t [[ 0.25538491]\n",
      " [-0.15191884]\n",
      " [-0.24008602]\n",
      " ...\n",
      " [ 0.36783458]\n",
      " [-0.24008602]\n",
      " [ 0.34452919]]\n",
      "t [[ 0.4321981 ]\n",
      " [-0.35960778]\n",
      " [-0.4274038 ]\n",
      " ...\n",
      " [ 0.60847668]\n",
      " [-0.4274038 ]\n",
      " [ 0.5493866 ]]\n",
      "t [[ 0.4321981 ]\n",
      " [-0.35960778]\n",
      " [-0.4274038 ]\n",
      " ...\n",
      " [ 0.60847668]\n",
      " [-0.4274038 ]\n",
      " [ 0.5493866 ]]\n",
      "Current iteration=2, loss=42949.29952224024\n",
      "t [[ 0.55950352]\n",
      " [-0.56284763]\n",
      " [-0.58222403]\n",
      " ...\n",
      " [ 0.77613034]\n",
      " [-0.58222403]\n",
      " [ 0.6818214 ]]\n",
      "t [[ 0.55950352]\n",
      " [-0.56284763]\n",
      " [-0.58222403]\n",
      " ...\n",
      " [ 0.77613034]\n",
      " [-0.58222403]\n",
      " [ 0.6818214 ]]\n",
      "t [[ 0.65401618]\n",
      " [-0.74707868]\n",
      " [-0.71461788]\n",
      " ...\n",
      " [ 0.89814542]\n",
      " [-0.71461788]\n",
      " [ 0.77252006]]\n",
      "t [[ 0.65401618]\n",
      " [-0.74707868]\n",
      " [-0.71461788]\n",
      " ...\n",
      " [ 0.89814542]\n",
      " [-0.71461788]\n",
      " [ 0.77252006]]\n",
      "Current iteration=4, loss=38687.43455437351\n",
      "t [[ 0.72590565]\n",
      " [-0.91112186]\n",
      " [-0.83026724]\n",
      " ...\n",
      " [ 0.98971879]\n",
      " [-0.83026724]\n",
      " [ 0.83706824]]\n",
      "t [[ 0.72590565]\n",
      " [-0.91112186]\n",
      " [-0.83026724]\n",
      " ...\n",
      " [ 0.98971879]\n",
      " [-0.83026724]\n",
      " [ 0.83706824]]\n",
      "t [[ 0.78170487]\n",
      " [-1.05714214]\n",
      " [-0.93277538]\n",
      " ...\n",
      " [ 1.06007555]\n",
      " [-0.93277538]\n",
      " [ 0.8842897 ]]\n",
      "t [[ 0.78170487]\n",
      " [-1.05714214]\n",
      " [-0.93277538]\n",
      " ...\n",
      " [ 1.06007555]\n",
      " [-0.93277538]\n",
      " [ 0.8842897 ]]\n",
      "Current iteration=6, loss=36213.89612244697\n",
      "t [[ 0.8257902 ]\n",
      " [-1.18778454]\n",
      " [-1.02462436]\n",
      " ...\n",
      " [ 1.11518604]\n",
      " [-1.02462436]\n",
      " [ 0.9196038 ]]\n",
      "t [[ 0.8257902 ]\n",
      " [-1.18778454]\n",
      " [-1.02462436]\n",
      " ...\n",
      " [ 1.11518604]\n",
      " [-1.02462436]\n",
      " [ 0.9196038 ]]\n",
      "t [[ 0.8611909 ]\n",
      " [-1.3054283 ]\n",
      " [-1.10762347]\n",
      " ...\n",
      " [ 1.15909518]\n",
      " [-1.10762347]\n",
      " [ 0.9465367 ]]\n",
      "t [[ 0.8611909 ]\n",
      " [-1.3054283 ]\n",
      " [-1.10762347]\n",
      " ...\n",
      " [ 1.15909518]\n",
      " [-1.10762347]\n",
      " [ 0.9465367 ]]\n",
      "Current iteration=8, loss=34612.717195713514\n",
      "t [[ 0.89005854]\n",
      " [-1.41205383]\n",
      " [-1.1831449 ]\n",
      " ...\n",
      " [ 1.19463586]\n",
      " [-1.1831449 ]\n",
      " [ 0.96747713]]\n",
      "t [[ 0.89005854]\n",
      " [-1.41205383]\n",
      " [-1.1831449 ]\n",
      " ...\n",
      " [ 1.19463586]\n",
      " [-1.1831449 ]\n",
      " [ 0.96747713]]\n",
      "t [[ 0.91395278]\n",
      " [-1.50927485]\n",
      " [-1.2522601 ]\n",
      " ...\n",
      " [ 1.22384162]\n",
      " [-1.2522601 ]\n",
      " [ 0.98408971]]\n",
      "loss=33501.03359271672\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0876715 ]\n",
      " [-0.41981478]\n",
      " [-0.24618421]\n",
      " ...\n",
      " [ 0.21052018]\n",
      " [ 0.0876715 ]\n",
      " [-0.35685615]]\n",
      "t [[ 0.0876715 ]\n",
      " [-0.41981478]\n",
      " [-0.24618421]\n",
      " ...\n",
      " [ 0.21052018]\n",
      " [ 0.0876715 ]\n",
      " [-0.35685615]]\n",
      "t [[ 0.12939933]\n",
      " [-0.76654932]\n",
      " [-0.43740796]\n",
      " ...\n",
      " [ 0.29887947]\n",
      " [ 0.12939933]\n",
      " [-0.56278472]]\n",
      "t [[ 0.12939933]\n",
      " [-0.76654932]\n",
      " [-0.43740796]\n",
      " ...\n",
      " [ 0.29887947]\n",
      " [ 0.12939933]\n",
      " [-0.56278472]]\n",
      "Current iteration=2, loss=42738.959936482184\n",
      "t [[ 0.14730302]\n",
      " [-1.05199395]\n",
      " [-0.59518682]\n",
      " ...\n",
      " [ 0.33598381]\n",
      " [ 0.14730302]\n",
      " [-0.70713189]]\n",
      "t [[ 0.14730302]\n",
      " [-1.05199395]\n",
      " [-0.59518682]\n",
      " ...\n",
      " [ 0.33598381]\n",
      " [ 0.14730302]\n",
      " [-0.70713189]]\n",
      "t [[ 0.15184395]\n",
      " [-1.28959541]\n",
      " [-0.72996458]\n",
      " ...\n",
      " [ 0.34848997]\n",
      " [ 0.15184395]\n",
      " [-0.82142255]]\n",
      "t [[ 0.15184395]\n",
      " [-1.28959541]\n",
      " [-0.72996458]\n",
      " ...\n",
      " [ 0.34848997]\n",
      " [ 0.15184395]\n",
      " [-0.82142255]]\n",
      "Current iteration=4, loss=38423.56660973492\n",
      "t [[ 0.14849973]\n",
      " [-1.49024082]\n",
      " [-0.84758895]\n",
      " ...\n",
      " [ 0.34786933]\n",
      " [ 0.14849973]\n",
      " [-0.91796594]]\n",
      "t [[ 0.14849973]\n",
      " [-1.49024082]\n",
      " [-0.84758895]\n",
      " ...\n",
      " [ 0.34786933]\n",
      " [ 0.14849973]\n",
      " [-0.91796594]]\n",
      "t [[ 0.14041257]\n",
      " [-1.66207737]\n",
      " [-0.95176023]\n",
      " ...\n",
      " [ 0.33968658]\n",
      " [ 0.14041257]\n",
      " [-1.00232137]]\n",
      "t [[ 0.14041257]\n",
      " [-1.66207737]\n",
      " [-0.95176023]\n",
      " ...\n",
      " [ 0.33968658]\n",
      " [ 0.14041257]\n",
      " [-1.00232137]]\n",
      "Current iteration=6, loss=35933.88415949715\n",
      "t [[ 0.12950385]\n",
      " [-1.81110735]\n",
      " [-1.04502728]\n",
      " ...\n",
      " [ 0.32695875]\n",
      " [ 0.12950385]\n",
      " [-1.07743404]]\n",
      "t [[ 0.12950385]\n",
      " [-1.81110735]\n",
      " [-1.04502728]\n",
      " ...\n",
      " [ 0.32695875]\n",
      " [ 0.12950385]\n",
      " [-1.07743404]]\n",
      "t [[ 0.11699895]\n",
      " [-1.94178382]\n",
      " [-1.12924871]\n",
      " ...\n",
      " [ 0.31147098]\n",
      " [ 0.11699895]\n",
      " [-1.14510368]]\n",
      "t [[ 0.11699895]\n",
      " [-1.94178382]\n",
      " [-1.12924871]\n",
      " ...\n",
      " [ 0.31147098]\n",
      " [ 0.11699895]\n",
      " [-1.14510368]]\n",
      "Current iteration=8, loss=34328.6367707679\n",
      "t [[ 0.10369991]\n",
      " [-2.05745918]\n",
      " [-1.20583425]\n",
      " ...\n",
      " [ 0.29434566]\n",
      " [ 0.10369991]\n",
      " [-1.20655667]]\n",
      "t [[ 0.10369991]\n",
      " [-2.05745918]\n",
      " [-1.20583425]\n",
      " ...\n",
      " [ 0.29434566]\n",
      " [ 0.10369991]\n",
      " [-1.20655667]]\n",
      "t [[ 0.09013882]\n",
      " [-2.16069883]\n",
      " [-1.27588429]\n",
      " ...\n",
      " [ 0.27631637]\n",
      " [ 0.09013882]\n",
      " [-1.26269436]]\n",
      "loss=33217.3390332931\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.26474839]\n",
      " [-0.1543377 ]\n",
      " [-0.24700329]\n",
      " ...\n",
      " [ 0.20743889]\n",
      " [ 0.08732729]\n",
      " [-0.35146307]]\n",
      "t [[ 0.26474839]\n",
      " [-0.1543377 ]\n",
      " [-0.24700329]\n",
      " ...\n",
      " [ 0.20743889]\n",
      " [ 0.08732729]\n",
      " [-0.35146307]]\n",
      "t [[ 0.44635817]\n",
      " [-0.36985903]\n",
      " [-0.43846668]\n",
      " ...\n",
      " [ 0.29358117]\n",
      " [ 0.128563  ]\n",
      " [-0.55209508]]\n",
      "t [[ 0.44635817]\n",
      " [-0.36985903]\n",
      " [-0.43846668]\n",
      " ...\n",
      " [ 0.29358117]\n",
      " [ 0.128563  ]\n",
      " [-0.55209508]]\n",
      "Current iteration=2, loss=42725.778229088646\n",
      "t [[ 0.57624905]\n",
      " [-0.58038054]\n",
      " [-0.59623243]\n",
      " ...\n",
      " [ 0.32914078]\n",
      " [ 0.14595556]\n",
      " [-0.69184549]]\n",
      "t [[ 0.57624905]\n",
      " [-0.58038054]\n",
      " [-0.59623243]\n",
      " ...\n",
      " [ 0.32914078]\n",
      " [ 0.14595556]\n",
      " [-0.69184549]]\n",
      "t [[ 0.67229645]\n",
      " [-0.77052923]\n",
      " [-0.73088788]\n",
      " ...\n",
      " [ 0.34059415]\n",
      " [ 0.15001008]\n",
      " [-0.80228134]]\n",
      "t [[ 0.67229645]\n",
      " [-0.77052923]\n",
      " [-0.73088788]\n",
      " ...\n",
      " [ 0.34059415]\n",
      " [ 0.15001008]\n",
      " [-0.80228134]]\n",
      "Current iteration=4, loss=38410.85196270191\n",
      "t [[ 0.74516301]\n",
      " [-0.93936913]\n",
      " [-0.84834395]\n",
      " ...\n",
      " [ 0.3392646 ]\n",
      " [ 0.14621829]\n",
      " [-0.89560583]]\n",
      "t [[ 0.74516301]\n",
      " [-0.93936913]\n",
      " [-0.84834395]\n",
      " ...\n",
      " [ 0.3392646 ]\n",
      " [ 0.14621829]\n",
      " [-0.89560583]]\n",
      "t [[ 0.80163405]\n",
      " [-1.08937061]\n",
      " [-0.95232979]\n",
      " ...\n",
      " [ 0.33061275]\n",
      " [ 0.1377255 ]\n",
      " [-0.97725681]]\n",
      "t [[ 0.80163405]\n",
      " [-1.08937061]\n",
      " [-0.95232979]\n",
      " ...\n",
      " [ 0.33061275]\n",
      " [ 0.1377255 ]\n",
      " [-0.97725681]]\n",
      "Current iteration=6, loss=35921.64093001415\n",
      "t [[ 0.84622804]\n",
      " [-1.22340122]\n",
      " [-1.04540786]\n",
      " ...\n",
      " [ 0.31758581]\n",
      " [ 0.12645206]\n",
      " [-1.05007629]]\n",
      "t [[ 0.84622804]\n",
      " [-1.22340122]\n",
      " [-1.04540786]\n",
      " ...\n",
      " [ 0.31758581]\n",
      " [ 0.12645206]\n",
      " [-1.05007629]]\n",
      "t [[ 0.88205734]\n",
      " [-1.34398907]\n",
      " [-1.1294434 ]\n",
      " ...\n",
      " [ 0.30192239]\n",
      " [ 0.11362081]\n",
      " [-1.11578312]]\n",
      "t [[ 0.88205734]\n",
      " [-1.34398907]\n",
      " [-1.1294434 ]\n",
      " ...\n",
      " [ 0.30192239]\n",
      " [ 0.11362081]\n",
      " [-1.11578312]]\n",
      "Current iteration=8, loss=34316.31722812872\n",
      "t [[ 0.91132257]\n",
      " [-1.45321422]\n",
      " [-1.20584937]\n",
      " ...\n",
      " [ 0.28471339]\n",
      " [ 0.10003076]\n",
      " [-1.17554147]]\n",
      "t [[ 0.91132257]\n",
      " [-1.45321422]\n",
      " [-1.20584937]\n",
      " ...\n",
      " [ 0.28471339]\n",
      " [ 0.10003076]\n",
      " [-1.17554147]]\n",
      "t [[ 0.93561204]\n",
      " [-1.55275894]\n",
      " [-1.27572771]\n",
      " ...\n",
      " [ 0.26667065]\n",
      " [ 0.08621091]\n",
      " [-1.23020473]]\n",
      "loss=33204.55640471365\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.26135512]\n",
      " [-0.14774553]\n",
      " [-0.24607823]\n",
      " ...\n",
      " [ 0.2106394 ]\n",
      " [ 0.08893682]\n",
      " [-0.36228638]]\n",
      "t [[ 0.26135512]\n",
      " [-0.14774553]\n",
      " [-0.24607823]\n",
      " ...\n",
      " [ 0.2106394 ]\n",
      " [ 0.08893682]\n",
      " [-0.36228638]]\n",
      "t [[ 0.44066917]\n",
      " [-0.35695242]\n",
      " [-0.4366301 ]\n",
      " ...\n",
      " [ 0.29678608]\n",
      " [ 0.13123929]\n",
      " [-0.56926943]]\n",
      "t [[ 0.44066917]\n",
      " [-0.35695242]\n",
      " [-0.4366301 ]\n",
      " ...\n",
      " [ 0.29678608]\n",
      " [ 0.13123929]\n",
      " [-0.56926943]]\n",
      "Current iteration=2, loss=42812.15144573647\n",
      "t [[ 0.56901354]\n",
      " [-0.56116047]\n",
      " [-0.59361375]\n",
      " ...\n",
      " [ 0.33115639]\n",
      " [ 0.14944356]\n",
      " [-0.71332485]]\n",
      "t [[ 0.56901354]\n",
      " [-0.56116047]\n",
      " [-0.59361375]\n",
      " ...\n",
      " [ 0.33115639]\n",
      " [ 0.14944356]\n",
      " [-0.71332485]]\n",
      "t [[ 0.66403127]\n",
      " [-0.74515581]\n",
      " [-0.72759804]\n",
      " ...\n",
      " [ 0.34083166]\n",
      " [ 0.15415462]\n",
      " [-0.82689015]]\n",
      "t [[ 0.66403127]\n",
      " [-0.74515581]\n",
      " [-0.72759804]\n",
      " ...\n",
      " [ 0.34083166]\n",
      " [ 0.15415462]\n",
      " [-0.82689015]]\n",
      "Current iteration=4, loss=38555.53298278329\n",
      "t [[ 0.7362117 ]\n",
      " [-0.90812334]\n",
      " [-0.84446545]\n",
      " ...\n",
      " [ 0.33742541]\n",
      " [ 0.15091121]\n",
      " [-0.9225928 ]]\n",
      "t [[ 0.7362117 ]\n",
      " [-0.90812334]\n",
      " [-0.84446545]\n",
      " ...\n",
      " [ 0.33742541]\n",
      " [ 0.15091121]\n",
      " [-0.9225928 ]]\n",
      "t [[ 0.79222292]\n",
      " [-1.05258526]\n",
      " [-0.94792294]\n",
      " ...\n",
      " [ 0.32655603]\n",
      " [ 0.14288617]\n",
      " [-1.00610779]]\n",
      "t [[ 0.79222292]\n",
      " [-1.05258526]\n",
      " [-0.94792294]\n",
      " ...\n",
      " [ 0.32655603]\n",
      " [ 0.14288617]\n",
      " [-1.00610779]]\n",
      "Current iteration=6, loss=36103.19232210803\n",
      "t [[ 0.83650703]\n",
      " [-1.18141897]\n",
      " [-1.0405171 ]\n",
      " ...\n",
      " [ 0.31125804]\n",
      " [ 0.13201802]\n",
      " [-1.08041977]]\n",
      "t [[ 0.83650703]\n",
      " [-1.18141897]\n",
      " [-1.0405171 ]\n",
      " ...\n",
      " [ 0.31125804]\n",
      " [ 0.13201802]\n",
      " [-1.08041977]]\n",
      "t [[ 0.87212643]\n",
      " [-1.29714169]\n",
      " [-1.1241019 ]\n",
      " ...\n",
      " [ 0.29331978]\n",
      " [ 0.1195424 ]\n",
      " [-1.14733923]]\n",
      "t [[ 0.87212643]\n",
      " [-1.29714169]\n",
      " [-1.1241019 ]\n",
      " ...\n",
      " [ 0.29331978]\n",
      " [ 0.1195424 ]\n",
      " [-1.14733923]]\n",
      "Current iteration=8, loss=34522.827715014486\n",
      "t [[ 0.90124853]\n",
      " [-1.40181371]\n",
      " [-1.20008238]\n",
      " ...\n",
      " [ 0.27386124]\n",
      " [ 0.10626782]\n",
      " [-1.20809264]]\n",
      "t [[ 0.90124853]\n",
      " [-1.40181371]\n",
      " [-1.20008238]\n",
      " ...\n",
      " [ 0.27386124]\n",
      " [ 0.10626782]\n",
      " [-1.20809264]]\n",
      "t [[ 0.92543922]\n",
      " [-1.49709448]\n",
      " [-1.26955478]\n",
      " ...\n",
      " [ 0.25361152]\n",
      " [ 0.09273055]\n",
      " [-1.26357775]]\n",
      "loss=33428.883088781695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.26093675]\n",
      " [-0.15522143]\n",
      " [-0.24530528]\n",
      " ...\n",
      " [ 0.37583098]\n",
      " [-0.24530528]\n",
      " [ 0.35201896]]\n",
      "t [[ 0.26093675]\n",
      " [-0.15522143]\n",
      " [-0.24530528]\n",
      " ...\n",
      " [ 0.37583098]\n",
      " [-0.24530528]\n",
      " [ 0.35201896]]\n",
      "t [[ 0.43990024]\n",
      " [-0.36862149]\n",
      " [-0.43555515]\n",
      " ...\n",
      " [ 0.61896361]\n",
      " [-0.43555515]\n",
      " [ 0.55832324]]\n",
      "t [[ 0.43990024]\n",
      " [-0.36862149]\n",
      " [-0.43555515]\n",
      " ...\n",
      " [ 0.61896361]\n",
      " [-0.43555515]\n",
      " [ 0.55832324]]\n",
      "Current iteration=2, loss=42811.651141650524\n",
      "t [[ 0.56786173]\n",
      " [-0.57610564]\n",
      " [-0.59234918]\n",
      " ...\n",
      " [ 0.78714711]\n",
      " [-0.59234918]\n",
      " [ 0.69054724]]\n",
      "t [[ 0.56786173]\n",
      " [-0.57610564]\n",
      " [-0.59234918]\n",
      " ...\n",
      " [ 0.78714711]\n",
      " [-0.59234918]\n",
      " [ 0.69054724]]\n",
      "t [[ 0.66232667]\n",
      " [-0.76315209]\n",
      " [-0.72617787]\n",
      " ...\n",
      " [ 0.90888607]\n",
      " [-0.72617787]\n",
      " [ 0.78053756]]\n",
      "t [[ 0.66232667]\n",
      " [-0.76315209]\n",
      " [-0.72617787]\n",
      " ...\n",
      " [ 0.90888607]\n",
      " [-0.72617787]\n",
      " [ 0.78053756]]\n",
      "Current iteration=4, loss=38541.34765630894\n",
      "t [[ 0.73383629]\n",
      " [-0.92905241]\n",
      " [-0.84290074]\n",
      " ...\n",
      " [ 0.999835  ]\n",
      " [-0.84290074]\n",
      " [ 0.84424111]]\n",
      "t [[ 0.73383629]\n",
      " [-0.92905241]\n",
      " [-0.84290074]\n",
      " ...\n",
      " [ 0.999835  ]\n",
      " [-0.84290074]\n",
      " [ 0.84424111]]\n",
      "t [[ 0.78911299]\n",
      " [-1.07632025]\n",
      " [-0.94622023]\n",
      " ...\n",
      " [ 1.06943315]\n",
      " [-0.94622023]\n",
      " [ 0.89061997]]\n",
      "t [[ 0.78911299]\n",
      " [-1.07632025]\n",
      " [-0.94622023]\n",
      " ...\n",
      " [ 1.06943315]\n",
      " [-0.94622023]\n",
      " [ 0.89061997]]\n",
      "Current iteration=6, loss=36079.77444248475\n",
      "t [[ 0.83263381]\n",
      " [-1.20781545]\n",
      " [-1.03868309]\n",
      " ...\n",
      " [ 1.12376071]\n",
      " [-1.03868309]\n",
      " [ 0.92515425]]\n",
      "t [[ 0.83263381]\n",
      " [-1.20781545]\n",
      " [-1.03868309]\n",
      " ...\n",
      " [ 1.12376071]\n",
      " [-1.03868309]\n",
      " [ 0.92515425]]\n",
      "t [[ 0.86748193]\n",
      " [-1.32604687]\n",
      " [-1.12214398]\n",
      " ...\n",
      " [ 1.16692035]\n",
      " [-1.12214398]\n",
      " [ 0.95139698]]\n",
      "t [[ 0.86748193]\n",
      " [-1.32604687]\n",
      " [-1.12214398]\n",
      " ...\n",
      " [ 1.16692035]\n",
      " [-1.12214398]\n",
      " [ 0.95139698]]\n",
      "Current iteration=8, loss=34493.21547220261\n",
      "t [[ 0.8958369 ]\n",
      " [-1.43307534]\n",
      " [-1.19800819]\n",
      " ...\n",
      " [ 1.2017742 ]\n",
      " [-1.19800819]\n",
      " [ 0.97174636]]\n",
      "t [[ 0.8958369 ]\n",
      " [-1.43307534]\n",
      " [-1.19800819]\n",
      " ...\n",
      " [ 1.2017742 ]\n",
      " [-1.19800819]\n",
      " [ 0.97174636]]\n",
      "t [[ 0.91927156]\n",
      " [-1.53056644]\n",
      " [-1.26737182]\n",
      " ...\n",
      " [ 1.23036861]\n",
      " [-1.26737182]\n",
      " [ 0.98786686]]\n",
      "loss=33395.15708334788\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08953685]\n",
      " [-0.42874701]\n",
      " [-0.25142217]\n",
      " ...\n",
      " [ 0.21499933]\n",
      " [ 0.08953685]\n",
      " [-0.36444883]]\n",
      "t [[ 0.08953685]\n",
      " [-0.42874701]\n",
      " [-0.25142217]\n",
      " ...\n",
      " [ 0.21499933]\n",
      " [ 0.08953685]\n",
      " [-0.36444883]]\n",
      "t [[ 0.13118633]\n",
      " [-0.78130388]\n",
      " [-0.44555397]\n",
      " ...\n",
      " [ 0.30266605]\n",
      " [ 0.13118633]\n",
      " [-0.57158624]]\n",
      "t [[ 0.13118633]\n",
      " [-0.78130388]\n",
      " [-0.44555397]\n",
      " ...\n",
      " [ 0.30266605]\n",
      " [ 0.13118633]\n",
      " [-0.57158624]]\n",
      "Current iteration=2, loss=42601.82876344152\n",
      "t [[ 0.14847842]\n",
      " [-1.07025415]\n",
      " [-0.6052892 ]\n",
      " ...\n",
      " [ 0.33840276]\n",
      " [ 0.14847842]\n",
      " [-0.7164228 ]]\n",
      "t [[ 0.14847842]\n",
      " [-1.07025415]\n",
      " [-0.6052892 ]\n",
      " ...\n",
      " [ 0.33840276]\n",
      " [ 0.14847842]\n",
      " [-0.7164228 ]]\n",
      "t [[ 0.15227767]\n",
      " [-1.30990985]\n",
      " [-0.74148728]\n",
      " ...\n",
      " [ 0.3496214 ]\n",
      " [ 0.15227767]\n",
      " [-0.83124045]]\n",
      "t [[ 0.15227767]\n",
      " [-1.30990985]\n",
      " [-0.74148728]\n",
      " ...\n",
      " [ 0.3496214 ]\n",
      " [ 0.15227767]\n",
      " [-0.83124045]]\n",
      "Current iteration=4, loss=38279.12001243868\n",
      "t [[ 0.14819902]\n",
      " [-1.51172518]\n",
      " [-0.86017109]\n",
      " ...\n",
      " [ 0.34787833]\n",
      " [ 0.14819902]\n",
      " [-0.92833218]]\n",
      "t [[ 0.14819902]\n",
      " [-1.51172518]\n",
      " [-0.86017109]\n",
      " ...\n",
      " [ 0.34787833]\n",
      " [ 0.14819902]\n",
      " [-0.92833218]]\n",
      "t [[ 0.13943795]\n",
      " [-1.68418599]\n",
      " [-0.96513978]\n",
      " ...\n",
      " [ 0.33872032]\n",
      " [ 0.13943795]\n",
      " [-1.01318899]]\n",
      "t [[ 0.13943795]\n",
      " [-1.68418599]\n",
      " [-0.96513978]\n",
      " ...\n",
      " [ 0.33872032]\n",
      " [ 0.13943795]\n",
      " [-1.01318899]]\n",
      "Current iteration=6, loss=35801.909157238224\n",
      "t [[ 0.12793637]\n",
      " [-1.83349556]\n",
      " [-1.0590073 ]\n",
      " ...\n",
      " [ 0.32514243]\n",
      " [ 0.12793637]\n",
      " [-1.08872289]]\n",
      "t [[ 0.12793637]\n",
      " [-1.83349556]\n",
      " [-1.0590073 ]\n",
      " ...\n",
      " [ 0.32514243]\n",
      " [ 0.12793637]\n",
      " [-1.08872289]]\n",
      "t [[ 0.11492523]\n",
      " [-1.96422902]\n",
      " [-1.14367807]\n",
      " ...\n",
      " [ 0.30891372]\n",
      " [ 0.11492523]\n",
      " [-1.15672615]]\n",
      "t [[ 0.11492523]\n",
      " [-1.96422902]\n",
      " [-1.14367807]\n",
      " ...\n",
      " [ 0.30891372]\n",
      " [ 0.11492523]\n",
      " [-1.15672615]]\n",
      "Current iteration=8, loss=34211.449568715085\n",
      "t [[ 0.1012047 ]\n",
      " [-2.07981491]\n",
      " [-1.22059513]\n",
      " ...\n",
      " [ 0.29114412]\n",
      " [ 0.1012047 ]\n",
      " [-1.21842964]]\n",
      "t [[ 0.1012047 ]\n",
      " [-2.07981491]\n",
      " [-1.22059513]\n",
      " ...\n",
      " [ 0.29114412]\n",
      " [ 0.1012047 ]\n",
      " [-1.21842964]]\n",
      "t [[ 0.08730128]\n",
      " [-2.18286737]\n",
      " [-1.29088363]\n",
      " ...\n",
      " [ 0.27255648]\n",
      " [ 0.08730128]\n",
      " [-1.27474359]]\n",
      "loss=33113.77790075951\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.27038134]\n",
      " [-0.15762148]\n",
      " [-0.25225868]\n",
      " ...\n",
      " [ 0.21185249]\n",
      " [ 0.08918532]\n",
      " [-0.35894101]]\n",
      "t [[ 0.27038134]\n",
      " [-0.15762148]\n",
      " [-0.25225868]\n",
      " ...\n",
      " [ 0.21185249]\n",
      " [ 0.08918532]\n",
      " [-0.35894101]]\n",
      "t [[ 0.45410784]\n",
      " [-0.37900716]\n",
      " [-0.44662328]\n",
      " ...\n",
      " [ 0.29727296]\n",
      " [ 0.13032926]\n",
      " [-0.56067185]]\n",
      "t [[ 0.45410784]\n",
      " [-0.37900716]\n",
      " [-0.44662328]\n",
      " ...\n",
      " [ 0.29727296]\n",
      " [ 0.13032926]\n",
      " [-0.56067185]]\n",
      "Current iteration=2, loss=42588.61462100543\n",
      "t [[ 0.58461294]\n",
      " [-0.59381157]\n",
      " [-0.60633468]\n",
      " ...\n",
      " [ 0.33146028]\n",
      " [ 0.14709845]\n",
      " [-0.70084312]]\n",
      "t [[ 0.58461294]\n",
      " [-0.59381157]\n",
      " [-0.60633468]\n",
      " ...\n",
      " [ 0.33146028]\n",
      " [ 0.14709845]\n",
      " [-0.70084312]]\n",
      "t [[ 0.68058221]\n",
      " [-0.78675949]\n",
      " [-0.74240091]\n",
      " ...\n",
      " [ 0.34163442]\n",
      " [ 0.15040234]\n",
      " [-0.81177005]]\n",
      "t [[ 0.68058221]\n",
      " [-0.78675949]\n",
      " [-0.74240091]\n",
      " ...\n",
      " [ 0.34163442]\n",
      " [ 0.15040234]\n",
      " [-0.81177005]]\n",
      "Current iteration=4, loss=38266.44015168645\n",
      "t [[ 0.75305026]\n",
      " [-0.95742868]\n",
      " [-0.86090883]\n",
      " ...\n",
      " [ 0.33919607]\n",
      " [ 0.14586973]\n",
      " [-0.90562729]]\n",
      "t [[ 0.75305026]\n",
      " [-0.95742868]\n",
      " [-0.86090883]\n",
      " ...\n",
      " [ 0.33919607]\n",
      " [ 0.14586973]\n",
      " [-0.90562729]]\n",
      "t [[ 0.80899051]\n",
      " [-1.1086528 ]\n",
      " [-0.96568619]\n",
      " ...\n",
      " [ 0.32958417]\n",
      " [ 0.13669876]\n",
      " [-0.98777589]]\n",
      "t [[ 0.80899051]\n",
      " [-1.1086528 ]\n",
      " [-0.96568619]\n",
      " ...\n",
      " [ 0.32958417]\n",
      " [ 0.13669876]\n",
      " [-0.98777589]]\n",
      "Current iteration=6, loss=35789.684199339266\n",
      "t [[ 0.85302032]\n",
      " [-1.24351702]\n",
      " [-1.05936013]\n",
      " ...\n",
      " [ 0.31572243]\n",
      " [ 0.12482988]\n",
      " [-1.06101972]]\n",
      "t [[ 0.85302032]\n",
      " [-1.24351702]\n",
      " [-1.05936013]\n",
      " ...\n",
      " [ 0.31572243]\n",
      " [ 0.12482988]\n",
      " [-1.06101972]]\n",
      "t [[ 0.8883042 ]\n",
      " [-1.36467835]\n",
      " [-1.14384139]\n",
      " ...\n",
      " [ 0.29933275]\n",
      " [ 0.11149116]\n",
      " [-1.12706728]]\n",
      "t [[ 0.8883042 ]\n",
      " [-1.36467835]\n",
      " [-1.14384139]\n",
      " ...\n",
      " [ 0.29933275]\n",
      " [ 0.11149116]\n",
      " [-1.12706728]]\n",
      "Current iteration=8, loss=34199.109596662\n",
      "t [[ 0.91706912]\n",
      " [-1.47429586]\n",
      " [-1.22057604]\n",
      " ...\n",
      " [ 0.28149348]\n",
      " [ 0.09747947]\n",
      " [-1.18708562]]\n",
      "t [[ 0.91706912]\n",
      " [-1.47429586]\n",
      " [-1.22057604]\n",
      " ...\n",
      " [ 0.28149348]\n",
      " [ 0.09747947]\n",
      " [-1.18708562]]\n",
      "t [[ 0.94091514]\n",
      " [-1.57410243]\n",
      " [-1.29069064]\n",
      " ...\n",
      " [ 0.26290572]\n",
      " [ 0.08331802]\n",
      " [-1.24193597]]\n",
      "loss=33100.93540244762\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.26691586]\n",
      " [-0.15088905]\n",
      " [-0.25131394]\n",
      " ...\n",
      " [ 0.21512109]\n",
      " [ 0.09082909]\n",
      " [-0.3699946 ]]\n",
      "t [[ 0.26691586]\n",
      " [-0.15088905]\n",
      " [-0.25131394]\n",
      " ...\n",
      " [ 0.21512109]\n",
      " [ 0.09082909]\n",
      " [-0.3699946 ]]\n",
      "t [[ 0.44832074]\n",
      " [-0.36583158]\n",
      " [-0.44474847]\n",
      " ...\n",
      " [ 0.30048335]\n",
      " [ 0.13305163]\n",
      " [-0.57812128]]\n",
      "t [[ 0.44832074]\n",
      " [-0.36583158]\n",
      " [-0.44474847]\n",
      " ...\n",
      " [ 0.30048335]\n",
      " [ 0.13305163]\n",
      " [-0.57812128]]\n",
      "Current iteration=2, loss=42676.65361896606\n",
      "t [[ 0.57727784]\n",
      " [-0.57418843]\n",
      " [-0.60366643]\n",
      " ...\n",
      " [ 0.33340486]\n",
      " [ 0.15063915]\n",
      " [-0.72260423]]\n",
      "t [[ 0.57727784]\n",
      " [-0.57418843]\n",
      " [-0.60366643]\n",
      " ...\n",
      " [ 0.33340486]\n",
      " [ 0.15063915]\n",
      " [-0.72260423]]\n",
      "t [[ 0.67222717]\n",
      " [-0.76086159]\n",
      " [-0.73905385]\n",
      " ...\n",
      " [ 0.34172448]\n",
      " [ 0.1546035 ]\n",
      " [-0.83665331]]\n",
      "t [[ 0.67222717]\n",
      " [-0.76086159]\n",
      " [-0.73905385]\n",
      " ...\n",
      " [ 0.34172448]\n",
      " [ 0.1546035 ]\n",
      " [-0.83665331]]\n",
      "Current iteration=4, loss=38413.17407734639\n",
      "t [[ 0.74402223]\n",
      " [-0.92555575]\n",
      " [-0.85696715]\n",
      " ...\n",
      " [ 0.33713915]\n",
      " [ 0.15062152]\n",
      " [-0.93287606]]\n",
      "t [[ 0.74402223]\n",
      " [-0.92555575]\n",
      " [-0.85696715]\n",
      " ...\n",
      " [ 0.33713915]\n",
      " [ 0.15062152]\n",
      " [-0.93287606]]\n",
      "t [[ 0.7995158 ]\n",
      " [-1.0711558 ]\n",
      " [-0.96121091]\n",
      " ...\n",
      " [ 0.32524678]\n",
      " [ 0.14191946]\n",
      " [-1.01687378]]\n",
      "t [[ 0.7995158 ]\n",
      " [-1.0711558 ]\n",
      " [-0.96121091]\n",
      " ...\n",
      " [ 0.32524678]\n",
      " [ 0.14191946]\n",
      " [-1.01687378]]\n",
      "Current iteration=6, loss=35973.24896845232\n",
      "t [[ 0.84324751]\n",
      " [-1.20075425]\n",
      " [-1.05439593]\n",
      " ...\n",
      " [ 0.309058  ]\n",
      " [ 0.13045632]\n",
      " [-1.0915942 ]]\n",
      "t [[ 0.84324751]\n",
      " [-1.20075425]\n",
      " [-1.05439593]\n",
      " ...\n",
      " [ 0.309058  ]\n",
      " [ 0.13045632]\n",
      " [-1.0915942 ]]\n",
      "t [[ 0.87833142]\n",
      " [-1.31699459]\n",
      " [-1.13842148]\n",
      " ...\n",
      " [ 0.29034371]\n",
      " [ 0.11747326]\n",
      " [-1.158838  ]]\n",
      "t [[ 0.87833142]\n",
      " [-1.31699459]\n",
      " [-1.13842148]\n",
      " ...\n",
      " [ 0.29034371]\n",
      " [ 0.11747326]\n",
      " [-1.158838  ]]\n",
      "Current iteration=8, loss=34407.485056585654\n",
      "t [[ 0.90696128]\n",
      " [-1.42201377]\n",
      " [-1.21472558]\n",
      " ...\n",
      " [ 0.27021062]\n",
      " [ 0.10377682]\n",
      " [-1.21983487]]\n",
      "t [[ 0.90696128]\n",
      " [-1.42201377]\n",
      " [-1.21472558]\n",
      " ...\n",
      " [ 0.27021062]\n",
      " [ 0.10377682]\n",
      " [-1.21983487]]\n",
      "t [[ 0.93071484]\n",
      " [-1.51751999]\n",
      " [-1.28442908]\n",
      " ...\n",
      " [ 0.24937647]\n",
      " [ 0.08989757]\n",
      " [-1.27549064]]\n",
      "loss=33326.95624651853\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.2664886 ]\n",
      " [-0.15852401]\n",
      " [-0.25052454]\n",
      " ...\n",
      " [ 0.38382739]\n",
      " [-0.25052454]\n",
      " [ 0.35950872]]\n",
      "t [[ 0.2664886 ]\n",
      " [-0.15852401]\n",
      " [-0.25052454]\n",
      " ...\n",
      " [ 0.38382739]\n",
      " [-0.25052454]\n",
      " [ 0.35950872]]\n",
      "t [[ 0.4475336 ]\n",
      " [-0.37768341]\n",
      " [-0.44366002]\n",
      " ...\n",
      " [ 0.62933925]\n",
      " [-0.44366002]\n",
      " [ 0.567138  ]]\n",
      "t [[ 0.4475336 ]\n",
      " [-0.37768341]\n",
      " [-0.44366002]\n",
      " ...\n",
      " [ 0.62933925]\n",
      " [-0.44366002]\n",
      " [ 0.567138  ]]\n",
      "Current iteration=2, loss=42675.8378893777\n",
      "t [[ 0.57609126]\n",
      " [-0.58935449]\n",
      " [-0.60238907]\n",
      " ...\n",
      " [ 0.79797386]\n",
      " [-0.60238907]\n",
      " [ 0.6990846 ]]\n",
      "t [[ 0.57609126]\n",
      " [-0.58935449]\n",
      " [-0.60238907]\n",
      " ...\n",
      " [ 0.79797386]\n",
      " [-0.60238907]\n",
      " [ 0.6990846 ]]\n",
      "t [[ 0.6704656 ]\n",
      " [-0.77913089]\n",
      " [-0.73761958]\n",
      " ...\n",
      " [ 0.91938752]\n",
      " [-0.73761958]\n",
      " [ 0.78833608]]\n",
      "t [[ 0.6704656 ]\n",
      " [-0.77913089]\n",
      " [-0.73761958]\n",
      " ...\n",
      " [ 0.91938752]\n",
      " [-0.73761958]\n",
      " [ 0.78833608]]\n",
      "Current iteration=4, loss=38398.497635993015\n",
      "t [[ 0.74156775]\n",
      " [-0.94681013]\n",
      " [-0.85538621]\n",
      " ...\n",
      " [ 1.00968303]\n",
      " [-0.85538621]\n",
      " [ 0.85118346]]\n",
      "t [[ 0.74156775]\n",
      " [-0.94681013]\n",
      " [-0.85538621]\n",
      " ...\n",
      " [ 1.00968303]\n",
      " [-0.85538621]\n",
      " [ 0.85118346]]\n",
      "t [[ 0.79630655]\n",
      " [-1.09526207]\n",
      " [-0.95948982]\n",
      " ...\n",
      " [ 1.07850799]\n",
      " [-0.95948982]\n",
      " [ 0.89671934]]\n",
      "t [[ 0.79630655]\n",
      " [-1.09526207]\n",
      " [-0.95948982]\n",
      " ...\n",
      " [ 1.07850799]\n",
      " [-0.95948982]\n",
      " [ 0.89671934]]\n",
      "Current iteration=6, loss=35949.32888954136\n",
      "t [[ 0.83925683]\n",
      " [-1.22755994]\n",
      " [-1.05254172]\n",
      " ...\n",
      " [ 1.13204865]\n",
      " [-1.05254172]\n",
      " [ 0.93048084]]\n",
      "t [[ 0.83925683]\n",
      " [-1.22755994]\n",
      " [-1.05254172]\n",
      " ...\n",
      " [ 1.13204865]\n",
      " [-1.05254172]\n",
      " [ 0.93048084]]\n",
      "t [[ 0.87355328]\n",
      " [-1.34633936]\n",
      " [-1.13644196]\n",
      " ...\n",
      " [ 1.17446267]\n",
      " [-1.13644196]\n",
      " [ 0.95604561]]\n",
      "t [[ 0.87355328]\n",
      " [-1.34633936]\n",
      " [-1.13644196]\n",
      " ...\n",
      " [ 1.17446267]\n",
      " [-1.13644196]\n",
      " [ 0.95604561]]\n",
      "Current iteration=8, loss=34377.42069043708\n",
      "t [[ 0.90140149]\n",
      " [-1.45373894]\n",
      " [-1.21262875]\n",
      " ...\n",
      " [ 1.2086392 ]\n",
      " [-1.21262875]\n",
      " [ 0.97581962]]\n",
      "t [[ 0.90140149]\n",
      " [-1.45373894]\n",
      " [-1.21262875]\n",
      " ...\n",
      " [ 1.2086392 ]\n",
      " [-1.21262875]\n",
      " [ 0.97581962]]\n",
      "t [[ 0.92438589]\n",
      " [-1.55147424]\n",
      " [-1.28222269]\n",
      " ...\n",
      " [ 1.23663558]\n",
      " [-1.28222269]\n",
      " [ 0.99146584]]\n",
      "loss=33292.856746788624\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0914022 ]\n",
      " [-0.43767924]\n",
      " [-0.25666013]\n",
      " ...\n",
      " [ 0.21947849]\n",
      " [ 0.0914022 ]\n",
      " [-0.37204152]]\n",
      "t [[ 0.0914022 ]\n",
      " [-0.43767924]\n",
      " [-0.25666013]\n",
      " ...\n",
      " [ 0.21947849]\n",
      " [ 0.0914022 ]\n",
      " [-0.37204152]]\n",
      "t [[ 0.13293506]\n",
      " [-0.79599572]\n",
      " [-0.45365372]\n",
      " ...\n",
      " [ 0.30635061]\n",
      " [ 0.13293506]\n",
      " [-0.5802622 ]]\n",
      "t [[ 0.13293506]\n",
      " [-0.79599572]\n",
      " [-0.45365372]\n",
      " ...\n",
      " [ 0.30635061]\n",
      " [ 0.13293506]\n",
      " [-0.5802622 ]]\n",
      "Current iteration=2, loss=42466.51108920911\n",
      "t [[ 0.14959466]\n",
      " [-1.08836056]\n",
      " [-0.61530765]\n",
      " ...\n",
      " [ 0.34069227]\n",
      " [ 0.14959466]\n",
      " [-0.72555869]]\n",
      "t [[ 0.14959466]\n",
      " [-1.08836056]\n",
      " [-0.61530765]\n",
      " ...\n",
      " [ 0.34069227]\n",
      " [ 0.14959466]\n",
      " [-0.72555869]]\n",
      "t [[ 0.15264442]\n",
      " [-1.3299845 ]\n",
      " [-0.75289392]\n",
      " ...\n",
      " [ 0.35062786]\n",
      " [ 0.15264442]\n",
      " [-0.84090451]]\n",
      "t [[ 0.15264442]\n",
      " [-1.3299845 ]\n",
      " [-0.75289392]\n",
      " ...\n",
      " [ 0.35062786]\n",
      " [ 0.15264442]\n",
      " [-0.84090451]]\n",
      "Current iteration=4, loss=38137.8428700842\n",
      "t [[ 0.14783172]\n",
      " [-1.5328989 ]\n",
      " [-0.87260807]\n",
      " ...\n",
      " [ 0.34777535]\n",
      " [ 0.14783172]\n",
      " [-0.93854585]]\n",
      "t [[ 0.14783172]\n",
      " [-1.5328989 ]\n",
      " [-0.87260807]\n",
      " ...\n",
      " [ 0.34777535]\n",
      " [ 0.14783172]\n",
      " [-0.93854585]]\n",
      "t [[ 0.13840253]\n",
      " [-1.70592835]\n",
      " [-0.97834754]\n",
      " ...\n",
      " [ 0.33765675]\n",
      " [ 0.13840253]\n",
      " [-1.02389924]]\n",
      "t [[ 0.13840253]\n",
      " [-1.70592835]\n",
      " [-0.97834754]\n",
      " ...\n",
      " [ 0.33765675]\n",
      " [ 0.13840253]\n",
      " [-1.02389924]]\n",
      "Current iteration=6, loss=35673.51048091926\n",
      "t [[ 0.12631759]\n",
      " [-1.8554747 ]\n",
      " [-1.07279128]\n",
      " ...\n",
      " [ 0.32324389]\n",
      " [ 0.12631759]\n",
      " [-1.09984466]]\n",
      "t [[ 0.12631759]\n",
      " [-1.8554747 ]\n",
      " [-1.07279128]\n",
      " ...\n",
      " [ 0.32324389]\n",
      " [ 0.12631759]\n",
      " [-1.09984466]]\n",
      "t [[ 0.11281222]\n",
      " [-1.98623242]\n",
      " [-1.15788952]\n",
      " ...\n",
      " [ 0.3062896 ]\n",
      " [ 0.11281222]\n",
      " [-1.16816899]]\n",
      "t [[ 0.11281222]\n",
      " [-1.98623242]\n",
      " [-1.15788952]\n",
      " ...\n",
      " [ 0.3062896 ]\n",
      " [ 0.11281222]\n",
      " [-1.16816899]]\n",
      "Current iteration=8, loss=34097.852219730936\n",
      "t [[ 0.09868374]\n",
      " [-2.10170389]\n",
      " [-1.23511849]\n",
      " ...\n",
      " [ 0.28789129]\n",
      " [ 0.09868374]\n",
      " [-1.23010925]]\n",
      "t [[ 0.09868374]\n",
      " [-2.10170389]\n",
      " [-1.23511849]\n",
      " ...\n",
      " [ 0.28789129]\n",
      " [ 0.09868374]\n",
      " [-1.23010925]]\n",
      "t [[ 0.08445229]\n",
      " [-2.20455018]\n",
      " [-1.30562795]\n",
      " ...\n",
      " [ 0.26876089]\n",
      " [ 0.08445229]\n",
      " [-1.28658553]]\n",
      "loss=33013.66751809829\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.27601428]\n",
      " [-0.16090526]\n",
      " [-0.25751407]\n",
      " ...\n",
      " [ 0.21626608]\n",
      " [ 0.09104334]\n",
      " [-0.36641894]]\n",
      "t [[ 0.27601428]\n",
      " [-0.16090526]\n",
      " [-0.25751407]\n",
      " ...\n",
      " [ 0.21626608]\n",
      " [ 0.09104334]\n",
      " [-0.36641894]]\n",
      "t [[ 0.46178832]\n",
      " [-0.3882055 ]\n",
      " [-0.45473316]\n",
      " ...\n",
      " [ 0.30086345]\n",
      " [ 0.13205715]\n",
      " [-0.56912321]]\n",
      "t [[ 0.46178832]\n",
      " [-0.3882055 ]\n",
      " [-0.45473316]\n",
      " ...\n",
      " [ 0.30086345]\n",
      " [ 0.13205715]\n",
      " [-0.56912321]]\n",
      "Current iteration=2, loss=42453.26903248193\n",
      "t [[ 0.59284889]\n",
      " [-0.60723225]\n",
      " [-0.61635237]\n",
      " ...\n",
      " [ 0.33365202]\n",
      " [ 0.14818214]\n",
      " [-0.7096875 ]]\n",
      "t [[ 0.59284889]\n",
      " [-0.60723225]\n",
      " [-0.61635237]\n",
      " ...\n",
      " [ 0.33365202]\n",
      " [ 0.14818214]\n",
      " [-0.7096875 ]]\n",
      "t [[ 0.68869856]\n",
      " [-0.80289273]\n",
      " [-0.75379732]\n",
      " ...\n",
      " [ 0.34255214]\n",
      " [ 0.15072773]\n",
      " [-0.82110858]]\n",
      "t [[ 0.68869856]\n",
      " [-0.80289273]\n",
      " [-0.75379732]\n",
      " ...\n",
      " [ 0.34255214]\n",
      " [ 0.15072773]\n",
      " [-0.82110858]]\n",
      "Current iteration=4, loss=38125.197966610285\n",
      "t [[ 0.76074182]\n",
      " [-0.97531377]\n",
      " [-0.87332815]\n",
      " ...\n",
      " [ 0.3390184 ]\n",
      " [ 0.14545489]\n",
      " [-0.91550143]]\n",
      "t [[ 0.76074182]\n",
      " [-0.97531377]\n",
      " [-0.87332815]\n",
      " ...\n",
      " [ 0.3390184 ]\n",
      " [ 0.14545489]\n",
      " [-0.91550143]]\n",
      "t [[ 0.81613712]\n",
      " [-1.12769868]\n",
      " [-0.97887053]\n",
      " ...\n",
      " [ 0.32846133]\n",
      " [ 0.13561173]\n",
      " [-0.99814406]]\n",
      "t [[ 0.81613712]\n",
      " [-1.12769868]\n",
      " [-0.97887053]\n",
      " ...\n",
      " [ 0.32846133]\n",
      " [ 0.13561173]\n",
      " [-0.99814406]]\n",
      "Current iteration=6, loss=35661.30153141354\n",
      "t [[ 0.8595978 ]\n",
      " [-1.26334814]\n",
      " [-1.07311625]\n",
      " ...\n",
      " [ 0.31377987]\n",
      " [ 0.12315713]\n",
      " [-1.07180337]]\n",
      "t [[ 0.8595978 ]\n",
      " [-1.26334814]\n",
      " [-1.07311625]\n",
      " ...\n",
      " [ 0.31377987]\n",
      " [ 0.12315713]\n",
      " [-1.07180337]]\n",
      "t [[ 0.89433805]\n",
      " [-1.3850448 ]\n",
      " [-1.15802151]\n",
      " ...\n",
      " [ 0.29667925]\n",
      " [ 0.10932314]\n",
      " [-1.13817973]]\n",
      "t [[ 0.89433805]\n",
      " [-1.3850448 ]\n",
      " [-1.15802151]\n",
      " ...\n",
      " [ 0.29667925]\n",
      " [ 0.10932314]\n",
      " [-1.13817973]]\n",
      "Current iteration=8, loss=34085.488893903974\n",
      "t [[ 0.92260924]\n",
      " [-1.49502406]\n",
      " [-1.23506536]\n",
      " ...\n",
      " [ 0.27822518]\n",
      " [ 0.09490354]\n",
      " [-1.19844472]]\n",
      "t [[ 0.92260924]\n",
      " [-1.49502406]\n",
      " [-1.23506536]\n",
      " ...\n",
      " [ 0.27822518]\n",
      " [ 0.09490354]\n",
      " [-1.19844472]]\n",
      "t [[ 0.94602159]\n",
      " [-1.59506758]\n",
      " [-1.30539881]\n",
      " ...\n",
      " [ 0.25910789]\n",
      " [ 0.08041495]\n",
      " [-1.25346851]]\n",
      "loss=33000.762485841915\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.27247661]\n",
      " [-0.15403258]\n",
      " [-0.25654964]\n",
      " ...\n",
      " [ 0.21960278]\n",
      " [ 0.09272136]\n",
      " [-0.37770282]]\n",
      "t [[ 0.27247661]\n",
      " [-0.15403258]\n",
      " [-0.25654964]\n",
      " ...\n",
      " [ 0.21960278]\n",
      " [ 0.09272136]\n",
      " [-0.37770282]]\n",
      "t [[ 0.45590403]\n",
      " [-0.37476114]\n",
      " [-0.45282017]\n",
      " ...\n",
      " [ 0.30407705]\n",
      " [ 0.13482521]\n",
      " [-0.58684435]]\n",
      "t [[ 0.45590403]\n",
      " [-0.37476114]\n",
      " [-0.45282017]\n",
      " ...\n",
      " [ 0.30407705]\n",
      " [ 0.13482521]\n",
      " [-0.58684435]]\n",
      "Current iteration=2, loss=42542.96183795197\n",
      "t [[ 0.58541606]\n",
      " [-0.58720605]\n",
      " [-0.61363487]\n",
      " ...\n",
      " [ 0.33552248]\n",
      " [ 0.1517749 ]\n",
      " [-0.73172535]]\n",
      "t [[ 0.58541606]\n",
      " [-0.58720605]\n",
      " [-0.61363487]\n",
      " ...\n",
      " [ 0.33552248]\n",
      " [ 0.1517749 ]\n",
      " [-0.73172535]]\n",
      "t [[ 0.6802561 ]\n",
      " [-0.77647106]\n",
      " [-0.75039355]\n",
      " ...\n",
      " [ 0.34249162]\n",
      " [ 0.1549847 ]\n",
      " [-0.84626042]]\n",
      "t [[ 0.6802561 ]\n",
      " [-0.77647106]\n",
      " [-0.75039355]\n",
      " ...\n",
      " [ 0.34249162]\n",
      " [ 0.1549847 ]\n",
      " [-0.84626042]]\n",
      "Current iteration=4, loss=38273.948958510504\n",
      "t [[ 0.75163976]\n",
      " [-0.94281578]\n",
      " [-0.86932392]\n",
      " ...\n",
      " [ 0.33674106]\n",
      " [ 0.15026457]\n",
      " [-0.94300567]]\n",
      "t [[ 0.75163976]\n",
      " [-0.94281578]\n",
      " [-0.86932392]\n",
      " ...\n",
      " [ 0.33674106]\n",
      " [ 0.15026457]\n",
      " [-0.94300567]]\n",
      "t [[ 0.8066015 ]\n",
      " [-1.08949384]\n",
      " [-0.97432752]\n",
      " ...\n",
      " [ 0.32384126]\n",
      " [ 0.14089137]\n",
      " [-1.02748226]]\n",
      "t [[ 0.8066015 ]\n",
      " [-1.08949384]\n",
      " [-0.97432752]\n",
      " ...\n",
      " [ 0.32384126]\n",
      " [ 0.14089137]\n",
      " [-1.02748226]]\n",
      "Current iteration=6, loss=35846.83291258314\n",
      "t [[ 0.84977568]\n",
      " [-1.21981051]\n",
      " [-1.06807932]\n",
      " ...\n",
      " [ 0.30677758]\n",
      " [ 0.12884288]\n",
      " [-1.10260205]]\n",
      "t [[ 0.84977568]\n",
      " [-1.21981051]\n",
      " [-1.06807932]\n",
      " ...\n",
      " [ 0.30677758]\n",
      " [ 0.12884288]\n",
      " [-1.10260205]]\n",
      "t [[ 0.88432568]\n",
      " [-1.33653214]\n",
      " [-1.15252387]\n",
      " ...\n",
      " [ 0.28730344]\n",
      " [ 0.11536451]\n",
      " [-1.17015813]]\n",
      "t [[ 0.88432568]\n",
      " [-1.33653214]\n",
      " [-1.15252387]\n",
      " ...\n",
      " [ 0.28730344]\n",
      " [ 0.11536451]\n",
      " [-1.17015813]]\n",
      "Current iteration=8, loss=34295.67836285497\n",
      "t [[ 0.91246959]\n",
      " [-1.44186962]\n",
      " [-1.2291321 ]\n",
      " ...\n",
      " [ 0.26651215]\n",
      " [ 0.10125988]\n",
      " [-1.23138504]]\n",
      "t [[ 0.91246959]\n",
      " [-1.44186962]\n",
      " [-1.2291321 ]\n",
      " ...\n",
      " [ 0.26651215]\n",
      " [ 0.10125988]\n",
      " [-1.23138504]]\n",
      "t [[ 0.93579548]\n",
      " [-1.537578  ]\n",
      " [-1.29904927]\n",
      " ...\n",
      " [ 0.24510992]\n",
      " [ 0.08705311]\n",
      " [-1.2871978 ]]\n",
      "loss=33228.42570193952\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.27204044]\n",
      " [-0.16182659]\n",
      " [-0.25574381]\n",
      " ...\n",
      " [ 0.39182379]\n",
      " [-0.25574381]\n",
      " [ 0.36699849]]\n",
      "t [[ 0.27204044]\n",
      " [-0.16182659]\n",
      " [-0.25574381]\n",
      " ...\n",
      " [ 0.39182379]\n",
      " [-0.25574381]\n",
      " [ 0.36699849]]\n",
      "t [[ 0.45509843]\n",
      " [-0.38679333]\n",
      " [-0.45171857]\n",
      " ...\n",
      " [ 0.63960402]\n",
      " [-0.45171857]\n",
      " [ 0.57583137]]\n",
      "t [[ 0.45509843]\n",
      " [-0.38679333]\n",
      " [-0.45171857]\n",
      " ...\n",
      " [ 0.63960402]\n",
      " [-0.45171857]\n",
      " [ 0.57583137]]\n",
      "Current iteration=2, loss=42541.82817382785\n",
      "t [[ 0.58419401]\n",
      " [-0.60259136]\n",
      " [-0.61234498]\n",
      " ...\n",
      " [ 0.80861396]\n",
      " [-0.61234498]\n",
      " [ 0.70743749]]\n",
      "t [[ 0.58419401]\n",
      " [-0.60259136]\n",
      " [-0.61234498]\n",
      " ...\n",
      " [ 0.80861396]\n",
      " [-0.61234498]\n",
      " [ 0.70743749]]\n",
      "t [[ 0.67843674]\n",
      " [-0.79501247]\n",
      " [-0.74894533]\n",
      " ...\n",
      " [ 0.92965586]\n",
      " [-0.74894533]\n",
      " [ 0.79592234]]\n",
      "t [[ 0.67843674]\n",
      " [-0.79501247]\n",
      " [-0.74894533]\n",
      " ...\n",
      " [ 0.92965586]\n",
      " [-0.74894533]\n",
      " [ 0.79592234]]\n",
      "Current iteration=4, loss=38258.78807227571\n",
      "t [[ 0.74910554]\n",
      " [-0.96439454]\n",
      " [-0.86772682]\n",
      " ...\n",
      " [ 1.01927111]\n",
      " [-0.86772682]\n",
      " [ 0.85790371]]\n",
      "t [[ 0.74910554]\n",
      " [-0.96439454]\n",
      " [-0.86772682]\n",
      " ...\n",
      " [ 1.01927111]\n",
      " [-0.86772682]\n",
      " [ 0.85790371]]\n",
      "t [[ 0.80329253]\n",
      " [-1.11396976]\n",
      " [-0.97258813]\n",
      " ...\n",
      " [ 1.08730996]\n",
      " [-0.97258813]\n",
      " [ 0.90259728]]\n",
      "t [[ 0.80329253]\n",
      " [-1.11396976]\n",
      " [-0.97258813]\n",
      " ...\n",
      " [ 1.08730996]\n",
      " [-0.97258813]\n",
      " [ 0.90259728]]\n",
      "Current iteration=6, loss=35822.42172148924\n",
      "t [[ 0.84566745]\n",
      " [-1.24702278]\n",
      " [-1.06620507]\n",
      " ...\n",
      " [ 1.14006107]\n",
      " [-1.06620507]\n",
      " [ 0.93559375]]\n",
      "t [[ 0.84566745]\n",
      " [-1.24702278]\n",
      " [-1.06620507]\n",
      " ...\n",
      " [ 1.14006107]\n",
      " [-1.06620507]\n",
      " [ 0.93559375]]\n",
      "t [[ 0.87941408]\n",
      " [-1.36631276]\n",
      " [-1.15052297]\n",
      " ...\n",
      " [ 1.18173435]\n",
      " [-1.15052297]\n",
      " [ 0.96049318]]\n",
      "t [[ 0.87941408]\n",
      " [-1.36631276]\n",
      " [-1.15052297]\n",
      " ...\n",
      " [ 1.18173435]\n",
      " [-1.15052297]\n",
      " [ 0.96049318]]\n",
      "Current iteration=8, loss=34265.176169749975\n",
      "t [[ 0.9067621 ]\n",
      " [-1.47405353]\n",
      " [-1.2270129 ]\n",
      " ...\n",
      " [ 1.21524372]\n",
      " [-1.2270129 ]\n",
      " [ 0.9797077 ]]\n",
      "t [[ 0.9067621 ]\n",
      " [-1.47405353]\n",
      " [-1.2270129 ]\n",
      " ...\n",
      " [ 1.21524372]\n",
      " [-1.2270129 ]\n",
      " [ 0.9797077 ]]\n",
      "t [[ 0.92930599]\n",
      " [-1.57200877]\n",
      " [-1.29681979]\n",
      " ...\n",
      " [ 1.2426558 ]\n",
      " [-1.29681979]\n",
      " [ 0.99489744]]\n",
      "loss=33193.96880572593\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=69254.41425128598\n",
      "t [[-0.07844999]\n",
      " [ 0.00878458]\n",
      " [ 0.00715734]\n",
      " ...\n",
      " [-0.01540242]\n",
      " [ 0.00708345]\n",
      " [-0.01540242]]\n",
      "t [[-0.07844999]\n",
      " [ 0.00878458]\n",
      " [ 0.00715734]\n",
      " ...\n",
      " [-0.01540242]\n",
      " [ 0.00708345]\n",
      " [-0.01540242]]\n",
      "t [[-0.15518199]\n",
      " [ 0.01752977]\n",
      " [ 0.01417684]\n",
      " ...\n",
      " [-0.03064349]\n",
      " [ 0.01411854]\n",
      " [-0.03064349]]\n",
      "t [[-0.15518199]\n",
      " [ 0.01752977]\n",
      " [ 0.01417684]\n",
      " ...\n",
      " [-0.03064349]\n",
      " [ 0.01411854]\n",
      " [-0.03064349]]\n",
      "Current iteration=2, loss=68468.25370881142\n",
      "t [[-0.2302381 ]\n",
      " [ 0.02623359]\n",
      " [ 0.02106067]\n",
      " ...\n",
      " [-0.04572565]\n",
      " [ 0.0211045 ]\n",
      " [-0.04572565]]\n",
      "t [[-0.2302381 ]\n",
      " [ 0.02623359]\n",
      " [ 0.02106067]\n",
      " ...\n",
      " [-0.04572565]\n",
      " [ 0.0211045 ]\n",
      " [-0.04572565]]\n",
      "t [[-0.30365959]\n",
      " [ 0.03489415]\n",
      " [ 0.02781098]\n",
      " ...\n",
      " [-0.06065134]\n",
      " [ 0.02804059]\n",
      " [-0.06065134]]\n",
      "t [[-0.30365959]\n",
      " [ 0.03489415]\n",
      " [ 0.02781098]\n",
      " ...\n",
      " [-0.06065134]\n",
      " [ 0.02804059]\n",
      " [-0.06065134]]\n",
      "Current iteration=4, loss=67714.10556395467\n",
      "t [[-0.37548689]\n",
      " [ 0.04350965]\n",
      " [ 0.03442993]\n",
      " ...\n",
      " [-0.07542294]\n",
      " [ 0.03492616]\n",
      " [-0.07542294]]\n",
      "t [[-0.37548689]\n",
      " [ 0.04350965]\n",
      " [ 0.03442993]\n",
      " ...\n",
      " [-0.07542294]\n",
      " [ 0.03492616]\n",
      " [-0.07542294]]\n",
      "t [[-0.44575959]\n",
      " [ 0.05207837]\n",
      " [ 0.04091962]\n",
      " ...\n",
      " [-0.09004282]\n",
      " [ 0.04176057]\n",
      " [-0.09004282]]\n",
      "t [[-0.44575959]\n",
      " [ 0.05207837]\n",
      " [ 0.04091962]\n",
      " ...\n",
      " [-0.09004282]\n",
      " [ 0.04176057]\n",
      " [-0.09004282]]\n",
      "Current iteration=6, loss=66990.2146470263\n",
      "t [[-0.51451642]\n",
      " [ 0.06059869]\n",
      " [ 0.04728217]\n",
      " ...\n",
      " [-0.10451331]\n",
      " [ 0.04854325]\n",
      " [-0.10451331]]\n",
      "t [[-0.51451642]\n",
      " [ 0.06059869]\n",
      " [ 0.04728217]\n",
      " ...\n",
      " [-0.10451331]\n",
      " [ 0.04854325]\n",
      " [-0.10451331]]\n",
      "t [[-0.58179527]\n",
      " [ 0.06906906]\n",
      " [ 0.05351966]\n",
      " ...\n",
      " [-0.11883672]\n",
      " [ 0.05527369]\n",
      " [-0.11883672]]\n",
      "t [[-0.58179527]\n",
      " [ 0.06906906]\n",
      " [ 0.05351966]\n",
      " ...\n",
      " [-0.11883672]\n",
      " [ 0.05527369]\n",
      " [-0.11883672]]\n",
      "Current iteration=8, loss=66294.93750006132\n",
      "t [[-0.64763318]\n",
      " [ 0.07748802]\n",
      " [ 0.05963415]\n",
      " ...\n",
      " [-0.1330153 ]\n",
      " [ 0.06195141]\n",
      " [-0.1330153 ]]\n",
      "t [[-0.64763318]\n",
      " [ 0.07748802]\n",
      " [ 0.05963415]\n",
      " ...\n",
      " [-0.1330153 ]\n",
      " [ 0.06195141]\n",
      " [-0.1330153 ]]\n",
      "t [[-0.71206631]\n",
      " [ 0.08585418]\n",
      " [ 0.06562769]\n",
      " ...\n",
      " [-0.14705129]\n",
      " [ 0.06857596]\n",
      " [-0.14705129]]\n",
      "t [[-0.71206631]\n",
      " [ 0.08585418]\n",
      " [ 0.06562769]\n",
      " ...\n",
      " [-0.14705129]\n",
      " [ 0.06857596]\n",
      " [-0.14705129]]\n",
      "Current iteration=10, loss=65626.73574255295\n",
      "t [[-0.77513003]\n",
      " [ 0.09416625]\n",
      " [ 0.07150228]\n",
      " ...\n",
      " [-0.1609469 ]\n",
      " [ 0.07514697]\n",
      " [-0.1609469 ]]\n",
      "t [[-0.77513003]\n",
      " [ 0.09416625]\n",
      " [ 0.07150228]\n",
      " ...\n",
      " [-0.1609469 ]\n",
      " [ 0.07514697]\n",
      " [-0.1609469 ]]\n",
      "t [[-0.83685881]\n",
      " [ 0.102423  ]\n",
      " [ 0.07725994]\n",
      " ...\n",
      " [-0.17470428]\n",
      " [ 0.08166409]\n",
      " [-0.17470428]]\n",
      "t [[-0.83685881]\n",
      " [ 0.102423  ]\n",
      " [ 0.07725994]\n",
      " ...\n",
      " [-0.17470428]\n",
      " [ 0.08166409]\n",
      " [-0.17470428]]\n",
      "Current iteration=12, loss=64984.16950885916\n",
      "t [[-0.89728634]\n",
      " [ 0.11062326]\n",
      " [ 0.08290261]\n",
      " ...\n",
      " [-0.18832558]\n",
      " [ 0.08812701]\n",
      " [-0.18832558]]\n",
      "t [[-0.89728634]\n",
      " [ 0.11062326]\n",
      " [ 0.08290261]\n",
      " ...\n",
      " [-0.18832558]\n",
      " [ 0.08812701]\n",
      " [-0.18832558]]\n",
      "t [[-0.95644545]\n",
      " [ 0.11876596]\n",
      " [ 0.08843226]\n",
      " ...\n",
      " [-0.20181288]\n",
      " [ 0.09453546]\n",
      " [-0.20181288]]\n",
      "t [[-0.95644545]\n",
      " [ 0.11876596]\n",
      " [ 0.08843226]\n",
      " ...\n",
      " [-0.20181288]\n",
      " [ 0.09453546]\n",
      " [-0.20181288]]\n",
      "Current iteration=14, loss=64365.891057945984\n",
      "t [[-1.01436817]\n",
      " [ 0.12685009]\n",
      " [ 0.09385079]\n",
      " ...\n",
      " [-0.21516826]\n",
      " [ 0.10088922]\n",
      " [-0.21516826]]\n",
      "t [[-1.01436817]\n",
      " [ 0.12685009]\n",
      " [ 0.09385079]\n",
      " ...\n",
      " [-0.21516826]\n",
      " [ 0.10088922]\n",
      " [-0.21516826]]\n",
      "t [[-1.07108575]\n",
      " [ 0.1348747 ]\n",
      " [ 0.09916009]\n",
      " ...\n",
      " [-0.22839374]\n",
      " [ 0.10718809]\n",
      " [-0.22839374]]\n",
      "t [[-1.07108575]\n",
      " [ 0.1348747 ]\n",
      " [ 0.09916009]\n",
      " ...\n",
      " [-0.22839374]\n",
      " [ 0.10718809]\n",
      " [-0.22839374]]\n",
      "Current iteration=16, loss=63770.63862613424\n",
      "t [[-1.12662862]\n",
      " [ 0.14283891]\n",
      " [ 0.10436205]\n",
      " ...\n",
      " [-0.24149132]\n",
      " [ 0.11343191]\n",
      " [-0.24149132]]\n",
      "t [[-1.12662862]\n",
      " [ 0.14283891]\n",
      " [ 0.10436205]\n",
      " ...\n",
      " [-0.24149132]\n",
      " [ 0.11343191]\n",
      " [-0.24149132]]\n",
      "t [[-1.18102646]\n",
      " [ 0.15074191]\n",
      " [ 0.10945848]\n",
      " ...\n",
      " [-0.25446297]\n",
      " [ 0.11962055]\n",
      " [-0.25446297]]\n",
      "t [[-1.18102646]\n",
      " [ 0.15074191]\n",
      " [ 0.10945848]\n",
      " ...\n",
      " [-0.25446297]\n",
      " [ 0.11962055]\n",
      " [-0.25446297]]\n",
      "Current iteration=18, loss=63197.23056928423\n",
      "t [[-1.23430816]\n",
      " [ 0.15858294]\n",
      " [ 0.11445122]\n",
      " ...\n",
      " [-0.26731061]\n",
      " [ 0.12575393]\n",
      " [-0.26731061]]\n",
      "t [[-1.23430816]\n",
      " [ 0.15858294]\n",
      " [ 0.11445122]\n",
      " ...\n",
      " [-0.26731061]\n",
      " [ 0.12575393]\n",
      " [-0.26731061]]\n",
      "t [[-1.28650189]\n",
      " [ 0.16636132]\n",
      " [ 0.11934203]\n",
      " ...\n",
      " [-0.28003614]\n",
      " [ 0.13183197]\n",
      " [-0.28003614]]\n",
      "t [[-1.28650189]\n",
      " [ 0.16636132]\n",
      " [ 0.11934203]\n",
      " ...\n",
      " [-0.28003614]\n",
      " [ 0.13183197]\n",
      " [-0.28003614]]\n",
      "Current iteration=20, loss=62644.5598218673\n",
      "t [[-1.33763508]\n",
      " [ 0.17407639]\n",
      " [ 0.12413268]\n",
      " ...\n",
      " [-0.29264142]\n",
      " [ 0.13785465]\n",
      " [-0.29264142]]\n",
      "t [[-1.33763508]\n",
      " [ 0.17407639]\n",
      " [ 0.12413268]\n",
      " ...\n",
      " [-0.29264142]\n",
      " [ 0.13785465]\n",
      " [-0.29264142]]\n",
      "t [[-1.38773443]\n",
      " [ 0.18172759]\n",
      " [ 0.12882489]\n",
      " ...\n",
      " [-0.30512829]\n",
      " [ 0.14382195]\n",
      " [-0.30512829]]\n",
      "t [[-1.38773443]\n",
      " [ 0.18172759]\n",
      " [ 0.12882489]\n",
      " ...\n",
      " [-0.30512829]\n",
      " [ 0.14382195]\n",
      " [-0.30512829]]\n",
      "Current iteration=22, loss=62111.58868593765\n",
      "t [[-1.43682594]\n",
      " [ 0.18931438]\n",
      " [ 0.13342038]\n",
      " ...\n",
      " [-0.31749854]\n",
      " [ 0.1497339 ]\n",
      " [-0.31749854]]\n",
      "t [[-1.43682594]\n",
      " [ 0.18931438]\n",
      " [ 0.13342038]\n",
      " ...\n",
      " [-0.31749854]\n",
      " [ 0.1497339 ]\n",
      " [-0.31749854]]\n",
      "t [[-1.48493493]\n",
      " [ 0.19683629]\n",
      " [ 0.1379208 ]\n",
      " ...\n",
      " [-0.32975394]\n",
      " [ 0.15559053]\n",
      " [-0.32975394]]\n",
      "t [[-1.48493493]\n",
      " [ 0.19683629]\n",
      " [ 0.1379208 ]\n",
      " ...\n",
      " [-0.32975394]\n",
      " [ 0.15559053]\n",
      " [-0.32975394]]\n",
      "Current iteration=24, loss=61597.34395240801\n",
      "t [[-1.53208604]\n",
      " [ 0.20429289]\n",
      " [ 0.14232782]\n",
      " ...\n",
      " [-0.34189623]\n",
      " [ 0.16139192]\n",
      " [-0.34189623]]\n",
      "t [[-1.53208604]\n",
      " [ 0.20429289]\n",
      " [ 0.14232782]\n",
      " ...\n",
      " [-0.34189623]\n",
      " [ 0.16139192]\n",
      " [-0.34189623]]\n",
      "t [[-1.57830326]\n",
      " [ 0.21168379]\n",
      " [ 0.14664304]\n",
      " ...\n",
      " [-0.35392711]\n",
      " [ 0.16713815]\n",
      " [-0.35392711]]\n",
      "t [[-1.57830326]\n",
      " [ 0.21168379]\n",
      " [ 0.14664304]\n",
      " ...\n",
      " [-0.35392711]\n",
      " [ 0.16713815]\n",
      " [-0.35392711]]\n",
      "Current iteration=26, loss=61100.91234948729\n",
      "t [[-1.62360993]\n",
      " [ 0.21900866]\n",
      " [ 0.15086806]\n",
      " ...\n",
      " [-0.36584827]\n",
      " [ 0.17282934]\n",
      " [-0.36584827]]\n",
      "t [[-1.62360993]\n",
      " [ 0.21900866]\n",
      " [ 0.15086806]\n",
      " ...\n",
      " [-0.36584827]\n",
      " [ 0.17282934]\n",
      " [-0.36584827]]\n",
      "t [[-1.66802877]\n",
      " [ 0.22626722]\n",
      " [ 0.15500445]\n",
      " ...\n",
      " [-0.37766133]\n",
      " [ 0.17846561]\n",
      " [-0.37766133]]\n",
      "t [[-1.66802877]\n",
      " [ 0.22626722]\n",
      " [ 0.15500445]\n",
      " ...\n",
      " [-0.37766133]\n",
      " [ 0.17846561]\n",
      " [-0.37766133]]\n",
      "Current iteration=28, loss=60621.43630799641\n",
      "t [[-1.71158187]\n",
      " [ 0.2334592 ]\n",
      " [ 0.15905374]\n",
      " ...\n",
      " [-0.38936791]\n",
      " [ 0.18404712]\n",
      " [-0.38936791]]\n",
      "t [[-1.71158187]\n",
      " [ 0.2334592 ]\n",
      " [ 0.15905374]\n",
      " ...\n",
      " [-0.38936791]\n",
      " [ 0.18404712]\n",
      " [-0.38936791]]\n",
      "t [[-1.75429076]\n",
      " [ 0.24058441]\n",
      " [ 0.16301743]\n",
      " ...\n",
      " [-0.40096961]\n",
      " [ 0.18957404]\n",
      " [-0.40096961]]\n",
      "t [[-1.75429076]\n",
      " [ 0.24058441]\n",
      " [ 0.16301743]\n",
      " ...\n",
      " [-0.40096961]\n",
      " [ 0.18957404]\n",
      " [-0.40096961]]\n",
      "Current iteration=30, loss=60158.11002992276\n",
      "t [[-1.79617636]\n",
      " [ 0.24764267]\n",
      " [ 0.16689702]\n",
      " ...\n",
      " [-0.41246798]\n",
      " [ 0.19504655]\n",
      " [-0.41246798]]\n",
      "t [[-1.79617636]\n",
      " [ 0.24764267]\n",
      " [ 0.16689702]\n",
      " ...\n",
      " [-0.41246798]\n",
      " [ 0.19504655]\n",
      " [-0.41246798]]\n",
      "t [[-1.83725902]\n",
      " [ 0.25463385]\n",
      " [ 0.17069396]\n",
      " ...\n",
      " [-0.42386455]\n",
      " [ 0.20046485]\n",
      " [-0.42386455]]\n",
      "t [[-1.83725902]\n",
      " [ 0.25463385]\n",
      " [ 0.17069396]\n",
      " ...\n",
      " [-0.42386455]\n",
      " [ 0.20046485]\n",
      " [-0.42386455]]\n",
      "Current iteration=32, loss=59710.175844539895\n",
      "t [[-1.87755855]\n",
      " [ 0.26155785]\n",
      " [ 0.17440968]\n",
      " ...\n",
      " [-0.43516081]\n",
      " [ 0.20582917]\n",
      " [-0.43516081]]\n",
      "t [[-1.87755855]\n",
      " [ 0.26155785]\n",
      " [ 0.17440968]\n",
      " ...\n",
      " [-0.43516081]\n",
      " [ 0.20582917]\n",
      " [-0.43516081]]\n",
      "t [[-1.91709423]\n",
      " [ 0.26841461]\n",
      " [ 0.17804559]\n",
      " ...\n",
      " [-0.44635824]\n",
      " [ 0.21113973]\n",
      " [-0.44635824]]\n",
      "t [[-1.91709423]\n",
      " [ 0.26841461]\n",
      " [ 0.17804559]\n",
      " ...\n",
      " [-0.44635824]\n",
      " [ 0.21113973]\n",
      " [-0.44635824]]\n",
      "Current iteration=34, loss=59276.9208353461\n",
      "t [[-1.95588479]\n",
      " [ 0.27520408]\n",
      " [ 0.18160306]\n",
      " ...\n",
      " [-0.4574583 ]\n",
      " [ 0.21639678]\n",
      " [-0.4574583 ]]\n",
      "t [[-1.95588479]\n",
      " [ 0.27520408]\n",
      " [ 0.18160306]\n",
      " ...\n",
      " [-0.4574583 ]\n",
      " [ 0.21639678]\n",
      " [-0.4574583 ]]\n",
      "t [[-1.99394848]\n",
      " [ 0.28192628]\n",
      " [ 0.18508346]\n",
      " ...\n",
      " [-0.46846238]\n",
      " [ 0.22160059]\n",
      " [-0.46846238]]\n",
      "t [[-1.99394848]\n",
      " [ 0.28192628]\n",
      " [ 0.18508346]\n",
      " ...\n",
      " [-0.46846238]\n",
      " [ 0.22160059]\n",
      " [-0.46846238]]\n",
      "Current iteration=36, loss=58857.67372064233\n",
      "t [[-2.03130303]\n",
      " [ 0.28858123]\n",
      " [ 0.1884881 ]\n",
      " ...\n",
      " [-0.47937191]\n",
      " [ 0.22675141]\n",
      " [-0.47937191]]\n",
      "t [[-2.03130303]\n",
      " [ 0.28858123]\n",
      " [ 0.1884881 ]\n",
      " ...\n",
      " [-0.47937191]\n",
      " [ 0.22675141]\n",
      " [-0.47937191]]\n",
      "t [[-2.0679657 ]\n",
      " [ 0.29516898]\n",
      " [ 0.1918183 ]\n",
      " ...\n",
      " [-0.49018823]\n",
      " [ 0.23184953]\n",
      " [-0.49018823]]\n",
      "t [[-2.0679657 ]\n",
      " [ 0.29516898]\n",
      " [ 0.1918183 ]\n",
      " ...\n",
      " [-0.49018823]\n",
      " [ 0.23184953]\n",
      " [-0.49018823]]\n",
      "Current iteration=38, loss=58451.801970640554\n",
      "t [[-2.10395327]\n",
      " [ 0.30168963]\n",
      " [ 0.19507534]\n",
      " ...\n",
      " [-0.50091269]\n",
      " [ 0.23689525]\n",
      " [-0.50091269]]\n",
      "t [[-2.10395327]\n",
      " [ 0.30168963]\n",
      " [ 0.19507534]\n",
      " ...\n",
      " [-0.50091269]\n",
      " [ 0.23689525]\n",
      " [-0.50091269]]\n",
      "t [[-2.13928207]\n",
      " [ 0.30814326]\n",
      " [ 0.19826047]\n",
      " ...\n",
      " [-0.51154662]\n",
      " [ 0.24188886]\n",
      " [-0.51154662]]\n",
      "t [[-2.13928207]\n",
      " [ 0.30814326]\n",
      " [ 0.19826047]\n",
      " ...\n",
      " [-0.51154662]\n",
      " [ 0.24188886]\n",
      " [-0.51154662]]\n",
      "Current iteration=40, loss=58058.70914436325\n",
      "t [[-2.17396798]\n",
      " [ 0.31453003]\n",
      " [ 0.20137494]\n",
      " ...\n",
      " [-0.52209131]\n",
      " [ 0.24683068]\n",
      " [-0.52209131]]\n",
      "t [[-2.17396798]\n",
      " [ 0.31453003]\n",
      " [ 0.20137494]\n",
      " ...\n",
      " [-0.52209131]\n",
      " [ 0.24683068]\n",
      " [-0.52209131]]\n",
      "t [[-2.20802646]\n",
      " [ 0.32085008]\n",
      " [ 0.20441995]\n",
      " ...\n",
      " [-0.53254803]\n",
      " [ 0.25172103]\n",
      " [-0.53254803]]\n",
      "t [[-2.20802646]\n",
      " [ 0.32085008]\n",
      " [ 0.20441995]\n",
      " ...\n",
      " [-0.53254803]\n",
      " [ 0.25172103]\n",
      " [-0.53254803]]\n",
      "Current iteration=42, loss=57677.832430183305\n",
      "t [[-2.24147254]\n",
      " [ 0.32710359]\n",
      " [ 0.20739668]\n",
      " ...\n",
      " [-0.54291803]\n",
      " [ 0.25656023]\n",
      " [-0.54291803]]\n",
      "t [[-2.24147254]\n",
      " [ 0.32710359]\n",
      " [ 0.20739668]\n",
      " ...\n",
      " [-0.54291803]\n",
      " [ 0.25656023]\n",
      " [-0.54291803]]\n",
      "t [[-2.27432084]\n",
      " [ 0.33329076]\n",
      " [ 0.21030632]\n",
      " ...\n",
      " [-0.55320254]\n",
      " [ 0.26134862]\n",
      " [-0.55320254]]\n",
      "t [[-2.27432084]\n",
      " [ 0.33329076]\n",
      " [ 0.21030632]\n",
      " ...\n",
      " [-0.55320254]\n",
      " [ 0.26134862]\n",
      " [-0.55320254]]\n",
      "Current iteration=44, loss=57308.64037460493\n",
      "t [[-2.30658558]\n",
      " [ 0.33941181]\n",
      " [ 0.21315001]\n",
      " ...\n",
      " [-0.56340275]\n",
      " [ 0.26608654]\n",
      " [-0.56340275]]\n",
      "t [[-2.30658558]\n",
      " [ 0.33941181]\n",
      " [ 0.21315001]\n",
      " ...\n",
      " [-0.56340275]\n",
      " [ 0.26608654]\n",
      " [-0.56340275]]\n",
      "t [[-2.33828059]\n",
      " [ 0.34546697]\n",
      " [ 0.21592886]\n",
      " ...\n",
      " [-0.57351985]\n",
      " [ 0.27077434]\n",
      " [-0.57351985]]\n",
      "t [[-2.33828059]\n",
      " [ 0.34546697]\n",
      " [ 0.21592886]\n",
      " ...\n",
      " [-0.57351985]\n",
      " [ 0.27077434]\n",
      " [-0.57351985]]\n",
      "Current iteration=46, loss=56950.63078470934\n",
      "t [[-2.36941935]\n",
      " [ 0.35145649]\n",
      " [ 0.21864399]\n",
      " ...\n",
      " [-0.583555  ]\n",
      " [ 0.27541237]\n",
      " [-0.583555  ]]\n",
      "t [[-2.36941935]\n",
      " [ 0.35145649]\n",
      " [ 0.21864399]\n",
      " ...\n",
      " [-0.583555  ]\n",
      " [ 0.27541237]\n",
      " [-0.583555  ]]\n",
      "t [[-2.40001495]\n",
      " [ 0.35738066]\n",
      " [ 0.22129647]\n",
      " ...\n",
      " [-0.59350934]\n",
      " [ 0.280001  ]\n",
      " [-0.59350934]]\n",
      "t [[-2.40001495]\n",
      " [ 0.35738066]\n",
      " [ 0.22129647]\n",
      " ...\n",
      " [-0.59350934]\n",
      " [ 0.280001  ]\n",
      " [-0.59350934]]\n",
      "Current iteration=48, loss=56603.32879057492\n",
      "t [[-2.43008013]\n",
      " [ 0.36323975]\n",
      " [ 0.22388738]\n",
      " ...\n",
      " [-0.60338399]\n",
      " [ 0.28454059]\n",
      " [-0.60338399]]\n",
      "t [[-2.43008013]\n",
      " [ 0.36323975]\n",
      " [ 0.22388738]\n",
      " ...\n",
      " [-0.60338399]\n",
      " [ 0.28454059]\n",
      " [-0.60338399]]\n",
      "t [[-2.4596273 ]\n",
      " [ 0.36903406]\n",
      " [ 0.22641775]\n",
      " ...\n",
      " [-0.61318005]\n",
      " [ 0.2890315 ]\n",
      " [-0.61318005]]\n",
      "loss=56266.2850548757\n",
      "Cross validation finished: optimal gamma 0.02\n",
      "logistic regression loss 56433.55177276884\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(set1_x.shape[1])\n",
    "gamma_opt = cross_validation(set1_y, set1_x_lr, k_fold, gammas, fonction=4)\n",
    "w_lr1, loss_lr = logistic_regression(set1_y, set1_x, initial_w, max_iters, gamma_opt)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"logistic regression loss {loss}\".format(loss=loss_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00272121]\n",
      " [-0.01162471]\n",
      " [-0.00730655]\n",
      " ...\n",
      " [-0.00529544]\n",
      " [ 0.00114649]\n",
      " [-0.00308696]]\n",
      "t [[ 0.00272121]\n",
      " [-0.01162471]\n",
      " [-0.00730655]\n",
      " ...\n",
      " [-0.00529544]\n",
      " [ 0.00114649]\n",
      " [-0.00308696]]\n",
      "t [[ 0.00540711]\n",
      " [-0.02312815]\n",
      " [-0.01453338]\n",
      " ...\n",
      " [-0.0105242 ]\n",
      " [ 0.00226895]\n",
      " [-0.00615564]]\n",
      "t [[ 0.00540711]\n",
      " [-0.02312815]\n",
      " [-0.01453338]\n",
      " ...\n",
      " [-0.0105242 ]\n",
      " [ 0.00226895]\n",
      " [-0.00615564]]\n",
      "Current iteration=2, loss=40198.00366476075\n",
      "t [[ 0.00805816]\n",
      " [-0.03451193]\n",
      " [-0.0216816 ]\n",
      " ...\n",
      " [-0.01568729]\n",
      " [ 0.00336768]\n",
      " [-0.00920621]]\n",
      "t [[ 0.00805816]\n",
      " [-0.03451193]\n",
      " [-0.0216816 ]\n",
      " ...\n",
      " [-0.01568729]\n",
      " [ 0.00336768]\n",
      " [-0.00920621]]\n",
      "t [[ 0.01067482]\n",
      " [-0.04577763]\n",
      " [-0.02875233]\n",
      " ...\n",
      " [-0.02078566]\n",
      " [ 0.00444299]\n",
      " [-0.01223884]]\n",
      "t [[ 0.01067482]\n",
      " [-0.04577763]\n",
      " [-0.02875233]\n",
      " ...\n",
      " [-0.02078566]\n",
      " [ 0.00444299]\n",
      " [-0.01223884]]\n",
      "Current iteration=4, loss=40087.6535873823\n",
      "t [[ 0.01325756]\n",
      " [-0.05692684]\n",
      " [-0.03574666]\n",
      " ...\n",
      " [-0.02582029]\n",
      " [ 0.0054952 ]\n",
      " [-0.01525369]]\n",
      "t [[ 0.01325756]\n",
      " [-0.05692684]\n",
      " [-0.03574666]\n",
      " ...\n",
      " [-0.02582029]\n",
      " [ 0.0054952 ]\n",
      " [-0.01525369]]\n",
      "t [[ 0.01580683]\n",
      " [-0.0679611 ]\n",
      " [-0.04266567]\n",
      " ...\n",
      " [-0.03079212]\n",
      " [ 0.0065246 ]\n",
      " [-0.01825094]]\n",
      "t [[ 0.01580683]\n",
      " [-0.0679611 ]\n",
      " [-0.04266567]\n",
      " ...\n",
      " [-0.03079212]\n",
      " [ 0.0065246 ]\n",
      " [-0.01825094]]\n",
      "Current iteration=6, loss=39980.82813214355\n",
      "t [[ 0.01832307]\n",
      " [-0.07888196]\n",
      " [-0.04951042]\n",
      " ...\n",
      " [-0.0357021 ]\n",
      " [ 0.0075315 ]\n",
      " [-0.02123075]]\n",
      "t [[ 0.01832307]\n",
      " [-0.07888196]\n",
      " [-0.04951042]\n",
      " ...\n",
      " [-0.0357021 ]\n",
      " [ 0.0075315 ]\n",
      " [-0.02123075]]\n",
      "t [[ 0.02080673]\n",
      " [-0.08969094]\n",
      " [-0.05628198]\n",
      " ...\n",
      " [-0.04055115]\n",
      " [ 0.0085162 ]\n",
      " [-0.02419329]]\n",
      "t [[ 0.02080673]\n",
      " [-0.08969094]\n",
      " [-0.05628198]\n",
      " ...\n",
      " [-0.04055115]\n",
      " [ 0.0085162 ]\n",
      " [-0.02419329]]\n",
      "Current iteration=8, loss=39877.36079180516\n",
      "t [[ 0.02325826]\n",
      " [-0.10038955]\n",
      " [-0.0629814 ]\n",
      " ...\n",
      " [-0.0453402 ]\n",
      " [ 0.00947899]\n",
      " [-0.02713871]]\n",
      "t [[ 0.02325826]\n",
      " [-0.10038955]\n",
      " [-0.0629814 ]\n",
      " ...\n",
      " [-0.0453402 ]\n",
      " [ 0.00947899]\n",
      " [-0.02713871]]\n",
      "t [[ 0.02567809]\n",
      " [-0.11097928]\n",
      " [-0.0696097 ]\n",
      " ...\n",
      " [-0.05007015]\n",
      " [ 0.01042016]\n",
      " [-0.03006718]]\n",
      "loss=39777.09357226104\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00035886]\n",
      " [-0.00220943]\n",
      " [-0.00692679]\n",
      " ...\n",
      " [-0.00531547]\n",
      " [ 0.00103336]\n",
      " [-0.00315337]]\n",
      "t [[ 0.00035886]\n",
      " [-0.00220943]\n",
      " [-0.00692679]\n",
      " ...\n",
      " [-0.00531547]\n",
      " [ 0.00103336]\n",
      " [-0.00315337]]\n",
      "t [[ 0.00070549]\n",
      " [-0.00443756]\n",
      " [-0.01377177]\n",
      " ...\n",
      " [-0.01056283]\n",
      " [ 0.00204325]\n",
      " [-0.00628805]]\n",
      "t [[ 0.00070549]\n",
      " [-0.00443756]\n",
      " [-0.01377177]\n",
      " ...\n",
      " [-0.01056283]\n",
      " [ 0.00204325]\n",
      " [-0.00628805]]\n",
      "Current iteration=2, loss=40196.658833863534\n",
      "t [[ 0.00104011]\n",
      " [-0.00668398]\n",
      " [-0.02053612]\n",
      " ...\n",
      " [-0.0157431 ]\n",
      " [ 0.00302998]\n",
      " [-0.0094042 ]]\n",
      "t [[ 0.00104011]\n",
      " [-0.00668398]\n",
      " [-0.02053612]\n",
      " ...\n",
      " [-0.0157431 ]\n",
      " [ 0.00302998]\n",
      " [-0.0094042 ]]\n",
      "t [[ 0.00136293]\n",
      " [-0.00894827]\n",
      " [-0.02722105]\n",
      " ...\n",
      " [-0.02085728]\n",
      " [ 0.00399386]\n",
      " [-0.01250201]]\n",
      "t [[ 0.00136293]\n",
      " [-0.00894827]\n",
      " [-0.02722105]\n",
      " ...\n",
      " [-0.02085728]\n",
      " [ 0.00399386]\n",
      " [-0.01250201]]\n",
      "Current iteration=4, loss=40085.01560997811\n",
      "t [[ 0.00167414]\n",
      " [-0.01123001]\n",
      " [-0.03382771]\n",
      " ...\n",
      " [-0.02590637]\n",
      " [ 0.0049352 ]\n",
      " [-0.01558165]]\n",
      "t [[ 0.00167414]\n",
      " [-0.01123001]\n",
      " [-0.03382771]\n",
      " ...\n",
      " [-0.02590637]\n",
      " [ 0.0049352 ]\n",
      " [-0.01558165]]\n",
      "t [[ 0.00197396]\n",
      " [-0.0135288 ]\n",
      " [-0.04035728]\n",
      " ...\n",
      " [-0.03089135]\n",
      " [ 0.0058543 ]\n",
      " [-0.01864329]]\n",
      "t [[ 0.00197396]\n",
      " [-0.0135288 ]\n",
      " [-0.04035728]\n",
      " ...\n",
      " [-0.03089135]\n",
      " [ 0.0058543 ]\n",
      " [-0.01864329]]\n",
      "Current iteration=6, loss=39976.94450690044\n",
      "t [[ 0.00226259]\n",
      " [-0.01584424]\n",
      " [-0.04681089]\n",
      " ...\n",
      " [-0.03581319]\n",
      " [ 0.00675147]\n",
      " [-0.0216871 ]]\n",
      "t [[ 0.00226259]\n",
      " [-0.01584424]\n",
      " [-0.04681089]\n",
      " ...\n",
      " [-0.03581319]\n",
      " [ 0.00675147]\n",
      " [-0.0216871 ]]\n",
      "t [[ 0.00254021]\n",
      " [-0.01817594]\n",
      " [-0.05318968]\n",
      " ...\n",
      " [-0.04067285]\n",
      " [ 0.007627  ]\n",
      " [-0.02471325]]\n",
      "t [[ 0.00254021]\n",
      " [-0.01817594]\n",
      " [-0.05318968]\n",
      " ...\n",
      " [-0.04067285]\n",
      " [ 0.007627  ]\n",
      " [-0.02471325]]\n",
      "Current iteration=8, loss=39872.27515785223\n",
      "t [[ 0.00280703]\n",
      " [-0.0205235 ]\n",
      " [-0.05949477]\n",
      " ...\n",
      " [-0.04547127]\n",
      " [ 0.00848119]\n",
      " [-0.02772191]]\n",
      "t [[ 0.00280703]\n",
      " [-0.0205235 ]\n",
      " [-0.05949477]\n",
      " ...\n",
      " [-0.04547127]\n",
      " [ 0.00848119]\n",
      " [-0.02772191]]\n",
      "t [[ 0.00306325]\n",
      " [-0.02288654]\n",
      " [-0.06572726]\n",
      " ...\n",
      " [-0.0502094 ]\n",
      " [ 0.00931433]\n",
      " [-0.03071324]]\n",
      "loss=39770.84601717691\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0003065 ]\n",
      " [-0.00193913]\n",
      " [-0.00700181]\n",
      " ...\n",
      " [-0.00522658]\n",
      " [ 0.00111107]\n",
      " [-0.00295788]]\n",
      "t [[ 0.0003065 ]\n",
      " [-0.00193913]\n",
      " [-0.00700181]\n",
      " ...\n",
      " [-0.00522658]\n",
      " [ 0.00111107]\n",
      " [-0.00295788]]\n",
      "t [[ 0.0006009 ]\n",
      " [-0.00389783]\n",
      " [-0.01392198]\n",
      " ...\n",
      " [-0.01038598]\n",
      " [ 0.00219843]\n",
      " [-0.00589769]]\n",
      "t [[ 0.0006009 ]\n",
      " [-0.00389783]\n",
      " [-0.01392198]\n",
      " ...\n",
      " [-0.01038598]\n",
      " [ 0.00219843]\n",
      " [-0.00589769]]\n",
      "Current iteration=2, loss=40197.86056761726\n",
      "t [[ 0.00088341]\n",
      " [-0.00587567]\n",
      " [-0.02076168]\n",
      " ...\n",
      " [-0.0154792 ]\n",
      " [ 0.00326241]\n",
      " [-0.0088196 ]]\n",
      "t [[ 0.00088341]\n",
      " [-0.00587567]\n",
      " [-0.02076168]\n",
      " ...\n",
      " [-0.0154792 ]\n",
      " [ 0.00326241]\n",
      " [-0.0088196 ]]\n",
      "t [[ 0.00115424]\n",
      " [-0.00787225]\n",
      " [-0.0275221 ]\n",
      " ...\n",
      " [-0.02050722]\n",
      " [ 0.00430331]\n",
      " [-0.01172379]]\n",
      "t [[ 0.00115424]\n",
      " [-0.00787225]\n",
      " [-0.0275221 ]\n",
      " ...\n",
      " [-0.02050722]\n",
      " [ 0.00430331]\n",
      " [-0.01172379]]\n",
      "Current iteration=4, loss=40087.38858871582\n",
      "t [[ 0.00141359]\n",
      " [-0.00988714]\n",
      " [-0.0342044 ]\n",
      " ...\n",
      " [-0.02547104]\n",
      " [ 0.00532143]\n",
      " [-0.01461042]]\n",
      "t [[ 0.00141359]\n",
      " [-0.00988714]\n",
      " [-0.0342044 ]\n",
      " ...\n",
      " [-0.02547104]\n",
      " [ 0.00532143]\n",
      " [-0.01461042]]\n",
      "t [[ 0.00166168]\n",
      " [-0.01191993]\n",
      " [-0.04080973]\n",
      " ...\n",
      " [-0.03037161]\n",
      " [ 0.00631708]\n",
      " [-0.01747967]]\n",
      "t [[ 0.00166168]\n",
      " [-0.01191993]\n",
      " [-0.04080973]\n",
      " ...\n",
      " [-0.03037161]\n",
      " [ 0.00631708]\n",
      " [-0.01747967]]\n",
      "Current iteration=6, loss=39980.460229434175\n",
      "t [[ 0.00189869]\n",
      " [-0.01397024]\n",
      " [-0.04733922]\n",
      " ...\n",
      " [-0.0352099 ]\n",
      " [ 0.00729057]\n",
      " [-0.0203317 ]]\n",
      "t [[ 0.00189869]\n",
      " [-0.01397024]\n",
      " [-0.04733922]\n",
      " ...\n",
      " [-0.0352099 ]\n",
      " [ 0.00729057]\n",
      " [-0.0203317 ]]\n",
      "t [[ 0.00212483]\n",
      " [-0.01603765]\n",
      " [-0.05379399]\n",
      " ...\n",
      " [-0.03998684]\n",
      " [ 0.00824219]\n",
      " [-0.02316668]]\n",
      "t [[ 0.00212483]\n",
      " [-0.01603765]\n",
      " [-0.05379399]\n",
      " ...\n",
      " [-0.03998684]\n",
      " [ 0.00824219]\n",
      " [-0.02316668]]\n",
      "Current iteration=8, loss=39876.906972706725\n",
      "t [[ 0.0023403 ]\n",
      " [-0.01812179]\n",
      " [-0.06017515]\n",
      " ...\n",
      " [-0.04470338]\n",
      " [ 0.00917223]\n",
      " [-0.02598477]]\n",
      "t [[ 0.0023403 ]\n",
      " [-0.01812179]\n",
      " [-0.06017515]\n",
      " ...\n",
      " [-0.04470338]\n",
      " [ 0.00917223]\n",
      " [-0.02598477]]\n",
      "t [[ 0.00254529]\n",
      " [-0.02022225]\n",
      " [-0.06648381]\n",
      " ...\n",
      " [-0.04936042]\n",
      " [ 0.010081  ]\n",
      " [-0.02878614]]\n",
      "loss=39776.568989900385\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00025526]\n",
      " [-0.00205387]\n",
      " [-0.00695561]\n",
      " ...\n",
      " [-0.00306238]\n",
      " [-0.00036882]\n",
      " [-0.00624774]]\n",
      "t [[ 0.00025526]\n",
      " [-0.00205387]\n",
      " [-0.00695561]\n",
      " ...\n",
      " [-0.00306238]\n",
      " [-0.00036882]\n",
      " [-0.00624774]]\n",
      "t [[ 0.00049865]\n",
      " [-0.00412718]\n",
      " [-0.0138289 ]\n",
      " ...\n",
      " [-0.00607199]\n",
      " [-0.0007202 ]\n",
      " [-0.01246479]]\n",
      "t [[ 0.00049865]\n",
      " [-0.00412718]\n",
      " [-0.0138289 ]\n",
      " ...\n",
      " [-0.00607199]\n",
      " [-0.0007202 ]\n",
      " [-0.01246479]]\n",
      "Current iteration=2, loss=40195.82388529231\n",
      "t [[ 0.00073039]\n",
      " [-0.0062195 ]\n",
      " [-0.02062105]\n",
      " ...\n",
      " [-0.00902969]\n",
      " [-0.00105448]\n",
      " [-0.01865138]]\n",
      "t [[ 0.00073039]\n",
      " [-0.0062195 ]\n",
      " [-0.02062105]\n",
      " ...\n",
      " [-0.00902969]\n",
      " [-0.00105448]\n",
      " [-0.01865138]]\n",
      "t [[ 0.00095068]\n",
      " [-0.00833041]\n",
      " [-0.02733328]\n",
      " ...\n",
      " [-0.01193631]\n",
      " [-0.001372  ]\n",
      " [-0.02480777]]\n",
      "t [[ 0.00095068]\n",
      " [-0.00833041]\n",
      " [-0.02733328]\n",
      " ...\n",
      " [-0.01193631]\n",
      " [-0.001372  ]\n",
      " [-0.02480777]]\n",
      "Current iteration=4, loss=40083.39157744605\n",
      "t [[ 0.00115974]\n",
      " [-0.0104595 ]\n",
      " [-0.03396674]\n",
      " ...\n",
      " [-0.01479268]\n",
      " [-0.0016731 ]\n",
      " [-0.03093421]]\n",
      "t [[ 0.00115974]\n",
      " [-0.0104595 ]\n",
      " [-0.03396674]\n",
      " ...\n",
      " [-0.01479268]\n",
      " [-0.0016731 ]\n",
      " [-0.03093421]]\n",
      "t [[ 0.00135776]\n",
      " [-0.01260635]\n",
      " [-0.04052262]\n",
      " ...\n",
      " [-0.01759961]\n",
      " [-0.00195811]\n",
      " [-0.03703097]]\n",
      "t [[ 0.00135776]\n",
      " [-0.01260635]\n",
      " [-0.04052262]\n",
      " ...\n",
      " [-0.01759961]\n",
      " [-0.00195811]\n",
      " [-0.03703097]]\n",
      "Current iteration=6, loss=39974.57497261882\n",
      "t [[ 0.00154494]\n",
      " [-0.01477057]\n",
      " [-0.04700205]\n",
      " ...\n",
      " [-0.02035793]\n",
      " [-0.00222736]\n",
      " [-0.04309827]]\n",
      "t [[ 0.00154494]\n",
      " [-0.01477057]\n",
      " [-0.04700205]\n",
      " ...\n",
      " [-0.02035793]\n",
      " [-0.00222736]\n",
      " [-0.04309827]]\n",
      "t [[ 0.00172149]\n",
      " [-0.01695174]\n",
      " [-0.05340617]\n",
      " ...\n",
      " [-0.02306842]\n",
      " [-0.00248116]\n",
      " [-0.04913637]]\n",
      "t [[ 0.00172149]\n",
      " [-0.01695174]\n",
      " [-0.05340617]\n",
      " ...\n",
      " [-0.02306842]\n",
      " [-0.00248116]\n",
      " [-0.04913637]]\n",
      "Current iteration=8, loss=39869.201521874435\n",
      "t [[ 0.0018876 ]\n",
      " [-0.01914949]\n",
      " [-0.05973611]\n",
      " ...\n",
      " [-0.02573188]\n",
      " [-0.00271985]\n",
      " [-0.05514551]]\n",
      "t [[ 0.0018876 ]\n",
      " [-0.01914949]\n",
      " [-0.05973611]\n",
      " ...\n",
      " [-0.02573188]\n",
      " [-0.00271985]\n",
      " [-0.05514551]]\n",
      "t [[ 0.00204346]\n",
      " [-0.02136341]\n",
      " [-0.06599298]\n",
      " ...\n",
      " [-0.02834907]\n",
      " [-0.00294372]\n",
      " [-0.06112593]]\n",
      "loss=39767.1075941708\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00544243]\n",
      " [-0.02324942]\n",
      " [-0.0146131 ]\n",
      " ...\n",
      " [-0.01059087]\n",
      " [ 0.00229298]\n",
      " [-0.00617392]]\n",
      "t [[ 0.00544243]\n",
      " [-0.02324942]\n",
      " [-0.0146131 ]\n",
      " ...\n",
      " [-0.01059087]\n",
      " [ 0.00229298]\n",
      " [-0.00617392]]\n",
      "t [[ 0.01074359]\n",
      " [-0.04601378]\n",
      " [-0.02890733]\n",
      " ...\n",
      " [-0.02091508]\n",
      " [ 0.00448982]\n",
      " [-0.01227472]]\n",
      "t [[ 0.01074359]\n",
      " [-0.04601378]\n",
      " [-0.02890733]\n",
      " ...\n",
      " [-0.02091508]\n",
      " [ 0.00448982]\n",
      " [-0.01227472]]\n",
      "Current iteration=2, loss=40086.76187560591\n",
      "t [[ 0.01590726]\n",
      " [-0.06830599]\n",
      " [-0.04289167]\n",
      " ...\n",
      " [-0.03098053]\n",
      " [ 0.00659304]\n",
      " [-0.01830376]]\n",
      "t [[ 0.01590726]\n",
      " [-0.06830599]\n",
      " [-0.04289167]\n",
      " ...\n",
      " [-0.03098053]\n",
      " [ 0.00659304]\n",
      " [-0.01830376]]\n",
      "t [[ 0.02093711]\n",
      " [-0.09013868]\n",
      " [-0.0565749 ]\n",
      " ...\n",
      " [-0.04079496]\n",
      " [ 0.00860508]\n",
      " [-0.02426241]]\n",
      "t [[ 0.02093711]\n",
      " [-0.09013868]\n",
      " [-0.0565749 ]\n",
      " ...\n",
      " [-0.04079496]\n",
      " [ 0.00860508]\n",
      " [-0.02426241]]\n",
      "Current iteration=4, loss=39875.74092270465\n",
      "t [[ 0.02583675]\n",
      " [-0.11152421]\n",
      " [-0.06996562]\n",
      " ...\n",
      " [-0.05036591]\n",
      " [ 0.01052838]\n",
      " [-0.030152  ]]\n",
      "t [[ 0.02583675]\n",
      " [-0.11152421]\n",
      " [-0.06996562]\n",
      " ...\n",
      " [-0.05036591]\n",
      " [ 0.01052838]\n",
      " [-0.030152  ]]\n",
      "t [[ 0.03060971]\n",
      " [-0.13247467]\n",
      " [-0.08307219]\n",
      " ...\n",
      " [-0.05970074]\n",
      " [ 0.0123653 ]\n",
      " [-0.03597383]]\n",
      "t [[ 0.03060971]\n",
      " [-0.13247467]\n",
      " [-0.08307219]\n",
      " ...\n",
      " [-0.05970074]\n",
      " [ 0.0123653 ]\n",
      " [-0.03597383]]\n",
      "Current iteration=6, loss=39677.66652901402\n",
      "t [[ 0.03525944]\n",
      " [-0.15300186]\n",
      " [-0.0959028 ]\n",
      " ...\n",
      " [-0.06880664]\n",
      " [ 0.01411818]\n",
      " [-0.04172919]]\n",
      "t [[ 0.03525944]\n",
      " [-0.15300186]\n",
      " [-0.0959028 ]\n",
      " ...\n",
      " [-0.06880664]\n",
      " [ 0.01411818]\n",
      " [-0.04172919]]\n",
      "t [[ 0.0397893 ]\n",
      " [-0.17311728]\n",
      " [-0.1084654 ]\n",
      " ...\n",
      " [-0.07769059]\n",
      " [ 0.01578927]\n",
      " [-0.04741934]]\n",
      "t [[ 0.0397893 ]\n",
      " [-0.17311728]\n",
      " [-0.1084654 ]\n",
      " ...\n",
      " [-0.07769059]\n",
      " [ 0.01578927]\n",
      " [-0.04741934]]\n",
      "Current iteration=8, loss=39491.34841084758\n",
      "t [[ 0.04420259]\n",
      " [-0.19283215]\n",
      " [-0.12076777]\n",
      " ...\n",
      " [-0.08635939]\n",
      " [ 0.0173808 ]\n",
      " [-0.0530455 ]]\n",
      "t [[ 0.04420259]\n",
      " [-0.19283215]\n",
      " [-0.12076777]\n",
      " ...\n",
      " [-0.08635939]\n",
      " [ 0.0173808 ]\n",
      " [-0.0530455 ]]\n",
      "t [[ 0.0485025 ]\n",
      " [-0.21215739]\n",
      " [-0.13281743]\n",
      " ...\n",
      " [-0.09481964]\n",
      " [ 0.01889494]\n",
      " [-0.05860889]]\n",
      "loss=39315.718832111714\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00071771]\n",
      " [-0.00441886]\n",
      " [-0.01385358]\n",
      " ...\n",
      " [-0.01063094]\n",
      " [ 0.00206673]\n",
      " [-0.00630675]]\n",
      "t [[ 0.00071771]\n",
      " [-0.00441886]\n",
      " [-0.01385358]\n",
      " ...\n",
      " [-0.01063094]\n",
      " [ 0.00206673]\n",
      " [-0.00630675]]\n",
      "t [[ 0.00138654]\n",
      " [-0.00891254]\n",
      " [-0.02737991]\n",
      " ...\n",
      " [-0.02098945]\n",
      " [ 0.00403957]\n",
      " [-0.0125387 ]]\n",
      "t [[ 0.00138654]\n",
      " [-0.00891254]\n",
      " [-0.02737991]\n",
      " ...\n",
      " [-0.02098945]\n",
      " [ 0.00403957]\n",
      " [-0.0125387 ]]\n",
      "Current iteration=2, loss=40084.11177741806\n",
      "t [[ 0.00200816]\n",
      " [-0.01347765]\n",
      " [-0.04058861]\n",
      " ...\n",
      " [-0.03108371]\n",
      " [ 0.00592103]\n",
      " [-0.01869729]]\n",
      "t [[ 0.00200816]\n",
      " [-0.01347765]\n",
      " [-0.04058861]\n",
      " ...\n",
      " [-0.03108371]\n",
      " [ 0.00592103]\n",
      " [-0.01869729]]\n",
      "t [[ 0.00258424]\n",
      " [-0.01811089]\n",
      " [-0.05348911]\n",
      " ...\n",
      " [-0.04092168]\n",
      " [ 0.0077136 ]\n",
      " [-0.02478391]]\n",
      "t [[ 0.00258424]\n",
      " [-0.01811089]\n",
      " [-0.05348911]\n",
      " ...\n",
      " [-0.04092168]\n",
      " [ 0.0077136 ]\n",
      " [-0.02478391]]\n",
      "Current iteration=4, loss=39870.63480446351\n",
      "t [[ 0.00311637]\n",
      " [-0.02280905]\n",
      " [-0.06609059]\n",
      " ...\n",
      " [-0.05051116]\n",
      " [ 0.0094197 ]\n",
      " [-0.03079993]]\n",
      "t [[ 0.00311637]\n",
      " [-0.02280905]\n",
      " [-0.06609059]\n",
      " ...\n",
      " [-0.05051116]\n",
      " [ 0.0094197 ]\n",
      " [-0.03079993]]\n",
      "t [[ 0.00360613]\n",
      " [-0.027569  ]\n",
      " [-0.07840203]\n",
      " ...\n",
      " [-0.05985975]\n",
      " [ 0.01104171]\n",
      " [-0.0367467 ]]\n",
      "t [[ 0.00360613]\n",
      " [-0.027569  ]\n",
      " [-0.07840203]\n",
      " ...\n",
      " [-0.05985975]\n",
      " [ 0.01104171]\n",
      " [-0.0367467 ]]\n",
      "Current iteration=6, loss=39670.267882845335\n",
      "t [[ 0.00405506]\n",
      " [-0.03238772]\n",
      " [-0.09043218]\n",
      " ...\n",
      " [-0.06897484]\n",
      " [ 0.01258196]\n",
      " [-0.04262554]]\n",
      "t [[ 0.00405506]\n",
      " [-0.03238772]\n",
      " [-0.09043218]\n",
      " ...\n",
      " [-0.06897484]\n",
      " [ 0.01258196]\n",
      " [-0.04262554]]\n",
      "t [[ 0.00446463]\n",
      " [-0.03726226]\n",
      " [-0.10218953]\n",
      " ...\n",
      " [-0.07786365]\n",
      " [ 0.01404271]\n",
      " [-0.04843775]]\n",
      "t [[ 0.00446463]\n",
      " [-0.03726226]\n",
      " [-0.10218953]\n",
      " ...\n",
      " [-0.07786365]\n",
      " [ 0.01404271]\n",
      " [-0.04843775]]\n",
      "Current iteration=8, loss=39481.79494711808\n",
      "t [[ 0.00483631]\n",
      " [-0.04218977]\n",
      " [-0.11368237]\n",
      " ...\n",
      " [-0.08653317]\n",
      " [ 0.01542619]\n",
      " [-0.05418459]]\n",
      "t [[ 0.00483631]\n",
      " [-0.04218977]\n",
      " [-0.11368237]\n",
      " ...\n",
      " [-0.08653317]\n",
      " [ 0.01542619]\n",
      " [-0.05418459]]\n",
      "t [[ 0.00517149]\n",
      " [-0.04716749]\n",
      " [-0.12491874]\n",
      " ...\n",
      " [-0.09499021]\n",
      " [ 0.01673456]\n",
      " [-0.05986731]]\n",
      "loss=39304.126686231066\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00061301]\n",
      " [-0.00387826]\n",
      " [-0.01400362]\n",
      " ...\n",
      " [-0.01045316]\n",
      " [ 0.00222214]\n",
      " [-0.00591575]]\n",
      "t [[ 0.00061301]\n",
      " [-0.00387826]\n",
      " [-0.01400362]\n",
      " ...\n",
      " [-0.01045316]\n",
      " [ 0.00222214]\n",
      " [-0.00591575]]\n",
      "t [[ 0.00117761]\n",
      " [-0.00783479]\n",
      " [-0.02768067]\n",
      " ...\n",
      " [-0.0206376 ]\n",
      " [ 0.00434947]\n",
      " [-0.01175924]]\n",
      "t [[ 0.00117761]\n",
      " [-0.00783479]\n",
      " [-0.02768067]\n",
      " ...\n",
      " [-0.0206376 ]\n",
      " [ 0.00434947]\n",
      " [-0.01175924]]\n",
      "Current iteration=2, loss=40086.492001289604\n",
      "t [[ 0.0016955 ]\n",
      " [-0.0118662 ]\n",
      " [-0.04104068]\n",
      " ...\n",
      " [-0.03056137]\n",
      " [ 0.00638451]\n",
      " [-0.01753183]]\n",
      "t [[ 0.0016955 ]\n",
      " [-0.0118662 ]\n",
      " [-0.04104068]\n",
      " ...\n",
      " [-0.03056137]\n",
      " [ 0.00638451]\n",
      " [-0.01753183]]\n",
      "t [[ 0.00216834]\n",
      " [-0.01596918]\n",
      " [-0.05409299]\n",
      " ...\n",
      " [-0.04023233]\n",
      " [ 0.00832973]\n",
      " [-0.02323492]]\n",
      "t [[ 0.00216834]\n",
      " [-0.01596918]\n",
      " [-0.05409299]\n",
      " ...\n",
      " [-0.04023233]\n",
      " [ 0.00832973]\n",
      " [-0.02323492]]\n",
      "Current iteration=4, loss=39875.279316458145\n",
      "t [[ 0.00259775]\n",
      " [-0.0201405 ]\n",
      " [-0.0668467 ]\n",
      " ...\n",
      " [-0.04965813]\n",
      " [ 0.01018754]\n",
      " [-0.02886983]]\n",
      "t [[ 0.00259775]\n",
      " [-0.0201405 ]\n",
      " [-0.0668467 ]\n",
      " ...\n",
      " [-0.04965813]\n",
      " [ 0.01018754]\n",
      " [-0.02886983]]\n",
      "t [[ 0.00298532]\n",
      " [-0.02437703]\n",
      " [-0.07931071]\n",
      " ...\n",
      " [-0.05884627]\n",
      " [ 0.01196031]\n",
      " [-0.03443789]]\n",
      "t [[ 0.00298532]\n",
      " [-0.02437703]\n",
      " [-0.07931071]\n",
      " ...\n",
      " [-0.05884627]\n",
      " [ 0.01196031]\n",
      " [-0.03443789]]\n",
      "Current iteration=6, loss=39677.075424830764\n",
      "t [[ 0.00333258]\n",
      " [-0.02867572]\n",
      " [-0.09149367]\n",
      " ...\n",
      " [-0.06780403]\n",
      " [ 0.01365038]\n",
      " [-0.0399404 ]]\n",
      "t [[ 0.00333258]\n",
      " [-0.02867572]\n",
      " [-0.09149367]\n",
      " ...\n",
      " [-0.06780403]\n",
      " [ 0.01365038]\n",
      " [-0.0399404 ]]\n",
      "t [[ 0.00364102]\n",
      " [-0.03303362]\n",
      " [-0.10340402]\n",
      " ...\n",
      " [-0.0765385 ]\n",
      " [ 0.01525999]\n",
      " [-0.04537862]]\n",
      "t [[ 0.00364102]\n",
      " [-0.03303362]\n",
      " [-0.10340402]\n",
      " ...\n",
      " [-0.0765385 ]\n",
      " [ 0.01525999]\n",
      " [-0.04537862]]\n",
      "Current iteration=8, loss=39490.6768449918\n",
      "t [[ 0.00391211]\n",
      " [-0.03744786]\n",
      " [-0.11504996]\n",
      " ...\n",
      " [-0.08505659]\n",
      " [ 0.01679137]\n",
      " [-0.0507538 ]]\n",
      "t [[ 0.00391211]\n",
      " [-0.03744786]\n",
      " [-0.11504996]\n",
      " ...\n",
      " [-0.08505659]\n",
      " [ 0.01679137]\n",
      " [-0.0507538 ]]\n",
      "t [[ 0.00414726]\n",
      " [-0.04191566]\n",
      " [-0.12643947]\n",
      " ...\n",
      " [-0.093365  ]\n",
      " [ 0.01824666]\n",
      " [-0.05606716]]\n",
      "loss=39315.00496587191\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00051051]\n",
      " [-0.00410774]\n",
      " [-0.01391122]\n",
      " ...\n",
      " [-0.00612475]\n",
      " [-0.00073765]\n",
      " [-0.01249549]]\n",
      "t [[ 0.00051051]\n",
      " [-0.00410774]\n",
      " [-0.01391122]\n",
      " ...\n",
      " [-0.00612475]\n",
      " [-0.00073765]\n",
      " [-0.01249549]]\n",
      "t [[ 0.00097357]\n",
      " [-0.00829323]\n",
      " [-0.02749315]\n",
      " ...\n",
      " [-0.01203846]\n",
      " [-0.00140552]\n",
      " [-0.02486816]]\n",
      "t [[ 0.00097357]\n",
      " [-0.00829323]\n",
      " [-0.02749315]\n",
      " ...\n",
      " [-0.01203846]\n",
      " [-0.00140552]\n",
      " [-0.02486816]]\n",
      "Current iteration=2, loss=40082.47671227152\n",
      "t [[ 0.00139087]\n",
      " [-0.01255305]\n",
      " [-0.04075543]\n",
      " ...\n",
      " [-0.01774793]\n",
      " [-0.00200638]\n",
      " [-0.03712007]]\n",
      "t [[ 0.00139087]\n",
      " [-0.01255305]\n",
      " [-0.04075543]\n",
      " ...\n",
      " [-0.01774793]\n",
      " [-0.00200638]\n",
      " [-0.03712007]]\n",
      "t [[ 0.00176407]\n",
      " [-0.01688386]\n",
      " [-0.05370754]\n",
      " ...\n",
      " [-0.0232598 ]\n",
      " [-0.00254294]\n",
      " [-0.04925322]]\n",
      "t [[ 0.00176407]\n",
      " [-0.01688386]\n",
      " [-0.05370754]\n",
      " ...\n",
      " [-0.0232598 ]\n",
      " [-0.00254294]\n",
      " [-0.04925322]]\n",
      "Current iteration=4, loss=39867.541258390615\n",
      "t [[ 0.00209477]\n",
      " [-0.02128242]\n",
      " [-0.06635869]\n",
      " ...\n",
      " [-0.02858057]\n",
      " [-0.00301781]\n",
      " [-0.06126962]]\n",
      "t [[ 0.00209477]\n",
      " [-0.02128242]\n",
      " [-0.06635869]\n",
      " ...\n",
      " [-0.02858057]\n",
      " [-0.00301781]\n",
      " [-0.06126962]]\n",
      "t [[ 0.00238455]\n",
      " [-0.02574558]\n",
      " [-0.0787179 ]\n",
      " ...\n",
      " [-0.03371655]\n",
      " [-0.00343354]\n",
      " [-0.07317121]]\n",
      "t [[ 0.00238455]\n",
      " [-0.02574558]\n",
      " [-0.0787179 ]\n",
      " ...\n",
      " [-0.03371655]\n",
      " [-0.00343354]\n",
      " [-0.07317121]]\n",
      "Current iteration=6, loss=39665.875072568364\n",
      "t [[ 0.00263494]\n",
      " [-0.03027027]\n",
      " [-0.09079393]\n",
      " ...\n",
      " [-0.03867389]\n",
      " [-0.0037926 ]\n",
      " [-0.08495993]]\n",
      "t [[ 0.00263494]\n",
      " [-0.03027027]\n",
      " [-0.09079393]\n",
      " ...\n",
      " [-0.03867389]\n",
      " [-0.0037926 ]\n",
      " [-0.08495993]]\n",
      "t [[ 0.00284743]\n",
      " [-0.03485352]\n",
      " [-0.10259533]\n",
      " ...\n",
      " [-0.04345855]\n",
      " [-0.00409738]\n",
      " [-0.09663766]]\n",
      "t [[ 0.00284743]\n",
      " [-0.03485352]\n",
      " [-0.10259533]\n",
      " ...\n",
      " [-0.04345855]\n",
      " [-0.00409738]\n",
      " [-0.09663766]]\n",
      "Current iteration=8, loss=39476.24624524176\n",
      "t [[ 0.00302346]\n",
      " [-0.03949245]\n",
      " [-0.1141304 ]\n",
      " ...\n",
      " [-0.04807635]\n",
      " [-0.00435019]\n",
      " [-0.10820628]]\n",
      "t [[ 0.00302346]\n",
      " [-0.03949245]\n",
      " [-0.1141304 ]\n",
      " ...\n",
      " [-0.04807635]\n",
      " [-0.00435019]\n",
      " [-0.10820628]]\n",
      "t [[ 0.00316445]\n",
      " [-0.04418426]\n",
      " [-0.12540723]\n",
      " ...\n",
      " [-0.05253293]\n",
      " [-0.00455328]\n",
      " [-0.1196676 ]]\n",
      "loss=39297.551034674565\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00816364]\n",
      " [-0.03487413]\n",
      " [-0.02191964]\n",
      " ...\n",
      " [-0.01588631]\n",
      " [ 0.00343947]\n",
      " [-0.00926088]]\n",
      "t [[ 0.00816364]\n",
      " [-0.03487413]\n",
      " [-0.02191964]\n",
      " ...\n",
      " [-0.01588631]\n",
      " [ 0.00343947]\n",
      " [-0.00926088]]\n",
      "t [[ 0.01600946]\n",
      " [-0.06865695]\n",
      " [-0.04312188]\n",
      " ...\n",
      " [-0.03117266]\n",
      " [ 0.00666265]\n",
      " [-0.01835724]]\n",
      "t [[ 0.01600946]\n",
      " [-0.06865695]\n",
      " [-0.04312188]\n",
      " ...\n",
      " [-0.03117266]\n",
      " [ 0.00666265]\n",
      " [-0.01835724]]\n",
      "Current iteration=2, loss=39978.259418688\n",
      "t [[ 0.02355021]\n",
      " [-0.10139218]\n",
      " [-0.06363714]\n",
      " ...\n",
      " [-0.04588584]\n",
      " [ 0.00967803]\n",
      " [-0.02729374]]\n",
      "t [[ 0.02355021]\n",
      " [-0.10139218]\n",
      " [-0.06363714]\n",
      " ...\n",
      " [-0.04588584]\n",
      " [ 0.00967803]\n",
      " [-0.02729374]]\n",
      "t [[ 0.03079822]\n",
      " [-0.13312215]\n",
      " [-0.08349481]\n",
      " ...\n",
      " [-0.06005171]\n",
      " [ 0.01249391]\n",
      " [-0.03607492]]\n",
      "t [[ 0.03079822]\n",
      " [-0.13312215]\n",
      " [-0.08349481]\n",
      " ...\n",
      " [-0.06005171]\n",
      " [ 0.01249391]\n",
      " [-0.03607492]]\n",
      "Current iteration=4, loss=39675.425373273174\n",
      "t [[ 0.03776544]\n",
      " [-0.16388772]\n",
      " [-0.10272323]\n",
      " ...\n",
      " [-0.07369516]\n",
      " [ 0.0151183 ]\n",
      " [-0.04470523]]\n",
      "t [[ 0.03776544]\n",
      " [-0.16388772]\n",
      " [-0.10272323]\n",
      " ...\n",
      " [-0.07369516]\n",
      " [ 0.0151183 ]\n",
      " [-0.04470523]]\n",
      "t [[ 0.04446337]\n",
      " [-0.19372828]\n",
      " [-0.12134968]\n",
      " ...\n",
      " [-0.08684011]\n",
      " [ 0.01755898]\n",
      " [-0.05318894]]\n",
      "t [[ 0.04446337]\n",
      " [-0.19372828]\n",
      " [-0.12134968]\n",
      " ...\n",
      " [-0.08684011]\n",
      " [ 0.01755898]\n",
      " [-0.05318894]]\n",
      "Current iteration=6, loss=39399.338491418115\n",
      "t [[ 0.05090312]\n",
      " [-0.22268169]\n",
      " [-0.13940033]\n",
      " ...\n",
      " [-0.09950951]\n",
      " [ 0.01982343]\n",
      " [-0.06153024]]\n",
      "t [[ 0.05090312]\n",
      " [-0.22268169]\n",
      " [-0.13940033]\n",
      " ...\n",
      " [-0.09950951]\n",
      " [ 0.01982343]\n",
      " [-0.06153024]]\n",
      "t [[ 0.05709533]\n",
      " [-0.25078428]\n",
      " [-0.15690026]\n",
      " ...\n",
      " [-0.11172531]\n",
      " [ 0.02191887]\n",
      " [-0.06973313]]\n",
      "t [[ 0.05709533]\n",
      " [-0.25078428]\n",
      " [-0.15690026]\n",
      " ...\n",
      " [-0.11172531]\n",
      " [ 0.02191887]\n",
      " [-0.06973313]]\n",
      "Current iteration=8, loss=39146.422654069596\n",
      "t [[ 0.06305026]\n",
      " [-0.27807091]\n",
      " [-0.17387347]\n",
      " ...\n",
      " [-0.12350852]\n",
      " [ 0.02385222]\n",
      " [-0.07780152]]\n",
      "t [[ 0.06305026]\n",
      " [-0.27807091]\n",
      " [-0.17387347]\n",
      " ...\n",
      " [-0.12350852]\n",
      " [ 0.02385222]\n",
      " [-0.07780152]]\n",
      "t [[ 0.06877773]\n",
      " [-0.30457492]\n",
      " [-0.1903429 ]\n",
      " ...\n",
      " [-0.13487918]\n",
      " [ 0.02563013]\n",
      " [-0.08573917]]\n",
      "loss=38913.65118838852\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00107657]\n",
      " [-0.00662829]\n",
      " [-0.02078038]\n",
      " ...\n",
      " [-0.01594641]\n",
      " [ 0.00310009]\n",
      " [-0.00946012]]\n",
      "t [[ 0.00107657]\n",
      " [-0.00662829]\n",
      " [-0.02078038]\n",
      " ...\n",
      " [-0.01594641]\n",
      " [ 0.00310009]\n",
      " [-0.00946012]]\n",
      "t [[ 0.00204315]\n",
      " [-0.01342491]\n",
      " [-0.04082447]\n",
      " ...\n",
      " [-0.0312799 ]\n",
      " [ 0.00598895]\n",
      " [-0.01875197]]\n",
      "t [[ 0.00204315]\n",
      " [-0.01342491]\n",
      " [-0.04082447]\n",
      " ...\n",
      " [-0.0312799 ]\n",
      " [ 0.00598895]\n",
      " [-0.01875197]]\n",
      "Current iteration=2, loss=39974.341930955474\n",
      "t [[ 0.00290547]\n",
      " [-0.02037838]\n",
      " [-0.06016491]\n",
      " ...\n",
      " [-0.04602813]\n",
      " [ 0.00867511]\n",
      " [-0.02788038]]\n",
      "t [[ 0.00290547]\n",
      " [-0.02037838]\n",
      " [-0.06016491]\n",
      " ...\n",
      " [-0.04602813]\n",
      " [ 0.00867511]\n",
      " [-0.02788038]]\n",
      "t [[ 0.00366903]\n",
      " [-0.0274777 ]\n",
      " [-0.07883322]\n",
      " ...\n",
      " [-0.06021778]\n",
      " [ 0.01116689]\n",
      " [-0.03685002]]\n",
      "t [[ 0.00366903]\n",
      " [-0.0274777 ]\n",
      " [-0.07883322]\n",
      " ...\n",
      " [-0.06021778]\n",
      " [ 0.01116689]\n",
      " [-0.03685002]]\n",
      "Current iteration=4, loss=39668.000131877474\n",
      "t [[ 0.00433917]\n",
      " [-0.03471231]\n",
      " [-0.09685971]\n",
      " ...\n",
      " [-0.07387455]\n",
      " [ 0.01347233]\n",
      " [-0.04566546]]\n",
      "t [[ 0.00433917]\n",
      " [-0.03471231]\n",
      " [-0.09685971]\n",
      " ...\n",
      " [-0.07387455]\n",
      " [ 0.01347233]\n",
      " [-0.04566546]]\n",
      "t [[ 0.00492097]\n",
      " [-0.04207212]\n",
      " [-0.11427353]\n",
      " ...\n",
      " [-0.0870231 ]\n",
      " [ 0.01559921]\n",
      " [-0.05433114]]\n",
      "t [[ 0.00492097]\n",
      " [-0.04207212]\n",
      " [-0.11427353]\n",
      " ...\n",
      " [-0.0870231 ]\n",
      " [ 0.01559921]\n",
      " [-0.05433114]]\n",
      "Current iteration=6, loss=39388.721290518675\n",
      "t [[ 0.00541934]\n",
      " [-0.04954753]\n",
      " [-0.1311026 ]\n",
      " ...\n",
      " [-0.09968706]\n",
      " [ 0.01755503]\n",
      " [-0.06285132]]\n",
      "t [[ 0.00541934]\n",
      " [-0.04954753]\n",
      " [-0.1311026 ]\n",
      " ...\n",
      " [-0.09968706]\n",
      " [ 0.01755503]\n",
      " [-0.06285132]]\n",
      "t [[ 0.00583893]\n",
      " [-0.05712938]\n",
      " [-0.14737365]\n",
      " ...\n",
      " [-0.11188905]\n",
      " [ 0.01934697]\n",
      " [-0.07123017]]\n",
      "t [[ 0.00583893]\n",
      " [-0.05712938]\n",
      " [-0.14737365]\n",
      " ...\n",
      " [-0.11188905]\n",
      " [ 0.01934697]\n",
      " [-0.07123017]]\n",
      "Current iteration=8, loss=39132.857455462814\n",
      "t [[ 0.0061842 ]\n",
      " [-0.06480898]\n",
      " [-0.16311223]\n",
      " ...\n",
      " [-0.12365069]\n",
      " [ 0.02098197]\n",
      " [-0.07947168]]\n",
      "t [[ 0.0061842 ]\n",
      " [-0.06480898]\n",
      " [-0.16311223]\n",
      " ...\n",
      " [-0.12365069]\n",
      " [ 0.02098197]\n",
      " [-0.07947168]]\n",
      "t [[ 0.0064594 ]\n",
      " [-0.07257807]\n",
      " [-0.17834271]\n",
      " ...\n",
      " [-0.13499261]\n",
      " [ 0.02246665]\n",
      " [-0.08757971]]\n",
      "loss=38897.32783575143\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00091951]\n",
      " [-0.00581738]\n",
      " [-0.02100543]\n",
      " ...\n",
      " [-0.01567974]\n",
      " [ 0.0033332 ]\n",
      " [-0.00887363]]\n",
      "t [[ 0.00091951]\n",
      " [-0.00581738]\n",
      " [-0.02100543]\n",
      " ...\n",
      " [-0.01567974]\n",
      " [ 0.0033332 ]\n",
      " [-0.00887363]]\n",
      "t [[ 0.00173011]\n",
      " [-0.01181088]\n",
      " [-0.04127612]\n",
      " ...\n",
      " [-0.0307549 ]\n",
      " [ 0.00645311]\n",
      " [-0.01758466]]\n",
      "t [[ 0.00173011]\n",
      " [-0.01181088]\n",
      " [-0.04127612]\n",
      " ...\n",
      " [-0.0307549 ]\n",
      " [ 0.00645311]\n",
      " [-0.01758466]]\n",
      "Current iteration=2, loss=39977.87820184317\n",
      "t [[ 0.00243757]\n",
      " [-0.01796898]\n",
      " [-0.06084437]\n",
      " ...\n",
      " [-0.04525274]\n",
      " [ 0.00936825]\n",
      " [-0.02613779]]\n",
      "t [[ 0.00243757]\n",
      " [-0.01796898]\n",
      " [-0.06084437]\n",
      " ...\n",
      " [-0.04525274]\n",
      " [ 0.00936825]\n",
      " [-0.02613779]]\n",
      "t [[ 0.00304743]\n",
      " [-0.02428063]\n",
      " [-0.07974142]\n",
      " ...\n",
      " [-0.05919951]\n",
      " [ 0.0120869 ]\n",
      " [-0.03453762]]\n",
      "t [[ 0.00304743]\n",
      " [-0.02428063]\n",
      " [-0.07974142]\n",
      " ...\n",
      " [-0.05919951]\n",
      " [ 0.0120869 ]\n",
      " [-0.03453762]]\n",
      "Current iteration=4, loss=39674.824727488376\n",
      "t [[ 0.00356504]\n",
      " [-0.03073523]\n",
      " [-0.0979973 ]\n",
      " ...\n",
      " [-0.07262051]\n",
      " [ 0.01461708]\n",
      " [-0.04278862]]\n",
      "t [[ 0.00356504]\n",
      " [-0.03073523]\n",
      " [-0.0979973 ]\n",
      " ...\n",
      " [-0.07262051]\n",
      " [ 0.01461708]\n",
      " [-0.04278862]]\n",
      "t [[ 0.00399553]\n",
      " [-0.03732265]\n",
      " [-0.1156409 ]\n",
      " ...\n",
      " [-0.08554003]\n",
      " [ 0.01696654]\n",
      " [-0.05089513]]\n",
      "t [[ 0.00399553]\n",
      " [-0.03732265]\n",
      " [-0.1156409 ]\n",
      " ...\n",
      " [-0.08554003]\n",
      " [ 0.01696654]\n",
      " [-0.05089513]]\n",
      "Current iteration=6, loss=39398.631773760906\n",
      "t [[ 0.00434381]\n",
      " [-0.04403324]\n",
      " [-0.13269988]\n",
      " ...\n",
      " [-0.09798136]\n",
      " [ 0.01914275]\n",
      " [-0.05886135]]\n",
      "t [[ 0.00434381]\n",
      " [-0.04403324]\n",
      " [-0.13269988]\n",
      " ...\n",
      " [-0.09798136]\n",
      " [ 0.01914275]\n",
      " [-0.05886135]]\n",
      "t [[ 0.00461458]\n",
      " [-0.05085779]\n",
      " [-0.14920076]\n",
      " ...\n",
      " [-0.10996678]\n",
      " [ 0.0211529 ]\n",
      " [-0.06669134]]\n",
      "t [[ 0.00461458]\n",
      " [-0.05085779]\n",
      " [-0.14920076]\n",
      " ...\n",
      " [-0.10996678]\n",
      " [ 0.0211529 ]\n",
      " [-0.06669134]]\n",
      "Current iteration=8, loss=39145.687047774125\n",
      "t [[ 0.00481232]\n",
      " [-0.05778755]\n",
      " [-0.16516888]\n",
      " ...\n",
      " [-0.12151759]\n",
      " [ 0.02300387]\n",
      " [-0.07438902]]\n",
      "t [[ 0.00481232]\n",
      " [-0.05778755]\n",
      " [-0.16516888]\n",
      " ...\n",
      " [-0.12151759]\n",
      " [ 0.02300387]\n",
      " [-0.07438902]]\n",
      "t [[ 0.00494127]\n",
      " [-0.06481421]\n",
      " [-0.18062841]\n",
      " ...\n",
      " [-0.13265414]\n",
      " [ 0.02470229]\n",
      " [-0.08195817]]\n",
      "loss=38912.93728339799\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00076577]\n",
      " [-0.00616161]\n",
      " [-0.02086683]\n",
      " ...\n",
      " [-0.00918713]\n",
      " [-0.00110647]\n",
      " [-0.01874323]]\n",
      "t [[ 0.00076577]\n",
      " [-0.00616161]\n",
      " [-0.02086683]\n",
      " ...\n",
      " [-0.00918713]\n",
      " [-0.00110647]\n",
      " [-0.01874323]]\n",
      "t [[ 0.00142478]\n",
      " [-0.01249814]\n",
      " [-0.04099279]\n",
      " ...\n",
      " [-0.01789943]\n",
      " [-0.00205595]\n",
      " [-0.03721014]]\n",
      "t [[ 0.00142478]\n",
      " [-0.01249814]\n",
      " [-0.04099279]\n",
      " ...\n",
      " [-0.01789943]\n",
      " [-0.00205595]\n",
      " [-0.03721014]]\n",
      "Current iteration=2, loss=39971.940692641736\n",
      "t [[ 0.00198277]\n",
      " [-0.01899801]\n",
      " [-0.06041061]\n",
      " ...\n",
      " [-0.02615997]\n",
      " [-0.00285782]\n",
      " [-0.05540767]]\n",
      "t [[ 0.00198277]\n",
      " [-0.01899801]\n",
      " [-0.06041061]\n",
      " ...\n",
      " [-0.02615997]\n",
      " [-0.00285782]\n",
      " [-0.05540767]]\n",
      "t [[ 0.00244527]\n",
      " [-0.02565009]\n",
      " [-0.07915193]\n",
      " ...\n",
      " [-0.03399096]\n",
      " [-0.00352108]\n",
      " [-0.07334261]]\n",
      "t [[ 0.00244527]\n",
      " [-0.02565009]\n",
      " [-0.07915193]\n",
      " ...\n",
      " [-0.03399096]\n",
      " [-0.00352108]\n",
      " [-0.07334261]]\n",
      "Current iteration=4, loss=39663.579972889886\n",
      "t [[ 0.00281759]\n",
      " [-0.03244373]\n",
      " [-0.09724718]\n",
      " ...\n",
      " [-0.04141375]\n",
      " [-0.00405432]\n",
      " [-0.09102158]]\n",
      "t [[ 0.00281759]\n",
      " [-0.03244373]\n",
      " [-0.09724718]\n",
      " ...\n",
      " [-0.04141375]\n",
      " [-0.00405432]\n",
      " [-0.09102158]]\n",
      "t [[ 0.00310485]\n",
      " [-0.03936872]\n",
      " [-0.11472561]\n",
      " ...\n",
      " [-0.04844881]\n",
      " [-0.00446574]\n",
      " [-0.10845106]]\n",
      "t [[ 0.00310485]\n",
      " [-0.03936872]\n",
      " [-0.11472561]\n",
      " ...\n",
      " [-0.04844881]\n",
      " [-0.00446574]\n",
      " [-0.10845106]]\n",
      "Current iteration=6, loss=39382.60881969628\n",
      "t [[ 0.00331191]\n",
      " [-0.04641536]\n",
      " [-0.13161527]\n",
      " ...\n",
      " [-0.05511573]\n",
      " [-0.00476314]\n",
      " [-0.12563733]]\n",
      "t [[ 0.00331191]\n",
      " [-0.04641536]\n",
      " [-0.13161527]\n",
      " ...\n",
      " [-0.05511573]\n",
      " [-0.00476314]\n",
      " [-0.12563733]]\n",
      "t [[ 0.00344344]\n",
      " [-0.05357438]\n",
      " [-0.14794299]\n",
      " ...\n",
      " [-0.06143322]\n",
      " [-0.00495395]\n",
      " [-0.1425865 ]]\n",
      "t [[ 0.00344344]\n",
      " [-0.05357438]\n",
      " [-0.14794299]\n",
      " ...\n",
      " [-0.06143322]\n",
      " [-0.00495395]\n",
      " [-0.1425865 ]]\n",
      "Current iteration=8, loss=39125.3308788209\n",
      "t [[ 0.0035039 ]\n",
      " [-0.06083697]\n",
      " [-0.16373443]\n",
      " ...\n",
      " [-0.06741914]\n",
      " [-0.00504521]\n",
      " [-0.1593045 ]]\n",
      "t [[ 0.0035039 ]\n",
      " [-0.06083697]\n",
      " [-0.16373443]\n",
      " ...\n",
      " [-0.06741914]\n",
      " [-0.00504521]\n",
      " [-0.1593045 ]]\n",
      "t [[ 0.0034975 ]\n",
      " [-0.06819479]\n",
      " [-0.17901409]\n",
      " ...\n",
      " [-0.07309051]\n",
      " [-0.00504361]\n",
      " [-0.17579708]]\n",
      "loss=38888.62346074794\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01088485]\n",
      " [-0.04649883]\n",
      " [-0.02922619]\n",
      " ...\n",
      " [-0.02118174]\n",
      " [ 0.00458596]\n",
      " [-0.01234784]]\n",
      "t [[ 0.01088485]\n",
      " [-0.04649883]\n",
      " [-0.02922619]\n",
      " ...\n",
      " [-0.02118174]\n",
      " [ 0.00458596]\n",
      " [-0.01234784]]\n",
      "t [[ 0.02120474]\n",
      " [-0.09105773]\n",
      " [-0.05717711]\n",
      " ...\n",
      " [-0.04129698]\n",
      " [ 0.00878743]\n",
      " [-0.02440321]]\n",
      "t [[ 0.02120474]\n",
      " [-0.09105773]\n",
      " [-0.05717711]\n",
      " ...\n",
      " [-0.04129698]\n",
      " [ 0.00878743]\n",
      " [-0.02440321]]\n",
      "Current iteration=2, loss=39872.427842535115\n",
      "t [[ 0.03098998]\n",
      " [-0.13378072]\n",
      " [-0.08392512]\n",
      " ...\n",
      " [-0.06040943]\n",
      " [ 0.01262468]\n",
      " [-0.03617722]]\n",
      "t [[ 0.03098998]\n",
      " [-0.13378072]\n",
      " [-0.08392512]\n",
      " ...\n",
      " [-0.06040943]\n",
      " [ 0.01262468]\n",
      " [-0.03617722]]\n",
      "t [[ 0.04026959]\n",
      " [-0.17476726]\n",
      " [-0.10953926]\n",
      " ...\n",
      " [-0.07857978]\n",
      " [ 0.0161172 ]\n",
      " [-0.04768059]]\n",
      "t [[ 0.04026959]\n",
      " [-0.17476726]\n",
      " [-0.10953926]\n",
      " ...\n",
      " [-0.07857978]\n",
      " [ 0.0161172 ]\n",
      " [-0.04768059]]\n",
      "Current iteration=4, loss=39485.872105825154\n",
      "t [[ 0.04907123]\n",
      " [-0.21411198]\n",
      " [-0.13408509]\n",
      " ...\n",
      " [-0.09586553]\n",
      " [ 0.01928365]\n",
      " [-0.05892362]]\n",
      "t [[ 0.04907123]\n",
      " [-0.21411198]\n",
      " [-0.13408509]\n",
      " ...\n",
      " [-0.09586553]\n",
      " [ 0.01928365]\n",
      " [-0.05892362]]\n",
      "t [[ 0.05742119]\n",
      " [-0.25190463]\n",
      " [-0.15762463]\n",
      " ...\n",
      " [-0.11232105]\n",
      " [ 0.02214178]\n",
      " [-0.06991619]]\n",
      "t [[ 0.05742119]\n",
      " [-0.25190463]\n",
      " [-0.15762463]\n",
      " ...\n",
      " [-0.11232105]\n",
      " [ 0.02214178]\n",
      " [-0.06991619]]\n",
      "Current iteration=6, loss=39142.98452831651\n",
      "t [[ 0.06534439]\n",
      " [-0.28823005]\n",
      " [-0.18021639]\n",
      " ...\n",
      " [-0.12799754]\n",
      " [ 0.02470844]\n",
      " [-0.08066774]]\n",
      "t [[ 0.06534439]\n",
      " [-0.28823005]\n",
      " [-0.18021639]\n",
      " ...\n",
      " [-0.12799754]\n",
      " [ 0.02470844]\n",
      " [-0.08066774]]\n",
      "t [[ 0.0728644 ]\n",
      " [-0.3231683 ]\n",
      " [-0.20191546]\n",
      " ...\n",
      " [-0.14294315]\n",
      " [ 0.02699957]\n",
      " [-0.09118727]]\n",
      "t [[ 0.0728644 ]\n",
      " [-0.3231683 ]\n",
      " [-0.20191546]\n",
      " ...\n",
      " [-0.14294315]\n",
      " [ 0.02699957]\n",
      " [-0.09118727]]\n",
      "Current iteration=8, loss=38836.24041788725\n",
      "t [[ 0.08000347]\n",
      " [-0.35679477]\n",
      " [-0.22277358]\n",
      " ...\n",
      " [-0.15720309]\n",
      " [ 0.02903024]\n",
      " [-0.10148334]]\n",
      "t [[ 0.08000347]\n",
      " [-0.35679477]\n",
      " [-0.22277358]\n",
      " ...\n",
      " [-0.15720309]\n",
      " [ 0.02903024]\n",
      " [-0.10148334]]\n",
      "t [[ 0.08678259]\n",
      " [-0.38918034]\n",
      " [-0.24283931]\n",
      " ...\n",
      " [-0.1708197 ]\n",
      " [ 0.03081463]\n",
      " [-0.11156411]]\n",
      "loss=38559.6349434251\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00143542]\n",
      " [-0.00883772]\n",
      " [-0.02770717]\n",
      " ...\n",
      " [-0.02126188]\n",
      " [ 0.00413345]\n",
      " [-0.01261349]]\n",
      "t [[ 0.00143542]\n",
      " [-0.00883772]\n",
      " [-0.02770717]\n",
      " ...\n",
      " [-0.02126188]\n",
      " [ 0.00413345]\n",
      " [-0.01261349]]\n",
      "t [[ 0.00267534]\n",
      " [-0.01797466]\n",
      " [-0.05410549]\n",
      " ...\n",
      " [-0.04143421]\n",
      " [ 0.00789142]\n",
      " [-0.02492787]]\n",
      "t [[ 0.00267534]\n",
      " [-0.01797466]\n",
      " [-0.05410549]\n",
      " ...\n",
      " [-0.04143421]\n",
      " [ 0.00789142]\n",
      " [-0.02492787]]\n",
      "Current iteration=2, loss=39867.27917824446\n",
      "t [[ 0.00373336]\n",
      " [-0.02738354]\n",
      " [-0.07927263]\n",
      " ...\n",
      " [-0.06058279]\n",
      " [ 0.01129424]\n",
      " [-0.03695458]]\n",
      "t [[ 0.00373336]\n",
      " [-0.02738354]\n",
      " [-0.07927263]\n",
      " ...\n",
      " [-0.06058279]\n",
      " [ 0.01129424]\n",
      " [-0.03695458]]\n",
      "t [[ 0.00462245]\n",
      " [-0.03703857]\n",
      " [-0.10328251]\n",
      " ...\n",
      " [-0.07877024]\n",
      " [ 0.01436149]\n",
      " [-0.04870469]]\n",
      "t [[ 0.00462245]\n",
      " [-0.03703857]\n",
      " [-0.10328251]\n",
      " ...\n",
      " [-0.07877024]\n",
      " [ 0.01436149]\n",
      " [-0.04870469]]\n",
      "Current iteration=4, loss=39476.25800543617\n",
      "t [[ 0.00535488]\n",
      " [-0.04691548]\n",
      " [-0.1262052 ]\n",
      " ...\n",
      " [-0.09605587]\n",
      " [ 0.01711182]\n",
      " [-0.06018882]]\n",
      "t [[ 0.00535488]\n",
      " [-0.04691548]\n",
      " [-0.1262052 ]\n",
      " ...\n",
      " [-0.09605587]\n",
      " [ 0.01711182]\n",
      " [-0.06018882]]\n",
      "t [[ 0.00594224]\n",
      " [-0.05699149]\n",
      " [-0.14810686]\n",
      " ...\n",
      " [-0.11249569]\n",
      " [ 0.01956302]\n",
      " [-0.07141712]]\n",
      "t [[ 0.00594224]\n",
      " [-0.05699149]\n",
      " [-0.14810686]\n",
      " ...\n",
      " [-0.11249569]\n",
      " [ 0.01956302]\n",
      " [-0.07141712]]\n",
      "Current iteration=6, loss=39129.38623643863\n",
      "t [[ 0.0063954 ]\n",
      " [-0.06724534]\n",
      " [-0.16904983]\n",
      " ...\n",
      " [-0.12814243]\n",
      " [ 0.0217319 ]\n",
      " [-0.08239932]]\n",
      "t [[ 0.0063954 ]\n",
      " [-0.06724534]\n",
      " [-0.16904983]\n",
      " ...\n",
      " [-0.12814243]\n",
      " [ 0.0217319 ]\n",
      " [-0.08239932]]\n",
      "t [[ 0.00672455]\n",
      " [-0.07765718]\n",
      " [-0.18909264]\n",
      " ...\n",
      " [-0.14304564]\n",
      " [ 0.02363438]\n",
      " [-0.09314466]]\n",
      "t [[ 0.00672455]\n",
      " [-0.07765718]\n",
      " [-0.18909264]\n",
      " ...\n",
      " [-0.14304564]\n",
      " [ 0.02363438]\n",
      " [-0.09314466]]\n",
      "Current iteration=8, loss=38819.00005902015\n",
      "t [[ 0.00693922]\n",
      " [-0.08820856]\n",
      " [-0.20829023]\n",
      " ...\n",
      " [-0.15725177]\n",
      " [ 0.02528544]\n",
      " [-0.10366196]]\n",
      "t [[ 0.00693922]\n",
      " [-0.08820856]\n",
      " [-0.20829023]\n",
      " ...\n",
      " [-0.15725177]\n",
      " [ 0.02528544]\n",
      " [-0.10366196]]\n",
      "t [[ 0.00704828]\n",
      " [-0.09888232]\n",
      " [-0.22669402]\n",
      " ...\n",
      " [-0.17080434]\n",
      " [ 0.0266992 ]\n",
      " [-0.11395958]]\n",
      "loss=38539.00114637026\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00122601]\n",
      " [-0.00775651]\n",
      " [-0.02800725]\n",
      " ...\n",
      " [-0.02090631]\n",
      " [ 0.00444427]\n",
      " [-0.01183151]]\n",
      "t [[ 0.00122601]\n",
      " [-0.00775651]\n",
      " [-0.02800725]\n",
      " ...\n",
      " [-0.02090631]\n",
      " [ 0.00444427]\n",
      " [-0.01183151]]\n",
      "t [[ 0.00225844]\n",
      " [-0.01582608]\n",
      " [-0.05470837]\n",
      " ...\n",
      " [-0.04073793]\n",
      " [ 0.00850939]\n",
      " [-0.02337396]]\n",
      "t [[ 0.00225844]\n",
      " [-0.01582608]\n",
      " [-0.05470837]\n",
      " ...\n",
      " [-0.04073793]\n",
      " [ 0.00850939]\n",
      " [-0.02337396]]\n",
      "Current iteration=2, loss=39871.94984900532\n",
      "t [[ 0.00311098]\n",
      " [-0.02418135]\n",
      " [-0.08018029]\n",
      " ...\n",
      " [-0.05955962]\n",
      " [ 0.01221566]\n",
      " [-0.03463857]]\n",
      "t [[ 0.00311098]\n",
      " [-0.02418135]\n",
      " [-0.08018029]\n",
      " ...\n",
      " [-0.05955962]\n",
      " [ 0.01221566]\n",
      " [-0.03463857]]\n",
      "t [[ 0.00379667]\n",
      " [-0.03279641]\n",
      " [-0.10449624]\n",
      " ...\n",
      " [-0.07743303]\n",
      " [ 0.01558257]\n",
      " [-0.04563617]]\n",
      "t [[ 0.00379667]\n",
      " [-0.03279641]\n",
      " [-0.10449624]\n",
      " ...\n",
      " [-0.07743303]\n",
      " [ 0.01558257]\n",
      " [-0.04563617]]\n",
      "Current iteration=4, loss=39485.18035572746\n",
      "t [[ 0.00432786]\n",
      " [-0.0416469 ]\n",
      " [-0.12772565]\n",
      " ...\n",
      " [-0.09441659]\n",
      " [ 0.01862872]\n",
      " [-0.05637717]]\n",
      "t [[ 0.00432786]\n",
      " [-0.0416469 ]\n",
      " [-0.12772565]\n",
      " ...\n",
      " [-0.09441659]\n",
      " [ 0.01862872]\n",
      " [-0.05637717]]\n",
      "t [[ 0.00471618]\n",
      " [-0.05070994]\n",
      " [-0.14993413]\n",
      " ...\n",
      " [-0.11056545]\n",
      " [ 0.02137184]\n",
      " [-0.06687152]]\n",
      "t [[ 0.00471618]\n",
      " [-0.05070994]\n",
      " [-0.14993413]\n",
      " ...\n",
      " [-0.11056545]\n",
      " [ 0.02137184]\n",
      " [-0.06687152]]\n",
      "Current iteration=6, loss=39142.23998548431\n",
      "t [[ 0.00497257]\n",
      " [-0.05996411]\n",
      " [-0.17118346]\n",
      " ...\n",
      " [-0.12593159]\n",
      " [ 0.0238287 ]\n",
      " [-0.07712874]]\n",
      "t [[ 0.00497257]\n",
      " [-0.05996411]\n",
      " [-0.17118346]\n",
      " ...\n",
      " [-0.12593159]\n",
      " [ 0.0238287 ]\n",
      " [-0.07712874]]\n",
      "t [[ 0.00510726]\n",
      " [-0.06938945]\n",
      " [-0.19153174]\n",
      " ...\n",
      " [-0.14056384]\n",
      " [ 0.02601517]\n",
      " [-0.08715789]]\n",
      "t [[ 0.00510726]\n",
      " [-0.06938945]\n",
      " [-0.19153174]\n",
      " ...\n",
      " [-0.14056384]\n",
      " [ 0.02601517]\n",
      " [-0.08715789]]\n",
      "Current iteration=8, loss=38835.53516562907\n",
      "t [[ 0.00512982]\n",
      " [-0.07896734]\n",
      " [-0.2110335 ]\n",
      " ...\n",
      " [-0.15450803]\n",
      " [ 0.02794621]\n",
      " [-0.0969676 ]]\n",
      "t [[ 0.00512982]\n",
      " [-0.07896734]\n",
      " [-0.2110335 ]\n",
      " ...\n",
      " [-0.15450803]\n",
      " [ 0.02794621]\n",
      " [-0.0969676 ]]\n",
      "t [[ 0.00504917]\n",
      " [-0.08868046]\n",
      " [-0.22973979]\n",
      " ...\n",
      " [-0.16780707]\n",
      " [ 0.02963591]\n",
      " [-0.10656605]]\n",
      "loss=38559.01650789535\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00102102]\n",
      " [-0.00821548]\n",
      " [-0.02782245]\n",
      " ...\n",
      " [-0.01224951]\n",
      " [-0.0014753 ]\n",
      " [-0.02499098]]\n",
      "t [[ 0.00102102]\n",
      " [-0.00821548]\n",
      " [-0.02782245]\n",
      " ...\n",
      " [-0.01224951]\n",
      " [-0.0014753 ]\n",
      " [-0.02499098]]\n",
      "t [[ 0.00185228]\n",
      " [-0.0167419 ]\n",
      " [-0.05432788]\n",
      " ...\n",
      " [-0.02365494]\n",
      " [-0.00267151]\n",
      " [-0.04949074]]\n",
      "t [[ 0.00185228]\n",
      " [-0.0167419 ]\n",
      " [-0.05432788]\n",
      " ...\n",
      " [-0.02365494]\n",
      " [-0.00267151]\n",
      " [-0.04949074]]\n",
      "Current iteration=2, loss=39864.14482887465\n",
      "t [[ 0.00250742]\n",
      " [-0.02555171]\n",
      " [-0.07959422]\n",
      " ...\n",
      " [-0.03427116]\n",
      " [-0.00361096]\n",
      " [-0.07351582]]\n",
      "t [[ 0.00250742]\n",
      " [-0.02555171]\n",
      " [-0.07959422]\n",
      " ...\n",
      " [-0.03427116]\n",
      " [-0.00361096]\n",
      " [-0.07351582]]\n",
      "t [[ 0.00299942]\n",
      " [-0.03461887]\n",
      " [-0.10369567]\n",
      " ...\n",
      " [-0.04415025]\n",
      " [-0.00431465]\n",
      " [-0.09708224]]\n",
      "t [[ 0.00299942]\n",
      " [-0.03461887]\n",
      " [-0.10369567]\n",
      " ...\n",
      " [-0.04415025]\n",
      " [-0.00431465]\n",
      " [-0.09708224]]\n",
      "Current iteration=4, loss=39470.64314480164\n",
      "t [[ 0.00334055]\n",
      " [-0.04391884]\n",
      " [-0.12670257]\n",
      " ...\n",
      " [-0.05334145]\n",
      " [-0.00480235]\n",
      " [-0.12020549]]\n",
      "t [[ 0.00334055]\n",
      " [-0.04391884]\n",
      " [-0.12670257]\n",
      " ...\n",
      " [-0.05334145]\n",
      " [-0.00480235]\n",
      " [-0.12020549]]\n",
      "t [[ 0.00354238]\n",
      " [-0.05342861]\n",
      " [-0.14868136]\n",
      " ...\n",
      " [-0.06189117]\n",
      " [-0.00509252]\n",
      " [-0.14290049]]\n",
      "t [[ 0.00354238]\n",
      " [-0.05342861]\n",
      " [-0.14868136]\n",
      " ...\n",
      " [-0.06189117]\n",
      " [-0.00509252]\n",
      " [-0.14290049]]\n",
      "Current iteration=6, loss=39121.81929408314\n",
      "t [[ 0.00361578]\n",
      " [-0.06312663]\n",
      " [-0.16969463]\n",
      " ...\n",
      " [-0.06984301]\n",
      " [-0.00520243]\n",
      " [-0.16518159]]\n",
      "t [[ 0.00361578]\n",
      " [-0.06312663]\n",
      " [-0.16969463]\n",
      " ...\n",
      " [-0.06984301]\n",
      " [-0.00520243]\n",
      " [-0.16518159]]\n",
      "t [[ 0.0035709 ]\n",
      " [-0.07299281]\n",
      " [-0.1898012 ]\n",
      " ...\n",
      " [-0.07723783]\n",
      " [-0.00514813]\n",
      " [-0.18706257]]\n",
      "t [[ 0.0035709 ]\n",
      " [-0.07299281]\n",
      " [-0.1898012 ]\n",
      " ...\n",
      " [-0.07723783]\n",
      " [-0.00514813]\n",
      " [-0.18706257]]\n",
      "Current iteration=8, loss=38809.90589765168\n",
      "t [[ 0.00341724]\n",
      " [-0.08300845]\n",
      " [-0.20905626]\n",
      " ...\n",
      " [-0.0841139 ]\n",
      " [-0.00494457]\n",
      " [-0.20855661]]\n",
      "t [[ 0.00341724]\n",
      " [-0.08300845]\n",
      " [-0.20905626]\n",
      " ...\n",
      " [-0.0841139 ]\n",
      " [-0.00494457]\n",
      " [-0.20855661]]\n",
      "t [[ 0.00316365]\n",
      " [-0.09315616]\n",
      " [-0.22751151]\n",
      " ...\n",
      " [-0.09050696]\n",
      " [-0.00460564]\n",
      " [-0.22967636]]\n",
      "loss=38528.71968053818\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01360606]\n",
      " [-0.05812354]\n",
      " [-0.03653274]\n",
      " ...\n",
      " [-0.02647718]\n",
      " [ 0.00573246]\n",
      " [-0.0154348 ]]\n",
      "t [[ 0.01360606]\n",
      " [-0.05812354]\n",
      " [-0.03653274]\n",
      " ...\n",
      " [-0.02647718]\n",
      " [ 0.00573246]\n",
      " [-0.0154348 ]]\n",
      "t [[ 0.02632946]\n",
      " [-0.11321623]\n",
      " [-0.07107307]\n",
      " ...\n",
      " [-0.05128811]\n",
      " [ 0.01086421]\n",
      " [-0.03041266]]\n",
      "t [[ 0.02632946]\n",
      " [-0.11321623]\n",
      " [-0.07107307]\n",
      " ...\n",
      " [-0.05128811]\n",
      " [ 0.01086421]\n",
      " [-0.03041266]]\n",
      "Current iteration=2, loss=39769.19925883704\n",
      "t [[ 0.03822962]\n",
      " [-0.16548205]\n",
      " [-0.10376285]\n",
      " ...\n",
      " [-0.07455767]\n",
      " [ 0.01543505]\n",
      " [-0.04495534]]\n",
      "t [[ 0.03822962]\n",
      " [-0.16548205]\n",
      " [-0.10376285]\n",
      " ...\n",
      " [-0.07455767]\n",
      " [ 0.01543505]\n",
      " [-0.04495534]]\n",
      "t [[ 0.04936266]\n",
      " [-0.21511327]\n",
      " [-0.13473548]\n",
      " ...\n",
      " [-0.09640301]\n",
      " [ 0.01948274]\n",
      " [-0.05908366]]\n",
      "t [[ 0.04936266]\n",
      " [-0.21511327]\n",
      " [-0.13473548]\n",
      " ...\n",
      " [-0.09640301]\n",
      " [ 0.01948274]\n",
      " [-0.05908366]]\n",
      "Current iteration=4, loss=39306.301082248436\n",
      "t [[ 0.0597813 ]\n",
      " [-0.26229001]\n",
      " [-0.16411556]\n",
      " ...\n",
      " [-0.11693339]\n",
      " [ 0.02304287]\n",
      " [-0.07281734]]\n",
      "t [[ 0.0597813 ]\n",
      " [-0.26229001]\n",
      " [-0.16411556]\n",
      " ...\n",
      " [-0.11693339]\n",
      " [ 0.02304287]\n",
      " [-0.07281734]]\n",
      "t [[ 0.06953482]\n",
      " [-0.30718017]\n",
      " [-0.19201891]\n",
      " ...\n",
      " [-0.13625018]\n",
      " [ 0.02614875]\n",
      " [-0.08617501]]\n",
      "t [[ 0.06953482]\n",
      " [-0.30718017]\n",
      " [-0.19201891]\n",
      " ...\n",
      " [-0.13625018]\n",
      " [ 0.02614875]\n",
      " [-0.08617501]]\n",
      "Current iteration=6, loss=38906.08768575905\n",
      "t [[ 0.07866917]\n",
      " [-0.34993973]\n",
      " [-0.21855284]\n",
      " ...\n",
      " [-0.15444719]\n",
      " [ 0.02883145]\n",
      " [-0.09917423]]\n",
      "t [[ 0.07866917]\n",
      " [-0.34993973]\n",
      " [-0.21855284]\n",
      " ...\n",
      " [-0.15444719]\n",
      " [ 0.02883145]\n",
      " [-0.09917423]]\n",
      "t [[ 0.08722703]\n",
      " [-0.39071315]\n",
      " [-0.24381644]\n",
      " ...\n",
      " [-0.17161094]\n",
      " [ 0.03111985]\n",
      " [-0.11183149]]\n",
      "t [[ 0.08722703]\n",
      " [-0.39071315]\n",
      " [-0.24381644]\n",
      " ...\n",
      " [-0.17161094]\n",
      " [ 0.03111985]\n",
      " [-0.11183149]]\n",
      "Current iteration=8, loss=38555.53922328925\n",
      "t [[ 0.09524803]\n",
      " [-0.42963403]\n",
      " [-0.26790107]\n",
      " ...\n",
      " [-0.18782109]\n",
      " [ 0.03304072]\n",
      " [-0.12416226]]\n",
      "t [[ 0.09524803]\n",
      " [-0.42963403]\n",
      " [-0.26790107]\n",
      " ...\n",
      " [-0.18782109]\n",
      " [ 0.03304072]\n",
      " [-0.12416226]]\n",
      "t [[ 0.10276887]\n",
      " [-0.46682578]\n",
      " [-0.29089086]\n",
      " ...\n",
      " [-0.20315091]\n",
      " [ 0.03461884]\n",
      " [-0.13618103]]\n",
      "loss=38244.855303733115\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00179428]\n",
      " [-0.01104715]\n",
      " [-0.03463396]\n",
      " ...\n",
      " [-0.02657735]\n",
      " [ 0.00516682]\n",
      " [-0.01576686]]\n",
      "t [[ 0.00179428]\n",
      " [-0.01104715]\n",
      " [-0.03463396]\n",
      " ...\n",
      " [-0.02657735]\n",
      " [ 0.00516682]\n",
      " [-0.01576686]]\n",
      "t [[ 0.00328313]\n",
      " [-0.02256176]\n",
      " [-0.06722306]\n",
      " ...\n",
      " [-0.05145245]\n",
      " [ 0.00974701]\n",
      " [-0.0310664 ]]\n",
      "t [[ 0.00328313]\n",
      " [-0.02256176]\n",
      " [-0.06722306]\n",
      " ...\n",
      " [-0.05145245]\n",
      " [ 0.00974701]\n",
      " [-0.0310664 ]]\n",
      "Current iteration=2, loss=39762.85399066431\n",
      "t [[ 0.00449323]\n",
      " [-0.03449041]\n",
      " [-0.09791952]\n",
      " ...\n",
      " [-0.07475426]\n",
      " [ 0.0137805 ]\n",
      " [-0.04592107]]\n",
      "t [[ 0.00449323]\n",
      " [-0.03449041]\n",
      " [-0.09791952]\n",
      " ...\n",
      " [-0.07475426]\n",
      " [ 0.0137805 ]\n",
      " [-0.04592107]]\n",
      "t [[ 0.00544963]\n",
      " [-0.04678341]\n",
      " [-0.12686607]\n",
      " ...\n",
      " [-0.09660366]\n",
      " [ 0.01730518]\n",
      " [-0.06035232]]\n",
      "t [[ 0.00544963]\n",
      " [-0.04678341]\n",
      " [-0.12686607]\n",
      " ...\n",
      " [-0.09660366]\n",
      " [ 0.01730518]\n",
      " [-0.06035232]]\n",
      "Current iteration=4, loss=39294.61163280877\n",
      "t [[ 0.00617563]\n",
      " [-0.05939485]\n",
      " [-0.15419577]\n",
      " ...\n",
      " [-0.11711324]\n",
      " [ 0.02035665]\n",
      " [-0.07438046]]\n",
      "t [[ 0.00617563]\n",
      " [-0.05939485]\n",
      " [-0.15419577]\n",
      " ...\n",
      " [-0.11711324]\n",
      " [ 0.02035665]\n",
      " [-0.07438046]]\n",
      "t [[ 0.00669277]\n",
      " [-0.07228255]\n",
      " [-0.180032  ]\n",
      " ...\n",
      " [-0.13638744]\n",
      " [ 0.02296819]\n",
      " [-0.08802468]]\n",
      "t [[ 0.00669277]\n",
      " [-0.07228255]\n",
      " [-0.180032  ]\n",
      " ...\n",
      " [-0.13638744]\n",
      " [ 0.02296819]\n",
      " [-0.08802468]]\n",
      "Current iteration=6, loss=38889.69830147816\n",
      "t [[ 0.0070209 ]\n",
      " [-0.08540791]\n",
      " [-0.20448879]\n",
      " ...\n",
      " [-0.15452274]\n",
      " [ 0.02517081]\n",
      " [-0.10130302]]\n",
      "t [[ 0.0070209 ]\n",
      " [-0.08540791]\n",
      " [-0.20448879]\n",
      " ...\n",
      " [-0.15452274]\n",
      " [ 0.02517081]\n",
      " [-0.10130302]]\n",
      "t [[ 0.00717821]\n",
      " [-0.09873569]\n",
      " [-0.22767126]\n",
      " ...\n",
      " [-0.17160806]\n",
      " [ 0.02699322]\n",
      " [-0.11423242]]\n",
      "t [[ 0.00717821]\n",
      " [-0.09873569]\n",
      " [-0.22767126]\n",
      " ...\n",
      " [-0.17160806]\n",
      " [ 0.02699322]\n",
      " [-0.11423242]]\n",
      "Current iteration=8, loss=38534.874255172974\n",
      "t [[ 0.00718134]\n",
      " [-0.11223385]\n",
      " [-0.2496761 ]\n",
      " ...\n",
      " [-0.1877252 ]\n",
      " [ 0.02846204]\n",
      " [-0.12682878]]\n",
      "t [[ 0.00718134]\n",
      " [-0.11223385]\n",
      " [-0.2496761 ]\n",
      " ...\n",
      " [-0.1877252 ]\n",
      " [ 0.02846204]\n",
      " [-0.12682878]]\n",
      "t [[ 0.00704551]\n",
      " [-0.12587326]\n",
      " [-0.27059213]\n",
      " ...\n",
      " [-0.20294929]\n",
      " [ 0.02960184]\n",
      " [-0.13910695]]\n",
      "loss=38220.20849338645\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00153251]\n",
      " [-0.00969564]\n",
      " [-0.03500906]\n",
      " ...\n",
      " [-0.02613289]\n",
      " [ 0.00555534]\n",
      " [-0.01478939]]\n",
      "t [[ 0.00153251]\n",
      " [-0.00969564]\n",
      " [-0.03500906]\n",
      " ...\n",
      " [-0.02613289]\n",
      " [ 0.00555534]\n",
      " [-0.01478939]]\n",
      "t [[ 0.0027626 ]\n",
      " [-0.01988037]\n",
      " [-0.06797751]\n",
      " ...\n",
      " [-0.05058674]\n",
      " [ 0.01051833]\n",
      " [-0.02912715]]\n",
      "t [[ 0.0027626 ]\n",
      " [-0.01988037]\n",
      " [-0.06797751]\n",
      " ...\n",
      " [-0.05058674]\n",
      " [ 0.01051833]\n",
      " [-0.02912715]]\n",
      "Current iteration=2, loss=39768.63820313081\n",
      "t [[ 0.00371711]\n",
      " [-0.03050059]\n",
      " [-0.0990561 ]\n",
      " ...\n",
      " [-0.07348848]\n",
      " [ 0.01492879]\n",
      " [-0.04303531]]\n",
      "t [[ 0.00371711]\n",
      " [-0.03050059]\n",
      " [-0.0990561 ]\n",
      " ...\n",
      " [-0.07348848]\n",
      " [ 0.01492879]\n",
      " [-0.04303531]]\n",
      "t [[ 0.00442123]\n",
      " [-0.04150642]\n",
      " [-0.12838626]\n",
      " ...\n",
      " [-0.09495712]\n",
      " [ 0.01882447]\n",
      " [-0.05653486]]\n",
      "t [[ 0.00442123]\n",
      " [-0.04150642]\n",
      " [-0.12838626]\n",
      " ...\n",
      " [-0.09495712]\n",
      " [ 0.01882447]\n",
      " [-0.05653486]]\n",
      "Current iteration=4, loss=39305.55764918958\n",
      "t [[ 0.00489837]\n",
      " [-0.05285175]\n",
      " [-0.15609984]\n",
      " ...\n",
      " [-0.11510359]\n",
      " [ 0.02224086]\n",
      " [-0.06964571]]\n",
      "t [[ 0.00489837]\n",
      " [-0.05285175]\n",
      " [-0.15609984]\n",
      " ...\n",
      " [-0.11510359]\n",
      " [ 0.02224086]\n",
      " [-0.06964571]]\n",
      "t [[ 0.0051702 ]\n",
      " [-0.06449413]\n",
      " [-0.1823192 ]\n",
      " ...\n",
      " [-0.13403079]\n",
      " [ 0.02521115]\n",
      " [-0.08238665]]\n",
      "t [[ 0.0051702 ]\n",
      " [-0.06449413]\n",
      " [-0.1823192 ]\n",
      " ...\n",
      " [-0.13403079]\n",
      " [ 0.02521115]\n",
      " [-0.08238665]]\n",
      "Current iteration=6, loss=38905.3595602441\n",
      "t [[ 0.00525665]\n",
      " [-0.07639469]\n",
      " [-0.20715751]\n",
      " ...\n",
      " [-0.15183382]\n",
      " [ 0.02776625]\n",
      " [-0.09477536]]\n",
      "t [[ 0.00525665]\n",
      " [-0.07639469]\n",
      " [-0.20715751]\n",
      " ...\n",
      " [-0.15183382]\n",
      " [ 0.02776625]\n",
      " [-0.09477536]]\n",
      "t [[ 0.00517599]\n",
      " [-0.0885179 ]\n",
      " [-0.23071909]\n",
      " ...\n",
      " [-0.16860039]\n",
      " [ 0.02993485]\n",
      " [-0.1068284 ]]\n",
      "t [[ 0.00517599]\n",
      " [-0.0885179 ]\n",
      " [-0.23071909]\n",
      " ...\n",
      " [-0.16860039]\n",
      " [ 0.02993485]\n",
      " [-0.1068284 ]]\n",
      "Current iteration=8, loss=38554.917260608185\n",
      "t [[ 0.00494494]\n",
      " [-0.10083137]\n",
      " [-0.2531    ]\n",
      " ...\n",
      " [-0.18441121]\n",
      " [ 0.03174351]\n",
      " [-0.11856133]]\n",
      "t [[ 0.00494494]\n",
      " [-0.10083137]\n",
      " [-0.2531    ]\n",
      " ...\n",
      " [-0.18441121]\n",
      " [ 0.03174351]\n",
      " [-0.11856133]]\n",
      "t [[ 0.00457876]\n",
      " [-0.11330563]\n",
      " [-0.27438854]\n",
      " ...\n",
      " [-0.19934045]\n",
      " [ 0.03321681]\n",
      " [-0.12998867]]\n",
      "loss=38244.37062332108\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00127628]\n",
      " [-0.01026935]\n",
      " [-0.03477806]\n",
      " ...\n",
      " [-0.01531188]\n",
      " [-0.00184412]\n",
      " [-0.03123872]]\n",
      "t [[ 0.00127628]\n",
      " [-0.01026935]\n",
      " [-0.03477806]\n",
      " ...\n",
      " [-0.01531188]\n",
      " [-0.00184412]\n",
      " [-0.03123872]]\n",
      "t [[ 0.0022561 ]\n",
      " [-0.02102447]\n",
      " [-0.06749849]\n",
      " ...\n",
      " [-0.02930504]\n",
      " [-0.00325222]\n",
      " [-0.06170998]]\n",
      "t [[ 0.0022561 ]\n",
      " [-0.02102447]\n",
      " [-0.06749849]\n",
      " ...\n",
      " [-0.02930504]\n",
      " [-0.00325222]\n",
      " [-0.06170998]]\n",
      "Current iteration=2, loss=39759.01871856283\n",
      "t [[ 0.00296621]\n",
      " [-0.03221142]\n",
      " [-0.09831404]\n",
      " ...\n",
      " [-0.04208696]\n",
      " [-0.00426797]\n",
      " [-0.09144618]]\n",
      "t [[ 0.00296621]\n",
      " [-0.03221142]\n",
      " [-0.09831404]\n",
      " ...\n",
      " [-0.04208696]\n",
      " [-0.00426797]\n",
      " [-0.09144618]]\n",
      "t [[ 0.00343166]\n",
      " [-0.04378001]\n",
      " [-0.12736797]\n",
      " ...\n",
      " [-0.05375811]\n",
      " [-0.00493183]\n",
      " [-0.12047848]]\n",
      "t [[ 0.00343166]\n",
      " [-0.04378001]\n",
      " [-0.12736797]\n",
      " ...\n",
      " [-0.05375811]\n",
      " [-0.00493183]\n",
      " [-0.12047848]]\n",
      "Current iteration=4, loss=39287.923604882126\n",
      "t [[ 0.00367573]\n",
      " [-0.05568381]\n",
      " [-0.15479386]\n",
      " ...\n",
      " [-0.06441187]\n",
      " [-0.00528109]\n",
      " [-0.14883662]]\n",
      "t [[ 0.00367573]\n",
      " [-0.05568381]\n",
      " [-0.15479386]\n",
      " ...\n",
      " [-0.06441187]\n",
      " [-0.00528109]\n",
      " [-0.14883662]]\n",
      "t [[ 0.00371996]\n",
      " [-0.06788016]\n",
      " [-0.18071564]\n",
      " ...\n",
      " [-0.0741346 ]\n",
      " [-0.00534996]\n",
      " [-0.17654891]]\n",
      "t [[ 0.00371996]\n",
      " [-0.06788016]\n",
      " [-0.18071564]\n",
      " ...\n",
      " [-0.0741346 ]\n",
      " [-0.00534996]\n",
      " [-0.17654891]]\n",
      "Current iteration=6, loss=38880.907589799885\n",
      "t [[ 0.00358411]\n",
      " [-0.08032995]\n",
      " [-0.20524787]\n",
      " ...\n",
      " [-0.0830059 ]\n",
      " [-0.00516971]\n",
      " [-0.20364224]]\n",
      "t [[ 0.00358411]\n",
      " [-0.08032995]\n",
      " [-0.20524787]\n",
      " ...\n",
      " [-0.0830059 ]\n",
      " [-0.00516971]\n",
      " [-0.20364224]]\n",
      "t [[ 0.00328634]\n",
      " [-0.09299746]\n",
      " [-0.22849618]\n",
      " ...\n",
      " [-0.09109892]\n",
      " [-0.00476887]\n",
      " [-0.23014204]]\n",
      "t [[ 0.00328634]\n",
      " [-0.09299746]\n",
      " [-0.22849618]\n",
      " ...\n",
      " [-0.09109892]\n",
      " [-0.00476887]\n",
      " [-0.23014204]]\n",
      "Current iteration=8, loss=38524.54886158138\n",
      "t [[ 0.00284321]\n",
      " [-0.10585016]\n",
      " [-0.25055779]\n",
      " ...\n",
      " [-0.09848079]\n",
      " [-0.00417337]\n",
      " [-0.2560724 ]]\n",
      "t [[ 0.00284321]\n",
      " [-0.10585016]\n",
      " [-0.25055779]\n",
      " ...\n",
      " [-0.09848079]\n",
      " [-0.00417337]\n",
      " [-0.2560724 ]]\n",
      "t [[ 0.00226986]\n",
      " [-0.11885848]\n",
      " [-0.27152202]\n",
      " ...\n",
      " [-0.10521298]\n",
      " [-0.0034068 ]\n",
      " [-0.28145607]]\n",
      "loss=38208.77533275602\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01632728]\n",
      " [-0.06974825]\n",
      " [-0.04383929]\n",
      " ...\n",
      " [-0.03177262]\n",
      " [ 0.00687895]\n",
      " [-0.01852176]]\n",
      "t [[ 0.01632728]\n",
      " [-0.06974825]\n",
      " [-0.04383929]\n",
      " ...\n",
      " [-0.03177262]\n",
      " [ 0.00687895]\n",
      " [-0.01852176]]\n",
      "t [[ 0.03138367]\n",
      " [-0.13513262]\n",
      " [-0.08480987]\n",
      " ...\n",
      " [-0.06114613]\n",
      " [ 0.01289302]\n",
      " [-0.03638558]]\n",
      "t [[ 0.03138367]\n",
      " [-0.13513262]\n",
      " [-0.08480987]\n",
      " ...\n",
      " [-0.06114613]\n",
      " [ 0.01289302]\n",
      " [-0.03638558]]\n",
      "Current iteration=2, loss=39668.50641224014\n",
      "t [[ 0.04527221]\n",
      " [-0.1965068 ]\n",
      " [-0.12315771]\n",
      " ...\n",
      " [-0.08833699]\n",
      " [ 0.01811125]\n",
      " [-0.05362925]]\n",
      "t [[ 0.04527221]\n",
      " [-0.1965068 ]\n",
      " [-0.12315771]\n",
      " ...\n",
      " [-0.08833699]\n",
      " [ 0.01811125]\n",
      " [-0.05362925]]\n",
      "t [[ 0.05808883]\n",
      " [-0.25419931]\n",
      " [-0.15911055]\n",
      " ...\n",
      " [-0.11354512]\n",
      " [ 0.02259828]\n",
      " [-0.07028839]]\n",
      "t [[ 0.05808883]\n",
      " [-0.25419931]\n",
      " [-0.15911055]\n",
      " ...\n",
      " [-0.11354512]\n",
      " [ 0.02259828]\n",
      " [-0.07028839]]\n",
      "Current iteration=4, loss=39135.985724397426\n",
      "t [[ 0.06992224]\n",
      " [-0.30851276]\n",
      " [-0.19287746]\n",
      " ...\n",
      " [-0.13695366]\n",
      " [ 0.02641403]\n",
      " [-0.08639637]]\n",
      "t [[ 0.06992224]\n",
      " [-0.30851276]\n",
      " [-0.19287746]\n",
      " ...\n",
      " [-0.13695366]\n",
      " [ 0.02641403]\n",
      " [-0.08639637]]\n",
      "t [[ 0.08085395]\n",
      " [-0.35972422]\n",
      " [-0.22464923]\n",
      " ...\n",
      " [-0.15872949]\n",
      " [ 0.02961361]\n",
      " [-0.10198422]]\n",
      "t [[ 0.08085395]\n",
      " [-0.35972422]\n",
      " [-0.22464923]\n",
      " ...\n",
      " [-0.15872949]\n",
      " [ 0.02961361]\n",
      " [-0.10198422]]\n",
      "Current iteration=6, loss=38686.441977093564\n",
      "t [[ 0.09095858]\n",
      " [-0.4080863 ]\n",
      " [-0.25459919]\n",
      " ...\n",
      " [-0.17902396]\n",
      " [ 0.03224747]\n",
      " [-0.11708071]]\n",
      "t [[ 0.09095858]\n",
      " [-0.4080863 ]\n",
      " [-0.25459919]\n",
      " ...\n",
      " [-0.17902396]\n",
      " [ 0.03224747]\n",
      " [-0.11708071]]\n",
      "t [[ 0.10030429]\n",
      " [-0.45382875]\n",
      " [-0.28288441]\n",
      " ...\n",
      " [-0.19797404]\n",
      " [ 0.03436165]\n",
      " [-0.13171249]]\n",
      "t [[ 0.10030429]\n",
      " [-0.45382875]\n",
      " [-0.28288441]\n",
      " ...\n",
      " [-0.19797404]\n",
      " [ 0.03436165]\n",
      " [-0.13171249]]\n",
      "Current iteration=8, loss=38299.97017122624\n",
      "t [[ 0.10895322]\n",
      " [-0.49716015]\n",
      " [-0.30964696]\n",
      " ...\n",
      " [-0.21570342]\n",
      " [ 0.03599806]\n",
      " [-0.14590417]]\n",
      "t [[ 0.10895322]\n",
      " [-0.49716015]\n",
      " [-0.30964696]\n",
      " ...\n",
      " [-0.21570342]\n",
      " [ 0.03599806]\n",
      " [-0.14590417]]\n",
      "t [[ 0.11696198]\n",
      " [-0.53826975]\n",
      " [-0.33501525]\n",
      " ...\n",
      " [-0.23232373]\n",
      " [ 0.03719486]\n",
      " [-0.15967851]]\n",
      "loss=37962.42109317938\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00215314]\n",
      " [-0.01325658]\n",
      " [-0.04156075]\n",
      " ...\n",
      " [-0.03189282]\n",
      " [ 0.00620018]\n",
      " [-0.01892024]]\n",
      "t [[ 0.00215314]\n",
      " [-0.01325658]\n",
      " [-0.04156075]\n",
      " ...\n",
      " [-0.03189282]\n",
      " [ 0.00620018]\n",
      " [-0.01892024]]\n",
      "t [[ 0.00386653]\n",
      " [-0.02718619]\n",
      " [-0.08017729]\n",
      " ...\n",
      " [-0.06133471]\n",
      " [ 0.01155575]\n",
      " [-0.03716758]]\n",
      "t [[ 0.00386653]\n",
      " [-0.02718619]\n",
      " [-0.08017729]\n",
      " ...\n",
      " [-0.06133471]\n",
      " [ 0.01155575]\n",
      " [-0.03716758]]\n",
      "Current iteration=2, loss=39660.99750009919\n",
      "t [[ 0.00518647]\n",
      " [-0.04169627]\n",
      " [-0.11611346]\n",
      " ...\n",
      " [-0.08854919]\n",
      " [ 0.01613601]\n",
      " [-0.05478102]]\n",
      "t [[ 0.00518647]\n",
      " [-0.04169627]\n",
      " [-0.11611346]\n",
      " ...\n",
      " [-0.08854919]\n",
      " [ 0.01613601]\n",
      " [-0.05478102]]\n",
      "t [[ 0.00615569]\n",
      " [-0.05670223]\n",
      " [-0.14961285]\n",
      " ...\n",
      " [-0.11374249]\n",
      " [ 0.02000576]\n",
      " [-0.07179728]]\n",
      "t [[ 0.00615569]\n",
      " [-0.05670223]\n",
      " [-0.14961285]\n",
      " ...\n",
      " [-0.11374249]\n",
      " [ 0.02000576]\n",
      " [-0.07179728]]\n",
      "Current iteration=4, loss=39122.319584575\n",
      "t [[ 0.00681319]\n",
      " [-0.07212746]\n",
      " [-0.18089843]\n",
      " ...\n",
      " [-0.13710332]\n",
      " [ 0.0232249 ]\n",
      " [-0.08825069]]\n",
      "t [[ 0.00681319]\n",
      " [-0.07212746]\n",
      " [-0.18089843]\n",
      " ...\n",
      " [-0.13710332]\n",
      " [ 0.0232249 ]\n",
      " [-0.08825069]]\n",
      "t [[ 0.00719433]\n",
      " [-0.08790304]\n",
      " [-0.21017313]\n",
      " ...\n",
      " [-0.15880338]\n",
      " [ 0.02584844]\n",
      " [-0.10417317]]\n",
      "t [[ 0.00719433]\n",
      " [-0.08790304]\n",
      " [-0.21017313]\n",
      " ...\n",
      " [-0.15880338]\n",
      " [ 0.02584844]\n",
      " [-0.10417317]]\n",
      "Current iteration=6, loss=38667.4138982772\n",
      "t [[ 0.00733096]\n",
      " [-0.10396726]\n",
      " [-0.23762085]\n",
      " ...\n",
      " [-0.17899827]\n",
      " [ 0.0279266 ]\n",
      " [-0.11959431]]\n",
      "t [[ 0.00733096]\n",
      " [-0.10396726]\n",
      " [-0.23762085]\n",
      " ...\n",
      " [-0.17899827]\n",
      " [ 0.0279266 ]\n",
      " [-0.11959431]]\n",
      "t [[ 0.00725166]\n",
      " [-0.12026509]\n",
      " [-0.26340778]\n",
      " ...\n",
      " [-0.19782857]\n",
      " [ 0.0295051 ]\n",
      " [-0.13454144]]\n",
      "t [[ 0.00725166]\n",
      " [-0.12026509]\n",
      " [-0.26340778]\n",
      " ...\n",
      " [-0.19782857]\n",
      " [ 0.0295051 ]\n",
      " [-0.13454144]]\n",
      "Current iteration=8, loss=38276.07073934251\n",
      "t [[ 0.00698205]\n",
      " [-0.13674755]\n",
      " [-0.28768385]\n",
      " ...\n",
      " [-0.21542112]\n",
      " [ 0.0306255 ]\n",
      " [-0.14903981]]\n",
      "t [[ 0.00698205]\n",
      " [-0.13674755]\n",
      " [-0.28768385]\n",
      " ...\n",
      " [-0.21542112]\n",
      " [ 0.0306255 ]\n",
      " [-0.14903981]]\n",
      "t [[ 0.00654499]\n",
      " [-0.15337118]\n",
      " [-0.31058419]\n",
      " ...\n",
      " [-0.23189024]\n",
      " [ 0.03132552]\n",
      " [-0.16311275]]\n",
      "loss=37933.98255937103\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00183902]\n",
      " [-0.01163477]\n",
      " [-0.04201087]\n",
      " ...\n",
      " [-0.03135947]\n",
      " [ 0.00666641]\n",
      " [-0.01774726]]\n",
      "t [[ 0.00183902]\n",
      " [-0.01163477]\n",
      " [-0.04201087]\n",
      " ...\n",
      " [-0.03135947]\n",
      " [ 0.00666641]\n",
      " [-0.01774726]]\n",
      "t [[ 0.00324262]\n",
      " [-0.02397372]\n",
      " [-0.08108363]\n",
      " ...\n",
      " [-0.06030142]\n",
      " [ 0.01247996]\n",
      " [-0.03484426]]\n",
      "t [[ 0.00324262]\n",
      " [-0.02397372]\n",
      " [-0.08108363]\n",
      " ...\n",
      " [-0.06030142]\n",
      " [ 0.01247996]\n",
      " [-0.03484426]]\n",
      "Current iteration=2, loss=39667.87517585917\n",
      "t [[ 0.00425737]\n",
      " [-0.03692398]\n",
      " [-0.11747961]\n",
      " ...\n",
      " [-0.08704585]\n",
      " [ 0.01750976]\n",
      " [-0.05132917]]\n",
      "t [[ 0.00425737]\n",
      " [-0.03692398]\n",
      " [-0.11747961]\n",
      " ...\n",
      " [-0.08704585]\n",
      " [ 0.01750976]\n",
      " [-0.05132917]]\n",
      "t [[ 0.00492624]\n",
      " [-0.05040065]\n",
      " [-0.15144012]\n",
      " ...\n",
      " [-0.11179585]\n",
      " [ 0.02182037]\n",
      " [-0.06723799]]\n",
      "t [[ 0.00492624]\n",
      " [-0.05040065]\n",
      " [-0.15144012]\n",
      " ...\n",
      " [-0.11179585]\n",
      " [ 0.02182037]\n",
      " [-0.06723799]]\n",
      "Current iteration=4, loss=39135.2225580142\n",
      "t [[ 0.00528843]\n",
      " [-0.06432669]\n",
      " [-0.18318624]\n",
      " ...\n",
      " [-0.13473732]\n",
      " [ 0.02547151]\n",
      " [-0.08260436]]\n",
      "t [[ 0.00528843]\n",
      " [-0.06432669]\n",
      " [-0.18318624]\n",
      " ...\n",
      " [-0.13473732]\n",
      " [ 0.02547151]\n",
      " [-0.08260436]]\n",
      "t [[ 0.00537947]\n",
      " [-0.07863273]\n",
      " [-0.21291927]\n",
      " ...\n",
      " [-0.15603952]\n",
      " [ 0.02851804]\n",
      " [-0.09745952]]\n",
      "t [[ 0.00537947]\n",
      " [-0.07863273]\n",
      " [-0.21291927]\n",
      " ...\n",
      " [-0.15603952]\n",
      " [ 0.02851804]\n",
      " [-0.09745952]]\n",
      "Current iteration=6, loss=38685.766222001446\n",
      "t [[ 0.00523133]\n",
      " [-0.09325654]\n",
      " [-0.24082177]\n",
      " ...\n",
      " [-0.17585589]\n",
      " [ 0.0310101 ]\n",
      " [-0.11183242]]\n",
      "t [[ 0.00523133]\n",
      " [-0.09325654]\n",
      " [-0.24082177]\n",
      " ...\n",
      " [-0.17585589]\n",
      " [ 0.0310101 ]\n",
      " [-0.11183242]]\n",
      "t [[ 0.00487275]\n",
      " [-0.10814249]\n",
      " [-0.26705885]\n",
      " ...\n",
      " [-0.19432516]\n",
      " [ 0.03299335]\n",
      " [-0.12574982]]\n",
      "t [[ 0.00487275]\n",
      " [-0.10814249]\n",
      " [-0.26705885]\n",
      " ...\n",
      " [-0.19432516]\n",
      " [ 0.03299335]\n",
      " [-0.12574982]]\n",
      "Current iteration=8, loss=38299.45643115796\n",
      "t [[ 0.00432942]\n",
      " [-0.123241  ]\n",
      " [-0.29177956]\n",
      " ...\n",
      " [-0.21157255]\n",
      " [ 0.03450933]\n",
      " [-0.1392364 ]]\n",
      "t [[ 0.00432942]\n",
      " [-0.123241  ]\n",
      " [-0.29177956]\n",
      " ...\n",
      " [-0.21157255]\n",
      " [ 0.03450933]\n",
      " [-0.1392364 ]]\n",
      "t [[ 0.00362431]\n",
      " [-0.13850795]\n",
      " [-0.31511837]\n",
      " ...\n",
      " [-0.22771096]\n",
      " [ 0.03559579]\n",
      " [-0.15231493]]\n",
      "loss=37962.07574532175\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00153153]\n",
      " [-0.01232322]\n",
      " [-0.04173367]\n",
      " ...\n",
      " [-0.01837426]\n",
      " [-0.00221295]\n",
      " [-0.03748647]]\n",
      "t [[ 0.00153153]\n",
      " [-0.01232322]\n",
      " [-0.04173367]\n",
      " ...\n",
      " [-0.01837426]\n",
      " [-0.00221295]\n",
      " [-0.03748647]]\n",
      "t [[ 0.00263624]\n",
      " [-0.02534583]\n",
      " [-0.08050474]\n",
      " ...\n",
      " [-0.0348498 ]\n",
      " [-0.00379809]\n",
      " [-0.07386787]]\n",
      "t [[ 0.00263624]\n",
      " [-0.02534583]\n",
      " [-0.08050474]\n",
      " ...\n",
      " [-0.0348498 ]\n",
      " [-0.00379809]\n",
      " [-0.07386787]]\n",
      "Current iteration=2, loss=39656.49262749441\n",
      "t [[ 0.00336052]\n",
      " [-0.03897438]\n",
      " [-0.11657797]\n",
      " ...\n",
      " [-0.04961288]\n",
      " [-0.00483106]\n",
      " [-0.10920047]]\n",
      "t [[ 0.00336052]\n",
      " [-0.03897438]\n",
      " [-0.11657797]\n",
      " ...\n",
      " [-0.04961288]\n",
      " [-0.00483106]\n",
      " [-0.10920047]]\n",
      "t [[ 0.00374708]\n",
      " [-0.0531234 ]\n",
      " [-0.15019786]\n",
      " ...\n",
      " [-0.06283479]\n",
      " [-0.00538067]\n",
      " [-0.14353774]]\n",
      "t [[ 0.00374708]\n",
      " [-0.0531234 ]\n",
      " [-0.15019786]\n",
      " ...\n",
      " [-0.06283479]\n",
      " [-0.00538067]\n",
      " [-0.14353774]]\n",
      "Current iteration=4, loss=39114.67014234568\n",
      "t [[ 0.00383491]\n",
      " [-0.06771541]\n",
      " [-0.18158831]\n",
      " ...\n",
      " [-0.07467185]\n",
      " [-0.0055091 ]\n",
      " [-0.17693012]]\n",
      "t [[ 0.00383491]\n",
      " [-0.06771541]\n",
      " [-0.18158831]\n",
      " ...\n",
      " [-0.07467185]\n",
      " [-0.0055091 ]\n",
      " [-0.17693012]]\n",
      "t [[ 0.00365928]\n",
      " [-0.08268062]\n",
      " [-0.21095318]\n",
      " ...\n",
      " [-0.08526582]\n",
      " [-0.0052722 ]\n",
      " [-0.20942503]]\n",
      "t [[ 0.00365928]\n",
      " [-0.08268062]\n",
      " [-0.21095318]\n",
      " ...\n",
      " [-0.08526582]\n",
      " [-0.0052722 ]\n",
      " [-0.20942503]]\n",
      "Current iteration=6, loss=38657.59930497724\n",
      "t [[ 0.00325193]\n",
      " [-0.09795648]\n",
      " [-0.23847728]\n",
      " ...\n",
      " [-0.09474473]\n",
      " [-0.00471994]\n",
      " [-0.24106686]]\n",
      "t [[ 0.00325193]\n",
      " [-0.09795648]\n",
      " [-0.23847728]\n",
      " ...\n",
      " [-0.09474473]\n",
      " [-0.00471994]\n",
      " [-0.24106686]]\n",
      "t [[ 0.00264134]\n",
      " [-0.11348713]\n",
      " [-0.26432769]\n",
      " ...\n",
      " [-0.10322394]\n",
      " [-0.00389689]\n",
      " [-0.27189711]]\n",
      "t [[ 0.00264134]\n",
      " [-0.11348713]\n",
      " [-0.26432769]\n",
      " ...\n",
      " [-0.10322394]\n",
      " [-0.00389689]\n",
      " [-0.27189711]]\n",
      "Current iteration=8, loss=38264.78805369277\n",
      "t [[ 0.00185298]\n",
      " [-0.1292228 ]\n",
      " [-0.28865522]\n",
      " ...\n",
      " [-0.11080712]\n",
      " [-0.0028427 ]\n",
      " [-0.30195451]]\n",
      "t [[ 0.00185298]\n",
      " [-0.1292228 ]\n",
      " [-0.28865522]\n",
      " ...\n",
      " [-0.11080712]\n",
      " [-0.0028427 ]\n",
      " [-0.30195451]]\n",
      "t [[ 0.00090958]\n",
      " [-0.14511924]\n",
      " [-0.31159584]\n",
      " ...\n",
      " [-0.1175874 ]\n",
      " [-0.00159262]\n",
      " [-0.33127521]]\n",
      "loss=37921.722684952154\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01904849]\n",
      " [-0.08137296]\n",
      " [-0.05114583]\n",
      " ...\n",
      " [-0.03706805]\n",
      " [ 0.00802544]\n",
      " [-0.02160873]]\n",
      "t [[ 0.01904849]\n",
      " [-0.08137296]\n",
      " [-0.05114583]\n",
      " ...\n",
      " [-0.03706805]\n",
      " [ 0.00802544]\n",
      " [-0.02160873]]\n",
      "t [[ 0.03636741]\n",
      " [-0.15680707]\n",
      " [-0.09838763]\n",
      " ...\n",
      " [-0.07087114]\n",
      " [ 0.01487389]\n",
      " [-0.04232201]]\n",
      "t [[ 0.03636741]\n",
      " [-0.15680707]\n",
      " [-0.09838763]\n",
      " ...\n",
      " [-0.07087114]\n",
      " [ 0.01487389]\n",
      " [-0.04232201]]\n",
      "Current iteration=2, loss=39570.28274598911\n",
      "t [[ 0.05212088]\n",
      " [-0.22686575]\n",
      " [-0.14211716]\n",
      " ...\n",
      " [-0.10175393]\n",
      " [ 0.02065542]\n",
      " [-0.06220009]]\n",
      "t [[ 0.05212088]\n",
      " [-0.22686575]\n",
      " [-0.14211716]\n",
      " ...\n",
      " [-0.10175393]\n",
      " [ 0.02065542]\n",
      " [-0.06220009]]\n",
      "t [[ 0.06645946]\n",
      " [-0.29206423]\n",
      " [-0.18269134]\n",
      " ...\n",
      " [-0.13002957]\n",
      " [ 0.02547156]\n",
      " [-0.08129903]]\n",
      "t [[ 0.06645946]\n",
      " [-0.29206423]\n",
      " [-0.18269134]\n",
      " ...\n",
      " [-0.13002957]\n",
      " [ 0.02547156]\n",
      " [-0.08129903]]\n",
      "Current iteration=4, loss=38974.25110300347\n",
      "t [[ 0.07952001]\n",
      " [-0.35286875]\n",
      " [-0.22043186]\n",
      " ...\n",
      " [-0.15597971]\n",
      " [ 0.02941478]\n",
      " [-0.09967053]]\n",
      "t [[ 0.07952001]\n",
      " [-0.35286875]\n",
      " [-0.22043186]\n",
      " ...\n",
      " [-0.15597971]\n",
      " [ 0.02941478]\n",
      " [-0.09967053]]\n",
      "t [[ 0.09142609]\n",
      " [-0.40969825]\n",
      " [-0.25562665]\n",
      " ...\n",
      " [-0.17985594]\n",
      " [ 0.03256859]\n",
      " [-0.11736196]]\n",
      "t [[ 0.09142609]\n",
      " [-0.40969825]\n",
      " [-0.25562665]\n",
      " ...\n",
      " [-0.17985594]\n",
      " [ 0.03256859]\n",
      " [-0.11736196]]\n",
      "Current iteration=6, loss=38482.11997588381\n",
      "t [[ 0.10228879]\n",
      " [-0.46292762]\n",
      " [-0.28853226]\n",
      " ...\n",
      " [-0.20188209]\n",
      " [ 0.03500802]\n",
      " [-0.13441658]]\n",
      "t [[ 0.10228879]\n",
      " [-0.46292762]\n",
      " [-0.28853226]\n",
      " ...\n",
      " [-0.20188209]\n",
      " [ 0.03500802]\n",
      " [-0.13441658]]\n",
      "t [[ 0.11220779]\n",
      " [-0.51289145]\n",
      " [-0.31937667]\n",
      " ...\n",
      " [-0.22225669]\n",
      " [ 0.03680028]\n",
      " [-0.15087383]]\n",
      "t [[ 0.11220779]\n",
      " [-0.51289145]\n",
      " [-0.31937667]\n",
      " ...\n",
      " [-0.22225669]\n",
      " [ 0.03680028]\n",
      " [-0.15087383]]\n",
      "Current iteration=8, loss=38065.94517242433\n",
      "t [[ 0.1212724 ]\n",
      " [-0.55988814]\n",
      " [-0.34836225]\n",
      " ...\n",
      " [-0.24115566]\n",
      " [ 0.03800553]\n",
      " [-0.16676964]]\n",
      "t [[ 0.1212724 ]\n",
      " [-0.55988814]\n",
      " [-0.34836225]\n",
      " ...\n",
      " [-0.24115566]\n",
      " [ 0.03800553]\n",
      " [-0.16676964]]\n",
      "t [[ 0.12956262]\n",
      " [-0.60418372]\n",
      " [-0.3756686 ]\n",
      " ...\n",
      " [-0.25873478]\n",
      " [ 0.0386776 ]\n",
      " [-0.18213679]]\n",
      "loss=37706.935229092975\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00251199]\n",
      " [-0.01546601]\n",
      " [-0.04848754]\n",
      " ...\n",
      " [-0.03720829]\n",
      " [ 0.00723355]\n",
      " [-0.02207361]]\n",
      "t [[ 0.00251199]\n",
      " [-0.01546601]\n",
      " [-0.04848754]\n",
      " ...\n",
      " [-0.03720829]\n",
      " [ 0.00723355]\n",
      " [-0.02207361]]\n",
      "t [[ 0.00442557]\n",
      " [-0.03184791]\n",
      " [-0.09296829]\n",
      " ...\n",
      " [-0.07108109]\n",
      " [ 0.01331769]\n",
      " [-0.04323144]]\n",
      "t [[ 0.00442557]\n",
      " [-0.03184791]\n",
      " [-0.09296829]\n",
      " ...\n",
      " [-0.07108109]\n",
      " [ 0.01331769]\n",
      " [-0.04323144]]\n",
      "Current iteration=2, loss=39561.64156742538\n",
      "t [[ 0.0058145 ]\n",
      " [-0.04899836]\n",
      " [-0.13386244]\n",
      " ...\n",
      " [-0.10197434]\n",
      " [ 0.01836292]\n",
      " [-0.06353563]]\n",
      "t [[ 0.0058145 ]\n",
      " [-0.04899836]\n",
      " [-0.13386244]\n",
      " ...\n",
      " [-0.10197434]\n",
      " [ 0.01836292]\n",
      " [-0.06353563]]\n",
      "t [[ 0.00674569]\n",
      " [-0.06678518]\n",
      " [-0.17155149]\n",
      " ...\n",
      " [-0.13021097]\n",
      " [ 0.02247097]\n",
      " [-0.08304395]]\n",
      "t [[ 0.00674569]\n",
      " [-0.06678518]\n",
      " [-0.17155149]\n",
      " ...\n",
      " [-0.13021097]\n",
      " [ 0.02247097]\n",
      " [-0.08304395]]\n",
      "Current iteration=4, loss=38958.6938653924\n",
      "t [[ 0.00727901]\n",
      " [-0.08509115]\n",
      " [-0.20637818]\n",
      " ...\n",
      " [-0.15608106]\n",
      " [ 0.02573422]\n",
      " [-0.1018096 ]]\n",
      "t [[ 0.00727901]\n",
      " [-0.08509115]\n",
      " [-0.20637818]\n",
      " ...\n",
      " [-0.15608106]\n",
      " [ 0.02573422]\n",
      " [-0.1018096 ]]\n",
      "t [[ 0.00746755]\n",
      " [-0.1038131 ]\n",
      " [-0.23864832]\n",
      " ...\n",
      " [-0.17984332]\n",
      " [ 0.0282359 ]\n",
      " [-0.11988129]]\n",
      "t [[ 0.00746755]\n",
      " [-0.1038131 ]\n",
      " [-0.23864832]\n",
      " ...\n",
      " [-0.17984332]\n",
      " [ 0.0282359 ]\n",
      " [-0.11988129]]\n",
      "Current iteration=6, loss=38460.57625002823\n",
      "t [[ 0.00735812]\n",
      " [-0.12286075]\n",
      " [-0.26863353]\n",
      " ...\n",
      " [-0.20172758]\n",
      " [ 0.03005058]\n",
      " [-0.13730342]]\n",
      "t [[ 0.00735812]\n",
      " [-0.12286075]\n",
      " [-0.26863353]\n",
      " ...\n",
      " [-0.20172758]\n",
      " [ 0.03005058]\n",
      " [-0.13730342]]\n",
      "t [[ 0.00699183]\n",
      " [-0.14215542]\n",
      " [-0.29657447]\n",
      " ...\n",
      " [-0.22193743]\n",
      " [ 0.03124488]\n",
      " [-0.15411646]]\n",
      "t [[ 0.00699183]\n",
      " [-0.14215542]\n",
      " [-0.29657447]\n",
      " ...\n",
      " [-0.22193743]\n",
      " [ 0.03124488]\n",
      " [-0.15411646]]\n",
      "Current iteration=8, loss=38038.96003456318\n",
      "t [[ 0.00640471]\n",
      " [-0.16162869]\n",
      " [-0.32268409]\n",
      " ...\n",
      " [-0.24065301]\n",
      " [ 0.03187825]\n",
      " [-0.17035722]]\n",
      "t [[ 0.00640471]\n",
      " [-0.16162869]\n",
      " [-0.32268409]\n",
      " ...\n",
      " [-0.24065301]\n",
      " [ 0.03187825]\n",
      " [-0.17035722]]\n",
      "t [[ 0.00562828]\n",
      " [-0.18122123]\n",
      " [-0.34715075]\n",
      " ...\n",
      " [-0.25803363]\n",
      " [ 0.03200376]\n",
      " [-0.18605926]]\n",
      "loss=37674.881042323745\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00214552]\n",
      " [-0.01357389]\n",
      " [-0.04901268]\n",
      " ...\n",
      " [-0.03658605]\n",
      " [ 0.00777747]\n",
      " [-0.02070514]]\n",
      "t [[ 0.00214552]\n",
      " [-0.01357389]\n",
      " [-0.04901268]\n",
      " ...\n",
      " [-0.03658605]\n",
      " [ 0.00777747]\n",
      " [-0.02070514]]\n",
      "t [[ 0.00369851]\n",
      " [-0.02810608]\n",
      " [-0.09402687]\n",
      " ...\n",
      " [-0.06988206]\n",
      " [ 0.01439433]\n",
      " [-0.0405253 ]]\n",
      "t [[ 0.00369851]\n",
      " [-0.02810608]\n",
      " [-0.09402687]\n",
      " ...\n",
      " [-0.06988206]\n",
      " [ 0.01439433]\n",
      " [-0.0405253 ]]\n",
      "Current iteration=2, loss=39569.59339654575\n",
      "t [[ 0.00473318]\n",
      " [-0.04344876]\n",
      " [-0.13545871]\n",
      " ...\n",
      " [-0.10023839]\n",
      " [ 0.0199607 ]\n",
      " [-0.05952133]]\n",
      "t [[ 0.00473318]\n",
      " [-0.04344876]\n",
      " [-0.13545871]\n",
      " ...\n",
      " [-0.10023839]\n",
      " [ 0.0199607 ]\n",
      " [-0.05952133]]\n",
      "t [[ 0.00531679]\n",
      " [-0.05946919]\n",
      " [-0.17368623]\n",
      " ...\n",
      " [-0.12797307]\n",
      " [ 0.02457796]\n",
      " [-0.07774986]]\n",
      "t [[ 0.00531679]\n",
      " [-0.05946919]\n",
      " [-0.17368623]\n",
      " ...\n",
      " [-0.12797307]\n",
      " [ 0.02457796]\n",
      " [-0.07774986]]\n",
      "Current iteration=4, loss=38973.49364365149\n",
      "t [[ 0.0055095 ]\n",
      " [-0.07604947]\n",
      " [-0.20904931]\n",
      " ...\n",
      " [-0.15337191]\n",
      " [ 0.02833824]\n",
      " [-0.09526298]]\n",
      "t [[ 0.0055095 ]\n",
      " [-0.07604947]\n",
      " [-0.20904931]\n",
      " ...\n",
      " [-0.15337191]\n",
      " [ 0.02833824]\n",
      " [-0.09526298]]\n",
      "t [[ 0.00536466]\n",
      " [-0.09308562]\n",
      " [-0.24185143]\n",
      " ...\n",
      " [-0.17669003]\n",
      " [ 0.03132458]\n",
      " [-0.11210838]]\n",
      "t [[ 0.00536466]\n",
      " [-0.09308562]\n",
      " [-0.24185143]\n",
      " ...\n",
      " [-0.17669003]\n",
      " [ 0.03132458]\n",
      " [-0.11210838]]\n",
      "Current iteration=6, loss=38481.51860399378\n",
      "t [[ 0.00492928]\n",
      " [-0.11048645]\n",
      " [-0.2723624 ]\n",
      " ...\n",
      " [-0.19815418]\n",
      " [ 0.03361144]\n",
      " [-0.1283295 ]]\n",
      "t [[ 0.00492928]\n",
      " [-0.11048645]\n",
      " [-0.2723624 ]\n",
      " ...\n",
      " [-0.19815418]\n",
      " [ 0.03361144]\n",
      " [-0.1283295 ]]\n",
      "t [[ 0.00424464]\n",
      " [-0.12817231]\n",
      " [-0.30082149]\n",
      " ...\n",
      " [-0.21796535]\n",
      " [ 0.03526543]\n",
      " [-0.1439659 ]]\n",
      "t [[ 0.00424464]\n",
      " [-0.12817231]\n",
      " [-0.30082149]\n",
      " ...\n",
      " [-0.21796535]\n",
      " [ 0.03526543]\n",
      " [-0.1439659 ]]\n",
      "Current iteration=8, loss=38065.54647818749\n",
      "t [[ 0.00334689]\n",
      " [-0.14607373]\n",
      " [-0.32744058]\n",
      " ...\n",
      " [-0.23630146]\n",
      " [ 0.03634603]\n",
      " [-0.15905354]]\n",
      "t [[ 0.00334689]\n",
      " [-0.14607373]\n",
      " [-0.32744058]\n",
      " ...\n",
      " [-0.23630146]\n",
      " [ 0.03634603]\n",
      " [-0.15905354]]\n",
      "t [[ 0.00226766]\n",
      " [-0.16413033]\n",
      " [-0.35240727]\n",
      " ...\n",
      " [-0.25331997]\n",
      " [ 0.0369064 ]\n",
      " [-0.17362518]]\n",
      "loss=37706.71793638922\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00178679]\n",
      " [-0.01437709]\n",
      " [-0.04868928]\n",
      " ...\n",
      " [-0.02143664]\n",
      " [-0.00258177]\n",
      " [-0.04373421]]\n",
      "t [[ 0.00178679]\n",
      " [-0.01437709]\n",
      " [-0.04868928]\n",
      " ...\n",
      " [-0.02143664]\n",
      " [-0.00258177]\n",
      " [-0.04373421]]\n",
      "t [[ 0.00299275]\n",
      " [-0.02970594]\n",
      " [-0.09334675]\n",
      " ...\n",
      " [-0.0402893 ]\n",
      " [-0.00430915]\n",
      " [-0.08596446]]\n",
      "t [[ 0.00299275]\n",
      " [-0.02970594]\n",
      " [-0.09334675]\n",
      " ...\n",
      " [-0.0402893 ]\n",
      " [-0.00430915]\n",
      " [-0.08596446]]\n",
      "Current iteration=2, loss=39556.49755830079\n",
      "t [[ 0.00369177]\n",
      " [-0.04583781]\n",
      " [-0.13439404]\n",
      " ...\n",
      " [-0.05685451]\n",
      " [-0.00530245]\n",
      " [-0.12678043]]\n",
      "t [[ 0.00369177]\n",
      " [-0.04583781]\n",
      " [-0.13439404]\n",
      " ...\n",
      " [-0.05685451]\n",
      " [-0.00530245]\n",
      " [-0.12678043]]\n",
      "t [[ 0.00395076]\n",
      " [-0.0626391 ]\n",
      " [-0.17221411]\n",
      " ...\n",
      " [-0.07140038]\n",
      " [-0.00566913]\n",
      " [-0.1662664 ]]\n",
      "t [[ 0.00395076]\n",
      " [-0.0626391 ]\n",
      " [-0.17221411]\n",
      " ...\n",
      " [-0.07140038]\n",
      " [-0.00566913]\n",
      " [-0.1662664 ]]\n",
      "Current iteration=4, loss=38950.18555952627\n",
      "t [[ 0.00382948]\n",
      " [-0.07999122]\n",
      " [-0.20715121]\n",
      " ...\n",
      " [-0.08416682]\n",
      " [-0.00550427]\n",
      " [-0.20450089]]\n",
      "t [[ 0.00382948]\n",
      " [-0.07999122]\n",
      " [-0.20715121]\n",
      " ...\n",
      " [-0.08416682]\n",
      " [-0.00550427]\n",
      " [-0.20450089]]\n",
      "t [[ 0.00338091]\n",
      " [-0.09778962]\n",
      " [-0.23951259]\n",
      " ...\n",
      " [-0.09536709]\n",
      " [-0.00489145]\n",
      " [-0.24155668]]\n",
      "t [[ 0.00338091]\n",
      " [-0.09778962]\n",
      " [-0.23951259]\n",
      " ...\n",
      " [-0.09536709]\n",
      " [-0.00489145]\n",
      " [-0.24155668]]\n",
      "Current iteration=6, loss=38449.91025941297\n",
      "t [[ 0.00265166]\n",
      " [-0.11594271]\n",
      " [-0.26957132]\n",
      " ...\n",
      " [-0.10518988]\n",
      " [-0.00390378]\n",
      " [-0.27750101]]\n",
      "t [[ 0.00265166]\n",
      " [-0.11594271]\n",
      " [-0.26957132]\n",
      " ...\n",
      " [-0.10518988]\n",
      " [-0.00390378]\n",
      " [-0.27750101]]\n",
      "t [[ 0.00168263]\n",
      " [-0.13437052]\n",
      " [-0.29756946]\n",
      " ...\n",
      " [-0.11380167]\n",
      " [-0.00260502]\n",
      " [-0.31239589]]\n",
      "t [[ 0.00168263]\n",
      " [-0.13437052]\n",
      " [-0.29756946]\n",
      " ...\n",
      " [-0.11380167]\n",
      " [-0.00260502]\n",
      " [-0.31239589]]\n",
      "Current iteration=8, loss=38026.941724607124\n",
      "t [[ 0.00050961]\n",
      " [-0.15300342]\n",
      " [-0.32372128]\n",
      " ...\n",
      " [-0.12134916]\n",
      " [-0.00105062]\n",
      " [-0.34629846]]\n",
      "t [[ 0.00050961]\n",
      " [-0.15300342]\n",
      " [-0.32372128]\n",
      " ...\n",
      " [-0.12134916]\n",
      " [-0.00105062]\n",
      " [-0.34629846]]\n",
      "t [[-0.00083613]\n",
      " [-0.17178091]\n",
      " [-0.34821641]\n",
      " ...\n",
      " [-0.12796148]\n",
      " [ 0.00071126]\n",
      " [-0.37926141]]\n",
      "loss=37662.04039436646\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0217697 ]\n",
      " [-0.09299767]\n",
      " [-0.05845238]\n",
      " ...\n",
      " [-0.04236349]\n",
      " [ 0.00917193]\n",
      " [-0.02469569]]\n",
      "t [[ 0.0217697 ]\n",
      " [-0.09299767]\n",
      " [-0.05845238]\n",
      " ...\n",
      " [-0.04236349]\n",
      " [ 0.00917193]\n",
      " [-0.02469569]]\n",
      "t [[ 0.04128075]\n",
      " [-0.17823978]\n",
      " [-0.1118065 ]\n",
      " ...\n",
      " [-0.08046325]\n",
      " [ 0.01680689]\n",
      " [-0.04822197]]\n",
      "t [[ 0.04128075]\n",
      " [-0.17823978]\n",
      " [-0.1118065 ]\n",
      " ...\n",
      " [-0.08046325]\n",
      " [ 0.01680689]\n",
      " [-0.04822197]]\n",
      "Current iteration=2, loss=39474.46246329937\n",
      "t [[ 0.0587788 ]\n",
      " [-0.25656981]\n",
      " [-0.16064877]\n",
      " ...\n",
      " [-0.11481508]\n",
      " [ 0.02306976]\n",
      " [-0.07066906]]\n",
      "t [[ 0.0587788 ]\n",
      " [-0.25656981]\n",
      " [-0.16064877]\n",
      " ...\n",
      " [-0.11481508]\n",
      " [ 0.02306976]\n",
      " [-0.07066906]]\n",
      "t [[ 0.07448577]\n",
      " [-0.32874648]\n",
      " [-0.20550439]\n",
      " ...\n",
      " [-0.14587958]\n",
      " [ 0.02811023]\n",
      " [-0.09211979]]\n",
      "t [[ 0.07448577]\n",
      " [-0.32874648]\n",
      " [-0.20550439]\n",
      " ...\n",
      " [-0.14587958]\n",
      " [ 0.02811023]\n",
      " [-0.09211979]]\n",
      "Current iteration=4, loss=38820.47176943463\n",
      "t [[ 0.08859979]\n",
      " [-0.39544373]\n",
      " [-0.2468378 ]\n",
      " ...\n",
      " [-0.17406308]\n",
      " [ 0.03206221]\n",
      " [-0.11264937]]\n",
      "t [[ 0.08859979]\n",
      " [-0.39544373]\n",
      " [-0.2468378 ]\n",
      " ...\n",
      " [-0.17406308]\n",
      " [ 0.03206221]\n",
      " [-0.11264937]]\n",
      "t [[ 0.10129651]\n",
      " [-0.45725568]\n",
      " [-0.28505646]\n",
      " ...\n",
      " [-0.19972141]\n",
      " [ 0.03504432]\n",
      " [-0.13232564]]\n",
      "t [[ 0.10129651]\n",
      " [-0.45725568]\n",
      " [-0.28505646]\n",
      " ...\n",
      " [-0.19972141]\n",
      " [ 0.03504432]\n",
      " [-0.13232564]]\n",
      "Current iteration=6, loss=38291.4412036011\n",
      "t [[ 0.11273104]\n",
      " [-0.51470387]\n",
      " [-0.32051631]\n",
      " ...\n",
      " [-0.22316476]\n",
      " [ 0.03716115]\n",
      " [-0.15120959]]\n",
      "t [[ 0.11273104]\n",
      " [-0.51470387]\n",
      " [-0.32051631]\n",
      " ...\n",
      " [-0.22316476]\n",
      " [ 0.03716115]\n",
      " [-0.15120959]]\n",
      "t [[ 0.12304006]\n",
      " [-0.56824533]\n",
      " [-0.35352764]\n",
      " ...\n",
      " [-0.24466299]\n",
      " [ 0.03850472]\n",
      " [-0.16935601]]\n",
      "t [[ 0.12304006]\n",
      " [-0.56824533]\n",
      " [-0.35352764]\n",
      " ...\n",
      " [-0.24466299]\n",
      " [ 0.03850472]\n",
      " [-0.16935601]]\n",
      "Current iteration=8, loss=37850.503302894685\n",
      "t [[ 0.13234399]\n",
      " [-0.61828045]\n",
      " [-0.38436079]\n",
      " ...\n",
      " [-0.26445068]\n",
      " [ 0.03915602]\n",
      " [-0.18681417]]\n",
      "t [[ 0.13234399]\n",
      " [-0.61828045]\n",
      " [-0.38436079]\n",
      " ...\n",
      " [-0.26445068]\n",
      " [ 0.03915602]\n",
      " [-0.18681417]]\n",
      "t [[ 0.14074894]\n",
      " [-0.66516033]\n",
      " [-0.41325147]\n",
      " ...\n",
      " [-0.28273177]\n",
      " [ 0.0391864 ]\n",
      " [-0.20362845]]\n",
      "loss=37474.152069790696\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00287085]\n",
      " [-0.01767544]\n",
      " [-0.05541434]\n",
      " ...\n",
      " [-0.04252376]\n",
      " [ 0.00826691]\n",
      " [-0.02522698]]\n",
      "t [[ 0.00287085]\n",
      " [-0.01767544]\n",
      " [-0.05541434]\n",
      " ...\n",
      " [-0.04252376]\n",
      " [ 0.00826691]\n",
      " [-0.02522698]]\n",
      "t [[ 0.00496029]\n",
      " [-0.03654686]\n",
      " [-0.10559622]\n",
      " ...\n",
      " [-0.08069171]\n",
      " [ 0.01503288]\n",
      " [-0.04925801]]\n",
      "t [[ 0.00496029]\n",
      " [-0.03654686]\n",
      " [-0.10559622]\n",
      " ...\n",
      " [-0.08069171]\n",
      " [ 0.01503288]\n",
      " [-0.04925801]]\n",
      "Current iteration=2, loss=39464.718846471354\n",
      "t [[ 0.00637877]\n",
      " [-0.05639388]\n",
      " [-0.15117454]\n",
      " ...\n",
      " [-0.1150365 ]\n",
      " [ 0.02046343]\n",
      " [-0.07218613]]\n",
      "t [[ 0.00637877]\n",
      " [-0.05639388]\n",
      " [-0.15117454]\n",
      " ...\n",
      " [-0.1150365 ]\n",
      " [ 0.02046343]\n",
      " [-0.07218613]]\n",
      "t [[ 0.00722464]\n",
      " [-0.07702257]\n",
      " [-0.19271032]\n",
      " ...\n",
      " [-0.14603303]\n",
      " [ 0.02470848]\n",
      " [-0.09409669]]\n",
      "t [[ 0.00722464]\n",
      " [-0.07702257]\n",
      " [-0.19271032]\n",
      " ...\n",
      " [-0.14603303]\n",
      " [ 0.02470848]\n",
      " [-0.09409669]]\n",
      "Current iteration=4, loss=38803.097614408885\n",
      "t [[ 0.00758408]\n",
      " [-0.09826463]\n",
      " [-0.23069786]\n",
      " ...\n",
      " [-0.17409956]\n",
      " [ 0.02790169]\n",
      " [-0.11506705]]\n",
      "t [[ 0.00758408]\n",
      " [-0.09826463]\n",
      " [-0.23069786]\n",
      " ...\n",
      " [-0.17409956]\n",
      " [ 0.02790169]\n",
      " [-0.11506705]]\n",
      "t [[ 0.00753184]\n",
      " [-0.11997532]\n",
      " [-0.26556927]\n",
      " ...\n",
      " [-0.19960172]\n",
      " [ 0.03016111]\n",
      " [-0.13516691]]\n",
      "t [[ 0.00753184]\n",
      " [-0.11997532]\n",
      " [-0.26556927]\n",
      " ...\n",
      " [-0.19960172]\n",
      " [ 0.03016111]\n",
      " [-0.13516691]]\n",
      "Current iteration=6, loss=38267.482301596516\n",
      "t [[ 0.00713236]\n",
      " [-0.1420309 ]\n",
      " [-0.29770069]\n",
      " ...\n",
      " [-0.22285773]\n",
      " [ 0.03159051]\n",
      " [-0.15445886]]\n",
      "t [[ 0.00713236]\n",
      " [-0.1420309 ]\n",
      " [-0.29770069]\n",
      " ...\n",
      " [-0.22285773]\n",
      " [ 0.03159051]\n",
      " [-0.15445886]]\n",
      "t [[ 0.006441  ]\n",
      " [-0.16432601]\n",
      " [-0.32741882]\n",
      " ...\n",
      " [-0.24414399]\n",
      " [ 0.0322809 ]\n",
      " [-0.17299905]]\n",
      "t [[ 0.006441  ]\n",
      " [-0.16432601]\n",
      " [-0.32741882]\n",
      " ...\n",
      " [-0.24414399]\n",
      " [ 0.0322809 ]\n",
      " [-0.17299905]]\n",
      "Current iteration=8, loss=37820.553595786994\n",
      "t [[ 0.00550527]\n",
      " [-0.1867712 ]\n",
      " [-0.35500729]\n",
      " ...\n",
      " [-0.26370039]\n",
      " [ 0.03231213]\n",
      " [-0.1908379 ]]\n",
      "t [[ 0.00550527]\n",
      " [-0.1867712 ]\n",
      " [-0.35500729]\n",
      " ...\n",
      " [-0.26370039]\n",
      " [ 0.03231213]\n",
      " [-0.1908379 ]]\n",
      "t [[ 0.00436587]\n",
      " [-0.20929067]\n",
      " [-0.38071244]\n",
      " ...\n",
      " [-0.28173521]\n",
      " [ 0.03175431]\n",
      " [-0.2080208 ]]\n",
      "loss=37438.63263805534\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00245202]\n",
      " [-0.01551302]\n",
      " [-0.05601449]\n",
      " ...\n",
      " [-0.04181263]\n",
      " [ 0.00888854]\n",
      " [-0.02366302]]\n",
      "t [[ 0.00245202]\n",
      " [-0.01551302]\n",
      " [-0.05601449]\n",
      " ...\n",
      " [-0.04181263]\n",
      " [ 0.00888854]\n",
      " [-0.02366302]]\n",
      "t [[ 0.00413032]\n",
      " [-0.03227742]\n",
      " [-0.10680736]\n",
      " ...\n",
      " [-0.0793288 ]\n",
      " [ 0.01626149]\n",
      " [-0.04617029]]\n",
      "t [[ 0.00413032]\n",
      " [-0.03227742]\n",
      " [-0.10680736]\n",
      " ...\n",
      " [-0.0793288 ]\n",
      " [ 0.01626149]\n",
      " [-0.04617029]]\n",
      "Current iteration=2, loss=39473.726274351735\n",
      "t [[ 0.00514599]\n",
      " [-0.05007212]\n",
      " [-0.15300141]\n",
      " ...\n",
      " [-0.1130728 ]\n",
      " [ 0.0222838 ]\n",
      " [-0.06761299]]\n",
      "t [[ 0.00514599]\n",
      " [-0.05007212]\n",
      " [-0.15300141]\n",
      " ...\n",
      " [-0.1130728 ]\n",
      " [ 0.0222838 ]\n",
      " [-0.06761299]]\n",
      "t [[ 0.0055979 ]\n",
      " [-0.06870233]\n",
      " [-0.19515264]\n",
      " ...\n",
      " [-0.14351234]\n",
      " [ 0.0271049 ]\n",
      " [-0.08807471]]\n",
      "t [[ 0.0055979 ]\n",
      " [-0.06870233]\n",
      " [-0.19515264]\n",
      " ...\n",
      " [-0.14351234]\n",
      " [ 0.0271049 ]\n",
      " [-0.08807471]]\n",
      "Current iteration=4, loss=38819.73982998158\n",
      "t [[ 0.00557263]\n",
      " [-0.08799865]\n",
      " [-0.23375133]\n",
      " ...\n",
      " [-0.17105969]\n",
      " [ 0.03085808]\n",
      " [-0.10763123]]\n",
      "t [[ 0.00557263]\n",
      " [-0.08799865]\n",
      " [-0.23375133]\n",
      " ...\n",
      " [-0.17105969]\n",
      " [ 0.03085808]\n",
      " [-0.10763123]]\n",
      "t [[ 0.00514529]\n",
      " [-0.10781506]\n",
      " [-0.26922654]\n",
      " ...\n",
      " [-0.19607546]\n",
      " [ 0.03366121]\n",
      " [-0.12635075]]\n",
      "t [[ 0.00514529]\n",
      " [-0.10781506]\n",
      " [-0.26922654]\n",
      " ...\n",
      " [-0.19607546]\n",
      " [ 0.03366121]\n",
      " [-0.12635075]]\n",
      "Current iteration=6, loss=38290.92583290898\n",
      "t [[ 0.00438058]\n",
      " [-0.12802636]\n",
      " [-0.30195209]\n",
      " ...\n",
      " [-0.21887376]\n",
      " [ 0.03561796]\n",
      " [-0.14429446]]\n",
      "t [[ 0.00438058]\n",
      " [-0.12802636]\n",
      " [-0.30195209]\n",
      " ...\n",
      " [-0.21887376]\n",
      " [ 0.03561796]\n",
      " [-0.14429446]]\n",
      "t [[ 0.00333409]\n",
      " [-0.14852566]\n",
      " [-0.33225304]\n",
      " ...\n",
      " [-0.23972759]\n",
      " [ 0.0368194 ]\n",
      " [-0.16151724]]\n",
      "t [[ 0.00333409]\n",
      " [-0.14852566]\n",
      " [-0.33225304]\n",
      " ...\n",
      " [-0.23972759]\n",
      " [ 0.0368194 ]\n",
      " [-0.16151724]]\n",
      "Current iteration=8, loss=37850.215465989444\n",
      "t [[ 0.00205347]\n",
      " [-0.16922191]\n",
      " [-0.36041185]\n",
      " ...\n",
      " [-0.25887401]\n",
      " [ 0.0373455 ]\n",
      " [-0.17806832]]\n",
      "t [[ 0.00205347]\n",
      " [-0.16922191]\n",
      " [-0.36041185]\n",
      " ...\n",
      " [-0.25887401]\n",
      " [ 0.0373455 ]\n",
      " [-0.17806832]]\n",
      "t [[ 0.00057953]\n",
      " [-0.19003771]\n",
      " [-0.38667406]\n",
      " ...\n",
      " [-0.27651896]\n",
      " [ 0.03726658]\n",
      " [-0.19399193]]\n",
      "loss=37474.044544807824\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00204204]\n",
      " [-0.01643097]\n",
      " [-0.05564489]\n",
      " ...\n",
      " [-0.02449902]\n",
      " [-0.0029506 ]\n",
      " [-0.04998196]]\n",
      "t [[ 0.00204204]\n",
      " [-0.01643097]\n",
      " [-0.05564489]\n",
      " ...\n",
      " [-0.02449902]\n",
      " [-0.0029506 ]\n",
      " [-0.04998196]]\n",
      "t [[ 0.00332564]\n",
      " [-0.03410476]\n",
      " [-0.10602466]\n",
      " ...\n",
      " [-0.04562363]\n",
      " [-0.00478543]\n",
      " [-0.09799978]]\n",
      "t [[ 0.00332564]\n",
      " [-0.03410476]\n",
      " [-0.10602466]\n",
      " ...\n",
      " [-0.04562363]\n",
      " [-0.00478543]\n",
      " [-0.09799978]]\n",
      "Current iteration=2, loss=39458.96531457224\n",
      "t [[ 0.00396141]\n",
      " [-0.05279887]\n",
      " [-0.15177033]\n",
      " ...\n",
      " [-0.06381751]\n",
      " [-0.00568439]\n",
      " [-0.14418784]]\n",
      "t [[ 0.00396141]\n",
      " [-0.05279887]\n",
      " [-0.15177033]\n",
      " ...\n",
      " [-0.06381751]\n",
      " [-0.00568439]\n",
      " [-0.14418784]]\n",
      "t [[ 0.00404767]\n",
      " [-0.0723173 ]\n",
      " [-0.19344516]\n",
      " ...\n",
      " [-0.07947464]\n",
      " [-0.00580503]\n",
      " [-0.18867087]]\n",
      "t [[ 0.00404767]\n",
      " [-0.0723173 ]\n",
      " [-0.19344516]\n",
      " ...\n",
      " [-0.07947464]\n",
      " [-0.00580503]\n",
      " [-0.18867087]]\n",
      "Current iteration=4, loss=38793.824356140125\n",
      "t [[ 0.00367042]\n",
      " [-0.09248967]\n",
      " [-0.23154565]\n",
      " ...\n",
      " [-0.09294057]\n",
      " [-0.0052838 ]\n",
      " [-0.23156359]]\n",
      "t [[ 0.00367042]\n",
      " [-0.09248967]\n",
      " [-0.23154565]\n",
      " ...\n",
      " [-0.09294057]\n",
      " [-0.0052838 ]\n",
      " [-0.23156359]]\n",
      "t [[ 0.0029042 ]\n",
      " [-0.11316923]\n",
      " [-0.26650611]\n",
      " ...\n",
      " [-0.10451612]\n",
      " [-0.00423796]\n",
      " [-0.27297082]]\n",
      "t [[ 0.0029042 ]\n",
      " [-0.11316923]\n",
      " [-0.26650611]\n",
      " ...\n",
      " [-0.10451612]\n",
      " [-0.00423796]\n",
      " [-0.27297082]]\n",
      "Current iteration=6, loss=38256.11314565181\n",
      "t [[ 0.00181313]\n",
      " [-0.13423029]\n",
      " [-0.29870478]\n",
      " ...\n",
      " [-0.11446189]\n",
      " [-0.00276778]\n",
      " [-0.3129881 ]]\n",
      "t [[ 0.00181313]\n",
      " [-0.13423029]\n",
      " [-0.29870478]\n",
      " ...\n",
      " [-0.11446189]\n",
      " [-0.00276778]\n",
      " [-0.3129881 ]]\n",
      "t [[ 0.00045225]\n",
      " [-0.15556565]\n",
      " [-0.3284704 ]\n",
      " ...\n",
      " [-0.12300319]\n",
      " [-0.00095867]\n",
      " [-0.35170238]]\n",
      "t [[ 0.00045225]\n",
      " [-0.15556565]\n",
      " [-0.3284704 ]\n",
      " ...\n",
      " [-0.12300319]\n",
      " [-0.00095867]\n",
      " [-0.35170238]]\n",
      "Current iteration=8, loss=37807.977868169175\n",
      "t [[-0.00113132]\n",
      " [-0.1770841 ]\n",
      " [-0.35608849]\n",
      " ...\n",
      " [-0.13033452]\n",
      " [ 0.00111674]\n",
      " [-0.38919282]]\n",
      "t [[-0.00113132]\n",
      " [-0.1770841 ]\n",
      " [-0.35608849]\n",
      " ...\n",
      " [-0.13033452]\n",
      " [ 0.00111674]\n",
      " [-0.38919282]]\n",
      "t [[-0.0028973 ]\n",
      " [-0.19870819]\n",
      " [-0.38180713]\n",
      " ...\n",
      " [-0.1366238 ]\n",
      " [ 0.00339687]\n",
      " [-0.42553155]]\n",
      "loss=37425.3955015285\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.02449091]\n",
      " [-0.10462238]\n",
      " [-0.06575893]\n",
      " ...\n",
      " [-0.04765892]\n",
      " [ 0.01031842]\n",
      " [-0.02778265]]\n",
      "t [[ 0.02449091]\n",
      " [-0.10462238]\n",
      " [-0.06575893]\n",
      " ...\n",
      " [-0.04765892]\n",
      " [ 0.01031842]\n",
      " [-0.02778265]]\n",
      "t [[ 0.04612375]\n",
      " [-0.19943102]\n",
      " [-0.12506664]\n",
      " ...\n",
      " [-0.0899226 ]\n",
      " [ 0.01869208]\n",
      " [-0.05408549]]\n",
      "t [[ 0.04612375]\n",
      " [-0.19943102]\n",
      " [-0.12506664]\n",
      " ...\n",
      " [-0.0899226 ]\n",
      " [ 0.01869208]\n",
      " [-0.05408549]]\n",
      "Current iteration=2, loss=39380.98058438813\n",
      "t [[ 0.06524919]\n",
      " [-0.28563002]\n",
      " [-0.17876016]\n",
      " ...\n",
      " [-0.1275271 ]\n",
      " [ 0.02535645]\n",
      " [-0.07903735]]\n",
      "t [[ 0.06524919]\n",
      " [-0.28563002]\n",
      " [-0.17876016]\n",
      " ...\n",
      " [-0.1275271 ]\n",
      " [ 0.02535645]\n",
      " [-0.07903735]]\n",
      "t [[ 0.08217886]\n",
      " [-0.36428398]\n",
      " [-0.2275759 ]\n",
      " ...\n",
      " [-0.16111799]\n",
      " [ 0.03052186]\n",
      " [-0.10275486]]\n",
      "t [[ 0.08217886]\n",
      " [-0.36428398]\n",
      " [-0.2275759 ]\n",
      " ...\n",
      " [-0.16111799]\n",
      " [ 0.03052186]\n",
      " [-0.10275486]]\n",
      "Current iteration=4, loss=38674.06933617782\n",
      "t [[ 0.09718596]\n",
      " [-0.43632056]\n",
      " [-0.27215225]\n",
      " ...\n",
      " [-0.1912535 ]\n",
      " [ 0.03437286]\n",
      " [-0.12534221]]\n",
      "t [[ 0.09718596]\n",
      " [-0.43632056]\n",
      " [-0.27215225]\n",
      " ...\n",
      " [-0.1912535 ]\n",
      " [ 0.03437286]\n",
      " [-0.12534221]]\n",
      "t [[ 0.11050821]\n",
      " [-0.5025417 ]\n",
      " [-0.3130381 ]\n",
      " ...\n",
      " [-0.21841259]\n",
      " [ 0.0370698 ]\n",
      " [-0.14689191]]\n",
      "t [[ 0.11050821]\n",
      " [-0.5025417 ]\n",
      " [-0.3130381 ]\n",
      " ...\n",
      " [-0.21841259]\n",
      " [ 0.0370698 ]\n",
      " [-0.14689191]]\n",
      "Current iteration=6, loss=38112.942352189566\n",
      "t [[ 0.12235172]\n",
      " [-0.56363808]\n",
      " [-0.35070344]\n",
      " ...\n",
      " [-0.24300443]\n",
      " [ 0.03875147]\n",
      " [-0.16748587]]\n",
      "t [[ 0.12235172]\n",
      " [-0.56363808]\n",
      " [-0.35070344]\n",
      " ...\n",
      " [-0.24300443]\n",
      " [ 0.03875147]\n",
      " [-0.16748587]]\n",
      "t [[ 0.13289498]\n",
      " [-0.62020403]\n",
      " [-0.38555028]\n",
      " ...\n",
      " [-0.26537813]\n",
      " [ 0.03953787]\n",
      " [-0.18719662]]\n",
      "t [[ 0.13289498]\n",
      " [-0.62020403]\n",
      " [-0.38555028]\n",
      " ...\n",
      " [-0.26537813]\n",
      " [ 0.03953787]\n",
      " [-0.18719662]]\n",
      "Current iteration=8, loss=37651.197232688355\n",
      "t [[ 0.14229261]\n",
      " [-0.67275143]\n",
      " [-0.41792258]\n",
      " ...\n",
      " [-0.28583151]\n",
      " [ 0.0395329 ]\n",
      " [-0.20608858]]\n",
      "t [[ 0.14229261]\n",
      " [-0.67275143]\n",
      " [-0.41792258]\n",
      " ...\n",
      " [-0.28583151]\n",
      " [ 0.0395329 ]\n",
      " [-0.20608858]]\n",
      "t [[ 0.15067874]\n",
      " [-0.7217219 ]\n",
      " [-0.44811507]\n",
      " ...\n",
      " [-0.30461885]\n",
      " [ 0.03882676]\n",
      " [-0.22421906]]\n",
      "loss=37260.710441196694\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00322971]\n",
      " [-0.01988487]\n",
      " [-0.06234113]\n",
      " ...\n",
      " [-0.04783923]\n",
      " [ 0.00930027]\n",
      " [-0.02838035]]\n",
      "t [[ 0.00322971]\n",
      " [-0.01988487]\n",
      " [-0.06234113]\n",
      " ...\n",
      " [-0.04783923]\n",
      " [ 0.00930027]\n",
      " [-0.02838035]]\n",
      "t [[ 0.00547072]\n",
      " [-0.04128301]\n",
      " [-0.11806125]\n",
      " ...\n",
      " [-0.09016671]\n",
      " [ 0.01670138]\n",
      " [-0.0552473 ]]\n",
      "t [[ 0.00547072]\n",
      " [-0.04128301]\n",
      " [-0.11806125]\n",
      " ...\n",
      " [-0.09016671]\n",
      " [ 0.01670138]\n",
      " [-0.0552473 ]]\n",
      "Current iteration=2, loss=39370.16284337966\n",
      "t [[ 0.00688072]\n",
      " [-0.06388004]\n",
      " [-0.16805789]\n",
      " ...\n",
      " [-0.12774257]\n",
      " [ 0.02243974]\n",
      " [-0.08073374]]\n",
      "t [[ 0.00688072]\n",
      " [-0.06388004]\n",
      " [-0.16805789]\n",
      " ...\n",
      " [-0.12774257]\n",
      " [ 0.02243974]\n",
      " [-0.08073374]]\n",
      "t [[ 0.00759746]\n",
      " [-0.08740488]\n",
      " [-0.21311722]\n",
      " ...\n",
      " [-0.16123222]\n",
      " [ 0.02672586]\n",
      " [-0.10495979]]\n",
      "t [[ 0.00759746]\n",
      " [-0.08740488]\n",
      " [-0.21311722]\n",
      " ...\n",
      " [-0.16123222]\n",
      " [ 0.02672586]\n",
      " [-0.10495979]]\n",
      "Current iteration=4, loss=38654.94254730242\n",
      "t [[ 0.00773898]\n",
      " [-0.11162751]\n",
      " [-0.25391798]\n",
      " ...\n",
      " [-0.19121001]\n",
      " [ 0.02974379]\n",
      " [-0.12803261]]\n",
      "t [[ 0.00773898]\n",
      " [-0.11162751]\n",
      " [-0.25391798]\n",
      " ...\n",
      " [-0.19121001]\n",
      " [ 0.02974379]\n",
      " [-0.12803261]]\n",
      "t [[ 0.00740542]\n",
      " [-0.1363547 ]\n",
      " [-0.29104134]\n",
      " ...\n",
      " [-0.21816767]\n",
      " [ 0.03165291]\n",
      " [-0.15004717]]\n",
      "t [[ 0.00740542]\n",
      " [-0.1363547 ]\n",
      " [-0.29104134]\n",
      " ...\n",
      " [-0.21816767]\n",
      " [ 0.03165291]\n",
      " [-0.15004717]]\n",
      "Current iteration=6, loss=38086.65164136216\n",
      "t [[ 0.00668117]\n",
      " [-0.16142518]\n",
      " [-0.32498299]\n",
      " ...\n",
      " [-0.24252461]\n",
      " [ 0.03259065]\n",
      " [-0.17108745]]\n",
      "t [[ 0.00668117]\n",
      " [-0.16142518]\n",
      " [-0.32498299]\n",
      " ...\n",
      " [-0.24252461]\n",
      " [ 0.03259065]\n",
      " [-0.17108745]]\n",
      "t [[ 0.00563715]\n",
      " [-0.18670483]\n",
      " [-0.35616511]\n",
      " ...\n",
      " [-0.26463796]\n",
      " [ 0.03267541]\n",
      " [-0.19122773]]\n",
      "t [[ 0.00563715]\n",
      " [-0.18670483]\n",
      " [-0.35616511]\n",
      " ...\n",
      " [-0.26463796]\n",
      " [ 0.03267541]\n",
      " [-0.19122773]]\n",
      "Current iteration=8, loss=37618.38620860166\n",
      "t [[ 0.00433293]\n",
      " [-0.2120825 ]\n",
      " [-0.38494742]\n",
      " ...\n",
      " [-0.2848119 ]\n",
      " [ 0.03200937]\n",
      " [-0.21053389]]\n",
      "t [[ 0.00433293]\n",
      " [-0.2120825 ]\n",
      " [-0.38494742]\n",
      " ...\n",
      " [-0.2848119 ]\n",
      " [ 0.03200937]\n",
      " [-0.21053389]]\n",
      "t [[ 0.00281854]\n",
      " [-0.23746623]\n",
      " [-0.41163676]\n",
      " ...\n",
      " [-0.30330576]\n",
      " [ 0.03068093]\n",
      " [-0.22906449]]\n",
      "loss=37221.8624120399\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00275853]\n",
      " [-0.01745215]\n",
      " [-0.0630163 ]\n",
      " ...\n",
      " [-0.04703921]\n",
      " [ 0.00999961]\n",
      " [-0.02662089]]\n",
      "t [[ 0.00275853]\n",
      " [-0.01745215]\n",
      " [-0.0630163 ]\n",
      " ...\n",
      " [-0.04703921]\n",
      " [ 0.00999961]\n",
      " [-0.02662089]]\n",
      "t [[ 0.00453808]\n",
      " [-0.03648769]\n",
      " [-0.11942527]\n",
      " ...\n",
      " [-0.08864175]\n",
      " [ 0.01808149]\n",
      " [-0.05177928]]\n",
      "t [[ 0.00453808]\n",
      " [-0.03648769]\n",
      " [-0.11942527]\n",
      " ...\n",
      " [-0.08864175]\n",
      " [ 0.01808149]\n",
      " [-0.05177928]]\n",
      "Current iteration=2, loss=39380.20805592604\n",
      "t [[ 0.00549726]\n",
      " [-0.05679126]\n",
      " [-0.17011575]\n",
      " ...\n",
      " [-0.12555583]\n",
      " [ 0.02448125]\n",
      " [-0.07560535]]\n",
      "t [[ 0.00549726]\n",
      " [-0.05679126]\n",
      " [-0.17011575]\n",
      " ...\n",
      " [-0.12555583]\n",
      " [ 0.02448125]\n",
      " [-0.07560535]]\n",
      "t [[ 0.0057745 ]\n",
      " [-0.0780905 ]\n",
      " [-0.21586696]\n",
      " ...\n",
      " [-0.15843687]\n",
      " [ 0.0294087 ]\n",
      " [-0.09821678]]\n",
      "t [[ 0.0057745 ]\n",
      " [-0.0780905 ]\n",
      " [-0.21586696]\n",
      " ...\n",
      " [-0.15843687]\n",
      " [ 0.0294087 ]\n",
      " [-0.09821678]]\n",
      "Current iteration=4, loss=38673.377903873625\n",
      "t [[ 0.00548844]\n",
      " [-0.10015372]\n",
      " [-0.25735234]\n",
      " ...\n",
      " [-0.18785109]\n",
      " [ 0.03304747]\n",
      " [-0.11971848]]\n",
      "t [[ 0.00548844]\n",
      " [-0.10015372]\n",
      " [-0.25735234]\n",
      " ...\n",
      " [-0.18785109]\n",
      " [ 0.03304747]\n",
      " [-0.11971848]]\n",
      "t [[ 0.00473964]\n",
      " [-0.12278573]\n",
      " [-0.29514921]\n",
      " ...\n",
      " [-0.21428368]\n",
      " [ 0.03555672]\n",
      " [-0.14020337]]\n",
      "t [[ 0.00473964]\n",
      " [-0.12278573]\n",
      " [-0.29514921]\n",
      " ...\n",
      " [-0.21428368]\n",
      " [ 0.03555672]\n",
      " [-0.14020337]]\n",
      "Current iteration=6, loss=38112.51705563225\n",
      "t [[ 0.00361284]\n",
      " [-0.14582309]\n",
      " [-0.32975053]\n",
      " ...\n",
      " [-0.23814877]\n",
      " [ 0.03707389]\n",
      " [-0.15975352]]\n",
      "t [[ 0.00361284]\n",
      " [-0.14582309]\n",
      " [-0.32975053]\n",
      " ...\n",
      " [-0.23814877]\n",
      " [ 0.03707389]\n",
      " [-0.15975352]]\n",
      "t [[ 0.00217922]\n",
      " [-0.16912943]\n",
      " [-0.36157665]\n",
      " ...\n",
      " [-0.25979925]\n",
      " [ 0.03771754]\n",
      " [-0.17844148]]\n",
      "t [[ 0.00217922]\n",
      " [-0.16912943]\n",
      " [-0.36157665]\n",
      " ...\n",
      " [-0.25979925]\n",
      " [ 0.03771754]\n",
      " [-0.17844148]]\n",
      "Current iteration=8, loss=37651.01001820724\n",
      "t [[ 0.00049853]\n",
      " [-0.19259128]\n",
      " [-0.39098609]\n",
      " ...\n",
      " [-0.27953587]\n",
      " [ 0.03759011]\n",
      " [-0.19633148]]\n",
      "t [[ 0.00049853]\n",
      " [-0.19259128]\n",
      " [-0.39098609]\n",
      " ...\n",
      " [-0.27953587]\n",
      " [ 0.03759011]\n",
      " [-0.19633148]]\n",
      "t [[-0.00137912]\n",
      " [-0.21611442]\n",
      " [-0.41828502]\n",
      " ...\n",
      " [-0.29761518]\n",
      " [ 0.03678036]\n",
      " [-0.2134806 ]]\n",
      "loss=37260.69319535321\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0022973 ]\n",
      " [-0.01848484]\n",
      " [-0.0626005 ]\n",
      " ...\n",
      " [-0.02756139]\n",
      " [-0.00331942]\n",
      " [-0.0562297 ]]\n",
      "t [[ 0.0022973 ]\n",
      " [-0.01848484]\n",
      " [-0.0626005 ]\n",
      " ...\n",
      " [-0.02756139]\n",
      " [-0.00331942]\n",
      " [-0.0562297 ]]\n",
      "t [[ 0.00363496]\n",
      " [-0.03854222]\n",
      " [-0.11853864]\n",
      " ...\n",
      " [-0.0508529 ]\n",
      " [-0.00522697]\n",
      " [-0.10997387]]\n",
      "t [[ 0.00363496]\n",
      " [-0.03854222]\n",
      " [-0.11853864]\n",
      " ...\n",
      " [-0.0508529 ]\n",
      " [-0.00522697]\n",
      " [-0.10997387]]\n",
      "Current iteration=2, loss=39363.828560382724\n",
      "t [[ 0.00417088]\n",
      " [-0.05985476]\n",
      " [-0.16871502]\n",
      " ...\n",
      " [-0.07050756]\n",
      " [-0.00597911]\n",
      " [-0.16142447]]\n",
      "t [[ 0.00417088]\n",
      " [-0.05985476]\n",
      " [-0.16871502]\n",
      " ...\n",
      " [-0.07050756]\n",
      " [-0.00597911]\n",
      " [-0.16142447]]\n",
      "t [[ 0.00404272]\n",
      " [-0.08214838]\n",
      " [-0.21391899]\n",
      " ...\n",
      " [-0.08707704]\n",
      " [-0.00579602]\n",
      " [-0.21075751]]\n",
      "t [[ 0.00404272]\n",
      " [-0.08214838]\n",
      " [-0.21391899]\n",
      " ...\n",
      " [-0.08707704]\n",
      " [-0.00579602]\n",
      " [-0.21075751]]\n",
      "Current iteration=4, loss=38644.99015513612\n",
      "t [[ 0.00336828]\n",
      " [-0.10519013]\n",
      " [-0.25483243]\n",
      " ...\n",
      " [-0.10103526]\n",
      " [-0.00486415]\n",
      " [-0.25813255]]\n",
      "t [[ 0.00336828]\n",
      " [-0.10519013]\n",
      " [-0.25483243]\n",
      " ...\n",
      " [-0.10103526]\n",
      " [-0.00486415]\n",
      " [-0.25813255]]\n",
      "t [[ 0.0022473 ]\n",
      " [-0.12878395]\n",
      " [-0.29203964]\n",
      " ...\n",
      " [-0.11278603]\n",
      " [-0.00334006]\n",
      " [-0.30369341]]\n",
      "t [[ 0.0022473 ]\n",
      " [-0.12878395]\n",
      " [-0.29203964]\n",
      " ...\n",
      " [-0.11278603]\n",
      " [-0.00334006]\n",
      " [-0.30369341]]\n",
      "Current iteration=6, loss=38074.70619013268\n",
      "t [[ 0.00076373]\n",
      " [-0.15276586]\n",
      " [-0.32603924]\n",
      " ...\n",
      " [-0.12267193]\n",
      " [-0.00135454]\n",
      " [-0.34756936]]\n",
      "t [[ 0.00076373]\n",
      " [-0.15276586]\n",
      " [-0.32603924]\n",
      " ...\n",
      " [-0.12267193]\n",
      " [-0.00135454]\n",
      " [-0.34756936]]\n",
      "t [[-0.00101203]\n",
      " [-0.17699921]\n",
      " [-0.35725619]\n",
      " ...\n",
      " [-0.13098302]\n",
      " [ 0.00098359]\n",
      " [-0.38987657]]\n",
      "t [[-0.00101203]\n",
      " [-0.17699921]\n",
      " [-0.35725619]\n",
      " ...\n",
      " [-0.13098302]\n",
      " [ 0.00098359]\n",
      " [-0.38987657]]\n",
      "Current iteration=8, loss=37605.395371701365\n",
      "t [[-0.00302098]\n",
      " [-0.20137043]\n",
      " [-0.38605274]\n",
      " ...\n",
      " [-0.13796481]\n",
      " [ 0.00358398]\n",
      " [-0.43071957]]\n",
      "t [[-0.00302098]\n",
      " [-0.20137043]\n",
      " [-0.38605274]\n",
      " ...\n",
      " [-0.13796481]\n",
      " [ 0.00358398]\n",
      " [-0.43071957]]\n",
      "t [[-0.00521372]\n",
      " [-0.22578534]\n",
      " [-0.41273804]\n",
      " ...\n",
      " [-0.14382514]\n",
      " [ 0.0063718 ]\n",
      " [-0.47019256]]\n",
      "loss=37208.3653508298\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.02721213]\n",
      " [-0.11624708]\n",
      " [-0.07306548]\n",
      " ...\n",
      " [-0.05295436]\n",
      " [ 0.01146491]\n",
      " [-0.03086961]]\n",
      "t [[ 0.02721213]\n",
      " [-0.11624708]\n",
      " [-0.07306548]\n",
      " ...\n",
      " [-0.05295436]\n",
      " [ 0.01146491]\n",
      " [-0.03086961]]\n",
      "t [[ 0.05089649]\n",
      " [-0.22038104]\n",
      " [-0.13816824]\n",
      " ...\n",
      " [-0.09924934]\n",
      " [ 0.0205295 ]\n",
      " [-0.05991259]]\n",
      "t [[ 0.05089649]\n",
      " [-0.22038104]\n",
      " [-0.13816824]\n",
      " ...\n",
      " [-0.09924934]\n",
      " [ 0.0205295 ]\n",
      " [-0.05991259]]\n",
      "Current iteration=2, loss=39289.772999139335\n",
      "t [[ 0.07153528]\n",
      " [-0.31405747]\n",
      " [-0.19645901]\n",
      " ...\n",
      " [-0.13989669]\n",
      " [ 0.02751773]\n",
      " [-0.08730617]]\n",
      "t [[ 0.07153528]\n",
      " [-0.31405747]\n",
      " [-0.19645901]\n",
      " ...\n",
      " [-0.13989669]\n",
      " [ 0.02751773]\n",
      " [-0.08730617]]\n",
      "t [[ 0.08954968]\n",
      " [-0.39871405]\n",
      " [-0.24893157]\n",
      " ...\n",
      " [-0.17576726]\n",
      " [ 0.0327139 ]\n",
      " [-0.11320836]]\n",
      "t [[ 0.08954968]\n",
      " [-0.39871405]\n",
      " [-0.24893157]\n",
      " ...\n",
      " [-0.17576726]\n",
      " [ 0.0327139 ]\n",
      " [-0.11320836]]\n",
      "Current iteration=4, loss=38534.509897244556\n",
      "t [[ 0.10530207]\n",
      " [-0.47557901]\n",
      " [-0.29642999]\n",
      " ...\n",
      " [-0.20759876]\n",
      " [ 0.03636263]\n",
      " [-0.13775809]]\n",
      "t [[ 0.10530207]\n",
      " [-0.47557901]\n",
      " [-0.29642999]\n",
      " ...\n",
      " [-0.20759876]\n",
      " [ 0.03636263]\n",
      " [-0.13775809]]\n",
      "t [[ 0.11910189]\n",
      " [-0.54569335]\n",
      " [-0.33966521]\n",
      " ...\n",
      " [-0.2360111 ]\n",
      " [ 0.03867241]\n",
      " [-0.16107668]]\n",
      "t [[ 0.11910189]\n",
      " [-0.54569335]\n",
      " [-0.33966521]\n",
      " ...\n",
      " [-0.2360111 ]\n",
      " [ 0.03867241]\n",
      " [-0.16107668]]\n",
      "Current iteration=6, loss=37945.349925554816\n",
      "t [[ 0.13121258]\n",
      " [-0.60993679]\n",
      " [-0.3792341 ]\n",
      " ...\n",
      " [-0.26152335]\n",
      " [ 0.03982036]\n",
      " [-0.18326998]]\n",
      "t [[ 0.13121258]\n",
      " [-0.60993679]\n",
      " [-0.3792341 ]\n",
      " ...\n",
      " [-0.26152335]\n",
      " [ 0.03982036]\n",
      " [-0.18326998]]\n",
      "t [[ 0.14185835]\n",
      " [-0.66905299]\n",
      " [-0.41563767]\n",
      " ...\n",
      " [-0.28456982]\n",
      " [ 0.03995707]\n",
      " [-0.2044305 ]]\n",
      "t [[ 0.14185835]\n",
      " [-0.66905299]\n",
      " [-0.41563767]\n",
      " ...\n",
      " [-0.28456982]\n",
      " [ 0.03995707]\n",
      " [-0.2044305 ]]\n",
      "Current iteration=8, loss=37465.998550622\n",
      "t [[ 0.15123033]\n",
      " [-0.72367188]\n",
      " [-0.44929705]\n",
      " ...\n",
      " [-0.30551426]\n",
      " [ 0.03921095]\n",
      " [-0.22463934]]\n",
      "t [[ 0.15123033]\n",
      " [-0.72367188]\n",
      " [-0.44929705]\n",
      " ...\n",
      " [-0.30551426]\n",
      " [ 0.03921095]\n",
      " [-0.22463934]]\n",
      "t [[ 0.15949177]\n",
      " [-0.77432862]\n",
      " [-0.48056713]\n",
      " ...\n",
      " [-0.32466181]\n",
      " [ 0.03769196]\n",
      " [-0.24396796]]\n",
      "loss=37063.92844471734\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00358856]\n",
      " [-0.02209429]\n",
      " [-0.06926792]\n",
      " ...\n",
      " [-0.0531547 ]\n",
      " [ 0.01033364]\n",
      " [-0.03153373]]\n",
      "t [[ 0.00358856]\n",
      " [-0.02209429]\n",
      " [-0.06926792]\n",
      " ...\n",
      " [-0.0531547 ]\n",
      " [ 0.01033364]\n",
      " [-0.03153373]]\n",
      "t [[ 0.00595689]\n",
      " [-0.04605628]\n",
      " [-0.13036357]\n",
      " ...\n",
      " [-0.09950625]\n",
      " [ 0.01832325]\n",
      " [-0.06119937]]\n",
      "t [[ 0.00595689]\n",
      " [-0.04605628]\n",
      " [-0.13036357]\n",
      " ...\n",
      " [-0.09950625]\n",
      " [ 0.01832325]\n",
      " [-0.06119937]]\n",
      "Current iteration=2, loss=39277.9079713463\n",
      "t [[ 0.00732183]\n",
      " [-0.07145402]\n",
      " [-0.18452069]\n",
      " ...\n",
      " [-0.14009945]\n",
      " [ 0.02429409]\n",
      " [-0.08917972]]\n",
      "t [[ 0.00732183]\n",
      " [-0.07145402]\n",
      " [-0.18452069]\n",
      " ...\n",
      " [-0.14009945]\n",
      " [ 0.02429409]\n",
      " [-0.08917972]]\n",
      "t [[ 0.00786896]\n",
      " [-0.09792281]\n",
      " [-0.23279957]\n",
      " ...\n",
      " [-0.17583169]\n",
      " [ 0.02853055]\n",
      " [-0.11563754]]\n",
      "t [[ 0.00786896]\n",
      " [-0.09792281]\n",
      " [-0.23279957]\n",
      " ...\n",
      " [-0.17583169]\n",
      " [ 0.02853055]\n",
      " [-0.11563754]]\n",
      "Current iteration=4, loss=38513.686236373396\n",
      "t [[ 0.00775384]\n",
      " [-0.12516032]\n",
      " [-0.27609666]\n",
      " ...\n",
      " [-0.20746153]\n",
      " [ 0.03127639]\n",
      " [-0.14071559]]\n",
      "t [[ 0.00775384]\n",
      " [-0.12516032]\n",
      " [-0.27609666]\n",
      " ...\n",
      " [-0.20746153]\n",
      " [ 0.03127639]\n",
      " [-0.14071559]]\n",
      "t [[ 0.00710535]\n",
      " [-0.15291863]\n",
      " [-0.3151636 ]\n",
      " ...\n",
      " [-0.23562498]\n",
      " [ 0.0327385 ]\n",
      " [-0.16453839]]\n",
      "t [[ 0.00710535]\n",
      " [-0.15291863]\n",
      " [-0.3151636 ]\n",
      " ...\n",
      " [-0.23562498]\n",
      " [ 0.0327385 ]\n",
      " [-0.16453839]]\n",
      "Current iteration=6, loss=37916.79802846552\n",
      "t [[ 0.00602971]\n",
      " [-0.18099574]\n",
      " [-0.35062854]\n",
      " ...\n",
      " [-0.2608535 ]\n",
      " [ 0.03309194]\n",
      " [-0.18721438]]\n",
      "t [[ 0.00602971]\n",
      " [-0.18099574]\n",
      " [-0.35062854]\n",
      " ...\n",
      " [-0.2608535 ]\n",
      " [ 0.03309194]\n",
      " [-0.18721438]]\n",
      "t [[ 0.00461428]\n",
      " [-0.20922774]\n",
      " [-0.38301621]\n",
      " ...\n",
      " [-0.28359094]\n",
      " [ 0.03248498]\n",
      " [-0.2088382 ]]\n",
      "t [[ 0.00461428]\n",
      " [-0.20922774]\n",
      " [-0.38301621]\n",
      " ...\n",
      " [-0.28359094]\n",
      " [ 0.03248498]\n",
      " [-0.2088382 ]]\n",
      "Current iteration=8, loss=37430.4183023956\n",
      "t [[ 0.00293089]\n",
      " [-0.23748199]\n",
      " [-0.41276557]\n",
      " ...\n",
      " [-0.30420835]\n",
      " [ 0.03104356]\n",
      " [-0.22949276]]\n",
      "t [[ 0.00293089]\n",
      " [-0.23748199]\n",
      " [-0.41276557]\n",
      " ...\n",
      " [-0.30420835]\n",
      " [ 0.03104356]\n",
      " [-0.22949276]]\n",
      "t [[ 0.00103863]\n",
      " [-0.26565154]\n",
      " [-0.44024454]\n",
      " ...\n",
      " [-0.32301646]\n",
      " [ 0.02887513]\n",
      " [-0.249251  ]]\n",
      "loss=37021.881598420325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00306503]\n",
      " [-0.01939128]\n",
      " [-0.07001812]\n",
      " ...\n",
      " [-0.05226578]\n",
      " [ 0.01111068]\n",
      " [-0.02957877]]\n",
      "t [[ 0.00306503]\n",
      " [-0.01939128]\n",
      " [-0.07001812]\n",
      " ...\n",
      " [-0.05226578]\n",
      " [ 0.01111068]\n",
      " [-0.02957877]]\n",
      "t [[ 0.00492182]\n",
      " [-0.04073682]\n",
      " [-0.1318808 ]\n",
      " ...\n",
      " [-0.09782109]\n",
      " [ 0.0198544 ]\n",
      " [-0.05735229]]\n",
      "t [[ 0.00492182]\n",
      " [-0.04073682]\n",
      " [-0.1318808 ]\n",
      " ...\n",
      " [-0.09782109]\n",
      " [ 0.0198544 ]\n",
      " [-0.05735229]]\n",
      "Current iteration=2, loss=39288.97387865269\n",
      "t [[ 0.00578845]\n",
      " [-0.06360338]\n",
      " [-0.18680985]\n",
      " ...\n",
      " [-0.1376943 ]\n",
      " [ 0.02655527]\n",
      " [-0.08349964]]\n",
      "t [[ 0.00578845]\n",
      " [-0.06360338]\n",
      " [-0.18680985]\n",
      " ...\n",
      " [-0.1376943 ]\n",
      " [ 0.02655527]\n",
      " [-0.08349964]]\n",
      "t [[ 0.00585144]\n",
      " [-0.08762437]\n",
      " [-0.23585634]\n",
      " ...\n",
      " [-0.17276946]\n",
      " [ 0.0314968 ]\n",
      " [-0.10818023]]\n",
      "t [[ 0.00585144]\n",
      " [-0.08762437]\n",
      " [-0.23585634]\n",
      " ...\n",
      " [-0.17276946]\n",
      " [ 0.0314968 ]\n",
      " [-0.10818023]]\n",
      "Current iteration=4, loss=38533.86985770139\n",
      "t [[ 0.00526708]\n",
      " [-0.11249504]\n",
      " [-0.27991002]\n",
      " ...\n",
      " [-0.20379457]\n",
      " [ 0.03492223]\n",
      " [-0.13153383]]\n",
      "t [[ 0.00526708]\n",
      " [-0.11249504]\n",
      " [-0.27991002]\n",
      " ...\n",
      " [-0.20379457]\n",
      " [ 0.03492223]\n",
      " [-0.13153383]]\n",
      "t [[ 0.00416482]\n",
      " [-0.13796464]\n",
      " [-0.31971789]\n",
      " ...\n",
      " [-0.23139739]\n",
      " [ 0.0370383 ]\n",
      " [-0.1536822 ]]\n",
      "t [[ 0.00416482]\n",
      " [-0.13796464]\n",
      " [-0.31971789]\n",
      " ...\n",
      " [-0.23139739]\n",
      " [ 0.0370383 ]\n",
      " [-0.1536822 ]]\n",
      "Current iteration=6, loss=37945.01348956119\n",
      "t [[ 0.0026513 ]\n",
      " [-0.16382811]\n",
      " [-0.35590503]\n",
      " ...\n",
      " [-0.25610291]\n",
      " [ 0.0380202 ]\n",
      " [-0.17473127]]\n",
      "t [[ 0.0026513 ]\n",
      " [-0.16382811]\n",
      " [-0.35590503]\n",
      " ...\n",
      " [-0.25610291]\n",
      " [ 0.0380202 ]\n",
      " [-0.17473127]]\n",
      "t [[ 0.00081415]\n",
      " [-0.18991834]\n",
      " [-0.38899428]\n",
      " ...\n",
      " [-0.27834989]\n",
      " [ 0.03801651]\n",
      " [-0.1947734 ]]\n",
      "t [[ 0.00081415]\n",
      " [-0.18991834]\n",
      " [-0.38899428]\n",
      " ...\n",
      " [-0.27834989]\n",
      " [ 0.03801651]\n",
      " [-0.1947734 ]]\n",
      "Current iteration=8, loss=37465.89905137279\n",
      "t [[-0.00127462]\n",
      " [-0.21609957]\n",
      " [-0.41942352]\n",
      " ...\n",
      " [-0.29850535]\n",
      " [ 0.03715365]\n",
      " [-0.21388939]]\n",
      "t [[-0.00127462]\n",
      " [-0.21609957]\n",
      " [-0.41942352]\n",
      " ...\n",
      " [-0.29850535]\n",
      " [ 0.03715365]\n",
      " [-0.21388939]]\n",
      "t [[-0.00355585]\n",
      " [-0.24226174]\n",
      " [-0.4475602 ]\n",
      " ...\n",
      " [-0.31687684]\n",
      " [ 0.03553962]\n",
      " [-0.23215025]]\n",
      "loss=37063.98396328325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00255255]\n",
      " [-0.02053871]\n",
      " [-0.06955611]\n",
      " ...\n",
      " [-0.03062377]\n",
      " [-0.00368825]\n",
      " [-0.06247745]]\n",
      "t [[ 0.00255255]\n",
      " [-0.02053871]\n",
      " [-0.06955611]\n",
      " ...\n",
      " [-0.03062377]\n",
      " [-0.00368825]\n",
      " [-0.06247745]]\n",
      "t [[ 0.00392073]\n",
      " [-0.04301827]\n",
      " [-0.1308889 ]\n",
      " ...\n",
      " [-0.05597724]\n",
      " [-0.00563379]\n",
      " [-0.12188679]]\n",
      "t [[ 0.00392073]\n",
      " [-0.04301827]\n",
      " [-0.1308889 ]\n",
      " ...\n",
      " [-0.05597724]\n",
      " [-0.00563379]\n",
      " [-0.12188679]]\n",
      "Current iteration=2, loss=39271.020875209695\n",
      "t [[ 0.00432165]\n",
      " [-0.06700261]\n",
      " [-0.18523634]\n",
      " ...\n",
      " [-0.07693039]\n",
      " [-0.00618888]\n",
      " [-0.17849217]]\n",
      "t [[ 0.00432165]\n",
      " [-0.06700261]\n",
      " [-0.18523634]\n",
      " ...\n",
      " [-0.07693039]\n",
      " [-0.00618888]\n",
      " [-0.17849217]]\n",
      "t [[ 0.00394073]\n",
      " [-0.09212292]\n",
      " [-0.2336631 ]\n",
      " ...\n",
      " [-0.09422669]\n",
      " [-0.00564962]\n",
      " [-0.23253262]]\n",
      "t [[ 0.00394073]\n",
      " [-0.09212292]\n",
      " [-0.2336631 ]\n",
      " ...\n",
      " [-0.09422669]\n",
      " [-0.00564962]\n",
      " [-0.23253262]]\n",
      "Current iteration=4, loss=38503.132966089135\n",
      "t [[ 0.00293313]\n",
      " [-0.11807286]\n",
      " [-0.27706998]\n",
      " ...\n",
      " [-0.1084913 ]\n",
      " [-0.00426107]\n",
      " [-0.28422176]]\n",
      "t [[ 0.00293313]\n",
      " [-0.11807286]\n",
      " [-0.27706998]\n",
      " ...\n",
      " [-0.1084913 ]\n",
      " [-0.00426107]\n",
      " [-0.28422176]]\n",
      "t [[ 0.00142719]\n",
      " [-0.14460068]\n",
      " [-0.31621282]\n",
      " ...\n",
      " [-0.12024549]\n",
      " [-0.00222421]\n",
      " [-0.3337494 ]]\n",
      "t [[ 0.00142719]\n",
      " [-0.14460068]\n",
      " [-0.31621282]\n",
      " ...\n",
      " [-0.12024549]\n",
      " [-0.00222421]\n",
      " [-0.3337494 ]]\n",
      "Current iteration=6, loss=37904.38442080155\n",
      "t [[-4.71543613e-04]\n",
      " [-1.71500766e-01]\n",
      " [-3.51723702e-01]\n",
      " ...\n",
      " [-1.29922177e-01]\n",
      " [ 2.97057141e-04]\n",
      " [-3.81283876e-01]]\n",
      "t [[-4.71543613e-04]\n",
      " [-1.71500766e-01]\n",
      " [-3.51723702e-01]\n",
      " ...\n",
      " [-1.29922177e-01]\n",
      " [ 2.97057141e-04]\n",
      " [-3.81283876e-01]]\n",
      "t [[-0.00267646]\n",
      " [-0.19860583]\n",
      " [-0.38413101]\n",
      " ...\n",
      " [-0.13788045]\n",
      " [ 0.00316954]\n",
      " [-0.4269746 ]]\n",
      "t [[-0.00267646]\n",
      " [-0.19860583]\n",
      " [-0.38413101]\n",
      " ...\n",
      " [-0.13788045]\n",
      " [ 0.00316954]\n",
      " [-0.4269746 ]]\n",
      "Current iteration=8, loss=37417.12518618403\n",
      "t [[-0.00511657]\n",
      " [-0.2257801 ]\n",
      " [-0.41387693]\n",
      " ...\n",
      " [-0.14441828]\n",
      " [ 0.00628532]\n",
      " [-0.47095452]]\n",
      "t [[-0.00511657]\n",
      " [-0.2257801 ]\n",
      " [-0.41387693]\n",
      " ...\n",
      " [-0.14441828]\n",
      " [ 0.00628532]\n",
      " [-0.47095452]]\n",
      "t [[-0.00773366]\n",
      " [-0.25291371]\n",
      " [-0.44133236]\n",
      " ...\n",
      " [-0.14978311]\n",
      " [ 0.00955722]\n",
      " [-0.51334224]]\n",
      "loss=37008.224495566246\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.02993334]\n",
      " [-0.12787179]\n",
      " [-0.08037202]\n",
      " ...\n",
      " [-0.05824979]\n",
      " [ 0.0126114 ]\n",
      " [-0.03395657]]\n",
      "t [[ 0.02993334]\n",
      " [-0.12787179]\n",
      " [-0.08037202]\n",
      " ...\n",
      " [-0.05824979]\n",
      " [ 0.0126114 ]\n",
      " [-0.03395657]]\n",
      "t [[ 0.05559906]\n",
      " [-0.24109017]\n",
      " [-0.1511115 ]\n",
      " ...\n",
      " [-0.10844363]\n",
      " [ 0.02231926]\n",
      " [-0.06570332]]\n",
      "t [[ 0.05559906]\n",
      " [-0.24109017]\n",
      " [-0.1511115 ]\n",
      " ...\n",
      " [-0.10844363]\n",
      " [ 0.02231926]\n",
      " [-0.06570332]]\n",
      "Current iteration=2, loss=39200.77651541376\n",
      "t [[ 0.07764031]\n",
      " [-0.34186335]\n",
      " [-0.21375306]\n",
      " ...\n",
      " [-0.15193059]\n",
      " [ 0.02955583]\n",
      " [-0.09547673]]\n",
      "t [[ 0.07764031]\n",
      " [-0.34186335]\n",
      " [-0.21375306]\n",
      " ...\n",
      " [-0.15193059]\n",
      " [ 0.02955583]\n",
      " [-0.09547673]]\n",
      "t [[ 0.09660896]\n",
      " [-0.43207332]\n",
      " [-0.26959665]\n",
      " ...\n",
      " [-0.1898494 ]\n",
      " [ 0.03469365]\n",
      " [-0.12348439]]\n",
      "t [[ 0.09660896]\n",
      " [-0.43207332]\n",
      " [-0.26959665]\n",
      " ...\n",
      " [-0.1898494 ]\n",
      " [ 0.03469365]\n",
      " [-0.12348439]]\n",
      "Current iteration=4, loss=38401.30136467834\n",
      "t [[ 0.11297079]\n",
      " [-0.51329575]\n",
      " [-0.31972354]\n",
      " ...\n",
      " [-0.22314467]\n",
      " [ 0.03804683]\n",
      " [-0.14990575]]\n",
      "t [[ 0.11297079]\n",
      " [-0.51329575]\n",
      " [-0.31972354]\n",
      " ...\n",
      " [-0.22314467]\n",
      " [ 0.03804683]\n",
      " [-0.14990575]]\n",
      "t [[ 0.12711602]\n",
      " [-0.58683973]\n",
      " [-0.36502582]\n",
      " ...\n",
      " [-0.25259363]\n",
      " [ 0.03987795]\n",
      " [-0.17489507]]\n",
      "t [[ 0.12711602]\n",
      " [-0.58683973]\n",
      " [-0.36502582]\n",
      " ...\n",
      " [-0.25259363]\n",
      " [ 0.03987795]\n",
      " [-0.17489507]]\n",
      "Current iteration=6, loss=37787.55555674896\n",
      "t [[ 0.1393709 ]\n",
      " [-0.65379103]\n",
      " [-0.40623783]\n",
      " ...\n",
      " [-0.27883411]\n",
      " [ 0.04040608]\n",
      " [-0.19858498]]\n",
      "t [[ 0.1393709 ]\n",
      " [-0.65379103]\n",
      " [-0.40623783]\n",
      " ...\n",
      " [-0.27883411]\n",
      " [ 0.04040608]\n",
      " [-0.19858498]]\n",
      "t [[ 0.15000849]\n",
      " [-0.71505168]\n",
      " [-0.44396467]\n",
      " ...\n",
      " [-0.30238968]\n",
      " [ 0.03981451]\n",
      " [-0.2210899 ]]\n",
      "t [[ 0.15000849]\n",
      " [-0.71505168]\n",
      " [-0.44396467]\n",
      " ...\n",
      " [-0.30238968]\n",
      " [ 0.03981451]\n",
      " [-0.2210899 ]]\n",
      "Current iteration=8, loss=37293.219842229824\n",
      "t [[ 0.15925783]\n",
      " [-0.77137357]\n",
      " [-0.47870632]\n",
      " ...\n",
      " [-0.32369091]\n",
      " [ 0.03825728]\n",
      " [-0.24250903]]\n",
      "t [[ 0.15925783]\n",
      " [-0.77137357]\n",
      " [-0.47870632]\n",
      " ...\n",
      " [-0.32369091]\n",
      " [ 0.03825728]\n",
      " [-0.24250903]]\n",
      "t [[ 0.16731173]\n",
      " [-0.82338614]\n",
      " [-0.51087727]\n",
      " ...\n",
      " [-0.34309276]\n",
      " [ 0.03586463]\n",
      " [-0.26292891]]\n",
      "loss=36881.64691340677\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00394742]\n",
      " [-0.02430372]\n",
      " [-0.07619471]\n",
      " ...\n",
      " [-0.05847017]\n",
      " [ 0.011367  ]\n",
      " [-0.0346871 ]]\n",
      "t [[ 0.00394742]\n",
      " [-0.02430372]\n",
      " [-0.07619471]\n",
      " ...\n",
      " [-0.05847017]\n",
      " [ 0.011367  ]\n",
      " [-0.0346871 ]]\n",
      "t [[ 0.00641887]\n",
      " [-0.05086662]\n",
      " [-0.14250339]\n",
      " ...\n",
      " [-0.1087105 ]\n",
      " [ 0.01989858]\n",
      " [-0.06711423]]\n",
      "t [[ 0.00641887]\n",
      " [-0.05086662]\n",
      " [-0.14250339]\n",
      " ...\n",
      " [-0.1087105 ]\n",
      " [ 0.01989858]\n",
      " [-0.06711423]]\n",
      "Current iteration=2, loss=39187.88960075628\n",
      "t [[ 0.00770355]\n",
      " [-0.07911301]\n",
      " [-0.20057117]\n",
      " ...\n",
      " [-0.15211411]\n",
      " [ 0.02602872]\n",
      " [-0.09752532]]\n",
      "t [[ 0.00770355]\n",
      " [-0.07911301]\n",
      " [-0.20057117]\n",
      " ...\n",
      " [-0.15211411]\n",
      " [ 0.02602872]\n",
      " [-0.09752532]]\n",
      "t [[ 0.00804387]\n",
      " [-0.10856727]\n",
      " [-0.25178422]\n",
      " ...\n",
      " [-0.18985412]\n",
      " [ 0.03012987]\n",
      " [-0.12613411]]\n",
      "t [[ 0.00804387]\n",
      " [-0.10856727]\n",
      " [-0.25178422]\n",
      " ...\n",
      " [-0.18985412]\n",
      " [ 0.03012987]\n",
      " [-0.12613411]]\n",
      "Current iteration=4, loss=38378.82930888973\n",
      "t [[ 0.0076383 ]\n",
      " [-0.13884451]\n",
      " [-0.29728963]\n",
      " ...\n",
      " [-0.22290124]\n",
      " [ 0.03251473]\n",
      " [-0.15312498]]\n",
      "t [[ 0.0076383 ]\n",
      " [-0.13884451]\n",
      " [-0.29728963]\n",
      " ...\n",
      " [-0.22290124]\n",
      " [ 0.03251473]\n",
      " [-0.15312498]]\n",
      "t [[ 0.0066475 ]\n",
      " [-0.16963675]\n",
      " [-0.33802904]\n",
      " ...\n",
      " [-0.2520523 ]\n",
      " [ 0.03344346]\n",
      " [-0.1786561 ]]\n",
      "t [[ 0.0066475 ]\n",
      " [-0.16963675]\n",
      " [-0.33802904]\n",
      " ...\n",
      " [-0.2520523 ]\n",
      " [ 0.03344346]\n",
      " [-0.1786561 ]]\n",
      "Current iteration=6, loss=37756.803775735025\n",
      "t [[ 0.00520095]\n",
      " [-0.20069912]\n",
      " [-0.37477361]\n",
      " ...\n",
      " [-0.27795971]\n",
      " [ 0.03313221]\n",
      " [-0.20286328]]\n",
      "t [[ 0.00520095]\n",
      " [-0.20069912]\n",
      " [-0.37477361]\n",
      " ...\n",
      " [-0.27795971]\n",
      " [ 0.03313221]\n",
      " [-0.20286328]]\n",
      "t [[ 0.00340281]\n",
      " [-0.23183777]\n",
      " [-0.40815535]\n",
      " ...\n",
      " [-0.30115794]\n",
      " [ 0.03176101]\n",
      " [-0.22586348]]\n",
      "t [[ 0.00340281]\n",
      " [-0.23183777]\n",
      " [-0.40815535]\n",
      " ...\n",
      " [-0.30115794]\n",
      " [ 0.03176101]\n",
      " [-0.22586348]]\n",
      "Current iteration=8, loss=37254.95582472687\n",
      "t [[ 0.00133699]\n",
      " [-0.26289977]\n",
      " [-0.43869345]\n",
      " ...\n",
      " [-0.32208565]\n",
      " [ 0.02948055]\n",
      " [-0.24775804]]\n",
      "t [[ 0.00133699]\n",
      " [-0.26289977]\n",
      " [-0.43869345]\n",
      " ...\n",
      " [-0.32208565]\n",
      " [ 0.02948055]\n",
      " [-0.24775804]]\n",
      "t [[-0.00092891]\n",
      " [-0.29376496]\n",
      " [-0.46681576]\n",
      " ...\n",
      " [-0.34110385]\n",
      " [ 0.02641771]\n",
      " [-0.26863525]]\n",
      "loss=36836.52786542825\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00337153]\n",
      " [-0.0213304 ]\n",
      " [-0.07701993]\n",
      " ...\n",
      " [-0.05749236]\n",
      " [ 0.01222174]\n",
      " [-0.03253665]]\n",
      "t [[ 0.00337153]\n",
      " [-0.0213304 ]\n",
      " [-0.07701993]\n",
      " ...\n",
      " [-0.05749236]\n",
      " [ 0.01222174]\n",
      " [-0.03253665]]\n",
      "t [[ 0.0052816 ]\n",
      " [-0.04502477]\n",
      " [-0.14417414]\n",
      " ...\n",
      " [-0.10686697]\n",
      " [ 0.0215803 ]\n",
      " [-0.06288935]]\n",
      "t [[ 0.0052816 ]\n",
      " [-0.04502477]\n",
      " [-0.14417414]\n",
      " ...\n",
      " [-0.10686697]\n",
      " [ 0.0215803 ]\n",
      " [-0.06288935]]\n",
      "Current iteration=2, loss=39199.959819479125\n",
      "t [[ 0.00602103]\n",
      " [-0.07050563]\n",
      " [-0.20309185]\n",
      " ...\n",
      " [-0.14949504]\n",
      " [ 0.02850809]\n",
      " [-0.09129708]]\n",
      "t [[ 0.00602103]\n",
      " [-0.07050563]\n",
      " [-0.20309185]\n",
      " ...\n",
      " [-0.14949504]\n",
      " [ 0.02850809]\n",
      " [-0.09129708]]\n",
      "t [[ 0.00583345]\n",
      " [-0.09729478]\n",
      " [-0.25514739]\n",
      " ...\n",
      " [-0.18653243]\n",
      " [ 0.03337646]\n",
      " [-0.11796917]]\n",
      "t [[ 0.00583345]\n",
      " [-0.09729478]\n",
      " [-0.25514739]\n",
      " ...\n",
      " [-0.18653243]\n",
      " [ 0.03337646]\n",
      " [-0.11796917]]\n",
      "Current iteration=4, loss=38400.72015103053\n",
      "t [[ 0.00491825]\n",
      " [-0.12500391]\n",
      " [-0.30147968]\n",
      " ...\n",
      " [-0.21893659]\n",
      " [ 0.03649755]\n",
      " [-0.14308607]]\n",
      "t [[ 0.00491825]\n",
      " [-0.12500391]\n",
      " [-0.30147968]\n",
      " ...\n",
      " [-0.21893659]\n",
      " [ 0.03649755]\n",
      " [-0.14308607]]\n",
      "t [[ 0.00343679]\n",
      " [-0.15332107]\n",
      " [-0.343025  ]\n",
      " ...\n",
      " [-0.24749423]\n",
      " [ 0.03813149]\n",
      " [-0.1668024 ]]\n",
      "t [[ 0.00343679]\n",
      " [-0.15332107]\n",
      " [-0.343025  ]\n",
      " ...\n",
      " [-0.24749423]\n",
      " [ 0.03813149]\n",
      " [-0.1668024 ]]\n",
      "Current iteration=6, loss=37787.30324568376\n",
      "t [[ 0.00151902]\n",
      " [-0.18199723]\n",
      " [-0.38055122]\n",
      " ...\n",
      " [-0.27285008]\n",
      " [ 0.03849472]\n",
      " [-0.18925076]]\n",
      "t [[ 0.00151902]\n",
      " [-0.18199723]\n",
      " [-0.38055122]\n",
      " ...\n",
      " [-0.27285008]\n",
      " [ 0.03849472]\n",
      " [-0.18925076]]\n",
      "t [[-0.00073059]\n",
      " [-0.21083431]\n",
      " [-0.41468853]\n",
      " ...\n",
      " [-0.29553271]\n",
      " [ 0.03776783]\n",
      " [-0.21054522]]\n",
      "t [[-0.00073059]\n",
      " [-0.21083431]\n",
      " [-0.41468853]\n",
      " ...\n",
      " [-0.29553271]\n",
      " [ 0.03776783]\n",
      " [-0.21054522]]\n",
      "Current iteration=8, loss=37293.1946889756\n",
      "t [[-0.00322801]\n",
      " [-0.23967518]\n",
      " [-0.44595528]\n",
      " ...\n",
      " [-0.31597619]\n",
      " [ 0.03610223]\n",
      " [-0.23078444]]\n",
      "t [[-0.00322801]\n",
      " [-0.23967518]\n",
      " [-0.44595528]\n",
      " ...\n",
      " [-0.31597619]\n",
      " [ 0.03610223]\n",
      " [-0.23078444]]\n",
      "t [[-0.00590561]\n",
      " [-0.26839567]\n",
      " [-0.47477918]\n",
      " ...\n",
      " [-0.33453797]\n",
      " [ 0.03362558]\n",
      " [-0.25005433]]\n",
      "loss=36881.76116172473\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00280781]\n",
      " [-0.02259258]\n",
      " [-0.07651173]\n",
      " ...\n",
      " [-0.03368615]\n",
      " [-0.00405707]\n",
      " [-0.06872519]]\n",
      "t [[ 0.00280781]\n",
      " [-0.02259258]\n",
      " [-0.07651173]\n",
      " ...\n",
      " [-0.03368615]\n",
      " [-0.00405707]\n",
      " [-0.06872519]]\n",
      "t [[ 0.00418302]\n",
      " [-0.04753284]\n",
      " [-0.14307564]\n",
      " ...\n",
      " [-0.06099679]\n",
      " [-0.00600595]\n",
      " [-0.13373858]]\n",
      "t [[ 0.00418302]\n",
      " [-0.04753284]\n",
      " [-0.14307564]\n",
      " ...\n",
      " [-0.06099679]\n",
      " [-0.00600595]\n",
      " [-0.13373858]]\n",
      "Current iteration=2, loss=39180.47680427156\n",
      "t [[ 0.00441516]\n",
      " [-0.07423959]\n",
      " [-0.20134255]\n",
      " ...\n",
      " [-0.08309174]\n",
      " [-0.00631596]\n",
      " [-0.19539275]]\n",
      "t [[ 0.00441516]\n",
      " [-0.07423959]\n",
      " [-0.20134255]\n",
      " ...\n",
      " [-0.08309174]\n",
      " [-0.00631596]\n",
      " [-0.19539275]]\n",
      "t [[ 0.00374639]\n",
      " [-0.10223173]\n",
      " [-0.25270446]\n",
      " ...\n",
      " [-0.10094227]\n",
      " [-0.00537314]\n",
      " [-0.25400246]]\n",
      "t [[ 0.00374639]\n",
      " [-0.10223173]\n",
      " [-0.25270446]\n",
      " ...\n",
      " [-0.10094227]\n",
      " [-0.00537314]\n",
      " [-0.25400246]]\n",
      "Current iteration=4, loss=38367.74636595817\n",
      "t [[ 0.00237459]\n",
      " [-0.13111905]\n",
      " [-0.29831428]\n",
      " ...\n",
      " [-0.11534739]\n",
      " [-0.00348957]\n",
      " [-0.30984487]]\n",
      "t [[ 0.00237459]\n",
      " [-0.13111905]\n",
      " [-0.29831428]\n",
      " ...\n",
      " [-0.11534739]\n",
      " [-0.00348957]\n",
      " [-0.30984487]]\n",
      "t [[ 0.00045967]\n",
      " [-0.16058856]\n",
      " [-0.33911915]\n",
      " ...\n",
      " [-0.12695881]\n",
      " [-0.00091504]\n",
      " [-0.36316277]]\n",
      "t [[ 0.00045967]\n",
      " [-0.16058856]\n",
      " [-0.33911915]\n",
      " ...\n",
      " [-0.12695881]\n",
      " [-0.00091504]\n",
      " [-0.36316277]]\n",
      "Current iteration=6, loss=37744.01379525784\n",
      "t [[-0.00186985]\n",
      " [-0.19039072]\n",
      " [-0.3758953 ]\n",
      " ...\n",
      " [-0.13630641]\n",
      " [ 0.00215159]\n",
      " [-0.41416866]]\n",
      "t [[-0.00186985]\n",
      " [-0.19039072]\n",
      " [-0.3758953 ]\n",
      " ...\n",
      " [-0.13630641]\n",
      " [ 0.00215159]\n",
      " [-0.41416866]]\n",
      "t [[-0.00451087]\n",
      " [-0.22032736]\n",
      " [-0.40927933]\n",
      " ...\n",
      " [-0.14382081]\n",
      " [ 0.00555264]\n",
      " [-0.46304895]]\n",
      "t [[-0.00451087]\n",
      " [-0.22032736]\n",
      " [-0.40927933]\n",
      " ...\n",
      " [-0.14382081]\n",
      " [ 0.00555264]\n",
      " [-0.46304895]]\n",
      "Current iteration=8, loss=37241.44917978262\n",
      "t [[-0.00738064]\n",
      " [-0.25024155]\n",
      " [-0.43979449]\n",
      " ...\n",
      " [-0.14985232]\n",
      " [ 0.00916352]\n",
      " [-0.50996775]]\n",
      "t [[-0.00738064]\n",
      " [-0.25024155]\n",
      " [-0.43979449]\n",
      " ...\n",
      " [-0.14985232]\n",
      " [ 0.00916352]\n",
      " [-0.50996775]]\n",
      "t [[-0.01041278]\n",
      " [-0.28000947]\n",
      " [-0.46787222]\n",
      " ...\n",
      " [-0.15468634]\n",
      " [ 0.01288617]\n",
      " [-0.55507011]]\n",
      "loss=36822.782594136435\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.03265455]\n",
      " [-0.1394965 ]\n",
      " [-0.08767857]\n",
      " ...\n",
      " [-0.06354523]\n",
      " [ 0.01375789]\n",
      " [-0.03704353]]\n",
      "t [[ 0.03265455]\n",
      " [-0.1394965 ]\n",
      " [-0.08767857]\n",
      " ...\n",
      " [-0.06354523]\n",
      " [ 0.01375789]\n",
      " [-0.03704353]]\n",
      "t [[ 0.06023156]\n",
      " [-0.26155875]\n",
      " [-0.16389666]\n",
      " ...\n",
      " [-0.11750568]\n",
      " [ 0.02406141]\n",
      " [-0.07145772]]\n",
      "t [[ 0.06023156]\n",
      " [-0.26155875]\n",
      " [-0.16389666]\n",
      " ...\n",
      " [-0.11750568]\n",
      " [ 0.02406141]\n",
      " [-0.07145772]]\n",
      "Current iteration=2, loss=39113.92890305495\n",
      "t [[ 0.08356756]\n",
      " [-0.36905885]\n",
      " [-0.23065003]\n",
      " ...\n",
      " [-0.16363553]\n",
      " [ 0.03147301]\n",
      " [-0.10355026]]\n",
      "t [[ 0.08356756]\n",
      " [-0.36905885]\n",
      " [-0.23065003]\n",
      " ...\n",
      " [-0.16363553]\n",
      " [ 0.03147301]\n",
      " [-0.10355026]]\n",
      "t [[ 0.10336728]\n",
      " [-0.46439771]\n",
      " [-0.28959582]\n",
      " ...\n",
      " [-0.20338597]\n",
      " [ 0.03646827]\n",
      " [-0.13358694]]\n",
      "t [[ 0.10336728]\n",
      " [-0.46439771]\n",
      " [-0.28959582]\n",
      " ...\n",
      " [-0.20338597]\n",
      " [ 0.03646827]\n",
      " [-0.13358694]]\n",
      "Current iteration=4, loss=38273.99078296925\n",
      "t [[ 0.12021393]\n",
      " [-0.54954429]\n",
      " [-0.34208323]\n",
      " ...\n",
      " [-0.23793511]\n",
      " [ 0.03944012]\n",
      " [-0.16179365]]\n",
      "t [[ 0.12021393]\n",
      " [-0.54954429]\n",
      " [-0.34208323]\n",
      " ...\n",
      " [-0.23793511]\n",
      " [ 0.03944012]\n",
      " [-0.16179365]]\n",
      "t [[ 0.13458688]\n",
      " [-0.62610223]\n",
      " [-0.38920253]\n",
      " ...\n",
      " [-0.26823209]\n",
      " [ 0.04071066]\n",
      " [-0.1883615 ]]\n",
      "t [[ 0.13458688]\n",
      " [-0.62610223]\n",
      " [-0.38920253]\n",
      " ...\n",
      " [-0.26823209]\n",
      " [ 0.04071066]\n",
      " [-0.1883615 ]]\n",
      "Current iteration=6, loss=37638.594037340525\n",
      "t [[ 0.14687983]\n",
      " [-0.69537723]\n",
      " [-0.43183388]\n",
      " ...\n",
      " [-0.29504031]\n",
      " [ 0.04054401]\n",
      " [-0.21345249]]\n",
      "t [[ 0.14687983]\n",
      " [-0.69537723]\n",
      " [-0.43183388]\n",
      " ...\n",
      " [-0.29504031]\n",
      " [ 0.04054401]\n",
      " [-0.21345249]]\n",
      "t [[ 0.15741684]\n",
      " [-0.75843582]\n",
      " [-0.47068952]\n",
      " ...\n",
      " [-0.31897468]\n",
      " [ 0.03915773]\n",
      " [-0.23720471]]\n",
      "t [[ 0.15741684]\n",
      " [-0.75843582]\n",
      " [-0.47068952]\n",
      " ...\n",
      " [-0.31897468]\n",
      " [ 0.03915773]\n",
      " [-0.23720471]]\n",
      "Current iteration=8, loss=37131.45109230439\n",
      "t [[ 0.16646558]\n",
      " [-0.81615334]\n",
      " [-0.50634808]\n",
      " ...\n",
      " [-0.34053188]\n",
      " [ 0.03673219]\n",
      " [-0.25973664]]\n",
      "t [[ 0.16646558]\n",
      " [-0.81615334]\n",
      " [-0.50634808]\n",
      " ...\n",
      " [-0.34053188]\n",
      " [ 0.03673219]\n",
      " [-0.25973664]]\n",
      "t [[ 0.17424814]\n",
      " [-0.86925224]\n",
      " [-0.53928177]\n",
      " ...\n",
      " [-0.36011437]\n",
      " [ 0.03341809]\n",
      " [-0.28115071]]\n",
      "loss=36712.11039347524\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00430627]\n",
      " [-0.02651315]\n",
      " [-0.0831215 ]\n",
      " ...\n",
      " [-0.06378564]\n",
      " [ 0.01240036]\n",
      " [-0.03784047]]\n",
      "t [[ 0.00430627]\n",
      " [-0.02651315]\n",
      " [-0.0831215 ]\n",
      " ...\n",
      " [-0.06378564]\n",
      " [ 0.01240036]\n",
      " [-0.03784047]]\n",
      "t [[ 0.00685669]\n",
      " [-0.05571395]\n",
      " [-0.15448096]\n",
      " ...\n",
      " [-0.11777966]\n",
      " [ 0.02142744]\n",
      " [-0.07299194]]\n",
      "t [[ 0.00685669]\n",
      " [-0.05571395]\n",
      " [-0.15448096]\n",
      " ...\n",
      " [-0.11777966]\n",
      " [ 0.02142744]\n",
      " [-0.07299194]]\n",
      "Current iteration=2, loss=39100.044104770306\n",
      "t [[ 0.00802737]\n",
      " [-0.08685417]\n",
      " [-0.21621761]\n",
      " ...\n",
      " [-0.16379349]\n",
      " [ 0.0276459 ]\n",
      " [-0.10577181]]\n",
      "t [[ 0.00802737]\n",
      " [-0.08685417]\n",
      " [-0.21621761]\n",
      " ...\n",
      " [-0.16379349]\n",
      " [ 0.0276459 ]\n",
      " [-0.10577181]]\n",
      "t [[ 0.00812679]\n",
      " [-0.11932943]\n",
      " [-0.27009741]\n",
      " ...\n",
      " [-0.20332168]\n",
      " [ 0.03153096]\n",
      " [-0.13645366]]\n",
      "t [[ 0.00812679]\n",
      " [-0.11932943]\n",
      " [-0.27009741]\n",
      " ...\n",
      " [-0.20332168]\n",
      " [ 0.03153096]\n",
      " [-0.13645366]]\n",
      "Current iteration=4, loss=38249.91262844893\n",
      "t [[ 0.00740156]\n",
      " [-0.15266248]\n",
      " [-0.31755015]\n",
      " ...\n",
      " [-0.23757421]\n",
      " [ 0.03347335]\n",
      " [-0.16526946]]\n",
      "t [[ 0.00740156]\n",
      " [-0.15266248]\n",
      " [-0.31755015]\n",
      " ...\n",
      " [-0.23757421]\n",
      " [ 0.03347335]\n",
      " [-0.16526946]]\n",
      "t [[ 0.00604669]\n",
      " [-0.1864809 ]\n",
      " [-0.35972477]\n",
      " ...\n",
      " [-0.26752333]\n",
      " [ 0.03379179]\n",
      " [-0.19241508]]\n",
      "t [[ 0.00604669]\n",
      " [-0.1864809 ]\n",
      " [-0.35972477]\n",
      " ...\n",
      " [-0.26752333]\n",
      " [ 0.03379179]\n",
      " [-0.19241508]]\n",
      "Current iteration=6, loss=37605.69700890043\n",
      "t [[ 0.00421582]\n",
      " [-0.22049588]\n",
      " [-0.39754346]\n",
      " ...\n",
      " [-0.2939492 ]\n",
      " [ 0.03274633]\n",
      " [-0.21805629]]\n",
      "t [[ 0.00421582]\n",
      " [-0.22049588]\n",
      " [-0.39754346]\n",
      " ...\n",
      " [-0.2939492 ]\n",
      " [ 0.03274633]\n",
      " [-0.21805629]]\n",
      "t [[ 0.00202992]\n",
      " [-0.25448449]\n",
      " [-0.43174789]\n",
      " ...\n",
      " [-0.3174788 ]\n",
      " [ 0.03055019]\n",
      " [-0.24233415]]\n",
      "t [[ 0.00202992]\n",
      " [-0.25448449]\n",
      " [-0.43174789]\n",
      " ...\n",
      " [-0.3174788 ]\n",
      " [ 0.03055019]\n",
      " [-0.24233415]]\n",
      "Current iteration=8, loss=37090.585060485486\n",
      "t [[-4.15606074e-04]\n",
      " [-2.88275353e-01]\n",
      " [-4.62936652e-01]\n",
      " ...\n",
      " [-3.38617525e-01]\n",
      " [ 2.73793453e-02]\n",
      " [-2.65369598e-01]]\n",
      "t [[-4.15606074e-04]\n",
      " [-2.88275353e-01]\n",
      " [-4.62936652e-01]\n",
      " ...\n",
      " [-3.38617525e-01]\n",
      " [ 2.73793453e-02]\n",
      " [-2.65369598e-01]]\n",
      "t [[-0.00304542]\n",
      " [-0.32173755]\n",
      " [-0.49159476]\n",
      " ...\n",
      " [-0.35777415]\n",
      " [ 0.02338016]\n",
      " [-0.28726707]]\n",
      "loss=36664.04432409243\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00367803]\n",
      " [-0.02326953]\n",
      " [-0.08402174]\n",
      " ...\n",
      " [-0.06271894]\n",
      " [ 0.01333281]\n",
      " [-0.03549452]]\n",
      "t [[ 0.00367803]\n",
      " [-0.02326953]\n",
      " [-0.08402174]\n",
      " ...\n",
      " [-0.06271894]\n",
      " [ 0.01333281]\n",
      " [-0.03549452]]\n",
      "t [[ 0.00561746]\n",
      " [-0.04935144]\n",
      " [-0.15630554]\n",
      " ...\n",
      " [-0.11577959]\n",
      " [ 0.02325926]\n",
      " [-0.06839051]]\n",
      "t [[ 0.00561746]\n",
      " [-0.04935144]\n",
      " [-0.15630554]\n",
      " ...\n",
      " [-0.11577959]\n",
      " [ 0.02325926]\n",
      " [-0.06839051]]\n",
      "Current iteration=2, loss=39113.10293937561\n",
      "t [[ 0.0061965 ]\n",
      " [-0.07749519]\n",
      " [-0.21896995]\n",
      " ...\n",
      " [-0.16096492]\n",
      " [ 0.03034196]\n",
      " [-0.09899891]]\n",
      "t [[ 0.0061965 ]\n",
      " [-0.07749519]\n",
      " [-0.21896995]\n",
      " ...\n",
      " [-0.16096492]\n",
      " [ 0.03034196]\n",
      " [-0.09899891]]\n",
      "t [[ 0.00572514]\n",
      " [-0.10709285]\n",
      " [-0.27376615]\n",
      " ...\n",
      " [-0.19974767]\n",
      " [ 0.0350548 ]\n",
      " [-0.12758765]]\n",
      "t [[ 0.00572514]\n",
      " [-0.10709285]\n",
      " [-0.27376615]\n",
      " ...\n",
      " [-0.19974767]\n",
      " [ 0.0350548 ]\n",
      " [-0.12758765]]\n",
      "Current iteration=4, loss=38273.472954068435\n",
      "t [[ 0.00445116]\n",
      " [-0.13766251]\n",
      " [-0.32211423]\n",
      " ...\n",
      " [-0.2333216 ]\n",
      " [ 0.03778799]\n",
      " [-0.15438369]]\n",
      "t [[ 0.00445116]\n",
      " [-0.13766251]\n",
      " [-0.32211423]\n",
      " ...\n",
      " [-0.2333216 ]\n",
      " [ 0.03778799]\n",
      " [-0.15438369]]\n",
      "t [[ 0.00257039]\n",
      " [-0.16882644]\n",
      " [-0.36515719]\n",
      " ...\n",
      " [-0.26264697]\n",
      " [ 0.03886032]\n",
      " [-0.17957841]]\n",
      "t [[ 0.00257039]\n",
      " [-0.16882644]\n",
      " [-0.36515719]\n",
      " ...\n",
      " [-0.26264697]\n",
      " [ 0.03886032]\n",
      " [-0.17957841]]\n",
      "Current iteration=6, loss=37638.418950783365\n",
      "t [[ 2.36984900e-04]\n",
      " [-2.00290325e-01]\n",
      " [-4.03813843e-01]\n",
      " ...\n",
      " [-2.88495003e-01]\n",
      " [ 3.85324256e-02]\n",
      " [-2.03333585e-01]]\n",
      "t [[ 2.36984900e-04]\n",
      " [-2.00290325e-01]\n",
      " [-4.03813843e-01]\n",
      " ...\n",
      " [-2.88495003e-01]\n",
      " [ 3.85324256e-02]\n",
      " [-2.03333585e-01]]\n",
      "t [[-0.00242778]\n",
      " [-0.23182572]\n",
      " [-0.43882428]\n",
      " ...\n",
      " [-0.311486  ]\n",
      " [ 0.03701836]\n",
      " [-0.22578669]]\n",
      "t [[-0.00242778]\n",
      " [-0.23182572]\n",
      " [-0.43882428]\n",
      " ...\n",
      " [-0.311486  ]\n",
      " [ 0.03701836]\n",
      " [-0.22578669]]\n",
      "Current iteration=8, loss=37131.48782825143\n",
      "t [[-0.00532844]\n",
      " [-0.26325595]\n",
      " [-0.47078661]\n",
      " ...\n",
      " [-0.33212029]\n",
      " [ 0.03449513]\n",
      " [-0.24705542]]\n",
      "t [[-0.00532844]\n",
      " [-0.26325595]\n",
      " [-0.47078661]\n",
      " ...\n",
      " [-0.33212029]\n",
      " [ 0.03449513]\n",
      " [-0.24705542]]\n",
      "t [[-0.00838976]\n",
      " [-0.29444502]\n",
      " [-0.50018614]\n",
      " ...\n",
      " [-0.35080272]\n",
      " [ 0.03111021]\n",
      " [-0.26724125]]\n",
      "loss=36712.27330310799\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00306306]\n",
      " [-0.02464645]\n",
      " [-0.08346734]\n",
      " ...\n",
      " [-0.03674852]\n",
      " [-0.0044259 ]\n",
      " [-0.07497294]]\n",
      "t [[ 0.00306306]\n",
      " [-0.02464645]\n",
      " [-0.08346734]\n",
      " ...\n",
      " [-0.03674852]\n",
      " [-0.0044259 ]\n",
      " [-0.07497294]]\n",
      "t [[ 0.00442185]\n",
      " [-0.05208587]\n",
      " [-0.1550991 ]\n",
      " ...\n",
      " [-0.06591169]\n",
      " [-0.00634348]\n",
      " [-0.1455293 ]]\n",
      "t [[ 0.00442185]\n",
      " [-0.05208587]\n",
      " [-0.1550991 ]\n",
      " ...\n",
      " [-0.06591169]\n",
      " [-0.00634348]\n",
      " [-0.1455293 ]]\n",
      "Current iteration=2, loss=39092.13190434284\n",
      "t [[ 0.0044529 ]\n",
      " [-0.08156284]\n",
      " [-0.21704194]\n",
      " ...\n",
      " [-0.08899736]\n",
      " [-0.00636259]\n",
      " [-0.21212808]]\n",
      "t [[ 0.0044529 ]\n",
      " [-0.08156284]\n",
      " [-0.21704194]\n",
      " ...\n",
      " [-0.08899736]\n",
      " [-0.00636259]\n",
      " [-0.21212808]]\n",
      "t [[ 0.00346429]\n",
      " [-0.11246585]\n",
      " [-0.27106944]\n",
      " ...\n",
      " [-0.10724207]\n",
      " [-0.00497373]\n",
      " [-0.27517321]]\n",
      "t [[ 0.00346429]\n",
      " [-0.11246585]\n",
      " [-0.27106944]\n",
      " ...\n",
      " [-0.10724207]\n",
      " [-0.00497373]\n",
      " [-0.27517321]]\n",
      "Current iteration=4, loss=38238.36466175136\n",
      "t [[ 0.00170183]\n",
      " [-0.14431084]\n",
      " [-0.31861889]\n",
      " ...\n",
      " [-0.12164049]\n",
      " [-0.00256393]\n",
      " [-0.33501513]]\n",
      "t [[ 0.00170183]\n",
      " [-0.14431084]\n",
      " [-0.31861889]\n",
      " ...\n",
      " [-0.12164049]\n",
      " [-0.00256393]\n",
      " [-0.33501513]]\n",
      "t [[-0.00064052]\n",
      " [-0.17671897]\n",
      " [-0.36084627]\n",
      " ...\n",
      " [-0.13298612]\n",
      " [ 0.00056453]\n",
      " [-0.39195645]]\n",
      "t [[-0.00064052]\n",
      " [-0.17671897]\n",
      " [-0.36084627]\n",
      " ...\n",
      " [-0.13298612]\n",
      " [ 0.00056453]\n",
      " [-0.39195645]]\n",
      "Current iteration=6, loss=37592.6082143541\n",
      "t [[-0.00341043]\n",
      " [-0.20939555]\n",
      " [-0.39868011]\n",
      " ...\n",
      " [-0.14191056]\n",
      " [ 0.00417689]\n",
      " [-0.44625869]]\n",
      "t [[-0.00341043]\n",
      " [-0.20939555]\n",
      " [-0.39868011]\n",
      " ...\n",
      " [-0.14191056]\n",
      " [ 0.00417689]\n",
      " [-0.44625869]]\n",
      "t [[-0.00648836]\n",
      " [-0.24211223]\n",
      " [-0.43286771]\n",
      " ...\n",
      " [-0.14891672]\n",
      " [ 0.00809161]\n",
      " [-0.49814868]]\n",
      "t [[-0.00648836]\n",
      " [-0.24211223]\n",
      " [-0.43286771]\n",
      " ...\n",
      " [-0.14891672]\n",
      " [ 0.00809161]\n",
      " [-0.49814868]]\n",
      "Current iteration=8, loss=37076.934064013025\n",
      "t [[-0.00978047]\n",
      " [-0.27469273]\n",
      " [-0.46401256]\n",
      " ...\n",
      " [-0.15440574]\n",
      " [ 0.01216896]\n",
      " [-0.54782408]]\n",
      "t [[-0.00978047]\n",
      " [-0.27469273]\n",
      " [-0.46401256]\n",
      " ...\n",
      " [-0.15440574]\n",
      " [ 0.01216896]\n",
      " [-0.54782408]]\n",
      "t [[-0.01321309]\n",
      " [-0.30700164]\n",
      " [-0.49260397]\n",
      " ...\n",
      " [-0.1586982 ]\n",
      " [ 0.01630196]\n",
      " [-0.59545815]]\n",
      "loss=36650.26151211111\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.03537576]\n",
      " [-0.15112121]\n",
      " [-0.09498512]\n",
      " ...\n",
      " [-0.06884067]\n",
      " [ 0.01490438]\n",
      " [-0.04013049]]\n",
      "t [[ 0.03537576]\n",
      " [-0.15112121]\n",
      " [-0.09498512]\n",
      " ...\n",
      " [-0.06884067]\n",
      " [ 0.01490438]\n",
      " [-0.04013049]]\n",
      "t [[ 0.06479408]\n",
      " [-0.28178713]\n",
      " [-0.17652396]\n",
      " ...\n",
      " [-0.12643567]\n",
      " [ 0.02575606]\n",
      " [-0.07717582]]\n",
      "t [[ 0.06479408]\n",
      " [-0.28178713]\n",
      " [-0.17652396]\n",
      " ...\n",
      " [-0.12643567]\n",
      " [ 0.02575606]\n",
      " [-0.07717582]]\n",
      "Current iteration=2, loss=39029.16893366933\n",
      "t [[ 0.08932029]\n",
      " [-0.39565523]\n",
      " [-0.24715771]\n",
      " ...\n",
      " [-0.17501827]\n",
      " [ 0.03327152]\n",
      " [-0.11152798]]\n",
      "t [[ 0.08932029]\n",
      " [-0.39565523]\n",
      " [-0.24715771]\n",
      " ...\n",
      " [-0.17501827]\n",
      " [ 0.03327152]\n",
      " [-0.11152798]]\n",
      "t [[ 0.10983496]\n",
      " [-0.49572229]\n",
      " [-0.30895322]\n",
      " ...\n",
      " [-0.216398  ]\n",
      " [ 0.03804476]\n",
      " [-0.14351998]]\n",
      "t [[ 0.10983496]\n",
      " [-0.49572229]\n",
      " [-0.30895322]\n",
      " ...\n",
      " [-0.216398  ]\n",
      " [ 0.03804476]\n",
      " [-0.14351998]]\n",
      "Current iteration=4, loss=38152.16167008603\n",
      "t [[ 0.1270524 ]\n",
      " [-0.584395  ]\n",
      " [-0.36355715]\n",
      " ...\n",
      " [-0.252012  ]\n",
      " [ 0.0405565 ]\n",
      " [-0.17342997]]\n",
      "t [[ 0.1270524 ]\n",
      " [-0.584395  ]\n",
      " [-0.36355715]\n",
      " ...\n",
      " [-0.252012  ]\n",
      " [ 0.0405565 ]\n",
      " [-0.17342997]]\n",
      "t [[ 0.14154867]\n",
      " [-0.66359487]\n",
      " [-0.41227273]\n",
      " ...\n",
      " [-0.2829938 ]\n",
      " [ 0.04119332]\n",
      " [-0.20148965]]\n",
      "t [[ 0.14154867]\n",
      " [-0.66359487]\n",
      " [-0.41227273]\n",
      " ...\n",
      " [-0.2829938 ]\n",
      " [ 0.04119332]\n",
      " [-0.20148965]]\n",
      "Current iteration=6, loss=37497.6239570009\n",
      "t [[ 0.15378858]\n",
      " [-0.73485826]\n",
      " [-0.456132  ]\n",
      " ...\n",
      " [-0.31023712]\n",
      " [ 0.04026684]\n",
      " [-0.22789279]]\n",
      "t [[ 0.15378858]\n",
      " [-0.73485826]\n",
      " [-0.456132  ]\n",
      " ...\n",
      " [-0.31023712]\n",
      " [ 0.04026684]\n",
      " [-0.22789279]]\n",
      "t [[ 0.16414854]\n",
      " [-0.79941957]\n",
      " [-0.49595535]\n",
      " ...\n",
      " [-0.33444847]\n",
      " [ 0.03803001]\n",
      " [-0.25280265]]\n",
      "t [[ 0.16414854]\n",
      " [-0.79941957]\n",
      " [-0.49595535]\n",
      " ...\n",
      " [-0.33444847]\n",
      " [ 0.03803001]\n",
      " [-0.25280265]]\n",
      "Current iteration=8, loss=36979.50805711144\n",
      "t [[ 0.17293484]\n",
      " [-0.85827692]\n",
      " [-0.53239837]\n",
      " ...\n",
      " [-0.35618842]\n",
      " [ 0.03468991]\n",
      " [-0.27635799]]\n",
      "t [[ 0.17293484]\n",
      " [-0.85827692]\n",
      " [-0.53239837]\n",
      " ...\n",
      " [-0.35618842]\n",
      " [ 0.03468991]\n",
      " [-0.27635799]]\n",
      "t [[ 0.18039804]\n",
      " [-0.91224289]\n",
      " [-0.56598786]\n",
      " ...\n",
      " [-0.37590355]\n",
      " [ 0.03041756]\n",
      " [-0.29867777]]\n",
      "loss=36553.87673561766\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00466513]\n",
      " [-0.02872258]\n",
      " [-0.0900483 ]\n",
      " ...\n",
      " [-0.06910111]\n",
      " [ 0.01343373]\n",
      " [-0.04099384]]\n",
      "t [[ 0.00466513]\n",
      " [-0.02872258]\n",
      " [-0.0900483 ]\n",
      " ...\n",
      " [-0.06910111]\n",
      " [ 0.01343373]\n",
      " [-0.04099384]]\n",
      "t [[ 0.00727041]\n",
      " [-0.0605982 ]\n",
      " [-0.16629652]\n",
      " ...\n",
      " [-0.12671394]\n",
      " [ 0.02290993]\n",
      " [-0.07883253]]\n",
      "t [[ 0.00727041]\n",
      " [-0.0605982 ]\n",
      " [-0.16629652]\n",
      " ...\n",
      " [-0.12671394]\n",
      " [ 0.02290993]\n",
      " [-0.07883253]]\n",
      "Current iteration=2, loss=39014.30890045171\n",
      "t [[ 0.00829476]\n",
      " [-0.0946747 ]\n",
      " [-0.23146826]\n",
      " ...\n",
      " [-0.17514459]\n",
      " [ 0.02914787]\n",
      " [-0.11392045]]\n",
      "t [[ 0.00829476]\n",
      " [-0.0946747 ]\n",
      " [-0.23146826]\n",
      " ...\n",
      " [-0.17514459]\n",
      " [ 0.02914787]\n",
      " [-0.11392045]]\n",
      "t [[ 0.00812219]\n",
      " [-0.13020066]\n",
      " [-0.28776477]\n",
      " ...\n",
      " [-0.21625604]\n",
      " [ 0.03274079]\n",
      " [-0.14660024]]\n",
      "t [[ 0.00812219]\n",
      " [-0.13020066]\n",
      " [-0.28776477]\n",
      " ...\n",
      " [-0.21625604]\n",
      " [ 0.03274079]\n",
      " [-0.14660024]]\n",
      "Current iteration=4, loss=38126.514510116926\n",
      "t [[ 0.00705235]\n",
      " [-0.1665975 ]\n",
      " [-0.3369291 ]\n",
      " ...\n",
      " [-0.25152345]\n",
      " [ 0.0341662 ]\n",
      " [-0.17715744]]\n",
      "t [[ 0.00705235]\n",
      " [-0.1665975 ]\n",
      " [-0.3369291 ]\n",
      " ...\n",
      " [-0.25152345]\n",
      " [ 0.0341662 ]\n",
      " [-0.17715744]]\n",
      "t [[ 0.00531666]\n",
      " [-0.203425  ]\n",
      " [-0.38033229]\n",
      " ...\n",
      " [-0.28210705]\n",
      " [ 0.03380602]\n",
      " [-0.20582937]]\n",
      "t [[ 0.00531666]\n",
      " [-0.203425  ]\n",
      " [-0.38033229]\n",
      " ...\n",
      " [-0.28210705]\n",
      " [ 0.03380602]\n",
      " [-0.20582937]]\n",
      "Current iteration=6, loss=37462.631680073835\n",
      "t [[ 0.00309336]\n",
      " [-0.24035037]\n",
      " [-0.41905306]\n",
      " ...\n",
      " [-0.30891926]\n",
      " [ 0.0319665 ]\n",
      " [-0.23281417]]\n",
      "t [[ 0.00309336]\n",
      " [-0.24035037]\n",
      " [-0.41905306]\n",
      " ...\n",
      " [-0.30891926]\n",
      " [ 0.0319665 ]\n",
      " [-0.23281417]]\n",
      "t [[ 0.00051982]\n",
      " [-0.27712329]\n",
      " [-0.453943  ]\n",
      " ...\n",
      " [-0.33267965]\n",
      " [ 0.02889496]\n",
      " [-0.25827853]]\n",
      "t [[ 0.00051982]\n",
      " [-0.27712329]\n",
      " [-0.453943  ]\n",
      " ...\n",
      " [-0.33267965]\n",
      " [ 0.02889496]\n",
      " [-0.25827853]]\n",
      "Current iteration=8, loss=36936.11988598644\n",
      "t [[-0.00229789]\n",
      " [-0.31355655]\n",
      " [-0.4856775 ]\n",
      " ...\n",
      " [-0.35395796]\n",
      " [ 0.02479289]\n",
      " [-0.28236398]]\n",
      "t [[-0.00229789]\n",
      " [-0.31355655]\n",
      " [-0.4856775 ]\n",
      " ...\n",
      " [-0.35395796]\n",
      " [ 0.02479289]\n",
      " [-0.28236398]]\n",
      "t [[-0.00527778]\n",
      " [-0.34951139]\n",
      " [-0.51479482]\n",
      " ...\n",
      " [-0.37320719]\n",
      " [ 0.01982592]\n",
      " [-0.30519176]]\n",
      "loss=36502.98794660499\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00398454]\n",
      " [-0.02520866]\n",
      " [-0.09102355]\n",
      " ...\n",
      " [-0.06794552]\n",
      " [ 0.01444388]\n",
      " [-0.0384524 ]]\n",
      "t [[ 0.00398454]\n",
      " [-0.02520866]\n",
      " [-0.09102355]\n",
      " ...\n",
      " [-0.06794552]\n",
      " [ 0.01444388]\n",
      " [-0.0384524 ]]\n",
      "t [[ 0.00592945]\n",
      " [-0.05371678]\n",
      " [-0.16827524]\n",
      " ...\n",
      " [-0.12455915]\n",
      " [ 0.02489137]\n",
      " [-0.07385581]]\n",
      "t [[ 0.00592945]\n",
      " [-0.05371678]\n",
      " [-0.16827524]\n",
      " ...\n",
      " [-0.12455915]\n",
      " [ 0.02489137]\n",
      " [-0.07385581]]\n",
      "Current iteration=2, loss=39028.34132350592\n",
      "t [[ 0.00631631]\n",
      " [-0.08456925]\n",
      " [-0.23445231]\n",
      " ...\n",
      " [-0.17211079]\n",
      " [ 0.03205912]\n",
      " [-0.10660639]]\n",
      "t [[ 0.00631631]\n",
      " [-0.08456925]\n",
      " [-0.23445231]\n",
      " ...\n",
      " [-0.17211079]\n",
      " [ 0.03205912]\n",
      " [-0.10660639]]\n",
      "t [[ 0.00553101]\n",
      " [-0.11700989]\n",
      " [-0.29173802]\n",
      " ...\n",
      " [-0.21243652]\n",
      " [ 0.0365388 ]\n",
      " [-0.13703965]]\n",
      "t [[ 0.00553101]\n",
      " [-0.11700989]\n",
      " [-0.29173802]\n",
      " ...\n",
      " [-0.21243652]\n",
      " [ 0.0365388 ]\n",
      " [-0.13703965]]\n",
      "Current iteration=4, loss=38151.70942214012\n",
      "t [[ 0.0038746 ]\n",
      " [-0.15045394]\n",
      " [-0.34186423]\n",
      " ...\n",
      " [-0.24699207]\n",
      " [ 0.03880745]\n",
      " [-0.16543488]]\n",
      "t [[ 0.0038746 ]\n",
      " [-0.15045394]\n",
      " [-0.34186423]\n",
      " ...\n",
      " [-0.24699207]\n",
      " [ 0.03880745]\n",
      " [-0.16543488]]\n",
      "t [[ 0.00157944]\n",
      " [-0.18445425]\n",
      " [-0.38619556]\n",
      " ...\n",
      " [-0.27692373]\n",
      " [ 0.03924732]\n",
      " [-0.19202391]]\n",
      "t [[ 0.00157944]\n",
      " [-0.18445425]\n",
      " [-0.38619556]\n",
      " ...\n",
      " [-0.27692373]\n",
      " [ 0.03924732]\n",
      " [-0.19202391]]\n",
      "Current iteration=6, loss=37497.51805566284\n",
      "t [[-0.00117568]\n",
      " [-0.21867098]\n",
      " [-0.42580749]\n",
      " ...\n",
      " [-0.30313383]\n",
      " [ 0.03816557]\n",
      " [-0.21699997]]\n",
      "t [[-0.00117568]\n",
      " [-0.21867098]\n",
      " [-0.42580749]\n",
      " ...\n",
      " [-0.30313383]\n",
      " [ 0.03816557]\n",
      " [-0.21699997]]\n",
      "t [[-0.00425312]\n",
      " [-0.25284685]\n",
      " [-0.46155038]\n",
      " ...\n",
      " [-0.32633453]\n",
      " [ 0.03581073]\n",
      " [-0.24052541]]\n",
      "t [[-0.00425312]\n",
      " [-0.25284685]\n",
      " [-0.46155038]\n",
      " ...\n",
      " [-0.32633453]\n",
      " [ 0.03581073]\n",
      " [-0.24052541]]\n",
      "Current iteration=8, loss=36979.59592354839\n",
      "t [[-0.00754685]\n",
      " [-0.28678807]\n",
      " [-0.49409966]\n",
      " ...\n",
      " [-0.34709003]\n",
      " [ 0.03238563]\n",
      " [-0.26273785]]\n",
      "t [[-0.00754685]\n",
      " [-0.28678807]\n",
      " [-0.49409966]\n",
      " ...\n",
      " [-0.34709003]\n",
      " [ 0.03238563]\n",
      " [-0.26273785]]\n",
      "t [[-0.01097515]\n",
      " [-0.32034966]\n",
      " [-0.52399445]\n",
      " ...\n",
      " [-0.36584913]\n",
      " [ 0.02805741]\n",
      " [-0.28375501]]\n",
      "loss=36554.08209786218\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00331832]\n",
      " [-0.02670032]\n",
      " [-0.09042295]\n",
      " ...\n",
      " [-0.0398109 ]\n",
      " [-0.00479472]\n",
      " [-0.08122068]]\n",
      "t [[ 0.00331832]\n",
      " [-0.02670032]\n",
      " [-0.09042295]\n",
      " ...\n",
      " [-0.0398109 ]\n",
      " [-0.00479472]\n",
      " [-0.08122068]]\n",
      "t [[ 0.00463729]\n",
      " [-0.05667727]\n",
      " [-0.16695954]\n",
      " ...\n",
      " [-0.07072212]\n",
      " [-0.00664645]\n",
      " [-0.15725902]]\n",
      "t [[ 0.00463729]\n",
      " [-0.05667727]\n",
      " [-0.16695954]\n",
      " ...\n",
      " [-0.07072212]\n",
      " [-0.00664645]\n",
      " [-0.15725902]]\n",
      "Current iteration=2, loss=39005.92278513576\n",
      "t [[ 0.00443634]\n",
      " [-0.0889695 ]\n",
      " [-0.23234281]\n",
      " ...\n",
      " [-0.09465301]\n",
      " [-0.00633105]\n",
      " [-0.22870002]]\n",
      "t [[ 0.00443634]\n",
      " [-0.0889695 ]\n",
      " [-0.23234281]\n",
      " ...\n",
      " [-0.09465301]\n",
      " [-0.00633105]\n",
      " [-0.22870002]]\n",
      "t [[ 0.00309889]\n",
      " [-0.12281656]\n",
      " [-0.28878378]\n",
      " ...\n",
      " [-0.11314392]\n",
      " [-0.00445835]\n",
      " [-0.29605099]]\n",
      "t [[ 0.00309889]\n",
      " [-0.12281656]\n",
      " [-0.28878378]\n",
      " ...\n",
      " [-0.11314392]\n",
      " [-0.00445835]\n",
      " [-0.29605099]]\n",
      "Current iteration=4, loss=38114.56008580873\n",
      "t [[ 0.00092355]\n",
      " [-0.15763123]\n",
      " [-0.33803498]\n",
      " ...\n",
      " [-0.12740583]\n",
      " [-0.00149773]\n",
      " [-0.35974542]]\n",
      "t [[ 0.00092355]\n",
      " [-0.15763123]\n",
      " [-0.33803498]\n",
      " ...\n",
      " [-0.12740583]\n",
      " [-0.00149773]\n",
      " [-0.35974542]]\n",
      "t [[-0.00185972]\n",
      " [-0.19296534]\n",
      " [-0.3814762 ]\n",
      " ...\n",
      " [-0.13838356]\n",
      " [ 0.00219322]\n",
      " [-0.42015242]]\n",
      "t [[-0.00185972]\n",
      " [-0.19296534]\n",
      " [-0.3814762 ]\n",
      " ...\n",
      " [-0.13838356]\n",
      " [ 0.00219322]\n",
      " [-0.42015242]]\n",
      "Current iteration=6, loss=37449.30930303282\n",
      "t [[-0.00507438]\n",
      " [-0.22847882]\n",
      " [-0.4201939 ]\n",
      " ...\n",
      " [-0.14681327]\n",
      " [ 0.00634376]\n",
      " [-0.47758705]]\n",
      "t [[-0.00507438]\n",
      " [-0.22847882]\n",
      " [-0.4201939 ]\n",
      " ...\n",
      " [-0.14681327]\n",
      " [ 0.00634376]\n",
      " [-0.47758705]]\n",
      "t [[-0.00858498]\n",
      " [-0.26391475]\n",
      " [-0.45504642]\n",
      " ...\n",
      " [-0.15326932]\n",
      " [ 0.01074985]\n",
      " [-0.5323196 ]]\n",
      "t [[-0.00858498]\n",
      " [-0.26391475]\n",
      " [-0.45504642]\n",
      " ...\n",
      " [-0.15326932]\n",
      " [ 0.01074985]\n",
      " [-0.5323196 ]]\n",
      "Current iteration=8, loss=36922.37787938795\n",
      "t [[-0.0122875 ]\n",
      " [-0.29907998]\n",
      " [-0.48671493]\n",
      " ...\n",
      " [-0.1582009 ]\n",
      " [ 0.0152587 ]\n",
      " [-0.58458349]]\n",
      "t [[-0.0122875 ]\n",
      " [-0.29907998]\n",
      " [-0.48671493]\n",
      " ...\n",
      " [-0.1582009 ]\n",
      " [ 0.0152587 ]\n",
      " [-0.58458349]]\n",
      "t [[-0.0161021 ]\n",
      " [-0.33383035]\n",
      " [-0.51574268]\n",
      " ...\n",
      " [-0.16196005]\n",
      " [ 0.01975682]\n",
      " [-0.63458162]]\n",
      "loss=36489.20222443329\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.03809698]\n",
      " [-0.16274592]\n",
      " [-0.10229167]\n",
      " ...\n",
      " [-0.0741361 ]\n",
      " [ 0.01605087]\n",
      " [-0.04321745]]\n",
      "t [[ 0.03809698]\n",
      " [-0.16274592]\n",
      " [-0.10229167]\n",
      " ...\n",
      " [-0.0741361 ]\n",
      " [ 0.01605087]\n",
      " [-0.04321745]]\n",
      "t [[ 0.06928674]\n",
      " [-0.30177574]\n",
      " [-0.18899368]\n",
      " ...\n",
      " [-0.13523382]\n",
      " [ 0.0274033 ]\n",
      " [-0.08285767]]\n",
      "t [[ 0.06928674]\n",
      " [-0.30177574]\n",
      " [-0.18899368]\n",
      " ...\n",
      " [-0.13523382]\n",
      " [ 0.0274033 ]\n",
      " [-0.08285767]]\n",
      "Current iteration=2, loss=38946.43641628859\n",
      "t [[ 0.09490179]\n",
      " [-0.42166372]\n",
      " [-0.26328383]\n",
      " ...\n",
      " [-0.18608559]\n",
      " [ 0.03495361]\n",
      " [-0.11941114]]\n",
      "t [[ 0.09490179]\n",
      " [-0.42166372]\n",
      " [-0.26328383]\n",
      " ...\n",
      " [-0.18608559]\n",
      " [ 0.03495361]\n",
      " [-0.11941114]]\n",
      "t [[ 0.11602212]\n",
      " [-0.52608136]\n",
      " [-0.32769237]\n",
      " ...\n",
      " [-0.22890604]\n",
      " [ 0.03942994]\n",
      " [-0.15328736]]\n",
      "t [[ 0.11602212]\n",
      " [-0.52608136]\n",
      " [-0.32769237]\n",
      " ...\n",
      " [-0.22890604]\n",
      " [ 0.03942994]\n",
      " [-0.15328736]]\n",
      "Current iteration=4, loss=38035.431422331116\n",
      "t [[ 0.13350627]\n",
      " [-0.61791516]\n",
      " [-0.38419119]\n",
      " ...\n",
      " [-0.26541534]\n",
      " [ 0.04140939]\n",
      " [-0.18482256]]\n",
      "t [[ 0.13350627]\n",
      " [-0.61791516]\n",
      " [-0.38419119]\n",
      " ...\n",
      " [-0.26541534]\n",
      " [ 0.04140939]\n",
      " [-0.18482256]]\n",
      "t [[ 0.14803358]\n",
      " [-0.69942463]\n",
      " [-0.43430889]\n",
      " ...\n",
      " [-0.29694176]\n",
      " [ 0.0413473 ]\n",
      " [-0.21429252]]\n",
      "t [[ 0.14803358]\n",
      " [-0.69942463]\n",
      " [-0.43430889]\n",
      " ...\n",
      " [-0.29694176]\n",
      " [ 0.0413473 ]\n",
      " [-0.21429252]]\n",
      "Current iteration=6, loss=37363.910771821145\n",
      "t [[ 0.1601427 ]\n",
      " [-0.77238436]\n",
      " [-0.47923311]\n",
      " ...\n",
      " [-0.32451197]\n",
      " [ 0.03960478]\n",
      " [-0.24192488]]\n",
      "t [[ 0.1601427 ]\n",
      " [-0.77238436]\n",
      " [-0.47923311]\n",
      " ...\n",
      " [-0.32451197]\n",
      " [ 0.03960478]\n",
      " [-0.24192488]]\n",
      "t [[ 0.17026296]\n",
      " [-0.83819749]\n",
      " [-0.51989162]\n",
      " ...\n",
      " [-0.3489226 ]\n",
      " [ 0.03647086]\n",
      " [-0.26790942]]\n",
      "t [[ 0.17026296]\n",
      " [-0.83819749]\n",
      " [-0.51989162]\n",
      " ...\n",
      " [-0.3489226 ]\n",
      " [ 0.03647086]\n",
      " [-0.26790942]]\n",
      "Current iteration=8, loss=36836.390490615784\n",
      "t [[ 0.17873861]\n",
      " [-0.89798231]\n",
      " [-0.55701389]\n",
      " ...\n",
      " [-0.3707947 ]\n",
      " [ 0.0321793 ]\n",
      " [-0.29240597]]\n",
      "t [[ 0.17873861]\n",
      " [-0.89798231]\n",
      " [-0.55701389]\n",
      " ...\n",
      " [-0.3707947 ]\n",
      " [ 0.0321793 ]\n",
      " [-0.29240597]]\n",
      "t [[ 0.18584745]\n",
      " [-0.95263737]\n",
      " [-0.59117732]\n",
      " ...\n",
      " [-0.39061472]\n",
      " [ 0.02692118]\n",
      " [-0.31555056]]\n",
      "loss=36405.74836277898\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00502399]\n",
      " [-0.03093201]\n",
      " [-0.09697509]\n",
      " ...\n",
      " [-0.07441658]\n",
      " [ 0.01446709]\n",
      " [-0.04414722]]\n",
      "t [[ 0.00502399]\n",
      " [-0.03093201]\n",
      " [-0.09697509]\n",
      " ...\n",
      " [-0.07441658]\n",
      " [ 0.01446709]\n",
      " [-0.04414722]]\n",
      "t [[ 0.00766008]\n",
      " [-0.06551927]\n",
      " [-0.17795036]\n",
      " ...\n",
      " [-0.13551356]\n",
      " [ 0.02434614]\n",
      " [-0.08463607]]\n",
      "t [[ 0.00766008]\n",
      " [-0.06551927]\n",
      " [-0.17795036]\n",
      " ...\n",
      " [-0.13551356]\n",
      " [ 0.02434614]\n",
      " [-0.08463607]]\n",
      "Current iteration=2, loss=38930.622485550324\n",
      "t [[ 0.0085072 ]\n",
      " [-0.10257177]\n",
      " [-0.24633138]\n",
      " ...\n",
      " [-0.18617438]\n",
      " [ 0.0305369 ]\n",
      " [-0.12197251]]\n",
      "t [[ 0.0085072 ]\n",
      " [-0.10257177]\n",
      " [-0.24633138]\n",
      " ...\n",
      " [-0.18617438]\n",
      " [ 0.0305369 ]\n",
      " [-0.12197251]]\n",
      "t [[ 0.00803443]\n",
      " [-0.14117262]\n",
      " [-0.30481129]\n",
      " ...\n",
      " [-0.22867833]\n",
      " [ 0.03376617]\n",
      " [-0.15657783]]\n",
      "t [[ 0.00803443]\n",
      " [-0.14117262]\n",
      " [-0.30481129]\n",
      " ...\n",
      " [-0.22867833]\n",
      " [ 0.03376617]\n",
      " [-0.15657783]]\n",
      "Current iteration=4, loss=38008.24800785993\n",
      "t [[ 0.00659898]\n",
      " [-0.18063374]\n",
      " [-0.35547499]\n",
      " ...\n",
      " [-0.26479003]\n",
      " [ 0.03460656]\n",
      " [-0.18879698]]\n",
      "t [[ 0.00659898]\n",
      " [-0.18063374]\n",
      " [-0.35547499]\n",
      " ...\n",
      " [-0.26479003]\n",
      " [ 0.03460656]\n",
      " [-0.18879698]]\n",
      "t [[ 0.00447019]\n",
      " [-0.22044492]\n",
      " [-0.39992778]\n",
      " ...\n",
      " [-0.29586792]\n",
      " [ 0.03350724]\n",
      " [-0.21891228]]\n",
      "t [[ 0.00447019]\n",
      " [-0.22044492]\n",
      " [-0.39992778]\n",
      " ...\n",
      " [-0.29586792]\n",
      " [ 0.03350724]\n",
      " [-0.21891228]]\n",
      "Current iteration=6, loss=37326.87013307461\n",
      "t [[ 0.0018509 ]\n",
      " [-0.26023036]\n",
      " [-0.43940793]\n",
      " ...\n",
      " [-0.32295912]\n",
      " [ 0.03082239]\n",
      " [-0.24715637]]\n",
      "t [[ 0.0018509 ]\n",
      " [-0.26023036]\n",
      " [-0.43940793]\n",
      " ...\n",
      " [-0.32295912]\n",
      " [ 0.03082239]\n",
      " [-0.24715637]]\n",
      "t [[-0.00110588]\n",
      " [-0.29971485]\n",
      " [-0.47487519]\n",
      " ...\n",
      " [-0.34687414]\n",
      " [ 0.02683395]\n",
      " [-0.27372289]]\n",
      "t [[-0.00110588]\n",
      " [-0.29971485]\n",
      " [-0.47487519]\n",
      " ...\n",
      " [-0.34687414]\n",
      " [ 0.02683395]\n",
      " [-0.27372289]]\n",
      "Current iteration=8, loss=36790.55922020253\n",
      "t [[-0.00428452]\n",
      " [-0.33869852]\n",
      " [-0.50707772]\n",
      " ...\n",
      " [-0.36824341]\n",
      " [ 0.02176878]\n",
      " [-0.29877478]]\n",
      "t [[-0.00428452]\n",
      " [-0.33869852]\n",
      " [-0.50707772]\n",
      " ...\n",
      " [-0.36824341]\n",
      " [ 0.02176878]\n",
      " [-0.29877478]]\n",
      "t [[-0.00759755]\n",
      " [-0.37703803]\n",
      " [-0.53660213]\n",
      " ...\n",
      " [-0.38755971]\n",
      " [ 0.01581134]\n",
      " [-0.32245059]]\n",
      "loss=36352.16017677187\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00429104]\n",
      " [-0.02714779]\n",
      " [-0.09802536]\n",
      " ...\n",
      " [-0.0731721 ]\n",
      " [ 0.01555495]\n",
      " [-0.04141028]]\n",
      "t [[ 0.00429104]\n",
      " [-0.02714779]\n",
      " [-0.09802536]\n",
      " ...\n",
      " [-0.0731721 ]\n",
      " [ 0.01555495]\n",
      " [-0.04141028]]\n",
      "t [[ 0.00621763]\n",
      " [-0.05812069]\n",
      " [-0.18008352]\n",
      " ...\n",
      " [-0.13320588]\n",
      " [ 0.02647673]\n",
      " [-0.07928531]]\n",
      "t [[ 0.00621763]\n",
      " [-0.05812069]\n",
      " [-0.18008352]\n",
      " ...\n",
      " [-0.13320588]\n",
      " [ 0.02647673]\n",
      " [-0.07928531]]\n",
      "Current iteration=2, loss=38945.614117218545\n",
      "t [[ 0.00638195]\n",
      " [-0.09172496]\n",
      " [-0.24954712]\n",
      " ...\n",
      " [-0.18293953]\n",
      " [ 0.03366182]\n",
      " [-0.11412074]]\n",
      "t [[ 0.00638195]\n",
      " [-0.09172496]\n",
      " [-0.24954712]\n",
      " ...\n",
      " [-0.18293953]\n",
      " [ 0.03366182]\n",
      " [-0.11412074]]\n",
      "t [[ 0.00525543]\n",
      " [-0.12703749]\n",
      " [-0.30908781]\n",
      " ...\n",
      " [-0.22461979]\n",
      " [ 0.03783522]\n",
      " [-0.14632905]]\n",
      "t [[ 0.00525543]\n",
      " [-0.12703749]\n",
      " [-0.30908781]\n",
      " ...\n",
      " [-0.22461979]\n",
      " [ 0.03783522]\n",
      " [-0.14632905]]\n",
      "Current iteration=4, loss=38035.04503874319\n",
      "t [[ 0.00319688]\n",
      " [-0.16336215]\n",
      " [-0.3607779 ]\n",
      " ...\n",
      " [-0.25998851]\n",
      " [ 0.03956922]\n",
      " [-0.17624755]]\n",
      "t [[ 0.00319688]\n",
      " [-0.16336215]\n",
      " [-0.3607779 ]\n",
      " ...\n",
      " [-0.25998851]\n",
      " [ 0.03956922]\n",
      " [-0.17624755]]\n",
      "t [[ 0.00047673]\n",
      " [-0.20017995]\n",
      " [-0.40621598]\n",
      " ...\n",
      " [-0.29038822]\n",
      " [ 0.03931364]\n",
      " [-0.20415189]]\n",
      "t [[ 0.00047673]\n",
      " [-0.20017995]\n",
      " [-0.40621598]\n",
      " ...\n",
      " [-0.29038822]\n",
      " [ 0.03931364]\n",
      " [-0.20415189]]\n",
      "Current iteration=6, loss=37363.86563680974\n",
      "t [[-0.0027016 ]\n",
      " [-0.23710623]\n",
      " [-0.44663736]\n",
      " ...\n",
      " [-0.31685483]\n",
      " [ 0.03742396]\n",
      " [-0.23026886]]\n",
      "t [[-0.0027016 ]\n",
      " [-0.23710623]\n",
      " [-0.44663736]\n",
      " ...\n",
      " [-0.31685483]\n",
      " [ 0.03742396]\n",
      " [-0.23026886]]\n",
      "t [[-0.00618495]\n",
      " [-0.27385725]\n",
      " [-0.48300114]\n",
      " ...\n",
      " [-0.34019077]\n",
      " [ 0.03418375]\n",
      " [-0.25478692]]\n",
      "t [[-0.00618495]\n",
      " [-0.27385725]\n",
      " [-0.48300114]\n",
      " ...\n",
      " [-0.34019077]\n",
      " [ 0.03418375]\n",
      " [-0.25478692]]\n",
      "Current iteration=8, loss=36836.520806562316\n",
      "t [[-0.0098579 ]\n",
      " [-0.31022511]\n",
      " [-0.51605616]\n",
      " ...\n",
      " [-0.36102053]\n",
      " [ 0.02982165]\n",
      " [-0.27786439]]\n",
      "t [[-0.0098579 ]\n",
      " [-0.31022511]\n",
      " [-0.51605616]\n",
      " ...\n",
      " [-0.36102053]\n",
      " [ 0.02982165]\n",
      " [-0.27786439]]\n",
      "t [[-0.01363345]\n",
      " [-0.34605901]\n",
      " [-0.5463905 ]\n",
      " ...\n",
      " [-0.37983246]\n",
      " [ 0.02452405]\n",
      " [-0.29963567]]\n",
      "loss=36405.993405690795\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00357357]\n",
      " [-0.02875419]\n",
      " [-0.09737856]\n",
      " ...\n",
      " [-0.04287328]\n",
      " [-0.00516355]\n",
      " [-0.08746843]]\n",
      "t [[ 0.00357357]\n",
      " [-0.02875419]\n",
      " [-0.09737856]\n",
      " ...\n",
      " [-0.04287328]\n",
      " [-0.00516355]\n",
      " [-0.08746843]]\n",
      "t [[ 0.0048294 ]\n",
      " [-0.06130695]\n",
      " [-0.17865724]\n",
      " ...\n",
      " [-0.07542826]\n",
      " [-0.0069149 ]\n",
      " [-0.16892782]]\n",
      "t [[ 0.0048294 ]\n",
      " [-0.06130695]\n",
      " [-0.17865724]\n",
      " ...\n",
      " [-0.07542826]\n",
      " [-0.0069149 ]\n",
      " [-0.16892782]]\n",
      "Current iteration=2, loss=38921.7871463639\n",
      "t [[ 0.00436693]\n",
      " [-0.09645674]\n",
      " [-0.24725345]\n",
      " ...\n",
      " [-0.10006446]\n",
      " [-0.00622358]\n",
      " [-0.24511045]]\n",
      "t [[ 0.00436693]\n",
      " [-0.09645674]\n",
      " [-0.24725345]\n",
      " ...\n",
      " [-0.10006446]\n",
      " [-0.00622358]\n",
      " [-0.24511045]]\n",
      "t [[ 0.00265454]\n",
      " [-0.13327539]\n",
      " [-0.30587259]\n",
      " ...\n",
      " [-0.11866518]\n",
      " [-0.00383374]\n",
      " [-0.31664181]]\n",
      "t [[ 0.00265454]\n",
      " [-0.13327539]\n",
      " [-0.30587259]\n",
      " ...\n",
      " [-0.11866518]\n",
      " [-0.00383374]\n",
      " [-0.31664181]]\n",
      "Current iteration=4, loss=37995.94006230878\n",
      "t [[ 4.79983240e-05]\n",
      " [-1.71064149e-01]\n",
      " [-3.56611335e-01]\n",
      " ...\n",
      " [-1.32676999e-01]\n",
      " [-3.03866916e-04]\n",
      " [-3.84048218e-01]]\n",
      "t [[ 4.79983240e-05]\n",
      " [-1.71064149e-01]\n",
      " [-3.56611335e-01]\n",
      " ...\n",
      " [-1.32676999e-01]\n",
      " [-3.03866916e-04]\n",
      " [-3.84048218e-01]]\n",
      "t [[-0.00318524]\n",
      " [-0.20930309]\n",
      " [-0.40108561]\n",
      " ...\n",
      " [-0.1432035 ]\n",
      " [ 0.00395131]\n",
      " [-0.44777168]]\n",
      "t [[-0.00318524]\n",
      " [-0.20930309]\n",
      " [-0.40108561]\n",
      " ...\n",
      " [-0.1432035 ]\n",
      " [ 0.00395131]\n",
      " [-0.44777168]]\n",
      "Current iteration=6, loss=37313.3687587694\n",
      "t [[-0.00684455]\n",
      " [-0.24760759]\n",
      " [-0.44054294]\n",
      " ...\n",
      " [-0.15108644]\n",
      " [ 0.00862576]\n",
      " [-0.50818495]]\n",
      "t [[-0.00684455]\n",
      " [-0.24760759]\n",
      " [-0.44054294]\n",
      " ...\n",
      " [-0.15108644]\n",
      " [ 0.00862576]\n",
      " [-0.50818495]]\n",
      "t [[-0.01077941]\n",
      " [-0.28569456]\n",
      " [-0.475951  ]\n",
      " ...\n",
      " [-0.15696933]\n",
      " [ 0.01349501]\n",
      " [-0.56560458]]\n",
      "t [[-0.01077941]\n",
      " [-0.28569456]\n",
      " [-0.475951  ]\n",
      " ...\n",
      " [-0.15696933]\n",
      " [ 0.01349501]\n",
      " [-0.56560458]]\n",
      "Current iteration=8, loss=36776.766793623334\n",
      "t [[-0.01487684]\n",
      " [-0.32335703]\n",
      " [-0.50806467]\n",
      " ...\n",
      " [-0.16134552]\n",
      " [ 0.01839576]\n",
      " [-0.62030154]]\n",
      "t [[-0.01487684]\n",
      " [-0.32335703]\n",
      " [-0.50806467]\n",
      " ...\n",
      " [-0.16134552]\n",
      " [ 0.01839576]\n",
      " [-0.62030154]]\n",
      "t [[-0.01905209]\n",
      " [-0.36044534]\n",
      " [-0.53747621]\n",
      " ...\n",
      " [-0.1645941 ]\n",
      " [ 0.02321072]\n",
      " [-0.67250964]]\n",
      "loss=36338.39423013731\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.04081819]\n",
      " [-0.17437063]\n",
      " [-0.10959822]\n",
      " ...\n",
      " [-0.07943154]\n",
      " [ 0.01719737]\n",
      " [-0.04630441]]\n",
      "t [[ 0.04081819]\n",
      " [-0.17437063]\n",
      " [-0.10959822]\n",
      " ...\n",
      " [-0.07943154]\n",
      " [ 0.01719737]\n",
      " [-0.04630441]]\n",
      "t [[ 0.07370967]\n",
      " [-0.32152499]\n",
      " [-0.20130611]\n",
      " ...\n",
      " [-0.14390038]\n",
      " [ 0.02900324]\n",
      " [-0.08850333]]\n",
      "t [[ 0.07370967]\n",
      " [-0.32152499]\n",
      " [-0.20130611]\n",
      " ...\n",
      " [-0.14390038]\n",
      " [ 0.02900324]\n",
      " [-0.08850333]]\n",
      "Current iteration=2, loss=38865.672229042335\n",
      "t [[ 0.10031534]\n",
      " [-0.44709556]\n",
      " [-0.27903615]\n",
      " ...\n",
      " [-0.19684421]\n",
      " [ 0.03652153]\n",
      " [-0.12720096]]\n",
      "t [[ 0.10031534]\n",
      " [-0.44709556]\n",
      " [-0.27903615]\n",
      " ...\n",
      " [-0.19684421]\n",
      " [ 0.03652153]\n",
      " [-0.12720096]]\n",
      "t [[ 0.12193863]\n",
      " [-0.55550832]\n",
      " [-0.3458362 ]\n",
      " ...\n",
      " [-0.24093006]\n",
      " [ 0.04063046]\n",
      " [-0.16289289]]\n",
      "t [[ 0.12193863]\n",
      " [-0.55550832]\n",
      " [-0.3458362 ]\n",
      " ...\n",
      " [-0.24093006]\n",
      " [ 0.04063046]\n",
      " [-0.16289289]]\n",
      "Current iteration=4, loss=37923.44881041585\n",
      "t [[ 0.13959475]\n",
      " [-0.65016905]\n",
      " [-0.40402913]\n",
      " ...\n",
      " [-0.27818328]\n",
      " [ 0.04201156]\n",
      " [-0.19597902]]\n",
      "t [[ 0.13959475]\n",
      " [-0.65016905]\n",
      " [-0.40402913]\n",
      " ...\n",
      " [-0.27818328]\n",
      " [ 0.04201156]\n",
      " [-0.19597902]]\n",
      "t [[ 0.15407187]\n",
      " [-0.73369183]\n",
      " [-0.45537877]\n",
      " ...\n",
      " [-0.31013485]\n",
      " [ 0.04119265]\n",
      " [-0.22678245]]\n",
      "t [[ 0.15407187]\n",
      " [-0.73369183]\n",
      " [-0.45537877]\n",
      " ...\n",
      " [-0.31013485]\n",
      " [ 0.04119265]\n",
      " [-0.22678245]]\n",
      "Current iteration=6, loss=37236.81208075559\n",
      "t [[ 0.16598435]\n",
      " [-0.80809412]\n",
      " [-0.50122998]\n",
      " ...\n",
      " [-0.33794505]\n",
      " [ 0.03858577]\n",
      " [-0.25556662]]\n",
      "t [[ 0.16598435]\n",
      " [-0.80809412]\n",
      " [-0.50122998]\n",
      " ...\n",
      " [-0.33794505]\n",
      " [ 0.03858577]\n",
      " [-0.25556662]]\n",
      "t [[ 0.17581423]\n",
      " [-0.87494638]\n",
      " [-0.54261542]\n",
      " ...\n",
      " [-0.36249767]\n",
      " [ 0.0345163 ]\n",
      " [-0.2825489 ]]\n",
      "t [[ 0.17581423]\n",
      " [-0.87494638]\n",
      " [-0.54261542]\n",
      " ...\n",
      " [-0.36249767]\n",
      " [ 0.0345163 ]\n",
      " [-0.2825489 ]]\n",
      "Current iteration=8, loss=36701.24840468094\n",
      "t [[ 0.18394253]\n",
      " [-0.93548287]\n",
      " [-0.58033425]\n",
      " ...\n",
      " [-0.38446974]\n",
      " [ 0.02924448]\n",
      " [-0.30791089]]\n",
      "t [[ 0.18394253]\n",
      " [-0.93548287]\n",
      " [-0.58033425]\n",
      " ...\n",
      " [-0.38446974]\n",
      " [ 0.02924448]\n",
      " [-0.30791089]]\n",
      "t [[ 0.19067266]\n",
      " [-0.99068288]\n",
      " [-0.61500974]\n",
      " ...\n",
      " [-0.40438262]\n",
      " [ 0.02298093]\n",
      " [-0.3318061 ]]\n",
      "loss=36266.719913450215\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00538284]\n",
      " [-0.03314144]\n",
      " [-0.10390188]\n",
      " ...\n",
      " [-0.07973205]\n",
      " [ 0.01550045]\n",
      " [-0.04730059]]\n",
      "t [[ 0.00538284]\n",
      " [-0.03314144]\n",
      " [-0.10390188]\n",
      " ...\n",
      " [-0.07973205]\n",
      " [ 0.01550045]\n",
      " [-0.04730059]]\n",
      "t [[ 0.00802578]\n",
      " [-0.07047709]\n",
      " [-0.18944279]\n",
      " ...\n",
      " [-0.14417878]\n",
      " [ 0.02573618]\n",
      " [-0.09040259]]\n",
      "t [[ 0.00802578]\n",
      " [-0.07047709]\n",
      " [-0.18944279]\n",
      " ...\n",
      " [-0.14417878]\n",
      " [ 0.02573618]\n",
      " [-0.09040259]]\n",
      "Current iteration=2, loss=38848.924471082966\n",
      "t [[ 0.00866614]\n",
      " [-0.11054259]\n",
      " [-0.26081524]\n",
      " ...\n",
      " [-0.19688981]\n",
      " [ 0.03181525]\n",
      " [-0.12992927]]\n",
      "t [[ 0.00866614]\n",
      " [-0.11054259]\n",
      " [-0.26081524]\n",
      " ...\n",
      " [-0.19688981]\n",
      " [ 0.03181525]\n",
      " [-0.12992927]]\n",
      "t [[ 0.00786774]\n",
      " [-0.1522372 ]\n",
      " [-0.32126129]\n",
      " ...\n",
      " [-0.24060911]\n",
      " [ 0.03461372]\n",
      " [-0.16639033]]\n",
      "t [[ 0.00786774]\n",
      " [-0.1522372 ]\n",
      " [-0.32126129]\n",
      " ...\n",
      " [-0.24060911]\n",
      " [ 0.03461372]\n",
      " [-0.16639033]]\n",
      "Current iteration=4, loss=37894.75830238659\n",
      "t [[ 0.00604929]\n",
      " [-0.19475621]\n",
      " [-0.37323404]\n",
      " ...\n",
      " [-0.27741305]\n",
      " [ 0.0348071 ]\n",
      " [-0.20019587]]\n",
      "t [[ 0.00604929]\n",
      " [-0.19475621]\n",
      " [-0.37323404]\n",
      " ...\n",
      " [-0.27741305]\n",
      " [ 0.0348071 ]\n",
      " [-0.20019587]]\n",
      "t [[ 0.0035191 ]\n",
      " [-0.23751837]\n",
      " [-0.4185824 ]\n",
      " ...\n",
      " [-0.30886614]\n",
      " [ 0.03291521]\n",
      " [-0.23167645]]\n",
      "t [[ 0.0035191 ]\n",
      " [-0.23751837]\n",
      " [-0.4185824 ]\n",
      " ...\n",
      " [-0.30886614]\n",
      " [ 0.03291521]\n",
      " [-0.23167645]]\n",
      "Current iteration=6, loss=37197.7679800568\n",
      "t [[ 0.00050417]\n",
      " [-0.28010678]\n",
      " [-0.45870481]\n",
      " ...\n",
      " [-0.33615061]\n",
      " [ 0.0293414 ]\n",
      " [-0.26110113]]\n",
      "t [[ 0.00050417]\n",
      " [-0.28010678]\n",
      " [-0.45870481]\n",
      " ...\n",
      " [-0.33615061]\n",
      " [ 0.0293414 ]\n",
      " [-0.26110113]]\n",
      "t [[-0.00282794]\n",
      " [-0.32222458]\n",
      " [-0.49466569]\n",
      " ...\n",
      " [-0.36016468]\n",
      " [ 0.02440233]\n",
      " [-0.28869161]]\n",
      "t [[-0.00282794]\n",
      " [-0.32222458]\n",
      " [-0.49466569]\n",
      " ...\n",
      " [-0.36016468]\n",
      " [ 0.02440233]\n",
      " [-0.28869161]]\n",
      "Current iteration=8, loss=36653.0527596096\n",
      "t [[-0.00635335]\n",
      " [-0.36366284]\n",
      " [-0.52728076]\n",
      " ...\n",
      " [-0.38159477]\n",
      " [ 0.01834985]\n",
      " [-0.3146329 ]]\n",
      "t [[-0.00635335]\n",
      " [-0.36366284]\n",
      " [-0.52728076]\n",
      " ...\n",
      " [-0.38159477]\n",
      " [ 0.01834985]\n",
      " [-0.3146329 ]]\n",
      "t [[-0.00998044]\n",
      " [-0.40427725]\n",
      " [-0.55717926]\n",
      " ...\n",
      " [-0.40096832]\n",
      " [ 0.01138668]\n",
      " [-0.3390813 ]]\n",
      "loss=36210.55424163175\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00459754]\n",
      " [-0.02908691]\n",
      " [-0.10502717]\n",
      " ...\n",
      " [-0.07839868]\n",
      " [ 0.01666601]\n",
      " [-0.04436816]]\n",
      "t [[ 0.00459754]\n",
      " [-0.02908691]\n",
      " [-0.10502717]\n",
      " ...\n",
      " [-0.07839868]\n",
      " [ 0.01666601]\n",
      " [-0.04436816]]\n",
      "t [[ 0.00648207]\n",
      " [-0.06256309]\n",
      " [-0.19173067]\n",
      " ...\n",
      " [-0.14172001]\n",
      " [ 0.02801543]\n",
      " [-0.08467904]]\n",
      "t [[ 0.00648207]\n",
      " [-0.06256309]\n",
      " [-0.19173067]\n",
      " ...\n",
      " [-0.14172001]\n",
      " [ 0.02801543]\n",
      " [-0.08467904]]\n",
      "Current iteration=2, loss=38864.86155798717\n",
      "t [[ 0.00639489]\n",
      " [-0.09895953]\n",
      " [-0.26426255]\n",
      " ...\n",
      " [-0.19345798]\n",
      " [ 0.0351523 ]\n",
      " [-0.12154323]]\n",
      "t [[ 0.00639489]\n",
      " [-0.09895953]\n",
      " [-0.26426255]\n",
      " ...\n",
      " [-0.19345798]\n",
      " [ 0.0351523 ]\n",
      " [-0.12154323]]\n",
      "t [[ 0.00490263]\n",
      " [-0.13716746]\n",
      " [-0.32583966]\n",
      " ...\n",
      " [-0.23631778]\n",
      " [ 0.03895069]\n",
      " [-0.15545967]]\n",
      "t [[ 0.00490263]\n",
      " [-0.13716746]\n",
      " [-0.32583966]\n",
      " ...\n",
      " [-0.23631778]\n",
      " [ 0.03895069]\n",
      " [-0.15545967]]\n",
      "Current iteration=4, loss=37923.12705474814\n",
      "t [[ 0.00242589]\n",
      " [-0.17637195]\n",
      " [-0.37890121]\n",
      " ...\n",
      " [-0.27234954]\n",
      " [ 0.04008596]\n",
      " [-0.18682926]]\n",
      "t [[ 0.00242589]\n",
      " [-0.17637195]\n",
      " [-0.37890121]\n",
      " ...\n",
      " [-0.27234954]\n",
      " [ 0.04008596]\n",
      " [-0.18682926]]\n",
      "t [[-0.00072583]\n",
      " [-0.21598082]\n",
      " [-0.42528932]\n",
      " ...\n",
      " [-0.30309994]\n",
      " [ 0.03907906]\n",
      " [-0.21597466]]\n",
      "t [[-0.00072583]\n",
      " [-0.21598082]\n",
      " [-0.42528932]\n",
      " ...\n",
      " [-0.30309994]\n",
      " [ 0.03907906]\n",
      " [-0.21597466]]\n",
      "Current iteration=6, loss=37236.81946052414\n",
      "t [[-0.00432499]\n",
      " [-0.25556629]\n",
      " [-0.46640001]\n",
      " ...\n",
      " [-0.32973894]\n",
      " [ 0.03633508]\n",
      " [-0.243158  ]]\n",
      "t [[-0.00432499]\n",
      " [-0.25556629]\n",
      " [-0.46640001]\n",
      " ...\n",
      " [-0.32973894]\n",
      " [ 0.03633508]\n",
      " [-0.243158  ]]\n",
      "t [[-0.00820402]\n",
      " [-0.29482123]\n",
      " [-0.50329773]\n",
      " ...\n",
      " [-0.35315607]\n",
      " [ 0.03217282]\n",
      " [-0.26859491]]\n",
      "t [[-0.00820402]\n",
      " [-0.29482123]\n",
      " [-0.50329773]\n",
      " ...\n",
      " [-0.35315607]\n",
      " [ 0.03217282]\n",
      " [-0.26859491]]\n",
      "Current iteration=8, loss=36701.41467418821\n",
      "t [[-0.01223946]\n",
      " [-0.33352712]\n",
      " [-0.53679964]\n",
      " ...\n",
      " [-0.37403151]\n",
      " [ 0.02684638]\n",
      " [-0.29246502]]\n",
      "t [[-0.01223946]\n",
      " [-0.33352712]\n",
      " [-0.53679964]\n",
      " ...\n",
      " [-0.37403151]\n",
      " [ 0.02684638]\n",
      " [-0.29246502]]\n",
      "t [[-0.01634043]\n",
      " [-0.37153084]\n",
      " [-0.5675372 ]\n",
      " ...\n",
      " [-0.39288806]\n",
      " [ 0.02056088]\n",
      " [-0.3149198 ]]\n",
      "loss=36267.00473664366\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00382883]\n",
      " [-0.03080806]\n",
      " [-0.10433417]\n",
      " ...\n",
      " [-0.04593565]\n",
      " [-0.00553237]\n",
      " [-0.09371617]]\n",
      "t [[ 0.00382883]\n",
      " [-0.03080806]\n",
      " [-0.10433417]\n",
      " ...\n",
      " [-0.04593565]\n",
      " [-0.00553237]\n",
      " [-0.09371617]]\n",
      "t [[ 0.00499822]\n",
      " [-0.06597482]\n",
      " [-0.19019251]\n",
      " ...\n",
      " [-0.08003029]\n",
      " [-0.00714889]\n",
      " [-0.18053575]]\n",
      "t [[ 0.00499822]\n",
      " [-0.06597482]\n",
      " [-0.19019251]\n",
      " ...\n",
      " [-0.08003029]\n",
      " [-0.00714889]\n",
      " [-0.18053575]]\n",
      "Current iteration=2, loss=38839.66381062587\n",
      "t [[ 0.00424615]\n",
      " [-0.10402172]\n",
      " [-0.26178216]\n",
      " ...\n",
      " [-0.10523743]\n",
      " [-0.00604242]\n",
      " [-0.26136124]]\n",
      "t [[ 0.00424615]\n",
      " [-0.10402172]\n",
      " [-0.26178216]\n",
      " ...\n",
      " [-0.10523743]\n",
      " [-0.00604242]\n",
      " [-0.26136124]]\n",
      "t [[ 0.00213545]\n",
      " [-0.14383412]\n",
      " [-0.32236032]\n",
      " ...\n",
      " [-0.12382275]\n",
      " [-0.00310647]\n",
      " [-0.33695158]]\n",
      "t [[ 0.00213545]\n",
      " [-0.14383412]\n",
      " [-0.32236032]\n",
      " ...\n",
      " [-0.12382275]\n",
      " [-0.00310647]\n",
      " [-0.33695158]]\n",
      "Current iteration=4, loss=37882.14457331037\n",
      "t [[-0.000917  ]\n",
      " [-0.18459436]\n",
      " [-0.37439444]\n",
      " ...\n",
      " [-0.13748592]\n",
      " [ 0.00100546]\n",
      " [-0.40793562]]\n",
      "t [[-0.000917  ]\n",
      " [-0.18459436]\n",
      " [-0.37439444]\n",
      " ...\n",
      " [-0.13748592]\n",
      " [ 0.00100546]\n",
      " [-0.40793562]]\n",
      "t [[-0.00460532]\n",
      " [-0.22570949]\n",
      " [-0.41974612]\n",
      " ...\n",
      " [-0.14749472]\n",
      " [ 0.00582055]\n",
      " [-0.4748343 ]]\n",
      "t [[-0.00460532]\n",
      " [-0.22570949]\n",
      " [-0.41974612]\n",
      " ...\n",
      " [-0.14749472]\n",
      " [ 0.00582055]\n",
      " [-0.4748343 ]]\n",
      "Current iteration=6, loss=37184.13302875486\n",
      "t [[-0.00870535]\n",
      " [-0.26675211]\n",
      " [-0.45982468]\n",
      " ...\n",
      " [-0.15479577]\n",
      " [ 0.01099903]\n",
      " [-0.5380819 ]]\n",
      "t [[-0.00870535]\n",
      " [-0.26675211]\n",
      " [-0.45982468]\n",
      " ...\n",
      " [-0.15479577]\n",
      " [ 0.01099903]\n",
      " [-0.5380819 ]]\n",
      "t [[-0.01305271]\n",
      " [-0.30741606]\n",
      " [-0.49570368]\n",
      " ...\n",
      " [-0.16009809]\n",
      " [ 0.01629858]\n",
      " [-0.59804368]]\n",
      "t [[-0.01305271]\n",
      " [-0.30741606]\n",
      " [-0.49570368]\n",
      " ...\n",
      " [-0.16009809]\n",
      " [ 0.01629858]\n",
      " [-0.59804368]]\n",
      "Current iteration=8, loss=36639.240286746135\n",
      "t [[-0.01752681]\n",
      " [-0.34748414]\n",
      " [-0.52820646]\n",
      " ...\n",
      " [-0.1639344 ]\n",
      " [ 0.02154841]\n",
      " [-0.65502974]]\n",
      "t [[-0.01752681]\n",
      " [-0.34748414]\n",
      " [-0.52820646]\n",
      " ...\n",
      " [-0.1639344 ]\n",
      " [ 0.02154841]\n",
      " [-0.65502974]]\n",
      "t [[-0.02203944]\n",
      " [-0.38680468]\n",
      " [-0.55796862]\n",
      " ...\n",
      " [-0.16670586]\n",
      " [ 0.02663035]\n",
      " [-0.70930585]]\n",
      "loss=36196.82192645794\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0435394 ]\n",
      " [-0.18599533]\n",
      " [-0.11690476]\n",
      " ...\n",
      " [-0.08472697]\n",
      " [ 0.01834386]\n",
      " [-0.04939137]]\n",
      "t [[ 0.0435394 ]\n",
      " [-0.18599533]\n",
      " [-0.11690476]\n",
      " ...\n",
      " [-0.08472697]\n",
      " [ 0.01834386]\n",
      " [-0.04939137]]\n",
      "t [[ 0.07806299]\n",
      " [-0.34103535]\n",
      " [-0.21346155]\n",
      " ...\n",
      " [-0.15243559]\n",
      " [ 0.03055597]\n",
      " [-0.09411284]]\n",
      "t [[ 0.07806299]\n",
      " [-0.34103535]\n",
      " [-0.21346155]\n",
      " ...\n",
      " [-0.15243559]\n",
      " [ 0.03055597]\n",
      " [-0.09411284]]\n",
      "Current iteration=2, loss=38786.81834698847\n",
      "t [[ 0.10556421]\n",
      " [-0.47196196]\n",
      " [-0.2944224 ]\n",
      " ...\n",
      " [-0.20730087]\n",
      " [ 0.03797753]\n",
      " [-0.13489867]]\n",
      "t [[ 0.10556421]\n",
      " [-0.47196196]\n",
      " [-0.2944224 ]\n",
      " ...\n",
      " [-0.20730087]\n",
      " [ 0.03797753]\n",
      " [-0.13489867]]\n",
      "t [[ 0.12759415]\n",
      " [-0.58403571]\n",
      " [-0.36340701]\n",
      " ...\n",
      " [-0.25248954]\n",
      " [ 0.04165278]\n",
      " [-0.17234028]]\n",
      "t [[ 0.12759415]\n",
      " [-0.58403571]\n",
      " [-0.36340701]\n",
      " ...\n",
      " [-0.25248954]\n",
      " [ 0.04165278]\n",
      " [-0.17234028]]\n",
      "Current iteration=4, loss=37815.89158599448\n",
      "t [[ 0.1453362 ]\n",
      " [-0.68121796]\n",
      " [-0.42311268]\n",
      " ...\n",
      " [-0.29035213]\n",
      " [ 0.0423752 ]\n",
      " [-0.20690664]]\n",
      "t [[ 0.1453362 ]\n",
      " [-0.68121796]\n",
      " [-0.42311268]\n",
      " ...\n",
      " [-0.29035213]\n",
      " [ 0.0423752 ]\n",
      " [-0.20690664]]\n",
      "t [[ 0.159692  ]\n",
      " [-0.7664905 ]\n",
      " [-0.47554577]\n",
      " ...\n",
      " [-0.32262806]\n",
      " [ 0.04074819]\n",
      " [-0.23897116]]\n",
      "t [[ 0.159692  ]\n",
      " [-0.7664905 ]\n",
      " [-0.47554577]\n",
      " ...\n",
      " [-0.32262806]\n",
      " [ 0.04074819]\n",
      " [-0.23897116]]\n",
      "Current iteration=6, loss=37115.76487690607\n",
      "t [[ 0.17135252]\n",
      " [-0.84211539]\n",
      " [-0.52220786]\n",
      " ...\n",
      " [-0.35060994]\n",
      " [ 0.03723563]\n",
      " [-0.26883474]]\n",
      "t [[ 0.17135252]\n",
      " [-0.84211539]\n",
      " [-0.52220786]\n",
      " ...\n",
      " [-0.35060994]\n",
      " [ 0.03723563]\n",
      " [-0.26883474]]\n",
      "t [[ 0.18085168]\n",
      " [-0.90982697]\n",
      " [-0.56423264]\n",
      " ...\n",
      " [-0.3752644 ]\n",
      " [ 0.03219928]\n",
      " [-0.29674329]]\n",
      "t [[ 0.18085168]\n",
      " [-0.90982697]\n",
      " [-0.56423264]\n",
      " ...\n",
      " [-0.3752644 ]\n",
      " [ 0.03219928]\n",
      " [-0.29674329]]\n",
      "Current iteration=8, loss=36573.3548385044\n",
      "t [[ 0.18860561]\n",
      " [-0.97096998]\n",
      " [-0.60248382]\n",
      " ...\n",
      " [-0.39731906]\n",
      " [ 0.02592534]\n",
      " [-0.3229007 ]]\n",
      "t [[ 0.18860561]\n",
      " [-0.97096998]\n",
      " [-0.60248382]\n",
      " ...\n",
      " [-0.39731906]\n",
      " [ 0.02592534]\n",
      " [-0.3229007 ]]\n",
      "t [[ 0.19494137]\n",
      " [-1.0265985 ]\n",
      " [-0.63762529]\n",
      " ...\n",
      " [-0.41732482]\n",
      " [ 0.01864336]\n",
      " [-0.34747829]]\n",
      "loss=36135.938245711295\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0057417 ]\n",
      " [-0.03535087]\n",
      " [-0.11082867]\n",
      " ...\n",
      " [-0.08504752]\n",
      " [ 0.01653382]\n",
      " [-0.05045396]]\n",
      "t [[ 0.0057417 ]\n",
      " [-0.03535087]\n",
      " [-0.11082867]\n",
      " ...\n",
      " [-0.08504752]\n",
      " [ 0.01653382]\n",
      " [-0.05045396]]\n",
      "t [[ 0.00836757]\n",
      " [-0.07547154]\n",
      " [-0.20077411]\n",
      " ...\n",
      " [-0.15270985]\n",
      " [ 0.02708015]\n",
      " [-0.09613215]]\n",
      "t [[ 0.00836757]\n",
      " [-0.07547154]\n",
      " [-0.20077411]\n",
      " ...\n",
      " [-0.15270985]\n",
      " [ 0.02708015]\n",
      " [-0.09613215]]\n",
      "Current iteration=2, loss=38769.15560986976\n",
      "t [[ 0.00877305]\n",
      " [-0.11858437]\n",
      " [-0.27492806]\n",
      " ...\n",
      " [-0.20729783]\n",
      " [ 0.03298515]\n",
      " [-0.137792  ]]\n",
      "t [[ 0.00877305]\n",
      " [-0.11858437]\n",
      " [-0.27492806]\n",
      " ...\n",
      " [-0.20729783]\n",
      " [ 0.03298515]\n",
      " [-0.137792  ]]\n",
      "t [[ 0.0076262 ]\n",
      " [-0.16338656]\n",
      " [-0.33713841]\n",
      " ...\n",
      " [-0.25206838]\n",
      " [ 0.03528988]\n",
      " [-0.17604158]]\n",
      "t [[ 0.0076262 ]\n",
      " [-0.16338656]\n",
      " [-0.33713841]\n",
      " ...\n",
      " [-0.25206838]\n",
      " [ 0.03528988]\n",
      " [-0.17604158]]\n",
      "Current iteration=4, loss=37785.72020890881\n",
      "t [[ 0.00541072]\n",
      " [-0.20895076]\n",
      " [-0.39025022]\n",
      " ...\n",
      " [-0.28942973]\n",
      " [ 0.03477988]\n",
      " [-0.21136158]]\n",
      "t [[ 0.00541072]\n",
      " [-0.20895076]\n",
      " [-0.39025022]\n",
      " ...\n",
      " [-0.28942973]\n",
      " [ 0.03477988]\n",
      " [-0.21136158]]\n",
      "t [[ 0.00247436]\n",
      " [-0.2546248 ]\n",
      " [-0.43636252]\n",
      " ...\n",
      " [-0.32115791]\n",
      " [ 0.03204845]\n",
      " [-0.24413387]]\n",
      "t [[ 0.00247436]\n",
      " [-0.2546248 ]\n",
      " [-0.43636252]\n",
      " ...\n",
      " [-0.32115791]\n",
      " [ 0.03204845]\n",
      " [-0.24413387]]\n",
      "Current iteration=6, loss=37074.76103816781\n",
      "t [[-0.00093252]\n",
      " [-0.29995348]\n",
      " [-0.47703239]\n",
      " ...\n",
      " [-0.34856872]\n",
      " [ 0.0275488 ]\n",
      " [-0.27466556]]\n",
      "t [[-0.00093252]\n",
      " [-0.29995348]\n",
      " [-0.47703239]\n",
      " ...\n",
      " [-0.34856872]\n",
      " [ 0.0275488 ]\n",
      " [-0.27466556]]\n",
      "t [[-0.00462922]\n",
      " [-0.34462214]\n",
      " [-0.51342381]\n",
      " ...\n",
      " [-0.37264351]\n",
      " [ 0.02163216]\n",
      " [-0.30320734]]\n",
      "t [[-0.00462922]\n",
      " [-0.34462214]\n",
      " [-0.51342381]\n",
      " ...\n",
      " [-0.37264351]\n",
      " [ 0.02163216]\n",
      " [-0.30320734]]\n",
      "Current iteration=8, loss=36522.87340171368\n",
      "t [[-0.00848506]\n",
      " [-0.38841679]\n",
      " [-0.54641382]\n",
      " ...\n",
      " [-0.39411909]\n",
      " [ 0.01457474]\n",
      " [-0.32996685]]\n",
      "t [[-0.00848506]\n",
      " [-0.38841679]\n",
      " [-0.54641382]\n",
      " ...\n",
      " [-0.39411909]\n",
      " [ 0.01457474]\n",
      " [-0.32996685]]\n",
      "t [[-0.01240567]\n",
      " [-0.43119592]\n",
      " [-0.57666814]\n",
      " ...\n",
      " [-0.41355207]\n",
      " [ 0.00659681]\n",
      " [-0.35511843]]\n",
      "loss=36077.31502401449\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00490405]\n",
      " [-0.03102604]\n",
      " [-0.11202898]\n",
      " ...\n",
      " [-0.08362525]\n",
      " [ 0.01777708]\n",
      " [-0.04732603]]\n",
      "t [[ 0.00490405]\n",
      " [-0.03102604]\n",
      " [-0.11202898]\n",
      " ...\n",
      " [-0.08362525]\n",
      " [ 0.01777708]\n",
      " [-0.04732603]]\n",
      "t [[ 0.00672282]\n",
      " [-0.06704389]\n",
      " [-0.203217  ]\n",
      " ...\n",
      " [-0.1501018 ]\n",
      " [ 0.02950759]\n",
      " [-0.09003708]]\n",
      "t [[ 0.00672282]\n",
      " [-0.06704389]\n",
      " [-0.203217  ]\n",
      " ...\n",
      " [-0.1501018 ]\n",
      " [ 0.02950759]\n",
      " [-0.09003708]]\n",
      "Current iteration=2, loss=38786.02500345\n",
      "t [[ 0.00635659]\n",
      " [-0.10627016]\n",
      " [-0.27860676]\n",
      " ...\n",
      " [-0.20367299]\n",
      " [ 0.0365328 ]\n",
      " [-0.12887509]]\n",
      "t [[ 0.00635659]\n",
      " [-0.10627016]\n",
      " [-0.27860676]\n",
      " ...\n",
      " [-0.20367299]\n",
      " [ 0.0365328 ]\n",
      " [-0.12887509]]\n",
      "t [[ 0.00447672]\n",
      " [-0.14739188]\n",
      " [-0.34201705]\n",
      " ...\n",
      " [-0.24755021]\n",
      " [ 0.03989161]\n",
      " [-0.16443524]]\n",
      "t [[ 0.00447672]\n",
      " [-0.14739188]\n",
      " [-0.34201705]\n",
      " ...\n",
      " [-0.24755021]\n",
      " [ 0.03989161]\n",
      " [-0.16443524]]\n",
      "Current iteration=4, loss=37815.63204300476\n",
      "t [[ 0.0015691 ]\n",
      " [-0.18946898]\n",
      " [-0.3962779 ]\n",
      " ...\n",
      " [-0.28411191]\n",
      " [ 0.04036976]\n",
      " [-0.19718734]]\n",
      "t [[ 0.0015691 ]\n",
      " [-0.18946898]\n",
      " [-0.3962779 ]\n",
      " ...\n",
      " [-0.28411191]\n",
      " [ 0.04036976]\n",
      " [-0.19718734]]\n",
      "t [[-0.00201725]\n",
      " [-0.23183586]\n",
      " [-0.44348176]\n",
      " ...\n",
      " [-0.31511444]\n",
      " [ 0.03856217]\n",
      " [-0.22750392]]\n",
      "t [[-0.00201725]\n",
      " [-0.23183586]\n",
      " [-0.44348176]\n",
      " ...\n",
      " [-0.31511444]\n",
      " [ 0.03856217]\n",
      " [-0.22750392]]\n",
      "Current iteration=6, loss=37115.81706691708\n",
      "t [[-0.00603152]\n",
      " [-0.27402428]\n",
      " [-0.48518397]\n",
      " ...\n",
      " [-0.34186035]\n",
      " [ 0.03492436]\n",
      " [-0.25568405]]\n",
      "t [[-0.00603152]\n",
      " [-0.27402428]\n",
      " [-0.48518397]\n",
      " ...\n",
      " [-0.34186035]\n",
      " [ 0.03492436]\n",
      " [-0.25568405]]\n",
      "t [[-0.01029318]\n",
      " [-0.31570736]\n",
      " [-0.52254942]\n",
      " ...\n",
      " [-0.36532177]\n",
      " [ 0.02981023]\n",
      " [-0.2819714 ]]\n",
      "t [[-0.01029318]\n",
      " [-0.31570736]\n",
      " [-0.52254942]\n",
      " ...\n",
      " [-0.36532177]\n",
      " [ 0.02981023]\n",
      " [-0.2819714 ]]\n",
      "Current iteration=8, loss=36573.5526820923\n",
      "t [[-0.01467226]\n",
      " [-0.35665988]\n",
      " [-0.5564575 ]\n",
      " ...\n",
      " [-0.38622902]\n",
      " [ 0.02349883]\n",
      " [-0.30656741]]\n",
      "t [[-0.01467226]\n",
      " [-0.35665988]\n",
      " [-0.5564575 ]\n",
      " ...\n",
      " [-0.38622902]\n",
      " [ 0.02349883]\n",
      " [-0.30656741]]\n",
      "t [[-0.01907547]\n",
      " [-0.39673006]\n",
      " [-0.58757693]\n",
      " ...\n",
      " [-0.40513388]\n",
      " [ 0.01621328]\n",
      " [-0.32964087]]\n",
      "loss=36136.265219430155\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00408408]\n",
      " [-0.03286193]\n",
      " [-0.11128978]\n",
      " ...\n",
      " [-0.04899803]\n",
      " [-0.0059012 ]\n",
      " [-0.09996392]]\n",
      "t [[ 0.00408408]\n",
      " [-0.03286193]\n",
      " [-0.11128978]\n",
      " ...\n",
      " [-0.04899803]\n",
      " [-0.0059012 ]\n",
      " [-0.09996392]]\n",
      "t [[ 0.00514384]\n",
      " [-0.07068079]\n",
      " [-0.20156565]\n",
      " ...\n",
      " [-0.08452843]\n",
      " [-0.00734849]\n",
      " [-0.19208292]]\n",
      "t [[ 0.00514384]\n",
      " [-0.07068079]\n",
      " [-0.20156565]\n",
      " ...\n",
      " [-0.08452843]\n",
      " [-0.00734849]\n",
      " [-0.19208292]]\n",
      "Current iteration=2, loss=38759.49275226308\n",
      "t [[ 0.00407544]\n",
      " [-0.11166163]\n",
      " [-0.27593719]\n",
      " ...\n",
      " [-0.11017766]\n",
      " [-0.00578982]\n",
      " [-0.2774543 ]]\n",
      "t [[ 0.00407544]\n",
      " [-0.11166163]\n",
      " [-0.27593719]\n",
      " ...\n",
      " [-0.11017766]\n",
      " [-0.00578982]\n",
      " [-0.2774543 ]]\n",
      "t [[ 0.00154569]\n",
      " [-0.1544848 ]\n",
      " [-0.33827074]\n",
      " ...\n",
      " [-0.12863305]\n",
      " [-0.00228293]\n",
      " [-0.35698615]]\n",
      "t [[ 0.00154569]\n",
      " [-0.1544848 ]\n",
      " [-0.33827074]\n",
      " ...\n",
      " [-0.12863305]\n",
      " [-0.00228293]\n",
      " [-0.35698615]]\n",
      "Current iteration=4, loss=37772.84364409282\n",
      "t [[-0.00196405]\n",
      " [-0.19820746]\n",
      " [-0.39142854]\n",
      " ...\n",
      " [-0.14186293]\n",
      " [ 0.00241868]\n",
      " [-0.43141933]]\n",
      "t [[-0.00196405]\n",
      " [-0.19820746]\n",
      " [-0.39142854]\n",
      " ...\n",
      " [-0.14186293]\n",
      " [ 0.00241868]\n",
      " [-0.43141933]]\n",
      "t [[-0.00610909]\n",
      " [-0.24216354]\n",
      " [-0.43752458]\n",
      " ...\n",
      " [-0.15130264]\n",
      " [ 0.00778406]\n",
      " [-0.50135944]]\n",
      "t [[-0.00610909]\n",
      " [-0.24216354]\n",
      " [-0.43752458]\n",
      " ...\n",
      " [-0.15130264]\n",
      " [ 0.00778406]\n",
      " [-0.50135944]]\n",
      "Current iteration=6, loss=37061.03006522458\n",
      "t [[-0.01064269]\n",
      " [-0.28588554]\n",
      " [-0.4781285 ]\n",
      " ...\n",
      " [-0.15800124]\n",
      " [ 0.01344199]\n",
      " [-0.56730577]]\n",
      "t [[-0.01064269]\n",
      " [-0.28588554]\n",
      " [-0.4781285 ]\n",
      " ...\n",
      " [-0.15800124]\n",
      " [ 0.01344199]\n",
      " [-0.56730577]]\n",
      "t [[-0.01538805]\n",
      " [-0.32904798]\n",
      " [-0.51441466]\n",
      " ...\n",
      " [-0.16272844]\n",
      " [ 0.01913548]\n",
      " [-0.6296744 ]]\n",
      "t [[-0.01538805]\n",
      " [-0.32904798]\n",
      " [-0.51441466]\n",
      " ...\n",
      " [-0.16272844]\n",
      " [ 0.01913548]\n",
      " [-0.6296744 ]]\n",
      "Current iteration=8, loss=36509.06311508655\n",
      "t [[-0.02021859]\n",
      " [-0.37142732]\n",
      " [-0.54726866]\n",
      " ...\n",
      " [-0.16605085]\n",
      " [ 0.02468956]\n",
      " [-0.68881593]]\n",
      "t [[-0.02021859]\n",
      " [-0.37142732]\n",
      " [-0.54726866]\n",
      " ...\n",
      " [-0.16605085]\n",
      " [ 0.02468956]\n",
      " [-0.68881593]]\n",
      "t [[-0.02504409]\n",
      " [-0.41287361]\n",
      " [-0.57736323]\n",
      " ...\n",
      " [-0.16838634]\n",
      " [ 0.0299882 ]\n",
      " [-0.7450289 ]]\n",
      "loss=36063.62374955288\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.04626062]\n",
      " [-0.19762004]\n",
      " [-0.12421131]\n",
      " ...\n",
      " [-0.09002241]\n",
      " [ 0.01949035]\n",
      " [-0.05247833]]\n",
      "t [[ 0.04626062]\n",
      " [-0.19762004]\n",
      " [-0.12421131]\n",
      " ...\n",
      " [-0.09002241]\n",
      " [ 0.01949035]\n",
      " [-0.05247833]]\n",
      "t [[ 0.08234684]\n",
      " [-0.36030731]\n",
      " [-0.22546034]\n",
      " ...\n",
      " [-0.16083973]\n",
      " [ 0.03206164]\n",
      " [-0.09968627]]\n",
      "t [[ 0.08234684]\n",
      " [-0.36030731]\n",
      " [-0.22546034]\n",
      " ...\n",
      " [-0.16083973]\n",
      " [ 0.03206164]\n",
      " [-0.09968627]]\n",
      "Current iteration=2, loss=38709.81786626164\n",
      "t [[ 0.11065167]\n",
      " [-0.49627407]\n",
      " [-0.30945027]\n",
      " ...\n",
      " [-0.21746228]\n",
      " [ 0.03932384]\n",
      " [-0.14250552]]\n",
      "t [[ 0.11065167]\n",
      " [-0.49627407]\n",
      " [-0.30945027]\n",
      " ...\n",
      " [-0.21746228]\n",
      " [ 0.03932384]\n",
      " [-0.14250552]]\n",
      "t [[ 0.13299806]\n",
      " [-0.6116952 ]\n",
      " [-0.38042649]\n",
      " ...\n",
      " [-0.26360339]\n",
      " [ 0.04250319]\n",
      " [-0.18163315]]\n",
      "t [[ 0.13299806]\n",
      " [-0.6116952 ]\n",
      " [-0.38042649]\n",
      " ...\n",
      " [-0.26360339]\n",
      " [ 0.04250319]\n",
      " [-0.18163315]]\n",
      "Current iteration=4, loss=37712.464211257\n",
      "t [[ 0.1507482 ]\n",
      " [-0.71112037]\n",
      " [-0.44148154]\n",
      " ...\n",
      " [-0.30195647]\n",
      " [ 0.04251195]\n",
      " [-0.21761243]]\n",
      "t [[ 0.1507482 ]\n",
      " [-0.71112037]\n",
      " [-0.44148154]\n",
      " ...\n",
      " [-0.30195647]\n",
      " [ 0.04251195]\n",
      " [-0.21761243]]\n",
      "t [[ 0.1649207 ]\n",
      " [-0.79790873]\n",
      " [-0.49486908]\n",
      " ...\n",
      " [-0.33447274]\n",
      " [ 0.04003155]\n",
      " [-0.25086978]]\n",
      "t [[ 0.1649207 ]\n",
      " [-0.79790873]\n",
      " [-0.49486908]\n",
      " ...\n",
      " [-0.33447274]\n",
      " [ 0.04003155]\n",
      " [-0.25086978]]\n",
      "Current iteration=6, loss=37000.27454409757\n",
      "t [[ 0.17628328]\n",
      " [-0.87456607]\n",
      " [-0.54224509]\n",
      " ...\n",
      " [-0.36257408]\n",
      " [ 0.03557828]\n",
      " [-0.28174497]]\n",
      "t [[ 0.17628328]\n",
      " [-0.87456607]\n",
      " [-0.54224509]\n",
      " ...\n",
      " [-0.36257408]\n",
      " [ 0.03557828]\n",
      " [-0.28174497]]\n",
      "t [[ 0.18542028]\n",
      " [-0.94298543]\n",
      " [-0.58483906]\n",
      " ...\n",
      " [-0.38730452]\n",
      " [ 0.02954992]\n",
      " [-0.31051324]]\n",
      "t [[ 0.18542028]\n",
      " [-0.94298543]\n",
      " [-0.58483906]\n",
      " ...\n",
      " [-0.38730452]\n",
      " [ 0.02954992]\n",
      " [-0.31051324]]\n",
      "Current iteration=8, loss=36452.083881235674\n",
      "t [[ 0.19278091]\n",
      " [-1.00461546]\n",
      " [-0.62357343]\n",
      " ...\n",
      " [-0.40943619]\n",
      " [ 0.02225802]\n",
      " [-0.33740124]]\n",
      "t [[ 0.19278091]\n",
      " [-1.00461546]\n",
      " [-0.62357343]\n",
      " ...\n",
      " [-0.40943619]\n",
      " [ 0.02225802]\n",
      " [-0.33740124]]\n",
      "t [[ 0.19871372]\n",
      " [-1.06057854]\n",
      " [-0.65914712]\n",
      " ...\n",
      " [-0.42954383]\n",
      " [ 0.01395026]\n",
      " [-0.36259829]]\n",
      "loss=36012.67177605613\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00610056]\n",
      " [-0.0375603 ]\n",
      " [-0.11775546]\n",
      " ...\n",
      " [-0.09036299]\n",
      " [ 0.01756718]\n",
      " [-0.05360733]]\n",
      "t [[ 0.00610056]\n",
      " [-0.0375603 ]\n",
      " [-0.11775546]\n",
      " ...\n",
      " [-0.09036299]\n",
      " [ 0.01756718]\n",
      " [-0.05360733]]\n",
      "t [[ 0.00868552]\n",
      " [-0.08050254]\n",
      " [-0.21194468]\n",
      " ...\n",
      " [-0.16110706]\n",
      " [ 0.02837819]\n",
      " [-0.10182482]]\n",
      "t [[ 0.00868552]\n",
      " [-0.08050254]\n",
      " [-0.21194468]\n",
      " ...\n",
      " [-0.16110706]\n",
      " [ 0.02837819]\n",
      " [-0.10182482]]\n",
      "Current iteration=2, loss=38691.257821199666\n",
      "t [[ 0.00882939]\n",
      " [-0.12669435]\n",
      " [-0.28867803]\n",
      " ...\n",
      " [-0.21740536]\n",
      " [ 0.03404886]\n",
      " [-0.14556198]]\n",
      "t [[ 0.00882939]\n",
      " [-0.12669435]\n",
      " [-0.28867803]\n",
      " ...\n",
      " [-0.21740536]\n",
      " [ 0.03404886]\n",
      " [-0.14556198]]\n",
      "t [[ 0.00731379]\n",
      " [-0.17461309]\n",
      " [-0.35246565]\n",
      " ...\n",
      " [-0.26307557]\n",
      " [ 0.03580089]\n",
      " [-0.18553528]]\n",
      "t [[ 0.00731379]\n",
      " [-0.17461309]\n",
      " [-0.35246565]\n",
      " ...\n",
      " [-0.26307557]\n",
      " [ 0.03580089]\n",
      " [-0.18553528]]\n",
      "Current iteration=4, loss=37680.83581735801\n",
      "t [[ 0.00469031]\n",
      " [-0.22320404]\n",
      " [-0.40656534]\n",
      " ...\n",
      " [-0.30087545]\n",
      " [ 0.03453637]\n",
      " [-0.22230133]]\n",
      "t [[ 0.00469031]\n",
      " [-0.22320404]\n",
      " [-0.40656534]\n",
      " ...\n",
      " [-0.30087545]\n",
      " [ 0.03453637]\n",
      " [-0.22230133]]\n",
      "t [[ 0.00134615]\n",
      " [-0.27174527]\n",
      " [-0.4533301 ]\n",
      " ...\n",
      " [-0.33279564]\n",
      " [ 0.03092428]\n",
      " [-0.25629594]]\n",
      "t [[ 0.00134615]\n",
      " [-0.27174527]\n",
      " [-0.4533301 ]\n",
      " ...\n",
      " [-0.33279564]\n",
      " [ 0.03092428]\n",
      " [-0.25629594]]\n",
      "Current iteration=6, loss=36957.354081025325\n",
      "t [[-0.00244622]\n",
      " [-0.31974695]\n",
      " [-0.49447195]\n",
      " ...\n",
      " [-0.36028214]\n",
      " [ 0.02546797]\n",
      " [-0.28786573]]\n",
      "t [[-0.00244622]\n",
      " [-0.31974695]\n",
      " [-0.49447195]\n",
      " ...\n",
      " [-0.36028214]\n",
      " [ 0.02546797]\n",
      " [-0.28786573]]\n",
      "t [[-0.00649451]\n",
      " [-0.36688099]\n",
      " [-0.53124802]\n",
      " ...\n",
      " [-0.38439366]\n",
      " [ 0.01855273]\n",
      " [-0.31729117]]\n",
      "t [[-0.00649451]\n",
      " [-0.36688099]\n",
      " [-0.53124802]\n",
      " ...\n",
      " [-0.38439366]\n",
      " [ 0.01855273]\n",
      " [-0.31729117]]\n",
      "Current iteration=8, loss=36399.395049219136\n",
      "t [[-0.01066285]\n",
      " [-0.41293264]\n",
      " [-0.56458971]\n",
      " ...\n",
      " [-0.40591119]\n",
      " [ 0.0104784 ]\n",
      " [-0.34480295]]\n",
      "t [[-0.01066285]\n",
      " [-0.41293264]\n",
      " [-0.56458971]\n",
      " ...\n",
      " [-0.40591119]\n",
      " [ 0.0104784 ]\n",
      " [-0.34480295]]\n",
      "t [[-0.01485555]\n",
      " [-0.45776701]\n",
      " [-0.59519273]\n",
      " ...\n",
      " [-0.42541464]\n",
      " [ 0.0014819 ]\n",
      " [-0.37059369]]\n",
      "loss=35951.70838821185\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00521055]\n",
      " [-0.03296517]\n",
      " [-0.1190308 ]\n",
      " ...\n",
      " [-0.08885183]\n",
      " [ 0.01888815]\n",
      " [-0.05028391]]\n",
      "t [[ 0.00521055]\n",
      " [-0.03296517]\n",
      " [-0.1190308 ]\n",
      " ...\n",
      " [-0.08885183]\n",
      " [ 0.01888815]\n",
      " [-0.05028391]]\n",
      "t [[ 0.00693997]\n",
      " [-0.07156298]\n",
      " [-0.21454286]\n",
      " ...\n",
      " [-0.15835153]\n",
      " [ 0.03095333]\n",
      " [-0.09535946]]\n",
      "t [[ 0.00693997]\n",
      " [-0.07156298]\n",
      " [-0.21454286]\n",
      " ...\n",
      " [-0.15835153]\n",
      " [ 0.03095333]\n",
      " [-0.09535946]]\n",
      "Current iteration=2, loss=38709.04695571012\n",
      "t [[ 0.00626851]\n",
      " [-0.11365405]\n",
      " [-0.29258785]\n",
      " ...\n",
      " [-0.21359136]\n",
      " [ 0.03780553]\n",
      " [-0.13611757]]\n",
      "t [[ 0.00626851]\n",
      " [-0.11365405]\n",
      " [-0.29258785]\n",
      " ...\n",
      " [-0.21359136]\n",
      " [ 0.03780553]\n",
      " [-0.13611757]]\n",
      "t [[ 0.00398168]\n",
      " [-0.15770307]\n",
      " [-0.35764281]\n",
      " ...\n",
      " [-0.25833625]\n",
      " [ 0.04066423]\n",
      " [-0.1732594 ]]\n",
      "t [[ 0.00398168]\n",
      " [-0.15770307]\n",
      " [-0.35764281]\n",
      " ...\n",
      " [-0.25833625]\n",
      " [ 0.04066423]\n",
      " [-0.1732594 ]]\n",
      "Current iteration=4, loss=37712.2635808607\n",
      "t [[ 0.00063357]\n",
      " [-0.20263965]\n",
      " [-0.41294959]\n",
      " ...\n",
      " [-0.29531059]\n",
      " [ 0.04043211]\n",
      " [-0.20732878]]\n",
      "t [[ 0.00063357]\n",
      " [-0.20263965]\n",
      " [-0.41294959]\n",
      " ...\n",
      " [-0.29531059]\n",
      " [ 0.04043211]\n",
      " [-0.20732878]]\n",
      "t [[-0.00338732]\n",
      " [-0.24772571]\n",
      " [-0.46085507]\n",
      " ...\n",
      " [-0.32648357]\n",
      " [ 0.03778036]\n",
      " [-0.23875074]]\n",
      "t [[-0.00338732]\n",
      " [-0.24772571]\n",
      " [-0.46085507]\n",
      " ...\n",
      " [-0.32648357]\n",
      " [ 0.03778036]\n",
      " [-0.23875074]]\n",
      "Current iteration=6, loss=37000.364636122395\n",
      "t [[-0.00780819]\n",
      " [-0.29245598]\n",
      " [-0.50307044]\n",
      " ...\n",
      " [-0.35328704]\n",
      " [ 0.0332153 ]\n",
      " [-0.26786263]]\n",
      "t [[-0.00780819]\n",
      " [-0.29245598]\n",
      " [-0.50307044]\n",
      " ...\n",
      " [-0.35328704]\n",
      " [ 0.0332153 ]\n",
      " [-0.26786263]]\n",
      "t [[-0.01243721]\n",
      " [-0.33648807]\n",
      " [-0.54085478]\n",
      " ...\n",
      " [-0.3767701 ]\n",
      " [ 0.0271255 ]\n",
      " [-0.29493686]]\n",
      "t [[-0.01243721]\n",
      " [-0.33648807]\n",
      " [-0.54085478]\n",
      " ...\n",
      " [-0.3767701 ]\n",
      " [ 0.0271255 ]\n",
      " [-0.29493686]]\n",
      "Current iteration=8, loss=36452.31086024466\n",
      "t [[-0.01713956]\n",
      " [-0.37959423]\n",
      " [-0.57514281]\n",
      " ...\n",
      " [-0.39770699]\n",
      " [ 0.01981426]\n",
      " [-0.32019706]]\n",
      "t [[-0.01713956]\n",
      " [-0.37959423]\n",
      " [-0.57514281]\n",
      " ...\n",
      " [-0.39770699]\n",
      " [ 0.01981426]\n",
      " [-0.32019706]]\n",
      "t [[-0.02182104]\n",
      " [-0.42162782]\n",
      " [-0.60663412]\n",
      " ...\n",
      " [-0.41667267]\n",
      " [ 0.01152197]\n",
      " [-0.34382959]]\n",
      "loss=36013.04496644939\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00433934]\n",
      " [-0.0349158 ]\n",
      " [-0.11824539]\n",
      " ...\n",
      " [-0.05206041]\n",
      " [-0.00627002]\n",
      " [-0.10621166]]\n",
      "t [[ 0.00433934]\n",
      " [-0.0349158 ]\n",
      " [-0.11824539]\n",
      " ...\n",
      " [-0.05206041]\n",
      " [-0.00627002]\n",
      " [-0.10621166]]\n",
      "t [[ 0.00526631]\n",
      " [-0.07542475]\n",
      " [-0.21277702]\n",
      " ...\n",
      " [-0.0889229 ]\n",
      " [-0.00751376]\n",
      " [-0.20356939]]\n",
      "t [[ 0.00526631]\n",
      " [-0.07542475]\n",
      " [-0.21277702]\n",
      " ...\n",
      " [-0.0889229 ]\n",
      " [-0.00751376]\n",
      " [-0.20356939]]\n",
      "Current iteration=2, loss=38681.21512236082\n",
      "t [[ 0.00385626]\n",
      " [-0.11937364]\n",
      " [-0.28972677]\n",
      " ...\n",
      " [-0.11489084]\n",
      " [-0.00546799]\n",
      " [-0.2933915 ]]\n",
      "t [[ 0.00385626]\n",
      " [-0.11937364]\n",
      " [-0.28972677]\n",
      " ...\n",
      " [-0.11489084]\n",
      " [-0.00546799]\n",
      " [-0.2933915 ]]\n",
      "t [[ 0.00088921]\n",
      " [-0.16521971]\n",
      " [-0.35362695]\n",
      " ...\n",
      " [-0.13311202]\n",
      " [-0.00136926]\n",
      " [-0.37675121]]\n",
      "t [[ 0.00088921]\n",
      " [-0.16521971]\n",
      " [-0.35362695]\n",
      " ...\n",
      " [-0.13311202]\n",
      " [-0.00136926]\n",
      " [-0.37675121]]\n",
      "Current iteration=4, loss=37667.7349606106\n",
      "t [[-0.00308615]\n",
      " [-0.21188985]\n",
      " [-0.40775571]\n",
      " ...\n",
      " [-0.14583685]\n",
      " [ 0.00392486]\n",
      " [-0.45451065]]\n",
      "t [[-0.00308615]\n",
      " [-0.21188985]\n",
      " [-0.40775571]\n",
      " ...\n",
      " [-0.14583685]\n",
      " [ 0.00392486]\n",
      " [-0.45451065]]\n",
      "t [[-0.00768648]\n",
      " [-0.25864589]\n",
      " [-0.45448338]\n",
      " ...\n",
      " [-0.15466951]\n",
      " [ 0.00982627]\n",
      " [-0.52736541]]\n",
      "t [[-0.00768648]\n",
      " [-0.25864589]\n",
      " [-0.45448338]\n",
      " ...\n",
      " [-0.15466951]\n",
      " [ 0.00982627]\n",
      " [-0.52736541]]\n",
      "Current iteration=6, loss=36943.557913608005\n",
      "t [[-0.01264376]\n",
      " [-0.30498373]\n",
      " [-0.49553629]\n",
      " ...\n",
      " [-0.16075756]\n",
      " [ 0.01593525]\n",
      " [-0.59588289]]\n",
      "t [[-0.01264376]\n",
      " [-0.30498373]\n",
      " [-0.49553629]\n",
      " ...\n",
      " [-0.16075756]\n",
      " [ 0.01593525]\n",
      " [-0.59588289]]\n",
      "t [[-0.01777051]\n",
      " [-0.35056285]\n",
      " [-0.53218326]\n",
      " ...\n",
      " [-0.16492558]\n",
      " [ 0.02198378]\n",
      " [-0.66053187]]\n",
      "t [[-0.01777051]\n",
      " [-0.35056285]\n",
      " [-0.53218326]\n",
      " ...\n",
      " [-0.16492558]\n",
      " [ 0.02198378]\n",
      " [-0.66053187]]\n",
      "Current iteration=8, loss=36385.60273334914\n",
      "t [[-0.02293584]\n",
      " [-0.39515761]\n",
      " [-0.56536513]\n",
      " ...\n",
      " [-0.16776801]\n",
      " [ 0.02779619]\n",
      " [-0.72170456]]\n",
      "t [[-0.02293584]\n",
      " [-0.39515761]\n",
      " [-0.56536513]\n",
      " ...\n",
      " [-0.16776801]\n",
      " [ 0.02779619]\n",
      " [-0.72170456]]\n",
      "t [[-0.02804906]\n",
      " [-0.43862362]\n",
      " [-0.59578523]\n",
      " ...\n",
      " [-0.16971386]\n",
      " [ 0.03326179]\n",
      " [-0.77973299]]\n",
      "loss=35938.060930938685\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.04898183]\n",
      " [-0.20924475]\n",
      " [-0.13151786]\n",
      " ...\n",
      " [-0.09531785]\n",
      " [ 0.02063684]\n",
      " [-0.05556529]]\n",
      "t [[ 0.04898183]\n",
      " [-0.20924475]\n",
      " [-0.13151786]\n",
      " ...\n",
      " [-0.09531785]\n",
      " [ 0.02063684]\n",
      " [-0.05556529]]\n",
      "t [[ 0.08656137]\n",
      " [-0.37934139]\n",
      " [-0.23730283]\n",
      " ...\n",
      " [-0.16911307]\n",
      " [ 0.03352034]\n",
      " [-0.10522367]]\n",
      "t [[ 0.08656137]\n",
      " [-0.37934139]\n",
      " [-0.23730283]\n",
      " ...\n",
      " [-0.16911307]\n",
      " [ 0.03352034]\n",
      " [-0.10522367]]\n",
      "Current iteration=2, loss=38634.615024711704\n",
      "t [[ 0.11558098]\n",
      " [-0.52004303]\n",
      " [-0.32412743]\n",
      " ...\n",
      " [-0.2273351 ]\n",
      " [ 0.04056269]\n",
      " [-0.15002272]]\n",
      "t [[ 0.11558098]\n",
      " [-0.52004303]\n",
      " [-0.32412743]\n",
      " ...\n",
      " [-0.2273351 ]\n",
      " [ 0.04056269]\n",
      " [-0.15002272]]\n",
      "t [[ 0.13815951]\n",
      " [-0.63851755]\n",
      " [-0.39691569]\n",
      " ...\n",
      " [-0.27428998]\n",
      " [ 0.04318778]\n",
      " [-0.19077506]]\n",
      "t [[ 0.13815951]\n",
      " [-0.63851755]\n",
      " [-0.39691569]\n",
      " ...\n",
      " [-0.27428998]\n",
      " [ 0.04318778]\n",
      " [-0.19077506]]\n",
      "Current iteration=4, loss=37612.89571889124\n",
      "t [[ 0.1558475 ]\n",
      " [-0.73993197]\n",
      " [-0.45917347]\n",
      " ...\n",
      " [-0.31302916]\n",
      " [ 0.04243286]\n",
      " [-0.22810314]]\n",
      "t [[ 0.1558475 ]\n",
      " [-0.73993197]\n",
      " [-0.45917347]\n",
      " ...\n",
      " [-0.31302916]\n",
      " [ 0.04243286]\n",
      " [-0.22810314]]\n",
      "t [[ 0.16978308]\n",
      " [-0.82802903]\n",
      " [-0.51340405]\n",
      " ...\n",
      " [-0.34571675]\n",
      " [ 0.03905929]\n",
      " [-0.26248886]]\n",
      "t [[ 0.16978308]\n",
      " [-0.82802903]\n",
      " [-0.51340405]\n",
      " ...\n",
      " [-0.34571675]\n",
      " [ 0.03905929]\n",
      " [-0.26248886]]\n",
      "Current iteration=6, loss=36889.90538244331\n",
      "t [[ 0.18080999]\n",
      " [-0.90555495]\n",
      " [-0.56141362]\n",
      " ...\n",
      " [-0.3738993 ]\n",
      " [ 0.03363588]\n",
      " [-0.29431208]]\n",
      "t [[ 0.18080999]\n",
      " [-0.90555495]\n",
      " [-0.56141362]\n",
      " ...\n",
      " [-0.3738993 ]\n",
      " [ 0.03363588]\n",
      " [-0.29431208]]\n",
      "t [[ 0.18956106]\n",
      " [-0.97455475]\n",
      " [-0.60452132]\n",
      " ...\n",
      " [-0.39869167]\n",
      " [ 0.02659586]\n",
      " [-0.32387803]]\n",
      "t [[ 0.18956106]\n",
      " [-0.97455475]\n",
      " [-0.60452132]\n",
      " ...\n",
      " [-0.39869167]\n",
      " [ 0.02659586]\n",
      " [-0.32387803]]\n",
      "Current iteration=8, loss=36336.892924175954\n",
      "t [[ 0.19651619]\n",
      " [-1.0365737 ]\n",
      " [-0.64370187]\n",
      " ...\n",
      " [-0.420904  ]\n",
      " [ 0.01827532]\n",
      " [-0.3514364 ]]\n",
      "t [[ 0.19651619]\n",
      " [-1.0365737 ]\n",
      " [-0.64370187]\n",
      " ...\n",
      " [-0.420904  ]\n",
      " [ 0.01827532]\n",
      " [-0.3514364 ]]\n",
      "t [[ 0.20204314]\n",
      " [-1.09279564]\n",
      " [-0.6796835 ]\n",
      " ...\n",
      " [-0.44112901]\n",
      " [ 0.00893925]\n",
      " [-0.37719476]]\n",
      "loss=35896.28687701702\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00645941]\n",
      " [-0.03976973]\n",
      " [-0.12468226]\n",
      " ...\n",
      " [-0.09567846]\n",
      " [ 0.01860055]\n",
      " [-0.05676071]]\n",
      "t [[ 0.00645941]\n",
      " [-0.03976973]\n",
      " [-0.12468226]\n",
      " ...\n",
      " [-0.09567846]\n",
      " [ 0.01860055]\n",
      " [-0.05676071]]\n",
      "t [[ 0.0089797 ]\n",
      " [-0.08556996]\n",
      " [-0.22295485]\n",
      " ...\n",
      " [-0.16937069]\n",
      " [ 0.0296304 ]\n",
      " [-0.10748066]]\n",
      "t [[ 0.0089797 ]\n",
      " [-0.08556996]\n",
      " [-0.22295485]\n",
      " ...\n",
      " [-0.16937069]\n",
      " [ 0.0296304 ]\n",
      " [-0.10748066]]\n",
      "Current iteration=2, loss=38615.17421180854\n",
      "t [[ 0.0088366 ]\n",
      " [-0.13486975]\n",
      " [-0.3020733 ]\n",
      " ...\n",
      " [-0.22721928]\n",
      " [ 0.03500858]\n",
      " [-0.15324047]]\n",
      "t [[ 0.0088366 ]\n",
      " [-0.13486975]\n",
      " [-0.3020733 ]\n",
      " ...\n",
      " [-0.22721928]\n",
      " [ 0.03500858]\n",
      " [-0.15324047]]\n",
      "t [[ 0.00693432]\n",
      " [-0.18590947]\n",
      " [-0.36726528]\n",
      " ...\n",
      " [-0.27364955]\n",
      " [ 0.03615281]\n",
      " [-0.19487511]]\n",
      "t [[ 0.00693432]\n",
      " [-0.18590947]\n",
      " [-0.36726528]\n",
      " ...\n",
      " [-0.27364955]\n",
      " [ 0.03615281]\n",
      " [-0.19487511]]\n",
      "Current iteration=4, loss=37579.83227206428\n",
      "t [[ 0.0038947 ]\n",
      " [-0.23750344]\n",
      " [-0.42221913]\n",
      " ...\n",
      " [-0.31178386]\n",
      " [ 0.03408752]\n",
      " [-0.23302201]]\n",
      "t [[ 0.0038947 ]\n",
      " [-0.23750344]\n",
      " [-0.42221913]\n",
      " ...\n",
      " [-0.31178386]\n",
      " [ 0.03408752]\n",
      " [-0.23302201]]\n",
      "t [[ 1.43865035e-04]\n",
      " [-2.88862346e-01]\n",
      " [-4.69542883e-01]\n",
      " ...\n",
      " [-3.43828180e-01]\n",
      " [ 2.95589461e-02]\n",
      " [-2.68173434e-01]]\n",
      "t [[ 1.43865035e-04]\n",
      " [-2.88862346e-01]\n",
      " [-4.69542883e-01]\n",
      " ...\n",
      " [-3.43828180e-01]\n",
      " [ 2.95589461e-02]\n",
      " [-2.68173434e-01]]\n",
      "Current iteration=6, loss=36845.11117539753\n",
      "t [[-0.00402516]\n",
      " [-0.33946611]\n",
      " [-0.51109795]\n",
      " ...\n",
      " [-0.37135374]\n",
      " [ 0.0231205 ]\n",
      " [-0.30071673]]\n",
      "t [[-0.00402516]\n",
      " [-0.33946611]\n",
      " [-0.51109795]\n",
      " ...\n",
      " [-0.37135374]\n",
      " [ 0.0231205 ]\n",
      " [-0.30071673]]\n",
      "t [[-0.00841026]\n",
      " [-0.38897802]\n",
      " [-0.54822707]\n",
      " ...\n",
      " [-0.39548986]\n",
      " [ 0.0151908 ]\n",
      " [-0.33096274]]\n",
      "t [[-0.00841026]\n",
      " [-0.38897802]\n",
      " [-0.54822707]\n",
      " ...\n",
      " [-0.39548986]\n",
      " [ 0.0151908 ]\n",
      " [-0.33096274]]\n",
      "Current iteration=8, loss=36282.07473311515\n",
      "t [[-0.01287208]\n",
      " [-0.43718705]\n",
      " [-0.58190848]\n",
      " ...\n",
      " [-0.41705495]\n",
      " [ 0.00609241]\n",
      " [-0.35916555]]\n",
      "t [[-0.01287208]\n",
      " [-0.43718705]\n",
      " [-0.58190848]\n",
      " ...\n",
      " [-0.41705495]\n",
      " [ 0.00609241]\n",
      " [-0.35916555]]\n",
      "t [[-0.01731507]\n",
      " [-0.48396881]\n",
      " [-0.61286122]\n",
      " ...\n",
      " [-0.43664627]\n",
      " [-0.003922  ]\n",
      " [-0.38553627]]\n",
      "loss=35833.097630057375\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00551705]\n",
      " [-0.0349043 ]\n",
      " [-0.12603261]\n",
      " ...\n",
      " [-0.09407841]\n",
      " [ 0.01999922]\n",
      " [-0.05324179]]\n",
      "t [[ 0.00551705]\n",
      " [-0.0349043 ]\n",
      " [-0.12603261]\n",
      " ...\n",
      " [-0.09407841]\n",
      " [ 0.01999922]\n",
      " [-0.05324179]]\n",
      "t [[ 0.00713358]\n",
      " [-0.07612025]\n",
      " [-0.22570859]\n",
      " ...\n",
      " [-0.16646949]\n",
      " [ 0.03235276]\n",
      " [-0.10064627]]\n",
      "t [[ 0.00713358]\n",
      " [-0.07612025]\n",
      " [-0.22570859]\n",
      " ...\n",
      " [-0.16646949]\n",
      " [ 0.03235276]\n",
      " [-0.10064627]]\n",
      "Current iteration=2, loss=38633.87108207015\n",
      "t [[ 0.0061321 ]\n",
      " [-0.12110846]\n",
      " [-0.30621391]\n",
      " ...\n",
      " [-0.22321986]\n",
      " [ 0.03897272]\n",
      " [-0.1432719 ]]\n",
      "t [[ 0.0061321 ]\n",
      " [-0.12110846]\n",
      " [-0.30621391]\n",
      " ...\n",
      " [-0.22321986]\n",
      " [ 0.03897272]\n",
      " [-0.1432719 ]]\n",
      "t [[ 0.00342136]\n",
      " [-0.16809362]\n",
      " [-0.37273907]\n",
      " ...\n",
      " [-0.26869452]\n",
      " [ 0.0412746 ]\n",
      " [-0.18193572]]\n",
      "t [[ 0.00342136]\n",
      " [-0.16809362]\n",
      " [-0.37273907]\n",
      " ...\n",
      " [-0.26869452]\n",
      " [ 0.0412746 ]\n",
      " [-0.18193572]]\n",
      "Current iteration=4, loss=37612.75006772074\n",
      "t [[-3.74021774e-04]\n",
      " [-2.15871130e-01]\n",
      " [-4.28955859e-01]\n",
      " ...\n",
      " [-3.05978822e-01]\n",
      " [ 4.02839598e-02]\n",
      " [-2.17260339e-01]]\n",
      "t [[-3.74021774e-04]\n",
      " [-2.15871130e-01]\n",
      " [-4.28955859e-01]\n",
      " ...\n",
      " [-3.05978822e-01]\n",
      " [ 4.02839598e-02]\n",
      " [-2.17260339e-01]]\n",
      "t [[-0.00482661]\n",
      " [-0.2636325 ]\n",
      " [-0.47746687]\n",
      " ...\n",
      " [-0.33725563]\n",
      " [ 0.03674993]\n",
      " [-0.24972563]]\n",
      "t [[-0.00482661]\n",
      " [-0.2636325 ]\n",
      " [-0.47746687]\n",
      " ...\n",
      " [-0.33725563]\n",
      " [ 0.03674993]\n",
      " [-0.24972563]]\n",
      "Current iteration=6, loss=36890.027413510456\n",
      "t [[-0.00964324]\n",
      " [-0.3108396 ]\n",
      " [-0.52013385]\n",
      " ...\n",
      " [-0.36408127]\n",
      " [ 0.03122962]\n",
      " [-0.27970838]]\n",
      "t [[-0.00964324]\n",
      " [-0.3108396 ]\n",
      " [-0.52013385]\n",
      " ...\n",
      " [-0.36408127]\n",
      " [ 0.03122962]\n",
      " [-0.27970838]]\n",
      "t [[-0.0146226 ]\n",
      " [-0.35713922]\n",
      " [-0.55830265]\n",
      " ...\n",
      " [-0.38757509]\n",
      " [ 0.02414564]\n",
      " [-0.30751034]]\n",
      "t [[-0.0146226 ]\n",
      " [-0.35713922]\n",
      " [-0.55830265]\n",
      " ...\n",
      " [-0.38757509]\n",
      " [ 0.02414564]\n",
      " [-0.30751034]]\n",
      "Current iteration=8, loss=36337.14830634132\n",
      "t [[-0.01962681]\n",
      " [-0.40230546]\n",
      " [-0.5929559 ]\n",
      " ...\n",
      " [-0.40854857]\n",
      " [ 0.01582467]\n",
      " [-0.33337759]]\n",
      "t [[-0.01962681]\n",
      " [-0.40230546]\n",
      " [-0.5929559 ]\n",
      " ...\n",
      " [-0.40854857]\n",
      " [ 0.01582467]\n",
      " [-0.33337759]]\n",
      "t [[-0.02456231]\n",
      " [-0.44620063]\n",
      " [-0.62481747]\n",
      " ...\n",
      " [-0.42759389]\n",
      " [ 0.00652348]\n",
      " [-0.35751417]]\n",
      "loss=35896.71153066836\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00459459]\n",
      " [-0.03696967]\n",
      " [-0.12520101]\n",
      " ...\n",
      " [-0.05512279]\n",
      " [-0.00663885]\n",
      " [-0.11245941]]\n",
      "t [[ 0.00459459]\n",
      " [-0.03696967]\n",
      " [-0.12520101]\n",
      " ...\n",
      " [-0.05512279]\n",
      " [-0.00663885]\n",
      " [-0.11245941]]\n",
      "t [[ 0.00536571]\n",
      " [-0.08020658]\n",
      " [-0.22382698]\n",
      " ...\n",
      " [-0.09321392]\n",
      " [-0.00764478]\n",
      " [-0.21499527]]\n",
      "t [[ 0.00536571]\n",
      " [-0.08020658]\n",
      " [-0.22382698]\n",
      " ...\n",
      " [-0.09321392]\n",
      " [-0.00764478]\n",
      " [-0.21499527]]\n",
      "Current iteration=2, loss=38604.77327007189\n",
      "t [[ 0.00359005]\n",
      " [-0.12715499]\n",
      " [-0.30315909]\n",
      " ...\n",
      " [-0.11938265]\n",
      " [-0.00507916]\n",
      " [-0.30917475]]\n",
      "t [[ 0.00359005]\n",
      " [-0.12715499]\n",
      " [-0.30315909]\n",
      " ...\n",
      " [-0.11938265]\n",
      " [-0.00507916]\n",
      " [-0.30917475]]\n",
      "t [[ 1.69841240e-04]\n",
      " [-1.76031423e-01]\n",
      " [-3.68451372e-01]\n",
      " ...\n",
      " [-1.37275123e-01]\n",
      " [-3.71467787e-04]\n",
      " [-3.96252390e-01]]\n",
      "t [[ 1.69841240e-04]\n",
      " [-1.76031423e-01]\n",
      " [-3.68451372e-01]\n",
      " ...\n",
      " [-1.37275123e-01]\n",
      " [-3.71467787e-04]\n",
      " [-3.96252390e-01]]\n",
      "Current iteration=4, loss=37566.54162634849\n",
      "t [[-0.00427669]\n",
      " [-0.22562871]\n",
      " [-0.42341594]\n",
      " ...\n",
      " [-0.14943502]\n",
      " [ 0.00551366]\n",
      " [-0.47722052]]\n",
      "t [[-0.00427669]\n",
      " [-0.22562871]\n",
      " [-0.42341594]\n",
      " ...\n",
      " [-0.14943502]\n",
      " [ 0.00551366]\n",
      " [-0.47722052]]\n",
      "t [[-0.00932817]\n",
      " [-0.27513869]\n",
      " [-0.47068067]\n",
      " ...\n",
      " [-0.15763462]\n",
      " [ 0.01193279]\n",
      " [-0.55286967]]\n",
      "t [[-0.00932817]\n",
      " [-0.27513869]\n",
      " [-0.47068067]\n",
      " ...\n",
      " [-0.15763462]\n",
      " [ 0.01193279]\n",
      " [-0.55286967]]\n",
      "Current iteration=6, loss=36831.27490320828\n",
      "t [[-0.014697  ]\n",
      " [-0.32402498]\n",
      " [-0.51212314]\n",
      " ...\n",
      " [-0.16311461]\n",
      " [ 0.01846138]\n",
      " [-0.62383813]]\n",
      "t [[-0.014697  ]\n",
      " [-0.32402498]\n",
      " [-0.51212314]\n",
      " ...\n",
      " [-0.16311461]\n",
      " [ 0.01846138]\n",
      " [-0.62383813]]\n",
      "t [[-0.02018689]\n",
      " [-0.37193669]\n",
      " [-0.54909902]\n",
      " ...\n",
      " [-0.16674782]\n",
      " [ 0.0248243 ]\n",
      " [-0.690649  ]]\n",
      "t [[-0.02018689]\n",
      " [-0.37193669]\n",
      " [-0.54909902]\n",
      " ...\n",
      " [-0.16674782]\n",
      " [ 0.0248243 ]\n",
      " [-0.690649  ]]\n",
      "Current iteration=8, loss=36268.31110129019\n",
      "t [[-0.02566444]\n",
      " [-0.41865055]\n",
      " [-0.58259687]\n",
      " ...\n",
      " [-0.16915002]\n",
      " [ 0.03084892]\n",
      " [-0.75373702]]\n",
      "t [[-0.02566444]\n",
      " [-0.41865055]\n",
      " [-0.58259687]\n",
      " ...\n",
      " [-0.16915002]\n",
      " [ 0.03084892]\n",
      " [-0.75373702]]\n",
      "t [[-0.03104003]\n",
      " [-0.46403154]\n",
      " [-0.61334395]\n",
      " ...\n",
      " [-0.17075571]\n",
      " [ 0.036433  ]\n",
      " [-0.81346827]]\n",
      "loss=35819.493506258834\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.05170304]\n",
      " [-0.22086946]\n",
      " [-0.13882441]\n",
      " ...\n",
      " [-0.10061328]\n",
      " [ 0.02178333]\n",
      " [-0.05865225]]\n",
      "t [[ 0.05170304]\n",
      " [-0.22086946]\n",
      " [-0.13882441]\n",
      " ...\n",
      " [-0.10061328]\n",
      " [ 0.02178333]\n",
      " [-0.05865225]]\n",
      "t [[ 0.09070673]\n",
      " [-0.39813814]\n",
      " [-0.24898938]\n",
      " ...\n",
      " [-0.17725593]\n",
      " [ 0.03493223]\n",
      " [-0.11072511]]\n",
      "t [[ 0.09070673]\n",
      " [-0.39813814]\n",
      " [-0.24898938]\n",
      " ...\n",
      " [-0.17725593]\n",
      " [ 0.03493223]\n",
      " [-0.11072511]]\n",
      "Current iteration=2, loss=38561.155219209715\n",
      "t [[ 0.12035538]\n",
      " [-0.54327988]\n",
      " [-0.33846147]\n",
      " ...\n",
      " [-0.23692598]\n",
      " [ 0.04169627]\n",
      " [-0.15745151]]\n",
      "t [[ 0.12035538]\n",
      " [-0.54327988]\n",
      " [-0.33846147]\n",
      " ...\n",
      " [-0.23692598]\n",
      " [ 0.04169627]\n",
      " [-0.15745151]]\n",
      "t [[ 0.14308741]\n",
      " [-0.66453265]\n",
      " [-0.41289503]\n",
      " ...\n",
      " [-0.28456713]\n",
      " [ 0.04371247]\n",
      " [-0.19976947]]\n",
      "t [[ 0.14308741]\n",
      " [-0.66453265]\n",
      " [-0.41289503]\n",
      " ...\n",
      " [-0.28456713]\n",
      " [ 0.04371247]\n",
      " [-0.19976947]]\n",
      "Current iteration=4, loss=37516.93770559045\n",
      "t [[ 0.16065015]\n",
      " [-0.76770583]\n",
      " [-0.47622439]\n",
      " ...\n",
      " [-0.32360144]\n",
      " [ 0.04214847]\n",
      " [-0.23838527]]\n",
      "t [[ 0.16065015]\n",
      " [-0.76770583]\n",
      " [-0.47622439]\n",
      " ...\n",
      " [-0.32360144]\n",
      " [ 0.04214847]\n",
      " [-0.23838527]]\n",
      "t [[ 0.1743027 ]\n",
      " [-0.85692869]\n",
      " [-0.53120232]\n",
      " ...\n",
      " [-0.35640478]\n",
      " [ 0.03784694]\n",
      " [-0.27383844]]\n",
      "t [[ 0.1743027 ]\n",
      " [-0.85692869]\n",
      " [-0.53120232]\n",
      " ...\n",
      " [-0.35640478]\n",
      " [ 0.03784694]\n",
      " [-0.27383844]]\n",
      "Current iteration=6, loss=36784.27246475859\n",
      "t [[ 0.1849635 ]\n",
      " [-0.93518238]\n",
      " [-0.57977958]\n",
      " ...\n",
      " [-0.38464221]\n",
      " [ 0.03142894]\n",
      " [-0.30655   ]]\n",
      "t [[ 0.1849635 ]\n",
      " [-0.93518238]\n",
      " [-0.57977958]\n",
      " ...\n",
      " [-0.38464221]\n",
      " [ 0.03142894]\n",
      " [-0.30655   ]]\n",
      "t [[ 0.19331139]\n",
      " [-1.00465602]\n",
      " [-0.62335778]\n",
      " ...\n",
      " [-0.40949215]\n",
      " [ 0.02336242]\n",
      " [-0.33685564]]\n",
      "t [[ 0.19331139]\n",
      " [-1.00465602]\n",
      " [-0.62335778]\n",
      " ...\n",
      " [-0.40949215]\n",
      " [ 0.02336242]\n",
      " [-0.33685564]]\n",
      "Current iteration=8, loss=36227.308314133596\n",
      "t [[ 0.19985441]\n",
      " [-1.06698351]\n",
      " [-0.66295721]\n",
      " ...\n",
      " [-0.43179585]\n",
      " [ 0.01400706]\n",
      " [-0.36502838]]\n",
      "t [[ 0.19985441]\n",
      " [-1.06698351]\n",
      " [-0.66295721]\n",
      " ...\n",
      " [-0.43179585]\n",
      " [ 0.01400706]\n",
      " [-0.36502838]]\n",
      "t [[ 0.20497713]\n",
      " [-1.12340329]\n",
      " [-0.6993296 ]\n",
      " ...\n",
      " [-0.45215816]\n",
      " [ 0.00364425]\n",
      " [-0.39129416]]\n",
      "loss=35786.22962232554\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00681827]\n",
      " [-0.04197916]\n",
      " [-0.13160905]\n",
      " ...\n",
      " [-0.10099393]\n",
      " [ 0.01963391]\n",
      " [-0.05991408]]\n",
      "t [[ 0.00681827]\n",
      " [-0.04197916]\n",
      " [-0.13160905]\n",
      " ...\n",
      " [-0.10099393]\n",
      " [ 0.01963391]\n",
      " [-0.05991408]]\n",
      "t [[ 0.00925021]\n",
      " [-0.09067369]\n",
      " [-0.23380501]\n",
      " ...\n",
      " [-0.17750107]\n",
      " [ 0.03083694]\n",
      " [-0.11309972]]\n",
      "t [[ 0.00925021]\n",
      " [-0.09067369]\n",
      " [-0.23380501]\n",
      " ...\n",
      " [-0.17750107]\n",
      " [ 0.03083694]\n",
      " [-0.11309972]]\n",
      "Current iteration=2, loss=38540.849093362376\n",
      "t [[ 0.00879611]\n",
      " [-0.14310787]\n",
      " [-0.31512197]\n",
      " ...\n",
      " [-0.23674642]\n",
      " [ 0.03586653]\n",
      " [-0.16082873]]\n",
      "t [[ 0.00879611]\n",
      " [-0.14310787]\n",
      " [-0.31512197]\n",
      " ...\n",
      " [-0.23674642]\n",
      " [ 0.03586653]\n",
      " [-0.16082873]]\n",
      "t [[ 0.0064915 ]\n",
      " [-0.19726861]\n",
      " [-0.38155893]\n",
      " ...\n",
      " [-0.28380862]\n",
      " [ 0.03635151]\n",
      " [-0.2040646 ]]\n",
      "t [[ 0.0064915 ]\n",
      " [-0.19726861]\n",
      " [-0.38155893]\n",
      " ...\n",
      " [-0.28380862]\n",
      " [ 0.03635151]\n",
      " [-0.2040646 ]]\n",
      "Current iteration=4, loss=37482.459693614685\n",
      "t [[ 0.00303018]\n",
      " [-0.25183711]\n",
      " [-0.43724934]\n",
      " ...\n",
      " [-0.3221869 ]\n",
      " [ 0.03344371]\n",
      " [-0.24353028]]\n",
      "t [[ 0.00303018]\n",
      " [-0.25183711]\n",
      " [-0.43724934]\n",
      " ...\n",
      " [-0.3221869 ]\n",
      " [ 0.03344371]\n",
      " [-0.24353028]]\n",
      "t [[-0.0011238 ]\n",
      " [-0.30596002]\n",
      " [-0.48505469]\n",
      " ...\n",
      " [-0.35430107]\n",
      " [ 0.02796766]\n",
      " [-0.27977662]]\n",
      "t [[-0.0011238 ]\n",
      " [-0.30596002]\n",
      " [-0.48505469]\n",
      " ...\n",
      " [-0.35430107]\n",
      " [ 0.02796766]\n",
      " [-0.27977662]]\n",
      "Current iteration=6, loss=36737.64739382295\n",
      "t [[-0.0056587 ]\n",
      " [-0.35909211]\n",
      " [-0.52697858]\n",
      " ...\n",
      " [-0.38184109]\n",
      " [ 0.02052638]\n",
      " [-0.31323273]]\n",
      "t [[-0.0056587 ]\n",
      " [-0.35909211]\n",
      " [-0.52697858]\n",
      " ...\n",
      " [-0.38184109]\n",
      " [ 0.02052638]\n",
      " [-0.31323273]]\n",
      "t [[-0.01036449]\n",
      " [-0.41089319]\n",
      " [-0.56444088]\n",
      " ...\n",
      " [-0.40599932]\n",
      " [ 0.01157089]\n",
      " [-0.34424037]]\n",
      "t [[-0.01036449]\n",
      " [-0.41089319]\n",
      " [-0.56444088]\n",
      " ...\n",
      " [-0.40599932]\n",
      " [ 0.01157089]\n",
      " [-0.34424037]]\n",
      "Current iteration=8, loss=36170.43819852336\n",
      "t [[-0.01510008]\n",
      " [-0.46116055]\n",
      " [-0.59845877]\n",
      " ...\n",
      " [-0.42762456]\n",
      " [ 0.00144545]\n",
      " [-0.37307725]]\n",
      "t [[-0.01510008]\n",
      " [-0.46116055]\n",
      " [-0.59845877]\n",
      " ...\n",
      " [-0.42762456]\n",
      " [ 0.00144545]\n",
      " [-0.37307725]]\n",
      "t [[-0.01977151]\n",
      " [-0.50978419]\n",
      " [-0.62976802]\n",
      " ...\n",
      " [-0.44732546]\n",
      " [-0.00958249]\n",
      " [-0.39997308]]\n",
      "loss=35720.92530677862\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00582355]\n",
      " [-0.03684342]\n",
      " [-0.13303442]\n",
      " ...\n",
      " [-0.09930499]\n",
      " [ 0.02111028]\n",
      " [-0.05619966]]\n",
      "t [[ 0.00582355]\n",
      " [-0.03684342]\n",
      " [-0.13303442]\n",
      " ...\n",
      " [-0.09930499]\n",
      " [ 0.02111028]\n",
      " [-0.05619966]]\n",
      "t [[ 0.00730373]\n",
      " [-0.0807156 ]\n",
      " [-0.23671457]\n",
      " ...\n",
      " [-0.17445597]\n",
      " [ 0.03370601]\n",
      " [-0.10589756]]\n",
      "t [[ 0.00730373]\n",
      " [-0.0807156 ]\n",
      " [-0.23671457]\n",
      " ...\n",
      " [-0.17445597]\n",
      " [ 0.03370601]\n",
      " [-0.10589756]]\n",
      "Current iteration=2, loss=38560.44223238241\n",
      "t [[ 0.00594879]\n",
      " [-0.12863063]\n",
      " [-0.31949296]\n",
      " ...\n",
      " [-0.23256523]\n",
      " [ 0.04003656]\n",
      " [-0.15033933]]\n",
      "t [[ 0.00594879]\n",
      " [-0.12863063]\n",
      " [-0.31949296]\n",
      " ...\n",
      " [-0.23256523]\n",
      " [ 0.04003656]\n",
      " [-0.15033933]]\n",
      "t [[ 0.00279947]\n",
      " [-0.17855634]\n",
      " [-0.38732734]\n",
      " ...\n",
      " [-0.27864307]\n",
      " [ 0.04172859]\n",
      " [-0.19046766]]\n",
      "t [[ 0.00279947]\n",
      " [-0.17855634]\n",
      " [-0.38732734]\n",
      " ...\n",
      " [-0.27864307]\n",
      " [ 0.04172859]\n",
      " [-0.19046766]]\n",
      "Current iteration=4, loss=37516.8426805812\n",
      "t [[-0.00144737]\n",
      " [-0.22915134]\n",
      " [-0.44433429]\n",
      " ...\n",
      " [-0.31614817]\n",
      " [ 0.03993572]\n",
      " [-0.22698848]]\n",
      "t [[-0.00144737]\n",
      " [-0.22915134]\n",
      " [-0.44433429]\n",
      " ...\n",
      " [-0.31614817]\n",
      " [ 0.03993572]\n",
      " [-0.22698848]]\n",
      "t [[-0.0063264 ]\n",
      " [-0.27953979]\n",
      " [-0.49337089]\n",
      " ...\n",
      " [-0.3474757 ]\n",
      " [ 0.03548618]\n",
      " [-0.26043857]]\n",
      "t [[-0.0063264 ]\n",
      " [-0.27953979]\n",
      " [-0.49337089]\n",
      " ...\n",
      " [-0.3474757 ]\n",
      " [ 0.03548618]\n",
      " [-0.26043857]]\n",
      "Current iteration=6, loss=36784.42148996206\n",
      "t [[-0.01152599]\n",
      " [-0.32915561]\n",
      " [-0.53644241]\n",
      " ...\n",
      " [-0.37430005]\n",
      " [ 0.02898747]\n",
      " [-0.2912351 ]]\n",
      "t [[-0.01152599]\n",
      " [-0.32915561]\n",
      " [-0.53644241]\n",
      " ...\n",
      " [-0.37430005]\n",
      " [ 0.02898747]\n",
      " [-0.2912351 ]]\n",
      "t [[-0.01683738]\n",
      " [-0.37763979]\n",
      " [-0.57497313]\n",
      " ...\n",
      " [-0.39780335]\n",
      " [ 0.0208954 ]\n",
      " [-0.3197096 ]]\n",
      "t [[-0.01683738]\n",
      " [-0.37763979]\n",
      " [-0.57497313]\n",
      " ...\n",
      " [-0.39780335]\n",
      " [ 0.0208954 ]\n",
      " [-0.3197096 ]]\n",
      "Current iteration=8, loss=36227.59281367755\n",
      "t [[-0.02212146]\n",
      " [-0.4247728 ]\n",
      " [-0.60998579]\n",
      " ...\n",
      " [-0.41882728]\n",
      " [ 0.01155907]\n",
      " [-0.34613085]]\n",
      "t [[-0.02212146]\n",
      " [-0.4247728 ]\n",
      " [-0.60998579]\n",
      " ...\n",
      " [-0.41882728]\n",
      " [ 0.01155907]\n",
      " [-0.34613085]]\n",
      "t [[-0.0272868 ]\n",
      " [-0.47042967]\n",
      " [-0.64222197]\n",
      " ...\n",
      " [-0.43797533]\n",
      " [ 0.00125073]\n",
      " [-0.37072064]]\n",
      "loss=35786.71172471689\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00484985]\n",
      " [-0.03902354]\n",
      " [-0.13215662]\n",
      " ...\n",
      " [-0.05818516]\n",
      " [-0.00700767]\n",
      " [-0.11870715]]\n",
      "t [[ 0.00484985]\n",
      " [-0.03902354]\n",
      " [-0.13215662]\n",
      " ...\n",
      " [-0.05818516]\n",
      " [-0.00700767]\n",
      " [-0.11870715]]\n",
      "t [[ 0.00544212]\n",
      " [-0.08502617]\n",
      " [-0.23471591]\n",
      " ...\n",
      " [-0.09740175]\n",
      " [-0.00774163]\n",
      " [-0.22636066]]\n",
      "t [[ 0.00544212]\n",
      " [-0.08502617]\n",
      " [-0.23471591]\n",
      " ...\n",
      " [-0.09740175]\n",
      " [-0.00774163]\n",
      " [-0.22636066]]\n",
      "Current iteration=2, loss=38530.11076044866\n",
      "t [[ 0.00327823]\n",
      " [-0.13500291]\n",
      " [-0.31624228]\n",
      " ...\n",
      " [-0.12365871]\n",
      " [-0.0046255 ]\n",
      " [-0.32480594]]\n",
      "t [[ 0.00327823]\n",
      " [-0.13500291]\n",
      " [-0.31624228]\n",
      " ...\n",
      " [-0.12365871]\n",
      " [-0.0046255 ]\n",
      " [-0.32480594]]\n",
      "t [[-0.00060874]\n",
      " [-0.18691273]\n",
      " [-0.38276574]\n",
      " ...\n",
      " [-0.14113734]\n",
      " [ 0.00070468]\n",
      " [-0.41549518]]\n",
      "t [[-0.00060874]\n",
      " [-0.18691273]\n",
      " [-0.38276574]\n",
      " ...\n",
      " [-0.14113734]\n",
      " [ 0.00070468]\n",
      " [-0.41549518]]\n",
      "Current iteration=4, loss=37469.01006154853\n",
      "t [[-0.00552943]\n",
      " [-0.23941192]\n",
      " [-0.4384472 ]\n",
      " ...\n",
      " [-0.15268335]\n",
      " [ 0.00717533]\n",
      " [-0.49955951]]\n",
      "t [[-0.00552943]\n",
      " [-0.23941192]\n",
      " [-0.4384472 ]\n",
      " ...\n",
      " [-0.15268335]\n",
      " [ 0.00717533]\n",
      " [-0.49955951]]\n",
      "t [[-0.01102557]\n",
      " [-0.29162554]\n",
      " [-0.48617069]\n",
      " ...\n",
      " [-0.16023445]\n",
      " [ 0.01409042]\n",
      " [-0.57788891]]\n",
      "t [[-0.01102557]\n",
      " [-0.29162554]\n",
      " [-0.48617069]\n",
      " ...\n",
      " [-0.16023445]\n",
      " [ 0.01409042]\n",
      " [-0.57788891]]\n",
      "Current iteration=6, loss=36723.79123010435\n",
      "t [[-0.01679194]\n",
      " [-0.34298982]\n",
      " [-0.5279578 ]\n",
      " ...\n",
      " [-0.16511784]\n",
      " [ 0.02100474]\n",
      " [-0.65119504]]\n",
      "t [[-0.01679194]\n",
      " [-0.34298982]\n",
      " [-0.5279578 ]\n",
      " ...\n",
      " [-0.16511784]\n",
      " [ 0.02100474]\n",
      " [-0.65119504]]\n",
      "t [[-0.0226255 ]\n",
      " [-0.39314863]\n",
      " [-0.5652426 ]\n",
      " ...\n",
      " [-0.16824722]\n",
      " [ 0.0276404 ]\n",
      " [-0.72005667]]\n",
      "t [[-0.0226255 ]\n",
      " [-0.39314863]\n",
      " [-0.5652426 ]\n",
      " ...\n",
      " [-0.16824722]\n",
      " [ 0.0276404 ]\n",
      " [-0.72005667]]\n",
      "Current iteration=8, loss=36156.71000841391\n",
      "t [[-0.0283922 ]\n",
      " [-0.4418856 ]\n",
      " [-0.59905343]\n",
      " ...\n",
      " [-0.17025299]\n",
      " [ 0.03383153]\n",
      " [-0.78495189]]\n",
      "t [[-0.0283922 ]\n",
      " [-0.4418856 ]\n",
      " [-0.59905343]\n",
      " ...\n",
      " [-0.17025299]\n",
      " [ 0.03383153]\n",
      " [-0.78495189]]\n",
      "t [[-0.034005  ]\n",
      " [-0.48907891]\n",
      " [-0.63013482]\n",
      " ...\n",
      " [-0.17156953]\n",
      " [ 0.03948745]\n",
      " [-0.8462812 ]]\n",
      "loss=35707.36180412342\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.05442425]\n",
      " [-0.23249417]\n",
      " [-0.14613095]\n",
      " ...\n",
      " [-0.10590872]\n",
      " [ 0.02292982]\n",
      " [-0.06173921]]\n",
      "t [[ 0.05442425]\n",
      " [-0.23249417]\n",
      " [-0.14613095]\n",
      " ...\n",
      " [-0.10590872]\n",
      " [ 0.02292982]\n",
      " [-0.06173921]]\n",
      "t [[ 0.09478309]\n",
      " [-0.41669814]\n",
      " [-0.26052039]\n",
      " ...\n",
      " [-0.1852686 ]\n",
      " [ 0.03629744]\n",
      " [-0.11619065]]\n",
      "t [[ 0.09478309]\n",
      " [-0.41669814]\n",
      " [-0.26052039]\n",
      " ...\n",
      " [-0.1852686 ]\n",
      " [ 0.03629744]\n",
      " [-0.11619065]]\n",
      "Current iteration=2, loss=38489.38501980485\n",
      "t [[ 0.12497809]\n",
      " [-0.56599561]\n",
      " [-0.35245997]\n",
      " ...\n",
      " [-0.24624148]\n",
      " [ 0.04272677]\n",
      " [-0.1647931 ]]\n",
      "t [[ 0.12497809]\n",
      " [-0.56599561]\n",
      " [-0.35245997]\n",
      " ...\n",
      " [-0.24624148]\n",
      " [ 0.04272677]\n",
      " [-0.1647931 ]]\n",
      "t [[ 0.14779041]\n",
      " [-0.68976949]\n",
      " [-0.42838432]\n",
      " ...\n",
      " [-0.29445212]\n",
      " [ 0.04408297]\n",
      " [-0.20861975]]\n",
      "t [[ 0.14779041]\n",
      " [-0.68976949]\n",
      " [-0.42838432]\n",
      " ...\n",
      " [-0.29445212]\n",
      " [ 0.04408297]\n",
      " [-0.20861975]]\n",
      "Current iteration=4, loss=37424.362459125834\n",
      "t [[ 0.16517141]\n",
      " [-0.79449246]\n",
      " [-0.49266843]\n",
      " ...\n",
      " [-0.33370297]\n",
      " [ 0.04166882]\n",
      " [-0.24846504]]\n",
      "t [[ 0.16517141]\n",
      " [-0.79449246]\n",
      " [-0.49266843]\n",
      " ...\n",
      " [-0.33370297]\n",
      " [ 0.04166882]\n",
      " [-0.24846504]]\n",
      "t [[ 0.17850169]\n",
      " [-0.88468005]\n",
      " [-0.54831214]\n",
      " ...\n",
      " [-0.36657844]\n",
      " [ 0.03640907]\n",
      " [-0.28492805]]\n",
      "t [[ 0.17850169]\n",
      " [-0.88468005]\n",
      " [-0.54831214]\n",
      " ...\n",
      " [-0.36657844]\n",
      " [ 0.03640907]\n",
      " [-0.28492805]]\n",
      "Current iteration=6, loss=36683.03464580114\n",
      "t [[ 0.18877232]\n",
      " [-0.96354099]\n",
      " [-0.59740368]\n",
      " ...\n",
      " [-0.39485466]\n",
      " [ 0.02897654]\n",
      " [-0.31847179]]\n",
      "t [[ 0.18877232]\n",
      " [-0.96354099]\n",
      " [-0.59740368]\n",
      " ...\n",
      " [-0.39485466]\n",
      " [ 0.02897654]\n",
      " [-0.31847179]]\n",
      "t [[ 0.19670539]\n",
      " [-1.03339948]\n",
      " [-0.64141935]\n",
      " ...\n",
      " [-0.41976565]\n",
      " [ 0.01987287]\n",
      " [-0.34946285]]\n",
      "t [[ 0.19670539]\n",
      " [-1.03339948]\n",
      " [-0.64141935]\n",
      " ...\n",
      " [-0.41976565]\n",
      " [ 0.01987287]\n",
      " [-0.34946285]]\n",
      "Current iteration=8, loss=36122.9137403514\n",
      "t [[ 0.20283429]\n",
      " [-1.09596983]\n",
      " [-0.68141796]\n",
      " ...\n",
      " [-0.44217668]\n",
      " [ 0.00948039]\n",
      " [-0.37819778]]\n",
      "t [[ 0.20283429]\n",
      " [-1.09596983]\n",
      " [-0.68141796]\n",
      " ...\n",
      " [-0.44217668]\n",
      " [ 0.00948039]\n",
      " [-0.37819778]]\n",
      "t [[ 0.20755794]\n",
      " [-1.15253812]\n",
      " [-0.71816915]\n",
      " ...\n",
      " [-0.46269899]\n",
      " [-0.00190412]\n",
      " [-0.40492093]]\n",
      "loss=35682.011592107025\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00717712]\n",
      " [-0.04418859]\n",
      " [-0.13853584]\n",
      " ...\n",
      " [-0.1063094 ]\n",
      " [ 0.02066727]\n",
      " [-0.06306745]]\n",
      "t [[ 0.00717712]\n",
      " [-0.04418859]\n",
      " [-0.13853584]\n",
      " ...\n",
      " [-0.1063094 ]\n",
      " [ 0.02066727]\n",
      " [-0.06306745]]\n",
      "t [[ 0.00949711]\n",
      " [-0.09581361]\n",
      " [-0.24449555]\n",
      " ...\n",
      " [-0.18549851]\n",
      " [ 0.03199792]\n",
      " [-0.11868209]]\n",
      "t [[ 0.00949711]\n",
      " [-0.09581361]\n",
      " [-0.24449555]\n",
      " ...\n",
      " [-0.18549851]\n",
      " [ 0.03199792]\n",
      " [-0.11868209]]\n",
      "Current iteration=2, loss=38468.22799664002\n",
      "t [[ 0.00870933]\n",
      " [-0.15140599]\n",
      " [-0.32783209]\n",
      " ...\n",
      " [-0.24599357]\n",
      " [ 0.03662489]\n",
      " [-0.16832802]]\n",
      "t [[ 0.00870933]\n",
      " [-0.15140599]\n",
      " [-0.32783209]\n",
      " ...\n",
      " [-0.24599357]\n",
      " [ 0.03662489]\n",
      " [-0.16832802]]\n",
      "t [[ 0.00598891]\n",
      " [-0.20868367]\n",
      " [-0.39536755]\n",
      " ...\n",
      " [-0.2935705 ]\n",
      " [ 0.03640267]\n",
      " [-0.21310723]]\n",
      "t [[ 0.00598891]\n",
      " [-0.20868367]\n",
      " [-0.39536755]\n",
      " ...\n",
      " [-0.2935705 ]\n",
      " [ 0.03640267]\n",
      " [-0.21310723]]\n",
      "Current iteration=4, loss=37388.48924234679\n",
      "t [[ 0.00210268]\n",
      " [-0.26619388]\n",
      " [-0.45169176]\n",
      " ...\n",
      " [-0.33211491]\n",
      " [ 0.03261483]\n",
      " [-0.25383252]]\n",
      "t [[ 0.00210268]\n",
      " [-0.26619388]\n",
      " [-0.45169176]\n",
      " ...\n",
      " [-0.33211491]\n",
      " [ 0.03261483]\n",
      " [-0.25383252]]\n",
      "t [[-0.00244882]\n",
      " [-0.3230236 ]\n",
      " [-0.49991566]\n",
      " ...\n",
      " [-0.36425671]\n",
      " [ 0.02616468]\n",
      " [-0.29111524]]\n",
      "t [[-0.00244882]\n",
      " [-0.3230236 ]\n",
      " [-0.49991566]\n",
      " ...\n",
      " [-0.36425671]\n",
      " [ 0.02616468]\n",
      " [-0.29111524]]\n",
      "Current iteration=6, loss=36634.62171597111\n",
      "t [[-0.00733717]\n",
      " [-0.37860811]\n",
      " [-0.54217629]\n",
      " ...\n",
      " [-0.39179684]\n",
      " [ 0.01770411]\n",
      " [-0.3254271 ]]\n",
      "t [[-0.00733717]\n",
      " [-0.37860811]\n",
      " [-0.54217629]\n",
      " ...\n",
      " [-0.39179684]\n",
      " [ 0.01770411]\n",
      " [-0.3254271 ]]\n",
      "t [[-0.01234656]\n",
      " [-0.43260918]\n",
      " [-0.57996142]\n",
      " ...\n",
      " [-0.41598249]\n",
      " [ 0.00771545]\n",
      " [-0.35714117]]\n",
      "t [[-0.01234656]\n",
      " [-0.43260918]\n",
      " [-0.57996142]\n",
      " ...\n",
      " [-0.41598249]\n",
      " [ 0.00771545]\n",
      " [-0.35714117]]\n",
      "Current iteration=8, loss=36064.06826569957\n",
      "t [[-0.01733588]\n",
      " [-0.48483708]\n",
      " [-0.61431914]\n",
      " ...\n",
      " [-0.4376856 ]\n",
      " [-0.00343645]\n",
      " [-0.38655901]]\n",
      "t [[-0.01733588]\n",
      " [-0.48483708]\n",
      " [-0.61431914]\n",
      " ...\n",
      " [-0.4376856 ]\n",
      " [-0.00343645]\n",
      " [-0.38655901]]\n",
      "t [[-0.02221417]\n",
      " [-0.53519992]\n",
      " [-0.64599551]\n",
      " ...\n",
      " [-0.45752043]\n",
      " [-0.01547038]\n",
      " [-0.41392901]]\n",
      "loss=35614.69913850759\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00613006]\n",
      " [-0.03878255]\n",
      " [-0.14003623]\n",
      " ...\n",
      " [-0.10453157]\n",
      " [ 0.02222135]\n",
      " [-0.05915754]]\n",
      "t [[ 0.00613006]\n",
      " [-0.03878255]\n",
      " [-0.14003623]\n",
      " ...\n",
      " [-0.10453157]\n",
      " [ 0.02222135]\n",
      " [-0.05915754]]\n",
      "t [[ 0.0074505 ]\n",
      " [-0.0853489 ]\n",
      " [-0.24756119]\n",
      " ...\n",
      " [-0.1823113 ]\n",
      " [ 0.03501322]\n",
      " [-0.1111134 ]]\n",
      "t [[ 0.0074505 ]\n",
      " [-0.0853489 ]\n",
      " [-0.24756119]\n",
      " ...\n",
      " [-0.1823113 ]\n",
      " [ 0.03501322]\n",
      " [-0.1111134 ]]\n",
      "Current iteration=2, loss=38488.70645319901\n",
      "t [[ 0.00571999]\n",
      " [-0.13621784]\n",
      " [-0.33243296]\n",
      " ...\n",
      " [-0.24163416]\n",
      " [ 0.04099923]\n",
      " [-0.15732109]]\n",
      "t [[ 0.00571999]\n",
      " [-0.13621784]\n",
      " [-0.33243296]\n",
      " ...\n",
      " [-0.24163416]\n",
      " [ 0.04099923]\n",
      " [-0.15732109]]\n",
      "t [[ 0.00211959]\n",
      " [-0.18908431]\n",
      " [-0.40142844]\n",
      " ...\n",
      " [-0.2881994 ]\n",
      " [ 0.04203188]\n",
      " [-0.1988586 ]]\n",
      "t [[ 0.00211959]\n",
      " [-0.18908431]\n",
      " [-0.40142844]\n",
      " ...\n",
      " [-0.2881994 ]\n",
      " [ 0.04203188]\n",
      " [-0.1988586 ]]\n",
      "Current iteration=4, loss=37424.313467274726\n",
      "t [[-0.00258052]\n",
      " [-0.24246887]\n",
      " [-0.45912059]\n",
      " ...\n",
      " [-0.32584863]\n",
      " [ 0.03939731]\n",
      " [-0.23651942]]\n",
      "t [[-0.00258052]\n",
      " [-0.24246887]\n",
      " [-0.45912059]\n",
      " ...\n",
      " [-0.32584863]\n",
      " [ 0.03939731]\n",
      " [-0.23651942]]\n",
      "t [[-0.00787862]\n",
      " [-0.29543245]\n",
      " [-0.50861722]\n",
      " ...\n",
      " [-0.35718573]\n",
      " [ 0.03400344]\n",
      " [-0.27089902]]\n",
      "t [[-0.00787862]\n",
      " [-0.29543245]\n",
      " [-0.50861722]\n",
      " ...\n",
      " [-0.35718573]\n",
      " [ 0.03400344]\n",
      " [-0.27089902]]\n",
      "Current iteration=6, loss=36683.206754367005\n",
      "t [[-0.0134468 ]\n",
      " [-0.34738651]\n",
      " [-0.5520586 ]\n",
      " ...\n",
      " [-0.38399553]\n",
      " [ 0.02650751]\n",
      " [-0.30245575]]\n",
      "t [[-0.0134468 ]\n",
      " [-0.34738651]\n",
      " [-0.5520586 ]\n",
      " ...\n",
      " [-0.38399553]\n",
      " [ 0.02650751]\n",
      " [-0.30245575]]\n",
      "t [[-0.01907098]\n",
      " [-0.39797152]\n",
      " [-0.59093838]\n",
      " ...\n",
      " [-0.40751476]\n",
      " [ 0.01739748]\n",
      " [-0.33155125]]\n",
      "t [[-0.01907098]\n",
      " [-0.39797152]\n",
      " [-0.59093838]\n",
      " ...\n",
      " [-0.40751476]\n",
      " [ 0.01739748]\n",
      " [-0.33155125]]\n",
      "Current iteration=8, loss=36123.22925495669\n",
      "t [[-0.02461267]\n",
      " [-0.44697891]\n",
      " [-0.62631141]\n",
      " ...\n",
      " [-0.42860813]\n",
      " [ 0.00704385]\n",
      " [-0.35847713]]\n",
      "t [[-0.02461267]\n",
      " [-0.44697891]\n",
      " [-0.62631141]\n",
      " ...\n",
      " [-0.42860813]\n",
      " [ 0.00704385]\n",
      " [-0.35847713]]\n",
      "t [[-0.02998405]\n",
      " [-0.49430012]\n",
      " [-0.65893055]\n",
      " ...\n",
      " [-0.44788462]\n",
      " [-0.00426658]\n",
      " [-0.38347301]]\n",
      "loss=35682.55750334991\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0051051 ]\n",
      " [-0.04107741]\n",
      " [-0.13911223]\n",
      " ...\n",
      " [-0.06124754]\n",
      " [-0.0073765 ]\n",
      " [-0.1249549 ]]\n",
      "t [[ 0.0051051 ]\n",
      " [-0.04107741]\n",
      " [-0.13911223]\n",
      " ...\n",
      " [-0.06124754]\n",
      " [-0.0073765 ]\n",
      " [-0.1249549 ]]\n",
      "t [[ 0.00549561]\n",
      " [-0.0898834 ]\n",
      " [-0.24544421]\n",
      " ...\n",
      " [-0.10148664]\n",
      " [-0.00780437]\n",
      " [-0.23766565]]\n",
      "t [[ 0.00549561]\n",
      " [-0.0898834 ]\n",
      " [-0.24544421]\n",
      " ...\n",
      " [-0.10148664]\n",
      " [-0.00780437]\n",
      " [-0.23766565]]\n",
      "Current iteration=2, loss=38457.17238897427\n",
      "t [[ 0.00292221]\n",
      " [-0.14291466]\n",
      " [-0.32898443]\n",
      " ...\n",
      " [-0.12772461]\n",
      " [-0.00410921]\n",
      " [-0.34028694]]\n",
      "t [[ 0.00292221]\n",
      " [-0.14291466]\n",
      " [-0.32898443]\n",
      " ...\n",
      " [-0.12772461]\n",
      " [-0.00410921]\n",
      " [-0.34028694]]\n",
      "t [[-0.00144297]\n",
      " [-0.19785668]\n",
      " [-0.39659113]\n",
      " ...\n",
      " [-0.14471318]\n",
      " [ 0.00185359]\n",
      " [-0.43448497]]\n",
      "t [[-0.00144297]\n",
      " [-0.19785668]\n",
      " [-0.39659113]\n",
      " ...\n",
      " [-0.14471318]\n",
      " [ 0.00185359]\n",
      " [-0.43448497]]\n",
      "Current iteration=4, loss=37374.90804449052\n",
      "t [[-0.00683848]\n",
      " [-0.2532281 ]\n",
      " [-0.45288557]\n",
      " ...\n",
      " [-0.15560641]\n",
      " [ 0.00890067]\n",
      " [-0.5215378 ]]\n",
      "t [[-0.00683848]\n",
      " [-0.2532281 ]\n",
      " [-0.45288557]\n",
      " ...\n",
      " [-0.15560641]\n",
      " [ 0.00890067]\n",
      " [-0.5215378 ]]\n",
      "t [[-0.01277072]\n",
      " [-0.30809135]\n",
      " [-0.50100397]\n",
      " ...\n",
      " [-0.1625029 ]\n",
      " [ 0.01628697]\n",
      " [-0.60243904]]\n",
      "t [[-0.01277072]\n",
      " [-0.30809135]\n",
      " [-0.50100397]\n",
      " ...\n",
      " [-0.1625029 ]\n",
      " [ 0.01628697]\n",
      " [-0.60243904]]\n",
      "Current iteration=6, loss=36620.76174382715\n",
      "t [[-0.01891912]\n",
      " [-0.36186085]\n",
      " [-0.54310326]\n",
      " ...\n",
      " [-0.16680861]\n",
      " [ 0.02355139]\n",
      " [-0.67797588]]\n",
      "t [[-0.01891912]\n",
      " [-0.36186085]\n",
      " [-0.54310326]\n",
      " ...\n",
      " [-0.16680861]\n",
      " [ 0.02355139]\n",
      " [-0.67797588]]\n",
      "t [[-0.02507605]\n",
      " [-0.41418057]\n",
      " [-0.58068665]\n",
      " ...\n",
      " [-0.16947024]\n",
      " [ 0.0304177 ]\n",
      " [-0.7487839 ]]\n",
      "t [[-0.02507605]\n",
      " [-0.41418057]\n",
      " [-0.58068665]\n",
      " ...\n",
      " [-0.16947024]\n",
      " [ 0.0304177 ]\n",
      " [-0.7487839 ]]\n",
      "Current iteration=8, loss=36050.37922003321\n",
      "t [[-0.03110865]\n",
      " [-0.46484565]\n",
      " [-0.61481419]\n",
      " ...\n",
      " [-0.17112597]\n",
      " [ 0.03673064]\n",
      " [-0.81538516]]\n",
      "t [[-0.03110865]\n",
      " [-0.46484565]\n",
      " [-0.61481419]\n",
      " ...\n",
      " [-0.17112597]\n",
      " [ 0.03673064]\n",
      " [-0.81538516]]\n",
      "t [[-0.03693399]\n",
      " [-0.51375124]\n",
      " [-0.64624115]\n",
      " ...\n",
      " [-0.17220459]\n",
      " [ 0.04241402]\n",
      " [-0.87821495]]\n",
      "loss=35601.172085046215\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.05714547]\n",
      " [-0.24411888]\n",
      " [-0.1534375 ]\n",
      " ...\n",
      " [-0.11120415]\n",
      " [ 0.02407631]\n",
      " [-0.06482618]]\n",
      "t [[ 0.05714547]\n",
      " [-0.24411888]\n",
      " [-0.1534375 ]\n",
      " ...\n",
      " [-0.11120415]\n",
      " [ 0.02407631]\n",
      " [-0.06482618]]\n",
      "t [[ 0.09879061]\n",
      " [-0.43502199]\n",
      " [-0.27189626]\n",
      " ...\n",
      " [-0.19315142]\n",
      " [ 0.0376161 ]\n",
      " [-0.12162036]]\n",
      "t [[ 0.09879061]\n",
      " [-0.43502199]\n",
      " [-0.27189626]\n",
      " ...\n",
      " [-0.19315142]\n",
      " [ 0.0376161 ]\n",
      " [-0.12162036]]\n",
      "Current iteration=2, loss=38419.25218091634\n",
      "t [[ 0.12945231]\n",
      " [-0.58820112]\n",
      " [-0.36613043]\n",
      " ...\n",
      " [-0.25528817]\n",
      " [ 0.04365637]\n",
      " [-0.1720487 ]]\n",
      "t [[ 0.12945231]\n",
      " [-0.58820112]\n",
      " [-0.36613043]\n",
      " ...\n",
      " [-0.25528817]\n",
      " [ 0.04365637]\n",
      " [-0.1720487 ]]\n",
      "t [[ 0.15227693]\n",
      " [-0.7142562 ]\n",
      " [-0.44340276]\n",
      " ...\n",
      " [-0.30396171]\n",
      " [ 0.04430485]\n",
      " [-0.21732921]]\n",
      "t [[ 0.15227693]\n",
      " [-0.7142562 ]\n",
      " [-0.44340276]\n",
      " ...\n",
      " [-0.30396171]\n",
      " [ 0.04430485]\n",
      " [-0.21732921]]\n",
      "Current iteration=4, loss=37334.961216644704\n",
      "t [[ 0.16942586]\n",
      " [-0.82033994]\n",
      " [-0.50853802]\n",
      " ...\n",
      " [-0.34336194]\n",
      " [ 0.04100347]\n",
      " [-0.25834845]]\n",
      "t [[ 0.16942586]\n",
      " [-0.82033994]\n",
      " [-0.50853802]\n",
      " ...\n",
      " [-0.34336194]\n",
      " [ 0.04100347]\n",
      " [-0.25834845]]\n",
      "t [[ 0.18240081]\n",
      " [-0.9113509 ]\n",
      " [-0.56477855]\n",
      " ...\n",
      " [-0.37627651]\n",
      " [ 0.0347594 ]\n",
      " [-0.29576673]]\n",
      "t [[ 0.18240081]\n",
      " [-0.9113509 ]\n",
      " [-0.56477855]\n",
      " ...\n",
      " [-0.37627651]\n",
      " [ 0.0347594 ]\n",
      " [-0.29576673]]\n",
      "Current iteration=6, loss=36585.88856656143\n",
      "t [[ 0.19226283]\n",
      " [-0.99071626]\n",
      " [-0.6143417 ]\n",
      " ...\n",
      " [-0.4045841 ]\n",
      " [ 0.02629635]\n",
      " [-0.33008979]]\n",
      "t [[ 0.19226283]\n",
      " [-0.99071626]\n",
      " [-0.6143417 ]\n",
      " ...\n",
      " [-0.4045841 ]\n",
      " [ 0.02629635]\n",
      " [-0.33008979]]\n",
      "t [[ 0.19977417]\n",
      " [-1.06088559]\n",
      " [-0.65877013]\n",
      " ...\n",
      " [-0.42956584]\n",
      " [ 0.01614863]\n",
      " [-0.36171541]]\n",
      "t [[ 0.19977417]\n",
      " [-1.06088559]\n",
      " [-0.65877013]\n",
      " ...\n",
      " [-0.42956584]\n",
      " [ 0.01614863]\n",
      " [-0.36171541]]\n",
      "Current iteration=8, loss=36023.340818414596\n",
      "t [[ 0.20549066]\n",
      " [-1.12364514]\n",
      " [-0.69915414]\n",
      " ...\n",
      " [-0.45210389]\n",
      " [ 0.00472007]\n",
      " [-0.39096377]]\n",
      "t [[ 0.20549066]\n",
      " [-1.12364514]\n",
      " [-0.69915414]\n",
      " ...\n",
      " [-0.45210389]\n",
      " [ 0.00472007]\n",
      " [-0.39096377]]\n",
      "t [[ 0.20982317]\n",
      " [-1.18032193]\n",
      " [-0.73627578]\n",
      " ...\n",
      " [-0.47281037]\n",
      " [-0.00767813]\n",
      " [-0.41809775]]\n",
      "loss=35583.19876824266\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00753598]\n",
      " [-0.04639802]\n",
      " [-0.14546263]\n",
      " ...\n",
      " [-0.11162487]\n",
      " [ 0.02170064]\n",
      " [-0.06622082]]\n",
      "t [[ 0.00753598]\n",
      " [-0.04639802]\n",
      " [-0.14546263]\n",
      " ...\n",
      " [-0.11162487]\n",
      " [ 0.02170064]\n",
      " [-0.06622082]]\n",
      "t [[ 0.0097205 ]\n",
      " [-0.1009896 ]\n",
      " [-0.2550269 ]\n",
      " ...\n",
      " [-0.19336337]\n",
      " [ 0.03311352]\n",
      " [-0.12422784]]\n",
      "t [[ 0.0097205 ]\n",
      " [-0.1009896 ]\n",
      " [-0.2550269 ]\n",
      " ...\n",
      " [-0.19336337]\n",
      " [ 0.03311352]\n",
      " [-0.12422784]]\n",
      "Current iteration=2, loss=38397.25768261335\n",
      "t [[ 0.00857767]\n",
      " [-0.15976143]\n",
      " [-0.34021163]\n",
      " ...\n",
      " [-0.25496748]\n",
      " [ 0.03728583]\n",
      " [-0.17573959]]\n",
      "t [[ 0.00857767]\n",
      " [-0.15976143]\n",
      " [-0.34021163]\n",
      " ...\n",
      " [-0.25496748]\n",
      " [ 0.03728583]\n",
      " [-0.17573959]]\n",
      "t [[ 0.00542999]\n",
      " [-0.22014804]\n",
      " [-0.40871143]\n",
      " ...\n",
      " [-0.30295239]\n",
      " [ 0.03631179]\n",
      " [-0.22200638]]\n",
      "t [[ 0.00542999]\n",
      " [-0.22014804]\n",
      " [-0.40871143]\n",
      " ...\n",
      " [-0.30295239]\n",
      " [ 0.03631179]\n",
      " [-0.22200638]]\n",
      "Current iteration=4, loss=37297.711320524766\n",
      "t [[ 0.00111779]\n",
      " [-0.28056323]\n",
      " [-0.4655804 ]\n",
      " ...\n",
      " [-0.34159665]\n",
      " [ 0.03161026]\n",
      " [-0.26393486]]\n",
      "t [[ 0.00111779]\n",
      " [-0.28056323]\n",
      " [-0.4655804 ]\n",
      " ...\n",
      " [-0.34159665]\n",
      " [ 0.03161026]\n",
      " [-0.26393486]]\n",
      "t [[-0.00382374]\n",
      " [-0.34003961]\n",
      " [-0.51417251]\n",
      " ...\n",
      " [-0.37373459]\n",
      " [ 0.02416338]\n",
      " [-0.30219852]]\n",
      "t [[-0.00382374]\n",
      " [-0.34003961]\n",
      " [-0.51417251]\n",
      " ...\n",
      " [-0.37373459]\n",
      " [ 0.02416338]\n",
      " [-0.30219852]]\n",
      "Current iteration=6, loss=36535.73095347062\n",
      "t [[-0.00905187]\n",
      " [-0.39799915]\n",
      " [-0.55674821]\n",
      " ...\n",
      " [-0.40126915]\n",
      " [ 0.01467086]\n",
      " [-0.33731241]]\n",
      "t [[-0.00905187]\n",
      " [-0.39799915]\n",
      " [-0.55674821]\n",
      " ...\n",
      " [-0.40126915]\n",
      " [ 0.01467086]\n",
      " [-0.33731241]]\n",
      "t [[-0.01434708]\n",
      " [-0.45411117]\n",
      " [-0.59485348]\n",
      " ...\n",
      " [-0.42549367]\n",
      " [ 0.0036451 ]\n",
      " [-0.36968117]]\n",
      "t [[-0.01434708]\n",
      " [-0.45411117]\n",
      " [-0.59485348]\n",
      " ...\n",
      " [-0.42549367]\n",
      " [ 0.0036451 ]\n",
      " [-0.36968117]]\n",
      "Current iteration=8, loss=35962.59541524637\n",
      "t [[-0.01957002]\n",
      " [-0.50820358]\n",
      " [-0.62955919]\n",
      " ...\n",
      " [-0.44729599]\n",
      " [-0.00852962]\n",
      " [-0.39963034]]\n",
      "t [[-0.01957002]\n",
      " [-0.50820358]\n",
      " [-0.62955919]\n",
      " ...\n",
      " [-0.44729599]\n",
      " [-0.00852962]\n",
      " [-0.39963034]]\n",
      "t [[-0.02463409]\n",
      " [-0.5602062 ]\n",
      " [-0.6616155 ]\n",
      " ...\n",
      " [-0.4672904 ]\n",
      " [-0.02155936]\n",
      " [-0.42742707]]\n",
      "loss=35513.980999333966\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00643656]\n",
      " [-0.04072168]\n",
      " [-0.14703804]\n",
      " ...\n",
      " [-0.10975815]\n",
      " [ 0.02333242]\n",
      " [-0.06211542]]\n",
      "t [[ 0.00643656]\n",
      " [-0.04072168]\n",
      " [-0.14703804]\n",
      " ...\n",
      " [-0.10975815]\n",
      " [ 0.02333242]\n",
      " [-0.06211542]]\n",
      "t [[ 0.00757399]\n",
      " [-0.09002004]\n",
      " [-0.25824886]\n",
      " ...\n",
      " [-0.19003581]\n",
      " [ 0.03627453]\n",
      " [-0.11629386]]\n",
      "t [[ 0.00757399]\n",
      " [-0.09002004]\n",
      " [-0.25824886]\n",
      " ...\n",
      " [-0.19003581]\n",
      " [ 0.03627453]\n",
      " [-0.11629386]]\n",
      "Current iteration=2, loss=38418.61099890989\n",
      "t [[ 0.00544712]\n",
      " [-0.14386739]\n",
      " [-0.34504184]\n",
      " ...\n",
      " [-0.25043329]\n",
      " [ 0.04186289]\n",
      " [-0.16421839]]\n",
      "t [[ 0.00544712]\n",
      " [-0.14386739]\n",
      " [-0.34504184]\n",
      " ...\n",
      " [-0.25043329]\n",
      " [ 0.04186289]\n",
      " [-0.16421839]]\n",
      "t [[ 0.0013852 ]\n",
      " [-0.19967083]\n",
      " [-0.41506255]\n",
      " ...\n",
      " [-0.29738049]\n",
      " [ 0.04218998]\n",
      " [-0.20711186]]\n",
      "t [[ 0.0013852 ]\n",
      " [-0.19967083]\n",
      " [-0.41506255]\n",
      " ...\n",
      " [-0.29738049]\n",
      " [ 0.04218998]\n",
      " [-0.20711186]]\n",
      "Current iteration=4, loss=37334.953574782885\n",
      "t [[-0.00376785]\n",
      " [-0.25581299]\n",
      " [-0.47334863]\n",
      " ...\n",
      " [-0.33510865]\n",
      " [ 0.03867817]\n",
      " [-0.24585914]]\n",
      "t [[-0.00376785]\n",
      " [-0.25581299]\n",
      " [-0.47334863]\n",
      " ...\n",
      " [-0.33510865]\n",
      " [ 0.03867817]\n",
      " [-0.24585914]]\n",
      "t [[-0.00947582]\n",
      " [-0.31129661]\n",
      " [-0.52325252]\n",
      " ...\n",
      " [-0.36642482]\n",
      " [ 0.03231517]\n",
      " [-0.28111597]]\n",
      "t [[-0.00947582]\n",
      " [-0.31129661]\n",
      " [-0.52325252]\n",
      " ...\n",
      " [-0.36642482]\n",
      " [ 0.03231517]\n",
      " [-0.28111597]]\n",
      "Current iteration=6, loss=36586.080856613684\n",
      "t [[-0.01539694]\n",
      " [-0.36551667]\n",
      " [-0.56703966]\n",
      " ...\n",
      " [-0.39321541]\n",
      " [ 0.02380704]\n",
      " [-0.31338252]]\n",
      "t [[-0.01539694]\n",
      " [-0.36551667]\n",
      " [-0.56703966]\n",
      " ...\n",
      " [-0.39321541]\n",
      " [ 0.02380704]\n",
      " [-0.31338252]]\n",
      "t [[-0.02131406]\n",
      " [-0.41811866]\n",
      " [-0.60626341]\n",
      " ...\n",
      " [-0.41676315]\n",
      " [ 0.01367273]\n",
      " [-0.3430508 ]]\n",
      "t [[-0.02131406]\n",
      " [-0.41811866]\n",
      " [-0.60626341]\n",
      " ...\n",
      " [-0.41676315]\n",
      " [ 0.01367273]\n",
      " [-0.3430508 ]]\n",
      "Current iteration=8, loss=36023.69017754097\n",
      "t [[-0.02709111]\n",
      " [-0.46890953]\n",
      " [-0.64200273]\n",
      " ...\n",
      " [-0.43794853]\n",
      " [ 0.00230306]\n",
      " [-0.3704353 ]]\n",
      "t [[-0.02709111]\n",
      " [-0.46890953]\n",
      " [-0.64200273]\n",
      " ...\n",
      " [-0.43794853]\n",
      " [ 0.00230306]\n",
      " [-0.3704353 ]]\n",
      "t [[-0.03264535]\n",
      " [-0.51780067]\n",
      " [-0.67501561]\n",
      " ...\n",
      " [-0.45738044]\n",
      " [-0.01000163]\n",
      " [-0.39579351]]\n",
      "loss=35583.81493272021\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00536036]\n",
      " [-0.04313128]\n",
      " [-0.14606784]\n",
      " ...\n",
      " [-0.06430992]\n",
      " [-0.00774532]\n",
      " [-0.13120264]]\n",
      "t [[ 0.00536036]\n",
      " [-0.04313128]\n",
      " [-0.14606784]\n",
      " ...\n",
      " [-0.06430992]\n",
      " [-0.00774532]\n",
      " [-0.13120264]]\n",
      "t [[ 0.00552628]\n",
      " [-0.09477814]\n",
      " [-0.2560123 ]\n",
      " ...\n",
      " [-0.10546887]\n",
      " [-0.00783309]\n",
      " [-0.24891035]]\n",
      "t [[ 0.00552628]\n",
      " [-0.09477814]\n",
      " [-0.2560123 ]\n",
      " ...\n",
      " [-0.10546887]\n",
      " [-0.00783309]\n",
      " [-0.24891035]]\n",
      "Current iteration=2, loss=38385.90419298496\n",
      "t [[ 0.0025234 ]\n",
      " [-0.15088753]\n",
      " [-0.34139354]\n",
      " ...\n",
      " [-0.1315859 ]\n",
      " [-0.00353245]\n",
      " [-0.35561966]]\n",
      "t [[ 0.0025234 ]\n",
      " [-0.15088753]\n",
      " [-0.34139354]\n",
      " ...\n",
      " [-0.1315859 ]\n",
      " [-0.00353245]\n",
      " [-0.35561966]]\n",
      "t [[-0.00232941]\n",
      " [-0.20885658]\n",
      " [-0.40994795]\n",
      " ...\n",
      " [-0.14801669]\n",
      " [ 0.00306986]\n",
      " [-0.45322704]]\n",
      "t [[-0.00232941]\n",
      " [-0.20885658]\n",
      " [-0.40994795]\n",
      " ...\n",
      " [-0.14801669]\n",
      " [ 0.00306986]\n",
      " [-0.45322704]]\n",
      "Current iteration=4, loss=37284.02289205933\n",
      "t [[-0.00819829]\n",
      " [-0.26706651]\n",
      " [-0.46676526]\n",
      " ...\n",
      " [-0.15822747]\n",
      " [ 0.010681  ]\n",
      " [-0.54316525]]\n",
      "t [[-0.00819829]\n",
      " [-0.26706651]\n",
      " [-0.46676526]\n",
      " ...\n",
      " [-0.15822747]\n",
      " [ 0.010681  ]\n",
      " [-0.54316525]]\n",
      "t [[-0.01455632]\n",
      " [-0.32452228]\n",
      " [-0.51522758]\n",
      " ...\n",
      " [-0.16447141]\n",
      " [ 0.0185113 ]\n",
      " [-0.62653527]]\n",
      "t [[-0.01455632]\n",
      " [-0.32452228]\n",
      " [-0.51522758]\n",
      " ...\n",
      " [-0.16447141]\n",
      " [ 0.0185113 ]\n",
      " [-0.62653527]]\n",
      "Current iteration=6, loss=36521.8797711972\n",
      "t [[-0.02107002]\n",
      " [-0.38062253]\n",
      " [-0.55761716]\n",
      " ...\n",
      " [-0.16822454]\n",
      " [ 0.02608887]\n",
      " [-0.70420171]]\n",
      "t [[-0.02107002]\n",
      " [-0.38062253]\n",
      " [-0.55761716]\n",
      " ...\n",
      " [-0.16822454]\n",
      " [ 0.02608887]\n",
      " [-0.70420171]]\n",
      "t [[-0.02752947]\n",
      " [-0.43501692]\n",
      " [-0.59549661]\n",
      " ...\n",
      " [-0.17045826]\n",
      " [ 0.03314384]\n",
      " [-0.77685795]]\n",
      "t [[-0.02752947]\n",
      " [-0.43501692]\n",
      " [-0.59549661]\n",
      " ...\n",
      " [-0.17045826]\n",
      " [ 0.03314384]\n",
      " [-0.77685795]]\n",
      "Current iteration=8, loss=35948.94688602626\n",
      "t [[-0.03380484]\n",
      " [-0.4875167 ]\n",
      " [-0.62994949]\n",
      " ...\n",
      " [-0.17181171]\n",
      " [ 0.0395353 ]\n",
      " [-0.84507048]]\n",
      "t [[-0.03380484]\n",
      " [-0.4875167 ]\n",
      " [-0.62994949]\n",
      " ...\n",
      " [-0.17181171]\n",
      " [ 0.0395353 ]\n",
      " [-0.84507048]]\n",
      "t [[-0.03981873]\n",
      " [-0.53803755]\n",
      " [-0.66173559]\n",
      " ...\n",
      " [-0.17270283]\n",
      " [ 0.04520436]\n",
      " [-0.90930966]]\n",
      "loss=35500.48533112435\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.05986668]\n",
      " [-0.25574358]\n",
      " [-0.16074405]\n",
      " ...\n",
      " [-0.11649959]\n",
      " [ 0.0252228 ]\n",
      " [-0.06791314]]\n",
      "t [[ 0.05986668]\n",
      " [-0.25574358]\n",
      " [-0.16074405]\n",
      " ...\n",
      " [-0.11649959]\n",
      " [ 0.0252228 ]\n",
      " [-0.06791314]]\n",
      "t [[ 0.10272948]\n",
      " [-0.45311032]\n",
      " [-0.28311741]\n",
      " ...\n",
      " [-0.20090474]\n",
      " [ 0.03888838]\n",
      " [-0.12701433]]\n",
      "t [[ 0.10272948]\n",
      " [-0.45311032]\n",
      " [-0.28311741]\n",
      " ...\n",
      " [-0.20090474]\n",
      " [ 0.03888838]\n",
      " [-0.12701433]]\n",
      "Current iteration=2, loss=38350.70564974434\n",
      "t [[ 0.13378123]\n",
      " [-0.60990722]\n",
      " [-0.37948028]\n",
      " ...\n",
      " [-0.26407251]\n",
      " [ 0.04448721]\n",
      " [-0.17921953]]\n",
      "t [[ 0.13378123]\n",
      " [-0.60990722]\n",
      " [-0.37948028]\n",
      " ...\n",
      " [-0.26407251]\n",
      " [ 0.04448721]\n",
      " [-0.17921953]]\n",
      "t [[ 0.15655513]\n",
      " [-0.73802005]\n",
      " [-0.45796895]\n",
      " ...\n",
      " [-0.31311212]\n",
      " [ 0.04438347]\n",
      " [-0.22590104]]\n",
      "t [[ 0.15655513]\n",
      " [-0.73802005]\n",
      " [-0.45796895]\n",
      " ...\n",
      " [-0.31311212]\n",
      " [ 0.04438347]\n",
      " [-0.22590104]]\n",
      "Current iteration=4, loss=37248.542550149985\n",
      "t [[ 0.17342741]\n",
      " [-0.84529401]\n",
      " [-0.52386395]\n",
      " ...\n",
      " [-0.35260506]\n",
      " [ 0.04016151]\n",
      " [-0.26804126]]\n",
      "t [[ 0.17342741]\n",
      " [-0.84529401]\n",
      " [-0.52386395]\n",
      " ...\n",
      " [-0.35260506]\n",
      " [ 0.04016151]\n",
      " [-0.26804126]]\n",
      "t [[ 0.18601954]\n",
      " [-0.93700469]\n",
      " [-0.58064355]\n",
      " ...\n",
      " [-0.38553513]\n",
      " [ 0.03291077]\n",
      " [-0.30636311]]\n",
      "t [[ 0.18601954]\n",
      " [-0.93700469]\n",
      " [-0.58064355]\n",
      " ...\n",
      " [-0.38553513]\n",
      " [ 0.03291077]\n",
      " [-0.30636311]]\n",
      "Current iteration=6, loss=36492.563515144575\n",
      "t [[ 0.19545943]\n",
      " [-1.0167871 ]\n",
      " [-0.63064484]\n",
      " ...\n",
      " [-0.41387394]\n",
      " [ 0.02340483]\n",
      " [-0.34141564]]\n",
      "t [[ 0.19545943]\n",
      " [-1.0167871 ]\n",
      " [-0.63064484]\n",
      " ...\n",
      " [-0.41387394]\n",
      " [ 0.02340483]\n",
      " [-0.34141564]]\n",
      "t [[ 0.2025461 ]\n",
      " [-1.08720592]\n",
      " [-0.6754681 ]\n",
      " ...\n",
      " [-0.43894097]\n",
      " [ 0.01220942]\n",
      " [-0.37362803]]\n",
      "t [[ 0.2025461 ]\n",
      " [-1.08720592]\n",
      " [-0.6754681 ]\n",
      " ...\n",
      " [-0.43894097]\n",
      " [ 0.01220942]\n",
      " [-0.37362803]]\n",
      "Current iteration=8, loss=35928.26144043405\n",
      "t [[ 2.07854908e-01]\n",
      " [-1.15011082e+00]\n",
      " [-7.16228138e-01]\n",
      " ...\n",
      " [-4.61628181e-01]\n",
      " [-2.51283621e-04]\n",
      " [-4.03344228e-01]]\n",
      "t [[ 2.07854908e-01]\n",
      " [-1.15011082e+00]\n",
      " [-7.16228138e-01]\n",
      " ...\n",
      " [-4.61628181e-01]\n",
      " [-2.51283621e-04]\n",
      " [-4.03344228e-01]]\n",
      "t [[ 0.2118063 ]\n",
      " [-1.20686335]\n",
      " [-0.75371427]\n",
      " ...\n",
      " [-0.48254339]\n",
      " [-0.01365265]\n",
      " [-0.43084564]]\n",
      "loss=35489.4027881978\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00789484]\n",
      " [-0.04860745]\n",
      " [-0.15238942]\n",
      " ...\n",
      " [-0.11694034]\n",
      " [ 0.022734  ]\n",
      " [-0.0693742 ]]\n",
      "t [[ 0.00789484]\n",
      " [-0.04860745]\n",
      " [-0.15238942]\n",
      " ...\n",
      " [-0.11694034]\n",
      " [ 0.022734  ]\n",
      " [-0.0693742 ]]\n",
      "t [[ 0.00992046]\n",
      " [-0.10620152]\n",
      " [-0.26539949]\n",
      " ...\n",
      " [-0.20109599]\n",
      " [ 0.03418386]\n",
      " [-0.12973703]]\n",
      "t [[ 0.00992046]\n",
      " [-0.10620152]\n",
      " [-0.26539949]\n",
      " ...\n",
      " [-0.20109599]\n",
      " [ 0.03418386]\n",
      " [-0.12973703]]\n",
      "Current iteration=2, loss=38327.88615062084\n",
      "t [[ 0.00840253]\n",
      " [-0.16817155]\n",
      " [-0.35226851]\n",
      " ...\n",
      " [-0.26367483]\n",
      " [ 0.0378515 ]\n",
      " [-0.18306469]]\n",
      "t [[ 0.00840253]\n",
      " [-0.16817155]\n",
      " [-0.35226851]\n",
      " ...\n",
      " [-0.26367483]\n",
      " [ 0.0378515 ]\n",
      " [-0.18306469]]\n",
      "t [[ 0.00481807]\n",
      " [-0.23165539]\n",
      " [-0.42161021]\n",
      " ...\n",
      " [-0.3119709 ]\n",
      " [ 0.03608419]\n",
      " [-0.23076535]]\n",
      "t [[ 0.00481807]\n",
      " [-0.23165539]\n",
      " [-0.42161021]\n",
      " ...\n",
      " [-0.3119709 ]\n",
      " [ 0.03608419]\n",
      " [-0.23076535]]\n",
      "Current iteration=4, loss=37209.93390851785\n",
      "t [[ 8.07902708e-05]\n",
      " [-2.94935309e-01]\n",
      " [-4.78947479e-01]\n",
      " ...\n",
      " [-3.50659425e-01]\n",
      " [ 3.04389648e-02]\n",
      " [-2.73843187e-01]]\n",
      "t [[ 8.07902708e-05]\n",
      " [-2.94935309e-01]\n",
      " [-4.78947479e-01]\n",
      " ...\n",
      " [-3.50659425e-01]\n",
      " [ 3.04389648e-02]\n",
      " [-2.73843187e-01]]\n",
      "t [[-0.00524172]\n",
      " [-0.35699574]\n",
      " [-0.52786871]\n",
      " ...\n",
      " [-0.38277144]\n",
      " [ 0.02197632]\n",
      " [-0.31303528]]\n",
      "t [[-0.00524172]\n",
      " [-0.35699574]\n",
      " [-0.52786871]\n",
      " ...\n",
      " [-0.38277144]\n",
      " [ 0.02197632]\n",
      " [-0.31303528]]\n",
      "Current iteration=6, loss=36440.70455366393\n",
      "t [[-0.0107949 ]\n",
      " [-0.41725193]\n",
      " [-0.57074664]\n",
      " ...\n",
      " [-0.41030201]\n",
      " [ 0.01144253]\n",
      " [-0.3489005 ]]\n",
      "t [[-0.0107949 ]\n",
      " [-0.41725193]\n",
      " [-0.57074664]\n",
      " ...\n",
      " [-0.41030201]\n",
      " [ 0.01144253]\n",
      " [-0.3489005 ]]\n",
      "t [[-0.01635775]\n",
      " [-0.47538654]\n",
      " [-0.60917534]\n",
      " ...\n",
      " [-0.43458162]\n",
      " [-0.00062124]\n",
      " [-0.38187536]]\n",
      "t [[-0.01635775]\n",
      " [-0.47538654]\n",
      " [-0.60917534]\n",
      " ...\n",
      " [-0.43458162]\n",
      " [-0.00062124]\n",
      " [-0.38187536]]\n",
      "Current iteration=8, loss=35865.69015663293\n",
      "t [[-0.02179439]\n",
      " [-0.53124963]\n",
      " [-0.64424049]\n",
      " ...\n",
      " [-0.45650682]\n",
      " [-0.0138125 ]\n",
      " [-0.41230943]]\n",
      "t [[-0.02179439]\n",
      " [-0.53124963]\n",
      " [-0.64424049]\n",
      " ...\n",
      " [-0.45650682]\n",
      " [-0.0138125 ]\n",
      " [-0.41230943]]\n",
      "t [[-0.02702383]\n",
      " [-0.58479614]\n",
      " [-0.67669057]\n",
      " ...\n",
      " [-0.47668671]\n",
      " [-0.02782562]\n",
      " [-0.44048868]]\n",
      "loss=35418.37825936684\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00674306]\n",
      " [-0.04266081]\n",
      " [-0.15403985]\n",
      " ...\n",
      " [-0.11498472]\n",
      " [ 0.02444349]\n",
      " [-0.06507329]]\n",
      "t [[ 0.00674306]\n",
      " [-0.04266081]\n",
      " [-0.15403985]\n",
      " ...\n",
      " [-0.11498472]\n",
      " [ 0.02444349]\n",
      " [-0.06507329]]\n",
      "t [[ 0.00767427]\n",
      " [-0.09472888]\n",
      " [-0.26877801]\n",
      " ...\n",
      " [-0.19762986]\n",
      " [ 0.0374901 ]\n",
      " [-0.12143902]]\n",
      "t [[ 0.00767427]\n",
      " [-0.09472888]\n",
      " [-0.26877801]\n",
      " ...\n",
      " [-0.19762986]\n",
      " [ 0.0374901 ]\n",
      " [-0.12143902]]\n",
      "Current iteration=2, loss=38350.10434005553\n",
      "t [[ 0.00513158]\n",
      " [-0.15157661]\n",
      " [-0.35732743]\n",
      " ...\n",
      " [-0.25896921]\n",
      " [ 0.04262966]\n",
      " [-0.17103244]]\n",
      "t [[ 0.00513158]\n",
      " [-0.15157661]\n",
      " [-0.35732743]\n",
      " ...\n",
      " [-0.25896921]\n",
      " [ 0.04262966]\n",
      " [-0.17103244]]\n",
      "t [[ 0.00059963]\n",
      " [-0.21030945]\n",
      " [-0.42824921]\n",
      " ...\n",
      " [-0.30620275]\n",
      " [ 0.04220821]\n",
      " [-0.21523063]]\n",
      "t [[ 0.00059963]\n",
      " [-0.21030945]\n",
      " [-0.42824921]\n",
      " ...\n",
      " [-0.30620275]\n",
      " [ 0.04220821]\n",
      " [-0.21523063]]\n",
      "Current iteration=4, loss=37248.571608259845\n",
      "t [[-0.00500406]\n",
      " [-0.2691736 ]\n",
      " [-0.48705056]\n",
      " ...\n",
      " [-0.34395522]\n",
      " [ 0.03778725]\n",
      " [-0.25501336]]\n",
      "t [[-0.00500406]\n",
      " [-0.2691736 ]\n",
      " [-0.48705056]\n",
      " ...\n",
      " [-0.34395522]\n",
      " [ 0.03778725]\n",
      " [-0.25501336]]\n",
      "t [[-0.01111114]\n",
      " [-0.32711952]\n",
      " [-0.53732028]\n",
      " ...\n",
      " [-0.37522933]\n",
      " [ 0.030434  ]\n",
      " [-0.29109797]]\n",
      "t [[-0.01111114]\n",
      " [-0.32711952]\n",
      " [-0.53732028]\n",
      " ...\n",
      " [-0.37522933]\n",
      " [ 0.030434  ]\n",
      " [-0.29109797]]\n",
      "Current iteration=6, loss=36492.77403937564\n",
      "t [[-0.01736856]\n",
      " [-0.38353218]\n",
      " [-0.58143797]\n",
      " ...\n",
      " [-0.40200329]\n",
      " [ 0.02090213]\n",
      " [-0.32402692]]\n",
      "t [[-0.01736856]\n",
      " [-0.38353218]\n",
      " [-0.58143797]\n",
      " ...\n",
      " [-0.40200329]\n",
      " [ 0.02090213]\n",
      " [-0.32402692]]\n",
      "t [[-0.0235584 ]\n",
      " [-0.43806772]\n",
      " [-0.62100676]\n",
      " ...\n",
      " [-0.42559686]\n",
      " [ 0.00974034]\n",
      " [-0.35422275]]\n",
      "t [[-0.0235584 ]\n",
      " [-0.43806772]\n",
      " [-0.62100676]\n",
      " ...\n",
      " [-0.42559686]\n",
      " [ 0.00974034]\n",
      " [-0.35422275]]\n",
      "Current iteration=8, loss=35928.6481737776\n",
      "t [[-0.02954885]\n",
      " [-0.49055307]\n",
      " [-0.65712174]\n",
      " ...\n",
      " [-0.44689914]\n",
      " [-0.0026414 ]\n",
      " [-0.38202293]]\n",
      "t [[-0.02954885]\n",
      " [-0.49055307]\n",
      " [-0.65712174]\n",
      " ...\n",
      " [-0.44689914]\n",
      " [-0.0026414 ]\n",
      " [-0.38202293]]\n",
      "t [[-0.03526353]\n",
      " [-0.54092297]\n",
      " [-0.6905403 ]\n",
      " ...\n",
      " [-0.46651367]\n",
      " [-0.01593012]\n",
      " [-0.40770277]]\n",
      "loss=35490.095511122614\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00561561]\n",
      " [-0.04518516]\n",
      " [-0.15302345]\n",
      " ...\n",
      " [-0.06737229]\n",
      " [-0.00811415]\n",
      " [-0.13745039]]\n",
      "t [[ 0.00561561]\n",
      " [-0.04518516]\n",
      " [-0.15302345]\n",
      " ...\n",
      " [-0.06737229]\n",
      " [-0.00811415]\n",
      " [-0.13745039]]\n",
      "t [[ 0.00553421]\n",
      " [-0.09971025]\n",
      " [-0.26642063]\n",
      " ...\n",
      " [-0.10934872]\n",
      " [-0.00782788]\n",
      " [-0.26009487]]\n",
      "t [[ 0.00553421]\n",
      " [-0.09971025]\n",
      " [-0.26642063]\n",
      " ...\n",
      " [-0.10934872]\n",
      " [-0.00782788]\n",
      " [-0.26009487]]\n",
      "Current iteration=2, loss=38316.253460175954\n",
      "t [[ 0.00208318]\n",
      " [-0.15891883]\n",
      " [-0.35347757]\n",
      " ...\n",
      " [-0.13524808]\n",
      " [-0.00289736]\n",
      " [-0.37080596]]\n",
      "t [[ 0.00208318]\n",
      " [-0.15891883]\n",
      " [-0.35347757]\n",
      " ...\n",
      " [-0.13524808]\n",
      " [-0.00289736]\n",
      " [-0.37080596]]\n",
      "t [[-0.00326475]\n",
      " [-0.21990597]\n",
      " [-0.42285596]\n",
      " ...\n",
      " [-0.15106144]\n",
      " [ 0.00434828]\n",
      " [-0.47172653]]\n",
      "t [[-0.00326475]\n",
      " [-0.21990597]\n",
      " [-0.42285596]\n",
      " ...\n",
      " [-0.15106144]\n",
      " [ 0.00434828]\n",
      " [-0.47172653]]\n",
      "Current iteration=4, loss=37196.159775066946\n",
      "t [[-0.00960361]\n",
      " [-0.28091707]\n",
      " [-0.48011872]\n",
      " ...\n",
      " [-0.16056853]\n",
      " [ 0.01250816]\n",
      " [-0.56445134]]\n",
      "t [[-0.00960361]\n",
      " [-0.28091707]\n",
      " [-0.48011872]\n",
      " ...\n",
      " [-0.16056853]\n",
      " [ 0.01250816]\n",
      " [-0.56445134]]\n",
      "t [[-0.01637559]\n",
      " [-0.34090564]\n",
      " [-0.52888536]\n",
      " ...\n",
      " [-0.16616916]\n",
      " [ 0.02075317]\n",
      " [-0.65019211]]\n",
      "t [[-0.01637559]\n",
      " [-0.34090564]\n",
      " [-0.52888536]\n",
      " ...\n",
      " [-0.16616916]\n",
      " [ 0.02075317]\n",
      " [-0.65019211]]\n",
      "Current iteration=6, loss=36426.87183143576\n",
      "t [[-0.02323696]\n",
      " [-0.39926106]\n",
      " [-0.57155227]\n",
      " ...\n",
      " [-0.1693998 ]\n",
      " [ 0.02860614]\n",
      " [-0.72989249]]\n",
      "t [[-0.02323696]\n",
      " [-0.39926106]\n",
      " [-0.57155227]\n",
      " ...\n",
      " [-0.1693998 ]\n",
      " [ 0.02860614]\n",
      " [-0.72989249]]\n",
      "t [[-0.0299778 ]\n",
      " [-0.45564434]\n",
      " [-0.60973134]\n",
      " ...\n",
      " [-0.17124808]\n",
      " [ 0.03580831]\n",
      " [-0.80430445]]\n",
      "t [[-0.0299778 ]\n",
      " [-0.45564434]\n",
      " [-0.60973134]\n",
      " ...\n",
      " [-0.17124808]\n",
      " [ 0.03580831]\n",
      " [-0.80430445]]\n",
      "Current iteration=8, loss=35852.08176485558\n",
      "t [[-0.03647315]\n",
      " [-0.50988738]\n",
      " [-0.64452161]\n",
      " ...\n",
      " [-0.17234735]\n",
      " [ 0.04223678]\n",
      " [-0.87403934]]\n",
      "t [[-0.03647315]\n",
      " [-0.50988738]\n",
      " [-0.64452161]\n",
      " ...\n",
      " [-0.17234735]\n",
      " [ 0.04223678]\n",
      " [-0.87403934]]\n",
      "t [[-0.04265247]\n",
      " [-0.56192982]\n",
      " [-0.67668148]\n",
      " ...\n",
      " [-0.17309979]\n",
      " [ 0.04785252]\n",
      " [-0.93960272]]\n",
      "loss=35404.90843436496\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.06258789]\n",
      " [-0.26736829]\n",
      " [-0.1680506 ]\n",
      " ...\n",
      " [-0.12179503]\n",
      " [ 0.02636929]\n",
      " [-0.0710001 ]]\n",
      "t [[ 0.06258789]\n",
      " [-0.26736829]\n",
      " [-0.1680506 ]\n",
      " ...\n",
      " [-0.12179503]\n",
      " [ 0.02636929]\n",
      " [-0.0710001 ]]\n",
      "t [[ 0.10659988]\n",
      " [-0.47096379]\n",
      " [-0.29418429]\n",
      " ...\n",
      " [-0.20852892]\n",
      " [ 0.04011442]\n",
      " [-0.13237261]]\n",
      "t [[ 0.10659988]\n",
      " [-0.47096379]\n",
      " [-0.29418429]\n",
      " ...\n",
      " [-0.20852892]\n",
      " [ 0.04011442]\n",
      " [-0.13237261]]\n",
      "Current iteration=2, loss=38283.695572082135\n",
      "t [[ 0.137968  ]\n",
      " [-0.63112463]\n",
      " [-0.39251689]\n",
      " ...\n",
      " [-0.27260095]\n",
      " [ 0.0452214 ]\n",
      " [-0.18630677]]\n",
      "t [[ 0.137968  ]\n",
      " [-0.63112463]\n",
      " [-0.39251689]\n",
      " ...\n",
      " [-0.27260095]\n",
      " [ 0.0452214 ]\n",
      " [-0.18630677]]\n",
      "t [[ 0.16063296]\n",
      " [-0.76108745]\n",
      " [-0.47210088]\n",
      " ...\n",
      " [-0.32191906]\n",
      " [ 0.04432403]\n",
      " [-0.23433838]]\n",
      "t [[ 0.16063296]\n",
      " [-0.76108745]\n",
      " [-0.47210088]\n",
      " ...\n",
      " [-0.32191906]\n",
      " [ 0.04432403]\n",
      " [-0.23433838]]\n",
      "Current iteration=4, loss=37164.93087393109\n",
      "t [[ 0.17718932]\n",
      " [-0.86939818]\n",
      " [-0.53867548]\n",
      " ...\n",
      " [-0.36145771]\n",
      " [ 0.0391516 ]\n",
      " [-0.277549  ]]\n",
      "t [[ 0.17718932]\n",
      " [-0.86939818]\n",
      " [-0.53867548]\n",
      " ...\n",
      " [-0.36145771]\n",
      " [ 0.0391516 ]\n",
      " [-0.277549  ]]\n",
      "t [[ 0.18937617]\n",
      " [-0.96170082]\n",
      " [-0.59594637]\n",
      " ...\n",
      " [-0.39438791]\n",
      " [ 0.03087531]\n",
      " [-0.31672538]]\n",
      "t [[ 0.18937617]\n",
      " [-0.96170082]\n",
      " [-0.59594637]\n",
      " ...\n",
      " [-0.39438791]\n",
      " [ 0.03087531]\n",
      " [-0.31672538]]\n",
      "Current iteration=6, loss=36402.817023603595\n",
      "t [[ 0.19838465]\n",
      " [-1.04182637]\n",
      " [-0.64636012]\n",
      " ...\n",
      " [-0.42276386]\n",
      " [ 0.02031729]\n",
      " [-0.3524603 ]]\n",
      "t [[ 0.19838465]\n",
      " [-1.04182637]\n",
      " [-0.64636012]\n",
      " ...\n",
      " [-0.42276386]\n",
      " [ 0.02031729]\n",
      " [-0.3524603 ]]\n",
      "t [[ 0.20504706]\n",
      " [-1.11244394]\n",
      " [-0.6915657 ]\n",
      " ...\n",
      " [-0.44793435]\n",
      " [ 0.00807339]\n",
      " [-0.38521454]]\n",
      "t [[ 0.20504706]\n",
      " [-1.11244394]\n",
      " [-0.6915657 ]\n",
      " ...\n",
      " [-0.44793435]\n",
      " [ 0.00807339]\n",
      " [-0.38521454]]\n",
      "Current iteration=8, loss=35837.38154599632\n",
      "t [[ 0.20995532]\n",
      " [-1.17545827]\n",
      " [-0.7326956 ]\n",
      " ...\n",
      " [-0.4707943 ]\n",
      " [-0.005413  ]\n",
      " [-0.41535584]]\n",
      "t [[ 0.20995532]\n",
      " [-1.17545827]\n",
      " [-0.7326956 ]\n",
      " ...\n",
      " [-0.4707943 ]\n",
      " [-0.005413  ]\n",
      " [-0.41535584]]\n",
      "t [[ 0.21353715]\n",
      " [-1.23225943]\n",
      " [-0.77054165]\n",
      " ...\n",
      " [-0.49194235]\n",
      " [-0.01980488]\n",
      " [-0.44318417]]\n",
      "loss=35400.27400438801\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00825369]\n",
      " [-0.05081688]\n",
      " [-0.15931622]\n",
      " ...\n",
      " [-0.12225581]\n",
      " [ 0.02376736]\n",
      " [-0.07252757]]\n",
      "t [[ 0.00825369]\n",
      " [-0.05081688]\n",
      " [-0.15931622]\n",
      " ...\n",
      " [-0.12225581]\n",
      " [ 0.02376736]\n",
      " [-0.07252757]]\n",
      "t [[ 0.0100971 ]\n",
      " [-0.11144924]\n",
      " [-0.27561378]\n",
      " ...\n",
      " [-0.20869676]\n",
      " [ 0.03520911]\n",
      " [-0.13520976]]\n",
      "t [[ 0.0100971 ]\n",
      " [-0.11144924]\n",
      " [-0.27561378]\n",
      " ...\n",
      " [-0.20869676]\n",
      " [ 0.03520911]\n",
      " [-0.13520976]]\n",
      "Current iteration=2, loss=38260.06264382892\n",
      "t [[ 0.00818526]\n",
      " [-0.17663371]\n",
      " [-0.36401056]\n",
      " ...\n",
      " [-0.27212224]\n",
      " [ 0.038324  ]\n",
      " [-0.19030455]]\n",
      "t [[ 0.00818526]\n",
      " [-0.17663371]\n",
      " [-0.36401056]\n",
      " ...\n",
      " [-0.27212224]\n",
      " [ 0.038324  ]\n",
      " [-0.19030455]]\n",
      "t [[ 0.00415636]\n",
      " [-0.24319959]\n",
      " [-0.4340829 ]\n",
      " ...\n",
      " [-0.32064215]\n",
      " [ 0.03572503]\n",
      " [-0.23938734]]\n",
      "t [[ 0.00415636]\n",
      " [-0.24319959]\n",
      " [-0.4340829 ]\n",
      " ...\n",
      " [-0.32064215]\n",
      " [ 0.03572503]\n",
      " [-0.23938734]]\n",
      "Current iteration=4, loss=37124.98102912007\n",
      "t [[-0.00100335]\n",
      " [-0.30930084]\n",
      " [-0.49182356]\n",
      " ...\n",
      " [-0.3593291 ]\n",
      " [ 0.02910943]\n",
      " [-0.28356317]]\n",
      "t [[-0.00100335]\n",
      " [-0.30930084]\n",
      " [-0.49182356]\n",
      " ...\n",
      " [-0.3593291 ]\n",
      " [ 0.02910943]\n",
      " [-0.28356317]]\n",
      "t [[-0.00669643]\n",
      " [-0.37388072]\n",
      " [-0.54104475]\n",
      " ...\n",
      " [-0.39140144]\n",
      " [ 0.01961526]\n",
      " [-0.32363385]]\n",
      "t [[-0.00669643]\n",
      " [-0.37388072]\n",
      " [-0.54104475]\n",
      " ...\n",
      " [-0.39140144]\n",
      " [ 0.01961526]\n",
      " [-0.32363385]]\n",
      "Current iteration=6, loss=36349.30015674265\n",
      "t [[-0.01255916]\n",
      " [-0.43635473]\n",
      " [-0.58421939]\n",
      " ...\n",
      " [-0.41893561]\n",
      " [ 0.0080339 ]\n",
      " [-0.36020257]]\n",
      "t [[-0.01255916]\n",
      " [-0.43635473]\n",
      " [-0.58421939]\n",
      " ...\n",
      " [-0.41893561]\n",
      " [ 0.0080339 ]\n",
      " [-0.36020257]]\n",
      "t [[-0.01837127]\n",
      " [-0.49642464]\n",
      " [-0.6229794 ]\n",
      " ...\n",
      " [-0.44329006]\n",
      " [-0.00506614]\n",
      " [-0.3937378 ]]\n",
      "t [[-0.01837127]\n",
      " [-0.49642464]\n",
      " [-0.6229794 ]\n",
      " ...\n",
      " [-0.44329006]\n",
      " [-0.00506614]\n",
      " [-0.3937378 ]]\n",
      "Current iteration=8, loss=35773.05682734818\n",
      "t [[-0.02400203]\n",
      " [-0.5539671 ]\n",
      " [-0.65841752]\n",
      " ...\n",
      " [-0.46536311]\n",
      " [-0.01926542]\n",
      " [-0.42461326]]\n",
      "t [[-0.02400203]\n",
      " [-0.5539671 ]\n",
      " [-0.65841752]\n",
      " ...\n",
      " [-0.46536311]\n",
      " [-0.01926542]\n",
      " [-0.42461326]]\n",
      "t [[-0.02937724]\n",
      " [-0.60896536]\n",
      " [-0.69127521]\n",
      " ...\n",
      " [-0.48575382]\n",
      " [-0.03424764]\n",
      " [-0.45313369]]\n",
      "loss=35327.53692150705\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00704957]\n",
      " [-0.04459993]\n",
      " [-0.16104166]\n",
      " ...\n",
      " [-0.1202113 ]\n",
      " [ 0.02555455]\n",
      " [-0.06803117]]\n",
      "t [[ 0.00704957]\n",
      " [-0.04459993]\n",
      " [-0.16104166]\n",
      " ...\n",
      " [-0.1202113 ]\n",
      " [ 0.02555455]\n",
      " [-0.06803117]]\n",
      "t [[ 0.00775145]\n",
      " [-0.09947529]\n",
      " [-0.27914909]\n",
      " ...\n",
      " [-0.2050938 ]\n",
      " [ 0.03866006]\n",
      " [-0.12654896]]\n",
      "t [[ 0.00775145]\n",
      " [-0.09947529]\n",
      " [-0.27914909]\n",
      " ...\n",
      " [-0.2050938 ]\n",
      " [ 0.03866006]\n",
      " [-0.12654896]]\n",
      "Current iteration=2, loss=38283.13616899905\n",
      "t [[ 0.00477472]\n",
      " [-0.15934287]\n",
      " [-0.36929752]\n",
      " ...\n",
      " [-0.26724844]\n",
      " [ 0.04330167]\n",
      " [-0.17776446]]\n",
      "t [[ 0.00477472]\n",
      " [-0.15934287]\n",
      " [-0.36929752]\n",
      " ...\n",
      " [-0.26724844]\n",
      " [ 0.04330167]\n",
      " [-0.17776446]]\n",
      "t [[-2.33899936e-04]\n",
      " [-2.20993967e-01]\n",
      " [-4.41007344e-01]\n",
      " ...\n",
      " [-3.14682088e-01]\n",
      " [ 4.20917209e-02]\n",
      " [-2.23218042e-01]]\n",
      "t [[-2.33899936e-04]\n",
      " [-2.20993967e-01]\n",
      " [-4.41007344e-01]\n",
      " ...\n",
      " [-3.14682088e-01]\n",
      " [ 4.20917209e-02]\n",
      " [-2.23218042e-01]]\n",
      "Current iteration=4, loss=37164.99211522563\n",
      "t [[-0.00628416]\n",
      " [-0.2825412 ]\n",
      " [-0.50025689]\n",
      " ...\n",
      " [-0.35241393]\n",
      " [ 0.03673312]\n",
      " [-0.2639876 ]]\n",
      "t [[-0.00628416]\n",
      " [-0.2825412 ]\n",
      " [-0.50025689]\n",
      " ...\n",
      " [-0.35241393]\n",
      " [ 0.03673312]\n",
      " [-0.2639876 ]]\n",
      "t [[-0.01277823]\n",
      " [-0.34288953]\n",
      " [-0.55086098]\n",
      " ...\n",
      " [-0.38363312]\n",
      " [ 0.0283718 ]\n",
      " [-0.30085313]]\n",
      "t [[-0.01277823]\n",
      " [-0.34288953]\n",
      " [-0.55086098]\n",
      " ...\n",
      " [-0.38363312]\n",
      " [ 0.0283718 ]\n",
      " [-0.30085313]]\n",
      "Current iteration=6, loss=36403.04471542586\n",
      "t [[-0.01935457]\n",
      " [-0.4014207 ]\n",
      " [-0.59530146]\n",
      " ...\n",
      " [-0.41039902]\n",
      " [ 0.0178077 ]\n",
      " [-0.33439979]]\n",
      "t [[-0.01935457]\n",
      " [-0.4014207 ]\n",
      " [-0.59530146]\n",
      " ...\n",
      " [-0.41039902]\n",
      " [ 0.0178077 ]\n",
      " [-0.33439979]]\n",
      "t [[-0.02579679]\n",
      " [-0.4578072 ]\n",
      " [-0.63522107]\n",
      " ...\n",
      " [-0.43405924]\n",
      " [ 0.00561795]\n",
      " [-0.36508073]]\n",
      "t [[-0.02579679]\n",
      " [-0.4578072 ]\n",
      " [-0.63522107]\n",
      " ...\n",
      " [-0.43405924]\n",
      " [ 0.00561795]\n",
      " [-0.36508073]]\n",
      "Current iteration=8, loss=35837.80967688562\n",
      "t [[-0.0319791 ]\n",
      " [-0.51190032]\n",
      " [-0.67172333]\n",
      " ...\n",
      " [-0.45550458]\n",
      " [-0.00776949]\n",
      " [-0.3932564 ]]\n",
      "t [[-0.0319791 ]\n",
      " [-0.51190032]\n",
      " [-0.67172333]\n",
      " ...\n",
      " [-0.45550458]\n",
      " [-0.00776949]\n",
      " [-0.3932564 ]]\n",
      "t [[-0.03783276]\n",
      " [-0.56366127]\n",
      " [-0.70555967]\n",
      " ...\n",
      " [-0.47532835]\n",
      " [-0.02203004]\n",
      " [-0.41921994]]\n",
      "loss=35401.04928661352\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00587087]\n",
      " [-0.04723903]\n",
      " [-0.15997906]\n",
      " ...\n",
      " [-0.07043467]\n",
      " [-0.00848297]\n",
      " [-0.14369813]]\n",
      "t [[ 0.00587087]\n",
      " [-0.04723903]\n",
      " [-0.15997906]\n",
      " ...\n",
      " [-0.07043467]\n",
      " [-0.00848297]\n",
      " [-0.14369813]]\n",
      "t [[ 0.0055195 ]\n",
      " [-0.1046796 ]\n",
      " [-0.27666965]\n",
      " ...\n",
      " [-0.11312647]\n",
      " [-0.00778883]\n",
      " [-0.27121934]]\n",
      "t [[ 0.0055195 ]\n",
      " [-0.1046796 ]\n",
      " [-0.27666965]\n",
      " ...\n",
      " [-0.11312647]\n",
      " [-0.00778883]\n",
      " [-0.27121934]]\n",
      "Current iteration=2, loss=38248.168734380815\n",
      "t [[ 0.00160291]\n",
      " [-0.16700591]\n",
      " [-0.36524439]\n",
      " ...\n",
      " [-0.13871659]\n",
      " [-0.00220605]\n",
      " [-0.38584772]]\n",
      "t [[ 0.00160291]\n",
      " [-0.16700591]\n",
      " [-0.36524439]\n",
      " ...\n",
      " [-0.13871659]\n",
      " [-0.00220605]\n",
      " [-0.38584772]]\n",
      "t [[-0.0042458 ]\n",
      " [-0.23099861]\n",
      " [-0.43533427]\n",
      " ...\n",
      " [-0.15386059]\n",
      " [ 0.00568379]\n",
      " [-0.4899885 ]]\n",
      "t [[-0.0042458 ]\n",
      " [-0.23099861]\n",
      " [-0.43533427]\n",
      " ...\n",
      " [-0.15386059]\n",
      " [ 0.00568379]\n",
      " [-0.4899885 ]]\n",
      "Current iteration=4, loss=37111.14016257928\n",
      "t [[-0.01104953]\n",
      " [-0.29477029]\n",
      " [-0.49297674]\n",
      " ...\n",
      " [-0.16265045]\n",
      " [ 0.01437447]\n",
      " [-0.58540525]]\n",
      "t [[-0.01104953]\n",
      " [-0.29477029]\n",
      " [-0.49297674]\n",
      " ...\n",
      " [-0.16265045]\n",
      " [ 0.01437447]\n",
      " [-0.58540525]]\n",
      "t [[-0.01822233]\n",
      " [-0.35722982]\n",
      " [-0.54201812]\n",
      " ...\n",
      " [-0.16762315]\n",
      " [ 0.02300322]\n",
      " [-0.67342344]]\n",
      "t [[-0.01822233]\n",
      " [-0.35722982]\n",
      " [-0.54201812]\n",
      " ...\n",
      " [-0.16762315]\n",
      " [ 0.02300322]\n",
      " [-0.67342344]]\n",
      "Current iteration=6, loss=36335.49311570435\n",
      "t [[-0.02541303]\n",
      " [-0.41776418]\n",
      " [-0.58495687]\n",
      " ...\n",
      " [-0.17036541]\n",
      " [ 0.03109344]\n",
      " [-0.75506715]]\n",
      "t [[-0.02541303]\n",
      " [-0.41776418]\n",
      " [-0.58495687]\n",
      " ...\n",
      " [-0.17036541]\n",
      " [ 0.03109344]\n",
      " [-0.75506715]]\n",
      "t [[-0.03241408]\n",
      " [-0.47605149]\n",
      " [-0.62344381]\n",
      " ...\n",
      " [-0.17187239]\n",
      " [ 0.03840223]\n",
      " [-0.83114758]]\n",
      "t [[-0.03241408]\n",
      " [-0.47605149]\n",
      " [-0.62344381]\n",
      " ...\n",
      " [-0.17187239]\n",
      " [ 0.03840223]\n",
      " [-0.83114758]]\n",
      "Current iteration=8, loss=35759.48690468999\n",
      "t [[-0.03910714]\n",
      " [-0.53194871]\n",
      " [-0.65858565]\n",
      " ...\n",
      " [-0.17276511]\n",
      " [ 0.04482821]\n",
      " [-0.90232124]]\n",
      "t [[-0.03910714]\n",
      " [-0.53194871]\n",
      " [-0.65858565]\n",
      " ...\n",
      " [-0.17276511]\n",
      " [ 0.04482821]\n",
      " [-0.90232124]]\n",
      "t [[-0.04542976]\n",
      " [-0.58542263]\n",
      " [-0.69113399]\n",
      " ...\n",
      " [-0.17342549]\n",
      " [ 0.0503546 ]\n",
      " [-0.96912905]]\n",
      "loss=35314.087216708795\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0653091 ]\n",
      " [-0.278993  ]\n",
      " [-0.17535714]\n",
      " ...\n",
      " [-0.12709046]\n",
      " [ 0.02751578]\n",
      " [-0.07408706]]\n",
      "t [[ 0.0653091 ]\n",
      " [-0.278993  ]\n",
      " [-0.17535714]\n",
      " ...\n",
      " [-0.12709046]\n",
      " [ 0.02751578]\n",
      " [-0.07408706]]\n",
      "t [[ 0.110402  ]\n",
      " [-0.48858307]\n",
      " [-0.30509735]\n",
      " ...\n",
      " [-0.21602432]\n",
      " [ 0.0412944 ]\n",
      " [-0.1376953 ]]\n",
      "t [[ 0.110402  ]\n",
      " [-0.48858307]\n",
      " [-0.30509735]\n",
      " ...\n",
      " [-0.21602432]\n",
      " [ 0.0412944 ]\n",
      " [-0.1376953 ]]\n",
      "Current iteration=2, loss=38218.17329570747\n",
      "t [[ 0.14201576]\n",
      " [-0.65186397]\n",
      " [-0.40524756]\n",
      " ...\n",
      " [-0.28087985]\n",
      " [ 0.04586106]\n",
      " [-0.19331162]]\n",
      "t [[ 0.14201576]\n",
      " [-0.65186397]\n",
      " [-0.40524756]\n",
      " ...\n",
      " [-0.28087985]\n",
      " [ 0.04586106]\n",
      " [-0.19331162]]\n",
      "t [[ 0.1645181 ]\n",
      " [-0.78348399]\n",
      " [-0.48581597]\n",
      " ...\n",
      " [-0.33039772]\n",
      " [ 0.04413157]\n",
      " [-0.24264428]]\n",
      "t [[ 0.1645181 ]\n",
      " [-0.78348399]\n",
      " [-0.48581597]\n",
      " ...\n",
      " [-0.33039772]\n",
      " [ 0.04413157]\n",
      " [-0.24264428]]\n",
      "Current iteration=4, loss=37083.96506793646\n",
      "t [[ 0.1807242 ]\n",
      " [-0.89269386]\n",
      " [-0.55300036]\n",
      " ...\n",
      " [-0.36994392]\n",
      " [ 0.037982  ]\n",
      " [-0.28687698]]\n",
      "t [[ 0.1807242 ]\n",
      " [-0.89269386]\n",
      " [-0.55300036]\n",
      " ...\n",
      " [-0.36994392]\n",
      " [ 0.037982  ]\n",
      " [-0.28687698]]\n",
      "t [[ 0.19248782]\n",
      " [-0.98549493]\n",
      " [-0.61072357]\n",
      " ...\n",
      " [-0.40286614]\n",
      " [ 0.02866437]\n",
      " [-0.32686132]]\n",
      "t [[ 0.19248782]\n",
      " [-0.98549493]\n",
      " [-0.61072357]\n",
      " ...\n",
      " [-0.40286614]\n",
      " [ 0.02866437]\n",
      " [-0.32686132]]\n",
      "Current iteration=6, loss=36316.4310961592\n",
      "t [[ 0.20105935]\n",
      " [-1.06590136]\n",
      " [-0.66153067]\n",
      " ...\n",
      " [-0.43129011]\n",
      " [ 0.01704796]\n",
      " [-0.36323415]]\n",
      "t [[ 0.20105935]\n",
      " [-1.06590136]\n",
      " [-0.66153067]\n",
      " ...\n",
      " [-0.43129011]\n",
      " [ 0.01704796]\n",
      " [-0.36323415]]\n",
      "t [[ 0.20730068]\n",
      " [-1.13667582]\n",
      " [-0.70711032]\n",
      " ...\n",
      " [-0.45658487]\n",
      " [ 0.00375734]\n",
      " [-0.39648792]]\n",
      "t [[ 0.20730068]\n",
      " [-1.13667582]\n",
      " [-0.70711032]\n",
      " ...\n",
      " [-0.45658487]\n",
      " [ 0.00375734]\n",
      " [-0.39648792]]\n",
      "Current iteration=8, loss=35750.43603674307\n",
      "t [[ 0.21181738]\n",
      " [-1.19977001]\n",
      " [-0.74860612]\n",
      " ...\n",
      " [-0.47964167]\n",
      " [-0.01074613]\n",
      " [-0.42701423]]\n",
      "t [[ 0.21181738]\n",
      " [-1.19977001]\n",
      " [-0.74860612]\n",
      " ...\n",
      " [-0.47964167]\n",
      " [-0.01074613]\n",
      " [-0.42701423]]\n",
      "t [[ 0.21504227]\n",
      " [-1.25659695]\n",
      " [-0.78680808]\n",
      " ...\n",
      " [-0.50104562]\n",
      " [-0.02611409]\n",
      " [-0.45513156]]\n",
      "loss=35315.495930547295\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00861255]\n",
      " [-0.05302631]\n",
      " [-0.16624301]\n",
      " ...\n",
      " [-0.12757128]\n",
      " [ 0.02480073]\n",
      " [-0.07568094]]\n",
      "t [[ 0.00861255]\n",
      " [-0.05302631]\n",
      " [-0.16624301]\n",
      " ...\n",
      " [-0.12757128]\n",
      " [ 0.02480073]\n",
      " [-0.07568094]]\n",
      "t [[ 0.01025051]\n",
      " [-0.1167326 ]\n",
      " [-0.28567025]\n",
      " ...\n",
      " [-0.21616606]\n",
      " [ 0.03618944]\n",
      " [-0.1406461 ]]\n",
      "t [[ 0.01025051]\n",
      " [-0.1167326 ]\n",
      " [-0.28567025]\n",
      " ...\n",
      " [-0.21616606]\n",
      " [ 0.03618944]\n",
      " [-0.1406461 ]]\n",
      "Current iteration=2, loss=38193.73765217075\n",
      "t [[ 0.00792724]\n",
      " [-0.18514533]\n",
      " [-0.37544554]\n",
      " ...\n",
      " [-0.28031627]\n",
      " [ 0.03870544]\n",
      " [-0.19746038]]\n",
      "t [[ 0.00792724]\n",
      " [-0.18514533]\n",
      " [-0.37544554]\n",
      " ...\n",
      " [-0.28031627]\n",
      " [ 0.03870544]\n",
      " [-0.19746038]]\n",
      "t [[ 0.00344793]\n",
      " [-0.25477474]\n",
      " [-0.44614787]\n",
      " ...\n",
      " [-0.32898169]\n",
      " [ 0.03523926]\n",
      " [-0.24787547]]\n",
      "t [[ 0.00344793]\n",
      " [-0.25477474]\n",
      " [-0.44614787]\n",
      " ...\n",
      " [-0.32898169]\n",
      " [ 0.03523926]\n",
      " [-0.24787547]]\n",
      "Current iteration=4, loss=37042.69133339341\n",
      "t [[-0.00212993]\n",
      " [-0.32365112]\n",
      " [-0.50423761]\n",
      " ...\n",
      " [-0.36763019]\n",
      " [ 0.02762975]\n",
      " [-0.29310024]]\n",
      "t [[-0.00212993]\n",
      " [-0.32365112]\n",
      " [-0.50423761]\n",
      " ...\n",
      " [-0.36763019]\n",
      " [ 0.02762975]\n",
      " [-0.29310024]]\n",
      "t [[-0.00818202]\n",
      " [-0.39068431]\n",
      " [-0.55373827]\n",
      " ...\n",
      " [-0.39965636]\n",
      " [ 0.01709127]\n",
      " [-0.33400221]]\n",
      "t [[-0.00818202]\n",
      " [-0.39068431]\n",
      " [-0.55373827]\n",
      " ...\n",
      " [-0.39965636]\n",
      " [ 0.01709127]\n",
      " [-0.33400221]]\n",
      "Current iteration=6, loss=36261.2997977614\n",
      "t [[-0.01433825]\n",
      " [-0.45529725]\n",
      " [-0.59721018]\n",
      " ...\n",
      " [-0.42720664]\n",
      " [ 0.00445871]\n",
      " [-0.37122918]]\n",
      "t [[-0.01433825]\n",
      " [-0.45529725]\n",
      " [-0.59721018]\n",
      " ...\n",
      " [-0.42720664]\n",
      " [ 0.00445871]\n",
      " [-0.37122918]]\n",
      "t [[-0.02038126]\n",
      " [-0.51721662]\n",
      " [-0.63631275]\n",
      " ...\n",
      " [-0.45165821]\n",
      " [-0.00967358]\n",
      " [-0.40528171]]\n",
      "t [[-0.02038126]\n",
      " [-0.51721662]\n",
      " [-0.63631275]\n",
      " ...\n",
      " [-0.45165821]\n",
      " [-0.00967358]\n",
      " [-0.40528171]]\n",
      "Current iteration=8, loss=35684.42854058517\n",
      "t [[-0.02618707]\n",
      " [-0.57634991]\n",
      " [-0.67213844]\n",
      " ...\n",
      " [-0.4739045 ]\n",
      " [-0.02487042]\n",
      " [-0.43655767]]\n",
      "t [[-0.02618707]\n",
      " [-0.57634991]\n",
      " [-0.67213844]\n",
      " ...\n",
      " [-0.4739045 ]\n",
      " [-0.02487042]\n",
      " [-0.43655767]]\n",
      "t [[-0.03168932]\n",
      " [-0.63271161]\n",
      " [-0.70541683]\n",
      " ...\n",
      " [-0.49453016]\n",
      " [-0.04080592]\n",
      " [-0.46538065]]\n",
      "loss=35241.136133186745\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00735607]\n",
      " [-0.04653906]\n",
      " [-0.16804348]\n",
      " ...\n",
      " [-0.12543788]\n",
      " [ 0.02666562]\n",
      " [-0.07098905]]\n",
      "t [[ 0.00735607]\n",
      " [-0.04653906]\n",
      " [-0.16804348]\n",
      " ...\n",
      " [-0.12543788]\n",
      " [ 0.02666562]\n",
      " [-0.07098905]]\n",
      "t [[ 0.00780561]\n",
      " [-0.10425913]\n",
      " [-0.28936255]\n",
      " ...\n",
      " [-0.21242802]\n",
      " [ 0.03978459]\n",
      " [-0.13162376]]\n",
      "t [[ 0.00780561]\n",
      " [-0.10425913]\n",
      " [-0.28936255]\n",
      " ...\n",
      " [-0.21242802]\n",
      " [ 0.03978459]\n",
      " [-0.13162376]]\n",
      "Current iteration=2, loss=38217.65740314001\n",
      "t [[ 0.00437792]\n",
      " [-0.16716354]\n",
      " [-0.38095979]\n",
      " ...\n",
      " [-0.27527745]\n",
      " [ 0.04388101]\n",
      " [-0.18441563]]\n",
      "t [[ 0.00437792]\n",
      " [-0.16716354]\n",
      " [-0.38095979]\n",
      " ...\n",
      " [-0.27527745]\n",
      " [ 0.04388101]\n",
      " [-0.18441563]]\n",
      "t [[-0.00111229]\n",
      " [-0.23171837]\n",
      " [-0.45335523]\n",
      " ...\n",
      " [-0.32283388]\n",
      " [ 0.0418455 ]\n",
      " [-0.23107714]]\n",
      "t [[-0.00111229]\n",
      " [-0.23171837]\n",
      " [-0.45335523]\n",
      " ...\n",
      " [-0.32283388]\n",
      " [ 0.0418455 ]\n",
      " [-0.23107714]]\n",
      "Current iteration=4, loss=37084.054188620124\n",
      "t [[-0.00760343]\n",
      " [-0.29590687]\n",
      " [-0.51299653]\n",
      " ...\n",
      " [-0.36050906]\n",
      " [ 0.03552389]\n",
      " [-0.27278713]]\n",
      "t [[-0.00760343]\n",
      " [-0.29590687]\n",
      " [-0.51299653]\n",
      " ...\n",
      " [-0.36050906]\n",
      " [ 0.03552389]\n",
      " [-0.27278713]]\n",
      "t [[-0.01447125]\n",
      " [-0.35859598]\n",
      " [-0.56391228]\n",
      " ...\n",
      " [-0.39166767]\n",
      " [ 0.02613971]\n",
      " [-0.31038918]]\n",
      "t [[-0.01447125]\n",
      " [-0.35859598]\n",
      " [-0.56391228]\n",
      " ...\n",
      " [-0.39166767]\n",
      " [ 0.02613971]\n",
      " [-0.31038918]]\n",
      "Current iteration=6, loss=36316.67568378415\n",
      "t [[-0.02134859]\n",
      " [-0.41917133]\n",
      " [-0.608674  ]\n",
      " ...\n",
      " [-0.41843894]\n",
      " [ 0.01453765]\n",
      " [-0.34451135]]\n",
      "t [[-0.02134859]\n",
      " [-0.41917133]\n",
      " [-0.608674  ]\n",
      " ...\n",
      " [-0.41843894]\n",
      " [ 0.01453765]\n",
      " [-0.34451135]]\n",
      "t [[-0.02802293]\n",
      " [-0.47732742]\n",
      " [-0.64895372]\n",
      " ...\n",
      " [-0.44218917]\n",
      " [ 0.00132184]\n",
      " [-0.37563749]]\n",
      "t [[-0.02802293]\n",
      " [-0.47732742]\n",
      " [-0.64895372]\n",
      " ...\n",
      " [-0.44218917]\n",
      " [ 0.00132184]\n",
      " [-0.37563749]]\n",
      "Current iteration=8, loss=35750.909902476415\n",
      "t [[-0.03437617]\n",
      " [-0.53294412]\n",
      " [-0.68585607]\n",
      " ...\n",
      " [-0.46380415]\n",
      " [-0.01306291]\n",
      " [-0.40415102]]\n",
      "t [[-0.03437617]\n",
      " [-0.53294412]\n",
      " [-0.68585607]\n",
      " ...\n",
      " [-0.46380415]\n",
      " [-0.01306291]\n",
      " [-0.40415102]]\n",
      "t [[-0.04034833]\n",
      " [-0.58601201]\n",
      " [-0.72012169]\n",
      " ...\n",
      " [-0.48386254]\n",
      " [-0.02828138]\n",
      " [-0.43036285]]\n",
      "loss=35316.35935254417\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00612613]\n",
      " [-0.0492929 ]\n",
      " [-0.16693467]\n",
      " ...\n",
      " [-0.07349705]\n",
      " [-0.0088518 ]\n",
      " [-0.14994587]]\n",
      "t [[ 0.00612613]\n",
      " [-0.0492929 ]\n",
      " [-0.16693467]\n",
      " ...\n",
      " [-0.07349705]\n",
      " [-0.0088518 ]\n",
      " [-0.14994587]]\n",
      "t [[ 0.00548223]\n",
      " [-0.10968603]\n",
      " [-0.28675984]\n",
      " ...\n",
      " [-0.11680244]\n",
      " [-0.00771603]\n",
      " [-0.28228387]]\n",
      "t [[ 0.00548223]\n",
      " [-0.10968603]\n",
      " [-0.28675984]\n",
      " ...\n",
      " [-0.11680244]\n",
      " [-0.00771603]\n",
      " [-0.28228387]]\n",
      "Current iteration=2, loss=38181.59981881005\n",
      "t [[ 0.00108396]\n",
      " [-0.17514613]\n",
      " [-0.37670181]\n",
      " ...\n",
      " [-0.14199684]\n",
      " [-0.00146065]\n",
      " [-0.4007468 ]]\n",
      "t [[ 0.00108396]\n",
      " [-0.17514613]\n",
      " [-0.37670181]\n",
      " ...\n",
      " [-0.14199684]\n",
      " [-0.00146065]\n",
      " [-0.4007468 ]]\n",
      "t [[-0.00526948]\n",
      " [-0.24212852]\n",
      " [-0.44740138]\n",
      " ...\n",
      " [-0.15642681]\n",
      " [ 0.00707155]\n",
      " [-0.50801788]]\n",
      "t [[-0.00526948]\n",
      " [-0.24212852]\n",
      " [-0.44740138]\n",
      " ...\n",
      " [-0.15642681]\n",
      " [ 0.00707155]\n",
      " [-0.50801788]]\n",
      "Current iteration=4, loss=37028.80038870704\n",
      "t [[-0.01253139]\n",
      " [-0.30861728]\n",
      " [-0.50536849]\n",
      " ...\n",
      " [-0.16449292]\n",
      " [ 0.01627271]\n",
      " [-0.60603582]]\n",
      "t [[-0.01253139]\n",
      " [-0.30861728]\n",
      " [-0.50536849]\n",
      " ...\n",
      " [-0.16449292]\n",
      " [ 0.01627271]\n",
      " [-0.60603582]]\n",
      "t [[-0.0200908 ]\n",
      " [-0.37348423]\n",
      " [-0.55466384]\n",
      " ...\n",
      " [-0.16885842]\n",
      " [ 0.0252529 ]\n",
      " [-0.69624251]]\n",
      "t [[-0.0200908 ]\n",
      " [-0.37348423]\n",
      " [-0.55466384]\n",
      " ...\n",
      " [-0.16885842]\n",
      " [ 0.0252529 ]\n",
      " [-0.69624251]]\n",
      "Current iteration=6, loss=36247.52362134874\n",
      "t [[-0.02759204]\n",
      " [-0.4361211 ]\n",
      " [-0.59787509]\n",
      " ...\n",
      " [-0.17114947]\n",
      " [ 0.03354217]\n",
      " [-0.77974361]]\n",
      "t [[-0.02759204]\n",
      " [-0.4361211 ]\n",
      " [-0.59787509]\n",
      " ...\n",
      " [-0.17114947]\n",
      " [ 0.03354217]\n",
      " [-0.77974361]]\n",
      "t [[-0.03483224]\n",
      " [-0.49622885]\n",
      " [-0.63668163]\n",
      " ...\n",
      " [-0.17236012]\n",
      " [ 0.0409182 ]\n",
      " [-0.85741011]]\n",
      "t [[-0.03483224]\n",
      " [-0.49622885]\n",
      " [-0.63668163]\n",
      " ...\n",
      " [-0.17236012]\n",
      " [ 0.0409182 ]\n",
      " [-0.85741011]]\n",
      "Current iteration=8, loss=35670.89449494463\n",
      "t [[-0.0417014 ]\n",
      " [-0.55369377]\n",
      " [-0.67219034]\n",
      " ...\n",
      " [-0.17309281]\n",
      " [ 0.04730442]\n",
      " [-0.92994387]]\n",
      "t [[-0.0417014 ]\n",
      " [-0.55369377]\n",
      " [-0.67219034]\n",
      " ...\n",
      " [-0.17309281]\n",
      " [ 0.04730442]\n",
      " [-0.92994387]]\n",
      "t [[-0.04814628]\n",
      " [-0.60851274]\n",
      " [-0.70514115]\n",
      " ...\n",
      " [-0.1737051 ]\n",
      " [ 0.05270843]\n",
      " [-0.99792124]]\n",
      "loss=35227.7008536502\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.06803032]\n",
      " [-0.29061771]\n",
      " [-0.18266369]\n",
      " ...\n",
      " [-0.1323859 ]\n",
      " [ 0.02866228]\n",
      " [-0.07717402]]\n",
      "t [[ 0.06803032]\n",
      " [-0.29061771]\n",
      " [-0.18266369]\n",
      " ...\n",
      " [-0.1323859 ]\n",
      " [ 0.02866228]\n",
      " [-0.07717402]]\n",
      "t [[ 0.11413604]\n",
      " [-0.50596889]\n",
      " [-0.31585706]\n",
      " ...\n",
      " [-0.22339134]\n",
      " [ 0.04242848]\n",
      " [-0.14298247]]\n",
      "t [[ 0.11413604]\n",
      " [-0.50596889]\n",
      " [-0.31585706]\n",
      " ...\n",
      " [-0.22339134]\n",
      " [ 0.04242848]\n",
      " [-0.14298247]]\n",
      "Current iteration=2, loss=38154.09137152744\n",
      "t [[ 0.14592762]\n",
      " [-0.67213574]\n",
      " [-0.41767953]\n",
      " ...\n",
      " [-0.28891552]\n",
      " [ 0.04640824]\n",
      " [-0.20023526]]\n",
      "t [[ 0.14592762]\n",
      " [-0.67213574]\n",
      " [-0.41767953]\n",
      " ...\n",
      " [-0.28891552]\n",
      " [ 0.04640824]\n",
      " [-0.20023526]]\n",
      "t [[ 0.16821803]\n",
      " [-0.80523444]\n",
      " [-0.49913109]\n",
      " ...\n",
      " [-0.33856281]\n",
      " [ 0.04381094]\n",
      " [-0.2508217 ]]\n",
      "t [[ 0.16821803]\n",
      " [-0.80523444]\n",
      " [-0.49913109]\n",
      " ...\n",
      " [-0.33856281]\n",
      " [ 0.04381094]\n",
      " [-0.2508217 ]]\n",
      "Current iteration=4, loss=37005.497210620175\n",
      "t [[ 0.1840441 ]\n",
      " [-0.91522039]\n",
      " [-0.56686493]\n",
      " ...\n",
      " [-0.37808649]\n",
      " [ 0.03666057]\n",
      " [-0.29603032]]\n",
      "t [[ 0.1840441 ]\n",
      " [-0.91522039]\n",
      " [-0.56686493]\n",
      " ...\n",
      " [-0.37808649]\n",
      " [ 0.03666057]\n",
      " [-0.29603032]]\n",
      "t [[ 0.19537059]\n",
      " [-1.00843909]\n",
      " [-0.62500926]\n",
      " ...\n",
      " [-0.41099892]\n",
      " [ 0.02628869]\n",
      " [-0.33677837]]\n",
      "t [[ 0.19537059]\n",
      " [-1.00843909]\n",
      " [-0.62500926]\n",
      " ...\n",
      " [-0.41099892]\n",
      " [ 0.02628869]\n",
      " [-0.33677837]]\n",
      "Current iteration=6, loss=36233.208978520044\n",
      "t [[ 0.20350279]\n",
      " [-1.08907421]\n",
      " [-0.67619609]\n",
      " ...\n",
      " [-0.43948578]\n",
      " [ 0.01361012]\n",
      " [-0.37374698]]\n",
      "t [[ 0.20350279]\n",
      " [-1.08907421]\n",
      " [-0.67619609]\n",
      " ...\n",
      " [-0.43948578]\n",
      " [ 0.01361012]\n",
      " [-0.37374698]]\n",
      "t [[ 2.09328494e-01]\n",
      " [-1.15997103e+00]\n",
      " [-7.22144791e-01]\n",
      " ...\n",
      " [-4.64927356e-01]\n",
      " [-7.23260962e-04]\n",
      " [-4.07460383e-01]]\n",
      "t [[ 2.09328494e-01]\n",
      " [-1.15997103e+00]\n",
      " [-7.22144791e-01]\n",
      " ...\n",
      " [-4.64927356e-01]\n",
      " [-7.23260962e-04]\n",
      " [-4.07460383e-01]]\n",
      "Current iteration=8, loss=35667.18461223612\n",
      "t [[ 0.21346406]\n",
      " [-1.22312052]\n",
      " [-0.76400389]\n",
      " ...\n",
      " [-0.48820499]\n",
      " [-0.01623332]\n",
      " [-0.43833399]]\n",
      "t [[ 0.21346406]\n",
      " [-1.22312052]\n",
      " [-0.76400389]\n",
      " ...\n",
      " [-0.48820499]\n",
      " [-0.01623332]\n",
      " [-0.43833399]]\n",
      "t [[ 0.21634535]\n",
      " [-1.2799536 ]\n",
      " [-0.80255774]\n",
      " ...\n",
      " [-0.50988633]\n",
      " [-0.03256139]\n",
      " [-0.46670485]]\n",
      "loss=35234.78075776025\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00897141]\n",
      " [-0.05523574]\n",
      " [-0.1731698 ]\n",
      " ...\n",
      " [-0.13288675]\n",
      " [ 0.02583409]\n",
      " [-0.07883432]]\n",
      "t [[ 0.00897141]\n",
      " [-0.05523574]\n",
      " [-0.1731698 ]\n",
      " ...\n",
      " [-0.13288675]\n",
      " [ 0.02583409]\n",
      " [-0.07883432]]\n",
      "t [[ 0.0103808 ]\n",
      " [-0.12205147]\n",
      " [-0.29556938]\n",
      " ...\n",
      " [-0.22350429]\n",
      " [ 0.03712502]\n",
      " [-0.14604615]]\n",
      "t [[ 0.0103808 ]\n",
      " [-0.12205147]\n",
      " [-0.29556938]\n",
      " ...\n",
      " [-0.22350429]\n",
      " [ 0.03712502]\n",
      " [-0.14604615]]\n",
      "Current iteration=2, loss=38128.86291294734\n",
      "t [[ 0.00762979]\n",
      " [-0.19370385]\n",
      " [-0.38658114]\n",
      " ...\n",
      " [-0.28826341]\n",
      " [ 0.03899788]\n",
      " [-0.20453341]]\n",
      "t [[ 0.00762979]\n",
      " [-0.19370385]\n",
      " [-0.38658114]\n",
      " ...\n",
      " [-0.28826341]\n",
      " [ 0.03899788]\n",
      " [-0.20453341]]\n",
      "t [[ 0.00269578]\n",
      " [-0.26637518]\n",
      " [-0.45782289]\n",
      " ...\n",
      " [-0.33700457]\n",
      " [ 0.03463171]\n",
      " [-0.2562328 ]]\n",
      "t [[ 0.00269578]\n",
      " [-0.26637518]\n",
      " [-0.45782289]\n",
      " ...\n",
      " [-0.33700457]\n",
      " [ 0.03463171]\n",
      " [-0.2562328 ]]\n",
      "Current iteration=4, loss=36962.9168009875\n",
      "t [[-0.00329454]\n",
      " [-0.33797802]\n",
      " [-0.51621707]\n",
      " ...\n",
      " [-0.37558593]\n",
      " [ 0.02600762]\n",
      " [-0.30245963]]\n",
      "t [[-0.00329454]\n",
      " [-0.33797802]\n",
      " [-0.51621707]\n",
      " ...\n",
      " [-0.37558593]\n",
      " [ 0.02600762]\n",
      " [-0.30245963]]\n",
      "t [[-0.00969314]\n",
      " [-0.40739716]\n",
      " [-0.5659843 ]\n",
      " ...\n",
      " [-0.40756574]\n",
      " [ 0.01441474]\n",
      " [-0.34414791]]\n",
      "t [[-0.00969314]\n",
      " [-0.40739716]\n",
      " [-0.5659843 ]\n",
      " ...\n",
      " [-0.40756574]\n",
      " [ 0.01441474]\n",
      " [-0.34414791]]\n",
      "Current iteration=6, loss=36176.50666009449\n",
      "t [[-0.01612641]\n",
      " [-0.47407047]\n",
      " [-0.60975892]\n",
      " ...\n",
      " [-0.43514856]\n",
      " [ 0.00072972]\n",
      " [-0.38199029]]\n",
      "t [[-0.01612641]\n",
      " [-0.47407047]\n",
      " [-0.60975892]\n",
      " ...\n",
      " [-0.43514856]\n",
      " [ 0.00072972]\n",
      " [-0.38199029]]\n",
      "t [[-0.0223821 ]\n",
      " [-0.53775521]\n",
      " [-0.64921767]\n",
      " ...\n",
      " [-0.45972116]\n",
      " [-0.01442879]\n",
      " [-0.41651949]]\n",
      "t [[-0.0223821 ]\n",
      " [-0.53775521]\n",
      " [-0.64921767]\n",
      " ...\n",
      " [-0.45972116]\n",
      " [-0.01442879]\n",
      " [-0.41651949]]\n",
      "Current iteration=8, loss=35599.56305574476\n",
      "t [[-0.02834451]\n",
      " [-0.59839375]\n",
      " [-0.68544576]\n",
      " ...\n",
      " [-0.48216582]\n",
      " [-0.03061109]\n",
      " [-0.44815754]]\n",
      "t [[-0.02834451]\n",
      " [-0.59839375]\n",
      " [-0.68544576]\n",
      " ...\n",
      " [-0.48216582]\n",
      " [-0.03061109]\n",
      " [-0.44815754]]\n",
      "t [[-0.03395606]\n",
      " [-0.6560345 ]\n",
      " [-0.71915665]\n",
      " ...\n",
      " [-0.50304892]\n",
      " [-0.04748274]\n",
      " [-0.47724684]]\n",
      "loss=35158.88375583205\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00766257]\n",
      " [-0.04847819]\n",
      " [-0.17504529]\n",
      " ...\n",
      " [-0.13066446]\n",
      " [ 0.02777669]\n",
      " [-0.07394693]]\n",
      "t [[ 0.00766257]\n",
      " [-0.04847819]\n",
      " [-0.17504529]\n",
      " ...\n",
      " [-0.13066446]\n",
      " [ 0.02777669]\n",
      " [-0.07394693]]\n",
      "t [[ 0.00783686]\n",
      " [-0.10908025]\n",
      " [-0.29941889]\n",
      " ...\n",
      " [-0.2196329 ]\n",
      " [ 0.04086385]\n",
      " [-0.1366635 ]]\n",
      "t [[ 0.00783686]\n",
      " [-0.10908025]\n",
      " [-0.29941889]\n",
      " ...\n",
      " [-0.2196329 ]\n",
      " [ 0.04086385]\n",
      " [-0.1366635 ]]\n",
      "Current iteration=2, loss=38153.620185845575\n",
      "t [[ 0.00394252]\n",
      " [-0.17503604]\n",
      " [-0.39232188]\n",
      " ...\n",
      " [-0.28306263]\n",
      " [ 0.04436973]\n",
      " [-0.19098714]]\n",
      "t [[ 0.00394252]\n",
      " [-0.17503604]\n",
      " [-0.39232188]\n",
      " ...\n",
      " [-0.28306263]\n",
      " [ 0.04436973]\n",
      " [-0.19098714]]\n",
      "t [[-0.00203254]\n",
      " [-0.2424769 ]\n",
      " [-0.46531057]\n",
      " ...\n",
      " [-0.33067299]\n",
      " [ 0.04147437]\n",
      " [-0.2388109 ]]\n",
      "t [[-0.00203254]\n",
      " [-0.2424769 ]\n",
      " [-0.46531057]\n",
      " ...\n",
      " [-0.33067299]\n",
      " [ 0.04147437]\n",
      " [-0.2388109 ]]\n",
      "Current iteration=4, loss=37005.6101819636\n",
      "t [[-0.00895745]\n",
      " [-0.30926223]\n",
      " [-0.52529687]\n",
      " ...\n",
      " [-0.36826357]\n",
      " [ 0.03416729]\n",
      " [-0.28141705]]\n",
      "t [[-0.00895745]\n",
      " [-0.30926223]\n",
      " [-0.52529687]\n",
      " ...\n",
      " [-0.36826357]\n",
      " [ 0.03416729]\n",
      " [-0.28141705]]\n",
      "t [[-0.01618482]\n",
      " [-0.37422915]\n",
      " [-0.57650926]\n",
      " ...\n",
      " [-0.39936222]\n",
      " [ 0.02374822]\n",
      " [-0.31971346]]\n",
      "t [[-0.01618482]\n",
      " [-0.37422915]\n",
      " [-0.57650926]\n",
      " ...\n",
      " [-0.39936222]\n",
      " [ 0.02374822]\n",
      " [-0.31971346]]\n",
      "Current iteration=6, loss=36233.470892692625\n",
      "t [[-0.0233449 ]\n",
      " [-0.43677446]\n",
      " [-0.62159565]\n",
      " ...\n",
      " [-0.42615623]\n",
      " [ 0.0111049 ]\n",
      " [-0.35437128]]\n",
      "t [[-0.0233449 ]\n",
      " [-0.43677446]\n",
      " [-0.62159565]\n",
      " ...\n",
      " [-0.42615623]\n",
      " [ 0.0111049 ]\n",
      " [-0.35437128]]\n",
      "t [[-0.03023132]\n",
      " [-0.49662033]\n",
      " [-0.66224726]\n",
      " ...\n",
      " [-0.45002145]\n",
      " [-0.00313297]\n",
      " [-0.38590505]]\n",
      "t [[-0.03023132]\n",
      " [-0.49662033]\n",
      " [-0.66224726]\n",
      " ...\n",
      " [-0.45002145]\n",
      " [-0.00313297]\n",
      " [-0.38590505]]\n",
      "Current iteration=8, loss=35667.70871146713\n",
      "t [[-0.03673526]\n",
      " [-0.55367915]\n",
      " [-0.69956289]\n",
      " ...\n",
      " [-0.47183236]\n",
      " [-0.01850489]\n",
      " [-0.41472111]]\n",
      "t [[-0.03673526]\n",
      " [-0.55367915]\n",
      " [-0.69956289]\n",
      " ...\n",
      " [-0.47183236]\n",
      " [-0.01850489]\n",
      " [-0.41472111]]\n",
      "t [[-0.04280653]\n",
      " [-0.60797353]\n",
      " [-0.73426814]\n",
      " ...\n",
      " [-0.49214908]\n",
      " [-0.03466596]\n",
      " [-0.44114813]]\n",
      "loss=35235.73740398974\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00638138]\n",
      " [-0.05134677]\n",
      " [-0.17389029]\n",
      " ...\n",
      " [-0.07655942]\n",
      " [-0.00922062]\n",
      " [-0.15619362]]\n",
      "t [[ 0.00638138]\n",
      " [-0.05134677]\n",
      " [-0.17389029]\n",
      " ...\n",
      " [-0.07655942]\n",
      " [-0.00922062]\n",
      " [-0.15619362]]\n",
      "t [[ 0.00542251]\n",
      " [-0.1147294 ]\n",
      " [-0.29669168]\n",
      " ...\n",
      " [-0.12037695]\n",
      " [-0.00760957]\n",
      " [-0.29328859]]\n",
      "t [[ 0.00542251]\n",
      " [-0.1147294 ]\n",
      " [-0.29669168]\n",
      " ...\n",
      " [-0.12037695]\n",
      " [-0.00760957]\n",
      " [-0.29328859]]\n",
      "Current iteration=2, loss=38116.49777693192\n",
      "t [[ 0.00052766]\n",
      " [-0.1833369 ]\n",
      " [-0.38785754]\n",
      " ...\n",
      " [-0.14509416]\n",
      " [-0.00066322]\n",
      " [-0.41550504]]\n",
      "t [[ 0.00052766]\n",
      " [-0.1833369 ]\n",
      " [-0.38785754]\n",
      " ...\n",
      " [-0.14509416]\n",
      " [-0.00066322]\n",
      " [-0.41550504]]\n",
      "t [[-0.00633283]\n",
      " [-0.25328991]\n",
      " [-0.45907515]\n",
      " ...\n",
      " [-0.15877237]\n",
      " [ 0.00850687]\n",
      " [-0.52581948]]\n",
      "t [[-0.00633283]\n",
      " [-0.25328991]\n",
      " [-0.45907515]\n",
      " ...\n",
      " [-0.15877237]\n",
      " [ 0.00850687]\n",
      " [-0.52581948]]\n",
      "Current iteration=4, loss=36948.99033486298\n",
      "t [[-0.01404482]\n",
      " [-0.32244967]\n",
      " [-0.51732162]\n",
      " ...\n",
      " [-0.16611457]\n",
      " [ 0.01819609]\n",
      " [-0.62635158]]\n",
      "t [[-0.01404482]\n",
      " [-0.32244967]\n",
      " [-0.51732162]\n",
      " ...\n",
      " [-0.16611457]\n",
      " [ 0.01819609]\n",
      " [-0.62635158]]\n",
      "t [[-0.02197573]\n",
      " [-0.38965918]\n",
      " [-0.56685782]\n",
      " ...\n",
      " [-0.16989813]\n",
      " [ 0.02749445]\n",
      " [-0.71866198]]\n",
      "t [[-0.02197573]\n",
      " [-0.38965918]\n",
      " [-0.56685782]\n",
      " ...\n",
      " [-0.16989813]\n",
      " [ 0.02749445]\n",
      " [-0.71866198]]\n",
      "Current iteration=6, loss=36162.76484628793\n",
      "t [[-0.02976843]\n",
      " [-0.45432231]\n",
      " [-0.61034725]\n",
      " ...\n",
      " [-0.17177743]\n",
      " [ 0.03594481]\n",
      " [-0.80393889]]\n",
      "t [[-0.02976843]\n",
      " [-0.45432231]\n",
      " [-0.61034725]\n",
      " ...\n",
      " [-0.17177743]\n",
      " [ 0.03594481]\n",
      " [-0.80393889]]\n",
      "t [[-0.03722704]\n",
      " [-0.51616853]\n",
      " [-0.64948755]\n",
      " ...\n",
      " [-0.17273684]\n",
      " [ 0.04335011]\n",
      " [-0.88311352]]\n",
      "t [[-0.03722704]\n",
      " [-0.51616853]\n",
      " [-0.64948755]\n",
      " ...\n",
      " [-0.17273684]\n",
      " [ 0.04335011]\n",
      " [-0.88311352]]\n",
      "Current iteration=8, loss=35586.061658702514\n",
      "t [[-0.04425146]\n",
      " [-0.57511749]\n",
      " [-0.68537874]\n",
      " ...\n",
      " [-0.17335439]\n",
      " [ 0.0496617 ]\n",
      " [-0.95693323]]\n",
      "t [[-0.04425146]\n",
      " [-0.57511749]\n",
      " [-0.68537874]\n",
      " ...\n",
      " [-0.17335439]\n",
      " [ 0.0496617 ]\n",
      " [-0.95693323]]\n",
      "t [[-0.05079868]\n",
      " [-0.63119883]\n",
      " [-0.71874473]\n",
      " ...\n",
      " [-0.1739596 ]\n",
      " [ 0.05491331]\n",
      " [-1.02600985]]\n",
      "loss=35145.45737766702\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.07075153]\n",
      " [-0.30224242]\n",
      " [-0.18997024]\n",
      " ...\n",
      " [-0.13768133]\n",
      " [ 0.02980877]\n",
      " [-0.08026098]]\n",
      "t [[ 0.07075153]\n",
      " [-0.30224242]\n",
      " [-0.18997024]\n",
      " ...\n",
      " [-0.13768133]\n",
      " [ 0.02980877]\n",
      " [-0.08026098]]\n",
      "t [[ 0.11780221]\n",
      " [-0.52312197]\n",
      " [-0.32646393]\n",
      " ...\n",
      " [-0.23063037]\n",
      " [ 0.04351683]\n",
      " [-0.14823422]]\n",
      "t [[ 0.11780221]\n",
      " [-0.52312197]\n",
      " [-0.32646393]\n",
      " ...\n",
      " [-0.23063037]\n",
      " [ 0.04351683]\n",
      " [-0.14823422]]\n",
      "Current iteration=2, loss=38091.40355264431\n",
      "t [[ 0.14970664]\n",
      " [-0.69195034]\n",
      " [-0.42981992]\n",
      " ...\n",
      " [-0.2967142 ]\n",
      " [ 0.04686501]\n",
      " [-0.20707884]]\n",
      "t [[ 0.14970664]\n",
      " [-0.69195034]\n",
      " [-0.42981992]\n",
      " ...\n",
      " [-0.2967142 ]\n",
      " [ 0.04686501]\n",
      " [-0.20707884]]\n",
      "t [[ 0.17173998]\n",
      " [-0.82636279]\n",
      " [-0.51206252]\n",
      " ...\n",
      " [-0.34642854]\n",
      " [ 0.04336685]\n",
      " [-0.25887352]]\n",
      "t [[ 0.17173998]\n",
      " [-0.82636279]\n",
      " [-0.51206252]\n",
      " ...\n",
      " [-0.34642854]\n",
      " [ 0.04336685]\n",
      " [-0.25887352]]\n",
      "Current iteration=4, loss=36929.39141457534\n",
      "t [[ 0.18716047]\n",
      " [-0.9370152 ]\n",
      " [-0.58029419]\n",
      " ...\n",
      " [-0.385907  ]\n",
      " [ 0.03519478]\n",
      " [-0.30501393]]\n",
      "t [[ 0.18716047]\n",
      " [-0.9370152 ]\n",
      " [-0.58029419]\n",
      " ...\n",
      " [-0.385907  ]\n",
      " [ 0.03519478]\n",
      " [-0.30501393]]\n",
      "t [[ 0.19803953]\n",
      " [-1.03058209]\n",
      " [-0.63883524]\n",
      " ...\n",
      " [-0.4188133 ]\n",
      " [ 0.02375833]\n",
      " [-0.34648359]]\n",
      "t [[ 0.19803953]\n",
      " [-1.03058209]\n",
      " [-0.63883524]\n",
      " ...\n",
      " [-0.4188133 ]\n",
      " [ 0.02375833]\n",
      " [-0.34648359]]\n",
      "Current iteration=6, loss=36152.97239056144\n",
      "t [[ 0.20573278]\n",
      " [-1.11140229]\n",
      " [-0.69039269]\n",
      " ...\n",
      " [-0.44738106]\n",
      " [ 0.01001615]\n",
      " [-0.38400808]]\n",
      "t [[ 0.20573278]\n",
      " [-1.11140229]\n",
      " [-0.69039269]\n",
      " ...\n",
      " [-0.44738106]\n",
      " [ 0.01001615]\n",
      " [-0.38400808]]\n",
      "t [[ 0.21115016]\n",
      " [-1.182393  ]\n",
      " [-0.73670781]\n",
      " ...\n",
      " [-0.47299301]\n",
      " [-0.00535407]\n",
      " [-0.41814342]]\n",
      "t [[ 0.21115016]\n",
      " [-1.182393  ]\n",
      " [-0.73670781]\n",
      " ...\n",
      " [-0.47299301]\n",
      " [-0.00535407]\n",
      " [-0.41814342]]\n",
      "Current iteration=8, loss=35587.40834865514\n",
      "t [[ 0.21491605]\n",
      " [-1.24557713]\n",
      " [-0.77892829]\n",
      " ...\n",
      " [-0.49651477]\n",
      " [-0.02185865]\n",
      " [-0.44932884]]\n",
      "t [[ 0.21491605]\n",
      " [-1.24557713]\n",
      " [-0.77892829]\n",
      " ...\n",
      " [-0.49651477]\n",
      " [-0.02185865]\n",
      " [-0.44932884]]\n",
      "t [[ 0.21746752]\n",
      " [-1.30239904]\n",
      " [-0.8178295 ]\n",
      " ...\n",
      " [-0.51849311]\n",
      " [-0.0391296 ]\n",
      " [-0.47791995]]\n",
      "loss=35157.86569914028\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00933026]\n",
      " [-0.05744517]\n",
      " [-0.18009659]\n",
      " ...\n",
      " [-0.13820222]\n",
      " [ 0.02686745]\n",
      " [-0.08198769]]\n",
      "t [[ 0.00933026]\n",
      " [-0.05744517]\n",
      " [-0.18009659]\n",
      " ...\n",
      " [-0.13820222]\n",
      " [ 0.02686745]\n",
      " [-0.08198769]]\n",
      "t [[ 0.01048806]\n",
      " [-0.12740569]\n",
      " [-0.30531168]\n",
      " ...\n",
      " [-0.23071187]\n",
      " [ 0.03801602]\n",
      " [-0.15140998]]\n",
      "t [[ 0.01048806]\n",
      " [-0.12740569]\n",
      " [-0.30531168]\n",
      " ...\n",
      " [-0.23071187]\n",
      " [ 0.03801602]\n",
      " [-0.15140998]]\n",
      "Current iteration=2, loss=38065.39140926892\n",
      "t [[ 0.00729424]\n",
      " [-0.20230673]\n",
      " [-0.39742496]\n",
      " ...\n",
      " [-0.2959701 ]\n",
      " [ 0.03920336]\n",
      " [-0.21152484]]\n",
      "t [[ 0.00729424]\n",
      " [-0.20230673]\n",
      " [-0.39742496]\n",
      " ...\n",
      " [-0.2959701 ]\n",
      " [ 0.03920336]\n",
      " [-0.21152484]]\n",
      "t [[ 0.00190275]\n",
      " [-0.27799545]\n",
      " [-0.46912512]\n",
      " ...\n",
      " [-0.34472535]\n",
      " [ 0.03390701]\n",
      " [-0.26446228]]\n",
      "t [[ 0.00190275]\n",
      " [-0.27799545]\n",
      " [-0.46912512]\n",
      " ...\n",
      " [-0.34472535]\n",
      " [ 0.03390701]\n",
      " [-0.26446228]]\n",
      "Current iteration=4, loss=36885.521547711156\n",
      "t [[-0.00449302]\n",
      " [-0.35227389]\n",
      " [-0.52778792]\n",
      " ...\n",
      " [-0.3832183 ]\n",
      " [ 0.02425035]\n",
      " [-0.31164635]]\n",
      "t [[-0.00449302]\n",
      " [-0.35227389]\n",
      " [-0.52778792]\n",
      " ...\n",
      " [-0.3832183 ]\n",
      " [ 0.02425035]\n",
      " [-0.31164635]]\n",
      "t [[-0.01122483]\n",
      " [-0.42401081]\n",
      " [-0.57781537]\n",
      " ...\n",
      " [-0.41515699]\n",
      " [ 0.01159545]\n",
      " [-0.35407817]]\n",
      "t [[-0.01122483]\n",
      " [-0.42401081]\n",
      " [-0.57781537]\n",
      " ...\n",
      " [-0.41515699]\n",
      " [ 0.01159545]\n",
      " [-0.35407817]]\n",
      "Current iteration=6, loss=36094.74230008634\n",
      "t [[-0.01791847]\n",
      " [-0.49266656]\n",
      " [-0.62190207]\n",
      " ...\n",
      " [-0.44279184]\n",
      " [-0.00314119]\n",
      " [-0.39249537]]\n",
      "t [[-0.01791847]\n",
      " [-0.49266656]\n",
      " [-0.62190207]\n",
      " ...\n",
      " [-0.44279184]\n",
      " [-0.00314119]\n",
      " [-0.39249537]]\n",
      "t [[-0.02436894]\n",
      " [-0.55803456]\n",
      " [-0.6617321 ]\n",
      " ...\n",
      " [-0.46751029]\n",
      " [-0.01931812]\n",
      " [-0.42746284]]\n",
      "t [[-0.02436894]\n",
      " [-0.55803456]\n",
      " [-0.6617321 ]\n",
      " ...\n",
      " [-0.46751029]\n",
      " [-0.01931812]\n",
      " [-0.42746284]]\n",
      "Current iteration=8, loss=35518.23939108678\n",
      "t [[-0.03047018]\n",
      " [-0.62009585]\n",
      " [-0.698377  ]\n",
      " ...\n",
      " [-0.49017765]\n",
      " [-0.03647241]\n",
      " [-0.45942678]]\n",
      "t [[-0.03047018]\n",
      " [-0.62009585]\n",
      " [-0.698377  ]\n",
      " ...\n",
      " [-0.49017765]\n",
      " [-0.03647241]\n",
      " [-0.45942678]]\n",
      "t [[-0.03617427]\n",
      " [-0.67893515]\n",
      " [-0.73253048]\n",
      " ...\n",
      " [-0.51133866]\n",
      " [-0.05426202]\n",
      " [-0.48874842]]\n",
      "loss=35080.51275186524\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00796907]\n",
      " [-0.05041732]\n",
      " [-0.1820471 ]\n",
      " ...\n",
      " [-0.13589104]\n",
      " [ 0.02888776]\n",
      " [-0.0769048 ]]\n",
      "t [[ 0.00796907]\n",
      " [-0.05041732]\n",
      " [-0.1820471 ]\n",
      " ...\n",
      " [-0.13589104]\n",
      " [ 0.02888776]\n",
      " [-0.0769048 ]]\n",
      "t [[ 0.0078453 ]\n",
      " [-0.11393851]\n",
      " [-0.3093186 ]\n",
      " ...\n",
      " [-0.22670887]\n",
      " [ 0.041898  ]\n",
      " [-0.14166828]]\n",
      "t [[ 0.0078453 ]\n",
      " [-0.11393851]\n",
      " [-0.3093186 ]\n",
      " ...\n",
      " [-0.22670887]\n",
      " [ 0.041898  ]\n",
      " [-0.14166828]]\n",
      "Current iteration=2, loss=38090.977885271874\n",
      "t [[ 0.00346984]\n",
      " [-0.18295781]\n",
      " [-0.40339132]\n",
      " ...\n",
      " [-0.29061033]\n",
      " [ 0.04476987]\n",
      " [-0.19748017]]\n",
      "t [[ 0.00346984]\n",
      " [-0.18295781]\n",
      " [-0.40339132]\n",
      " ...\n",
      " [-0.29061033]\n",
      " [ 0.04476987]\n",
      " [-0.19748017]]\n",
      "t [[-0.00299178]\n",
      " [-0.25326399]\n",
      " [-0.47689046]\n",
      " ...\n",
      " [-0.33821381]\n",
      " [ 0.04098298]\n",
      " [-0.24642218]]\n",
      "t [[-0.00299178]\n",
      " [-0.25326399]\n",
      " [-0.47689046]\n",
      " ...\n",
      " [-0.33821381]\n",
      " [ 0.04098298]\n",
      " [-0.24642218]]\n",
      "Current iteration=4, loss=36929.524529668895\n",
      "t [[-0.01034203]\n",
      " [-0.32259943]\n",
      " [-0.53718389]\n",
      " ...\n",
      " [-0.37569926]\n",
      " [ 0.0326707 ]\n",
      " [-0.28988223]]\n",
      "t [[-0.01034203]\n",
      " [-0.32259943]\n",
      " [-0.53718389]\n",
      " ...\n",
      " [-0.37569926]\n",
      " [ 0.0326707 ]\n",
      " [-0.28988223]]\n",
      "t [[-0.01791399]\n",
      " [-0.3897802 ]\n",
      " [-0.58868451]\n",
      " ...\n",
      " [-0.40674397]\n",
      " [ 0.02120719]\n",
      " [-0.32883297]]\n",
      "t [[-0.01791399]\n",
      " [-0.3897802 ]\n",
      " [-0.58868451]\n",
      " ...\n",
      " [-0.40674397]\n",
      " [ 0.02120719]\n",
      " [-0.32883297]]\n",
      "Current iteration=6, loss=36153.25267029535\n",
      "t [[-0.02533839]\n",
      " [-0.45422171]\n",
      " [-0.63410303]\n",
      " ...\n",
      " [-0.43358112]\n",
      " [ 0.00752147]\n",
      " [-0.36398872]]\n",
      "t [[-0.02533839]\n",
      " [-0.45422171]\n",
      " [-0.63410303]\n",
      " ...\n",
      " [-0.43358112]\n",
      " [ 0.00752147]\n",
      " [-0.36398872]]\n",
      "t [[-0.0324172 ]\n",
      " [-0.51567932]\n",
      " [-0.67513992]\n",
      " ...\n",
      " [-0.4575872 ]\n",
      " [-0.00773262]\n",
      " [-0.39589468]]\n",
      "t [[-0.0324172 ]\n",
      " [-0.51567932]\n",
      " [-0.67513992]\n",
      " ...\n",
      " [-0.4575872 ]\n",
      " [-0.00773262]\n",
      " [-0.39589468]]\n",
      "Current iteration=8, loss=35587.98721468911\n",
      "t [[-0.03905242]\n",
      " [-0.57410169]\n",
      " [-0.71288172]\n",
      " ...\n",
      " [-0.47961951]\n",
      " [-0.02408006]\n",
      " [-0.42498009]]\n",
      "t [[-0.03905242]\n",
      " [-0.57410169]\n",
      " [-0.71288172]\n",
      " ...\n",
      " [-0.47961951]\n",
      " [-0.02408006]\n",
      " [-0.42498009]]\n",
      "t [[-0.04520451]\n",
      " [-0.6295458 ]\n",
      " [-0.74803534]\n",
      " ...\n",
      " [-0.50021622]\n",
      " [-0.04116721]\n",
      " [-0.45159132]]\n",
      "loss=35158.92011479841\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00663664]\n",
      " [-0.05340064]\n",
      " [-0.1808459 ]\n",
      " ...\n",
      " [-0.0796218 ]\n",
      " [-0.00958945]\n",
      " [-0.16244136]]\n",
      "t [[ 0.00663664]\n",
      " [-0.05340064]\n",
      " [-0.1808459 ]\n",
      " ...\n",
      " [-0.0796218 ]\n",
      " [-0.00958945]\n",
      " [-0.16244136]]\n",
      "t [[ 0.00534044]\n",
      " [-0.11980955]\n",
      " [-0.3064657 ]\n",
      " ...\n",
      " [-0.12385032]\n",
      " [-0.00746957]\n",
      " [-0.30423364]]\n",
      "t [[ 0.00534044]\n",
      " [-0.11980955]\n",
      " [-0.3064657 ]\n",
      " ...\n",
      " [-0.12385032]\n",
      " [-0.00746957]\n",
      " [-0.30423364]]\n",
      "Current iteration=2, loss=38052.81493117046\n",
      "t [[-6.46761151e-05]\n",
      " [-1.91575665e-01]\n",
      " [-3.98719222e-01]\n",
      " ...\n",
      " [-1.48013840e-01]\n",
      " [ 1.84164866e-04]\n",
      " [-4.30124293e-01]]\n",
      "t [[-6.46761151e-05]\n",
      " [-1.91575665e-01]\n",
      " [-3.98719222e-01]\n",
      " ...\n",
      " [-1.48013840e-01]\n",
      " [ 1.84164866e-04]\n",
      " [-4.30124293e-01]]\n",
      "t [[-0.00743299]\n",
      " [-0.26447724]\n",
      " [-0.47037287]\n",
      " ...\n",
      " [-0.16090911]\n",
      " [ 0.00998524]\n",
      " [-0.54339801]]\n",
      "t [[-0.00743299]\n",
      " [-0.26447724]\n",
      " [-0.47037287]\n",
      " ...\n",
      " [-0.16090911]\n",
      " [ 0.00998524]\n",
      " [-0.54339801]]\n",
      "Current iteration=4, loss=36871.57222028429\n",
      "t [[-0.01558571]\n",
      " [-0.33625964]\n",
      " [-0.52886232]\n",
      " ...\n",
      " [-0.16753299]\n",
      " [ 0.02013827]\n",
      " [-0.64636076]]\n",
      "t [[-0.01558571]\n",
      " [-0.33625964]\n",
      " [-0.52886232]\n",
      " ...\n",
      " [-0.16753299]\n",
      " [ 0.02013827]\n",
      " [-0.64636076]]\n",
      "t [[-0.02387231]\n",
      " [-0.4057459 ]\n",
      " [-0.57863293]\n",
      " ...\n",
      " [-0.17076367]\n",
      " [ 0.0297208 ]\n",
      " [-0.74069395]]\n",
      "t [[-0.02387231]\n",
      " [-0.4057459 ]\n",
      " [-0.57863293]\n",
      " ...\n",
      " [-0.17076367]\n",
      " [ 0.0297208 ]\n",
      " [-0.74069395]]\n",
      "Current iteration=6, loss=36081.03696227624\n",
      "t [[-0.03193726]\n",
      " [-0.47235953]\n",
      " [-0.62241019]\n",
      " ...\n",
      " [-0.17227228]\n",
      " [ 0.03829483]\n",
      " [-0.82766917]]\n",
      "t [[-0.03193726]\n",
      " [-0.47235953]\n",
      " [-0.62241019]\n",
      " ...\n",
      " [-0.17227228]\n",
      " [ 0.03829483]\n",
      " [-0.82766917]]\n",
      "t [[-0.03959393]\n",
      " [-0.53586408]\n",
      " [-0.66189994]\n",
      " ...\n",
      " [-0.17302512]\n",
      " [ 0.04569306]\n",
      " [-0.90827811]]\n",
      "t [[-0.03959393]\n",
      " [-0.53586408]\n",
      " [-0.66189994]\n",
      " ...\n",
      " [-0.17302512]\n",
      " [ 0.04569306]\n",
      " [-0.90827811]]\n",
      "Current iteration=8, loss=35504.767002165376\n",
      "t [[-0.04675362]\n",
      " [-0.59621634]\n",
      " [-0.69818885]\n",
      " ...\n",
      " [-0.17357034]\n",
      " [ 0.05189758]\n",
      " [-0.9833138 ]]\n",
      "t [[-0.04675362]\n",
      " [-0.59621634]\n",
      " [-0.69818885]\n",
      " ...\n",
      " [-0.17357034]\n",
      " [ 0.05189758]\n",
      " [-0.9833138 ]]\n",
      "t [[-0.05338443]\n",
      " [-0.65348115]\n",
      " [-0.73198103]\n",
      " ...\n",
      " [-0.17420634]\n",
      " [ 0.05696981]\n",
      " [-1.0534235 ]]\n",
      "loss=35067.09001615844\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.07347274]\n",
      " [-0.31386713]\n",
      " [-0.19727679]\n",
      " ...\n",
      " [-0.14297677]\n",
      " [ 0.03095526]\n",
      " [-0.08334794]]\n",
      "t [[ 0.07347274]\n",
      " [-0.31386713]\n",
      " [-0.19727679]\n",
      " ...\n",
      " [-0.14297677]\n",
      " [ 0.03095526]\n",
      " [-0.08334794]]\n",
      "t [[ 0.12140073]\n",
      " [-0.54004306]\n",
      " [-0.33691845]\n",
      " ...\n",
      " [-0.23774183]\n",
      " [ 0.04455964]\n",
      " [-0.15345062]]\n",
      "t [[ 0.12140073]\n",
      " [-0.54004306]\n",
      " [-0.33691845]\n",
      " ...\n",
      " [-0.23774183]\n",
      " [ 0.04455964]\n",
      " [-0.15345062]]\n",
      "Current iteration=2, loss=38030.06479150455\n",
      "t [[ 0.15335587]\n",
      " [-0.71131806]\n",
      " [-0.44167583]\n",
      " ...\n",
      " [-0.30428207]\n",
      " [ 0.04723337]\n",
      " [-0.21384353]]\n",
      "t [[ 0.15335587]\n",
      " [-0.71131806]\n",
      " [-0.44167583]\n",
      " ...\n",
      " [-0.30428207]\n",
      " [ 0.04723337]\n",
      " [-0.21384353]]\n",
      "t [[ 0.17509096]\n",
      " [-0.84689225]\n",
      " [-0.52462603]\n",
      " ...\n",
      " [-0.35400865]\n",
      " [ 0.04280386]\n",
      " [-0.26680257]]\n",
      "t [[ 0.17509096]\n",
      " [-0.84689225]\n",
      " [-0.52462603]\n",
      " ...\n",
      " [-0.35400865]\n",
      " [ 0.04280386]\n",
      " [-0.26680257]]\n",
      "Current iteration=4, loss=36855.522758232015\n",
      "t [[ 0.19008424]\n",
      " [-0.95811386]\n",
      " [-0.59331184]\n",
      " ...\n",
      " [-0.39342592]\n",
      " [ 0.03359179]\n",
      " [-0.31383254]]\n",
      "t [[ 0.19008424]\n",
      " [-0.95811386]\n",
      " [-0.59331184]\n",
      " ...\n",
      " [-0.39342592]\n",
      " [ 0.03359179]\n",
      " [-0.31383254]]\n",
      "t [[ 0.20050878]\n",
      " [-1.05196955]\n",
      " [-0.65223114]\n",
      " ...\n",
      " [-0.42633439]\n",
      " [ 0.0210828 ]\n",
      " [-0.35598373]]\n",
      "t [[ 0.20050878]\n",
      " [-1.05196955]\n",
      " [-0.65223114]\n",
      " ...\n",
      " [-0.42633439]\n",
      " [ 0.0210828 ]\n",
      " [-0.35598373]]\n",
      "Current iteration=6, loss=36075.55915555518\n",
      "t [[ 0.20776579]\n",
      " [-1.13293863]\n",
      " [-0.70415376]\n",
      " ...\n",
      " [-0.45500347]\n",
      " [ 0.0062776 ]\n",
      " [-0.39402624]]\n",
      "t [[ 0.20776579]\n",
      " [-1.13293863]\n",
      " [-0.70415376]\n",
      " ...\n",
      " [-0.45500347]\n",
      " [ 0.0062776 ]\n",
      " [-0.39402624]]\n",
      "t [[ 0.2127836 ]\n",
      " [-1.20399964]\n",
      " [-0.75083431]\n",
      " ...\n",
      " [-0.48080974]\n",
      " [-0.01012184]\n",
      " [-0.42854786]]\n",
      "t [[ 0.2127836 ]\n",
      " [-1.20399964]\n",
      " [-0.75083431]\n",
      " ...\n",
      " [-0.48080974]\n",
      " [-0.01012184]\n",
      " [-0.42854786]]\n",
      "Current iteration=8, loss=35510.90687704021\n",
      "t [[ 0.21619204]\n",
      " [-1.26720073]\n",
      " [-0.79341442]\n",
      " ...\n",
      " [-0.50459776]\n",
      " [-0.02760748]\n",
      " [-0.46001166]]\n",
      "t [[ 0.21619204]\n",
      " [-1.26720073]\n",
      " [-0.79341442]\n",
      " ...\n",
      " [-0.50459776]\n",
      " [-0.02760748]\n",
      " [-0.46001166]]\n",
      "t [[ 0.2184276 ]\n",
      " [-1.32399581]\n",
      " [-0.83265765]\n",
      " ...\n",
      " [-0.52689058]\n",
      " [-0.04580302]\n",
      " [-0.48879176]]\n",
      "loss=35084.50997977958\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00968912]\n",
      " [-0.0596546 ]\n",
      " [-0.18702339]\n",
      " ...\n",
      " [-0.14351769]\n",
      " [ 0.02790082]\n",
      " [-0.08514106]]\n",
      "t [[ 0.00968912]\n",
      " [-0.0596546 ]\n",
      " [-0.18702339]\n",
      " ...\n",
      " [-0.14351769]\n",
      " [ 0.02790082]\n",
      " [-0.08514106]]\n",
      "t [[ 0.01057242]\n",
      " [-0.13279511]\n",
      " [-0.31489769]\n",
      " ...\n",
      " [-0.23778923]\n",
      " [ 0.03886262]\n",
      " [-0.15673769]]\n",
      "t [[ 0.01057242]\n",
      " [-0.13279511]\n",
      " [-0.31489769]\n",
      " ...\n",
      " [-0.23778923]\n",
      " [ 0.03886262]\n",
      " [-0.15673769]]\n",
      "Current iteration=2, loss=38003.277366508875\n",
      "t [[ 0.00692189]\n",
      " [-0.21095147]\n",
      " [-0.4079845 ]\n",
      " ...\n",
      " [-0.30344269]\n",
      " [ 0.0393239 ]\n",
      " [-0.21843586]]\n",
      "t [[ 0.00692189]\n",
      " [-0.21095147]\n",
      " [-0.4079845 ]\n",
      " ...\n",
      " [-0.30344269]\n",
      " [ 0.0393239 ]\n",
      " [-0.21843586]]\n",
      "t [[ 0.00107161]\n",
      " [-0.28963032]\n",
      " [-0.48007116]\n",
      " ...\n",
      " [-0.35215809]\n",
      " [ 0.03306965]\n",
      " [-0.27256678]]\n",
      "t [[ 0.00107161]\n",
      " [-0.28963032]\n",
      " [-0.48007116]\n",
      " ...\n",
      " [-0.35215809]\n",
      " [ 0.03306965]\n",
      " [-0.27256678]]\n",
      "Current iteration=4, loss=36810.38073313573\n",
      "t [[-0.00572146]\n",
      " [-0.36653161]\n",
      " [-0.53897478]\n",
      " ...\n",
      " [-0.39054813]\n",
      " [ 0.02236493]\n",
      " [-0.32066523]]\n",
      "t [[-0.00572146]\n",
      " [-0.36653161]\n",
      " [-0.53897478]\n",
      " ...\n",
      " [-0.39054813]\n",
      " [ 0.02236493]\n",
      " [-0.32066523]]\n",
      "t [[-0.01277254]\n",
      " [-0.4405176 ]\n",
      " [-0.58926172]\n",
      " ...\n",
      " [-0.42245557]\n",
      " [ 0.00864257]\n",
      " [-0.36379985]]\n",
      "t [[-0.01277254]\n",
      " [-0.4405176 ]\n",
      " [-0.58926172]\n",
      " ...\n",
      " [-0.42245557]\n",
      " [ 0.00864257]\n",
      " [-0.36379985]]\n",
      "Current iteration=6, loss=36015.84427409717\n",
      "t [[-0.0197098 ]\n",
      " [-0.51107878]\n",
      " [-0.63367287]\n",
      " ...\n",
      " [-0.45016425]\n",
      " [-0.00714295]\n",
      " [-0.40275334]]\n",
      "t [[-0.0197098 ]\n",
      " [-0.51107878]\n",
      " [-0.63367287]\n",
      " ...\n",
      " [-0.45016425]\n",
      " [-0.00714295]\n",
      " [-0.40275334]]\n",
      "t [[-0.02633754]\n",
      " [-0.57805011]\n",
      " [-0.67389004]\n",
      " ...\n",
      " [-0.47505363]\n",
      " [-0.024329  ]\n",
      " [-0.43812276]]\n",
      "t [[-0.02633754]\n",
      " [-0.57805011]\n",
      " [-0.67389004]\n",
      " ...\n",
      " [-0.47505363]\n",
      " [-0.024329  ]\n",
      " [-0.43812276]]\n",
      "Current iteration=8, loss=35440.25503384332\n",
      "t [[-0.03256064]\n",
      " [-0.64145482]\n",
      " [-0.71096523]\n",
      " ...\n",
      " [-0.49796678]\n",
      " [-0.04244062]\n",
      " [-0.47037848]]\n",
      "t [[-0.03256064]\n",
      " [-0.64145482]\n",
      " [-0.71096523]\n",
      " ...\n",
      " [-0.49796678]\n",
      " [-0.04244062]\n",
      " [-0.47037848]]\n",
      "t [[-0.03834152]\n",
      " [-0.70141603]\n",
      " [-0.7455694 ]\n",
      " ...\n",
      " [-0.51942398]\n",
      " [-0.06112914]\n",
      " [-0.49990054]]\n",
      "loss=35005.77820708091\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00827558]\n",
      " [-0.05235644]\n",
      " [-0.18904891]\n",
      " ...\n",
      " [-0.14111762]\n",
      " [ 0.02999882]\n",
      " [-0.07986268]]\n",
      "t [[ 0.00827558]\n",
      " [-0.05235644]\n",
      " [-0.18904891]\n",
      " ...\n",
      " [-0.14111762]\n",
      " [ 0.02999882]\n",
      " [-0.07986268]]\n",
      "t [[ 0.00783104]\n",
      " [-0.11883375]\n",
      " [-0.31906219]\n",
      " ...\n",
      " [-0.23365633]\n",
      " [ 0.04288724]\n",
      " [-0.14663817]]\n",
      "t [[ 0.00783104]\n",
      " [-0.11883375]\n",
      " [-0.31906219]\n",
      " ...\n",
      " [-0.23365633]\n",
      " [ 0.04288724]\n",
      " [-0.14663817]]\n",
      "Current iteration=2, loss=38029.68509123857\n",
      "t [[ 0.0029612 ]\n",
      " [-0.19092634]\n",
      " [-0.41417558]\n",
      " ...\n",
      " [-0.29792681]\n",
      " [ 0.04508345]\n",
      " [-0.20389587]]\n",
      "t [[ 0.0029612 ]\n",
      " [-0.19092634]\n",
      " [-0.41417558]\n",
      " ...\n",
      " [-0.29792681]\n",
      " [ 0.04508345]\n",
      " [-0.20389587]]\n",
      "t [[-0.00398724]\n",
      " [-0.26407431]\n",
      " [-0.48811143]\n",
      " ...\n",
      " [-0.34547023]\n",
      " [ 0.04037583]\n",
      " [-0.2539138 ]]\n",
      "t [[-0.00398724]\n",
      " [-0.26407431]\n",
      " [-0.48811143]\n",
      " ...\n",
      " [-0.34547023]\n",
      " [ 0.04037583]\n",
      " [-0.2539138 ]]\n",
      "Current iteration=4, loss=36855.67266553473\n",
      "t [[-0.01175325]\n",
      " [-0.33591111]\n",
      " [-0.54868217]\n",
      " ...\n",
      " [-0.38283671]\n",
      " [ 0.03104113]\n",
      " [-0.29818736]]\n",
      "t [[-0.01175325]\n",
      " [-0.33591111]\n",
      " [-0.54868217]\n",
      " ...\n",
      " [-0.38283671]\n",
      " [ 0.03104113]\n",
      " [-0.29818736]]\n",
      "t [[-0.01965424]\n",
      " [-0.40524108]\n",
      " [-0.60046834]\n",
      " ...\n",
      " [-0.41383812]\n",
      " [ 0.01852589]\n",
      " [-0.33775437]]\n",
      "t [[-0.01965424]\n",
      " [-0.40524108]\n",
      " [-0.60046834]\n",
      " ...\n",
      " [-0.41383812]\n",
      " [ 0.01852589]\n",
      " [-0.33775437]]\n",
      "Current iteration=6, loss=36075.85935510642\n",
      "t [[-0.02732447]\n",
      " [-0.4715058 ]\n",
      " [-0.64622955]\n",
      " ...\n",
      " [-0.44074114]\n",
      " [ 0.00379858]\n",
      " [-0.37337232]]\n",
      "t [[-0.02732447]\n",
      " [-0.4715058 ]\n",
      " [-0.64622955]\n",
      " ...\n",
      " [-0.44074114]\n",
      " [ 0.00379858]\n",
      " [-0.37337232]]\n",
      "t [[-0.03457648]\n",
      " [-0.5344991 ]\n",
      " [-0.68766596]\n",
      " ...\n",
      " [-0.4649142 ]\n",
      " [-0.01246428]\n",
      " [-0.40561702]]\n",
      "t [[-0.03457648]\n",
      " [-0.5344991 ]\n",
      " [-0.68766596]\n",
      " ...\n",
      " [-0.4649142 ]\n",
      " [-0.01246428]\n",
      " [-0.40561702]]\n",
      "Current iteration=8, loss=35511.54497513205\n",
      "t [[-0.04132441]\n",
      " [-0.59420939]\n",
      " [-0.72584603]\n",
      " ...\n",
      " [-0.48719213]\n",
      " [-0.02977433]\n",
      " [-0.43494055]]\n",
      "t [[-0.04132441]\n",
      " [-0.59420939]\n",
      " [-0.72584603]\n",
      " ...\n",
      " [-0.48719213]\n",
      " [-0.02977433]\n",
      " [-0.43494055]]\n",
      "t [[-0.04754015]\n",
      " [-0.65073011]\n",
      " [-0.76145491]\n",
      " ...\n",
      " [-0.50808827]\n",
      " [-0.04777004]\n",
      " [-0.46170694]]\n",
      "loss=35085.66615283182\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00689189]\n",
      " [-0.05545451]\n",
      " [-0.18780151]\n",
      " ...\n",
      " [-0.08268418]\n",
      " [-0.00995827]\n",
      " [-0.16868911]]\n",
      "t [[ 0.00689189]\n",
      " [-0.05545451]\n",
      " [-0.18780151]\n",
      " ...\n",
      " [-0.08268418]\n",
      " [-0.00995827]\n",
      " [-0.16868911]]\n",
      "t [[ 0.00523612]\n",
      " [-0.12492633]\n",
      " [-0.31608242]\n",
      " ...\n",
      " [-0.12722289]\n",
      " [-0.00729612]\n",
      " [-0.31511915]]\n",
      "t [[ 0.00523612]\n",
      " [-0.12492633]\n",
      " [-0.31608242]\n",
      " ...\n",
      " [-0.12722289]\n",
      " [-0.00729612]\n",
      " [-0.31511915]]\n",
      "Current iteration=2, loss=37990.50485959058\n",
      "t [[-0.00069174]\n",
      " [-0.19985988]\n",
      " [-0.4092944 ]\n",
      " ...\n",
      " [-0.15076109]\n",
      " [ 0.00107947]\n",
      " [-0.44460638]]\n",
      "t [[-0.00069174]\n",
      " [-0.19985988]\n",
      " [-0.4092944 ]\n",
      " ...\n",
      " [-0.15076109]\n",
      " [ 0.00107947]\n",
      " [-0.44460638]]\n",
      "t [[-0.00856723]\n",
      " [-0.27568517]\n",
      " [-0.48131124]\n",
      " ...\n",
      " [-0.16284846]\n",
      " [ 0.0115023 ]\n",
      " [-0.56075806]]\n",
      "t [[-0.00856723]\n",
      " [-0.27568517]\n",
      " [-0.48131124]\n",
      " ...\n",
      " [-0.16284846]\n",
      " [ 0.0115023 ]\n",
      " [-0.56075806]]\n",
      "Current iteration=4, loss=36796.41949360429\n",
      "t [[-0.0171502 ]\n",
      " [-0.35003984]\n",
      " [-0.54001538]\n",
      " ...\n",
      " [-0.1687648 ]\n",
      " [ 0.02209329]\n",
      " [-0.6660713 ]]\n",
      "t [[-0.0171502 ]\n",
      " [-0.35003984]\n",
      " [-0.54001538]\n",
      " ...\n",
      " [-0.1687648 ]\n",
      " [ 0.02209329]\n",
      " [-0.6660713 ]]\n",
      "t [[-0.02577608]\n",
      " [-0.42173641]\n",
      " [-0.59001966]\n",
      " ...\n",
      " [-0.17147483]\n",
      " [ 0.03192554]\n",
      " [-0.76235001]]\n",
      "t [[-0.02577608]\n",
      " [-0.42173641]\n",
      " [-0.59001966]\n",
      " ...\n",
      " [-0.17147483]\n",
      " [ 0.03192554]\n",
      " [-0.76235001]]\n",
      "Current iteration=6, loss=36002.176397320254\n",
      "t [[-0.0340941 ]\n",
      " [-0.49022557]\n",
      " [-0.63409751]\n",
      " ...\n",
      " [-0.17265475]\n",
      " [ 0.0405866 ]\n",
      " [-0.85094979]]\n",
      "t [[-0.0340941 ]\n",
      " [-0.49022557]\n",
      " [-0.63409751]\n",
      " ...\n",
      " [-0.17265475]\n",
      " [ 0.0405866 ]\n",
      " [-0.85094979]]\n",
      "t [[-0.04192902]\n",
      " [-0.55531036]\n",
      " [-0.67395322]\n",
      " ...\n",
      " [-0.17324473]\n",
      " [ 0.0479432 ]\n",
      " [-0.93292308]]\n",
      "t [[-0.04192902]\n",
      " [-0.55531036]\n",
      " [-0.67395322]\n",
      " ...\n",
      " [-0.17324473]\n",
      " [ 0.0479432 ]\n",
      " [-0.93292308]]\n",
      "Current iteration=8, loss=35426.80777380367\n",
      "t [[-0.04920492]\n",
      " [-0.61698824]\n",
      " [-0.71065416]\n",
      " ...\n",
      " [-0.17375812]\n",
      " [ 0.05401071]\n",
      " [-1.0091086 ]]\n",
      "t [[-0.04920492]\n",
      " [-0.61698824]\n",
      " [-0.71065416]\n",
      " ...\n",
      " [-0.17375812]\n",
      " [ 0.05401071]\n",
      " [-1.0091086 ]]\n",
      "t [[-0.05590174]\n",
      " [-0.67536135]\n",
      " [-0.74488156]\n",
      " ...\n",
      " [-0.17445951]\n",
      " [ 0.05887949]\n",
      " [-1.08018907]]\n",
      "loss=34992.35417773543\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.07619395]\n",
      " [-0.32549184]\n",
      " [-0.20458334]\n",
      " ...\n",
      " [-0.14827221]\n",
      " [ 0.03210175]\n",
      " [-0.0864349 ]]\n",
      "t [[ 0.07619395]\n",
      " [-0.32549184]\n",
      " [-0.20458334]\n",
      " ...\n",
      " [-0.14827221]\n",
      " [ 0.03210175]\n",
      " [-0.0864349 ]]\n",
      "t [[ 0.12493181]\n",
      " [-0.55673296]\n",
      " [-0.34722116]\n",
      " ...\n",
      " [-0.24472613]\n",
      " [ 0.04555708]\n",
      " [-0.15863178]]\n",
      "t [[ 0.12493181]\n",
      " [-0.55673296]\n",
      " [-0.34722116]\n",
      " ...\n",
      " [-0.24472613]\n",
      " [ 0.04555708]\n",
      " [-0.15863178]]\n",
      "Current iteration=2, loss=37970.0312352855\n",
      "t [[ 0.15687833]\n",
      " [-0.73024906]\n",
      " [-0.45325423]\n",
      " ...\n",
      " [-0.31162523]\n",
      " [ 0.04751531]\n",
      " [-0.22053048]]\n",
      "t [[ 0.15687833]\n",
      " [-0.73024906]\n",
      " [-0.45325423]\n",
      " ...\n",
      " [-0.31162523]\n",
      " [ 0.04751531]\n",
      " [-0.22053048]]\n",
      "t [[ 0.17827778]\n",
      " [-0.86684525]\n",
      " [-0.53683685]\n",
      " ...\n",
      " [-0.36131643]\n",
      " [ 0.04212636]\n",
      " [-0.27461156]]\n",
      "t [[ 0.17827778]\n",
      " [-0.86684525]\n",
      " [-0.53683685]\n",
      " ...\n",
      " [-0.36131643]\n",
      " [ 0.04212636]\n",
      " [-0.27461156]]\n",
      "Current iteration=4, loss=36783.7763069949\n",
      "t [[ 0.19282578]\n",
      " [-0.97855019]\n",
      " [-0.60594034]\n",
      " ...\n",
      " [-0.40066261]\n",
      " [ 0.03185838]\n",
      " [-0.3224907 ]]\n",
      "t [[ 0.19282578]\n",
      " [-0.97855019]\n",
      " [-0.60594034]\n",
      " ...\n",
      " [-0.40066261]\n",
      " [ 0.03185838]\n",
      " [-0.3224907 ]]\n",
      "t [[ 0.20279158]\n",
      " [-1.07264423]\n",
      " [-0.66522457]\n",
      " ...\n",
      " [-0.43358551]\n",
      " [ 0.01827107]\n",
      " [-0.36528521]]\n",
      "t [[ 0.20279158]\n",
      " [-1.07264423]\n",
      " [-0.66522457]\n",
      " ...\n",
      " [-0.43358551]\n",
      " [ 0.01827107]\n",
      " [-0.36528521]]\n",
      "Current iteration=6, loss=36000.82116861953\n",
      "t [[ 0.20961702]\n",
      " [-1.15373219]\n",
      " [-0.7175098 ]\n",
      " ...\n",
      " [-0.46237804]\n",
      " [ 0.00240525]\n",
      " [-0.40380977]]\n",
      "t [[ 0.20961702]\n",
      " [-1.15373219]\n",
      " [-0.7175098 ]\n",
      " ...\n",
      " [-0.46237804]\n",
      " [ 0.00240525]\n",
      " [-0.40380977]]\n",
      "t [[ 0.21424518]\n",
      " [-1.22484384]\n",
      " [-0.76455584]\n",
      " ...\n",
      " [-0.48840242]\n",
      " [-0.01501428]\n",
      " [-0.43868392]]\n",
      "t [[ 0.21424518]\n",
      " [-1.22484384]\n",
      " [-0.76455584]\n",
      " ...\n",
      " [-0.48840242]\n",
      " [-0.01501428]\n",
      " [-0.43868392]]\n",
      "Current iteration=8, loss=35437.496045972526\n",
      "t [[ 0.21730884]\n",
      " [-1.28804643]\n",
      " [-0.80749352]\n",
      " ...\n",
      " [-0.51247739]\n",
      " [-0.03346635]\n",
      " [-0.47039455]]\n",
      "t [[ 0.21730884]\n",
      " [-1.28804643]\n",
      " [-0.80749352]\n",
      " ...\n",
      " [-0.51247739]\n",
      " [-0.03346635]\n",
      " [-0.47039455]]\n",
      "t [[ 0.21924242]\n",
      " [-1.34480012]\n",
      " [-0.84707237]\n",
      " ...\n",
      " [-0.5350999 ]\n",
      " [-0.05256731]\n",
      " [-0.49933428]]\n",
      "loss=35014.49233218989\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01004797]\n",
      " [-0.06186402]\n",
      " [-0.19395018]\n",
      " ...\n",
      " [-0.14883316]\n",
      " [ 0.02893418]\n",
      " [-0.08829443]]\n",
      "t [[ 0.01004797]\n",
      " [-0.06186402]\n",
      " [-0.19395018]\n",
      " ...\n",
      " [-0.14883316]\n",
      " [ 0.02893418]\n",
      " [-0.08829443]]\n",
      "t [[ 0.01063397]\n",
      " [-0.13821955]\n",
      " [-0.32432793]\n",
      " ...\n",
      " [-0.24473681]\n",
      " [ 0.039665  ]\n",
      " [-0.16202938]]\n",
      "t [[ 0.01063397]\n",
      " [-0.13821955]\n",
      " [-0.32432793]\n",
      " ...\n",
      " [-0.24473681]\n",
      " [ 0.039665  ]\n",
      " [-0.16202938]]\n",
      "Current iteration=2, loss=37942.476246933446\n",
      "t [[ 0.00651403]\n",
      " [-0.2196356 ]\n",
      " [-0.4182672 ]\n",
      " ...\n",
      " [-0.31068745]\n",
      " [ 0.03936148]\n",
      " [-0.22526764]]\n",
      "t [[ 0.00651403]\n",
      " [-0.2196356 ]\n",
      " [-0.4182672 ]\n",
      " ...\n",
      " [-0.31068745]\n",
      " [ 0.03936148]\n",
      " [-0.22526764]]\n",
      "t [[ 2.05003973e-04]\n",
      " [-3.01274735e-01]\n",
      " [-4.90677016e-01]\n",
      " ...\n",
      " [-3.59316353e-01]\n",
      " [ 3.21239766e-02]\n",
      " [-2.80549117e-01]]\n",
      "t [[ 2.05003973e-04]\n",
      " [-3.01274735e-01]\n",
      " [-4.90677016e-01]\n",
      " ...\n",
      " [-3.59316353e-01]\n",
      " [ 3.21239766e-02]\n",
      " [-2.80549117e-01]]\n",
      "Current iteration=4, loss=36737.379561151414\n",
      "t [[-0.00697618]\n",
      " [-0.3807445 ]\n",
      " [-0.54980094]\n",
      " ...\n",
      " [-0.39759511]\n",
      " [ 0.02035797]\n",
      " [-0.32952091]]\n",
      "t [[-0.00697618]\n",
      " [-0.3807445 ]\n",
      " [-0.54980094]\n",
      " ...\n",
      " [-0.39759511]\n",
      " [ 0.02035797]\n",
      " [-0.32952091]]\n",
      "t [[-0.0143321 ]\n",
      " [-0.45691063]\n",
      " [-0.60035143]\n",
      " ...\n",
      " [-0.42948508]\n",
      " [ 0.00556477]\n",
      " [-0.37331951]]\n",
      "t [[-0.0143321 ]\n",
      " [-0.45691063]\n",
      " [-0.60035143]\n",
      " ...\n",
      " [-0.42948508]\n",
      " [ 0.00556477]\n",
      " [-0.37331951]]\n",
      "Current iteration=6, loss=35939.66410904002\n",
      "t [[-0.02149628]\n",
      " [-0.52930135]\n",
      " [-0.64510163]\n",
      " ...\n",
      " [-0.45729102]\n",
      " [-0.01126527]\n",
      " [-0.41277268]]\n",
      "t [[-0.02149628]\n",
      " [-0.52930135]\n",
      " [-0.64510163]\n",
      " ...\n",
      " [-0.45729102]\n",
      " [-0.01126527]\n",
      " [-0.41277268]]\n",
      "t [[-0.02828427]\n",
      " [-0.59779841]\n",
      " [-0.68572191]\n",
      " ...\n",
      " [-0.48237618]\n",
      " [-0.02944982]\n",
      " [-0.44850965]]\n",
      "t [[-0.02828427]\n",
      " [-0.59779841]\n",
      " [-0.68572191]\n",
      " ...\n",
      " [-0.48237618]\n",
      " [-0.02944982]\n",
      " [-0.44850965]]\n",
      "Current iteration=8, loss=35365.42363187595\n",
      "t [[-0.03461305]\n",
      " [-0.66247042]\n",
      " [-0.72323958]\n",
      " ...\n",
      " [-0.50555664]\n",
      " [-0.0485031 ]\n",
      " [-0.48102494]]\n",
      "t [[-0.03461305]\n",
      " [-0.66247042]\n",
      " [-0.72323958]\n",
      " ...\n",
      " [-0.50555664]\n",
      " [-0.0485031 ]\n",
      " [-0.48102494]]\n",
      "t [[-0.04045598]\n",
      " [-0.72348068]\n",
      " [-0.75830037]\n",
      " ...\n",
      " [-0.52732594]\n",
      " [-0.06807078]\n",
      " [-0.51071739]]\n",
      "loss=34934.45484996673\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00858208]\n",
      " [-0.05429557]\n",
      " [-0.19605072]\n",
      " ...\n",
      " [-0.1463442 ]\n",
      " [ 0.03110989]\n",
      " [-0.08282056]]\n",
      "t [[ 0.00858208]\n",
      " [-0.05429557]\n",
      " [-0.19605072]\n",
      " ...\n",
      " [-0.1463442 ]\n",
      " [ 0.03110989]\n",
      " [-0.08282056]]\n",
      "t [[ 0.00779419]\n",
      " [-0.1237658 ]\n",
      " [-0.32865019]\n",
      " ...\n",
      " [-0.24047572]\n",
      " [ 0.04383173]\n",
      " [-0.15157328]]\n",
      "t [[ 0.00779419]\n",
      " [-0.1237658 ]\n",
      " [-0.32865019]\n",
      " ...\n",
      " [-0.24047572]\n",
      " [ 0.04383173]\n",
      " [-0.15157328]]\n",
      "Current iteration=2, loss=37969.69761031604\n",
      "t [[ 0.00241787]\n",
      " [-0.19893912]\n",
      " [-0.42468201]\n",
      " ...\n",
      " [-0.30501827]\n",
      " [ 0.04531244]\n",
      " [-0.2102354 ]]\n",
      "t [[ 0.00241787]\n",
      " [-0.19893912]\n",
      " [-0.42468201]\n",
      " ...\n",
      " [-0.30501827]\n",
      " [ 0.04531244]\n",
      " [-0.2102354 ]]\n",
      "t [[-0.00501625]\n",
      " [-0.27490271]\n",
      " [-0.49898945]\n",
      " ...\n",
      " [-0.35245567]\n",
      " [ 0.03965728]\n",
      " [-0.26128849]]\n",
      "t [[-0.00501625]\n",
      " [-0.27490271]\n",
      " [-0.49898945]\n",
      " ...\n",
      " [-0.35245567]\n",
      " [ 0.03965728]\n",
      " [-0.26128849]]\n",
      "Current iteration=4, loss=36783.94003256716\n",
      "t [[-0.01318742]\n",
      " [-0.34919038]\n",
      " [-0.55981501]\n",
      " ...\n",
      " [-0.38969545]\n",
      " [ 0.02928525]\n",
      " [-0.30633696]]\n",
      "t [[-0.01318742]\n",
      " [-0.34919038]\n",
      " [-0.55981501]\n",
      " ...\n",
      " [-0.38969545]\n",
      " [ 0.02928525]\n",
      " [-0.30633696]]\n",
      "t [[-0.02140138]\n",
      " [-0.42060453]\n",
      " [-0.6118889 ]\n",
      " ...\n",
      " [-0.42066809]\n",
      " [ 0.01571305]\n",
      " [-0.34648401]]\n",
      "t [[-0.02140138]\n",
      " [-0.42060453]\n",
      " [-0.6118889 ]\n",
      " ...\n",
      " [-0.42066809]\n",
      " [ 0.01571305]\n",
      " [-0.34648401]]\n",
      "Current iteration=6, loss=36001.143268049644\n",
      "t [[-2.92990688e-02]\n",
      " [-4.88620429e-01]\n",
      " [-6.58005706e-01]\n",
      " ...\n",
      " [-4.47661314e-01]\n",
      " [-5.33032285e-05]\n",
      " [-3.82530269e-01]]\n",
      "t [[-2.92990688e-02]\n",
      " [-4.88620429e-01]\n",
      " [-6.58005706e-01]\n",
      " ...\n",
      " [-4.47661314e-01]\n",
      " [-5.33032285e-05]\n",
      " [-3.82530269e-01]]\n",
      "t [[-0.03670564]\n",
      " [-0.55307552]\n",
      " [-0.69985613]\n",
      " ...\n",
      " [-0.47202725]\n",
      " [-0.01731611]\n",
      " [-0.4150821 ]]\n",
      "t [[-0.03670564]\n",
      " [-0.55307552]\n",
      " [-0.69985613]\n",
      " ...\n",
      " [-0.47202725]\n",
      " [-0.01731611]\n",
      " [-0.4150821 ]]\n",
      "Current iteration=8, loss=35438.197692284535\n",
      "t [[-0.04354861]\n",
      " [-0.61400114]\n",
      " [-0.73848534]\n",
      " ...\n",
      " [-0.49457343]\n",
      " [-0.03557472]\n",
      " [-0.44461433]]\n",
      "t [[-0.04354861]\n",
      " [-0.61400114]\n",
      " [-0.73848534]\n",
      " ...\n",
      " [-0.49457343]\n",
      " [-0.03557472]\n",
      " [-0.44461433]]\n",
      "t [[-0.04981196]\n",
      " [-0.67152896]\n",
      " [-0.77455428]\n",
      " ...\n",
      " [-0.51578603]\n",
      " [-0.05446066]\n",
      " [-0.47150864]]\n",
      "loss=35015.75369462673\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00714715]\n",
      " [-0.05750838]\n",
      " [-0.19475712]\n",
      " ...\n",
      " [-0.08574656]\n",
      " [-0.0103271 ]\n",
      " [-0.17493685]]\n",
      "t [[ 0.00714715]\n",
      " [-0.05750838]\n",
      " [-0.19475712]\n",
      " ...\n",
      " [-0.08574656]\n",
      " [-0.0103271 ]\n",
      " [-0.17493685]]\n",
      "t [[ 0.00510967]\n",
      " [-0.13007956]\n",
      " [-0.32554238]\n",
      " ...\n",
      " [-0.13049501]\n",
      " [-0.00708933]\n",
      " [-0.32594526]]\n",
      "t [[ 0.00510967]\n",
      " [-0.13007956]\n",
      " [-0.32554238]\n",
      " ...\n",
      " [-0.13049501]\n",
      " [-0.00708933]\n",
      " [-0.32594526]]\n",
      "Current iteration=2, loss=37929.522390733335\n",
      "t [[-0.00135226]\n",
      " [-0.20818705]\n",
      " [-0.41959054]\n",
      " ...\n",
      " [-0.15334108]\n",
      " [ 0.00202068]\n",
      " [-0.45895313]]\n",
      "t [[-0.00135226]\n",
      " [-0.20818705]\n",
      " [-0.41959054]\n",
      " ...\n",
      " [-0.15334108]\n",
      " [ 0.00202068]\n",
      " [-0.45895313]]\n",
      "t [[-0.00973291]\n",
      " [-0.28690855]\n",
      " [-0.49190636]\n",
      " ...\n",
      " [-0.16460145]\n",
      " [ 0.01305388]\n",
      " [-0.57790411]]\n",
      "t [[-0.00973291]\n",
      " [-0.28690855]\n",
      " [-0.49190636]\n",
      " ...\n",
      " [-0.16460145]\n",
      " [ 0.01305388]\n",
      " [-0.57790411]]\n",
      "Current iteration=4, loss=36723.41581838319\n",
      "t [[-0.01873466]\n",
      " [-0.36378343]\n",
      " [-0.55080429]\n",
      " ...\n",
      " [-0.16982567]\n",
      " [ 0.02405558]\n",
      " [-0.68549086]]\n",
      "t [[-0.01873466]\n",
      " [-0.36378343]\n",
      " [-0.55080429]\n",
      " ...\n",
      " [-0.16982567]\n",
      " [ 0.02405558]\n",
      " [-0.68549086]]\n",
      "t [[-0.027683  ]\n",
      " [-0.4376235 ]\n",
      " [-0.60104638]\n",
      " ...\n",
      " [-0.17204984]\n",
      " [ 0.03410292]\n",
      " [-0.78364123]]\n",
      "t [[-0.027683  ]\n",
      " [-0.4376235 ]\n",
      " [-0.60104638]\n",
      " ...\n",
      " [-0.17204984]\n",
      " [ 0.03410292]\n",
      " [-0.78364123]]\n",
      "Current iteration=6, loss=35926.033767526\n",
      "t [[-0.03623503]\n",
      " [-0.50791424]\n",
      " [-0.64543985]\n",
      " ...\n",
      " [-0.17294351]\n",
      " [ 0.04281531]\n",
      " [-0.87379536]]\n",
      "t [[-0.03623503]\n",
      " [-0.50791424]\n",
      " [-0.64543985]\n",
      " ...\n",
      " [-0.17294351]\n",
      " [ 0.04281531]\n",
      " [-0.87379536]]\n",
      "t [[-0.04422902]\n",
      " [-0.57450338]\n",
      " [-0.68567821]\n",
      " ...\n",
      " [-0.17341304]\n",
      " [ 0.05009761]\n",
      " [-0.95706658]]\n",
      "t [[-0.04422902]\n",
      " [-0.57450338]\n",
      " [-0.68567821]\n",
      " ...\n",
      " [-0.17341304]\n",
      " [ 0.05009761]\n",
      " [-0.95706658]]\n",
      "Current iteration=8, loss=35351.99751509158\n",
      "t [[-0.051603  ]\n",
      " [-0.63743227]\n",
      " [-0.72280421]\n",
      " ...\n",
      " [-0.1739325 ]\n",
      " [ 0.05600071]\n",
      " [-1.03433936]]\n",
      "t [[-0.051603  ]\n",
      " [-0.63743227]\n",
      " [-0.72280421]\n",
      " ...\n",
      " [-0.1739325 ]\n",
      " [ 0.05600071]\n",
      " [-1.03433936]]\n",
      "t [[-0.0583494 ]\n",
      " [-0.69684219]\n",
      " [-0.75747368]\n",
      " ...\n",
      " [-0.17473061]\n",
      " [ 0.06064481]\n",
      " [-1.10633182]]\n",
      "loss=34921.02494532962\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.07891517]\n",
      " [-0.33711654]\n",
      " [-0.21188988]\n",
      " ...\n",
      " [-0.15356764]\n",
      " [ 0.03324824]\n",
      " [-0.08952186]]\n",
      "t [[ 0.07891517]\n",
      " [-0.33711654]\n",
      " [-0.21188988]\n",
      " ...\n",
      " [-0.15356764]\n",
      " [ 0.03324824]\n",
      " [-0.08952186]]\n",
      "t [[ 0.12839569]\n",
      " [-0.57319245]\n",
      " [-0.35737258]\n",
      " ...\n",
      " [-0.25158373]\n",
      " [ 0.04650936]\n",
      " [-0.16377778]]\n",
      "t [[ 0.12839569]\n",
      " [-0.57319245]\n",
      " [-0.35737258]\n",
      " ...\n",
      " [-0.25158373]\n",
      " [ 0.04650936]\n",
      " [-0.16377778]]\n",
      "Current iteration=2, loss=37911.26021966828\n",
      "t [[ 0.160277  ]\n",
      " [-0.74875338]\n",
      " [-0.46456202]\n",
      " ...\n",
      " [-0.31874972]\n",
      " [ 0.04771281]\n",
      " [-0.22714081]]\n",
      "t [[ 0.160277  ]\n",
      " [-0.74875338]\n",
      " [-0.46456202]\n",
      " ...\n",
      " [-0.31874972]\n",
      " [ 0.04771281]\n",
      " [-0.22714081]]\n",
      "t [[ 0.18130703]\n",
      " [-0.88624351]\n",
      " [-0.5487097 ]\n",
      " ...\n",
      " [-0.36836471]\n",
      " [ 0.04133861]\n",
      " [-0.28230318]]\n",
      "t [[ 0.18130703]\n",
      " [-0.88624351]\n",
      " [-0.5487097 ]\n",
      " ...\n",
      " [-0.36836471]\n",
      " [ 0.04133861]\n",
      " [-0.28230318]]\n",
      "Current iteration=4, loss=36714.04621738909\n",
      "t [[ 0.195395  ]\n",
      " [-0.99835634]\n",
      " [-0.61820101]\n",
      " ...\n",
      " [-0.40763538]\n",
      " [ 0.03000104]\n",
      " [-0.33099276]]\n",
      "t [[ 0.195395  ]\n",
      " [-0.99835634]\n",
      " [-0.61820101]\n",
      " ...\n",
      " [-0.40763538]\n",
      " [ 0.03000104]\n",
      " [-0.33099276]]\n",
      "t [[ 0.20490035]\n",
      " [-1.09264611]\n",
      " [-0.67784122]\n",
      " ...\n",
      " [-0.4405883 ]\n",
      " [ 0.01533156]\n",
      " [-0.37439417]]\n",
      "t [[ 0.20490035]\n",
      " [-1.09264611]\n",
      " [-0.67784122]\n",
      " ...\n",
      " [-0.4405883 ]\n",
      " [ 0.01533156]\n",
      " [-0.37439417]]\n",
      "Current iteration=6, loss=35928.622655239014\n",
      "t [[ 0.21130052]\n",
      " [-1.17382821]\n",
      " [-0.73048875]\n",
      " ...\n",
      " [-0.46952756]\n",
      " [-0.00159083]\n",
      " [-0.41336659]]\n",
      "t [[ 0.21130052]\n",
      " [-1.17382821]\n",
      " [-0.73048875]\n",
      " ...\n",
      " [-0.46952756]\n",
      " [-0.00159083]\n",
      " [-0.41336659]]\n",
      "t [[ 0.21554981]\n",
      " [-1.24497393]\n",
      " [-0.77790086]\n",
      " ...\n",
      " [-0.49579328]\n",
      " [-0.02002001]\n",
      " [-0.44856128]]\n",
      "t [[ 0.21554981]\n",
      " [-1.24497393]\n",
      " [-0.77790086]\n",
      " ...\n",
      " [-0.49579328]\n",
      " [-0.02002001]\n",
      " [-0.44856128]]\n",
      "Current iteration=8, loss=35367.005976190514\n",
      "t [[ 0.21828165]\n",
      " [-1.30816416]\n",
      " [-0.82119346]\n",
      " ...\n",
      " [-0.52017416]\n",
      " [-0.03942288]\n",
      " [-0.48048893]]\n",
      "t [[ 0.21828165]\n",
      " [-1.30816416]\n",
      " [-0.82119346]\n",
      " ...\n",
      " [-0.52017416]\n",
      " [-0.03942288]\n",
      " [-0.48048893]]\n",
      "t [[ 0.219927  ]\n",
      " [-1.36486263]\n",
      " [-0.8611003 ]\n",
      " ...\n",
      " [-0.54313924]\n",
      " [-0.05940938]\n",
      " [-0.50956065]]\n",
      "loss=34947.60889045273\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01040683]\n",
      " [-0.06407345]\n",
      " [-0.20087697]\n",
      " ...\n",
      " [-0.15414863]\n",
      " [ 0.02996755]\n",
      " [-0.09144781]]\n",
      "t [[ 0.01040683]\n",
      " [-0.06407345]\n",
      " [-0.20087697]\n",
      " ...\n",
      " [-0.15414863]\n",
      " [ 0.02996755]\n",
      " [-0.09144781]]\n",
      "t [[ 0.01067283]\n",
      " [-0.14367886]\n",
      " [-0.33360296]\n",
      " ...\n",
      " [-0.25155507]\n",
      " [ 0.04042337]\n",
      " [-0.16728514]]\n",
      "t [[ 0.01067283]\n",
      " [-0.14367886]\n",
      " [-0.33360296]\n",
      " ...\n",
      " [-0.25155507]\n",
      " [ 0.04042337]\n",
      " [-0.16728514]]\n",
      "Current iteration=2, loss=37882.944742664506\n",
      "t [[ 0.00607192]\n",
      " [-0.22835671]\n",
      " [-0.42828039]\n",
      " ...\n",
      " [-0.31771061]\n",
      " [ 0.03931806]\n",
      " [-0.23202136]]\n",
      "t [[ 0.00607192]\n",
      " [-0.22835671]\n",
      " [-0.42828039]\n",
      " ...\n",
      " [-0.31771061]\n",
      " [ 0.03931806]\n",
      " [-0.23202136]]\n",
      "t [[-0.00069451]\n",
      " [-0.31292387]\n",
      " [-0.50095817]\n",
      " ...\n",
      " [-0.36621326]\n",
      " [ 0.03107417]\n",
      " [-0.28841201]]\n",
      "t [[-0.00069451]\n",
      " [-0.31292387]\n",
      " [-0.50095817]\n",
      " ...\n",
      " [-0.36621326]\n",
      " [ 0.03107417]\n",
      " [-0.28841201]]\n",
      "Current iteration=4, loss=36666.4123666331\n",
      "t [[-0.00825371]\n",
      " [-0.39490634]\n",
      " [-0.56028842]\n",
      " ...\n",
      " [-0.40437785]\n",
      " [ 0.01823581]\n",
      " [-0.33821786]]\n",
      "t [[-0.00825371]\n",
      " [-0.39490634]\n",
      " [-0.56028842]\n",
      " ...\n",
      " [-0.40437785]\n",
      " [ 0.01823581]\n",
      " [-0.33821786]]\n",
      "t [[-0.01589968]\n",
      " [-0.47318367]\n",
      " [-0.61111054]\n",
      " ...\n",
      " [-0.4362674 ]\n",
      " [ 0.00237017]\n",
      " [-0.38264339]]\n",
      "t [[-0.01589968]\n",
      " [-0.47318367]\n",
      " [-0.61111054]\n",
      " ...\n",
      " [-0.4362674 ]\n",
      " [ 0.00237017]\n",
      " [-0.38264339]]\n",
      "Current iteration=6, loss=35866.065566021105\n",
      "t [[-0.02327423]\n",
      " [-0.54732943]\n",
      " [-0.65621596]\n",
      " ...\n",
      " [-0.46419509]\n",
      " [-0.01549852]\n",
      " [-0.42256141]]\n",
      "t [[-0.02327423]\n",
      " [-0.54732943]\n",
      " [-0.65621596]\n",
      " ...\n",
      " [-0.46419509]\n",
      " [-0.01549852]\n",
      " [-0.42256141]]\n",
      "t [[-0.03020602]\n",
      " [-0.61727706]\n",
      " [-0.69725497]\n",
      " ...\n",
      " [-0.48950021]\n",
      " [-0.03466982]\n",
      " [-0.45863331]]\n",
      "t [[-0.03020602]\n",
      " [-0.61727706]\n",
      " [-0.69725497]\n",
      " ...\n",
      " [-0.48950021]\n",
      " [-0.03466982]\n",
      " [-0.45863331]]\n",
      "Current iteration=8, loss=35293.57307396843\n",
      "t [[-0.03662513]\n",
      " [-0.68314342]\n",
      " [-0.73522566]\n",
      " ...\n",
      " [-0.51296768]\n",
      " [-0.05464827]\n",
      " [-0.49137773]]\n",
      "t [[-0.03662513]\n",
      " [-0.68314342]\n",
      " [-0.73522566]\n",
      " ...\n",
      " [-0.51296768]\n",
      " [-0.05464827]\n",
      " [-0.49137773]]\n",
      "t [[-0.04251637]\n",
      " [-0.74513357]\n",
      " [-0.77074675]\n",
      " ...\n",
      " [-0.53506262]\n",
      " [-0.07507479]\n",
      " [-0.52121234]]\n",
      "loss=34866.33496254143\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00888858]\n",
      " [-0.0562347 ]\n",
      " [-0.20305253]\n",
      " ...\n",
      " [-0.15157077]\n",
      " [ 0.03222096]\n",
      " [-0.08577843]]\n",
      "t [[ 0.00888858]\n",
      " [-0.0562347 ]\n",
      " [-0.20305253]\n",
      " ...\n",
      " [-0.15157077]\n",
      " [ 0.03222096]\n",
      " [-0.08577843]]\n",
      "t [[ 0.00773486]\n",
      " [-0.12873452]\n",
      " [-0.33808316]\n",
      " ...\n",
      " [-0.24716748]\n",
      " [ 0.04473167]\n",
      " [-0.15647371]]\n",
      "t [[ 0.00773486]\n",
      " [-0.12873452]\n",
      " [-0.33808316]\n",
      " ...\n",
      " [-0.24716748]\n",
      " [ 0.04473167]\n",
      " [-0.15647371]]\n",
      "Current iteration=2, loss=37910.97245927511\n",
      "t [[ 0.00184113]\n",
      " [-0.2069937 ]\n",
      " [-0.43491792]\n",
      " ...\n",
      " [-0.31189083]\n",
      " [ 0.0454588 ]\n",
      " [-0.21649989]]\n",
      "t [[ 0.00184113]\n",
      " [-0.2069937 ]\n",
      " [-0.43491792]\n",
      " ...\n",
      " [-0.31189083]\n",
      " [ 0.0454588 ]\n",
      " [-0.21649989]]\n",
      "t [[-0.00607625]\n",
      " [-0.28574425]\n",
      " [-0.50953993]\n",
      " ...\n",
      " [-0.3591831 ]\n",
      " [ 0.03883154]\n",
      " [-0.26854889]]\n",
      "t [[-0.00607625]\n",
      " [-0.28574425]\n",
      " [-0.50953993]\n",
      " ...\n",
      " [-0.3591831 ]\n",
      " [ 0.03883154]\n",
      " [-0.26854889]]\n",
      "Current iteration=4, loss=36714.22117748904\n",
      "t [[-0.01464106]\n",
      " [-0.3624308 ]\n",
      " [-0.57060443]\n",
      " ...\n",
      " [-0.39629391]\n",
      " [ 0.02740943]\n",
      " [-0.31433535]]\n",
      "t [[-0.01464106]\n",
      " [-0.3624308 ]\n",
      " [-0.57060443]\n",
      " ...\n",
      " [-0.39629391]\n",
      " [ 0.02740943]\n",
      " [-0.31433535]]\n",
      "t [[-0.02315161]\n",
      " [-0.43586399]\n",
      " [-0.62297232]\n",
      " ...\n",
      " [-0.42725557]\n",
      " [ 0.01277691]\n",
      " [-0.35502795]]\n",
      "t [[-0.02315161]\n",
      " [-0.43586399]\n",
      " [-0.62297232]\n",
      " ...\n",
      " [-0.42725557]\n",
      " [ 0.01277691]\n",
      " [-0.35502795]]\n",
      "Current iteration=6, loss=35928.96897621607\n",
      "t [[-0.03125858]\n",
      " [-0.50556024]\n",
      " [-0.66945927]\n",
      " ...\n",
      " [-0.45436439]\n",
      " [-0.00402442]\n",
      " [-0.39147034]]\n",
      "t [[-0.03125858]\n",
      " [-0.50556024]\n",
      " [-0.66945927]\n",
      " ...\n",
      " [-0.45436439]\n",
      " [-0.00402442]\n",
      " [-0.39147034]]\n",
      "t [[-0.03880171]\n",
      " [-0.57140549]\n",
      " [-0.71173794]\n",
      " ...\n",
      " [-0.4789484 ]\n",
      " [-0.02227711]\n",
      " [-0.42429936]]\n",
      "t [[-0.03880171]\n",
      " [-0.57140549]\n",
      " [-0.71173794]\n",
      " ...\n",
      " [-0.4789484 ]\n",
      " [-0.02227711]\n",
      " [-0.42429936]]\n",
      "Current iteration=8, loss=35367.77527587784\n",
      "t [[-0.04572298]\n",
      " [-0.63347686]\n",
      " [-0.75082564]\n",
      " ...\n",
      " [-0.50178363]\n",
      " [-0.04146932]\n",
      " [-0.45401256]]\n",
      "t [[-0.04572298]\n",
      " [-0.63347686]\n",
      " [-0.75082564]\n",
      " ...\n",
      " [-0.50178363]\n",
      " [-0.04146932]\n",
      " [-0.45401256]]\n",
      "t [[-0.05201899]\n",
      " [-0.69194577]\n",
      " [-0.78735732]\n",
      " ...\n",
      " [-0.52332731]\n",
      " [-0.06122646]\n",
      " [-0.48100919]]\n",
      "loss=34948.97833368813\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0074024 ]\n",
      " [-0.05956225]\n",
      " [-0.20171273]\n",
      " ...\n",
      " [-0.08880893]\n",
      " [-0.01069592]\n",
      " [-0.1811846 ]]\n",
      "t [[ 0.0074024 ]\n",
      " [-0.05956225]\n",
      " [-0.20171273]\n",
      " ...\n",
      " [-0.08880893]\n",
      " [-0.01069592]\n",
      " [-0.1811846 ]]\n",
      "t [[ 0.00496119]\n",
      " [-0.13526907]\n",
      " [-0.33484615]\n",
      " ...\n",
      " [-0.13366704]\n",
      " [-0.0068493 ]\n",
      " [-0.33671212]]\n",
      "t [[ 0.00496119]\n",
      " [-0.13526907]\n",
      " [-0.33484615]\n",
      " ...\n",
      " [-0.13366704]\n",
      " [-0.0068493 ]\n",
      " [-0.33671212]]\n",
      "Current iteration=2, loss=37869.82359675603\n",
      "t [[-0.00204496]\n",
      " [-0.21655471]\n",
      " [-0.42961503]\n",
      " ...\n",
      " [-0.15575892]\n",
      " [ 0.0030058 ]\n",
      " [-0.47316632]]\n",
      "t [[-0.00204496]\n",
      " [-0.21655471]\n",
      " [-0.42961503]\n",
      " ...\n",
      " [-0.15575892]\n",
      " [ 0.0030058 ]\n",
      " [-0.47316632]]\n",
      "t [[-0.01092747]\n",
      " [-0.29814246]\n",
      " [-0.50217381]\n",
      " ...\n",
      " [-0.16617871]\n",
      " [ 0.01463593]\n",
      " [-0.59484055]]\n",
      "t [[-0.01092747]\n",
      " [-0.29814246]\n",
      " [-0.50217381]\n",
      " ...\n",
      " [-0.16617871]\n",
      " [ 0.01463593]\n",
      " [-0.59484055]]\n",
      "Current iteration=4, loss=36652.45414573023\n",
      "t [[-0.02033566]\n",
      " [-0.37748399]\n",
      " [-0.56125125]\n",
      " ...\n",
      " [-0.17073039]\n",
      " [ 0.02601994]\n",
      " [-0.70462684]]\n",
      "t [[-0.02033566]\n",
      " [-0.37748399]\n",
      " [-0.56125125]\n",
      " ...\n",
      " [-0.17073039]\n",
      " [ 0.02601994]\n",
      " [-0.70462684]]\n",
      "t [[-0.02958935]\n",
      " [-0.45340065]\n",
      " [-0.61173937]\n",
      " ...\n",
      " [-0.1725055 ]\n",
      " [ 0.03624773]\n",
      " [-0.80457823]]\n",
      "t [[-0.02958935]\n",
      " [-0.45340065]\n",
      " [-0.61173937]\n",
      " ...\n",
      " [-0.1725055 ]\n",
      " [ 0.03624773]\n",
      " [-0.80457823]]\n",
      "Current iteration=6, loss=35852.4721072571\n",
      "t [[-0.03835661]\n",
      " [-0.52542028]\n",
      " [-0.65646513]\n",
      " ...\n",
      " [-0.17315532]\n",
      " [ 0.0449769 ]\n",
      " [-0.89621977]]\n",
      "t [[-0.03835661]\n",
      " [-0.52542028]\n",
      " [-0.65646513]\n",
      " ...\n",
      " [-0.17315532]\n",
      " [ 0.0449769 ]\n",
      " [-0.89621977]]\n",
      "t [[-0.04649114]\n",
      " [-0.5934402 ]\n",
      " [-0.6971025 ]\n",
      " ...\n",
      " [-0.17354514]\n",
      " [ 0.0521542 ]\n",
      " [-0.98072582]]\n",
      "t [[-0.04649114]\n",
      " [-0.5934402 ]\n",
      " [-0.6971025 ]\n",
      " ...\n",
      " [-0.17354514]\n",
      " [ 0.0521542 ]\n",
      " [-0.98072582]]\n",
      "Current iteration=8, loss=35280.16410809738\n",
      "t [[-0.05394605]\n",
      " [-0.65754859]\n",
      " [-0.73466497]\n",
      " ...\n",
      " [-0.1741059 ]\n",
      " [ 0.05786798]\n",
      " [-1.05902659]]\n",
      "t [[-0.05394605]\n",
      " [-0.65754859]\n",
      " [-0.73466497]\n",
      " ...\n",
      " [-0.1741059 ]\n",
      " [ 0.05786798]\n",
      " [-1.05902659]]\n",
      "t [[-0.06072676]\n",
      " [-0.71792744]\n",
      " [-0.76978111]\n",
      " ...\n",
      " [-0.17502879]\n",
      " [ 0.06226891]\n",
      " [-1.13187555]]\n",
      "loss=34852.894968285575\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.08163638]\n",
      " [-0.34874125]\n",
      " [-0.21919643]\n",
      " ...\n",
      " [-0.15886308]\n",
      " [ 0.03439473]\n",
      " [-0.09260882]]\n",
      "t [[ 0.08163638]\n",
      " [-0.34874125]\n",
      " [-0.21919643]\n",
      " ...\n",
      " [-0.15886308]\n",
      " [ 0.03439473]\n",
      " [-0.09260882]]\n",
      "t [[ 0.13179259]\n",
      " [-0.58942238]\n",
      " [-0.36737327]\n",
      " ...\n",
      " [-0.25831507]\n",
      " [ 0.04741666]\n",
      " [-0.16888872]]\n",
      "t [[ 0.13179259]\n",
      " [-0.58942238]\n",
      " [-0.36737327]\n",
      " ...\n",
      " [-0.25831507]\n",
      " [ 0.04741666]\n",
      " [-0.16888872]]\n",
      "Current iteration=2, loss=37853.71026113633\n",
      "t [[ 0.16355483]\n",
      " [-0.76684097]\n",
      " [-0.47560603]\n",
      " ...\n",
      " [-0.3256615 ]\n",
      " [ 0.04782779]\n",
      " [-0.23367566]]\n",
      "t [[ 0.16355483]\n",
      " [-0.76684097]\n",
      " [-0.47560603]\n",
      " ...\n",
      " [-0.3256615 ]\n",
      " [ 0.04782779]\n",
      " [-0.23367566]]\n",
      "t [[ 0.18418508]\n",
      " [-0.90510804]\n",
      " [-0.5602588 ]\n",
      " ...\n",
      " [-0.37516586]\n",
      " [ 0.04044471]\n",
      " [-0.28987999]]\n",
      "t [[ 0.18418508]\n",
      " [-0.90510804]\n",
      " [-0.5602588 ]\n",
      " ...\n",
      " [-0.37516586]\n",
      " [ 0.04044471]\n",
      " [-0.28987999]]\n",
      "Current iteration=4, loss=36646.234918040944\n",
      "t [[ 0.19780129]\n",
      " [-1.01756282]\n",
      " [-0.63011402]\n",
      " ...\n",
      " [-0.41436157]\n",
      " [ 0.02802596]\n",
      " [-0.33934295]]\n",
      "t [[ 0.19780129]\n",
      " [-1.01756282]\n",
      " [-0.63011402]\n",
      " ...\n",
      " [-0.41436157]\n",
      " [ 0.02802596]\n",
      " [-0.33934295]]\n",
      "t [[ 0.2068467 ]\n",
      " [-1.11201264]\n",
      " [-0.69010503]\n",
      " ...\n",
      " [-0.44736279]\n",
      " [ 0.01227224]\n",
      " [-0.38331646]]\n",
      "t [[ 0.2068467 ]\n",
      " [-1.11201264]\n",
      " [-0.69010503]\n",
      " ...\n",
      " [-0.44736279]\n",
      " [ 0.01227224]\n",
      " [-0.38331646]]\n",
      "Current iteration=6, loss=35858.838677744774\n",
      "t [[ 0.21282927]\n",
      " [-1.19326847]\n",
      " [-0.74311618]\n",
      " ...\n",
      " [-0.4764727 ]\n",
      " [-0.0057012 ]\n",
      " [-0.42270418]]\n",
      "t [[ 0.21282927]\n",
      " [-1.19326847]\n",
      " [-0.74311618]\n",
      " ...\n",
      " [-0.4764727 ]\n",
      " [-0.0057012 ]\n",
      " [-0.42270418]]\n",
      "t [[ 0.21671109]\n",
      " [-1.26443409]\n",
      " [-0.79089504]\n",
      " ...\n",
      " [-0.50300208]\n",
      " [-0.02512847]\n",
      " [-0.45818906]]\n",
      "t [[ 0.21671109]\n",
      " [-1.26443409]\n",
      " [-0.79089504]\n",
      " ...\n",
      " [-0.50300208]\n",
      " [-0.02512847]\n",
      " [-0.45818906]]\n",
      "Current iteration=8, loss=35299.279432776624\n",
      "t [[ 0.21912416]\n",
      " [-1.32759919]\n",
      " [-0.83453903]\n",
      " ...\n",
      " [-0.52770594]\n",
      " [-0.04546565]\n",
      " [-0.49030555]]\n",
      "t [[ 0.21912416]\n",
      " [-1.32759919]\n",
      " [-0.83453903]\n",
      " ...\n",
      " [-0.52770594]\n",
      " [-0.04546565]\n",
      " [-0.49030555]]\n",
      "t [[ 0.22049476]\n",
      " [-1.384229  ]\n",
      " [-0.87476497]\n",
      " ...\n",
      " [-0.55102413]\n",
      " [-0.06631722]\n",
      " [-0.51948324]]\n",
      "loss=34883.67140130469\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01076569]\n",
      " [-0.06628288]\n",
      " [-0.20780376]\n",
      " ...\n",
      " [-0.1594641 ]\n",
      " [ 0.03100091]\n",
      " [-0.09460118]]\n",
      "t [[ 0.01076569]\n",
      " [-0.06628288]\n",
      " [-0.20780376]\n",
      " ...\n",
      " [-0.1594641 ]\n",
      " [ 0.03100091]\n",
      " [-0.09460118]]\n",
      "t [[ 0.01068913]\n",
      " [-0.14917287]\n",
      " [-0.34272337]\n",
      " ...\n",
      " [-0.25824447]\n",
      " [ 0.04113792]\n",
      " [-0.17250508]]\n",
      "t [[ 0.01068913]\n",
      " [-0.14917287]\n",
      " [-0.34272337]\n",
      " ...\n",
      " [-0.25824447]\n",
      " [ 0.04113792]\n",
      " [-0.17250508]]\n",
      "Current iteration=2, loss=37824.640767122575\n",
      "t [[ 0.00559682]\n",
      " [-0.23711237]\n",
      " [-0.43803132]\n",
      " ...\n",
      " [-0.3245183 ]\n",
      " [ 0.03919556]\n",
      " [-0.23869818]]\n",
      "t [[ 0.00559682]\n",
      " [-0.23711237]\n",
      " [-0.43803132]\n",
      " ...\n",
      " [-0.3245183 ]\n",
      " [ 0.03919556]\n",
      " [-0.23869818]]\n",
      "t [[-0.00162447]\n",
      " [-0.32457307]\n",
      " [-0.51092954]\n",
      " ...\n",
      " [-0.37286147]\n",
      " [ 0.02992429]\n",
      " [-0.29615811]]\n",
      "t [[-0.00162447]\n",
      " [-0.32457307]\n",
      " [-0.51092954]\n",
      " ...\n",
      " [-0.37286147]\n",
      " [ 0.02992429]\n",
      " [-0.29615811]]\n",
      "Current iteration=4, loss=36597.381781671786\n",
      "t [[-0.0095508 ]\n",
      " [-0.40901133]\n",
      " [-0.57045806]\n",
      " ...\n",
      " [-0.41091397]\n",
      " [ 0.01600446]\n",
      " [-0.34676036]]\n",
      "t [[-0.0095508 ]\n",
      " [-0.40901133]\n",
      " [-0.57045806]\n",
      " ...\n",
      " [-0.41091397]\n",
      " [ 0.01600446]\n",
      " [-0.34676036]]\n",
      "t [[-0.01747178]\n",
      " [-0.48933117]\n",
      " [-0.62156322]\n",
      " ...\n",
      " [-0.44282278]\n",
      " [-0.00093356]\n",
      " [-0.39177745]]\n",
      "t [[-0.01747178]\n",
      " [-0.48933117]\n",
      " [-0.62156322]\n",
      " ...\n",
      " [-0.44282278]\n",
      " [-0.00093356]\n",
      " [-0.39177745]]\n",
      "Current iteration=6, loss=35794.92315400499\n",
      "t [[-0.0250404 ]\n",
      " [-0.56515899]\n",
      " [-0.66704095]\n",
      " ...\n",
      " [-0.47089726]\n",
      " [-0.01983375]\n",
      " [-0.43212718]]\n",
      "t [[-0.0250404 ]\n",
      " [-0.56515899]\n",
      " [-0.66704095]\n",
      " ...\n",
      " [-0.47089726]\n",
      " [-0.01983375]\n",
      " [-0.43212718]]\n",
      "t [[-0.03210013]\n",
      " [-0.63648451]\n",
      " [-0.70851353]\n",
      " ...\n",
      " [-0.49644548]\n",
      " [-0.03997906]\n",
      " [-0.46850302]]\n",
      "t [[-0.03210013]\n",
      " [-0.63648451]\n",
      " [-0.70851353]\n",
      " ...\n",
      " [-0.49644548]\n",
      " [-0.03997906]\n",
      " [-0.46850302]]\n",
      "Current iteration=8, loss=35224.54388426828\n",
      "t [[-0.0385951 ]\n",
      " [-0.7034755 ]\n",
      " [-0.74694597]\n",
      " ...\n",
      " [-0.52021772]\n",
      " [-0.06086551]\n",
      " [-0.50144779]]\n",
      "t [[-0.0385951 ]\n",
      " [-0.7034755 ]\n",
      " [-0.74694597]\n",
      " ...\n",
      " [-0.52021772]\n",
      " [-0.06086551]\n",
      " [-0.50144779]]\n",
      "t [[-0.04452186]\n",
      " [-0.76637993]\n",
      " [-0.78292879]\n",
      " ...\n",
      " [-0.54264944]\n",
      " [-0.0821301 ]\n",
      " [-0.53139793]]\n",
      "loss=34801.22660220652\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00919509]\n",
      " [-0.05817383]\n",
      " [-0.21005435]\n",
      " ...\n",
      " [-0.15679735]\n",
      " [ 0.03333203]\n",
      " [-0.08873631]]\n",
      "t [[ 0.00919509]\n",
      " [-0.05817383]\n",
      " [-0.21005435]\n",
      " ...\n",
      " [-0.15679735]\n",
      " [ 0.03333203]\n",
      " [-0.08873631]]\n",
      "t [[ 0.00765315]\n",
      " [-0.13373972]\n",
      " [-0.34736164]\n",
      " ...\n",
      " [-0.25373207]\n",
      " [ 0.04558724]\n",
      " [-0.16133954]]\n",
      "t [[ 0.00765315]\n",
      " [-0.13373972]\n",
      " [-0.34736164]\n",
      " ...\n",
      " [-0.25373207]\n",
      " [ 0.04558724]\n",
      " [-0.16133954]]\n",
      "Current iteration=2, loss=37853.46785704256\n",
      "t [[ 0.00123224]\n",
      " [-0.21508766]\n",
      " [-0.44489051]\n",
      " ...\n",
      " [-0.31855055]\n",
      " [ 0.04552446]\n",
      " [-0.22269048]]\n",
      "t [[ 0.00123224]\n",
      " [-0.21508766]\n",
      " [-0.44489051]\n",
      " ...\n",
      " [-0.31855055]\n",
      " [ 0.04552446]\n",
      " [-0.22269048]]\n",
      "t [[-0.00716476]\n",
      " [-0.29659419]\n",
      " [-0.51977778]\n",
      " ...\n",
      " [-0.36566503]\n",
      " [ 0.03790266]\n",
      " [-0.27569759]]\n",
      "t [[-0.00716476]\n",
      " [-0.29659419]\n",
      " [-0.51977778]\n",
      " ...\n",
      " [-0.36566503]\n",
      " [ 0.03790266]\n",
      " [-0.27569759]]\n",
      "Current iteration=4, loss=36646.41892357649\n",
      "t [[-0.0161109 ]\n",
      " [-0.37562637]\n",
      " [-0.58107128]\n",
      " ...\n",
      " [-0.40264953]\n",
      " [ 0.02541973]\n",
      " [-0.3221867 ]]\n",
      "t [[-0.0161109 ]\n",
      " [-0.37562637]\n",
      " [-0.58107128]\n",
      " ...\n",
      " [-0.40264953]\n",
      " [ 0.02541973]\n",
      " [-0.3221867 ]]\n",
      "t [[-0.02490144]\n",
      " [-0.45101356]\n",
      " [-0.63374285]\n",
      " ...\n",
      " [-0.43362064]\n",
      " [ 0.00972522]\n",
      " [-0.36339196]]\n",
      "t [[-0.02490144]\n",
      " [-0.45101356]\n",
      " [-0.63374285]\n",
      " ...\n",
      " [-0.43362064]\n",
      " [ 0.00972522]\n",
      " [-0.36339196]]\n",
      "Current iteration=6, loss=35859.211805672596\n",
      "t [[-0.03319981]\n",
      " [-0.52232071]\n",
      " [-0.68061555]\n",
      " ...\n",
      " [-0.460871  ]\n",
      " [-0.00810567]\n",
      " [-0.4001999 ]]\n",
      "t [[-0.03319981]\n",
      " [-0.52232071]\n",
      " [-0.68061555]\n",
      " ...\n",
      " [-0.460871  ]\n",
      " [-0.00810567]\n",
      " [-0.4001999 ]]\n",
      "t [[-0.04086217]\n",
      " [-0.58948683]\n",
      " [-0.723336  ]\n",
      " ...\n",
      " [-0.48569727]\n",
      " [-0.02733712]\n",
      " [-0.43327775]]\n",
      "t [[-0.04086217]\n",
      " [-0.58948683]\n",
      " [-0.723336  ]\n",
      " ...\n",
      " [-0.48569727]\n",
      " [-0.02733712]\n",
      " [-0.43327775]]\n",
      "Current iteration=8, loss=35300.12023464317\n",
      "t [[-0.04784596]\n",
      " [-0.65263739]\n",
      " [-0.76288983]\n",
      " ...\n",
      " [-0.50884036]\n",
      " [-0.04744717]\n",
      " [-0.46314573]]\n",
      "t [[-0.04784596]\n",
      " [-0.65263739]\n",
      " [-0.76288983]\n",
      " ...\n",
      " [-0.50884036]\n",
      " [-0.04744717]\n",
      " [-0.46314573]]\n",
      "t [[-0.05416074]\n",
      " [-0.71198478]\n",
      " [-0.79988472]\n",
      " ...\n",
      " [-0.53072729]\n",
      " [-0.06805593]\n",
      " [-0.49022063]]\n",
      "loss=34885.15130155764\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00765766]\n",
      " [-0.06161612]\n",
      " [-0.20866834]\n",
      " ...\n",
      " [-0.09187131]\n",
      " [-0.01106475]\n",
      " [-0.18743234]]\n",
      "t [[ 0.00765766]\n",
      " [-0.06161612]\n",
      " [-0.20866834]\n",
      " ...\n",
      " [-0.09187131]\n",
      " [-0.01106475]\n",
      " [-0.18743234]]\n",
      "t [[ 0.0047908 ]\n",
      " [-0.1404947 ]\n",
      " [-0.34399429]\n",
      " ...\n",
      " [-0.13673935]\n",
      " [-0.00657616]\n",
      " [-0.34741989]]\n",
      "t [[ 0.0047908 ]\n",
      " [-0.1404947 ]\n",
      " [-0.34399429]\n",
      " ...\n",
      " [-0.13673935]\n",
      " [-0.00657616]\n",
      " [-0.34741989]]\n",
      "Current iteration=2, loss=37811.36578502557\n",
      "t [[-0.0027686 ]\n",
      " [-0.22496042]\n",
      " [-0.43937513]\n",
      " ...\n",
      " [-0.15801964]\n",
      " [ 0.00403286]\n",
      " [-0.48724776]]\n",
      "t [[-0.0027686 ]\n",
      " [-0.22496042]\n",
      " [-0.43937513]\n",
      " ...\n",
      " [-0.15801964]\n",
      " [ 0.00403286]\n",
      " [-0.48724776]]\n",
      "t [[-0.0121485 ]\n",
      " [-0.30938215]\n",
      " [-0.51212863]\n",
      " ...\n",
      " [-0.16759051]\n",
      " [ 0.01624459]\n",
      " [-0.61157163]]\n",
      "t [[-0.0121485 ]\n",
      " [-0.30938215]\n",
      " [-0.51212863]\n",
      " ...\n",
      " [-0.16759051]\n",
      " [ 0.01624459]\n",
      " [-0.61157163]]\n",
      "Current iteration=4, loss=36583.43586744105\n",
      "t [[-0.02195002]\n",
      " [-0.39113554]\n",
      " [-0.57137727]\n",
      " ...\n",
      " [-0.17149291]\n",
      " [ 0.02798151]\n",
      " [-0.72348636]]\n",
      "t [[-0.02195002]\n",
      " [-0.39113554]\n",
      " [-0.57137727]\n",
      " ...\n",
      " [-0.17149291]\n",
      " [ 0.02798151]\n",
      " [-0.72348636]]\n",
      "t [[-0.03149174]\n",
      " [-0.46906203]\n",
      " [-0.62212304]\n",
      " ...\n",
      " [-0.17285728]\n",
      " [ 0.03835535]\n",
      " [-0.82517114]]\n",
      "t [[-0.03149174]\n",
      " [-0.46906203]\n",
      " [-0.62212304]\n",
      " ...\n",
      " [-0.17285728]\n",
      " [ 0.03835535]\n",
      " [-0.82517114]]\n",
      "Current iteration=6, loss=35781.365353880254\n",
      "t [[-0.04045578]\n",
      " [-0.54273926]\n",
      " [-0.66719875]\n",
      " ...\n",
      " [-0.1733052 ]\n",
      " [ 0.04706799]\n",
      " [-0.91823624]]\n",
      "t [[-0.04045578]\n",
      " [-0.54273926]\n",
      " [-0.66719875]\n",
      " ...\n",
      " [-0.1733052 ]\n",
      " [ 0.04706799]\n",
      " [-0.91823624]]\n",
      "t [[-0.04871307]\n",
      " [-0.6121188 ]\n",
      " [-0.70825073]\n",
      " ...\n",
      " [-0.17365418]\n",
      " [ 0.05411163]\n",
      " [-1.00391711]]\n",
      "t [[-0.04871307]\n",
      " [-0.6121188 ]\n",
      " [-0.70825073]\n",
      " ...\n",
      " [-0.17365418]\n",
      " [ 0.05411163]\n",
      " [-1.00391711]]\n",
      "Current iteration=8, loss=35211.14814393238\n",
      "t [[-0.05623274]\n",
      " [-0.67733827]\n",
      " [-0.74625928]\n",
      " ...\n",
      " [-0.17428865]\n",
      " [ 0.05961364]\n",
      " [-1.08318968]]\n",
      "t [[-0.05623274]\n",
      " [-0.67733827]\n",
      " [-0.74625928]\n",
      " ...\n",
      " [-0.17428865]\n",
      " [ 0.05961364]\n",
      " [-1.08318968]]\n",
      "t [[-0.06303357]\n",
      " [-0.73862161]\n",
      " [-0.78182438]\n",
      " ...\n",
      " [-0.17536119]\n",
      " [ 0.06375552]\n",
      " [-1.1568427 ]]\n",
      "loss=34787.772671074315\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.08435759]\n",
      " [-0.36036596]\n",
      " [-0.22650298]\n",
      " ...\n",
      " [-0.16415851]\n",
      " [ 0.03554122]\n",
      " [-0.09569578]]\n",
      "t [[ 0.08435759]\n",
      " [-0.36036596]\n",
      " [-0.22650298]\n",
      " ...\n",
      " [-0.16415851]\n",
      " [ 0.03554122]\n",
      " [-0.09569578]]\n",
      "t [[ 0.13512277]\n",
      " [-0.60542357]\n",
      " [-0.37722379]\n",
      " ...\n",
      " [-0.26492062]\n",
      " [ 0.04827919]\n",
      " [-0.1739647 ]]\n",
      "t [[ 0.13512277]\n",
      " [-0.60542357]\n",
      " [-0.37722379]\n",
      " ...\n",
      " [-0.26492062]\n",
      " [ 0.04827919]\n",
      " [-0.1739647 ]]\n",
      "Current iteration=2, loss=37797.34104793377\n",
      "t [[ 0.16671475]\n",
      " [-0.78452161]\n",
      " [-0.48639299]\n",
      " ...\n",
      " [-0.33236648]\n",
      " [ 0.04786216]\n",
      " [-0.24013613]]\n",
      "t [[ 0.16671475]\n",
      " [-0.78452161]\n",
      " [-0.48639299]\n",
      " ...\n",
      " [-0.33236648]\n",
      " [ 0.04786216]\n",
      " [-0.24013613]]\n",
      "t [[ 0.18691812]\n",
      " [-0.92345913]\n",
      " [-0.57149789]\n",
      " ...\n",
      " [-0.38173186]\n",
      " [ 0.03944865]\n",
      " [-0.29734454]]\n",
      "t [[ 0.18691812]\n",
      " [-0.92345913]\n",
      " [-0.57149789]\n",
      " ...\n",
      " [-0.38173186]\n",
      " [ 0.03944865]\n",
      " [-0.29734454]]\n",
      "Current iteration=4, loss=36580.25236162371\n",
      "t [[ 0.20005363]\n",
      " [-1.03619865]\n",
      " [-0.64169851]\n",
      " ...\n",
      " [-0.42085756]\n",
      " [ 0.02593903]\n",
      " [-0.34754531]]\n",
      "t [[ 0.20005363]\n",
      " [-1.03619865]\n",
      " [-0.64169851]\n",
      " ...\n",
      " [-0.42085756]\n",
      " [ 0.02593903]\n",
      " [-0.34754531]]\n",
      "t [[ 0.20864153]\n",
      " [-1.13077883]\n",
      " [-0.70203825]\n",
      " ...\n",
      " [-0.45392755]\n",
      " [ 0.00910064]\n",
      " [-0.39205767]]\n",
      "t [[ 0.20864153]\n",
      " [-1.13077883]\n",
      " [-0.70203825]\n",
      " ...\n",
      " [-0.45392755]\n",
      " [ 0.00910064]\n",
      " [-0.39205767]]\n",
      "Current iteration=6, loss=35791.35385369453\n",
      "t [[ 0.21421526]\n",
      " [-1.21209158]\n",
      " [-0.75541551]\n",
      " ...\n",
      " [-0.48323224]\n",
      " [-0.00991707]\n",
      " [-0.43182968]]\n",
      "t [[ 0.21421526]\n",
      " [-1.21209158]\n",
      " [-0.75541551]\n",
      " ...\n",
      " [-0.48323224]\n",
      " [-0.00991707]\n",
      " [-0.43182968]]\n",
      "t [[ 0.21774144]\n",
      " [-1.28326472]\n",
      " [-0.80356153]\n",
      " ...\n",
      " [-0.51004638]\n",
      " [-0.03032986]\n",
      " [-0.46757591]]\n",
      "t [[ 0.21774144]\n",
      " [-1.28326472]\n",
      " [-0.80356153]\n",
      " ...\n",
      " [-0.51004638]\n",
      " [-0.03032986]\n",
      " [-0.46757591]]\n",
      "Current iteration=8, loss=35234.17045511595\n",
      "t [[ 0.21984872]\n",
      " [-1.34639264]\n",
      " [-0.84755235]\n",
      " ...\n",
      " [-0.53508828]\n",
      " [-0.05158415]\n",
      " [-0.49985457]]\n",
      "t [[ 0.21984872]\n",
      " [-1.34639264]\n",
      " [-0.84755235]\n",
      " ...\n",
      " [-0.53508828]\n",
      " [-0.05158415]\n",
      " [-0.49985457]]\n",
      "t [[ 0.2209577 ]\n",
      " [-1.40294053]\n",
      " [-0.88808715]\n",
      " ...\n",
      " [-0.55876782]\n",
      " [-0.07327986]\n",
      " [-0.52911371]]\n",
      "loss=34822.50568934305\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01112454]\n",
      " [-0.06849231]\n",
      " [-0.21473055]\n",
      " ...\n",
      " [-0.16477957]\n",
      " [ 0.03203427]\n",
      " [-0.09775455]]\n",
      "t [[ 0.01112454]\n",
      " [-0.06849231]\n",
      " [-0.21473055]\n",
      " ...\n",
      " [-0.16477957]\n",
      " [ 0.03203427]\n",
      " [-0.09775455]]\n",
      "t [[ 0.01068297]\n",
      " [-0.15470138]\n",
      " [-0.35168972]\n",
      " ...\n",
      " [-0.26480549]\n",
      " [ 0.04180884]\n",
      " [-0.17768929]]\n",
      "t [[ 0.01068297]\n",
      " [-0.15470138]\n",
      " [-0.35168972]\n",
      " ...\n",
      " [-0.26480549]\n",
      " [ 0.04180884]\n",
      " [-0.17768929]]\n",
      "Current iteration=2, loss=37767.52344509152\n",
      "t [[ 0.00508995]\n",
      " [-0.24590023]\n",
      " [-0.44752715]\n",
      " ...\n",
      " [-0.33111658]\n",
      " [ 0.03899587]\n",
      " [-0.24529922]]\n",
      "t [[ 0.00508995]\n",
      " [-0.24590023]\n",
      " [-0.44752715]\n",
      " ...\n",
      " [-0.33111658]\n",
      " [ 0.03899587]\n",
      " [-0.24529922]]\n",
      "t [[-0.00258252]\n",
      " [-0.33621789]\n",
      " [-0.52060555]\n",
      " ...\n",
      " [-0.3792732 ]\n",
      " [ 0.02867824]\n",
      " [-0.30379   ]]\n",
      "t [[-0.00258252]\n",
      " [-0.33621789]\n",
      " [-0.52060555]\n",
      " ...\n",
      " [-0.3792732 ]\n",
      " [ 0.02867824]\n",
      " [-0.30379   ]]\n",
      "Current iteration=4, loss=36530.197975167895\n",
      "t [[-0.01086441]\n",
      " [-0.42305408]\n",
      " [-0.58032955]\n",
      " ...\n",
      " [-0.4172201 ]\n",
      " [ 0.01366964]\n",
      " [-0.35515254]]\n",
      "t [[-0.01086441]\n",
      " [-0.42305408]\n",
      " [-0.58032955]\n",
      " ...\n",
      " [-0.4172201 ]\n",
      " [ 0.01366964]\n",
      " [-0.35515254]]\n",
      "t [[-0.01904521]\n",
      " [-0.5053482 ]\n",
      " [-0.63173183]\n",
      " ...\n",
      " [-0.44916997]\n",
      " [-0.0043392 ]\n",
      " [-0.40072738]]\n",
      "t [[-0.01904521]\n",
      " [-0.5053482 ]\n",
      " [-0.63173183]\n",
      " ...\n",
      " [-0.44916997]\n",
      " [-0.0043392 ]\n",
      " [-0.40072738]]\n",
      "Current iteration=6, loss=35726.120856691734\n",
      "t [[-0.02679189]\n",
      " [-0.58278674]\n",
      " [-0.67759942]\n",
      " ...\n",
      " [-0.47741637]\n",
      " [-0.02426258]\n",
      " [-0.44147721]]\n",
      "t [[-0.02679189]\n",
      " [-0.58278674]\n",
      " [-0.67759942]\n",
      " ...\n",
      " [-0.47741637]\n",
      " [-0.02426258]\n",
      " [-0.44147721]]\n",
      "t [[-0.03396438]\n",
      " [-0.65542006]\n",
      " [-0.7195193 ]\n",
      " ...\n",
      " [-0.50322958]\n",
      " [-0.04536833]\n",
      " [-0.47812757]]\n",
      "t [[-0.03396438]\n",
      " [-0.65542006]\n",
      " [-0.7195193 ]\n",
      " ...\n",
      " [-0.50322958]\n",
      " [-0.04536833]\n",
      " [-0.47812757]]\n",
      "Current iteration=8, loss=35158.187871141665\n",
      "t [[-0.04052157]\n",
      " [-0.72346904]\n",
      " [-0.75842028]\n",
      " ...\n",
      " [-0.52732225]\n",
      " [-0.06714502]\n",
      " [-0.51124542]]\n",
      "t [[-0.04052157]\n",
      " [-0.72346904]\n",
      " [-0.75842028]\n",
      " ...\n",
      " [-0.52732225]\n",
      " [-0.06714502]\n",
      " [-0.51124542]]\n",
      "t [[-0.04647201]\n",
      " [-0.7872256 ]\n",
      " [-0.79486402]\n",
      " ...\n",
      " [-0.55009954]\n",
      " [-0.08922661]\n",
      " [-0.541286  ]]\n",
      "loss=34738.95207294455\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00950159]\n",
      " [-0.06011295]\n",
      " [-0.21705616]\n",
      " ...\n",
      " [-0.16202393]\n",
      " [ 0.03444309]\n",
      " [-0.09169419]]\n",
      "t [[ 0.00950159]\n",
      " [-0.06011295]\n",
      " [-0.21705616]\n",
      " ...\n",
      " [-0.16202393]\n",
      " [ 0.03444309]\n",
      " [-0.09169419]]\n",
      "t [[ 0.0075492 ]\n",
      " [-0.13878124]\n",
      " [-0.35648622]\n",
      " ...\n",
      " [-0.26016997]\n",
      " [ 0.04639865]\n",
      " [-0.16617088]]\n",
      "t [[ 0.0075492 ]\n",
      " [-0.13878124]\n",
      " [-0.35648622]\n",
      " ...\n",
      " [-0.26016997]\n",
      " [ 0.04639865]\n",
      " [-0.16617088]]\n",
      "Current iteration=2, loss=37797.14321529836\n",
      "t [[ 0.00059243]\n",
      " [-0.22321859]\n",
      " [-0.45460688]\n",
      " ...\n",
      " [-0.3250034 ]\n",
      " [ 0.0455113 ]\n",
      " [-0.22880827]]\n",
      "t [[ 0.00059243]\n",
      " [-0.22321859]\n",
      " [-0.45460688]\n",
      " ...\n",
      " [-0.3250034 ]\n",
      " [ 0.0455113 ]\n",
      " [-0.22880827]]\n",
      "t [[-0.0082794 ]\n",
      " [-0.30744795]\n",
      " [-0.52971735]\n",
      " ...\n",
      " [-0.37191356]\n",
      " [ 0.03687458]\n",
      " [-0.2827371 ]]\n",
      "t [[-0.0082794 ]\n",
      " [-0.30744795]\n",
      " [-0.52971735]\n",
      " ...\n",
      " [-0.37191356]\n",
      " [ 0.03687458]\n",
      " [-0.2827371 ]]\n",
      "Current iteration=4, loss=36580.443615780714\n",
      "t [[-0.0175939 ]\n",
      " [-0.38877148]\n",
      " [-0.59123527]\n",
      " ...\n",
      " [-0.40877879]\n",
      " [ 0.02332193]\n",
      " [-0.32989503]]\n",
      "t [[-0.0175939 ]\n",
      " [-0.38877148]\n",
      " [-0.59123527]\n",
      " ...\n",
      " [-0.40877879]\n",
      " [ 0.02332193]\n",
      " [-0.32989503]]\n",
      "t [[-0.02664769]\n",
      " [-0.46604795]\n",
      " [-0.64422296]\n",
      " ...\n",
      " [-0.4397819 ]\n",
      " [ 0.00656528]\n",
      " [-0.37158155]]\n",
      "t [[-0.02664769]\n",
      " [-0.46604795]\n",
      " [-0.64422296]\n",
      " ...\n",
      " [-0.4397819 ]\n",
      " [ 0.00656528]\n",
      " [-0.37158155]]\n",
      "Current iteration=6, loss=35791.75656682472\n",
      "t [[-0.03511995]\n",
      " [-0.53889807]\n",
      " [-0.69149754]\n",
      " ...\n",
      " [-0.46719985]\n",
      " [-0.01228852]\n",
      " [-0.40872595]]\n",
      "t [[-0.03511995]\n",
      " [-0.53889807]\n",
      " [-0.69149754]\n",
      " ...\n",
      " [-0.46719985]\n",
      " [-0.01228852]\n",
      " [-0.40872595]]\n",
      "t [[-0.04288495]\n",
      " [-0.6073182 ]\n",
      " [-0.73467229]\n",
      " ...\n",
      " [-0.49229125]\n",
      " [-0.03248669]\n",
      " [-0.44202573]]\n",
      "t [[-0.04288495]\n",
      " [-0.6073182 ]\n",
      " [-0.73467229]\n",
      " ...\n",
      " [-0.49229125]\n",
      " [-0.03248669]\n",
      " [-0.44202573]]\n",
      "Current iteration=8, loss=35235.08632037282\n",
      "t [[-0.0499164 ]\n",
      " [-0.67148437]\n",
      " [-0.77469802]\n",
      " ...\n",
      " [-0.51575892]\n",
      " [-0.05349815]\n",
      " [-0.47202374]]\n",
      "t [[-0.0499164 ]\n",
      " [-0.67148437]\n",
      " [-0.77469802]\n",
      " ...\n",
      " [-0.51575892]\n",
      " [-0.05349815]\n",
      " [-0.47202374]]\n",
      "t [[-0.05623711]\n",
      " [-0.7316509 ]\n",
      " [-0.81215445]\n",
      " ...\n",
      " [-0.53799888]\n",
      " [-0.0749385 ]\n",
      " [-0.49915429]]\n",
      "loss=34824.09793966479\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00791291]\n",
      " [-0.06366999]\n",
      " [-0.21562395]\n",
      " ...\n",
      " [-0.09493369]\n",
      " [-0.01143357]\n",
      " [-0.19368009]]\n",
      "t [[ 0.00791291]\n",
      " [-0.06366999]\n",
      " [-0.21562395]\n",
      " ...\n",
      " [-0.09493369]\n",
      " [-0.01143357]\n",
      " [-0.19368009]]\n",
      "t [[ 0.00459862]\n",
      " [-0.14575626]\n",
      " [-0.3529874 ]\n",
      " ...\n",
      " [-0.13971232]\n",
      " [-0.00627002]\n",
      " [-0.35806871]]\n",
      "t [[ 0.00459862]\n",
      " [-0.14575626]\n",
      " [-0.3529874 ]\n",
      " ...\n",
      " [-0.13971232]\n",
      " [-0.00627002]\n",
      " [-0.35806871]]\n",
      "Current iteration=2, loss=37754.10748830488\n",
      "t [[-0.00352195]\n",
      " [-0.23340178]\n",
      " [-0.44887806]\n",
      " ...\n",
      " [-0.16012822]\n",
      " [ 0.00509991]\n",
      " [-0.5011992 ]]\n",
      "t [[-0.00352195]\n",
      " [-0.23340178]\n",
      " [-0.44887806]\n",
      " ...\n",
      " [-0.16012822]\n",
      " [ 0.00509991]\n",
      " [-0.5011992 ]]\n",
      "t [[-0.01339362]\n",
      " [-0.32062307]\n",
      " [-0.52178532]\n",
      " ...\n",
      " [-0.16884674]\n",
      " [ 0.01787613]\n",
      " [-0.62810152]]\n",
      "t [[-0.01339362]\n",
      " [-0.32062307]\n",
      " [-0.52178532]\n",
      " ...\n",
      " [-0.16884674]\n",
      " [ 0.01787613]\n",
      " [-0.62810152]]\n",
      "Current iteration=4, loss=36516.27004340652\n",
      "t [[-0.02357474]\n",
      " [-0.40473251]\n",
      " [-0.58120221]\n",
      " ...\n",
      " [-0.17212633]\n",
      " [ 0.02993576]\n",
      " [-0.74207632]]\n",
      "t [[-0.02357474]\n",
      " [-0.40473251]\n",
      " [-0.58120221]\n",
      " ...\n",
      " [-0.17212633]\n",
      " [ 0.02993576]\n",
      " [-0.74207632]]\n",
      "t [[-0.03338711]\n",
      " [-0.48460242]\n",
      " [-0.63222   ]\n",
      " ...\n",
      " [-0.17311937]\n",
      " [ 0.04042162]\n",
      " [-0.8454297 ]]\n",
      "t [[-0.03338711]\n",
      " [-0.48460242]\n",
      " [-0.63222   ]\n",
      " ...\n",
      " [-0.17311937]\n",
      " [ 0.04042162]\n",
      " [-0.8454297 ]]\n",
      "Current iteration=6, loss=35712.597049713055\n",
      "t [[-0.04252987]\n",
      " [-0.55986752]\n",
      " [-0.67766381]\n",
      " ...\n",
      " [-0.17340658]\n",
      " [ 0.04908585]\n",
      " [-0.93985737]]\n",
      "t [[-0.04252987]\n",
      " [-0.55986752]\n",
      " [-0.67766381]\n",
      " ...\n",
      " [-0.17340658]\n",
      " [ 0.04908585]\n",
      " [-0.93985737]]\n",
      "t [[-0.05089292]\n",
      " [-0.63053798]\n",
      " [-0.71914491]\n",
      " ...\n",
      " [-0.17375146]\n",
      " [ 0.05596922]\n",
      " [-1.02665592]]\n",
      "t [[-0.05089292]\n",
      " [-0.63053798]\n",
      " [-0.71914491]\n",
      " ...\n",
      " [-0.17375146]\n",
      " [ 0.05596922]\n",
      " [-1.02665592]]\n",
      "Current iteration=8, loss=35144.80155107459\n",
      "t [[-0.05846216]\n",
      " [-0.69680315]\n",
      " [-0.75760717]\n",
      " ...\n",
      " [-0.17448927]\n",
      " [ 0.06123939]\n",
      " [-1.10684696]]\n",
      "t [[-0.05846216]\n",
      " [-0.69680315]\n",
      " [-0.75760717]\n",
      " ...\n",
      " [-0.17448927]\n",
      " [ 0.06123939]\n",
      " [-1.10684696]]\n",
      "t [[-0.06526999]\n",
      " [-0.75892991]\n",
      " [-0.79362129]\n",
      " ...\n",
      " [-0.17573324]\n",
      " [ 0.0651088 ]\n",
      " [-1.18125443]]\n",
      "loss=34725.48071553065\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0870788 ]\n",
      " [-0.37199067]\n",
      " [-0.23380953]\n",
      " ...\n",
      " [-0.16945395]\n",
      " [ 0.03668771]\n",
      " [-0.09878274]]\n",
      "t [[ 0.0870788 ]\n",
      " [-0.37199067]\n",
      " [-0.23380953]\n",
      " ...\n",
      " [-0.16945395]\n",
      " [ 0.03668771]\n",
      " [-0.09878274]]\n",
      "t [[ 0.13838645]\n",
      " [-0.6211969 ]\n",
      " [-0.38692473]\n",
      " ...\n",
      " [-0.27140083]\n",
      " [ 0.04909715]\n",
      " [-0.17900582]]\n",
      "t [[ 0.13838645]\n",
      " [-0.6211969 ]\n",
      " [-0.38692473]\n",
      " ...\n",
      " [-0.27140083]\n",
      " [ 0.04909715]\n",
      " [-0.17900582]]\n",
      "Current iteration=2, loss=37742.113429808334\n",
      "t [[ 0.16975962]\n",
      " [-0.80180499]\n",
      " [-0.49692957]\n",
      " ...\n",
      " [-0.33887047]\n",
      " [ 0.0478178 ]\n",
      " [-0.24652332]]\n",
      "t [[ 0.16975962]\n",
      " [-0.80180499]\n",
      " [-0.49692957]\n",
      " ...\n",
      " [-0.33887047]\n",
      " [ 0.0478178 ]\n",
      " [-0.24652332]]\n",
      "t [[ 0.18951212]\n",
      " [-0.94131642]\n",
      " [-0.58244024]\n",
      " ...\n",
      " [-0.38807425]\n",
      " [ 0.03835427]\n",
      " [-0.30469926]]\n",
      "t [[ 0.18951212]\n",
      " [-0.94131642]\n",
      " [-0.58244024]\n",
      " ...\n",
      " [-0.38807425]\n",
      " [ 0.03835427]\n",
      " [-0.30469926]]\n",
      "Current iteration=4, loss=36516.015342223734\n",
      "t [[ 0.20216053]\n",
      " [-1.05429139]\n",
      " [-0.65297257]\n",
      " ...\n",
      " [-0.42713882]\n",
      " [ 0.02374588]\n",
      " [-0.35560373]]\n",
      "t [[ 0.20216053]\n",
      " [-1.05429139]\n",
      " [-0.65297257]\n",
      " ...\n",
      " [-0.42713882]\n",
      " [ 0.02374588]\n",
      " [-0.35560373]]\n",
      "t [[ 0.21029502]\n",
      " [-1.14897746]\n",
      " [-0.71366157]\n",
      " ...\n",
      " [-0.46029976]\n",
      " [ 0.00582384]\n",
      " [-0.40062316]]\n",
      "t [[ 0.21029502]\n",
      " [-1.14897746]\n",
      " [-0.71366157]\n",
      " ...\n",
      " [-0.46029976]\n",
      " [ 0.00582384]\n",
      " [-0.40062316]]\n",
      "Current iteration=6, loss=35726.06125528158\n",
      "t [[ 0.21546954]\n",
      " [-1.23033318]\n",
      " [-0.76740812]\n",
      " ...\n",
      " [-0.48982316]\n",
      " [-0.01423018]\n",
      " [-0.44074987]]\n",
      "t [[ 0.21546954]\n",
      " [-1.23033318]\n",
      " [-0.76740812]\n",
      " ...\n",
      " [-0.48982316]\n",
      " [-0.01423018]\n",
      " [-0.44074987]]\n",
      "t [[ 0.21865217]\n",
      " [-1.30150276]\n",
      " [-0.81592118]\n",
      " ...\n",
      " [-0.51694175]\n",
      " [-0.03561507]\n",
      " [-0.47673002]]\n",
      "t [[ 0.21865217]\n",
      " [-1.30150276]\n",
      " [-0.81592118]\n",
      " ...\n",
      " [-0.51694175]\n",
      " [-0.03561507]\n",
      " [-0.47673002]]\n",
      "Current iteration=8, loss=35171.543196530365\n",
      "t [[ 0.22046649]\n",
      " [-1.36458184]\n",
      " [-0.86025311]\n",
      " ...\n",
      " [-0.54233471]\n",
      " [-0.05776867]\n",
      " [-0.50914558]]\n",
      "t [[ 0.22046649]\n",
      " [-1.36458184]\n",
      " [-0.86025311]\n",
      " ...\n",
      " [-0.54233471]\n",
      " [-0.05776867]\n",
      " [-0.50914558]]\n",
      "t [[ 0.22132656]\n",
      " [-1.4210346 ]\n",
      " [-0.90108525]\n",
      " ...\n",
      " [-0.5663816 ]\n",
      " [-0.08028722]\n",
      " [-0.53846308]]\n",
      "loss=34763.95032792616\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0114834 ]\n",
      " [-0.07070174]\n",
      " [-0.22165735]\n",
      " ...\n",
      " [-0.17009504]\n",
      " [ 0.03306764]\n",
      " [-0.10090792]]\n",
      "t [[ 0.0114834 ]\n",
      " [-0.07070174]\n",
      " [-0.22165735]\n",
      " ...\n",
      " [-0.17009504]\n",
      " [ 0.03306764]\n",
      " [-0.10090792]]\n",
      "t [[ 0.0106545 ]\n",
      " [-0.16026424]\n",
      " [-0.36050263]\n",
      " ...\n",
      " [-0.27123863]\n",
      " [ 0.04243635]\n",
      " [-0.18283789]]\n",
      "t [[ 0.0106545 ]\n",
      " [-0.16026424]\n",
      " [-0.36050263]\n",
      " ...\n",
      " [-0.27123863]\n",
      " [ 0.04243635]\n",
      " [-0.18283789]]\n",
      "Current iteration=2, loss=37711.553101536694\n",
      "t [[ 0.00455253]\n",
      " [-0.25471796]\n",
      " [-0.45677495]\n",
      " ...\n",
      " [-0.33751143]\n",
      " [ 0.03872088]\n",
      " [-0.25182564]]\n",
      "t [[ 0.00455253]\n",
      " [-0.25471796]\n",
      " [-0.45677495]\n",
      " ...\n",
      " [-0.33751143]\n",
      " [ 0.03872088]\n",
      " [-0.25182564]]\n",
      "t [[-0.0035664 ]\n",
      " [-0.34785404]\n",
      " [-0.53000009]\n",
      " ...\n",
      " [-0.38546023]\n",
      " [ 0.0273398 ]\n",
      " [-0.31131019]]\n",
      "t [[-0.0035664 ]\n",
      " [-0.34785404]\n",
      " [-0.53000009]\n",
      " ...\n",
      " [-0.38546023]\n",
      " [ 0.0273398 ]\n",
      " [-0.31131019]]\n",
      "Current iteration=4, loss=36464.777959944724\n",
      "t [[-0.01219166]\n",
      " [-0.43702957]\n",
      " [-0.5899215 ]\n",
      " ...\n",
      " [-0.42331192]\n",
      " [ 0.01123683]\n",
      " [-0.36339839]]\n",
      "t [[-0.01219166]\n",
      " [-0.43702957]\n",
      " [-0.5899215 ]\n",
      " ...\n",
      " [-0.42331192]\n",
      " [ 0.01123683]\n",
      " [-0.36339839]]\n",
      "t [[-0.02061703]\n",
      " [-0.52123036]\n",
      " [-0.6416371 ]\n",
      " ...\n",
      " [-0.45532629]\n",
      " [-0.00783995]\n",
      " [-0.40949862]]\n",
      "t [[-0.02061703]\n",
      " [-0.52123036]\n",
      " [-0.6416371 ]\n",
      " ...\n",
      " [-0.45532629]\n",
      " [-0.00783995]\n",
      " [-0.40949862]]\n",
      "Current iteration=6, loss=35659.55104115463\n",
      "t [[-0.02852619]\n",
      " [-0.60021009]\n",
      " [-0.6879121 ]\n",
      " ...\n",
      " [-0.48376948]\n",
      " [-0.02877722]\n",
      " [-0.45061838]]\n",
      "t [[-0.02852619]\n",
      " [-0.60021009]\n",
      " [-0.6879121 ]\n",
      " ...\n",
      " [-0.48376948]\n",
      " [-0.02877722]\n",
      " [-0.45061838]]\n",
      "t [[-0.03579693]\n",
      " [-0.67408369]\n",
      " [-0.73029163]\n",
      " ...\n",
      " [-0.50986803]\n",
      " [-0.05082912]\n",
      " [-0.48751527]]\n",
      "t [[-0.03579693]\n",
      " [-0.67408369]\n",
      " [-0.73029163]\n",
      " ...\n",
      " [-0.50986803]\n",
      " [-0.05082912]\n",
      " [-0.48751527]]\n",
      "Current iteration=8, loss=35094.3669825312\n",
      "t [[-0.04240354]\n",
      " [-0.74312711]\n",
      " [-0.76966591]\n",
      " ...\n",
      " [-0.53429471]\n",
      " [-0.07347782]\n",
      " [-0.52078037]]\n",
      "t [[-0.04240354]\n",
      " [-0.74312711]\n",
      " [-0.76966591]\n",
      " ...\n",
      " [-0.53429471]\n",
      " [-0.07347782]\n",
      " [-0.52078037]]\n",
      "t [[-0.04836672]\n",
      " [-0.80767691]\n",
      " [-0.80656761]\n",
      " ...\n",
      " [-0.5574241 ]\n",
      " [-0.09635506]\n",
      " [-0.55088773]]\n",
      "loss=34679.34659843994\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00980809]\n",
      " [-0.06205208]\n",
      " [-0.22405797]\n",
      " ...\n",
      " [-0.16725051]\n",
      " [ 0.03555416]\n",
      " [-0.09465206]]\n",
      "t [[ 0.00980809]\n",
      " [-0.06205208]\n",
      " [-0.22405797]\n",
      " ...\n",
      " [-0.16725051]\n",
      " [ 0.03555416]\n",
      " [-0.09465206]]\n",
      "t [[ 0.00742312]\n",
      " [-0.14385889]\n",
      " [-0.36545748]\n",
      " ...\n",
      " [-0.26648165]\n",
      " [ 0.04716609]\n",
      " [-0.17096784]]\n",
      "t [[ 0.00742312]\n",
      " [-0.14385889]\n",
      " [-0.36545748]\n",
      " ...\n",
      " [-0.26648165]\n",
      " [ 0.04716609]\n",
      " [-0.17096784]]\n",
      "Current iteration=2, loss=37741.95912784268\n",
      "t [[-7.70804558e-05]\n",
      " [-2.31384130e-01]\n",
      " [-4.64074052e-01]\n",
      " ...\n",
      " [-3.31255296e-01]\n",
      " [ 4.54212077e-02]\n",
      " [-2.34854361e-01]]\n",
      "t [[-7.70804558e-05]\n",
      " [-2.31384130e-01]\n",
      " [-4.64074052e-01]\n",
      " ...\n",
      " [-3.31255296e-01]\n",
      " [ 4.54212077e-02]\n",
      " [-2.34854361e-01]]\n",
      "t [[-0.00941791]\n",
      " [-0.31830118]\n",
      " [-0.53937254]\n",
      " ...\n",
      " [-0.37794033]\n",
      " [ 0.0357511 ]\n",
      " [-0.28966985]]\n",
      "t [[-0.00941791]\n",
      " [-0.31830118]\n",
      " [-0.53937254]\n",
      " ...\n",
      " [-0.37794033]\n",
      " [ 0.0357511 ]\n",
      " [-0.28966985]]\n",
      "Current iteration=4, loss=36516.21243243765\n",
      "t [[-0.01908718]\n",
      " [-0.40186091]\n",
      " [-0.60111503]\n",
      " ...\n",
      " [-0.41469726]\n",
      " [ 0.02112154]\n",
      " [-0.33746419]]\n",
      "t [[-0.01908718]\n",
      " [-0.40186091]\n",
      " [-0.60111503]\n",
      " ...\n",
      " [-0.41469726]\n",
      " [ 0.02112154]\n",
      " [-0.33746419]]\n",
      "t [[-0.02838747]\n",
      " [-0.48096246]\n",
      " [-0.65443347]\n",
      " ...\n",
      " [-0.44575653]\n",
      " [ 0.00330399]\n",
      " [-0.37960199]]\n",
      "t [[-0.02838747]\n",
      " [-0.48096246]\n",
      " [-0.65443347]\n",
      " ...\n",
      " [-0.44575653]\n",
      " [ 0.00330399]\n",
      " [-0.37960199]]\n",
      "Current iteration=6, loss=35726.49646112175\n",
      "t [[-0.03701655]\n",
      " [-0.55528928]\n",
      " [-0.70212614]\n",
      " ...\n",
      " [-0.47336784]\n",
      " [-0.01656502]\n",
      " [-0.41705512]]\n",
      "t [[-0.03701655]\n",
      " [-0.55528928]\n",
      " [-0.70212614]\n",
      " ...\n",
      " [-0.47336784]\n",
      " [-0.01656502]\n",
      " [-0.41705512]]\n",
      "t [[-0.04486836]\n",
      " [-0.62489898]\n",
      " [-0.74576644]\n",
      " ...\n",
      " [-0.49874574]\n",
      " [-0.03771707]\n",
      " [-0.45055129]]\n",
      "t [[-0.04486836]\n",
      " [-0.62489898]\n",
      " [-0.74576644]\n",
      " ...\n",
      " [-0.49874574]\n",
      " [-0.03771707]\n",
      " [-0.45055129]]\n",
      "Current iteration=8, loss=35172.537379348825\n",
      "t [[-0.05193353]\n",
      " [-0.69002008]\n",
      " [-0.78626788]\n",
      " ...\n",
      " [-0.52255256]\n",
      " [-0.05961293]\n",
      " [-0.48065594]]\n",
      "t [[-0.05193353]\n",
      " [-0.69002008]\n",
      " [-0.78626788]\n",
      " ...\n",
      " [-0.52255256]\n",
      " [-0.05961293]\n",
      " [-0.48065594]]\n",
      "t [[-0.05824832]\n",
      " [-0.75094955]\n",
      " [-0.82418211]\n",
      " ...\n",
      " [-0.54515304]\n",
      " [-0.08186449]\n",
      " [-0.50782084]]\n",
      "loss=34765.65637424866\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00816817]\n",
      " [-0.06572386]\n",
      " [-0.22257957]\n",
      " ...\n",
      " [-0.09799606]\n",
      " [-0.0118024 ]\n",
      " [-0.19992783]]\n",
      "t [[ 0.00816817]\n",
      " [-0.06572386]\n",
      " [-0.22257957]\n",
      " ...\n",
      " [-0.09799606]\n",
      " [-0.0118024 ]\n",
      " [-0.19992783]]\n",
      "t [[ 0.00438476]\n",
      " [-0.15105357]\n",
      " [-0.36182609]\n",
      " ...\n",
      " [-0.14258634]\n",
      " [-0.00593099]\n",
      " [-0.36865875]]\n",
      "t [[ 0.00438476]\n",
      " [-0.15105357]\n",
      " [-0.36182609]\n",
      " ...\n",
      " [-0.14258634]\n",
      " [-0.00593099]\n",
      " [-0.36865875]]\n",
      "Current iteration=2, loss=37698.00845366477\n",
      "t [[-0.00430379]\n",
      " [-0.24187642]\n",
      " [-0.45813091]\n",
      " ...\n",
      " [-0.16208958]\n",
      " [ 0.00620502]\n",
      " [-0.51502242]]\n",
      "t [[-0.00430379]\n",
      " [-0.24187642]\n",
      " [-0.45813091]\n",
      " ...\n",
      " [-0.16208958]\n",
      " [ 0.00620502]\n",
      " [-0.51502242]]\n",
      "t [[-0.0146606 ]\n",
      " [-0.33186084]\n",
      " [-0.53115787]\n",
      " ...\n",
      " [-0.16995692]\n",
      " [ 0.01952696]\n",
      " [-0.6444343 ]]\n",
      "t [[-0.0146606 ]\n",
      " [-0.33186084]\n",
      " [-0.53115787]\n",
      " ...\n",
      " [-0.16995692]\n",
      " [ 0.01952696]\n",
      " [-0.6444343 ]]\n",
      "Current iteration=4, loss=36450.872697409344\n",
      "t [[-0.025207  ]\n",
      " [-0.4182697 ]\n",
      " [-0.59074484]\n",
      " ...\n",
      " [-0.17264302]\n",
      " [ 0.0318785 ]\n",
      " [-0.76040335]]\n",
      "t [[-0.025207  ]\n",
      " [-0.4182697 ]\n",
      " [-0.59074484]\n",
      " ...\n",
      " [-0.17264302]\n",
      " [ 0.0318785 ]\n",
      " [-0.76040335]]\n",
      "t [[-0.03527265]\n",
      " [-0.50001716]\n",
      " [-0.64205118]\n",
      " ...\n",
      " [-0.17330481]\n",
      " [ 0.04244291]\n",
      " [-0.86536323]]\n",
      "t [[-0.03527265]\n",
      " [-0.50001716]\n",
      " [-0.64205118]\n",
      " ...\n",
      " [-0.17330481]\n",
      " [ 0.04244291]\n",
      " [-0.86536323]]\n",
      "Current iteration=6, loss=35646.059229220315\n",
      "t [[-0.04457657]\n",
      " [-0.5768021 ]\n",
      " [-0.68788129]\n",
      " ...\n",
      " [-0.1734714 ]\n",
      " [ 0.05102828]\n",
      " [-0.96109519]]\n",
      "t [[-0.04457657]\n",
      " [-0.5768021 ]\n",
      " [-0.68788129]\n",
      " ...\n",
      " [-0.1734714 ]\n",
      " [ 0.05102828]\n",
      " [-0.96109519]]\n",
      "t [[-0.05302914]\n",
      " [-0.64869728]\n",
      " [-0.72980466]\n",
      " ...\n",
      " [-0.17384673]\n",
      " [ 0.05772684]\n",
      " [-1.04895697]]\n",
      "t [[-0.05302914]\n",
      " [-0.64869728]\n",
      " [-0.72980466]\n",
      " ...\n",
      " [-0.17384673]\n",
      " [ 0.05772684]\n",
      " [-1.04895697]]\n",
      "Current iteration=8, loss=35080.98643462374\n",
      "t [[-0.06063373]\n",
      " [-0.71594574]\n",
      " [-0.76872624]\n",
      " ...\n",
      " [-0.17471466]\n",
      " [ 0.06274744]\n",
      " [-1.13001582]]\n",
      "t [[-0.06063373]\n",
      " [-0.71594574]\n",
      " [-0.76872624]\n",
      " ...\n",
      " [-0.17471466]\n",
      " [ 0.06274744]\n",
      " [-1.13001582]]\n",
      "t [[-0.06743647]\n",
      " [-0.77885808]\n",
      " [-0.80518725]\n",
      " ...\n",
      " [-0.17614894]\n",
      " [ 0.06633327]\n",
      " [-1.20513077]]\n",
      "loss=34665.85466809606\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.08980002]\n",
      " [-0.38361538]\n",
      " [-0.24111607]\n",
      " ...\n",
      " [-0.17474938]\n",
      " [ 0.0378342 ]\n",
      " [-0.1018697 ]]\n",
      "t [[ 0.08980002]\n",
      " [-0.38361538]\n",
      " [-0.24111607]\n",
      " ...\n",
      " [-0.17474938]\n",
      " [ 0.0378342 ]\n",
      " [-0.1018697 ]]\n",
      "t [[ 0.14158391]\n",
      " [-0.63674327]\n",
      " [-0.39647669]\n",
      " ...\n",
      " [-0.27775621]\n",
      " [ 0.04987075]\n",
      " [-0.18401219]]\n",
      "t [[ 0.14158391]\n",
      " [-0.63674327]\n",
      " [-0.39647669]\n",
      " ...\n",
      " [-0.27775621]\n",
      " [ 0.04987075]\n",
      " [-0.18401219]]\n",
      "Current iteration=2, loss=37687.98940665784\n",
      "t [[ 0.17269231]\n",
      " [-0.81870066]\n",
      " [-0.50722231]\n",
      " ...\n",
      " [-0.34517924]\n",
      " [ 0.04769655]\n",
      " [-0.25283831]]\n",
      "t [[ 0.17269231]\n",
      " [-0.81870066]\n",
      " [-0.50722231]\n",
      " ...\n",
      " [-0.34517924]\n",
      " [ 0.04769655]\n",
      " [-0.25283831]]\n",
      "t [[ 0.19197288]\n",
      " [-0.95869886]\n",
      " [-0.59309866]\n",
      " ...\n",
      " [-0.39420417]\n",
      " [ 0.0371653 ]\n",
      " [-0.31194654]]\n",
      "t [[ 0.19197288]\n",
      " [-0.95869886]\n",
      " [-0.59309866]\n",
      " ...\n",
      " [-0.39420417]\n",
      " [ 0.0371653 ]\n",
      " [-0.31194654]]\n",
      "Current iteration=4, loss=36453.44687292146\n",
      "t [[ 0.20413007]\n",
      " [-1.0718672 ]\n",
      " [-0.66395334]\n",
      " ...\n",
      " [-0.43321997]\n",
      " [ 0.02145188]\n",
      " [-0.36352198]]\n",
      "t [[ 0.20413007]\n",
      " [-1.0718672 ]\n",
      " [-0.66395334]\n",
      " ...\n",
      " [-0.43321997]\n",
      " [ 0.02145188]\n",
      " [-0.36352198]]\n",
      "t [[ 0.21181672]\n",
      " [-1.16663918]\n",
      " [-0.72499421]\n",
      " ...\n",
      " [-0.46649528]\n",
      " [ 0.00244854]\n",
      " [-0.40901802]]\n",
      "t [[ 0.21181672]\n",
      " [-1.16663918]\n",
      " [-0.72499421]\n",
      " ...\n",
      " [-0.46649528]\n",
      " [ 0.00244854]\n",
      " [-0.40901802]]\n",
      "Current iteration=6, loss=35662.861463350055\n",
      "t [[ 0.21660233]\n",
      " [-1.24802624]\n",
      " [-0.77911355]\n",
      " ...\n",
      " [-0.49626085]\n",
      " [-0.01863282]\n",
      " [-0.44947118]]\n",
      "t [[ 0.21660233]\n",
      " [-1.24802624]\n",
      " [-0.77911355]\n",
      " ...\n",
      " [-0.49626085]\n",
      " [-0.01863282]\n",
      " [-0.44947118]]\n",
      "t [[ 0.2194536 ]\n",
      " [-1.31918204]\n",
      " [-0.82799277]\n",
      " ...\n",
      " [-0.52370198]\n",
      " [-0.04097565]\n",
      " [-0.48565917]]\n",
      "t [[ 0.2194536 ]\n",
      " [-1.31918204]\n",
      " [-0.82799277]\n",
      " ...\n",
      " [-0.52370198]\n",
      " [-0.04097565]\n",
      " [-0.48565917]]\n",
      "Current iteration=8, loss=35111.27093489687\n",
      "t [[ 0.22098754]\n",
      " [-1.38220079]\n",
      " [-0.87265887]\n",
      " ...\n",
      " [-0.54945694]\n",
      " [-0.06401026]\n",
      " [-0.5181877 ]]\n",
      "t [[ 0.22098754]\n",
      " [-1.38220079]\n",
      " [-0.87265887]\n",
      " ...\n",
      " [-0.54945694]\n",
      " [-0.06401026]\n",
      " [-0.5181877 ]]\n",
      "t [[ 0.22161096]\n",
      " [-1.43854516]\n",
      " [-0.91377558]\n",
      " ...\n",
      " [-0.57387506]\n",
      " [-0.08733007]\n",
      " [-0.54754174]]\n",
      "loss=34707.85547826743\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01184225]\n",
      " [-0.07291117]\n",
      " [-0.22858414]\n",
      " ...\n",
      " [-0.17541051]\n",
      " [ 0.034101  ]\n",
      " [-0.1040613 ]]\n",
      "t [[ 0.01184225]\n",
      " [-0.07291117]\n",
      " [-0.22858414]\n",
      " ...\n",
      " [-0.17541051]\n",
      " [ 0.034101  ]\n",
      " [-0.1040613 ]]\n",
      "t [[ 0.01060382]\n",
      " [-0.16586125]\n",
      " [-0.36916271]\n",
      " ...\n",
      " [-0.27754439]\n",
      " [ 0.04302065]\n",
      " [-0.18795098]]\n",
      "t [[ 0.01060382]\n",
      " [-0.16586125]\n",
      " [-0.36916271]\n",
      " ...\n",
      " [-0.27754439]\n",
      " [ 0.04302065]\n",
      " [-0.18795098]]\n",
      "Current iteration=2, loss=37656.69124930094\n",
      "t [[ 0.00398575]\n",
      " [-0.26356324]\n",
      " [-0.46578169]\n",
      " ...\n",
      " [-0.34370878]\n",
      " [ 0.03837241]\n",
      " [-0.25827853]]\n",
      "t [[ 0.00398575]\n",
      " [-0.26356324]\n",
      " [-0.46578169]\n",
      " ...\n",
      " [-0.34370878]\n",
      " [ 0.03837241]\n",
      " [-0.25827853]]\n",
      "t [[-0.00457391]\n",
      " [-0.35947743]\n",
      " [-0.53912657]\n",
      " ...\n",
      " [-0.39143393]\n",
      " [ 0.02591262]\n",
      " [-0.31872111]]\n",
      "t [[-0.00457391]\n",
      " [-0.35947743]\n",
      " [-0.53912657]\n",
      " ...\n",
      " [-0.39143393]\n",
      " [ 0.02591262]\n",
      " [-0.31872111]]\n",
      "Current iteration=4, loss=36401.044961911786\n",
      "t [[-0.01352989]\n",
      " [-0.45093316]\n",
      " [-0.59925146]\n",
      " ...\n",
      " [-0.42920426]\n",
      " [ 0.00871123]\n",
      " [-0.37150171]]\n",
      "t [[-0.01352989]\n",
      " [-0.45093316]\n",
      " [-0.59925146]\n",
      " ...\n",
      " [-0.42920426]\n",
      " [ 0.00871123]\n",
      " [-0.37150171]]\n",
      "t [[-0.0221846 ]\n",
      " [-0.53697382]\n",
      " [-0.65129817]\n",
      " ...\n",
      " [-0.46130773]\n",
      " [-0.01142939]\n",
      " [-0.41809637]]\n",
      "t [[-0.0221846 ]\n",
      " [-0.53697382]\n",
      " [-0.65129817]\n",
      " ...\n",
      " [-0.46130773]\n",
      " [-0.01142939]\n",
      " [-0.41809637]]\n",
      "Current iteration=6, loss=35595.113521375075\n",
      "t [[-0.03024106]\n",
      " [-0.61742704]\n",
      " [-0.69799776]\n",
      " ...\n",
      " [-0.48997199]\n",
      " [-0.03337036]\n",
      " [-0.45955726]]\n",
      "t [[-0.03024106]\n",
      " [-0.61742704]\n",
      " [-0.69799776]\n",
      " ...\n",
      " [-0.48997199]\n",
      " [-0.03337036]\n",
      " [-0.45955726]]\n",
      "t [[-0.03759626]\n",
      " [-0.69247601]\n",
      " [-0.74084774]\n",
      " ...\n",
      " [-0.51637459]\n",
      " [-0.05635351]\n",
      " [-0.49667401]]\n",
      "t [[-0.03759626]\n",
      " [-0.69247601]\n",
      " [-0.74084774]\n",
      " ...\n",
      " [-0.51637459]\n",
      " [-0.05635351]\n",
      " [-0.49667401]]\n",
      "Current iteration=8, loss=35032.95232938189\n",
      "t [[-0.04424032]\n",
      " [-0.76245329]\n",
      " [-0.78069803]\n",
      " ...\n",
      " [-0.54114671]\n",
      " [-0.07985562]\n",
      " [-0.53006187]]\n",
      "t [[-0.04424032]\n",
      " [-0.76245329]\n",
      " [-0.78069803]\n",
      " ...\n",
      " [-0.54114671]\n",
      " [-0.07985562]\n",
      " [-0.53006187]]\n",
      "t [[-0.05020613]\n",
      " [-0.82774057]\n",
      " [-0.8180527 ]\n",
      " ...\n",
      " [-0.56463261]\n",
      " [-0.103507  ]\n",
      " [-0.56021369]]\n",
      "loss=34622.257160478344\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0101146 ]\n",
      " [-0.06399121]\n",
      " [-0.23105978]\n",
      " ...\n",
      " [-0.17247709]\n",
      " [ 0.03666523]\n",
      " [-0.09760994]]\n",
      "t [[ 0.0101146 ]\n",
      " [-0.06399121]\n",
      " [-0.23105978]\n",
      " ...\n",
      " [-0.17247709]\n",
      " [ 0.03666523]\n",
      " [-0.09760994]]\n",
      "t [[ 0.00727504]\n",
      " [-0.1489725 ]\n",
      " [-0.37427602]\n",
      " ...\n",
      " [-0.2726676 ]\n",
      " [ 0.04788978]\n",
      " [-0.17573053]]\n",
      "t [[ 0.00727504]\n",
      " [-0.1489725 ]\n",
      " [-0.37427602]\n",
      " ...\n",
      " [-0.2726676 ]\n",
      " [ 0.04788978]\n",
      " [-0.17573053]]\n",
      "Current iteration=2, loss=37687.877358853\n",
      "t [[-0.00077509]\n",
      " [-0.23958195]\n",
      " [-0.47329897]\n",
      " ...\n",
      " [-0.33731207]\n",
      " [ 0.04525602]\n",
      " [-0.24082985]]\n",
      "t [[-0.00077509]\n",
      " [-0.23958195]\n",
      " [-0.47329897]\n",
      " ...\n",
      " [-0.33731207]\n",
      " [ 0.04525602]\n",
      " [-0.24082985]]\n",
      "t [[-0.01057808]\n",
      " [-0.32914965]\n",
      " [-0.54875672]\n",
      " ...\n",
      " [-0.3837566 ]\n",
      " [ 0.03453588]\n",
      " [-0.29649822]]\n",
      "t [[-0.01057808]\n",
      " [-0.32914965]\n",
      " [-0.54875672]\n",
      " ...\n",
      " [-0.3837566 ]\n",
      " [ 0.03453588]\n",
      " [-0.29649822]]\n",
      "Current iteration=4, loss=36453.648758222065\n",
      "t [[-0.02058806]\n",
      " [-0.41488982]\n",
      " [-0.61072815]\n",
      " ...\n",
      " [-0.42041961]\n",
      " [ 0.0188238 ]\n",
      " [-0.3448979 ]]\n",
      "t [[-0.02058806]\n",
      " [-0.41488982]\n",
      " [-0.61072815]\n",
      " ...\n",
      " [-0.42041961]\n",
      " [ 0.0188238 ]\n",
      " [-0.3448979 ]]\n",
      "t [[-3.01181385e-02]\n",
      " [-4.95752918e-01]\n",
      " [-6.64393658e-01]\n",
      " ...\n",
      " [-4.51560389e-01]\n",
      " [-5.21645852e-05]\n",
      " [-3.87458303e-01]]\n",
      "t [[-3.01181385e-02]\n",
      " [-4.95752918e-01]\n",
      " [-6.64393658e-01]\n",
      " ...\n",
      " [-4.51560389e-01]\n",
      " [-5.21645852e-05]\n",
      " [-3.87458303e-01]]\n",
      "Current iteration=6, loss=35663.3321424219\n",
      "t [[-0.03888746]\n",
      " [-0.57149189]\n",
      " [-0.71252033]\n",
      " ...\n",
      " [-0.47939026]\n",
      " [-0.02092773]\n",
      " [-0.42519374]]\n",
      "t [[-0.03888746]\n",
      " [-0.57149189]\n",
      " [-0.71252033]\n",
      " ...\n",
      " [-0.47939026]\n",
      " [-0.02092773]\n",
      " [-0.42519374]]\n",
      "t [[-0.04681103]\n",
      " [-0.64222919]\n",
      " [-0.75663593]\n",
      " ...\n",
      " [-0.50507435]\n",
      " [-0.04302013]\n",
      " [-0.45886204]]\n",
      "t [[-0.04681103]\n",
      " [-0.64222919]\n",
      " [-0.75663593]\n",
      " ...\n",
      " [-0.50507435]\n",
      " [-0.04302013]\n",
      " [-0.45886204]]\n",
      "Current iteration=8, loss=35112.34637265063\n",
      "t [[-0.05389691]\n",
      " [-0.70824741]\n",
      " [-0.79761494]\n",
      " ...\n",
      " [-0.52923275]\n",
      " [-0.06578292]\n",
      " [-0.48905118]]\n",
      "t [[-0.05389691]\n",
      " [-0.70824741]\n",
      " [-0.79761494]\n",
      " ...\n",
      " [-0.52923275]\n",
      " [-0.06578292]\n",
      " [-0.48905118]]\n",
      "t [[-0.06019486]\n",
      " [-0.7698866 ]\n",
      " [-0.83598125]\n",
      " ...\n",
      " [-0.55219903]\n",
      " [-0.08882501]\n",
      " [-0.51623037]]\n",
      "loss=34709.67635745494\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00842342]\n",
      " [-0.06777773]\n",
      " [-0.22953518]\n",
      " ...\n",
      " [-0.10105844]\n",
      " [-0.01217122]\n",
      " [-0.20617558]]\n",
      "t [[ 0.00842342]\n",
      " [-0.06777773]\n",
      " [-0.22953518]\n",
      " ...\n",
      " [-0.10105844]\n",
      " [-0.01217122]\n",
      " [-0.20617558]]\n",
      "t [[ 0.00414936]\n",
      " [-0.15638644]\n",
      " [-0.37051097]\n",
      " ...\n",
      " [-0.14536179]\n",
      " [-0.0055592 ]\n",
      " [-0.37919017]]\n",
      "t [[ 0.00414936]\n",
      " [-0.15638644]\n",
      " [-0.37051097]\n",
      " ...\n",
      " [-0.14536179]\n",
      " [-0.0055592 ]\n",
      " [-0.37919017]]\n",
      "Current iteration=2, loss=37643.029630246456\n",
      "t [[-0.00511294]\n",
      " [-0.25038202]\n",
      " [-0.4671407 ]\n",
      " ...\n",
      " [-0.16390858]\n",
      " [ 0.00734629]\n",
      " [-0.52871914]]\n",
      "t [[-0.00511294]\n",
      " [-0.25038202]\n",
      " [-0.4671407 ]\n",
      " ...\n",
      " [-0.16390858]\n",
      " [ 0.00734629]\n",
      " [-0.52871914]]\n",
      "t [[-0.01594726]\n",
      " [-0.34309129]\n",
      " [-0.54025979]\n",
      " ...\n",
      " [-0.17093026]\n",
      " [ 0.02119365]\n",
      " [-0.66057391]]\n",
      "t [[-0.01594726]\n",
      " [-0.34309129]\n",
      " [-0.54025979]\n",
      " ...\n",
      " [-0.17093026]\n",
      " [ 0.02119365]\n",
      " [-0.66057391]]\n",
      "Current iteration=4, loss=36387.166175791564\n",
      "t [[-0.02684417]\n",
      " [-0.43174229]\n",
      " [-0.60002287]\n",
      " ...\n",
      " [-0.17305458]\n",
      " [ 0.03380583]\n",
      " [-0.77847387]]\n",
      "t [[-0.02684417]\n",
      " [-0.43174229]\n",
      " [-0.60002287]\n",
      " ...\n",
      " [-0.17305458]\n",
      " [ 0.03380583]\n",
      " [-0.77847387]]\n",
      "t [[-0.03714583]\n",
      " [-0.51530214]\n",
      " [-0.65163597]\n",
      " ...\n",
      " [-0.17342554]\n",
      " [ 0.04441598]\n",
      " [-0.88498065]]\n",
      "t [[-0.03714583]\n",
      " [-0.51530214]\n",
      " [-0.65163597]\n",
      " ...\n",
      " [-0.17342554]\n",
      " [ 0.04441598]\n",
      " [-0.88498065]]\n",
      "Current iteration=6, loss=35581.65146415126\n",
      "t [[-0.04659388]\n",
      " [-0.59354067]\n",
      " [-0.69787021]\n",
      " ...\n",
      " [-0.17351028]\n",
      " [ 0.05289363]\n",
      " [-0.98196114]]\n",
      "t [[-0.04659388]\n",
      " [-0.59354067]\n",
      " [-0.69787021]\n",
      " ...\n",
      " [-0.17351028]\n",
      " [ 0.05289363]\n",
      " [-0.98196114]]\n",
      "t [[-0.05512057]\n",
      " [-0.66659687]\n",
      " [-0.74024746]\n",
      " ...\n",
      " [-0.17394825]\n",
      " [ 0.05938489]\n",
      " [-1.07083422]]\n",
      "t [[-0.05512057]\n",
      " [-0.66659687]\n",
      " [-0.74024746]\n",
      " ...\n",
      " [-0.17394825]\n",
      " [ 0.05938489]\n",
      " [-1.07083422]]\n",
      "Current iteration=8, loss=35019.57408720187\n",
      "t [[-0.06274721]\n",
      " [-0.73476912]\n",
      " [-0.7796319 ]\n",
      " ...\n",
      " [-0.17497034]\n",
      " [ 0.0641404 ]\n",
      " [-1.15271271]]\n",
      "t [[-0.06274721]\n",
      " [-0.73476912]\n",
      " [-0.7796319 ]\n",
      " ...\n",
      " [-0.17497034]\n",
      " [ 0.0641404 ]\n",
      " [-1.15271271]]\n",
      "t [[-0.06953372]\n",
      " [-0.79841225]\n",
      " [-0.8165356 ]\n",
      " ...\n",
      " [-0.17661103]\n",
      " [ 0.06743369]\n",
      " [-1.22849065]]\n",
      "loss=34608.74183459111\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.09252123]\n",
      " [-0.39524009]\n",
      " [-0.24842262]\n",
      " ...\n",
      " [-0.18004482]\n",
      " [ 0.03898069]\n",
      " [-0.10495666]]\n",
      "t [[ 0.09252123]\n",
      " [-0.39524009]\n",
      " [-0.24842262]\n",
      " ...\n",
      " [-0.18004482]\n",
      " [ 0.03898069]\n",
      " [-0.10495666]]\n",
      "t [[ 0.14471539]\n",
      " [-0.65206357]\n",
      " [-0.40588025]\n",
      " ...\n",
      " [-0.28398725]\n",
      " [ 0.0506002 ]\n",
      " [-0.18898391]]\n",
      "t [[ 0.14471539]\n",
      " [-0.65206357]\n",
      " [-0.40588025]\n",
      " ...\n",
      " [-0.28398725]\n",
      " [ 0.0506002 ]\n",
      " [-0.18898391]]\n",
      "Current iteration=2, loss=37634.932116191056\n",
      "t [[ 0.17551564]\n",
      " [-0.83521805]\n",
      " [-0.51727772]\n",
      " ...\n",
      " [-0.35129844]\n",
      " [ 0.04750025]\n",
      " [-0.25908218]]\n",
      "t [[ 0.17551564]\n",
      " [-0.83521805]\n",
      " [-0.51727772]\n",
      " ...\n",
      " [-0.35129844]\n",
      " [ 0.04750025]\n",
      " [-0.25908218]]\n",
      "t [[ 0.194306  ]\n",
      " [-0.9756248 ]\n",
      " [-0.60348551]\n",
      " ...\n",
      " [-0.40013236]\n",
      " [ 0.03588531]\n",
      " [-0.31908871]]\n",
      "t [[ 0.194306  ]\n",
      " [-0.9756248 ]\n",
      " [-0.60348551]\n",
      " ...\n",
      " [-0.40013236]\n",
      " [ 0.03588531]\n",
      " [-0.31908871]]\n",
      "Current iteration=4, loss=36392.47561872305\n",
      "t [[ 0.20596997]\n",
      " [-1.08895091]\n",
      " [-0.67465702]\n",
      " ...\n",
      " [-0.4391148 ]\n",
      " [ 0.01906215]\n",
      " [-0.37130364]]\n",
      "t [[ 0.20596997]\n",
      " [-1.08895091]\n",
      " [-0.67465702]\n",
      " ...\n",
      " [-0.4391148 ]\n",
      " [ 0.01906215]\n",
      " [-0.37130364]]\n",
      "t [[ 2.13215544e-01]\n",
      " [-1.18379263e+00]\n",
      " [-7.36054013e-01]\n",
      " ...\n",
      " [-4.72528772e-01]\n",
      " [-1.01891480e-03]\n",
      " [-4.17247153e-01]]\n",
      "t [[ 2.13215544e-01]\n",
      " [-1.18379263e+00]\n",
      " [-7.36054013e-01]\n",
      " ...\n",
      " [-4.72528772e-01]\n",
      " [-1.01891480e-03]\n",
      " [-4.17247153e-01]]\n",
      "Current iteration=6, loss=35601.661753407745\n",
      "t [[ 0.21762305]\n",
      " [-1.26520117]\n",
      " [-0.79054965]\n",
      " ...\n",
      " [-0.50255917]\n",
      " [-0.02311776]\n",
      " [-0.45799976]]\n",
      "t [[ 0.21762305]\n",
      " [-1.26520117]\n",
      " [-0.79054965]\n",
      " ...\n",
      " [-0.50255917]\n",
      " [-0.02311776]\n",
      " [-0.45799976]]\n",
      "t [[ 0.22015514]\n",
      " [-1.33633354]\n",
      " [-0.83979322]\n",
      " ...\n",
      " [-0.53033925]\n",
      " [-0.04640371]\n",
      " [-0.49437074]]\n",
      "t [[ 0.22015514]\n",
      " [-1.33633354]\n",
      " [-0.83979322]\n",
      " ...\n",
      " [-0.53033925]\n",
      " [-0.04640371]\n",
      " [-0.49437074]]\n",
      "Current iteration=8, loss=35053.235223119205\n",
      "t [[ 0.22142096]\n",
      " [-1.39928048]\n",
      " [-0.88478529]\n",
      " ...\n",
      " [-0.55646508]\n",
      " [-0.07030061]\n",
      " [-0.52698953]]\n",
      "t [[ 0.22142096]\n",
      " [-1.39928048]\n",
      " [-0.88478529]\n",
      " ...\n",
      " [-0.55646508]\n",
      " [-0.07030061]\n",
      " [-0.52698953]]\n",
      "t [[ 0.2218195 ]\n",
      " [-1.45550308]\n",
      " [-0.92617263]\n",
      " ...\n",
      " [-0.58125633]\n",
      " [-0.09439992]\n",
      " [-0.55635955]]\n",
      "loss=34654.08186752643\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01220111]\n",
      " [-0.0751206 ]\n",
      " [-0.23551093]\n",
      " ...\n",
      " [-0.18072598]\n",
      " [ 0.03513436]\n",
      " [-0.10721467]]\n",
      "t [[ 0.01220111]\n",
      " [-0.0751206 ]\n",
      " [-0.23551093]\n",
      " ...\n",
      " [-0.18072598]\n",
      " [ 0.03513436]\n",
      " [-0.10721467]]\n",
      "t [[ 0.01053107]\n",
      " [-0.17149223]\n",
      " [-0.37767058]\n",
      " ...\n",
      " [-0.28372328]\n",
      " [ 0.04356197]\n",
      " [-0.19302867]]\n",
      "t [[ 0.01053107]\n",
      " [-0.17149223]\n",
      " [-0.37767058]\n",
      " ...\n",
      " [-0.28372328]\n",
      " [ 0.04356197]\n",
      " [-0.19302867]]\n",
      "Current iteration=2, loss=37602.90057579577\n",
      "t [[ 0.00339078]\n",
      " [-0.27243381]\n",
      " [-0.47455427]\n",
      " ...\n",
      " [-0.34971446]\n",
      " [ 0.03795227]\n",
      " [-0.26465901]]\n",
      "t [[ 0.00339078]\n",
      " [-0.27243381]\n",
      " [-0.47455427]\n",
      " ...\n",
      " [-0.34971446]\n",
      " [ 0.03795227]\n",
      " [-0.26465901]]\n",
      "t [[-0.00560297]\n",
      " [-0.37108413]\n",
      " [-0.54799794]\n",
      " ...\n",
      " [-0.39720526]\n",
      " [ 0.02440024]\n",
      " [-0.32602515]]\n",
      "t [[-0.00560297]\n",
      " [-0.37108413]\n",
      " [-0.54799794]\n",
      " ...\n",
      " [-0.39720526]\n",
      " [ 0.02440024]\n",
      " [-0.32602515]]\n",
      "Current iteration=4, loss=36338.927846178165\n",
      "t [[-0.01487656]\n",
      " [-0.46476055]\n",
      " [-0.60833601]\n",
      " ...\n",
      " [-0.43491108]\n",
      " [ 0.00609779]\n",
      " [-0.3794662 ]]\n",
      "t [[-0.01487656]\n",
      " [-0.46476055]\n",
      " [-0.60833601]\n",
      " ...\n",
      " [-0.43491108]\n",
      " [ 0.00609779]\n",
      " [-0.3794662 ]]\n",
      "t [[-0.0237455 ]\n",
      " [-0.5525752 ]\n",
      " [-0.66073276]\n",
      " ...\n",
      " [-0.46712903]\n",
      " [-0.01510149]\n",
      " [-0.42652559]]\n",
      "t [[-0.0237455 ]\n",
      " [-0.5525752 ]\n",
      " [-0.66073276]\n",
      " ...\n",
      " [-0.46712903]\n",
      " [-0.01510149]\n",
      " [-0.42652559]]\n",
      "Current iteration=6, loss=35532.71475373376\n",
      "t [[-0.03193461]\n",
      " [-0.6344362 ]\n",
      " [-0.7078734 ]\n",
      " ...\n",
      " [-0.4960378 ]\n",
      " [-0.03803519]\n",
      " [-0.46830007]]\n",
      "t [[-0.03193461]\n",
      " [-0.6344362 ]\n",
      " [-0.7078734 ]\n",
      " ...\n",
      " [-0.4960378 ]\n",
      " [-0.03803519]\n",
      " [-0.46830007]]\n",
      "t [[-0.03936116]\n",
      " [-0.71059816]\n",
      " [-0.75120294]\n",
      " ...\n",
      " [-0.52276137]\n",
      " [-0.06193417]\n",
      " [-0.50561129]]\n",
      "t [[-0.03936116]\n",
      " [-0.71059816]\n",
      " [-0.75120294]\n",
      " ...\n",
      " [-0.52276137]\n",
      " [-0.06193417]\n",
      " [-0.50561129]]\n",
      "Current iteration=8, loss=34973.82334629886\n",
      "t [[-0.04603151]\n",
      " [-0.78145161]\n",
      " [-0.79152992]\n",
      " ...\n",
      " [-0.54788829]\n",
      " [-0.08627079]\n",
      " [-0.53909866]]\n",
      "t [[-0.04603151]\n",
      " [-0.78145161]\n",
      " [-0.79152992]\n",
      " ...\n",
      " [-0.54788829]\n",
      " [-0.08627079]\n",
      " [-0.53909866]]\n",
      "t [[-0.05199065]\n",
      " [-0.84742362]\n",
      " [-0.82933067]\n",
      " ...\n",
      " [-0.57173307]\n",
      " [-0.11067469]\n",
      " [-0.56927387]]\n",
      "loss=34567.54147414446\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0104211 ]\n",
      " [-0.06593034]\n",
      " [-0.23806159]\n",
      " ...\n",
      " [-0.17770367]\n",
      " [ 0.0377763 ]\n",
      " [-0.10056782]]\n",
      "t [[ 0.0104211 ]\n",
      " [-0.06593034]\n",
      " [-0.23806159]\n",
      " ...\n",
      " [-0.17770367]\n",
      " [ 0.0377763 ]\n",
      " [-0.10056782]]\n",
      "t [[ 0.00710507]\n",
      " [-0.15412189]\n",
      " [-0.38294247]\n",
      " ...\n",
      " [-0.27872833]\n",
      " [ 0.04856992]\n",
      " [-0.18045904]]\n",
      "t [[ 0.00710507]\n",
      " [-0.15412189]\n",
      " [-0.38294247]\n",
      " ...\n",
      " [-0.27872833]\n",
      " [ 0.04856992]\n",
      " [-0.18045904]]\n",
      "Current iteration=2, loss=37634.86083014487\n",
      "t [[-0.00150042]\n",
      " [-0.24780975]\n",
      " [-0.48228849]\n",
      " ...\n",
      " [-0.34317948]\n",
      " [ 0.04501754]\n",
      " [-0.2467358 ]]\n",
      "t [[-0.00150042]\n",
      " [-0.24780975]\n",
      " [-0.48228849]\n",
      " ...\n",
      " [-0.34317948]\n",
      " [ 0.04501754]\n",
      " [-0.2467358 ]]\n",
      "t [[-0.0117578 ]\n",
      " [-0.33998935]\n",
      " [-0.55788279]\n",
      " ...\n",
      " [-0.38937322]\n",
      " [ 0.03323247]\n",
      " [-0.30322452]]\n",
      "t [[-0.0117578 ]\n",
      " [-0.33998935]\n",
      " [-0.55788279]\n",
      " ...\n",
      " [-0.38937322]\n",
      " [ 0.03323247]\n",
      " [-0.30322452]]\n",
      "Current iteration=4, loss=36392.68161335867\n",
      "t [[-0.02209401]\n",
      " [-0.4278537 ]\n",
      " [-0.62009124]\n",
      " ...\n",
      " [-0.42595969]\n",
      " [ 0.01643373]\n",
      " [-0.35219971]]\n",
      "t [[-0.02209401]\n",
      " [-0.4278537 ]\n",
      " [-0.62009124]\n",
      " ...\n",
      " [-0.42595969]\n",
      " [ 0.01643373]\n",
      " [-0.35219971]]\n",
      "t [[-0.03183733]\n",
      " [-0.51041566]\n",
      " [-0.67412132]\n",
      " ...\n",
      " [-0.45720811]\n",
      " [-0.00349704]\n",
      " [-0.39515531]]\n",
      "t [[-0.03183733]\n",
      " [-0.51041566]\n",
      " [-0.67412132]\n",
      " ...\n",
      " [-0.45720811]\n",
      " [-0.00349704]\n",
      " [-0.39515531]]\n",
      "Current iteration=6, loss=35602.170910210785\n",
      "t [[-0.04073086]\n",
      " [-0.58750407]\n",
      " [-0.7226973 ]\n",
      " ...\n",
      " [-0.48528087]\n",
      " [-0.02536968]\n",
      " [-0.43314783]]\n",
      "t [[-0.04073086]\n",
      " [-0.58750407]\n",
      " [-0.7226973 ]\n",
      " ...\n",
      " [-0.48528087]\n",
      " [-0.02536968]\n",
      " [-0.43314783]]\n",
      "t [[-0.0487119 ]\n",
      " [-0.65930943]\n",
      " [-0.76729634]\n",
      " ...\n",
      " [-0.51128907]\n",
      " [-0.0483883 ]\n",
      " [-0.46696516]]\n",
      "t [[-0.0487119 ]\n",
      " [-0.65930943]\n",
      " [-0.76729634]\n",
      " ...\n",
      " [-0.51128907]\n",
      " [-0.0483883 ]\n",
      " [-0.46696516]]\n",
      "Current iteration=8, loss=35054.39453443545\n",
      "t [[-0.05580636]\n",
      " [-0.72616968]\n",
      " [-0.8087528 ]\n",
      " ...\n",
      " [-0.53580933]\n",
      " [-0.07200015]\n",
      " [-0.49721782]]\n",
      "t [[-0.05580636]\n",
      " [-0.72616968]\n",
      " [-0.8087528 ]\n",
      " ...\n",
      " [-0.53580933]\n",
      " [-0.07200015]\n",
      " [-0.49721782]]\n",
      "t [[-0.06207744]\n",
      " [-0.78846825]\n",
      " [-0.84756362]\n",
      " ...\n",
      " [-0.55914465]\n",
      " [-0.0958119 ]\n",
      " [-0.52439244]]\n",
      "loss=34656.01824591435\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00867868]\n",
      " [-0.0698316 ]\n",
      " [-0.23649079]\n",
      " ...\n",
      " [-0.10412082]\n",
      " [-0.01254005]\n",
      " [-0.21242332]]\n",
      "t [[ 0.00867868]\n",
      " [-0.0698316 ]\n",
      " [-0.23649079]\n",
      " ...\n",
      " [-0.10412082]\n",
      " [-0.01254005]\n",
      " [-0.21242332]]\n",
      "t [[ 0.00389252]\n",
      " [-0.16175468]\n",
      " [-0.37904268]\n",
      " ...\n",
      " [-0.1480391 ]\n",
      " [-0.00515477]\n",
      " [-0.38966314]]\n",
      "t [[ 0.00389252]\n",
      " [-0.16175468]\n",
      " [-0.37904268]\n",
      " ...\n",
      " [-0.1480391 ]\n",
      " [-0.00515477]\n",
      " [-0.38966314]]\n",
      "Current iteration=2, loss=37589.133155991505\n",
      "t [[-0.00594821]\n",
      " [-0.25891626]\n",
      " [-0.47591435]\n",
      " ...\n",
      " [-0.16559001]\n",
      " [ 0.00852184]\n",
      " [-0.54229109]]\n",
      "t [[-0.00594821]\n",
      " [-0.25891626]\n",
      " [-0.47591435]\n",
      " ...\n",
      " [-0.16559001]\n",
      " [ 0.00852184]\n",
      " [-0.54229109]]\n",
      "t [[-0.01725152]\n",
      " [-0.35431039]\n",
      " [-0.5491041 ]\n",
      " ...\n",
      " [-0.17177558]\n",
      " [ 0.0228729 ]\n",
      " [-0.67652422]]\n",
      "t [[-0.01725152]\n",
      " [-0.35431039]\n",
      " [-0.5491041 ]\n",
      " ...\n",
      " [-0.17177558]\n",
      " [ 0.0228729 ]\n",
      " [-0.67652422]]\n",
      "Current iteration=4, loss=36325.07856384701\n",
      "t [[-0.0284838 ]\n",
      " [-0.44514583]\n",
      " [-0.60905304]\n",
      " ...\n",
      " [-0.17337192]\n",
      " [ 0.03571414]\n",
      " [-0.79629407]]\n",
      "t [[-0.0284838 ]\n",
      " [-0.44514583]\n",
      " [-0.60905304]\n",
      " ...\n",
      " [-0.17337192]\n",
      " [ 0.03571414]\n",
      " [-0.79629407]]\n",
      "t [[-0.03900437]\n",
      " [-0.53045376]\n",
      " [-0.66099227]\n",
      " ...\n",
      " [-0.17349245]\n",
      " [ 0.04633805]\n",
      " [-0.90429052]]\n",
      "t [[-0.03900437]\n",
      " [-0.53045376]\n",
      " [-0.66099227]\n",
      " ...\n",
      " [-0.17349245]\n",
      " [ 0.04633805]\n",
      " [-0.90429052]]\n",
      "Current iteration=6, loss=35519.28004328702\n",
      "t [[-0.04858009]\n",
      " [-0.61008148]\n",
      " [-0.70764781]\n",
      " ...\n",
      " [-0.17353259]\n",
      " [ 0.05468071]\n",
      " [-1.00246616]]\n",
      "t [[-0.04858009]\n",
      " [-0.61008148]\n",
      " [-0.70764781]\n",
      " ...\n",
      " [-0.17353259]\n",
      " [ 0.05468071]\n",
      " [-1.00246616]]\n",
      "t [[-0.05716629]\n",
      " [-0.6842375 ]\n",
      " [-0.75048885]\n",
      " ...\n",
      " [-0.17406302]\n",
      " [ 0.0609442 ]\n",
      " [-1.09230097]]\n",
      "t [[-0.05716629]\n",
      " [-0.6842375 ]\n",
      " [-0.75048885]\n",
      " ...\n",
      " [-0.17406302]\n",
      " [ 0.0609442 ]\n",
      " [-1.09230097]]\n",
      "Current iteration=8, loss=34960.44413996303\n",
      "t [[-0.0648026 ]\n",
      " [-0.75327686]\n",
      " [-0.79033764]\n",
      " ...\n",
      " [-0.17526064]\n",
      " [ 0.06542123]\n",
      " [-1.17495328]]\n",
      "t [[-0.0648026 ]\n",
      " [-0.75327686]\n",
      " [-0.79033764]\n",
      " ...\n",
      " [-0.17526064]\n",
      " [ 0.06542123]\n",
      " [-1.17495328]]\n",
      "t [[-0.07156267]\n",
      " [-0.8175989 ]\n",
      " [-0.82767789]\n",
      " ...\n",
      " [-0.1771212 ]\n",
      " [ 0.06841504]\n",
      " [-1.25135202]]\n",
      "loss=34554.00023340741\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.09524244]\n",
      " [-0.40686479]\n",
      " [-0.25572917]\n",
      " ...\n",
      " [-0.18534026]\n",
      " [ 0.04012719]\n",
      " [-0.10804363]]\n",
      "t [[ 0.09524244]\n",
      " [-0.40686479]\n",
      " [-0.25572917]\n",
      " ...\n",
      " [-0.18534026]\n",
      " [ 0.04012719]\n",
      " [-0.10804363]]\n",
      "t [[ 0.14778117]\n",
      " [-0.66715873]\n",
      " [-0.41513606]\n",
      " ...\n",
      " [-0.29009444]\n",
      " [ 0.05128572]\n",
      " [-0.1939211 ]]\n",
      "t [[ 0.14778117]\n",
      " [-0.66715873]\n",
      " [-0.41513606]\n",
      " ...\n",
      " [-0.29009444]\n",
      " [ 0.05128572]\n",
      " [-0.1939211 ]]\n",
      "Current iteration=2, loss=37582.90582070749\n",
      "t [[ 0.17823238]\n",
      " [-0.85136648]\n",
      " [-0.52710219]\n",
      " ...\n",
      " [-0.35723371]\n",
      " [ 0.04723068]\n",
      " [-0.265256  ]]\n",
      "t [[ 0.17823238]\n",
      " [-0.85136648]\n",
      " [-0.52710219]\n",
      " ...\n",
      " [-0.35723371]\n",
      " [ 0.04723068]\n",
      " [-0.265256  ]]\n",
      "t [[ 0.1965169 ]\n",
      " [-0.99211194]\n",
      " [-0.61361271]\n",
      " ...\n",
      " [-0.4058692 ]\n",
      " [ 0.0345178 ]\n",
      " [-0.32612802]]\n",
      "t [[ 0.1965169 ]\n",
      " [-0.99211194]\n",
      " [-0.61361271]\n",
      " ...\n",
      " [-0.4058692 ]\n",
      " [ 0.0345178 ]\n",
      " [-0.32612802]]\n",
      "Current iteration=4, loss=36333.03538031356\n",
      "t [[ 0.20768753]\n",
      " [-1.1055661 ]\n",
      " [-0.68509895]\n",
      " ...\n",
      " [-0.44483631]\n",
      " [ 0.01658159]\n",
      " [-0.37895222]]\n",
      "t [[ 0.20768753]\n",
      " [-1.1055661 ]\n",
      " [-0.68509895]\n",
      " ...\n",
      " [-0.44483631]\n",
      " [ 0.01658159]\n",
      " [-0.37895222]]\n",
      "t [[ 0.21449984]\n",
      " [-1.20046462]\n",
      " [-0.74685755]\n",
      " ...\n",
      " [-0.47841373]\n",
      " [-0.00457256]\n",
      " [-0.42531522]]\n",
      "t [[ 0.21449984]\n",
      " [-1.20046462]\n",
      " [-0.74685755]\n",
      " ...\n",
      " [-0.47841373]\n",
      " [-0.00457256]\n",
      " [-0.42531522]]\n",
      "Current iteration=6, loss=35542.37539429333\n",
      "t [[ 0.2185404 ]\n",
      " [-1.28188609]\n",
      " [-0.8017327 ]\n",
      " ...\n",
      " [-0.50873064]\n",
      " [-0.02767821]\n",
      " [-0.46634147]]\n",
      "t [[ 0.2185404 ]\n",
      " [-1.28188609]\n",
      " [-0.8017327 ]\n",
      " ...\n",
      " [-0.50873064]\n",
      " [-0.02767821]\n",
      " [-0.46634147]]\n",
      "t [[ 0.22076538]\n",
      " [-1.35298564]\n",
      " [-0.85133777]\n",
      " ...\n",
      " [-0.53686427]\n",
      " [-0.05189194]\n",
      " [-0.50287174]]\n",
      "t [[ 0.22076538]\n",
      " [-1.35298564]\n",
      " [-0.85133777]\n",
      " ...\n",
      " [-0.53686427]\n",
      " [-0.05189194]\n",
      " [-0.50287174]]\n",
      "Current iteration=8, loss=34997.32515439944\n",
      "t [[ 0.22177497]\n",
      " [-1.41584915]\n",
      " [-0.89664637]\n",
      " ...\n",
      " [-0.56336786]\n",
      " [-0.07663209]\n",
      " [-0.53555925]]\n",
      "t [[ 0.22177497]\n",
      " [-1.41584915]\n",
      " [-0.89664637]\n",
      " ...\n",
      " [-0.56336786]\n",
      " [-0.07663209]\n",
      " [-0.53555925]]\n",
      "t [[ 0.22195991]\n",
      " [-1.47193651]\n",
      " [-0.93828932]\n",
      " ...\n",
      " [-0.58853227]\n",
      " [-0.10148896]\n",
      " [-0.56492584]]\n",
      "loss=34602.49988301808\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01255997]\n",
      " [-0.07733003]\n",
      " [-0.24243772]\n",
      " ...\n",
      " [-0.18604145]\n",
      " [ 0.03616773]\n",
      " [-0.11036804]]\n",
      "t [[ 0.01255997]\n",
      " [-0.07733003]\n",
      " [-0.24243772]\n",
      " ...\n",
      " [-0.18604145]\n",
      " [ 0.03616773]\n",
      " [-0.11036804]]\n",
      "t [[ 0.01043638]\n",
      " [-0.17715698]\n",
      " [-0.38602689]\n",
      " ...\n",
      " [-0.28977583]\n",
      " [ 0.04406051]\n",
      " [-0.19807108]]\n",
      "t [[ 0.01043638]\n",
      " [-0.17715698]\n",
      " [-0.38602689]\n",
      " ...\n",
      " [-0.28977583]\n",
      " [ 0.04406051]\n",
      " [-0.19807108]]\n",
      "Current iteration=2, loss=37550.144928796486\n",
      "t [[ 0.0027688 ]\n",
      " [-0.28132743]\n",
      " [-0.48309948]\n",
      " ...\n",
      " [-0.35553423]\n",
      " [ 0.03746226]\n",
      " [-0.27096817]]\n",
      "t [[ 0.0027688 ]\n",
      " [-0.28132743]\n",
      " [-0.48309948]\n",
      " ...\n",
      " [-0.35553423]\n",
      " [ 0.03746226]\n",
      " [-0.27096817]]\n",
      "t [[-0.00665156]\n",
      " [-0.38267038]\n",
      " [-0.55662664]\n",
      " ...\n",
      " [-0.40278478]\n",
      " [ 0.02280606]\n",
      " [-0.33322461]]\n",
      "t [[-0.00665156]\n",
      " [-0.38267038]\n",
      " [-0.55662664]\n",
      " ...\n",
      " [-0.40278478]\n",
      " [ 0.02280606]\n",
      " [-0.33322461]]\n",
      "Current iteration=4, loss=36278.36059537813\n",
      "t [[-0.01622935]\n",
      " [-0.47850777]\n",
      " [-0.61719078]\n",
      " ...\n",
      " [-0.44044555]\n",
      " [ 0.00340125]\n",
      " [-0.3872954 ]]\n",
      "t [[-0.01622935]\n",
      " [-0.47850777]\n",
      " [-0.61719078]\n",
      " ...\n",
      " [-0.44044555]\n",
      " [ 0.00340125]\n",
      " [-0.3872954 ]]\n",
      "t [[-0.02529756]\n",
      " [-0.56803162]\n",
      " [-0.66995721]\n",
      " ...\n",
      " [-0.47280378]\n",
      " [-0.01885055]\n",
      " [-0.43479104]]\n",
      "t [[-0.02529756]\n",
      " [-0.56803162]\n",
      " [-0.66995721]\n",
      " ...\n",
      " [-0.47280378]\n",
      " [-0.01885055]\n",
      " [-0.43479104]]\n",
      "Current iteration=6, loss=35472.26714487214\n",
      "t [[-0.03360515]\n",
      " [-0.65123665]\n",
      " [-0.71755441]\n",
      " ...\n",
      " [-0.50197939]\n",
      " [-0.04276533]\n",
      " [-0.47685276]]\n",
      "t [[-0.03360515]\n",
      " [-0.65123665]\n",
      " [-0.71755441]\n",
      " ...\n",
      " [-0.50197939]\n",
      " [-0.04276533]\n",
      " [-0.47685276]]\n",
      "t [[-0.04109068]\n",
      " [-0.72845179]\n",
      " [-0.76137082]\n",
      " ...\n",
      " [-0.52903903]\n",
      " [-0.0675643 ]\n",
      " [-0.51433422]]\n",
      "t [[-0.04109068]\n",
      " [-0.72845179]\n",
      " [-0.76137082]\n",
      " ...\n",
      " [-0.52903903]\n",
      " [-0.0675643 ]\n",
      " [-0.51433422]]\n",
      "Current iteration=8, loss=34916.867064691316\n",
      "t [[-0.04777691]\n",
      " [-0.8001265 ]\n",
      " [-0.80217321]\n",
      " ...\n",
      " [-0.55452807]\n",
      " [-0.09271629]\n",
      " [-0.54789906]]\n",
      "t [[-0.04777691]\n",
      " [-0.8001265 ]\n",
      " [-0.80217321]\n",
      " ...\n",
      " [-0.55452807]\n",
      " [-0.09271629]\n",
      " [-0.54789906]]\n",
      "t [[-0.05372086]\n",
      " [-0.86673329]\n",
      " [-0.84041139]\n",
      " ...\n",
      " [-0.57873225]\n",
      " [-0.11785105]\n",
      " [-0.57807774]]\n",
      "loss=34515.06707752927\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0107276 ]\n",
      " [-0.06786947]\n",
      " [-0.2450634 ]\n",
      " ...\n",
      " [-0.18293024]\n",
      " [ 0.03888736]\n",
      " [-0.1035257 ]]\n",
      "t [[ 0.0107276 ]\n",
      " [-0.06786947]\n",
      " [-0.2450634 ]\n",
      " ...\n",
      " [-0.18293024]\n",
      " [ 0.03888736]\n",
      " [-0.1035257 ]]\n",
      "t [[ 0.00691336]\n",
      " [-0.15930686]\n",
      " [-0.39145745]\n",
      " ...\n",
      " [-0.28466435]\n",
      " [ 0.04920672]\n",
      " [-0.18515349]]\n",
      "t [[ 0.00691336]\n",
      " [-0.15930686]\n",
      " [-0.39145745]\n",
      " ...\n",
      " [-0.28466435]\n",
      " [ 0.04920672]\n",
      " [-0.18515349]]\n",
      "Current iteration=2, loss=37582.873607542955\n",
      "t [[-0.00225191]\n",
      " [-0.25606526]\n",
      " [-0.49104935]\n",
      " ...\n",
      " [-0.34886322]\n",
      " [ 0.04470756]\n",
      " [-0.25257329]]\n",
      "t [[-0.00225191]\n",
      " [-0.25606526]\n",
      " [-0.49104935]\n",
      " ...\n",
      " [-0.34886322]\n",
      " [ 0.04470756]\n",
      " [-0.25257329]]\n",
      "t [[-0.01295504]\n",
      " [-0.3508164 ]\n",
      " [-0.56676322]\n",
      " ...\n",
      " [-0.39480064]\n",
      " [ 0.0318443 ]\n",
      " [-0.30985099]]\n",
      "t [[-0.01295504]\n",
      " [-0.3508164 ]\n",
      " [-0.56676322]\n",
      " ...\n",
      " [-0.39480064]\n",
      " [ 0.0318443 ]\n",
      " [-0.30985099]]\n",
      "Current iteration=4, loss=36333.245134449564\n",
      "t [[-0.0236027 ]\n",
      " [-0.4407484 ]\n",
      " [-0.62921997]\n",
      " ...\n",
      " [-0.43133056]\n",
      " [ 0.01395609]\n",
      " [-0.35937309]]\n",
      "t [[-0.0236027 ]\n",
      " [-0.4407484 ]\n",
      " [-0.62921997]\n",
      " ...\n",
      " [-0.43133056]\n",
      " [ 0.01395609]\n",
      " [-0.35937309]]\n",
      "t [[-0.03354288]\n",
      " [-0.52494749]\n",
      " [-0.68363291]\n",
      " ...\n",
      " [-0.46271318]\n",
      " [-0.00702486]\n",
      " [-0.40269759]]\n",
      "t [[-0.03354288]\n",
      " [-0.52494749]\n",
      " [-0.68363291]\n",
      " ...\n",
      " [-0.46271318]\n",
      " [-0.00702486]\n",
      " [-0.40269759]]\n",
      "Current iteration=6, loss=35542.926015185454\n",
      "t [[-0.04254518]\n",
      " [-0.6033245 ]\n",
      " [-0.7326726 ]\n",
      " ...\n",
      " [-0.49105209]\n",
      " [-0.02988434]\n",
      " [-0.4409231 ]]\n",
      "t [[-0.04254518]\n",
      " [-0.6033245 ]\n",
      " [-0.7326726 ]\n",
      " ...\n",
      " [-0.49105209]\n",
      " [-0.02988434]\n",
      " [-0.4409231 ]]\n",
      "t [[-0.05057018]\n",
      " [-0.6761408 ]\n",
      " [-0.77776151]\n",
      " ...\n",
      " [-0.51740043]\n",
      " [-0.05381455]\n",
      " [-0.4748675 ]]\n",
      "t [[-0.05057018]\n",
      " [-0.6761408 ]\n",
      " [-0.77776151]\n",
      " ...\n",
      " [-0.51740043]\n",
      " [-0.05381455]\n",
      " [-0.4748675 ]]\n",
      "Current iteration=8, loss=34998.57064336656\n",
      "t [[-0.05766194]\n",
      " [-0.74379067]\n",
      " [-0.8196934 ]\n",
      " ...\n",
      " [-0.54229078]\n",
      " [-0.07825727]\n",
      " [-0.50516382]]\n",
      "t [[-0.05766194]\n",
      " [-0.74379067]\n",
      " [-0.8196934 ]\n",
      " ...\n",
      " [-0.54229078]\n",
      " [-0.07825727]\n",
      " [-0.50516382]]\n",
      "t [[-0.06389696]\n",
      " [-0.80670095]\n",
      " [-0.85893948]\n",
      " ...\n",
      " [-0.56599646]\n",
      " [-0.10281766]\n",
      " [-0.53231606]]\n",
      "loss=34604.552094334525\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00893393]\n",
      " [-0.07188547]\n",
      " [-0.2434464 ]\n",
      " ...\n",
      " [-0.10718319]\n",
      " [-0.01290887]\n",
      " [-0.21867107]]\n",
      "t [[ 0.00893393]\n",
      " [-0.07188547]\n",
      " [-0.2434464 ]\n",
      " ...\n",
      " [-0.10718319]\n",
      " [-0.01290887]\n",
      " [-0.21867107]]\n",
      "t [[ 0.00361439]\n",
      " [-0.1671581 ]\n",
      " [-0.38742186]\n",
      " ...\n",
      " [-0.15061868]\n",
      " [-0.00471782]\n",
      " [-0.40007784]]\n",
      "t [[ 0.00361439]\n",
      " [-0.1671581 ]\n",
      " [-0.38742186]\n",
      " ...\n",
      " [-0.15061868]\n",
      " [-0.00471782]\n",
      " [-0.40007784]]\n",
      "Current iteration=2, loss=37536.28234344996\n",
      "t [[-0.00680846]\n",
      " [-0.26747688]\n",
      " [-0.4844587 ]\n",
      " ...\n",
      " [-0.16713859]\n",
      " [ 0.00972981]\n",
      " [-0.55573998]]\n",
      "t [[-0.00680846]\n",
      " [-0.26747688]\n",
      " [-0.4844587 ]\n",
      " ...\n",
      " [-0.16713859]\n",
      " [ 0.00972981]\n",
      " [-0.55573998]]\n",
      "t [[-0.01857138]\n",
      " [-0.36551429]\n",
      " [-0.55770335]\n",
      " ...\n",
      " [-0.1725014 ]\n",
      " [ 0.02456155]\n",
      " [-0.692289  ]]\n",
      "t [[-0.01857138]\n",
      " [-0.36551429]\n",
      " [-0.55770335]\n",
      " ...\n",
      " [-0.1725014 ]\n",
      " [ 0.02456155]\n",
      " [-0.692289  ]]\n",
      "Current iteration=4, loss=36264.54315515379\n",
      "t [[-0.0301236 ]\n",
      " [-0.45847618]\n",
      " [-0.61785111]\n",
      " ...\n",
      " [-0.17360529]\n",
      " [ 0.03760009]\n",
      " [-0.81386993]]\n",
      "t [[-0.0301236 ]\n",
      " [-0.45847618]\n",
      " [-0.61785111]\n",
      " ...\n",
      " [-0.17360529]\n",
      " [ 0.03760009]\n",
      " [-0.81386993]]\n",
      "t [[-0.0408462 ]\n",
      " [-0.54546887]\n",
      " [-0.6701366 ]\n",
      " ...\n",
      " [-0.1735155 ]\n",
      " [ 0.04820669]\n",
      " [-0.92330108]]\n",
      "t [[-0.0408462 ]\n",
      " [-0.54546887]\n",
      " [-0.6701366 ]\n",
      " ...\n",
      " [-0.1735155 ]\n",
      " [ 0.04820669]\n",
      " [-0.92330108]]\n",
      "Current iteration=6, loss=35458.85726686935\n",
      "t [[-0.05053376]\n",
      " [-0.6264233 ]\n",
      " [-0.71722967]\n",
      " ...\n",
      " [-0.17354658]\n",
      " [ 0.05638874]\n",
      " [-1.02262071]]\n",
      "t [[-0.05053376]\n",
      " [-0.6264233 ]\n",
      " [-0.71722967]\n",
      " ...\n",
      " [-0.17354658]\n",
      " [ 0.05638874]\n",
      " [-1.02262071]]\n",
      "t [[-0.05916568]\n",
      " [-0.7016204 ]\n",
      " [-0.76054261]\n",
      " ...\n",
      " [-0.17419689]\n",
      " [ 0.06240597]\n",
      " [-1.11336989]]\n",
      "t [[-0.05916568]\n",
      " [-0.7016204 ]\n",
      " [-0.76054261]\n",
      " ...\n",
      " [-0.17419689]\n",
      " [ 0.06240597]\n",
      " [-1.11336989]]\n",
      "Current iteration=8, loss=34903.48382839145\n",
      "t [[-0.06680016]\n",
      " [-0.77147292]\n",
      " [-0.80085526]\n",
      " ...\n",
      " [-0.17558881]\n",
      " [ 0.06659315]\n",
      " [-1.1967524 ]]\n",
      "t [[-0.06680016]\n",
      " [-0.77147292]\n",
      " [-0.80085526]\n",
      " ...\n",
      " [-0.17558881]\n",
      " [ 0.06659315]\n",
      " [-1.1967524 ]]\n",
      "t [[-0.07352442]\n",
      " [-0.83642478]\n",
      " [-0.83862414]\n",
      " ...\n",
      " [-0.17768031]\n",
      " [ 0.0692824 ]\n",
      " [-1.27373191]]\n",
      "loss=34501.49768435747\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.09796366]\n",
      " [-0.4184895 ]\n",
      " [-0.26303572]\n",
      " ...\n",
      " [-0.19063569]\n",
      " [ 0.04127368]\n",
      " [-0.11113059]]\n",
      "t [[ 0.09796366]\n",
      " [-0.4184895 ]\n",
      " [-0.26303572]\n",
      " ...\n",
      " [-0.19063569]\n",
      " [ 0.04127368]\n",
      " [-0.11113059]]\n",
      "t [[ 0.1507815 ]\n",
      " [-0.68202969]\n",
      " [-0.42424472]\n",
      " ...\n",
      " [-0.29607831]\n",
      " [ 0.05192754]\n",
      " [-0.19882386]]\n",
      "t [[ 0.1507815 ]\n",
      " [-0.68202969]\n",
      " [-0.42424472]\n",
      " ...\n",
      " [-0.29607831]\n",
      " [ 0.05192754]\n",
      " [-0.19882386]]\n",
      "Current iteration=2, loss=37531.875893093136\n",
      "t [[ 0.1808453 ]\n",
      " [-0.86715513]\n",
      " [-0.53670203]\n",
      " ...\n",
      " [-0.36299057]\n",
      " [ 0.0468896 ]\n",
      " [-0.2713608 ]]\n",
      "t [[ 0.1808453 ]\n",
      " [-0.86715513]\n",
      " [-0.53670203]\n",
      " ...\n",
      " [-0.36299057]\n",
      " [ 0.0468896 ]\n",
      " [-0.2713608 ]]\n",
      "t [[ 0.19861081]\n",
      " [-1.00817739]\n",
      " [-0.6234918 ]\n",
      " ...\n",
      " [-0.41142468]\n",
      " [ 0.03306612]\n",
      " [-0.33306666]]\n",
      "t [[ 0.19861081]\n",
      " [-1.00817739]\n",
      " [-0.6234918 ]\n",
      " ...\n",
      " [-0.41142468]\n",
      " [ 0.03306612]\n",
      " [-0.33306666]]\n",
      "Current iteration=4, loss=36275.0646244277\n",
      "t [[ 0.2092897 ]\n",
      " [-1.12173514]\n",
      " [-0.69529361]\n",
      " ...\n",
      " [-0.45039674]\n",
      " [ 0.01401486]\n",
      " [-0.38647106]]\n",
      "t [[ 0.2092897 ]\n",
      " [-1.12173514]\n",
      " [-0.69529361]\n",
      " ...\n",
      " [-0.45039674]\n",
      " [ 0.01401486]\n",
      " [-0.38647106]]\n",
      "t [[ 0.21567741]\n",
      " [-1.21668019]\n",
      " [-0.75742016]\n",
      " ...\n",
      " [-0.48416259]\n",
      " [-0.00820673]\n",
      " [-0.43322671]]\n",
      "t [[ 0.21567741]\n",
      " [-1.21668019]\n",
      " [-0.75742016]\n",
      " ...\n",
      " [-0.48416259]\n",
      " [-0.00820673]\n",
      " [-0.43322671]]\n",
      "Current iteration=6, loss=35484.92104295511\n",
      "t [[ 0.21936237]\n",
      " [-1.29810698]\n",
      " [-0.81267754]\n",
      " ...\n",
      " [-0.51478651]\n",
      " [-0.03230783]\n",
      " [-0.47450187]]\n",
      "t [[ 0.21936237]\n",
      " [-1.29810698]\n",
      " [-0.81267754]\n",
      " ...\n",
      " [-0.51478651]\n",
      " [-0.03230783]\n",
      " [-0.47450187]]\n",
      "t [[ 0.22129214]\n",
      " [-1.36916439]\n",
      " [-0.86264012]\n",
      " ...\n",
      " [-0.54328647]\n",
      " [-0.05743351]\n",
      " [-0.51116885]]\n",
      "t [[ 0.22129214]\n",
      " [-1.36916439]\n",
      " [-0.86264012]\n",
      " ...\n",
      " [-0.54328647]\n",
      " [-0.05743351]\n",
      " [-0.51116885]]\n",
      "Current iteration=8, loss=34943.43672213904\n",
      "t [[ 0.222057  ]\n",
      " [-1.43193266]\n",
      " [-0.9082546 ]\n",
      " ...\n",
      " [-0.57017278]\n",
      " [-0.08299759]\n",
      " [-0.54390463]]\n",
      "t [[ 0.222057  ]\n",
      " [-1.43193266]\n",
      " [-0.9082546 ]\n",
      " ...\n",
      " [-0.57017278]\n",
      " [-0.08299759]\n",
      " [-0.54390463]]\n",
      "t [[ 0.22203914]\n",
      " [-1.48787122]\n",
      " [-0.95013718]\n",
      " ...\n",
      " [-0.59570868]\n",
      " [-0.10859004]\n",
      " [-0.57324948]]\n",
      "loss=34552.9887644767\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01291882]\n",
      " [-0.07953946]\n",
      " [-0.24936451]\n",
      " ...\n",
      " [-0.19135692]\n",
      " [ 0.03720109]\n",
      " [-0.11352141]]\n",
      "t [[ 0.01291882]\n",
      " [-0.07953946]\n",
      " [-0.24936451]\n",
      " ...\n",
      " [-0.19135692]\n",
      " [ 0.03720109]\n",
      " [-0.11352141]]\n",
      "t [[ 0.01031988]\n",
      " [-0.18285532]\n",
      " [-0.3942323 ]\n",
      " ...\n",
      " [-0.29570258]\n",
      " [ 0.04451651]\n",
      " [-0.20307832]]\n",
      "t [[ 0.01031988]\n",
      " [-0.18285532]\n",
      " [-0.3942323 ]\n",
      " ...\n",
      " [-0.29570258]\n",
      " [ 0.04451651]\n",
      " [-0.20307832]]\n",
      "Current iteration=2, loss=37498.38930144335\n",
      "t [[ 0.00212093]\n",
      " [-0.2902419 ]\n",
      " [-0.49142403]\n",
      " ...\n",
      " [-0.36117378]\n",
      " [ 0.03690413]\n",
      " [-0.27720708]]\n",
      "t [[ 0.00212093]\n",
      " [-0.2902419 ]\n",
      " [-0.49142403]\n",
      " ...\n",
      " [-0.36117378]\n",
      " [ 0.03690413]\n",
      " [-0.27720708]]\n",
      "t [[-0.00771774]\n",
      " [-0.39423257]\n",
      " [-0.5650247 ]\n",
      " ...\n",
      " [-0.40818267]\n",
      " [ 0.02113337]\n",
      " [-0.34032173]]\n",
      "t [[-0.00771774]\n",
      " [-0.39423257]\n",
      " [-0.5650247 ]\n",
      " ...\n",
      " [-0.40818267]\n",
      " [ 0.02113337]\n",
      " [-0.34032173]]\n",
      "Current iteration=4, loss=36219.28183582068\n",
      "t [[-0.01758605]\n",
      " [-0.49217117]\n",
      " [-0.6258305 ]\n",
      " ...\n",
      " [-0.44582005]\n",
      " [ 0.00062611]\n",
      " [-0.39499272]]\n",
      "t [[-0.01758605]\n",
      " [-0.49217117]\n",
      " [-0.6258305 ]\n",
      " ...\n",
      " [-0.44582005]\n",
      " [ 0.00062611]\n",
      " [-0.39499272]]\n",
      "t [[-0.0268388 ]\n",
      " [-0.5833406 ]\n",
      " [-0.67898656]\n",
      " ...\n",
      " [-0.47834446]\n",
      " [-0.02267118]\n",
      " [-0.44289727]]\n",
      "t [[-0.0268388 ]\n",
      " [-0.5833406 ]\n",
      " [-0.67898656]\n",
      " ...\n",
      " [-0.47834446]\n",
      " [-0.02267118]\n",
      " [-0.44289727]]\n",
      "Current iteration=6, loss=35413.68845520807\n",
      "t [[-0.03525127]\n",
      " [-0.66782796]\n",
      " [-0.72705466]\n",
      " ...\n",
      " [-0.507808  ]\n",
      " [-0.04755482]\n",
      " [-0.48522098]]\n",
      "t [[-0.03525127]\n",
      " [-0.66782796]\n",
      " [-0.72705466]\n",
      " ...\n",
      " [-0.507808  ]\n",
      " [-0.04755482]\n",
      " [-0.48522098]]\n",
      "t [[-0.0427841 ]\n",
      " [-0.74603895]\n",
      " [-0.77136342]\n",
      " ...\n",
      " [-0.53521693]\n",
      " [-0.07323757]\n",
      " [-0.52284958]]\n",
      "t [[-0.0427841 ]\n",
      " [-0.74603895]\n",
      " [-0.77136342]\n",
      " ...\n",
      " [-0.53521693]\n",
      " [-0.07323757]\n",
      " [-0.52284958]]\n",
      "Current iteration=8, loss=34861.977478536945\n",
      "t [[-0.04947656]\n",
      " [-0.81848268]\n",
      " [-0.81263805]\n",
      " ...\n",
      " [-0.56107346]\n",
      " [-0.09918564]\n",
      " [-0.55647093]]\n",
      "t [[-0.04947656]\n",
      " [-0.81848268]\n",
      " [-0.81263805]\n",
      " ...\n",
      " [-0.56107346]\n",
      " [-0.09918564]\n",
      " [-0.55647093]]\n",
      "t [[-0.05539748]\n",
      " [-0.88567696]\n",
      " [-0.85130343]\n",
      " ...\n",
      " [-0.58563585]\n",
      " [-0.12502958]\n",
      " [-0.5866343 ]]\n",
      "loss=34464.71051835986\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0110341 ]\n",
      " [-0.06980859]\n",
      " [-0.25206521]\n",
      " ...\n",
      " [-0.18815682]\n",
      " [ 0.03999843]\n",
      " [-0.10648357]]\n",
      "t [[ 0.0110341 ]\n",
      " [-0.06980859]\n",
      " [-0.25206521]\n",
      " ...\n",
      " [-0.18815682]\n",
      " [ 0.03999843]\n",
      " [-0.10648357]]\n",
      "t [[ 0.00670001]\n",
      " [-0.16452722]\n",
      " [-0.3998216 ]\n",
      " ...\n",
      " [-0.29047619]\n",
      " [ 0.04980041]\n",
      " [-0.18981401]]\n",
      "t [[ 0.00670001]\n",
      " [-0.16452722]\n",
      " [-0.3998216 ]\n",
      " ...\n",
      " [-0.29047619]\n",
      " [ 0.04980041]\n",
      " [-0.18981401]]\n",
      "Current iteration=2, loss=37531.880886460516\n",
      "t [[-0.0030284 ]\n",
      " [-0.26434624]\n",
      " [-0.49958824]\n",
      " ...\n",
      " [-0.3543689 ]\n",
      " [ 0.04432784]\n",
      " [-0.25834336]]\n",
      "t [[-0.0030284 ]\n",
      " [-0.26434624]\n",
      " [-0.49958824]\n",
      " ...\n",
      " [-0.3543689 ]\n",
      " [ 0.04432784]\n",
      " [-0.25834336]]\n",
      "t [[-0.01416787]\n",
      " [-0.36162711]\n",
      " [-0.57541   ]\n",
      " ...\n",
      " [-0.40004893]\n",
      " [ 0.03037469]\n",
      " [-0.31637982]]\n",
      "t [[-0.01416787]\n",
      " [-0.36162711]\n",
      " [-0.57541   ]\n",
      " ...\n",
      " [-0.40004893]\n",
      " [ 0.03037469]\n",
      " [-0.31637982]]\n",
      "Current iteration=4, loss=36275.27810261679\n",
      "t [[-0.02511193]\n",
      " [-0.45357007]\n",
      " [-0.63812911]\n",
      " ...\n",
      " [-0.4365445 ]\n",
      " [ 0.01139544]\n",
      " [-0.36642132]]\n",
      "t [[-0.02511193]\n",
      " [-0.45357007]\n",
      " [-0.63812911]\n",
      " ...\n",
      " [-0.4365445 ]\n",
      " [ 0.01139544]\n",
      " [-0.36642132]]\n",
      "t [[-0.03523288]\n",
      " [-0.53934563]\n",
      " [-0.69294361]\n",
      " ...\n",
      " [-0.46808799]\n",
      " [-0.01063015]\n",
      " [-0.41008956]]\n",
      "t [[-0.03523288]\n",
      " [-0.53934563]\n",
      " [-0.69294361]\n",
      " ...\n",
      " [-0.46808799]\n",
      " [-0.01063015]\n",
      " [-0.41008956]]\n",
      "Current iteration=6, loss=35485.51606055785\n",
      "t [[-0.04432908]\n",
      " [-0.61895232]\n",
      " [-0.74246032]\n",
      " ...\n",
      " [-0.49671503]\n",
      " [-0.03446562]\n",
      " [-0.44852502]]\n",
      "t [[-0.04432908]\n",
      " [-0.61895232]\n",
      " [-0.74246032]\n",
      " ...\n",
      " [-0.49671503]\n",
      " [-0.03446562]\n",
      " [-0.44852502]]\n",
      "t [[-0.05238533]\n",
      " [-0.69272483]\n",
      " [-0.78804373]\n",
      " ...\n",
      " [-0.52341767]\n",
      " [-0.05929233]\n",
      " [-0.48257556]]\n",
      "t [[-0.05238533]\n",
      " [-0.69272483]\n",
      " [-0.78804373]\n",
      " ...\n",
      " [-0.52341767]\n",
      " [-0.05929233]\n",
      " [-0.48257556]]\n",
      "Current iteration=8, loss=34944.77038724494\n",
      "t [[-0.05946392]\n",
      " [-0.76111445]\n",
      " [-0.83044719]\n",
      " ...\n",
      " [-0.54868436]\n",
      " [-0.08454747]\n",
      " [-0.5128967 ]]\n",
      "t [[-0.05946392]\n",
      " [-0.76111445]\n",
      " [-0.83044719]\n",
      " ...\n",
      " [-0.54868436]\n",
      " [-0.08454747]\n",
      " [-0.5128967 ]]\n",
      "t [[-0.06565449]\n",
      " [-0.82459132]\n",
      " [-0.87011772]\n",
      " ...\n",
      " [-0.57275994]\n",
      " [-0.10983538]\n",
      " [-0.54000981]]\n",
      "loss=34555.1568463706\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00918919]\n",
      " [-0.07393935]\n",
      " [-0.25040201]\n",
      " ...\n",
      " [-0.11024557]\n",
      " [-0.0132777 ]\n",
      " [-0.22491881]]\n",
      "t [[ 0.00918919]\n",
      " [-0.07393935]\n",
      " [-0.25040201]\n",
      " ...\n",
      " [-0.11024557]\n",
      " [-0.0132777 ]\n",
      " [-0.22491881]]\n",
      "t [[ 0.00331509]\n",
      " [-0.1725965 ]\n",
      " [-0.39564917]\n",
      " ...\n",
      " [-0.15310094]\n",
      " [-0.0042485 ]\n",
      " [-0.41043444]]\n",
      "t [[ 0.00331509]\n",
      " [-0.1725965 ]\n",
      " [-0.39564917]\n",
      " ...\n",
      " [-0.15310094]\n",
      " [-0.0042485 ]\n",
      " [-0.41043444]]\n",
      "Current iteration=2, loss=37484.44166476896\n",
      "t [[-0.00769253]\n",
      " [-0.27606164]\n",
      " [-0.49278048]\n",
      " ...\n",
      " [-0.16855901]\n",
      " [ 0.01096837]\n",
      " [-0.5690675 ]]\n",
      "t [[-0.00769253]\n",
      " [-0.27606164]\n",
      " [-0.49278048]\n",
      " ...\n",
      " [-0.16855901]\n",
      " [ 0.01096837]\n",
      " [-0.5690675 ]]\n",
      "t [[-0.01990492]\n",
      " [-0.3766993 ]\n",
      " [-0.56606964]\n",
      " ...\n",
      " [-0.17311592]\n",
      " [ 0.02625655]\n",
      " [-0.70787193]]\n",
      "t [[-0.01990492]\n",
      " [-0.3766993 ]\n",
      " [-0.56606964]\n",
      " ...\n",
      " [-0.17311592]\n",
      " [ 0.02625655]\n",
      " [-0.70787193]]\n",
      "Current iteration=4, loss=36205.49796941333\n",
      "t [[-0.03176143]\n",
      " [-0.47172953]\n",
      " [-0.62643196]\n",
      " ...\n",
      " [-0.17376426]\n",
      " [ 0.03946063]\n",
      " [-0.83120723]]\n",
      "t [[-0.03176143]\n",
      " [-0.47172953]\n",
      " [-0.62643196]\n",
      " ...\n",
      " [-0.17376426]\n",
      " [ 0.03946063]\n",
      " [-0.83120723]]\n",
      "t [[-0.04266948]\n",
      " [-0.56034476]\n",
      " [-0.67908421]\n",
      " ...\n",
      " [-0.17350371]\n",
      " [ 0.05001985]\n",
      " [-0.94202019]]\n",
      "t [[-0.04266948]\n",
      " [-0.56034476]\n",
      " [-0.67908421]\n",
      " ...\n",
      " [-0.17350371]\n",
      " [ 0.05001985]\n",
      " [-0.94202019]]\n",
      "Current iteration=6, loss=35400.3008386913\n",
      "t [[-0.05245366]\n",
      " [-0.64256538]\n",
      " [-0.72662989]\n",
      " ...\n",
      " [-0.17355948]\n",
      " [ 0.05801732]\n",
      " [-1.04243476]]\n",
      "t [[-0.05245366]\n",
      " [-0.64256538]\n",
      " [-0.72662989]\n",
      " ...\n",
      " [-0.17355948]\n",
      " [ 0.05801732]\n",
      " [-1.04243476]]\n",
      "t [[-0.06111832]\n",
      " [-0.71874726]\n",
      " [-0.77042099]\n",
      " ...\n",
      " [-0.17435468]\n",
      " [ 0.06377175]\n",
      " [-1.13405305]]\n",
      "t [[-0.06111832]\n",
      " [-0.71874726]\n",
      " [-0.77042099]\n",
      " ...\n",
      " [-0.17435468]\n",
      " [ 0.06377175]\n",
      " [-1.13405305]]\n",
      "Current iteration=8, loss=34848.587352552066\n",
      "t [[-0.06874032]\n",
      " [-0.78936159]\n",
      " [-0.81119509]\n",
      " ...\n",
      " [-0.17595721]\n",
      " [ 0.06765962]\n",
      " [-1.21812418]]\n",
      "t [[-0.06874032]\n",
      " [-0.78936159]\n",
      " [-0.81119509]\n",
      " ...\n",
      " [-0.17595721]\n",
      " [ 0.06765962]\n",
      " [-1.21812418]]\n",
      "t [[-0.0754202 ]\n",
      " [-0.85489679]\n",
      " [-0.84938305]\n",
      " ...\n",
      " [-0.17828847]\n",
      " [ 0.0700409 ]\n",
      " [-1.29564648]]\n",
      "loss=34451.11099524453\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.10068487]\n",
      " [-0.43011421]\n",
      " [-0.27034227]\n",
      " ...\n",
      " [-0.19593113]\n",
      " [ 0.04242017]\n",
      " [-0.11421755]]\n",
      "t [[ 0.10068487]\n",
      " [-0.43011421]\n",
      " [-0.27034227]\n",
      " ...\n",
      " [-0.19593113]\n",
      " [ 0.04242017]\n",
      " [-0.11421755]]\n",
      "t [[ 0.15371667]\n",
      " [-0.69667743]\n",
      " [-0.4332069 ]\n",
      " ...\n",
      " [-0.30193938]\n",
      " [ 0.05252588]\n",
      " [-0.20369231]]\n",
      "t [[ 0.15371667]\n",
      " [-0.69667743]\n",
      " [-0.4332069 ]\n",
      " ...\n",
      " [-0.30193938]\n",
      " [ 0.05252588]\n",
      " [-0.20369231]]\n",
      "Current iteration=2, loss=37481.80880212378\n",
      "t [[ 0.1833571 ]\n",
      " [-0.88259307]\n",
      " [-0.54608349]\n",
      " ...\n",
      " [-0.36857449]\n",
      " [ 0.04647875]\n",
      " [-0.27739762]]\n",
      "t [[ 0.1833571 ]\n",
      " [-0.88259307]\n",
      " [-0.54608349]\n",
      " ...\n",
      " [-0.36857449]\n",
      " [ 0.04647875]\n",
      " [-0.27739762]]\n",
      "t [[ 0.20059282]\n",
      " [-1.02383768]\n",
      " [-0.63313386]\n",
      " ...\n",
      " [-0.41680842]\n",
      " [ 0.03153354]\n",
      " [-0.33990679]]\n",
      "t [[ 0.20059282]\n",
      " [-1.02383768]\n",
      " [-0.63313386]\n",
      " ...\n",
      " [-0.41680842]\n",
      " [ 0.03153354]\n",
      " [-0.33990679]]\n",
      "Current iteration=4, loss=36218.50605694618\n",
      "t [[ 0.21078307]\n",
      " [-1.13747923]\n",
      " [-0.70525467]\n",
      " ...\n",
      " [-0.45580765]\n",
      " [ 0.01136642]\n",
      " [-0.3938634 ]]\n",
      "t [[ 0.21078307]\n",
      " [-1.13747923]\n",
      " [-0.70525467]\n",
      " ...\n",
      " [-0.45580765]\n",
      " [ 0.01136642]\n",
      " [-0.3938634 ]]\n",
      "t [[ 0.21675555]\n",
      " [-1.23246273]\n",
      " [-0.76775606]\n",
      " ...\n",
      " [-0.4897868 ]\n",
      " [-0.0119161 ]\n",
      " [-0.44098591]]\n",
      "t [[ 0.21675555]\n",
      " [-1.23246273]\n",
      " [-0.76775606]\n",
      " ...\n",
      " [-0.4897868 ]\n",
      " [-0.0119161 ]\n",
      " [-0.44098591]]\n",
      "Current iteration=6, loss=35429.22222119716\n",
      "t [[ 0.22009637]\n",
      " [-1.31388785]\n",
      " [-0.82339769]\n",
      " ...\n",
      " [-0.52073688]\n",
      " [-0.03700066]\n",
      " [-0.48248631]]\n",
      "t [[ 0.22009637]\n",
      " [-1.31388785]\n",
      " [-0.82339769]\n",
      " ...\n",
      " [-0.52073688]\n",
      " [-0.03700066]\n",
      " [-0.48248631]]\n",
      "t [[ 0.22174255]\n",
      " [-1.38489368]\n",
      " [-0.8737126 ]\n",
      " ...\n",
      " [-0.54961412]\n",
      " [-0.06302206]\n",
      " [-0.51926843]]\n",
      "t [[ 0.22174255]\n",
      " [-1.38489368]\n",
      " [-0.8737126 ]\n",
      " ...\n",
      " [-0.54961412]\n",
      " [-0.06302206]\n",
      " [-0.51926843]]\n",
      "Current iteration=8, loss=34891.472258223235\n",
      "t [[ 0.22227376]\n",
      " [-1.44755467]\n",
      " [-0.91962117]\n",
      " ...\n",
      " [-0.57688626]\n",
      " [-0.08939056]\n",
      " [-0.55203306]]\n",
      "t [[ 0.22227376]\n",
      " [-1.44755467]\n",
      " [-0.91962117]\n",
      " ...\n",
      " [-0.57688626]\n",
      " [-0.08939056]\n",
      " [-0.55203306]]\n",
      "t [[ 0.22206342]\n",
      " [-1.50333078]\n",
      " [-0.96172655]\n",
      " ...\n",
      " [-0.6027904 ]\n",
      " [-0.11569656]\n",
      " [-0.58133886]]\n",
      "loss=34505.43587998868\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01327768]\n",
      " [-0.08174889]\n",
      " [-0.25629131]\n",
      " ...\n",
      " [-0.19667239]\n",
      " [ 0.03823445]\n",
      " [-0.11667479]]\n",
      "t [[ 0.01327768]\n",
      " [-0.08174889]\n",
      " [-0.25629131]\n",
      " ...\n",
      " [-0.19667239]\n",
      " [ 0.03823445]\n",
      " [-0.11667479]]\n",
      "t [[ 0.01018171]\n",
      " [-0.18858704]\n",
      " [-0.40228745]\n",
      " ...\n",
      " [-0.30150406]\n",
      " [ 0.04493018]\n",
      " [-0.20805052]]\n",
      "t [[ 0.01018171]\n",
      " [-0.18858704]\n",
      " [-0.40228745]\n",
      " ...\n",
      " [-0.30150406]\n",
      " [ 0.04493018]\n",
      " [-0.20805052]]\n",
      "Current iteration=2, loss=37447.599816544505\n",
      "t [[ 0.0014483 ]\n",
      " [-0.29917503]\n",
      " [-0.49953455]\n",
      " ...\n",
      " [-0.36663872]\n",
      " [ 0.03627959]\n",
      " [-0.28337682]]\n",
      "t [[ 0.0014483 ]\n",
      " [-0.29917503]\n",
      " [-0.49953455]\n",
      " ...\n",
      " [-0.36663872]\n",
      " [ 0.03627959]\n",
      " [-0.28337682]]\n",
      "t [[-0.00879966]\n",
      " [-0.40576726]\n",
      " [-0.57320368]\n",
      " ...\n",
      " [-0.41340875]\n",
      " [ 0.01938537]\n",
      " [-0.34731871]]\n",
      "t [[-0.00879966]\n",
      " [-0.40576726]\n",
      " [-0.57320368]\n",
      " ...\n",
      " [-0.41340875]\n",
      " [ 0.01938537]\n",
      " [-0.34731871]]\n",
      "Current iteration=4, loss=36161.634407407655\n",
      "t [[-0.01894461]\n",
      " [-0.5057474 ]\n",
      " [-0.63426901]\n",
      " ...\n",
      " [-0.45104623]\n",
      " [-0.00222333]\n",
      " [-0.40256145]]\n",
      "t [[-0.01894461]\n",
      " [-0.5057474 ]\n",
      " [-0.63426901]\n",
      " ...\n",
      " [-0.45104623]\n",
      " [-0.00222333]\n",
      " [-0.40256145]]\n",
      "t [[-0.02836747]\n",
      " [-0.59850007]\n",
      " [-0.68783469]\n",
      " ...\n",
      " [-0.48376255]\n",
      " [-0.02655832]\n",
      " [-0.45084862]]\n",
      "t [[-0.02836747]\n",
      " [-0.59850007]\n",
      " [-0.68783469]\n",
      " ...\n",
      " [-0.48376255]\n",
      " [-0.02655832]\n",
      " [-0.45084862]]\n",
      "Current iteration=6, loss=35356.90128384294\n",
      "t [[-0.03687178]\n",
      " [-0.68421009]\n",
      " [-0.73638666]\n",
      " ...\n",
      " [-0.51353369]\n",
      " [-0.05239809]\n",
      " [-0.49341014]]\n",
      "t [[-0.03687178]\n",
      " [-0.68421009]\n",
      " [-0.73638666]\n",
      " ...\n",
      " [-0.51353369]\n",
      " [-0.05239809]\n",
      " [-0.49341014]]\n",
      "t [[-0.04444091]\n",
      " [-0.76336205]\n",
      " [-0.7811914 ]\n",
      " ...\n",
      " [-0.54130325]\n",
      " [-0.07894811]\n",
      " [-0.53116382]]\n",
      "t [[-0.04444091]\n",
      " [-0.76336205]\n",
      " [-0.7811914 ]\n",
      " ...\n",
      " [-0.54130325]\n",
      " [-0.07894811]\n",
      " [-0.53116382]]\n",
      "Current iteration=8, loss=34809.054986810355\n",
      "t [[-0.05113067]\n",
      " [-0.8365251 ]\n",
      " [-0.82293331]\n",
      " ...\n",
      " [-0.56753079]\n",
      " [-0.10567282]\n",
      " [-0.56482178]]\n",
      "t [[-0.05113067]\n",
      " [-0.8365251 ]\n",
      " [-0.82293331]\n",
      " ...\n",
      " [-0.56753079]\n",
      " [-0.10567282]\n",
      " [-0.56482178]]\n",
      "t [[-0.05702139]\n",
      " [-0.90426212]\n",
      " [-0.86201422]\n",
      " ...\n",
      " [-0.59244865]\n",
      " [-0.1322043 ]\n",
      " [-0.59495209]]\n",
      "loss=34416.35662354595\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01134061]\n",
      " [-0.07174772]\n",
      " [-0.25906703]\n",
      " ...\n",
      " [-0.1933834 ]\n",
      " [ 0.0411095 ]\n",
      " [-0.10944145]]\n",
      "t [[ 0.01134061]\n",
      " [-0.07174772]\n",
      " [-0.25906703]\n",
      " ...\n",
      " [-0.1933834 ]\n",
      " [ 0.0411095 ]\n",
      " [-0.10944145]]\n",
      "t [[ 0.00646518]\n",
      " [-0.16978277]\n",
      " [-0.40803558]\n",
      " ...\n",
      " [-0.29616438]\n",
      " [ 0.0503512 ]\n",
      " [-0.19444069]]\n",
      "t [[ 0.00646518]\n",
      " [-0.16978277]\n",
      " [-0.40803558]\n",
      " ...\n",
      " [-0.29616438]\n",
      " [ 0.0503512 ]\n",
      " [-0.19444069]]\n",
      "Current iteration=2, loss=37481.848976781264\n",
      "t [[-0.00382877]\n",
      " [-0.27265049]\n",
      " [-0.50791173]\n",
      " ...\n",
      " [-0.35970207]\n",
      " [ 0.04388011]\n",
      " [-0.26404704]]\n",
      "t [[-0.00382877]\n",
      " [-0.27265049]\n",
      " [-0.50791173]\n",
      " ...\n",
      " [-0.35970207]\n",
      " [ 0.04388011]\n",
      " [-0.26404704]]\n",
      "t [[-0.01539441]\n",
      " [-0.37241794]\n",
      " [-0.58383469]\n",
      " ...\n",
      " [-0.40512782]\n",
      " [ 0.02882684]\n",
      " [-0.32281313]]\n",
      "t [[-0.01539441]\n",
      " [-0.37241794]\n",
      " [-0.58383469]\n",
      " ...\n",
      " [-0.40512782]\n",
      " [ 0.02882684]\n",
      " [-0.32281313]]\n",
      "Current iteration=4, loss=36218.72351497879\n",
      "t [[-0.02661965]\n",
      " [-0.46631517]\n",
      " [-0.64683257]\n",
      " ...\n",
      " [-0.44161307]\n",
      " [ 0.00875612]\n",
      " [-0.37334761]]\n",
      "t [[-0.02661965]\n",
      " [-0.46631517]\n",
      " [-0.64683257]\n",
      " ...\n",
      " [-0.44161307]\n",
      " [ 0.00875612]\n",
      " [-0.37334761]]\n",
      "t [[-0.03690559]\n",
      " [-0.55360774]\n",
      " [-0.7020674 ]\n",
      " ...\n",
      " [-0.47334391]\n",
      " [-0.01430777]\n",
      " [-0.41733543]]\n",
      "t [[-0.03690559]\n",
      " [-0.55360774]\n",
      " [-0.7020674 ]\n",
      " ...\n",
      " [-0.47334391]\n",
      " [-0.01430777]\n",
      " [-0.41733543]]\n",
      "Current iteration=6, loss=35429.864484864025\n",
      "t [[-0.04608146]\n",
      " [-0.63438712]\n",
      " [-0.75207314]\n",
      " ...\n",
      " [-0.50227968]\n",
      " [-0.03910778]\n",
      " [-0.45595879]]\n",
      "t [[-0.04608146]\n",
      " [-0.63438712]\n",
      " [-0.75207314]\n",
      " ...\n",
      " [-0.50227968]\n",
      " [-0.03910778]\n",
      " [-0.45595879]]\n",
      "t [[-0.054157  ]\n",
      " [-0.70906342]\n",
      " [-0.79815391]\n",
      " ...\n",
      " [-0.52934888]\n",
      " [-0.06481556]\n",
      " [-0.49009553]]\n",
      "t [[-0.054157  ]\n",
      " [-0.70906342]\n",
      " [-0.79815391]\n",
      " ...\n",
      " [-0.52934888]\n",
      " [-0.06481556]\n",
      " [-0.49009553]]\n",
      "Current iteration=8, loss=34892.89580481019\n",
      "t [[-0.06121274]\n",
      " [-0.77814537]\n",
      " [-0.84102335]\n",
      " ...\n",
      " [-0.55499624]\n",
      " [-0.09086446]\n",
      " [-0.52042364]]\n",
      "t [[-0.06121274]\n",
      " [-0.77814537]\n",
      " [-0.84102335]\n",
      " ...\n",
      " [-0.55499624]\n",
      " [-0.09086446]\n",
      " [-0.52042364]]\n",
      "t [[-0.06735119]\n",
      " [-0.84214615]\n",
      " [-0.88110614]\n",
      " ...\n",
      " [-0.57943968]\n",
      " [-0.1168587 ]\n",
      " [-0.54748182]]\n",
      "loss=34507.71960864751\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00944444]\n",
      " [-0.07599322]\n",
      " [-0.25735762]\n",
      " ...\n",
      " [-0.11330795]\n",
      " [-0.01364652]\n",
      " [-0.23116656]]\n",
      "t [[ 0.00944444]\n",
      " [-0.07599322]\n",
      " [-0.25735762]\n",
      " ...\n",
      " [-0.11330795]\n",
      " [-0.01364652]\n",
      " [-0.23116656]]\n",
      "t [[ 0.00299476]\n",
      " [-0.17806968]\n",
      " [-0.40372528]\n",
      " ...\n",
      " [-0.15548631]\n",
      " [-0.00374692]\n",
      " [-0.42073312]]\n",
      "t [[ 0.00299476]\n",
      " [-0.17806968]\n",
      " [-0.40372528]\n",
      " ...\n",
      " [-0.15548631]\n",
      " [-0.00374692]\n",
      " [-0.42073312]]\n",
      "Current iteration=2, loss=37433.576735958704\n",
      "t [[-0.00859931]\n",
      " [-0.28466833]\n",
      " [-0.50088635]\n",
      " ...\n",
      " [-0.16985587]\n",
      " [ 0.0122357 ]\n",
      " [-0.58227531]]\n",
      "t [[-0.00859931]\n",
      " [-0.28466833]\n",
      " [-0.50088635]\n",
      " ...\n",
      " [-0.16985587]\n",
      " [ 0.0122357 ]\n",
      " [-0.58227531]]\n",
      "t [[-0.0212503 ]\n",
      " [-0.38786188]\n",
      " [-0.57421463]\n",
      " ...\n",
      " [-0.17362702]\n",
      " [ 0.02795501]\n",
      " [-0.72327659]]\n",
      "t [[-0.0212503 ]\n",
      " [-0.38786188]\n",
      " [-0.57421463]\n",
      " ...\n",
      " [-0.17362702]\n",
      " [ 0.02795501]\n",
      " [-0.72327659]]\n",
      "Current iteration=4, loss=36147.885314696054\n",
      "t [[-0.03339531]\n",
      " [-0.48490236]\n",
      " [-0.63480958]\n",
      " ...\n",
      " [-0.17385784]\n",
      " [ 0.04129293]\n",
      " [-0.84831153]]\n",
      "t [[-0.03339531]\n",
      " [-0.48490236]\n",
      " [-0.63480958]\n",
      " ...\n",
      " [-0.17385784]\n",
      " [ 0.04129293]\n",
      " [-0.84831153]]\n",
      "t [[-0.04447258]\n",
      " [-0.57507912]\n",
      " [-0.68784914]\n",
      " ...\n",
      " [-0.1734653 ]\n",
      " [ 0.05177581]\n",
      " [-0.96045544]]\n",
      "t [[-0.04447258]\n",
      " [-0.57507912]\n",
      " [-0.68784914]\n",
      " ...\n",
      " [-0.1734653 ]\n",
      " [ 0.05177581]\n",
      " [-0.96045544]]\n",
      "Current iteration=6, loss=35343.53334132363\n",
      "t [[-0.05433883]\n",
      " [-0.65850739]\n",
      " [-0.73586115]\n",
      " ...\n",
      " [-0.17357756]\n",
      " [ 0.05956642]\n",
      " [-1.06191788]]\n",
      "t [[-0.05433883]\n",
      " [-0.65850739]\n",
      " [-0.73586115]\n",
      " ...\n",
      " [-0.17357756]\n",
      " [ 0.05956642]\n",
      " [-1.06191788]]\n",
      "t [[-0.06302402]\n",
      " [-0.73562013]\n",
      " [-0.78013481]\n",
      " ...\n",
      " [-0.17454032]\n",
      " [ 0.06504335]\n",
      " [-1.15436196]]\n",
      "t [[-0.06302402]\n",
      " [-0.73562013]\n",
      " [-0.78013481]\n",
      " ...\n",
      " [-0.17454032]\n",
      " [ 0.06504335]\n",
      " [-1.15436196]]\n",
      "Current iteration=8, loss=34795.65531545479\n",
      "t [[-0.07062366]\n",
      " [-0.80694743]\n",
      " [-0.82136613]\n",
      " ...\n",
      " [-0.17636743]\n",
      " [ 0.06862422]\n",
      " [-1.23908211]]\n",
      "t [[-0.07062366]\n",
      " [-0.80694743]\n",
      " [-0.82136613]\n",
      " ...\n",
      " [-0.17636743]\n",
      " [ 0.06862422]\n",
      " [-1.23908211]]\n",
      "t [[-0.07725135]\n",
      " [-0.87302195]\n",
      " [-0.85996215]\n",
      " ...\n",
      " [-0.17894522]\n",
      " [ 0.07069572]\n",
      " [-1.31711109]]\n",
      "loss=34402.72523188589\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.10340608]\n",
      " [-0.44173892]\n",
      " [-0.27764881]\n",
      " ...\n",
      " [-0.20122656]\n",
      " [ 0.04356666]\n",
      " [-0.11730451]]\n",
      "t [[ 0.10340608]\n",
      " [-0.44173892]\n",
      " [-0.27764881]\n",
      " ...\n",
      " [-0.20122656]\n",
      " [ 0.04356666]\n",
      " [-0.11730451]]\n",
      "t [[ 0.15658696]\n",
      " [-0.71110292]\n",
      " [-0.44202324]\n",
      " ...\n",
      " [-0.30767818]\n",
      " [ 0.05308097]\n",
      " [-0.20852656]]\n",
      "t [[ 0.15658696]\n",
      " [-0.71110292]\n",
      " [-0.44202324]\n",
      " ...\n",
      " [-0.30767818]\n",
      " [ 0.05308097]\n",
      " [-0.20852656]]\n",
      "Current iteration=2, loss=37432.67209715998\n",
      "t [[ 0.18577047]\n",
      " [-0.89768923]\n",
      " [-0.5552527 ]\n",
      " ...\n",
      " [-0.37399086]\n",
      " [ 0.04599985]\n",
      " [-0.2833675 ]]\n",
      "t [[ 0.18577047]\n",
      " [-0.89768923]\n",
      " [-0.5552527 ]\n",
      " ...\n",
      " [-0.37399086]\n",
      " [ 0.04599985]\n",
      " [-0.2833675 ]]\n",
      "t [[ 0.20246782]\n",
      " [-1.03910876]\n",
      " [-0.64254961]\n",
      " ...\n",
      " [-0.42202969]\n",
      " [ 0.02992319]\n",
      " [-0.34665047]]\n",
      "t [[ 0.20246782]\n",
      " [-1.03910876]\n",
      " [-0.64254961]\n",
      " ...\n",
      " [-0.42202969]\n",
      " [ 0.02992319]\n",
      " [-0.34665047]]\n",
      "Current iteration=4, loss=36163.30623512171\n",
      "t [[ 0.21217391]\n",
      " [-1.1528185 ]\n",
      " [-0.71499505]\n",
      " ...\n",
      " [-0.46107986]\n",
      " [ 0.00864054]\n",
      " [-0.40113234]]\n",
      "t [[ 0.21217391]\n",
      " [-1.1528185 ]\n",
      " [-0.71499505]\n",
      " ...\n",
      " [-0.46107986]\n",
      " [ 0.00864054]\n",
      " [-0.40113234]]\n",
      "t [[ 0.21774106]\n",
      " [-1.24783409]\n",
      " [-0.77787841]\n",
      " ...\n",
      " [-0.49529683]\n",
      " [-0.01569561]\n",
      " [-0.44859693]]\n",
      "t [[ 0.21774106]\n",
      " [-1.24783409]\n",
      " [-0.77787841]\n",
      " ...\n",
      " [-0.49529683]\n",
      " [-0.01569561]\n",
      " [-0.44859693]]\n",
      "Current iteration=6, loss=35375.20686230573\n",
      "t [[ 0.2207492 ]\n",
      " [-1.32925086]\n",
      " [-0.83390546]\n",
      " ...\n",
      " [-0.52659079]\n",
      " [-0.0417511 ]\n",
      " [-0.49029986]]\n",
      "t [[ 0.2207492 ]\n",
      " [-1.32925086]\n",
      " [-0.83390546]\n",
      " ...\n",
      " [-0.52659079]\n",
      " [-0.0417511 ]\n",
      " [-0.49029986]]\n",
      "t [[ 0.22212313]\n",
      " [-1.40019546]\n",
      " [-0.88456633]\n",
      " ...\n",
      " [-0.55585443]\n",
      " [-0.06865167]\n",
      " [-0.52717654]]\n",
      "t [[ 0.22212313]\n",
      " [-1.40019546]\n",
      " [-0.88456633]\n",
      " ...\n",
      " [-0.55585443]\n",
      " [-0.06865167]\n",
      " [-0.52717654]]\n",
      "Current iteration=8, loss=34841.33993659163\n",
      "t [[ 0.22243133]\n",
      " [-1.46273689]\n",
      " [-0.93075609]\n",
      " ...\n",
      " [-0.58351379]\n",
      " [-0.09580491]\n",
      " [-0.55995157]]\n",
      "t [[ 0.22243133]\n",
      " [-1.46273689]\n",
      " [-0.93075609]\n",
      " ...\n",
      " [-0.58351379]\n",
      " [-0.09580491]\n",
      " [-0.55995157]]\n",
      "t [[ 0.22203836]\n",
      " [-1.51833692]\n",
      " [-0.97306673]\n",
      " ...\n",
      " [-0.60978154]\n",
      " [-0.12280244]\n",
      " [-0.589202  ]]\n",
      "loss=34459.73607402473\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01363654]\n",
      " [-0.08395832]\n",
      " [-0.2632181 ]\n",
      " ...\n",
      " [-0.20198786]\n",
      " [ 0.03926782]\n",
      " [-0.11982816]]\n",
      "t [[ 0.01363654]\n",
      " [-0.08395832]\n",
      " [-0.2632181 ]\n",
      " ...\n",
      " [-0.20198786]\n",
      " [ 0.03926782]\n",
      " [-0.11982816]]\n",
      "t [[ 0.01002201]\n",
      " [-0.19435194]\n",
      " [-0.41019304]\n",
      " ...\n",
      " [-0.30718084]\n",
      " [ 0.04530176]\n",
      " [-0.21298778]]\n",
      "t [[ 0.01002201]\n",
      " [-0.19435194]\n",
      " [-0.41019304]\n",
      " ...\n",
      " [-0.30718084]\n",
      " [ 0.04530176]\n",
      " [-0.21298778]]\n",
      "Current iteration=2, loss=37397.74371026848\n",
      "t [[ 0.00075202]\n",
      " [-0.3081247 ]\n",
      " [-0.50743755]\n",
      " ...\n",
      " [-0.37193461]\n",
      " [ 0.03559036]\n",
      " [-0.28947842]]\n",
      "t [[ 0.00075202]\n",
      " [-0.3081247 ]\n",
      " [-0.50743755]\n",
      " ...\n",
      " [-0.37193461]\n",
      " [ 0.03559036]\n",
      " [-0.28947842]]\n",
      "t [[-0.00989554]\n",
      " [-0.41727116]\n",
      " [-0.58117474]\n",
      " ...\n",
      " [-0.41847245]\n",
      " [ 0.01756512]\n",
      " [-0.35421767]]\n",
      "t [[-0.00989554]\n",
      " [-0.41727116]\n",
      " [-0.58117474]\n",
      " ...\n",
      " [-0.41847245]\n",
      " [ 0.01756512]\n",
      " [-0.35421767]]\n",
      "Current iteration=4, loss=36105.364973580436\n",
      "t [[-0.02030314]\n",
      " [-0.51923339]\n",
      " [-0.64251938]\n",
      " ...\n",
      " [-0.45613506]\n",
      " [-0.00514296]\n",
      " [-0.41000477]]\n",
      "t [[-0.02030314]\n",
      " [-0.51923339]\n",
      " [-0.64251938]\n",
      " ...\n",
      " [-0.45613506]\n",
      " [-0.00514296]\n",
      " [-0.41000477]]\n",
      "t [[-0.02988197]\n",
      " [-0.61350832]\n",
      " [-0.69651435]\n",
      " ...\n",
      " [-0.48906855]\n",
      " [-0.03050717]\n",
      " [-0.45864929]]\n",
      "t [[-0.02988197]\n",
      " [-0.61350832]\n",
      " [-0.69651435]\n",
      " ...\n",
      " [-0.48906855]\n",
      " [-0.03050717]\n",
      " [-0.45864929]]\n",
      "Current iteration=6, loss=35301.832622698705\n",
      "t [[-0.03846567]\n",
      " [-0.70038341]\n",
      " [-0.7455617 ]\n",
      " ...\n",
      " [-0.51916547]\n",
      " [-0.05728992]\n",
      " [-0.50142541]]\n",
      "t [[-0.03846567]\n",
      " [-0.70038341]\n",
      " [-0.7455617 ]\n",
      " ...\n",
      " [-0.51916547]\n",
      " [-0.05728992]\n",
      " [-0.50142541]]\n",
      "t [[-0.0460608 ]\n",
      " [-0.78042381]\n",
      " [-0.79086418]\n",
      " ...\n",
      " [-0.54730514]\n",
      " [-0.08469046]\n",
      " [-0.53928309]]\n",
      "t [[-0.0460608 ]\n",
      " [-0.78042381]\n",
      " [-0.79086418]\n",
      " ...\n",
      " [-0.54730514]\n",
      " [-0.08469046]\n",
      " [-0.53928309]]\n",
      "Current iteration=8, loss=34758.005899748656\n",
      "t [[-0.05273957]\n",
      " [-0.85425893]\n",
      " [-0.83306674]\n",
      " ...\n",
      " [-0.57390547]\n",
      " [-0.11217231]\n",
      " [-0.57295875]]\n",
      "t [[-0.05273957]\n",
      " [-0.85425893]\n",
      " [-0.83306674]\n",
      " ...\n",
      " [-0.57390547]\n",
      " [-0.11217231]\n",
      " [-0.57295875]]\n",
      "t [[-0.05859353]\n",
      " [-0.9224963 ]\n",
      " [-0.87255027]\n",
      " ...\n",
      " [-0.59917465]\n",
      " [-0.13936976]\n",
      " [-0.60303925]]\n",
      "loss=34369.89784037156\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01164711]\n",
      " [-0.07368685]\n",
      " [-0.26606884]\n",
      " ...\n",
      " [-0.19860998]\n",
      " [ 0.04222057]\n",
      " [-0.11239933]]\n",
      "t [[ 0.01164711]\n",
      " [-0.07368685]\n",
      " [-0.26606884]\n",
      " ...\n",
      " [-0.19860998]\n",
      " [ 0.04222057]\n",
      " [-0.11239933]]\n",
      "t [[ 0.00620899]\n",
      " [-0.17507332]\n",
      " [-0.41610004]\n",
      " ...\n",
      " [-0.30172947]\n",
      " [ 0.05085933]\n",
      " [-0.19903366]]\n",
      "t [[ 0.00620899]\n",
      " [-0.17507332]\n",
      " [-0.41610004]\n",
      " ...\n",
      " [-0.30172947]\n",
      " [ 0.05085933]\n",
      " [-0.19903366]]\n",
      "Current iteration=2, loss=37432.74528712909\n",
      "t [[-0.00465189]\n",
      " [-0.28097583]\n",
      " [-0.51602634]\n",
      " ...\n",
      " [-0.36486819]\n",
      " [ 0.04336606]\n",
      " [-0.26968537]]\n",
      "t [[-0.00465189]\n",
      " [-0.28097583]\n",
      " [-0.51602634]\n",
      " ...\n",
      " [-0.36486819]\n",
      " [ 0.04336606]\n",
      " [-0.26968537]]\n",
      "t [[-0.01663287]\n",
      " [-0.38318549]\n",
      " [-0.59204843]\n",
      " ...\n",
      " [-0.41004665]\n",
      " [ 0.02720385]\n",
      " [-0.32915299]]\n",
      "t [[-0.01663287]\n",
      " [-0.38318549]\n",
      " [-0.59204843]\n",
      " ...\n",
      " [-0.41004665]\n",
      " [ 0.02720385]\n",
      " [-0.32915299]]\n",
      "Current iteration=4, loss=36163.528195785955\n",
      "t [[-0.02812397]\n",
      " [-0.47898046]\n",
      " [-0.65534343]\n",
      " ...\n",
      " [-0.44654714]\n",
      " [ 0.00604228]\n",
      " [-0.38015503]]\n",
      "t [[-0.02812397]\n",
      " [-0.47898046]\n",
      " [-0.65534343]\n",
      " ...\n",
      " [-0.44654714]\n",
      " [ 0.00604228]\n",
      " [-0.38015503]]\n",
      "t [[-0.03855947]\n",
      " [-0.56773181]\n",
      " [-0.71101713]\n",
      " ...\n",
      " [-0.4784914 ]\n",
      " [-0.01805282]\n",
      " [-0.42443922]]\n",
      "t [[-0.03855947]\n",
      " [-0.56773181]\n",
      " [-0.71101713]\n",
      " ...\n",
      " [-0.4784914 ]\n",
      " [-0.01805282]\n",
      " [-0.42443922]]\n",
      "Current iteration=6, loss=35375.899114149004\n",
      "t [[-0.04780142]\n",
      " [-0.64962888]\n",
      " [-0.76152251]\n",
      " ...\n",
      " [-0.50775495]\n",
      " [-0.04380547]\n",
      " [-0.46322938]]\n",
      "t [[-0.04780142]\n",
      " [-0.64962888]\n",
      " [-0.76152251]\n",
      " ...\n",
      " [-0.50775495]\n",
      " [-0.04380547]\n",
      " [-0.46322938]]\n",
      "t [[-0.05588503]\n",
      " [-0.72515883]\n",
      " [-0.80810169]\n",
      " ...\n",
      " [-0.53520107]\n",
      " [-0.07037855]\n",
      " [-0.49743329]]\n",
      "t [[-0.05588503]\n",
      " [-0.72515883]\n",
      " [-0.80810169]\n",
      " ...\n",
      " [-0.53520107]\n",
      " [-0.07037855]\n",
      " [-0.49743329]]\n",
      "Current iteration=8, loss=34842.85479181008\n",
      "t [[-0.06290899]\n",
      " [-0.79488801]\n",
      " [-0.85142989]\n",
      " ...\n",
      " [-0.5612317 ]\n",
      " [-0.09720237]\n",
      " [-0.52775145]]\n",
      "t [[-0.06290899]\n",
      " [-0.79488801]\n",
      " [-0.85142989]\n",
      " ...\n",
      " [-0.5612317 ]\n",
      " [-0.09720237]\n",
      " [-0.52775145]]\n",
      "t [[-0.06898832]\n",
      " [-0.85937225]\n",
      " [-0.89191155]\n",
      " ...\n",
      " [-0.58603948]\n",
      " [-0.12388175]\n",
      " [-0.55473982]]\n",
      "loss=34462.134996564426\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0096997 ]\n",
      " [-0.07804709]\n",
      " [-0.26431323]\n",
      " ...\n",
      " [-0.11637033]\n",
      " [-0.01401534]\n",
      " [-0.2374143 ]]\n",
      "t [[ 0.0096997 ]\n",
      " [-0.07804709]\n",
      " [-0.26431323]\n",
      " ...\n",
      " [-0.11637033]\n",
      " [-0.01401534]\n",
      " [-0.2374143 ]]\n",
      "t [[ 0.00265352]\n",
      " [-0.18357742]\n",
      " [-0.41165088]\n",
      " ...\n",
      " [-0.15777525]\n",
      " [-0.00321323]\n",
      " [-0.43097406]]\n",
      "t [[ 0.00265352]\n",
      " [-0.18357742]\n",
      " [-0.41165088]\n",
      " ...\n",
      " [-0.15777525]\n",
      " [-0.00321323]\n",
      " [-0.43097406]]\n",
      "Current iteration=2, loss=37383.65430052473\n",
      "t [[-0.00952768]\n",
      " [-0.29329478]\n",
      " [-0.50878288]\n",
      " ...\n",
      " [-0.17103371]\n",
      " [ 0.01353001]\n",
      " [-0.59536508]]\n",
      "t [[-0.00952768]\n",
      " [-0.29329478]\n",
      " [-0.50878288]\n",
      " ...\n",
      " [-0.17103371]\n",
      " [ 0.01353001]\n",
      " [-0.59536508]]\n",
      "t [[-0.02260576]\n",
      " [-0.39899867]\n",
      " [-0.58214954]\n",
      " ...\n",
      " [-0.17404227]\n",
      " [ 0.02965415]\n",
      " [-0.73850646]]\n",
      "t [[-0.02260576]\n",
      " [-0.39899867]\n",
      " [-0.58214954]\n",
      " ...\n",
      " [-0.17404227]\n",
      " [ 0.02965415]\n",
      " [-0.73850646]]\n",
      "Current iteration=4, loss=36091.65139031074\n",
      "t [[-0.03502338]\n",
      " [-0.49799146]\n",
      " [-0.64299715]\n",
      " ...\n",
      " [-0.17389441]\n",
      " [ 0.04309442]\n",
      " [-0.86518824]]\n",
      "t [[-0.03502338]\n",
      " [-0.49799146]\n",
      " [-0.64299715]\n",
      " ...\n",
      " [-0.17389441]\n",
      " [ 0.04309442]\n",
      " [-0.86518824]]\n",
      "t [[-0.04625403]\n",
      " [-0.58967003]\n",
      " [-0.69644431]\n",
      " ...\n",
      " [-0.17340768]\n",
      " [ 0.05347315]\n",
      " [-0.9786141 ]]\n",
      "t [[-0.04625403]\n",
      " [-0.58967003]\n",
      " [-0.69644431]\n",
      " ...\n",
      " [-0.17340768]\n",
      " [ 0.05347315]\n",
      " [-0.9786141 ]]\n",
      "Current iteration=6, loss=35288.48178208065\n",
      "t [[-0.05618845]\n",
      " [-0.67424941]\n",
      " [-0.74493489]\n",
      " ...\n",
      " [-0.17360628]\n",
      " [ 0.06103629]\n",
      " [-1.08107919]]\n",
      "t [[-0.05618845]\n",
      " [-0.67424941]\n",
      " [-0.74493489]\n",
      " ...\n",
      " [-0.17360628]\n",
      " [ 0.06103629]\n",
      " [-1.08107919]]\n",
      "t [[-0.06488276]\n",
      " [-0.7522414 ]\n",
      " [-0.78969367]\n",
      " ...\n",
      " [-0.17475693]\n",
      " [ 0.06622283]\n",
      " [-1.17430763]]\n",
      "t [[-0.06488276]\n",
      " [-0.7522414 ]\n",
      " [-0.78969367]\n",
      " ...\n",
      " [-0.17475693]\n",
      " [ 0.06622283]\n",
      " [-1.17430763]]\n",
      "Current iteration=8, loss=34744.59422639273\n",
      "t [[-0.07245093]\n",
      " [-0.8242352 ]\n",
      " [-0.83137626]\n",
      " ...\n",
      " [-0.17682037]\n",
      " [ 0.0694907 ]\n",
      " [-1.25963899]]\n",
      "t [[-0.07245093]\n",
      " [-0.8242352 ]\n",
      " [-0.83137626]\n",
      " ...\n",
      " [-0.17682037]\n",
      " [ 0.0694907 ]\n",
      " [-1.25963899]]\n",
      "t [[-0.07901932]\n",
      " [-0.89080738]\n",
      " [-0.87036805]\n",
      " ...\n",
      " [-0.17964958]\n",
      " [ 0.07125201]\n",
      " [-1.33814037]]\n",
      "loss=34356.233060129125\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.10612729]\n",
      " [-0.45336363]\n",
      " [-0.28495536]\n",
      " ...\n",
      " [-0.206522  ]\n",
      " [ 0.04471315]\n",
      " [-0.12039147]]\n",
      "t [[ 0.10612729]\n",
      " [-0.45336363]\n",
      " [-0.28495536]\n",
      " ...\n",
      " [-0.206522  ]\n",
      " [ 0.04471315]\n",
      " [-0.12039147]]\n",
      "t [[ 0.15939265]\n",
      " [-0.72530716]\n",
      " [-0.4506944 ]\n",
      " ...\n",
      " [-0.31329526]\n",
      " [ 0.05359304]\n",
      " [-0.21332673]]\n",
      "t [[ 0.15939265]\n",
      " [-0.72530716]\n",
      " [-0.4506944 ]\n",
      " ...\n",
      " [-0.31329526]\n",
      " [ 0.05359304]\n",
      " [-0.21332673]]\n",
      "Current iteration=2, loss=37384.43439231336\n",
      "t [[ 0.18808806]\n",
      " [-0.91245244]\n",
      " [-0.56421575]\n",
      " ...\n",
      " [-0.37924502]\n",
      " [ 0.04545458]\n",
      " [-0.28927143]]\n",
      "t [[ 0.18808806]\n",
      " [-0.91245244]\n",
      " [-0.56421575]\n",
      " ...\n",
      " [-0.37924502]\n",
      " [ 0.04545458]\n",
      " [-0.28927143]]\n",
      "t [[ 0.20424056]\n",
      " [-1.05400604]\n",
      " [-0.65174936]\n",
      " ...\n",
      " [-0.42709744]\n",
      " [ 0.02823813]\n",
      " [-0.35329974]]\n",
      "t [[ 0.20424056]\n",
      " [-1.05400604]\n",
      " [-0.65174936]\n",
      " ...\n",
      " [-0.42709744]\n",
      " [ 0.02823813]\n",
      " [-0.35329974]]\n",
      "Current iteration=4, loss=36109.41521561704\n",
      "t [[ 0.21346815]\n",
      " [-1.16777198]\n",
      " [-0.72452692]\n",
      " ...\n",
      " [-0.46622358]\n",
      " [ 0.00584128]\n",
      " [-0.40828089]]\n",
      "t [[ 0.21346815]\n",
      " [-1.16777198]\n",
      " [-0.72452692]\n",
      " ...\n",
      " [-0.46622358]\n",
      " [ 0.00584128]\n",
      " [-0.40828089]]\n",
      "t [[ 0.2186403 ]\n",
      " [-1.26281467]\n",
      " [-0.78779937]\n",
      " ...\n",
      " [-0.50070232]\n",
      " [-0.01954047]\n",
      " [-0.45606369]]\n",
      "t [[ 0.2186403 ]\n",
      " [-1.26281467]\n",
      " [-0.78779937]\n",
      " ...\n",
      " [-0.50070232]\n",
      " [-0.01954047]\n",
      " [-0.45606369]]\n",
      "Current iteration=6, loss=35322.80691723179\n",
      "t [[ 0.22132714]\n",
      " [-1.34421652]\n",
      " [-0.84421204]\n",
      " ...\n",
      " [-0.53235633]\n",
      " [-0.04655391]\n",
      " [-0.49794738]]\n",
      "t [[ 0.22132714]\n",
      " [-1.34421652]\n",
      " [-0.84421204]\n",
      " ...\n",
      " [-0.53235633]\n",
      " [-0.04655391]\n",
      " [-0.49794738]]\n",
      "t [[ 0.22243983]\n",
      " [-1.41508993]\n",
      " [-0.8952113 ]\n",
      " ...\n",
      " [-0.5620137 ]\n",
      " [-0.07431682]\n",
      " [-0.53489896]]\n",
      "t [[ 0.22243983]\n",
      " [-1.41508993]\n",
      " [-0.8952113 ]\n",
      " ...\n",
      " [-0.5620137 ]\n",
      " [-0.07431682]\n",
      " [-0.53489896]]\n",
      "Current iteration=8, loss=34792.95333152519\n",
      "t [[ 0.22253522]\n",
      " [-1.47749928]\n",
      " [-0.94166838]\n",
      " ...\n",
      " [-0.59006004]\n",
      " [-0.10223499]\n",
      " [-0.56766688]]\n",
      "t [[ 0.22253522]\n",
      " [-1.47749928]\n",
      " [-0.94166838]\n",
      " ...\n",
      " [-0.59006004]\n",
      " [-0.10223499]\n",
      " [-0.56766688]]\n",
      "t [[ 0.221969  ]\n",
      " [-1.53290962]\n",
      " [-0.9841661 ]\n",
      " ...\n",
      " [-0.61668551]\n",
      " [-0.1299021 ]\n",
      " [-0.59684653]]\n",
      "loss=34415.79107816918\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01399539]\n",
      " [-0.08616775]\n",
      " [-0.27014489]\n",
      " ...\n",
      " [-0.20730333]\n",
      " [ 0.04030118]\n",
      " [-0.12298153]]\n",
      "t [[ 0.01399539]\n",
      " [-0.08616775]\n",
      " [-0.27014489]\n",
      " ...\n",
      " [-0.20730333]\n",
      " [ 0.04030118]\n",
      " [-0.12298153]]\n",
      "t [[ 0.0098409 ]\n",
      " [-0.20014982]\n",
      " [-0.41794975]\n",
      " ...\n",
      " [-0.31273348]\n",
      " [ 0.04563149]\n",
      " [-0.21789024]]\n",
      "t [[ 0.0098409 ]\n",
      " [-0.20014982]\n",
      " [-0.41794975]\n",
      " ...\n",
      " [-0.31273348]\n",
      " [ 0.04563149]\n",
      " [-0.21789024]]\n",
      "Current iteration=2, loss=37348.78931530878\n",
      "t [[ 3.31904262e-05]\n",
      " [-3.17088782e-01]\n",
      " [-5.15139504e-01]\n",
      " ...\n",
      " [-3.77066909e-01]\n",
      " [ 3.48381009e-02]\n",
      " [-2.95512945e-01]]\n",
      "t [[ 3.31904262e-05]\n",
      " [-3.17088782e-01]\n",
      " [-5.15139504e-01]\n",
      " ...\n",
      " [-3.77066909e-01]\n",
      " [ 3.48381009e-02]\n",
      " [-2.95512945e-01]]\n",
      "t [[-0.01100367]\n",
      " [-0.42874113]\n",
      " [-0.58894859]\n",
      " ...\n",
      " [-0.42338286]\n",
      " [ 0.01567562]\n",
      " [-0.36102068]]\n",
      "t [[-0.01100367]\n",
      " [-0.42874113]\n",
      " [-0.58894859]\n",
      " ...\n",
      " [-0.42338286]\n",
      " [ 0.01567562]\n",
      " [-0.36102068]]\n",
      "Current iteration=4, loss=36050.42366785055\n",
      "t [[-0.02165985]\n",
      " [-0.53262633]\n",
      " [-0.65059384]\n",
      " ...\n",
      " [-0.46109679]\n",
      " [-0.00812889]\n",
      " [-0.41732571]]\n",
      "t [[-0.02165985]\n",
      " [-0.53262633]\n",
      " [-0.65059384]\n",
      " ...\n",
      " [-0.46109679]\n",
      " [-0.00812889]\n",
      " [-0.41732571]]\n",
      "t [[-0.0313809 ]\n",
      " [-0.62836398]\n",
      " [-0.70503725]\n",
      " ...\n",
      " [-0.4942721 ]\n",
      " [-0.03451323]\n",
      " [-0.46630325]]\n",
      "t [[-0.0313809 ]\n",
      " [-0.62836398]\n",
      " [-0.70503725]\n",
      " ...\n",
      " [-0.4942721 ]\n",
      " [-0.03451323]\n",
      " [-0.46630325]]\n",
      "Current iteration=6, loss=35248.41346951995\n",
      "t [[-0.04003212]\n",
      " [-0.71634859]\n",
      " [-0.75458987]\n",
      " ...\n",
      " [-0.52471137]\n",
      " [-0.06222541]\n",
      " [-0.50927171]]\n",
      "t [[-0.04003212]\n",
      " [-0.71634859]\n",
      " [-0.75458987]\n",
      " ...\n",
      " [-0.52471137]\n",
      " [-0.06222541]\n",
      " [-0.50927171]]\n",
      "t [[-0.0476436 ]\n",
      " [-0.79722721]\n",
      " [-0.80039009]\n",
      " ...\n",
      " [-0.55322881]\n",
      " [-0.09045954]\n",
      " [-0.54721325]]\n",
      "t [[-0.0476436 ]\n",
      " [-0.79722721]\n",
      " [-0.80039009]\n",
      " ...\n",
      " [-0.55322881]\n",
      " [-0.09045954]\n",
      " [-0.54721325]]\n",
      "Current iteration=8, loss=34708.741998628364\n",
      "t [[-0.05430375]\n",
      " [-0.87168948]\n",
      " [-0.84304511]\n",
      " ...\n",
      " [-0.58020209]\n",
      " [-0.11867897]\n",
      " [-0.58088865]]\n",
      "t [[-0.05430375]\n",
      " [-0.87168948]\n",
      " [-0.84304511]\n",
      " ...\n",
      " [-0.58020209]\n",
      " [-0.11867897]\n",
      " [-0.58088865]]\n",
      "t [[-0.06011496]\n",
      " [-0.94038703]\n",
      " [-0.88291726]\n",
      " ...\n",
      " [-0.60581722]\n",
      " [-0.14652093]\n",
      " [-0.61090351]]\n",
      "loss=34325.233640157734\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01195361]\n",
      " [-0.07562598]\n",
      " [-0.27307065]\n",
      " ...\n",
      " [-0.20383656]\n",
      " [ 0.04333164]\n",
      " [-0.1153572 ]]\n",
      "t [[ 0.01195361]\n",
      " [-0.07562598]\n",
      " [-0.27307065]\n",
      " ...\n",
      " [-0.20383656]\n",
      " [ 0.04333164]\n",
      " [-0.1153572 ]]\n",
      "t [[ 0.00593158]\n",
      " [-0.18039867]\n",
      " [-0.42401567]\n",
      " ...\n",
      " [-0.307172  ]\n",
      " [ 0.05132501]\n",
      " [-0.20359304]]\n",
      "t [[ 0.00593158]\n",
      " [-0.18039867]\n",
      " [-0.42401567]\n",
      " ...\n",
      " [-0.307172  ]\n",
      " [ 0.05132501]\n",
      " [-0.20359304]]\n",
      "Current iteration=2, loss=37384.53830860591\n",
      "t [[-0.00549668]\n",
      " [-0.28932012]\n",
      " [-0.52393847]\n",
      " ...\n",
      " [-0.36987268]\n",
      " [ 0.04278739]\n",
      " [-0.27525935]]\n",
      "t [[-0.00549668]\n",
      " [-0.28932012]\n",
      " [-0.52393847]\n",
      " ...\n",
      " [-0.36987268]\n",
      " [ 0.04278739]\n",
      " [-0.27525935]]\n",
      "t [[-0.01788153]\n",
      " [-0.39392652]\n",
      " [-0.60006196]\n",
      " ...\n",
      " [-0.41481442]\n",
      " [ 0.02550872]\n",
      " [-0.33540141]]\n",
      "t [[-0.01788153]\n",
      " [-0.39392652]\n",
      " [-0.60006196]\n",
      " ...\n",
      " [-0.41481442]\n",
      " [ 0.02550872]\n",
      " [-0.33540141]]\n",
      "Current iteration=4, loss=36109.64244382808\n",
      "t [[-0.02962311]\n",
      " [-0.49156296]\n",
      " [-0.66367402]\n",
      " ...\n",
      " [-0.45135691]\n",
      " [ 0.00325786]\n",
      " [-0.38684654]]\n",
      "t [[-0.02962311]\n",
      " [-0.49156296]\n",
      " [-0.66367402]\n",
      " ...\n",
      " [-0.45135691]\n",
      " [ 0.00325786]\n",
      " [-0.38684654]]\n",
      "t [[-0.04019316]\n",
      " [-0.58171623]\n",
      " [-0.71980465]\n",
      " ...\n",
      " [-0.48354   ]\n",
      " [-0.02186073]\n",
      " [-0.43140479]]\n",
      "t [[-0.04019316]\n",
      " [-0.58171623]\n",
      " [-0.71980465]\n",
      " ...\n",
      " [-0.48354   ]\n",
      " [-0.02186073]\n",
      " [-0.43140479]]\n",
      "Current iteration=6, loss=35323.55177318241\n",
      "t [[-0.04948824]\n",
      " [-0.66467791]\n",
      " [-0.77081873]\n",
      " ...\n",
      " [-0.5131488 ]\n",
      " [-0.04855364]\n",
      " [-0.47034153]]\n",
      "t [[-0.04948824]\n",
      " [-0.66467791]\n",
      " [-0.77081873]\n",
      " ...\n",
      " [-0.5131488 ]\n",
      " [-0.04855364]\n",
      " [-0.47034153]]\n",
      "t [[-0.05756942]\n",
      " [-0.74101355]\n",
      " [-0.81789564]\n",
      " ...\n",
      " [-0.54098037]\n",
      " [-0.075976  ]\n",
      " [-0.50459447]]\n",
      "t [[-0.05756942]\n",
      " [-0.74101355]\n",
      " [-0.81789564]\n",
      " ...\n",
      " [-0.54098037]\n",
      " [-0.075976  ]\n",
      " [-0.50459447]]\n",
      "Current iteration=8, loss=34794.560660946765\n",
      "t [[-0.06455335]\n",
      " [-0.8113471 ]\n",
      " [-0.86167388]\n",
      " ...\n",
      " [-0.56739516]\n",
      " [-0.10355579]\n",
      " [-0.53488662]]\n",
      "t [[-0.06455335]\n",
      " [-0.8113471 ]\n",
      " [-0.86167388]\n",
      " ...\n",
      " [-0.56739516]\n",
      " [-0.10355579]\n",
      " [-0.53488662]]\n",
      "t [[-0.07056723]\n",
      " [-0.8762765 ]\n",
      " [-0.90253993]\n",
      " ...\n",
      " [-0.59256251]\n",
      " [-0.13089914]\n",
      " [-0.56179116]]\n",
      "loss=34418.304542630154\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00995495]\n",
      " [-0.08010096]\n",
      " [-0.27126885]\n",
      " ...\n",
      " [-0.1194327 ]\n",
      " [-0.01438417]\n",
      " [-0.24366205]]\n",
      "t [[ 0.00995495]\n",
      " [-0.08010096]\n",
      " [-0.27126885]\n",
      " ...\n",
      " [-0.1194327 ]\n",
      " [-0.01438417]\n",
      " [-0.24366205]]\n",
      "t [[ 0.00229151]\n",
      " [-0.18911953]\n",
      " [-0.41942666]\n",
      " ...\n",
      " [-0.15996818]\n",
      " [-0.00264755]\n",
      " [-0.44115747]]\n",
      "t [[ 0.00229151]\n",
      " [-0.18911953]\n",
      " [-0.41942666]\n",
      " ...\n",
      " [-0.15996818]\n",
      " [-0.00264755]\n",
      " [-0.44115747]]\n",
      "Current iteration=2, loss=37334.642212550374\n",
      "t [[-0.01047656]\n",
      " [-0.30193884]\n",
      " [-0.51647654]\n",
      " ...\n",
      " [-0.17209704]\n",
      " [ 0.01484953]\n",
      " [-0.60833844]]\n",
      "t [[-0.01047656]\n",
      " [-0.30193884]\n",
      " [-0.51647654]\n",
      " ...\n",
      " [-0.17209704]\n",
      " [ 0.01484953]\n",
      " [-0.60833844]]\n",
      "t [[-0.02396961]\n",
      " [-0.41010644]\n",
      " [-0.5898852 ]\n",
      " ...\n",
      " [-0.17436897]\n",
      " [ 0.0313513 ]\n",
      " [-0.75356495]]\n",
      "t [[-0.02396961]\n",
      " [-0.41010644]\n",
      " [-0.5898852 ]\n",
      " ...\n",
      " [-0.17436897]\n",
      " [ 0.0313513 ]\n",
      " [-0.75356495]]\n",
      "Current iteration=4, loss=36036.74592681246\n",
      "t [[-0.03664393]\n",
      " [-0.51099387]\n",
      " [-0.65100707]\n",
      " ...\n",
      " [-0.17388181]\n",
      " [ 0.04486277]\n",
      " [-0.88184255]]\n",
      "t [[-0.03664393]\n",
      " [-0.51099387]\n",
      " [-0.65100707]\n",
      " ...\n",
      " [-0.17388181]\n",
      " [ 0.04486277]\n",
      " [-0.88184255]]\n",
      "t [[-0.04801256]\n",
      " [-0.60411591]\n",
      " [-0.70488158]\n",
      " ...\n",
      " [-0.17333754]\n",
      " [ 0.05511074]\n",
      " [-0.99650315]]\n",
      "t [[-0.04801256]\n",
      " [-0.60411591]\n",
      " [-0.70488158]\n",
      " ...\n",
      " [-0.17333754]\n",
      " [ 0.05511074]\n",
      " [-0.99650315]]\n",
      "Current iteration=6, loss=35235.07719915636\n",
      "t [[-0.05800193]\n",
      " [-0.68979185]\n",
      " [-0.7538614 ]\n",
      " ...\n",
      " [-0.17365027]\n",
      " [ 0.06242745]\n",
      " [-1.09992747]]\n",
      "t [[-0.05800193]\n",
      " [-0.68979185]\n",
      " [-0.7538614 ]\n",
      " ...\n",
      " [-0.17365027]\n",
      " [ 0.06242745]\n",
      " [-1.09992747]]\n",
      "t [[-0.06669465]\n",
      " [-0.76861372]\n",
      " [-0.79910601]\n",
      " ...\n",
      " [-0.17500691]\n",
      " [ 0.06731243]\n",
      " [-1.19390055]]\n",
      "t [[-0.06669465]\n",
      " [-0.76861372]\n",
      " [-0.79910601]\n",
      " ...\n",
      " [-0.17500691]\n",
      " [ 0.06731243]\n",
      " [-1.19390055]]\n",
      "Current iteration=8, loss=34695.3160586787\n",
      "t [[-0.07422298]\n",
      " [-0.84122986]\n",
      " [-0.84123237]\n",
      " ...\n",
      " [-0.17731639]\n",
      " [ 0.07026287]\n",
      " [-1.27980706]]\n",
      "t [[-0.07422298]\n",
      " [-0.84122986]\n",
      " [-0.84123237]\n",
      " ...\n",
      " [-0.17731639]\n",
      " [ 0.07026287]\n",
      " [-1.27980706]]\n",
      "t [[-0.08072558]\n",
      " [-0.90826019]\n",
      " [-0.88060648]\n",
      " ...\n",
      " [-0.18040023]\n",
      " [ 0.07171484]\n",
      " [-1.35874824]]\n",
      "loss=34311.53415055144\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.10884851]\n",
      " [-0.46498834]\n",
      " [-0.29226191]\n",
      " ...\n",
      " [-0.21181744]\n",
      " [ 0.04585964]\n",
      " [-0.12347843]]\n",
      "t [[ 0.10884851]\n",
      " [-0.46498834]\n",
      " [-0.29226191]\n",
      " ...\n",
      " [-0.21181744]\n",
      " [ 0.04585964]\n",
      " [-0.12347843]]\n",
      "t [[ 0.16213403]\n",
      " [-0.73929116]\n",
      " [-0.45922107]\n",
      " ...\n",
      " [-0.31879118]\n",
      " [ 0.05406232]\n",
      " [-0.21809295]]\n",
      "t [[ 0.16213403]\n",
      " [-0.73929116]\n",
      " [-0.45922107]\n",
      " ...\n",
      " [-0.31879118]\n",
      " [ 0.05406232]\n",
      " [-0.21809295]]\n",
      "Current iteration=2, loss=37337.06535015649\n",
      "t [[ 0.19031248]\n",
      " [-0.9268914 ]\n",
      " [-0.57297863]\n",
      " ...\n",
      " [-0.38434222]\n",
      " [ 0.0448446 ]\n",
      " [-0.29511043]]\n",
      "t [[ 0.19031248]\n",
      " [-0.9268914 ]\n",
      " [-0.57297863]\n",
      " ...\n",
      " [-0.38434222]\n",
      " [ 0.0448446 ]\n",
      " [-0.29511043]]\n",
      "t [[ 0.2059156 ]\n",
      " [-1.06854439]\n",
      " [-0.66074307]\n",
      " ...\n",
      " [-0.43202027]\n",
      " [ 0.02648129]\n",
      " [-0.35985656]]\n",
      "t [[ 0.2059156 ]\n",
      " [-1.06854439]\n",
      " [-0.66074307]\n",
      " ...\n",
      " [-0.43202027]\n",
      " [ 0.02648129]\n",
      " [-0.35985656]]\n",
      "Current iteration=4, loss=36056.78623529858\n",
      "t [[ 0.21467143]\n",
      " [-1.18235775]\n",
      " [-0.73386174]\n",
      " ...\n",
      " [-0.47124837]\n",
      " [ 0.00297251]\n",
      " [-0.41531194]]\n",
      "t [[ 0.21467143]\n",
      " [-1.18235775]\n",
      " [-0.73386174]\n",
      " ...\n",
      " [-0.47124837]\n",
      " [ 0.00297251]\n",
      " [-0.41531194]]\n",
      "t [[ 0.21945921]\n",
      " [-1.27742351]\n",
      " [-0.79753018]\n",
      " ...\n",
      " [-0.50601205]\n",
      " [-0.02344618]\n",
      " [-0.46339   ]]\n",
      "t [[ 0.21945921]\n",
      " [-1.27742351]\n",
      " [-0.79753018]\n",
      " ...\n",
      " [-0.50601205]\n",
      " [-0.02344618]\n",
      " [-0.46339   ]]\n",
      "Current iteration=6, loss=35271.958011517316\n",
      "t [[ 0.22183597]\n",
      " [-1.35880378]\n",
      " [-0.85432761]\n",
      " ...\n",
      " [-0.53804069]\n",
      " [-0.05140416]\n",
      " [-0.50543353]]\n",
      "t [[ 0.22183597]\n",
      " [-1.35880378]\n",
      " [-0.85432761]\n",
      " ...\n",
      " [-0.53804069]\n",
      " [-0.05140416]\n",
      " [-0.50543353]]\n",
      "t [[ 0.22269805]\n",
      " [-1.42959568]\n",
      " [-0.90565651]\n",
      " ...\n",
      " [-0.5680974 ]\n",
      " [-0.08001233]\n",
      " [-0.54244122]]\n",
      "t [[ 0.22269805]\n",
      " [-1.42959568]\n",
      " [-0.90565651]\n",
      " ...\n",
      " [-0.5680974 ]\n",
      " [-0.08001233]\n",
      " [-0.54244122]]\n",
      "Current iteration=8, loss=34746.23102210515\n",
      "t [[ 0.22259042]\n",
      " [-1.49186024]\n",
      " [-0.95236612]\n",
      " ...\n",
      " [-0.59652899]\n",
      " [-0.10867558]\n",
      " [-0.57518538]]\n",
      "t [[ 0.22259042]\n",
      " [-1.49186024]\n",
      " [-0.95236612]\n",
      " ...\n",
      " [-0.59652899]\n",
      " [-0.10867558]\n",
      " [-0.57518538]]\n",
      "t [[ 0.22185991]\n",
      " [-1.54706739]\n",
      " [-0.99503229]\n",
      " ...\n",
      " [-0.62350518]\n",
      " [-0.13699039]\n",
      " [-0.60427971]]\n",
      "loss=34373.50897681771\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01435425]\n",
      " [-0.08837718]\n",
      " [-0.27707168]\n",
      " ...\n",
      " [-0.2126188 ]\n",
      " [ 0.04133455]\n",
      " [-0.1261349 ]]\n",
      "t [[ 0.01435425]\n",
      " [-0.08837718]\n",
      " [-0.27707168]\n",
      " ...\n",
      " [-0.2126188 ]\n",
      " [ 0.04133455]\n",
      " [-0.1261349 ]]\n",
      "t [[ 0.00963854]\n",
      " [-0.20598047]\n",
      " [-0.42555828]\n",
      " ...\n",
      " [-0.31816255]\n",
      " [ 0.0459196 ]\n",
      " [-0.22275801]]\n",
      "t [[ 0.00963854]\n",
      " [-0.20598047]\n",
      " [-0.42555828]\n",
      " ...\n",
      " [-0.31816255]\n",
      " [ 0.0459196 ]\n",
      " [-0.22275801]]\n",
      "Current iteration=2, loss=37300.7060435969\n",
      "t [[-0.00070713]\n",
      " [-0.32606519]\n",
      " [-0.52264676]\n",
      " ...\n",
      " [-0.38204102]\n",
      " [ 0.03402446]\n",
      " [-0.30148141]]\n",
      "t [[-0.00070713]\n",
      " [-0.32606519]\n",
      " [-0.52264676]\n",
      " ...\n",
      " [-0.38204102]\n",
      " [ 0.03402446]\n",
      " [-0.30148141]]\n",
      "t [[-0.01212241]\n",
      " [-0.44017415]\n",
      " [-0.59653557]\n",
      " ...\n",
      " [-0.42814872]\n",
      " [ 0.01371973]\n",
      " [-0.36772974]]\n",
      "t [[-0.01212241]\n",
      " [-0.44017415]\n",
      " [-0.59653557]\n",
      " ...\n",
      " [-0.42814872]\n",
      " [ 0.01371973]\n",
      " [-0.36772974]]\n",
      "Current iteration=4, loss=35996.76377374522\n",
      "t [[-0.02301312]\n",
      " [-0.54592371]\n",
      " [-0.65850391]\n",
      " ...\n",
      " [-0.46594108]\n",
      " [-0.01117738]\n",
      " [-0.42452724]]\n",
      "t [[-0.02301312]\n",
      " [-0.54592371]\n",
      " [-0.65850391]\n",
      " ...\n",
      " [-0.46594108]\n",
      " [-0.01117738]\n",
      " [-0.42452724]]\n",
      "t [[-0.03286302]\n",
      " [-0.64306601]\n",
      " [-0.7134141 ]\n",
      " ...\n",
      " [-0.49938198]\n",
      " [-0.03857224]\n",
      " [-0.47381436]]\n",
      "t [[-0.03286302]\n",
      " [-0.64306601]\n",
      " [-0.7134141 ]\n",
      " ...\n",
      " [-0.49938198]\n",
      " [-0.03857224]\n",
      " [-0.47381436]]\n",
      "Current iteration=6, loss=35196.57849091466\n",
      "t [[-0.04157048]\n",
      " [-0.73210662]\n",
      " [-0.76348028]\n",
      " ...\n",
      " [-0.53017852]\n",
      " [-0.0672    ]\n",
      " [-0.51695377]]\n",
      "t [[-0.04157048]\n",
      " [-0.73210662]\n",
      " [-0.76348028]\n",
      " ...\n",
      " [-0.53017852]\n",
      " [-0.0672    ]\n",
      " [-0.51695377]]\n",
      "t [[-0.0491893 ]\n",
      " [-0.81377547]\n",
      " [-0.80977646]\n",
      " ...\n",
      " [-0.55907964]\n",
      " [-0.09625062]\n",
      " [-0.55495992]]\n",
      "t [[-0.0491893 ]\n",
      " [-0.81377547]\n",
      " [-0.80977646]\n",
      " ...\n",
      " [-0.55907964]\n",
      " [-0.09625062]\n",
      " [-0.55495992]]\n",
      "Current iteration=8, loss=34661.1801407282\n",
      "t [[-0.05582376]\n",
      " [-0.88882215]\n",
      " [-0.85287436]\n",
      " ...\n",
      " [-0.58642453]\n",
      " [-0.12518807]\n",
      " [-0.58861797]]\n",
      "t [[-0.05582376]\n",
      " [-0.88882215]\n",
      " [-0.85287436]\n",
      " ...\n",
      " [-0.58642453]\n",
      " [-0.12518807]\n",
      " [-0.58861797]]\n",
      "t [[-0.06158675]\n",
      " [-0.9579418 ]\n",
      " [-0.89312018]\n",
      " ...\n",
      " [-0.61237915]\n",
      " [-0.1536532 ]\n",
      " [-0.61855225]]\n",
      "loss=34282.269976841446\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01226012]\n",
      " [-0.0775651 ]\n",
      " [-0.28007246]\n",
      " ...\n",
      " [-0.20906314]\n",
      " [ 0.0444427 ]\n",
      " [-0.11831508]]\n",
      "t [[ 0.01226012]\n",
      " [-0.0775651 ]\n",
      " [-0.28007246]\n",
      " ...\n",
      " [-0.20906314]\n",
      " [ 0.0444427 ]\n",
      " [-0.11831508]]\n",
      "t [[ 0.00563308]\n",
      " [-0.1857586 ]\n",
      " [-0.43178316]\n",
      " ...\n",
      " [-0.31249254]\n",
      " [ 0.05174849]\n",
      " [-0.20811896]]\n",
      "t [[ 0.00563308]\n",
      " [-0.1857586 ]\n",
      " [-0.43178316]\n",
      " ...\n",
      " [-0.31249254]\n",
      " [ 0.05174849]\n",
      " [-0.20811896]]\n",
      "Current iteration=2, loss=37337.19759807292\n",
      "t [[-0.00636205]\n",
      " [-0.29768123]\n",
      " [-0.53165445]\n",
      " ...\n",
      " [-0.37472086]\n",
      " [ 0.04214573]\n",
      " [-0.28076998]]\n",
      "t [[-0.00636205]\n",
      " [-0.29768123]\n",
      " [-0.53165445]\n",
      " ...\n",
      " [-0.37472086]\n",
      " [ 0.04214573]\n",
      " [-0.28076998]]\n",
      "t [[-0.01913874]\n",
      " [-0.40463794]\n",
      " [-0.60788558]\n",
      " ...\n",
      " [-0.4194398 ]\n",
      " [ 0.02374433]\n",
      " [-0.34156033]]\n",
      "t [[-0.01913874]\n",
      " [-0.40463794]\n",
      " [-0.60788558]\n",
      " ...\n",
      " [-0.4194398 ]\n",
      " [ 0.02374433]\n",
      " [-0.34156033]]\n",
      "Current iteration=4, loss=36057.01971299314\n",
      "t [[-3.11154474e-02]\n",
      " [-5.04059957e-01]\n",
      " [-6.71835885e-01]\n",
      " ...\n",
      " [-4.56051940e-01]\n",
      " [ 4.06638672e-04]\n",
      " [-3.93424987e-01]]\n",
      "t [[-3.11154474e-02]\n",
      " [-5.04059957e-01]\n",
      " [-6.71835885e-01]\n",
      " ...\n",
      " [-4.56051940e-01]\n",
      " [ 4.06638672e-04]\n",
      " [-3.93424987e-01]]\n",
      "t [[-0.04180545]\n",
      " [-0.59555967]\n",
      " [-0.72844078]\n",
      " ...\n",
      " [-0.48849844]\n",
      " [-0.02572713]\n",
      " [-0.43823586]]\n",
      "t [[-0.04180545]\n",
      " [-0.59555967]\n",
      " [-0.72844078]\n",
      " ...\n",
      " [-0.48849844]\n",
      " [-0.02572713]\n",
      " [-0.43823586]]\n",
      "Current iteration=6, loss=35272.75794688814\n",
      "t [[-0.05114136]\n",
      " [-0.67953484]\n",
      " [-0.77997104]\n",
      " ...\n",
      " [-0.5184683 ]\n",
      " [-0.05334759]\n",
      " [-0.47729978]]\n",
      "t [[-0.05114136]\n",
      " [-0.67953484]\n",
      " [-0.77997104]\n",
      " ...\n",
      " [-0.5184683 ]\n",
      " [-0.05334759]\n",
      " [-0.47729978]]\n",
      "t [[-0.05921032]\n",
      " [-0.75663036]\n",
      " [-0.8275433 ]\n",
      " ...\n",
      " [-0.54669206]\n",
      " [-0.08160295]\n",
      " [-0.51158444]]\n",
      "t [[-0.05921032]\n",
      " [-0.75663036]\n",
      " [-0.8275433 ]\n",
      " ...\n",
      " [-0.54669206]\n",
      " [-0.08160295]\n",
      " [-0.51158444]]\n",
      "Current iteration=8, loss=34747.93174731243\n",
      "t [[-0.06614665]\n",
      " [-0.82752751]\n",
      " [-0.87176148]\n",
      " ...\n",
      " [-0.57349039]\n",
      " [-0.10991967]\n",
      " [-0.54183534]]\n",
      "t [[-0.06614665]\n",
      " [-0.82752751]\n",
      " [-0.87176148]\n",
      " ...\n",
      " [-0.57349039]\n",
      " [-0.10991967]\n",
      " [-0.54183534]]\n",
      "t [[-0.0720893 ]\n",
      " [-0.89286575]\n",
      " [-0.91299656]\n",
      " ...\n",
      " [-0.59901137]\n",
      " [-0.13790588]\n",
      " [-0.56864284]]\n",
      "loss=34376.136159712325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01021021]\n",
      " [-0.08215483]\n",
      " [-0.27822446]\n",
      " ...\n",
      " [-0.12249508]\n",
      " [-0.01475299]\n",
      " [-0.24990979]]\n",
      "t [[ 0.01021021]\n",
      " [-0.08215483]\n",
      " [-0.27822446]\n",
      " ...\n",
      " [-0.12249508]\n",
      " [-0.01475299]\n",
      " [-0.24990979]]\n",
      "t [[ 0.00190888]\n",
      " [-0.19469579]\n",
      " [-0.42705333]\n",
      " ...\n",
      " [-0.16206557]\n",
      " [-0.00205004]\n",
      " [-0.45128352]]\n",
      "t [[ 0.00190888]\n",
      " [-0.19469579]\n",
      " [-0.42705333]\n",
      " ...\n",
      " [-0.16206557]\n",
      " [-0.00205004]\n",
      " [-0.45128352]]\n",
      "Current iteration=2, loss=37286.5094193066\n",
      "t [[-0.01144486]\n",
      " [-0.31059841]\n",
      " [-0.52397374]\n",
      " ...\n",
      " [-0.17305028]\n",
      " [ 0.01619251]\n",
      " [-0.62119702]]\n",
      "t [[-0.01144486]\n",
      " [-0.31059841]\n",
      " [-0.52397374]\n",
      " ...\n",
      " [-0.17305028]\n",
      " [ 0.01619251]\n",
      " [-0.62119702]]\n",
      "t [[-0.02534021]\n",
      " [-0.42118209]\n",
      " [-0.59743199]\n",
      " ...\n",
      " [-0.1746141 ]\n",
      " [ 0.03304392]\n",
      " [-0.76845538]]\n",
      "t [[-0.02534021]\n",
      " [-0.42118209]\n",
      " [-0.59743199]\n",
      " ...\n",
      " [-0.1746141 ]\n",
      " [ 0.03304392]\n",
      " [-0.76845538]]\n",
      "Current iteration=4, loss=35983.12185994231\n",
      "t [[-0.03825538]\n",
      " [-0.52390693]\n",
      " [-0.65885096]\n",
      " ...\n",
      " [-0.17382736]\n",
      " [ 0.04659583]\n",
      " [-0.89827949]]\n",
      "t [[-0.03825538]\n",
      " [-0.52390693]\n",
      " [-0.65885096]\n",
      " ...\n",
      " [-0.17382736]\n",
      " [ 0.04659583]\n",
      " [-0.89827949]]\n",
      "t [[-0.04974704]\n",
      " [-0.61841549]\n",
      " [-0.71317183]\n",
      " ...\n",
      " [-0.1732609 ]\n",
      " [ 0.05668774]\n",
      " [-1.01412931]]\n",
      "t [[-0.04974704]\n",
      " [-0.61841549]\n",
      " [-0.71317183]\n",
      " ...\n",
      " [-0.1732609 ]\n",
      " [ 0.05668774]\n",
      " [-1.01412931]]\n",
      "Current iteration=6, loss=35183.254318925116\n",
      "t [[-0.05977882]\n",
      " [-0.70513542]\n",
      " [-0.76264991]\n",
      " ...\n",
      " [-0.17371351]\n",
      " [ 0.06374069]\n",
      " [-1.11847109]]\n",
      "t [[-0.05977882]\n",
      " [-0.70513542]\n",
      " [-0.76264991]\n",
      " ...\n",
      " [-0.17371351]\n",
      " [ 0.06374069]\n",
      " [-1.11847109]]\n",
      "t [[-0.06845997]\n",
      " [-0.78473999]\n",
      " [-0.8083793 ]\n",
      " ...\n",
      " [-0.17529206]\n",
      " [ 0.06831459]\n",
      " [-1.21315078]]\n",
      "t [[-0.06845997]\n",
      " [-0.78473999]\n",
      " [-0.8083793 ]\n",
      " ...\n",
      " [-0.17529206]\n",
      " [ 0.06831459]\n",
      " [-1.21315078]]\n",
      "Current iteration=8, loss=34647.737853250845\n",
      "t [[-0.07594075]\n",
      " [-0.85793646]\n",
      " [-0.85094047]\n",
      " ...\n",
      " [-0.17785536]\n",
      " [ 0.07094457]\n",
      " [-1.29959802]]\n",
      "t [[-0.07594075]\n",
      " [-0.85793646]\n",
      " [-0.85094047]\n",
      " ...\n",
      " [-0.17785536]\n",
      " [ 0.07094457]\n",
      " [-1.29959802]]\n",
      "t [[-0.08237167]\n",
      " [-0.9253875 ]\n",
      " [-0.89068251]\n",
      " ...\n",
      " [-0.18119552]\n",
      " [ 0.07208923]\n",
      " [-1.37894799]]\n",
      "loss=34268.53463818882\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.11156972]\n",
      " [-0.47661304]\n",
      " [-0.29956846]\n",
      " ...\n",
      " [-0.21711287]\n",
      " [ 0.04700613]\n",
      " [-0.12656539]]\n",
      "t [[ 0.11156972]\n",
      " [-0.47661304]\n",
      " [-0.29956846]\n",
      " ...\n",
      " [-0.21711287]\n",
      " [ 0.04700613]\n",
      " [-0.12656539]]\n",
      "t [[ 0.1648114 ]\n",
      " [-0.75305594]\n",
      " [-0.46760392]\n",
      " ...\n",
      " [-0.32416648]\n",
      " [ 0.05448907]\n",
      " [-0.22282534]]\n",
      "t [[ 0.1648114 ]\n",
      " [-0.75305594]\n",
      " [-0.46760392]\n",
      " ...\n",
      " [-0.32416648]\n",
      " [ 0.05448907]\n",
      " [-0.22282534]]\n",
      "Current iteration=2, loss=37290.53566504514\n",
      "t [[ 0.19244633]\n",
      " [-0.94101471]\n",
      " [-0.58154723]\n",
      " ...\n",
      " [-0.38928766]\n",
      " [ 0.04417153]\n",
      " [-0.30088546]]\n",
      "t [[ 0.19244633]\n",
      " [-0.94101471]\n",
      " [-0.58154723]\n",
      " ...\n",
      " [-0.38928766]\n",
      " [ 0.04417153]\n",
      " [-0.30088546]]\n",
      "t [[ 0.20749737]\n",
      " [-1.08273814]\n",
      " [-0.66954031]\n",
      " ...\n",
      " [-0.43680643]\n",
      " [ 0.02465554]\n",
      " [-0.36632285]]\n",
      "t [[ 0.20749737]\n",
      " [-1.08273814]\n",
      " [-0.66954031]\n",
      " ...\n",
      " [-0.43680643]\n",
      " [ 0.02465554]\n",
      " [-0.36632285]]\n",
      "Current iteration=4, loss=36005.37542197265\n",
      "t [[ 2.15789071e-01]\n",
      " [-1.19659289e+00]\n",
      " [-7.43010327e-01]\n",
      " ...\n",
      " [-4.76163189e-01]\n",
      " [ 3.79613457e-05]\n",
      " [-4.22228279e-01]]\n",
      "t [[ 2.15789071e-01]\n",
      " [-1.19659289e+00]\n",
      " [-7.43010327e-01]\n",
      " ...\n",
      " [-4.76163189e-01]\n",
      " [ 3.79613457e-05]\n",
      " [-4.22228279e-01]]\n",
      "t [[ 0.2202033 ]\n",
      " [-1.2916784 ]\n",
      " [-0.8070812 ]\n",
      " ...\n",
      " [-0.51123406]\n",
      " [-0.02740846]\n",
      " [-0.47057946]]\n",
      "t [[ 0.2202033 ]\n",
      " [-1.2916784 ]\n",
      " [-0.8070812 ]\n",
      " ...\n",
      " [-0.51123406]\n",
      " [-0.02740846]\n",
      " [-0.47057946]]\n",
      "Current iteration=6, loss=35222.59914545053\n",
      "t [[ 0.22228101]\n",
      " [-1.37303018]\n",
      " [-0.8642614 ]\n",
      " ...\n",
      " [-0.54365028]\n",
      " [-0.05629722]\n",
      " [-0.51276276]]\n",
      "t [[ 0.22228101]\n",
      " [-1.37303018]\n",
      " [-0.8642614 ]\n",
      " ...\n",
      " [-0.54365028]\n",
      " [-0.05629722]\n",
      " [-0.51276276]]\n",
      "t [[ 0.22290275]\n",
      " [-1.44372986]\n",
      " [-0.91591008]\n",
      " ...\n",
      " [-0.57411025]\n",
      " [-0.0857334 ]\n",
      " [-0.5498086 ]]\n",
      "t [[ 0.22290275]\n",
      " [-1.44372986]\n",
      " [-0.91591008]\n",
      " ...\n",
      " [-0.57411025]\n",
      " [-0.0857334 ]\n",
      " [-0.5498086 ]]\n",
      "Current iteration=8, loss=34701.09623592215\n",
      "t [[ 0.22260145]\n",
      " [-1.50583676]\n",
      " [-0.96285665]\n",
      " ...\n",
      " [-0.60292399]\n",
      " [-0.11512182]\n",
      " [-0.58251319]]\n",
      "t [[ 0.22260145]\n",
      " [-1.50583676]\n",
      " [-0.96285665]\n",
      " ...\n",
      " [-0.60292399]\n",
      " [-0.11512182]\n",
      " [-0.58251319]]\n",
      "t [[ 0.22171519]\n",
      " [-1.56082737]\n",
      " [-1.00567224]\n",
      " ...\n",
      " [-0.63024295]\n",
      " [-0.14406256]\n",
      " [-0.61150847]]\n",
      "loss=34332.80372141724\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0147131 ]\n",
      " [-0.09058661]\n",
      " [-0.28399847]\n",
      " ...\n",
      " [-0.21793427]\n",
      " [ 0.04236791]\n",
      " [-0.12928828]]\n",
      "t [[ 0.0147131 ]\n",
      " [-0.09058661]\n",
      " [-0.28399847]\n",
      " ...\n",
      " [-0.21793427]\n",
      " [ 0.04236791]\n",
      " [-0.12928828]]\n",
      "t [[ 0.00941506]\n",
      " [-0.21184369]\n",
      " [-0.43301934]\n",
      " ...\n",
      " [-0.32346864]\n",
      " [ 0.04616632]\n",
      " [-0.22759124]]\n",
      "t [[ 0.00941506]\n",
      " [-0.21184369]\n",
      " [-0.43301934]\n",
      " ...\n",
      " [-0.32346864]\n",
      " [ 0.04616632]\n",
      " [-0.22759124]]\n",
      "Current iteration=2, loss=37253.46436863381\n",
      "t [[-0.00146787]\n",
      " [-0.33505188]\n",
      " [-0.52996559]\n",
      " ...\n",
      " [-0.38686227]\n",
      " [ 0.03315106]\n",
      " [-0.30738482]]\n",
      "t [[-0.00146787]\n",
      " [-0.33505188]\n",
      " [-0.52996559]\n",
      " ...\n",
      " [-0.38686227]\n",
      " [ 0.03315106]\n",
      " [-0.30738482]]\n",
      "t [[-0.01325019]\n",
      " [-0.45156738]\n",
      " [-0.60394561]\n",
      " ...\n",
      " [-0.43277844]\n",
      " [ 0.01170023]\n",
      " [-0.37434682]]\n",
      "t [[-0.01325019]\n",
      " [-0.45156738]\n",
      " [-0.60394561]\n",
      " ...\n",
      " [-0.43277844]\n",
      " [ 0.01170023]\n",
      " [-0.37434682]]\n",
      "Current iteration=4, loss=35944.34143525628\n",
      "t [[-0.0243614 ]\n",
      " [-0.55912322]\n",
      " [-0.66626039]\n",
      " ...\n",
      " [-0.47067696]\n",
      " [-0.01428489]\n",
      " [-0.43161218]]\n",
      "t [[-0.0243614 ]\n",
      " [-0.55912322]\n",
      " [-0.66626039]\n",
      " ...\n",
      " [-0.47067696]\n",
      " [-0.01428489]\n",
      " [-0.43161218]]\n",
      "t [[-0.03432721]\n",
      " [-0.65761365]\n",
      " [-0.72165474]\n",
      " ...\n",
      " [-0.50440621]\n",
      " [-0.04268016]\n",
      " [-0.48118628]]\n",
      "t [[-0.03432721]\n",
      " [-0.65761365]\n",
      " [-0.72165474]\n",
      " ...\n",
      " [-0.50440621]\n",
      " [-0.04268016]\n",
      " [-0.48118628]]\n",
      "Current iteration=6, loss=35146.265727924256\n",
      "t [[-0.04308023]\n",
      " [-0.74765876]\n",
      " [-0.77224103]\n",
      " ...\n",
      " [-0.53557327]\n",
      " [-0.07220939]\n",
      " [-0.52447609]]\n",
      "t [[-0.04308023]\n",
      " [-0.74765876]\n",
      " [-0.77224103]\n",
      " ...\n",
      " [-0.53557327]\n",
      " [-0.07220939]\n",
      " [-0.52447609]]\n",
      "t [[-0.05069798]\n",
      " [-0.83007198]\n",
      " [-0.81902976]\n",
      " ...\n",
      " [-0.56486229]\n",
      " [-0.10205929]\n",
      " [-0.56252844]]\n",
      "t [[-0.05069798]\n",
      " [-0.83007198]\n",
      " [-0.81902976]\n",
      " ...\n",
      " [-0.56486229]\n",
      " [-0.10205929]\n",
      " [-0.56252844]]\n",
      "Current iteration=8, loss=34615.24190274712\n",
      "t [[-0.05730028]\n",
      " [-0.90566243]\n",
      " [-0.86255966]\n",
      " ...\n",
      " [-0.59257611]\n",
      " [-0.1316952 ]\n",
      " [-0.59615292]]\n",
      "t [[-0.05730028]\n",
      " [-0.90566243]\n",
      " [-0.86255966]\n",
      " ...\n",
      " [-0.59257611]\n",
      " [-0.1316952 ]\n",
      " [-0.59615292]]\n",
      "t [[-0.06301005]\n",
      " [-0.97516805]\n",
      " [-0.90316342]\n",
      " ...\n",
      " [-0.61886278]\n",
      " [-0.16076233]\n",
      " [-0.62599252]]\n",
      "loss=34240.91879417167\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01256662]\n",
      " [-0.07950423]\n",
      " [-0.28707427]\n",
      " ...\n",
      " [-0.21428971]\n",
      " [ 0.04555377]\n",
      " [-0.12127296]]\n",
      "t [[ 0.01256662]\n",
      " [-0.07950423]\n",
      " [-0.28707427]\n",
      " ...\n",
      " [-0.21428971]\n",
      " [ 0.04555377]\n",
      " [-0.12127296]]\n",
      "t [[ 0.00531364]\n",
      " [-0.19115292]\n",
      " [-0.43940319]\n",
      " ...\n",
      " [-0.31769166]\n",
      " [ 0.05213   ]\n",
      " [-0.21261153]]\n",
      "t [[ 0.00531364]\n",
      " [-0.19115292]\n",
      " [-0.43940319]\n",
      " ...\n",
      " [-0.31769166]\n",
      " [ 0.05213   ]\n",
      " [-0.21261153]]\n",
      "Current iteration=2, loss=37290.693761042836\n",
      "t [[-0.00724694]\n",
      " [-0.30605709]\n",
      " [-0.53918054]\n",
      " ...\n",
      " [-0.379418  ]\n",
      " [ 0.04144272]\n",
      " [-0.28621824]]\n",
      "t [[-0.00724694]\n",
      " [-0.30605709]\n",
      " [-0.53918054]\n",
      " ...\n",
      " [-0.379418  ]\n",
      " [ 0.04144272]\n",
      " [-0.28621824]]\n",
      "t [[-0.02040293]\n",
      " [-0.41531679]\n",
      " [-0.61552924]\n",
      " ...\n",
      " [-0.4239311 ]\n",
      " [ 0.0219135 ]\n",
      " [-0.34763167]]\n",
      "t [[-0.02040293]\n",
      " [-0.41531679]\n",
      " [-0.61552924]\n",
      " ...\n",
      " [-0.4239311 ]\n",
      " [ 0.0219135 ]\n",
      " [-0.34763167]]\n",
      "Current iteration=4, loss=36005.616323108785\n",
      "t [[-0.03259946]\n",
      " [-0.51646899]\n",
      " [-0.67983989]\n",
      " ...\n",
      " [-0.4606412 ]\n",
      " [-0.00250779]\n",
      " [-0.39989312]]\n",
      "t [[-0.03259946]\n",
      " [-0.51646899]\n",
      " [-0.67983989]\n",
      " ...\n",
      " [-0.4606412 ]\n",
      " [-0.00250779]\n",
      " [-0.39989312]]\n",
      "t [[-0.04339529]\n",
      " [-0.60926113]\n",
      " [-0.73693548]\n",
      " ...\n",
      " [-0.49337468]\n",
      " [-0.02964793]\n",
      " [-0.44493597]]\n",
      "t [[-0.04339529]\n",
      " [-0.60926113]\n",
      " [-0.73693548]\n",
      " ...\n",
      " [-0.49337468]\n",
      " [-0.02964793]\n",
      " [-0.44493597]]\n",
      "Current iteration=6, loss=35223.456484481176\n",
      "t [[-0.05276038]\n",
      " [-0.69420058]\n",
      " [-0.78898775]\n",
      " ...\n",
      " [-0.52371971]\n",
      " [-0.05818287]\n",
      " [-0.48410846]]\n",
      "t [[-0.05276038]\n",
      " [-0.69420058]\n",
      " [-0.78898775]\n",
      " ...\n",
      " [-0.52371971]\n",
      " [-0.05818287]\n",
      " [-0.48410846]]\n",
      "t [[-0.06080799]\n",
      " [-0.77201221]\n",
      " [-0.83705135]\n",
      " ...\n",
      " [-0.55234069]\n",
      " [-0.0872548 ]\n",
      " [-0.51840831]]\n",
      "t [[-0.06080799]\n",
      " [-0.77201221]\n",
      " [-0.83705135]\n",
      " ...\n",
      " [-0.55234069]\n",
      " [-0.0872548 ]\n",
      " [-0.51840831]]\n",
      "Current iteration=8, loss=34702.89105253106\n",
      "t [[-0.06768976]\n",
      " [-0.8434342 ]\n",
      " [-0.88169813]\n",
      " ...\n",
      " [-0.57952054]\n",
      " [-0.11628935]\n",
      " [-0.54860352]]\n",
      "t [[-0.06768976]\n",
      " [-0.8434342 ]\n",
      " [-0.88169813]\n",
      " ...\n",
      " [-0.57952054]\n",
      " [-0.11628935]\n",
      " [-0.54860352]]\n",
      "t [[-0.07355596]\n",
      " [-0.90914684]\n",
      " [-0.92328613]\n",
      " ...\n",
      " [-0.60538821]\n",
      " [-0.14489737]\n",
      " [-0.57530156]]\n",
      "loss=34335.54365285422\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01046546]\n",
      " [-0.0842087 ]\n",
      " [-0.28518007]\n",
      " ...\n",
      " [-0.12555746]\n",
      " [-0.01512182]\n",
      " [-0.25615754]]\n",
      "t [[ 0.01046546]\n",
      " [-0.0842087 ]\n",
      " [-0.28518007]\n",
      " ...\n",
      " [-0.12555746]\n",
      " [-0.01512182]\n",
      " [-0.25615754]]\n",
      "t [[ 0.00150576]\n",
      " [-0.20030598]\n",
      " [-0.43453159]\n",
      " ...\n",
      " [-0.16406786]\n",
      " [-0.00142082]\n",
      " [-0.46135242]]\n",
      "t [[ 0.00150576]\n",
      " [-0.20030598]\n",
      " [-0.43453159]\n",
      " ...\n",
      " [-0.16406786]\n",
      " [-0.00142082]\n",
      " [-0.46135242]]\n",
      "Current iteration=2, loss=37239.22594346066\n",
      "t [[-0.01243154]\n",
      " [-0.31927139]\n",
      " [-0.53128077]\n",
      " ...\n",
      " [-0.17389781]\n",
      " [ 0.01755722]\n",
      " [-0.6339424 ]]\n",
      "t [[-0.01243154]\n",
      " [-0.31927139]\n",
      " [-0.53128077]\n",
      " ...\n",
      " [-0.17389781]\n",
      " [ 0.01755722]\n",
      " [-0.6339424 ]]\n",
      "t [[-0.02671603]\n",
      " [-0.4322227 ]\n",
      " [-0.60479993]\n",
      " ...\n",
      " [-0.17478437]\n",
      " [ 0.0347296 ]\n",
      " [-0.783181  ]]\n",
      "t [[-0.02671603]\n",
      " [-0.4322227 ]\n",
      " [-0.60479993]\n",
      " ...\n",
      " [-0.17478437]\n",
      " [ 0.0347296 ]\n",
      " [-0.783181  ]]\n",
      "Current iteration=4, loss=35930.7350355506\n",
      "t [[-0.03985627]\n",
      " [-0.53672819]\n",
      " [-0.66653973]\n",
      " ...\n",
      " [-0.17373786]\n",
      " [ 0.04829171]\n",
      " [-0.91450393]]\n",
      "t [[-0.03985627]\n",
      " [-0.53672819]\n",
      " [-0.66653973]\n",
      " ...\n",
      " [-0.17373786]\n",
      " [ 0.04829171]\n",
      " [-0.91450393]]\n",
      "t [[-0.0514565 ]\n",
      " [-0.63256783]\n",
      " [-0.72132502]\n",
      " ...\n",
      " [-0.17318312]\n",
      " [ 0.05820351]\n",
      " [-1.03149904]]\n",
      "t [[-0.0514565 ]\n",
      " [-0.63256783]\n",
      " [-0.72132502]\n",
      " ...\n",
      " [-0.17318312]\n",
      " [ 0.05820351]\n",
      " [-1.03149904]]\n",
      "Current iteration=6, loss=35132.95125673851\n",
      "t [[-0.06151879]\n",
      " [-0.72028114]\n",
      " [-0.77130867]\n",
      " ...\n",
      " [-0.17379929]\n",
      " [ 0.06497698]\n",
      " [-1.1367181 ]]\n",
      "t [[-0.06151879]\n",
      " [-0.72028114]\n",
      " [-0.77130867]\n",
      " ...\n",
      " [-0.17379929]\n",
      " [ 0.06497698]\n",
      " [-1.1367181 ]]\n",
      "t [[-0.0701791 ]\n",
      " [-0.80062332]\n",
      " [-0.81752013]\n",
      " ...\n",
      " [-0.17561363]\n",
      " [ 0.06923183]\n",
      " [-1.23206793]]\n",
      "t [[-0.0701791 ]\n",
      " [-0.80062332]\n",
      " [-0.81752013]\n",
      " ...\n",
      " [-0.17561363]\n",
      " [ 0.06923183]\n",
      " [-1.23206793]]\n",
      "Current iteration=8, loss=34601.78136125586\n",
      "t [[-0.07760523]\n",
      " [-0.87436014]\n",
      " [-0.86050583]\n",
      " ...\n",
      " [-0.17843678]\n",
      " [ 0.07153968]\n",
      " [-1.31902301]]\n",
      "t [[-0.07760523]\n",
      " [-0.87436014]\n",
      " [-0.86050583]\n",
      " ...\n",
      " [-0.17843678]\n",
      " [ 0.07153968]\n",
      " [-1.31902301]]\n",
      "t [[-0.08395915]\n",
      " [-0.94219639]\n",
      " [-0.90060056]\n",
      " ...\n",
      " [-0.18203356]\n",
      " [ 0.07238009]\n",
      " [-1.39875228]]\n",
      "loss=34227.146630929805\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.11429093]\n",
      " [-0.48823775]\n",
      " [-0.306875  ]\n",
      " ...\n",
      " [-0.22240831]\n",
      " [ 0.04815262]\n",
      " [-0.12965235]]\n",
      "t [[ 0.11429093]\n",
      " [-0.48823775]\n",
      " [-0.306875  ]\n",
      " ...\n",
      " [-0.22240831]\n",
      " [ 0.04815262]\n",
      " [-0.12965235]]\n",
      "t [[ 0.16742505]\n",
      " [-0.76660256]\n",
      " [-0.47584365]\n",
      " ...\n",
      " [-0.32942175]\n",
      " [ 0.05487351]\n",
      " [-0.22752401]]\n",
      "t [[ 0.16742505]\n",
      " [-0.76660256]\n",
      " [-0.47584365]\n",
      " ...\n",
      " [-0.32942175]\n",
      " [ 0.05487351]\n",
      " [-0.22752401]]\n",
      "Current iteration=2, loss=37244.8170461148\n",
      "t [[ 0.19449216]\n",
      " [-0.95483084]\n",
      " [-0.58992742]\n",
      " ...\n",
      " [-0.39408645]\n",
      " [ 0.04343699]\n",
      " [-0.30659752]]\n",
      "t [[ 0.19449216]\n",
      " [-0.95483084]\n",
      " [-0.58992742]\n",
      " ...\n",
      " [-0.39408645]\n",
      " [ 0.04343699]\n",
      " [-0.30659752]]\n",
      "t [[ 0.20899015]\n",
      " [-1.09660114]\n",
      " [-0.67815031]\n",
      " ...\n",
      " [-0.44146389]\n",
      " [ 0.02276361]\n",
      " [-0.37270047]]\n",
      "t [[ 0.20899015]\n",
      " [-1.09660114]\n",
      " [-0.67815031]\n",
      " ...\n",
      " [-0.44146389]\n",
      " [ 0.02276361]\n",
      " [-0.37270047]]\n",
      "Current iteration=4, loss=35955.14153247717\n",
      "t [[ 0.21682614]\n",
      " [-1.21049357]\n",
      " [-0.75198283]\n",
      " ...\n",
      " [-0.48097645]\n",
      " [-0.00295884]\n",
      " [-0.42903261]]\n",
      "t [[ 0.21682614]\n",
      " [-1.21049357]\n",
      " [-0.75198283]\n",
      " ...\n",
      " [-0.48097645]\n",
      " [-0.00295884]\n",
      " [-0.42903261]]\n",
      "t [[ 0.22087775]\n",
      " [-1.30559589]\n",
      " [-0.81646197]\n",
      " ...\n",
      " [-0.51637568]\n",
      " [-0.03142326]\n",
      " [-0.47763557]]\n",
      "t [[ 0.22087775]\n",
      " [-1.30559589]\n",
      " [-0.81646197]\n",
      " ...\n",
      " [-0.51637568]\n",
      " [-0.03142326]\n",
      " [-0.47763557]]\n",
      "Current iteration=6, loss=35174.67243104572\n",
      "t [[ 0.22266718]\n",
      " [-1.38691192]\n",
      " [-0.87402182]\n",
      " ...\n",
      " [-0.54919077]\n",
      " [-0.06122874]\n",
      " [-0.51993933]]\n",
      "t [[ 0.22266718]\n",
      " [-1.38691192]\n",
      " [-0.87402182]\n",
      " ...\n",
      " [-0.54919077]\n",
      " [-0.06122874]\n",
      " [-0.51993933]]\n",
      "t [[ 0.22305844]\n",
      " [-1.4575083 ]\n",
      " [-0.92597932]\n",
      " ...\n",
      " [-0.58005633]\n",
      " [-0.09147551]\n",
      " [-0.55700615]]\n",
      "t [[ 0.22305844]\n",
      " [-1.4575083 ]\n",
      " [-0.92597932]\n",
      " ...\n",
      " [-0.58005633]\n",
      " [-0.09147551]\n",
      " [-0.55700615]]\n",
      "Current iteration=8, loss=34657.47652641185\n",
      "t [[ 0.22257242]\n",
      " [-1.51944455]\n",
      " [-0.97314656]\n",
      " ...\n",
      " [-0.60924787]\n",
      " [-0.12156919]\n",
      " [-0.58965614]]\n",
      "t [[ 0.22257242]\n",
      " [-1.51944455]\n",
      " [-0.97314656]\n",
      " ...\n",
      " [-0.60924787]\n",
      " [-0.12156919]\n",
      " [-0.58965614]]\n",
      "t [[ 0.22153857]\n",
      " [-1.57420553]\n",
      " [-1.01609229]\n",
      " ...\n",
      " [-0.63690081]\n",
      " [-0.15111423]\n",
      " [-0.61853943]]\n",
      "loss=34293.59468784271\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01507196]\n",
      " [-0.09279604]\n",
      " [-0.29092527]\n",
      " ...\n",
      " [-0.22324973]\n",
      " [ 0.04340127]\n",
      " [-0.13244165]]\n",
      "t [[ 0.01507196]\n",
      " [-0.09279604]\n",
      " [-0.29092527]\n",
      " ...\n",
      " [-0.22324973]\n",
      " [ 0.04340127]\n",
      " [-0.13244165]]\n",
      "t [[ 0.00917062]\n",
      " [-0.21773925]\n",
      " [-0.44033365]\n",
      " ...\n",
      " [-0.32865233]\n",
      " [ 0.04637191]\n",
      " [-0.23239003]]\n",
      "t [[ 0.00917062]\n",
      " [-0.21773925]\n",
      " [-0.44033365]\n",
      " ...\n",
      " [-0.32865233]\n",
      " [ 0.04637191]\n",
      " [-0.23239003]]\n",
      "Current iteration=2, loss=37207.03580750528\n",
      "t [[-0.00224798]\n",
      " [-0.34404683]\n",
      " [-0.53710221]\n",
      " ...\n",
      " [-0.39153592]\n",
      " [ 0.0322195 ]\n",
      " [-0.31322419]]\n",
      "t [[-0.00224798]\n",
      " [-0.34404683]\n",
      " [-0.53710221]\n",
      " ...\n",
      " [-0.39153592]\n",
      " [ 0.0322195 ]\n",
      " [-0.31322419]]\n",
      "t [[-0.01438552]\n",
      " [-0.46291809]\n",
      " [-0.61118824]\n",
      " ...\n",
      " [-0.43728009]\n",
      " [ 0.00961981]\n",
      " [-0.38087382]]\n",
      "t [[-0.01438552]\n",
      " [-0.46291809]\n",
      " [-0.61118824]\n",
      " ...\n",
      " [-0.43728009]\n",
      " [ 0.00961981]\n",
      " [-0.38087382]]\n",
      "Current iteration=4, loss=35893.115395117915\n",
      "t [[-0.0257033 ]\n",
      " [-0.57222282]\n",
      " [-0.67387337]\n",
      " ...\n",
      " [-0.47531286]\n",
      " [-0.01744803]\n",
      " [-0.43858327]]\n",
      "t [[-0.0257033 ]\n",
      " [-0.57222282]\n",
      " [-0.67387337]\n",
      " ...\n",
      " [-0.47531286]\n",
      " [-0.01744803]\n",
      " [-0.43858327]]\n",
      "t [[-0.03577252]\n",
      " [-0.67200643]\n",
      " [-0.72976813]\n",
      " ...\n",
      " [-0.50935208]\n",
      " [-0.04683321]\n",
      " [-0.48842255]]\n",
      "t [[-0.03577252]\n",
      " [-0.67200643]\n",
      " [-0.72976813]\n",
      " ...\n",
      " [-0.50935208]\n",
      " [-0.04683321]\n",
      " [-0.48842255]]\n",
      "Current iteration=6, loss=35097.41633773945\n",
      "t [[-0.044561  ]\n",
      " [-0.76300649]\n",
      " [-0.7808794 ]\n",
      " ...\n",
      " [-0.54090123]\n",
      " [-0.07724956]\n",
      " [-0.53184299]]\n",
      "t [[-0.044561  ]\n",
      " [-0.76300649]\n",
      " [-0.7808794 ]\n",
      " ...\n",
      " [-0.54090123]\n",
      " [-0.07724956]\n",
      " [-0.53184299]]\n",
      "t [[-0.05216986]\n",
      " [-0.84612028]\n",
      " [-0.82815568]\n",
      " ...\n",
      " [-0.57058077]\n",
      " [-0.10788144]\n",
      " [-0.56992394]]\n",
      "t [[-0.05216986]\n",
      " [-0.84612028]\n",
      " [-0.82815568]\n",
      " ...\n",
      " [-0.57058077]\n",
      " [-0.10788144]\n",
      " [-0.56992394]]\n",
      "Current iteration=8, loss=34570.853257220646\n",
      "t [[-0.05873402]\n",
      " [-0.92221582]\n",
      " [-0.87210557]\n",
      " ...\n",
      " [-0.59865957]\n",
      " [-0.13819632]\n",
      " [-0.60349943]]\n",
      "t [[-0.05873402]\n",
      " [-0.92221582]\n",
      " [-0.87210557]\n",
      " ...\n",
      " [-0.59865957]\n",
      " [-0.13819632]\n",
      " [-0.60349943]]\n",
      "t [[-0.06438602]\n",
      " [-0.99207313]\n",
      " [-0.91305091]\n",
      " ...\n",
      " [-0.62527009]\n",
      " [-0.16784441]\n",
      " [-0.63323105]]\n",
      "loss=34201.0975762084\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01287312]\n",
      " [-0.08144336]\n",
      " [-0.29407608]\n",
      " ...\n",
      " [-0.21951629]\n",
      " [ 0.04666484]\n",
      " [-0.12423084]]\n",
      "t [[ 0.01287312]\n",
      " [-0.08144336]\n",
      " [-0.29407608]\n",
      " ...\n",
      " [-0.21951629]\n",
      " [ 0.04666484]\n",
      " [-0.12423084]]\n",
      "t [[ 0.00497339]\n",
      " [-0.1965814 ]\n",
      " [-0.44687648]\n",
      " ...\n",
      " [-0.32276994]\n",
      " [ 0.05246978]\n",
      " [-0.21707088]]\n",
      "t [[ 0.00497339]\n",
      " [-0.1965814 ]\n",
      " [-0.44687648]\n",
      " ...\n",
      " [-0.32276994]\n",
      " [ 0.05246978]\n",
      " [-0.21707088]]\n",
      "Current iteration=2, loss=37244.998434248155\n",
      "t [[-0.00815028]\n",
      " [-0.31444564]\n",
      " [-0.5465229 ]\n",
      " ...\n",
      " [-0.38396929]\n",
      " [ 0.04067995]\n",
      " [-0.29160512]]\n",
      "t [[-0.00815028]\n",
      " [-0.31444564]\n",
      " [-0.5465229 ]\n",
      " ...\n",
      " [-0.38396929]\n",
      " [ 0.04067995]\n",
      " [-0.29160512]]\n",
      "t [[-0.02167258]\n",
      " [-0.42596025]\n",
      " [-0.62300249]\n",
      " ...\n",
      " [-0.42829635]\n",
      " [ 0.02001892]\n",
      " [-0.35361727]]\n",
      "t [[-0.02167258]\n",
      " [-0.42596025]\n",
      " [-0.62300249]\n",
      " ...\n",
      " [-0.42829635]\n",
      " [ 0.02001892]\n",
      " [-0.35361727]]\n",
      "Current iteration=4, loss=35955.39119842915\n",
      "t [[-0.03407376]\n",
      " [-0.52878784]\n",
      " [-0.68769619]\n",
      " ...\n",
      " [-0.46513308]\n",
      " [-0.005482  ]\n",
      " [-0.4062536 ]]\n",
      "t [[-0.03407376]\n",
      " [-0.52878784]\n",
      " [-0.68769619]\n",
      " ...\n",
      " [-0.46513308]\n",
      " [-0.005482  ]\n",
      " [-0.4062536 ]]\n",
      "t [[-0.04496178]\n",
      " [-0.62281987]\n",
      " [-0.74529782]\n",
      " ...\n",
      " [-0.49817596]\n",
      " [-0.03361923]\n",
      " [-0.45150853]]\n",
      "t [[-0.04496178]\n",
      " [-0.62281987]\n",
      " [-0.74529782]\n",
      " ...\n",
      " [-0.49817596]\n",
      " [-0.03361923]\n",
      " [-0.45150853]]\n",
      "Current iteration=6, loss=35175.58933992675\n",
      "t [[-0.05434503]\n",
      " [-0.70867629]\n",
      " [-0.79787629]\n",
      " ...\n",
      " [-0.52890858]\n",
      " [-0.06305532]\n",
      " [-0.49077172]]\n",
      "t [[-0.05434503]\n",
      " [-0.70867629]\n",
      " [-0.79787629]\n",
      " ...\n",
      " [-0.52890858]\n",
      " [-0.06305532]\n",
      " [-0.49077172]]\n",
      "t [[-0.0623628 ]\n",
      " [-0.78716223]\n",
      " [-0.84642569]\n",
      " ...\n",
      " [-0.55793019]\n",
      " [-0.09292722]\n",
      " [-0.52507098]]\n",
      "t [[-0.0623628 ]\n",
      " [-0.78716223]\n",
      " [-0.84642569]\n",
      " ...\n",
      " [-0.55793019]\n",
      " [-0.09292722]\n",
      " [-0.52507098]]\n",
      "Current iteration=8, loss=34659.36592210011\n",
      "t [[-0.06918363]\n",
      " [-0.85907216]\n",
      " [-0.89148862]\n",
      " ...\n",
      " [-0.58548823]\n",
      " [-0.12266046]\n",
      " [-0.55519681]]\n",
      "t [[-0.06918363]\n",
      " [-0.85907216]\n",
      " [-0.89148862]\n",
      " ...\n",
      " [-0.58548823]\n",
      " [-0.12266046]\n",
      " [-0.55519681]]\n",
      "t [[-0.07496864]\n",
      " [-0.92512652]\n",
      " [-0.9334128 ]\n",
      " ...\n",
      " [-0.6116948 ]\n",
      " [-0.15186935]\n",
      " [-0.58177368]]\n",
      "loss=34296.446274309215\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01072072]\n",
      " [-0.08626257]\n",
      " [-0.29213568]\n",
      " ...\n",
      " [-0.12861983]\n",
      " [-0.01549064]\n",
      " [-0.26240528]]\n",
      "t [[ 0.01072072]\n",
      " [-0.08626257]\n",
      " [-0.29213568]\n",
      " ...\n",
      " [-0.12861983]\n",
      " [-0.01549064]\n",
      " [-0.26240528]]\n",
      "t [[ 0.00108229]\n",
      " [-0.2059499 ]\n",
      " [-0.44186219]\n",
      " ...\n",
      " [-0.16597554]\n",
      " [-0.00076004]\n",
      " [-0.47136437]]\n",
      "t [[ 0.00108229]\n",
      " [-0.2059499 ]\n",
      " [-0.44186219]\n",
      " ...\n",
      " [-0.16597554]\n",
      " [-0.00076004]\n",
      " [-0.47136437]]\n",
      "Current iteration=2, loss=37192.76286495087\n",
      "t [[-0.01343554]\n",
      " [-0.32795573]\n",
      " [-0.53840386]\n",
      " ...\n",
      " [-0.17464394]\n",
      " [ 0.01894196]\n",
      " [-0.64657618]]\n",
      "t [[-0.01343554]\n",
      " [-0.32795573]\n",
      " [-0.53840386]\n",
      " ...\n",
      " [-0.17464394]\n",
      " [ 0.01894196]\n",
      " [-0.64657618]]\n",
      "t [[-0.02809557]\n",
      " [-0.44322544]\n",
      " [-0.61199865]\n",
      " ...\n",
      " [-0.17488624]\n",
      " [ 0.03640603]\n",
      " [-0.79774494]]\n",
      "t [[-0.02809557]\n",
      " [-0.44322544]\n",
      " [-0.61199865]\n",
      " ...\n",
      " [-0.17488624]\n",
      " [ 0.03640603]\n",
      " [-0.79774494]]\n",
      "Current iteration=4, loss=35879.54394279525\n",
      "t [[-0.04144526]\n",
      " [-0.54945546]\n",
      " [-0.67408362]\n",
      " ...\n",
      " [-0.17361961]\n",
      " [ 0.04994869]\n",
      " [-0.93052057]]\n",
      "t [[-0.04144526]\n",
      " [-0.54945546]\n",
      " [-0.67408362]\n",
      " ...\n",
      " [-0.17361961]\n",
      " [ 0.04994869]\n",
      " [-0.93052057]]\n",
      "t [[-0.05314011]\n",
      " [-0.64657225]\n",
      " [-0.72935027]\n",
      " ...\n",
      " [-0.17310898]\n",
      " [ 0.05965769]\n",
      " [-1.04861856]]\n",
      "t [[-0.05314011]\n",
      " [-0.64657225]\n",
      " [-0.72935027]\n",
      " ...\n",
      " [-0.17310898]\n",
      " [ 0.05965769]\n",
      " [-1.04861856]]\n",
      "Current iteration=6, loss=35084.109254698415\n",
      "t [[-0.06322169]\n",
      " [-0.73523025]\n",
      " [-0.77984508]\n",
      " ...\n",
      " [-0.17391038]\n",
      " [ 0.06613752]\n",
      " [-1.15467622]]\n",
      "t [[-0.06322169]\n",
      " [-0.73523025]\n",
      " [-0.77984508]\n",
      " ...\n",
      " [-0.17391038]\n",
      " [ 0.06613752]\n",
      " [-1.15467622]]\n",
      "t [[-0.07185253]\n",
      " [-0.81626697]\n",
      " [-0.82653426]\n",
      " ...\n",
      " [-0.1759724 ]\n",
      " [ 0.07006681]\n",
      " [-1.25066122]]\n",
      "t [[-0.07185253]\n",
      " [-0.81626697]\n",
      " [-0.82653426]\n",
      " ...\n",
      " [-0.1759724 ]\n",
      " [ 0.07006681]\n",
      " [-1.25066122]]\n",
      "Current iteration=8, loss=34557.37272002357\n",
      "t [[-0.07921752]\n",
      " [-0.89050612]\n",
      " [-0.86993307]\n",
      " ...\n",
      " [-0.1790598 ]\n",
      " [ 0.07205207]\n",
      " [-1.33809273]]\n",
      "t [[-0.07921752]\n",
      " [-0.89050612]\n",
      " [-0.86993307]\n",
      " ...\n",
      " [-0.1790598 ]\n",
      " [ 0.07205207]\n",
      " [-1.33809273]]\n",
      "t [[-0.08548959]\n",
      " [-0.95869386]\n",
      " [-0.9103646 ]\n",
      " ...\n",
      " [-0.18291232]\n",
      " [ 0.0725922 ]\n",
      " [-1.41817321]]\n",
      "loss=34187.28776121372\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.11701214]\n",
      " [-0.49986246]\n",
      " [-0.31418155]\n",
      " ...\n",
      " [-0.22770374]\n",
      " [ 0.04929911]\n",
      " [-0.13273931]]\n",
      "t [[ 0.11701214]\n",
      " [-0.49986246]\n",
      " [-0.31418155]\n",
      " ...\n",
      " [-0.22770374]\n",
      " [ 0.04929911]\n",
      " [-0.13273931]]\n",
      "t [[ 0.1699753 ]\n",
      " [-0.77993207]\n",
      " [-0.48394097]\n",
      " ...\n",
      " [-0.33455755]\n",
      " [ 0.0552159 ]\n",
      " [-0.23218909]]\n",
      "t [[ 0.1699753 ]\n",
      " [-0.77993207]\n",
      " [-0.48394097]\n",
      " ...\n",
      " [-0.33455755]\n",
      " [ 0.0552159 ]\n",
      " [-0.23218909]]\n",
      "Current iteration=2, loss=37199.88220001009\n",
      "t [[ 0.19645248]\n",
      " [-0.96834816]\n",
      " [-0.59812493]\n",
      " ...\n",
      " [-0.39874367]\n",
      " [ 0.04264255]\n",
      " [-0.31224756]]\n",
      "t [[ 0.19645248]\n",
      " [-0.96834816]\n",
      " [-0.59812493]\n",
      " ...\n",
      " [-0.39874367]\n",
      " [ 0.04264255]\n",
      " [-0.31224756]]\n",
      "t [[ 0.21039805]\n",
      " [-1.11014672]\n",
      " [-0.68658195]\n",
      " ...\n",
      " [-0.4460003 ]\n",
      " [ 0.02080819]\n",
      " [-0.37899125]]\n",
      "t [[ 0.21039805]\n",
      " [-1.11014672]\n",
      " [-0.68658195]\n",
      " ...\n",
      " [-0.4460003 ]\n",
      " [ 0.02080819]\n",
      " [-0.37899125]]\n",
      "Current iteration=4, loss=35906.04571575074\n",
      "t [[ 0.21778741]\n",
      " [-1.22407509]\n",
      " [-0.76078881]\n",
      " ...\n",
      " [-0.48569599]\n",
      " [-0.00601451]\n",
      " [-0.43572752]]\n",
      "t [[ 0.21778741]\n",
      " [-1.22407509]\n",
      " [-0.76078881]\n",
      " ...\n",
      " [-0.48569599]\n",
      " [-0.00601451]\n",
      " [-0.43572752]]\n",
      "t [[ 0.22148736]\n",
      " [-1.31919145]\n",
      " [-0.8256813 ]\n",
      " ...\n",
      " [-0.52144354]\n",
      " [-0.03548676]\n",
      " [-0.48456166]]\n",
      "t [[ 0.22148736]\n",
      " [-1.31919145]\n",
      " [-0.8256813 ]\n",
      " ...\n",
      " [-0.52144354]\n",
      " [-0.03548676]\n",
      " [-0.48456166]]\n",
      "Current iteration=6, loss=35128.12286039788\n",
      "t [[ 0.22299898]\n",
      " [-1.40046403]\n",
      " [-0.88361647]\n",
      " ...\n",
      " [-0.55466718]\n",
      " [-0.06619463]\n",
      " [-0.52696732]]\n",
      "t [[ 0.22299898]\n",
      " [-1.40046403]\n",
      " [-0.88361647]\n",
      " ...\n",
      " [-0.55466718]\n",
      " [-0.06619463]\n",
      " [-0.52696732]]\n",
      "t [[ 0.22316926]\n",
      " [-1.47094564]\n",
      " [-0.93587084]\n",
      " ...\n",
      " [-0.58593916]\n",
      " [-0.09723446]\n",
      " [-0.56403871]]\n",
      "t [[ 0.22316926]\n",
      " [-1.47094564]\n",
      " [-0.93587084]\n",
      " ...\n",
      " [-0.58593916]\n",
      " [-0.09723446]\n",
      " [-0.56403871]]\n",
      "Current iteration=8, loss=34615.30347923111\n",
      "t [[ 0.22250706]\n",
      " [-1.53269823]\n",
      " [-0.98324188]\n",
      " ...\n",
      " [-0.61550302]\n",
      " [-0.12801349]\n",
      " [-0.59661984]]\n",
      "t [[ 0.22250706]\n",
      " [-1.53269823]\n",
      " [-0.98324188]\n",
      " ...\n",
      " [-0.61550302]\n",
      " [-0.12801349]\n",
      " [-0.59661984]]\n",
      "t [[ 0.2213334 ]\n",
      " [-1.58721675]\n",
      " [-1.02629832]\n",
      " ...\n",
      " [-0.64348044]\n",
      " [-0.15814138]\n",
      " [-0.62537894]]\n",
      "loss=34255.80627231268\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01543082]\n",
      " [-0.09500547]\n",
      " [-0.29785206]\n",
      " ...\n",
      " [-0.2285652 ]\n",
      " [ 0.04443464]\n",
      " [-0.13559502]]\n",
      "t [[ 0.01543082]\n",
      " [-0.09500547]\n",
      " [-0.29785206]\n",
      " ...\n",
      " [-0.2285652 ]\n",
      " [ 0.04443464]\n",
      " [-0.13559502]]\n",
      "t [[ 0.00890535]\n",
      " [-0.22366695]\n",
      " [-0.44750194]\n",
      " ...\n",
      " [-0.33371422]\n",
      " [ 0.04653661]\n",
      " [-0.23715453]]\n",
      "t [[ 0.00890535]\n",
      " [-0.22366695]\n",
      " [-0.44750194]\n",
      " ...\n",
      " [-0.33371422]\n",
      " [ 0.04653661]\n",
      " [-0.23715453]]\n",
      "Current iteration=2, loss=37161.39290264118\n",
      "t [[-0.00304645]\n",
      " [-0.35304802]\n",
      " [-0.54406271]\n",
      " ...\n",
      " [-0.39606717]\n",
      " [ 0.03123135]\n",
      " [-0.3190005 ]]\n",
      "t [[-0.00304645]\n",
      " [-0.35304802]\n",
      " [-0.54406271]\n",
      " ...\n",
      " [-0.39606717]\n",
      " [ 0.03123135]\n",
      " [-0.3190005 ]]\n",
      "t [[-0.01552694]\n",
      " [-0.47422369]\n",
      " [-0.61827265]\n",
      " ...\n",
      " [-0.44166144]\n",
      " [ 0.00748106]\n",
      " [-0.38731259]]\n",
      "t [[-0.01552694]\n",
      " [-0.47422369]\n",
      " [-0.61827265]\n",
      " ...\n",
      " [-0.44166144]\n",
      " [ 0.00748106]\n",
      " [-0.38731259]]\n",
      "Current iteration=4, loss=35843.046758457946\n",
      "t [[-0.02703752]\n",
      " [-0.58522068]\n",
      " [-0.68135232]\n",
      " ...\n",
      " [-0.47985667]\n",
      " [-0.02066358]\n",
      " [-0.44544315]]\n",
      "t [[-0.02703752]\n",
      " [-0.58522068]\n",
      " [-0.68135232]\n",
      " ...\n",
      " [-0.47985667]\n",
      " [-0.02066358]\n",
      " [-0.44544315]]\n",
      "t [[-0.03719812]\n",
      " [-0.68624411]\n",
      " [-0.73776245]\n",
      " ...\n",
      " [-0.5142262 ]\n",
      " [-0.05102779]\n",
      " [-0.49552655]]\n",
      "t [[-0.03719812]\n",
      " [-0.68624411]\n",
      " [-0.73776245]\n",
      " ...\n",
      " [-0.5142262 ]\n",
      " [-0.05102779]\n",
      " [-0.49552655]]\n",
      "Current iteration=6, loss=35049.974366143615\n",
      "t [[-0.04601253]\n",
      " [-0.77815153]\n",
      " [-0.78940187]\n",
      " ...\n",
      " [-0.54616733]\n",
      " [-0.08231674]\n",
      " [-0.53905863]]\n",
      "t [[-0.04601253]\n",
      " [-0.77815153]\n",
      " [-0.78940187]\n",
      " ...\n",
      " [-0.54616733]\n",
      " [-0.08231674]\n",
      " [-0.53905863]]\n",
      "t [[-0.05360523]\n",
      " [-0.86192403]\n",
      " [-0.83715921]\n",
      " ...\n",
      " [-0.57623852]\n",
      " [-0.11371325]\n",
      " [-0.57715134]]\n",
      "t [[-0.05360523]\n",
      " [-0.86192403]\n",
      " [-0.83715921]\n",
      " ...\n",
      " [-0.57623852]\n",
      " [-0.11371325]\n",
      " [-0.57715134]]\n",
      "Current iteration=8, loss=34527.94427749182\n",
      "t [[-0.06012576]\n",
      " [-0.93848785]\n",
      " [-0.8815161 ]\n",
      " ...\n",
      " [-0.60467726]\n",
      " [-0.14468764]\n",
      " [-0.61066318]]\n",
      "t [[-0.06012576]\n",
      " [-0.93848785]\n",
      " [-0.8815161 ]\n",
      " ...\n",
      " [-0.60467726]\n",
      " [-0.14468764]\n",
      " [-0.61066318]]\n",
      "t [[-0.06571584]\n",
      " [-1.00866429]\n",
      " [-0.92278614]\n",
      " ...\n",
      " [-0.6316027 ]\n",
      " [-0.17489589]\n",
      " [-0.64027426]]\n",
      "loss=34162.72893659102\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01317962]\n",
      " [-0.08338249]\n",
      " [-0.30107789]\n",
      " ...\n",
      " [-0.22474287]\n",
      " [ 0.04777591]\n",
      " [-0.12718871]]\n",
      "t [[ 0.01317962]\n",
      " [-0.08338249]\n",
      " [-0.30107789]\n",
      " ...\n",
      " [-0.22474287]\n",
      " [ 0.04777591]\n",
      " [-0.12718871]]\n",
      "t [[ 0.00461248]\n",
      " [-0.20204384]\n",
      " [-0.45420375]\n",
      " ...\n",
      " [-0.32772796]\n",
      " [ 0.05276806]\n",
      " [-0.22149714]]\n",
      "t [[ 0.00461248]\n",
      " [-0.20204384]\n",
      " [-0.45420375]\n",
      " ...\n",
      " [-0.32772796]\n",
      " [ 0.05276806]\n",
      " [-0.22149714]]\n",
      "Current iteration=2, loss=37200.08426794397\n",
      "t [[-0.00907105]\n",
      " [-0.32284483]\n",
      " [-0.55368762]\n",
      " ...\n",
      " [-0.38837987]\n",
      " [ 0.03985901]\n",
      " [-0.29693158]]\n",
      "t [[-0.00907105]\n",
      " [-0.32284483]\n",
      " [-0.55368762]\n",
      " ...\n",
      " [-0.38837987]\n",
      " [ 0.03985901]\n",
      " [-0.29693158]]\n",
      "t [[-0.02294625]\n",
      " [-0.43656563]\n",
      " [-0.63031451]\n",
      " ...\n",
      " [-0.43254323]\n",
      " [ 0.01806321]\n",
      " [-0.35951893]]\n",
      "t [[-0.02294625]\n",
      " [-0.43656563]\n",
      " [-0.63031451]\n",
      " ...\n",
      " [-0.43254323]\n",
      " [ 0.01806321]\n",
      " [-0.35951893]]\n",
      "Current iteration=4, loss=35906.30563134287\n",
      "t [[-0.03553705]\n",
      " [-0.54101451]\n",
      " [-0.69541432]\n",
      " ...\n",
      " [-0.4695354 ]\n",
      " [-0.00851273]\n",
      " [-0.41250897]]\n",
      "t [[-0.03553705]\n",
      " [-0.54101451]\n",
      " [-0.69541432]\n",
      " ...\n",
      " [-0.4695354 ]\n",
      " [-0.00851273]\n",
      " [-0.41250897]]\n",
      "t [[-0.04650414]\n",
      " [-0.63623543]\n",
      " [-0.75353611]\n",
      " ...\n",
      " [-0.50290884]\n",
      " [-0.03763738]\n",
      " [-0.4579568 ]]\n",
      "t [[-0.04650414]\n",
      " [-0.63623543]\n",
      " [-0.75353611]\n",
      " ...\n",
      " [-0.50290884]\n",
      " [-0.03763738]\n",
      " [-0.4579568 ]]\n",
      "Current iteration=6, loss=35129.10134329674\n",
      "t [[-0.05589515]\n",
      " [-0.72296335]\n",
      " [-0.8066433 ]\n",
      " ...\n",
      " [-0.5340398 ]\n",
      " [-0.06796104]\n",
      " [-0.49729354]]\n",
      "t [[-0.05589515]\n",
      " [-0.72296335]\n",
      " [-0.8066433 ]\n",
      " ...\n",
      " [-0.5340398 ]\n",
      " [-0.06796104]\n",
      " [-0.49729354]]\n",
      "t [[-0.06387522]\n",
      " [-0.80208367]\n",
      " [-0.85567151]\n",
      " ...\n",
      " [-0.56346389]\n",
      " [-0.09861617]\n",
      " [-0.53157715]]\n",
      "t [[-0.06387522]\n",
      " [-0.80208367]\n",
      " [-0.85567151]\n",
      " ...\n",
      " [-0.56346389]\n",
      " [-0.09861617]\n",
      " [-0.53157715]]\n",
      "Current iteration=8, loss=34617.2877514486\n",
      "t [[-0.07062926]\n",
      " [-0.87444646]\n",
      " [-0.90113717]\n",
      " ...\n",
      " [-0.59139565]\n",
      " [-0.12902896]\n",
      " [-0.5616206 ]]\n",
      "t [[-0.07062926]\n",
      " [-0.87444646]\n",
      " [-0.90113717]\n",
      " ...\n",
      " [-0.59139565]\n",
      " [-0.12902896]\n",
      " [-0.5616206 ]]\n",
      "t [[-0.07632881]\n",
      " [-0.94081148]\n",
      " [-0.94338031]\n",
      " ...\n",
      " [-0.6179326 ]\n",
      " [-0.15881791]\n",
      " [-0.5880653 ]]\n",
      "loss=34258.76831723055\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01097597]\n",
      " [-0.08831644]\n",
      " [-0.29909129]\n",
      " ...\n",
      " [-0.13168221]\n",
      " [-0.01585947]\n",
      " [-0.26865303]]\n",
      "t [[ 0.01097597]\n",
      " [-0.08831644]\n",
      " [-0.29909129]\n",
      " ...\n",
      " [-0.13168221]\n",
      " [-0.01585947]\n",
      " [-0.26865303]]\n",
      "t [[ 6.38615878e-04]\n",
      " [-2.11627315e-01]\n",
      " [-4.49045860e-01]\n",
      " ...\n",
      " [-1.67789068e-01]\n",
      " [-6.78432136e-05]\n",
      " [-4.81319568e-01]]\n",
      "t [[ 6.38615878e-04]\n",
      " [-2.11627315e-01]\n",
      " [-4.49045860e-01]\n",
      " ...\n",
      " [-1.67789068e-01]\n",
      " [-6.78432136e-05]\n",
      " [-4.81319568e-01]]\n",
      "Current iteration=2, loss=37147.09230258745\n",
      "t [[-0.01445583]\n",
      " [-0.33664941]\n",
      " [-0.54534916]\n",
      " ...\n",
      " [-0.17529294]\n",
      " [ 0.02034504]\n",
      " [-0.65909993]]\n",
      "t [[-0.01445583]\n",
      " [-0.33664941]\n",
      " [-0.54534916]\n",
      " ...\n",
      " [-0.17529294]\n",
      " [ 0.02034504]\n",
      " [-0.65909993]]\n",
      "t [[-0.02947742]\n",
      " [-0.45418766]\n",
      " [-0.61903738]\n",
      " ...\n",
      " [-0.17492587]\n",
      " [ 0.038071  ]\n",
      " [-0.81215028]]\n",
      "t [[-0.02947742]\n",
      " [-0.45418766]\n",
      " [-0.61903738]\n",
      " ...\n",
      " [-0.17492587]\n",
      " [ 0.038071  ]\n",
      " [-0.81215028]]\n",
      "Current iteration=4, loss=35829.50947312915\n",
      "t [[-0.0430211 ]\n",
      " [-0.56208677]\n",
      " [-0.68149219]\n",
      " ...\n",
      " [-0.17347848]\n",
      " [ 0.05156524]\n",
      " [-0.94633394]]\n",
      "t [[-0.0430211 ]\n",
      " [-0.56208677]\n",
      " [-0.68149219]\n",
      " ...\n",
      " [-0.17347848]\n",
      " [ 0.05156524]\n",
      " [-0.94633394]]\n",
      "t [[-0.05479716]\n",
      " [-0.66042832]\n",
      " [-0.73725587]\n",
      " ...\n",
      " [-0.17304272]\n",
      " [ 0.06105008]\n",
      " [-1.06549384]]\n",
      "t [[-0.05479716]\n",
      " [-0.66042832]\n",
      " [-0.73725587]\n",
      " ...\n",
      " [-0.17304272]\n",
      " [ 0.06105008]\n",
      " [-1.06549384]]\n",
      "Current iteration=6, loss=35036.67245086578\n",
      "t [[-0.06488745]\n",
      " [-0.74998423]\n",
      " [-0.78826573]\n",
      " ...\n",
      " [-0.17404898]\n",
      " [ 0.06722363]\n",
      " [-1.17235286]]\n",
      "t [[-0.06488745]\n",
      " [-0.74998423]\n",
      " [-0.78826573]\n",
      " ...\n",
      " [-0.17404898]\n",
      " [ 0.06722363]\n",
      " [-1.17235286]]\n",
      "t [[-0.07348081]\n",
      " [-0.83167434]\n",
      " [-0.83542681]\n",
      " ...\n",
      " [-0.17636874]\n",
      " [ 0.07082225]\n",
      " [-1.26893946]]\n",
      "t [[-0.07348081]\n",
      " [-0.83167434]\n",
      " [-0.83542681]\n",
      " ...\n",
      " [-0.17636874]\n",
      " [ 0.07082225]\n",
      " [-1.26893946]]\n",
      "Current iteration=8, loss=34514.4421578879\n",
      "t [[-0.08077872]\n",
      " [-0.90637961]\n",
      " [-0.87922624]\n",
      " ...\n",
      " [-0.1797233 ]\n",
      " [ 0.07248558]\n",
      " [-1.35681737]]\n",
      "t [[-0.08077872]\n",
      " [-0.90637961]\n",
      " [-0.87922624]\n",
      " ...\n",
      " [-0.1797233 ]\n",
      " [ 0.07248558]\n",
      " [-1.35681737]]\n",
      "t [[-0.08696456]\n",
      " [-0.97488683]\n",
      " [-0.91997813]\n",
      " ...\n",
      " [-0.18382961]\n",
      " [ 0.07273021]\n",
      " [-1.43722236]]\n",
      "loss=34148.88077646886\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.11973336]\n",
      " [-0.51148717]\n",
      " [-0.3214881 ]\n",
      " ...\n",
      " [-0.23299918]\n",
      " [ 0.0504456 ]\n",
      " [-0.13582627]]\n",
      "t [[ 0.11973336]\n",
      " [-0.51148717]\n",
      " [-0.3214881 ]\n",
      " ...\n",
      " [-0.23299918]\n",
      " [ 0.0504456 ]\n",
      " [-0.13582627]]\n",
      "t [[ 0.17246244]\n",
      " [-0.79304554]\n",
      " [-0.49189657]\n",
      " ...\n",
      " [-0.33957448]\n",
      " [ 0.05551649]\n",
      " [-0.23682073]]\n",
      "t [[ 0.17246244]\n",
      " [-0.79304554]\n",
      " [-0.49189657]\n",
      " ...\n",
      " [-0.33957448]\n",
      " [ 0.05551649]\n",
      " [-0.23682073]]\n",
      "Current iteration=2, loss=37155.70481340034\n",
      "t [[ 0.19832979]\n",
      " [-0.9815749 ]\n",
      " [-0.60614546]\n",
      " ...\n",
      " [-0.40326429]\n",
      " [ 0.04178979]\n",
      " [-0.31783653]]\n",
      "t [[ 0.19832979]\n",
      " [-0.9815749 ]\n",
      " [-0.60614546]\n",
      " ...\n",
      " [-0.40326429]\n",
      " [ 0.04178979]\n",
      " [-0.31783653]]\n",
      "t [[ 0.21172505]\n",
      " [-1.12338776]\n",
      " [-0.69484377]\n",
      " ...\n",
      " [-0.45042299]\n",
      " [ 0.01879185]\n",
      " [-0.38519694]]\n",
      "t [[ 0.21172505]\n",
      " [-1.12338776]\n",
      " [-0.69484377]\n",
      " ...\n",
      " [-0.45042299]\n",
      " [ 0.01879185]\n",
      " [-0.38519694]]\n",
      "Current iteration=4, loss=35858.05129869419\n",
      "t [[ 0.21867742]\n",
      " [-1.23735191]\n",
      " [-0.76943722]\n",
      " ...\n",
      " [-0.49032915]\n",
      " [-0.00912582]\n",
      " [-0.44231552]]\n",
      "t [[ 0.21867742]\n",
      " [-1.23735191]\n",
      " [-0.76943722]\n",
      " ...\n",
      " [-0.49032915]\n",
      " [-0.00912582]\n",
      " [-0.44231552]]\n",
      "t [[ 0.22203659]\n",
      " [-1.33247949]\n",
      " [-0.83474726]\n",
      " ...\n",
      " [-0.52644369]\n",
      " [-0.03959533]\n",
      " [-0.49136094]]\n",
      "t [[ 0.22203659]\n",
      " [-1.33247949]\n",
      " [-0.83474726]\n",
      " ...\n",
      " [-0.52644369]\n",
      " [-0.03959533]\n",
      " [-0.49136094]]\n",
      "Current iteration=6, loss=35082.89810077753\n",
      "t [[ 0.22328057]\n",
      " [-1.4137004 ]\n",
      " [-0.89305226]\n",
      " ...\n",
      " [-0.5600839 ]\n",
      " [-0.07119104]\n",
      " [-0.53385067]]\n",
      "t [[ 0.22328057]\n",
      " [-1.4137004 ]\n",
      " [-0.89305226]\n",
      " ...\n",
      " [-0.5600839 ]\n",
      " [-0.07119104]\n",
      " [-0.53385067]]\n",
      "t [[ 0.22323896]\n",
      " [-1.48405545]\n",
      " [-0.94559062]\n",
      " ...\n",
      " [-0.59176173]\n",
      " [-0.10300631]\n",
      " [-0.57091092]]\n",
      "t [[ 0.22323896]\n",
      " [-1.48405545]\n",
      " [-0.94559062]\n",
      " ...\n",
      " [-0.59176173]\n",
      " [-0.10300631]\n",
      " [-0.57091092]]\n",
      "Current iteration=8, loss=34574.512443919535\n",
      "t [[ 0.22240877]\n",
      " [-1.54561136]\n",
      " [-0.99314808]\n",
      " ...\n",
      " [-0.62169142]\n",
      " [-0.13445084]\n",
      " [-0.60340963]]\n",
      "t [[ 0.22240877]\n",
      " [-1.54561136]\n",
      " [-0.99314808]\n",
      " ...\n",
      " [-0.62169142]\n",
      " [-0.13445084]\n",
      " [-0.60340963]]\n",
      "t [[ 0.22110273]\n",
      " [-1.59987493]\n",
      " [-1.03629572]\n",
      " ...\n",
      " [-0.64998327]\n",
      " [-0.16514029]\n",
      " [-0.63203304]]\n",
      "loss=34219.36752189088\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01578967]\n",
      " [-0.0972149 ]\n",
      " [-0.30477885]\n",
      " ...\n",
      " [-0.23388067]\n",
      " [ 0.045468  ]\n",
      " [-0.13874839]]\n",
      "t [[ 0.01578967]\n",
      " [-0.0972149 ]\n",
      " [-0.30477885]\n",
      " ...\n",
      " [-0.23388067]\n",
      " [ 0.045468  ]\n",
      " [-0.13874839]]\n",
      "t [[ 0.0086194 ]\n",
      " [-0.22962657]\n",
      " [-0.45452494]\n",
      " ...\n",
      " [-0.33865493]\n",
      " [ 0.04666067]\n",
      " [-0.24188487]]\n",
      "t [[ 0.0086194 ]\n",
      " [-0.22962657]\n",
      " [-0.45452494]\n",
      " ...\n",
      " [-0.33865493]\n",
      " [ 0.04666067]\n",
      " [-0.24188487]]\n",
      "Current iteration=2, loss=37116.50920337371\n",
      "t [[-0.00386225]\n",
      " [-0.3620535 ]\n",
      " [-0.55085314]\n",
      " ...\n",
      " [-0.40046114]\n",
      " [ 0.03018815]\n",
      " [-0.32471474]]\n",
      "t [[-0.00386225]\n",
      " [-0.3620535 ]\n",
      " [-0.55085314]\n",
      " ...\n",
      " [-0.40046114]\n",
      " [ 0.03018815]\n",
      " [-0.32471474]]\n",
      "t [[-0.01667309]\n",
      " [-0.48548171]\n",
      " [-0.62520763]\n",
      " ...\n",
      " [-0.44592993]\n",
      " [ 0.00528651]\n",
      " [-0.39366494]]\n",
      "t [[-0.01667309]\n",
      " [-0.48548171]\n",
      " [-0.62520763]\n",
      " ...\n",
      " [-0.44592993]\n",
      " [ 0.00528651]\n",
      " [-0.39366494]]\n",
      "Current iteration=4, loss=35794.09877957024\n",
      "t [[-0.02836287]\n",
      " [-0.59811517]\n",
      " [-0.68870606]\n",
      " ...\n",
      " [-0.48431574]\n",
      " [-0.02392845]\n",
      " [-0.45219436]]\n",
      "t [[-0.02836287]\n",
      " [-0.59811517]\n",
      " [-0.68870606]\n",
      " ...\n",
      " [-0.48431574]\n",
      " [-0.02392845]\n",
      " [-0.45219436]]\n",
      "t [[-0.0386033 ]\n",
      " [-0.70032669]\n",
      " [-0.74564516]\n",
      " ...\n",
      " [-0.51903458]\n",
      " [-0.05526053]\n",
      " [-0.50250156]]\n",
      "t [[-0.0386033 ]\n",
      " [-0.70032669]\n",
      " [-0.74564516]\n",
      " ...\n",
      " [-0.51903458]\n",
      " [-0.05526053]\n",
      " [-0.50250156]]\n",
      "Current iteration=6, loss=35003.88654608917\n",
      "t [[-0.04743466]\n",
      " [-0.79309576]\n",
      " [-0.7978142 ]\n",
      " ...\n",
      " [-0.55137592]\n",
      " [-0.08740739]\n",
      " [-0.54612698]]\n",
      "t [[-0.04743466]\n",
      " [-0.79309576]\n",
      " [-0.7978142 ]\n",
      " ...\n",
      " [-0.55137592]\n",
      " [-0.08740739]\n",
      " [-0.54612698]]\n",
      "t [[-0.05500447]\n",
      " [-0.87748701]\n",
      " [-0.84604477]\n",
      " ...\n",
      " [-0.58183847]\n",
      " [-0.11955113]\n",
      " [-0.58421534]]\n",
      "t [[-0.05500447]\n",
      " [-0.87748701]\n",
      " [-0.84604477]\n",
      " ...\n",
      " [-0.58183847]\n",
      " [-0.11955113]\n",
      " [-0.58421534]]\n",
      "Current iteration=8, loss=34486.44886760096\n",
      "t [[-0.06147633]\n",
      " [-0.95448403]\n",
      " [-0.89079476]\n",
      " ...\n",
      " [-0.61063112]\n",
      " [-0.15116567]\n",
      " [-0.61764961]]\n",
      "t [[-0.06147633]\n",
      " [-0.95448403]\n",
      " [-0.89079476]\n",
      " ...\n",
      " [-0.61063112]\n",
      " [-0.15116567]\n",
      " [-0.61764961]]\n",
      "t [[-0.06700071]\n",
      " [-1.02494865]\n",
      " [-0.93237225]\n",
      " ...\n",
      " [-0.637862  ]\n",
      " [-0.18191346]\n",
      " [-0.64712833]]\n",
      "loss=34125.74024266305\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01348613]\n",
      " [-0.08532161]\n",
      " [-0.30807971]\n",
      " ...\n",
      " [-0.22996945]\n",
      " [ 0.04888697]\n",
      " [-0.13014659]]\n",
      "t [[ 0.01348613]\n",
      " [-0.08532161]\n",
      " [-0.30807971]\n",
      " ...\n",
      " [-0.22996945]\n",
      " [ 0.04888697]\n",
      " [-0.13014659]]\n",
      "t [[ 0.00423105]\n",
      " [-0.20754002]\n",
      " [-0.46138571]\n",
      " ...\n",
      " [-0.33256631]\n",
      " [ 0.0530251 ]\n",
      " [-0.22589043]]\n",
      "t [[ 0.00423105]\n",
      " [-0.20754002]\n",
      " [-0.46138571]\n",
      " ...\n",
      " [-0.33256631]\n",
      " [ 0.0530251 ]\n",
      " [-0.22589043]]\n",
      "Current iteration=2, loss=37155.92490799885\n",
      "t [[-0.01000822]\n",
      " [-0.33125268]\n",
      " [-0.56068071]\n",
      " ...\n",
      " [-0.39265479]\n",
      " [ 0.03898145]\n",
      " [-0.30219856]]\n",
      "t [[-0.01000822]\n",
      " [-0.33125268]\n",
      " [-0.56068071]\n",
      " ...\n",
      " [-0.39265479]\n",
      " [ 0.03898145]\n",
      " [-0.30219856]]\n",
      "t [[-0.02422256]\n",
      " [-0.44713039]\n",
      " [-0.63747411]\n",
      " ...\n",
      " [-0.43667912]\n",
      " [ 0.01604891]\n",
      " [-0.36533841]]\n",
      "t [[-0.02422256]\n",
      " [-0.44713039]\n",
      " [-0.63747411]\n",
      " ...\n",
      " [-0.43667912]\n",
      " [ 0.01604891]\n",
      " [-0.36533841]]\n",
      "Current iteration=4, loss=35858.32306907793\n",
      "t [[-0.03698817]\n",
      " [-0.55314722]\n",
      " [-0.70300317]\n",
      " ...\n",
      " [-0.47385547]\n",
      " [-0.01159684]\n",
      " [-0.4186617 ]]\n",
      "t [[-0.03698817]\n",
      " [-0.55314722]\n",
      " [-0.70300317]\n",
      " ...\n",
      " [-0.47385547]\n",
      " [-0.01159684]\n",
      " [-0.4186617 ]]\n",
      "t [[-0.04802171]\n",
      " [-0.64950756]\n",
      " [-0.76165791]\n",
      " ...\n",
      " [-0.50757928]\n",
      " [-0.0416989 ]\n",
      " [-0.46428394]]\n",
      "t [[-0.04802171]\n",
      " [-0.64950756]\n",
      " [-0.76165791]\n",
      " ...\n",
      " [-0.50757928]\n",
      " [-0.0416989 ]\n",
      " [-0.46428394]]\n",
      "Current iteration=6, loss=35083.93999842057\n",
      "t [[-0.05741069]\n",
      " [-0.73706333]\n",
      " [-0.81529469]\n",
      " ...\n",
      " [-0.53911763]\n",
      " [-0.07289632]\n",
      " [-0.50367774]]\n",
      "t [[-0.05741069]\n",
      " [-0.73706333]\n",
      " [-0.81529469]\n",
      " ...\n",
      " [-0.53911763]\n",
      " [-0.07289632]\n",
      " [-0.50367774]]\n",
      "t [[-0.06534577]\n",
      " [-0.81677991]\n",
      " [-0.86479341]\n",
      " ...\n",
      " [-0.56894464]\n",
      " [-0.10431787]\n",
      " [-0.5379313 ]]\n",
      "t [[-0.06534577]\n",
      " [-0.81677991]\n",
      " [-0.86479341]\n",
      " ...\n",
      " [-0.56894464]\n",
      " [-0.10431787]\n",
      " [-0.5379313 ]]\n",
      "Current iteration=8, loss=34576.59171703872\n",
      "t [[-0.07202769]\n",
      " [-0.88956212]\n",
      " [-0.91064753]\n",
      " ...\n",
      " [-0.59724461]\n",
      " [-0.13539108]\n",
      " [-0.56788005]]\n",
      "t [[-0.07202769]\n",
      " [-0.88956212]\n",
      " [-0.91064753]\n",
      " ...\n",
      " [-0.59724461]\n",
      " [-0.13539108]\n",
      " [-0.56788005]]\n",
      "t [[-0.07763791]\n",
      " [-0.9562083 ]\n",
      " [-0.95319206]\n",
      " ...\n",
      " [-0.62410279]\n",
      " [-0.16573943]\n",
      " [-0.59418225]]\n",
      "loss=34222.438744085994\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01123123]\n",
      " [-0.09037031]\n",
      " [-0.3060469 ]\n",
      " ...\n",
      " [-0.13474459]\n",
      " [-0.01622829]\n",
      " [-0.27490077]]\n",
      "t [[ 0.01123123]\n",
      " [-0.09037031]\n",
      " [-0.3060469 ]\n",
      " ...\n",
      " [-0.13474459]\n",
      " [-0.01622829]\n",
      " [-0.27490077]]\n",
      "t [[ 1.74881921e-04]\n",
      " [-2.17338007e-01]\n",
      " [-4.56083339e-01]\n",
      " ...\n",
      " [-1.69508922e-01]\n",
      " [ 6.55620749e-04]\n",
      " [-4.91218225e-01]]\n",
      "t [[ 1.74881921e-04]\n",
      " [-2.17338007e-01]\n",
      " [-4.56083339e-01]\n",
      " ...\n",
      " [-1.69508922e-01]\n",
      " [ 6.55620749e-04]\n",
      " [-4.91218225e-01]]\n",
      "Current iteration=2, loss=37102.187395437155\n",
      "t [[-0.01549141]\n",
      " [-0.34535042]\n",
      " [-0.55212273]\n",
      " ...\n",
      " [-0.17584901]\n",
      " [ 0.0217648 ]\n",
      " [-0.67151519]]\n",
      "t [[-0.01549141]\n",
      " [-0.34535042]\n",
      " [-0.55212273]\n",
      " ...\n",
      " [-0.17584901]\n",
      " [ 0.0217648 ]\n",
      " [-0.67151519]]\n",
      "t [[-0.03086021]\n",
      " [-0.46510681]\n",
      " [-0.62592502]\n",
      " ...\n",
      " [-0.17490918]\n",
      " [ 0.03972244]\n",
      " [-0.82640003]]\n",
      "t [[-0.03086021]\n",
      " [-0.46510681]\n",
      " [-0.62592502]\n",
      " ...\n",
      " [-0.17490918]\n",
      " [ 0.03972244]\n",
      " [-0.82640003]]\n",
      "Current iteration=4, loss=35780.59470279351\n",
      "t [[-0.04458268]\n",
      " [-0.57462038]\n",
      " [-0.68877439]\n",
      " ...\n",
      " [-0.17331986]\n",
      " [ 0.05314002]\n",
      " [-0.96194844]]\n",
      "t [[-0.04458268]\n",
      " [-0.57462038]\n",
      " [-0.68877439]\n",
      " ...\n",
      " [-0.17331986]\n",
      " [ 0.05314002]\n",
      " [-0.96194844]]\n",
      "t [[-0.05642707]\n",
      " [-0.67413587]\n",
      " [-0.74504939]\n",
      " ...\n",
      " [-0.17298806]\n",
      " [ 0.0623807 ]\n",
      " [-1.08213066]]\n",
      "t [[-0.05642707]\n",
      " [-0.67413587]\n",
      " [-0.74504939]\n",
      " ...\n",
      " [-0.17298806]\n",
      " [ 0.0623807 ]\n",
      " [-1.08213066]]\n",
      "Current iteration=6, loss=34990.58767520581\n",
      "t [[-0.06651612]\n",
      " [-0.76454472]\n",
      " [-0.7965765 ]\n",
      " ...\n",
      " [-0.17421687]\n",
      " [ 0.0682368 ]\n",
      " [-1.18975514]]\n",
      "t [[-0.06651612]\n",
      " [-0.76454472]\n",
      " [-0.7965765 ]\n",
      " ...\n",
      " [-0.17421687]\n",
      " [ 0.0682368 ]\n",
      " [-1.18975514]]\n",
      "t [[-0.07506459]\n",
      " [-0.84684893]\n",
      " [-0.84420223]\n",
      " ...\n",
      " [-0.17680266]\n",
      " [ 0.07150093]\n",
      " [-1.28691112]]\n",
      "t [[-0.07506459]\n",
      " [-0.84684893]\n",
      " [-0.84420223]\n",
      " ...\n",
      " [-0.17680266]\n",
      " [ 0.07150093]\n",
      " [-1.28691112]]\n",
      "Current iteration=8, loss=34472.92372413839\n",
      "t [[-0.08228998]\n",
      " [-0.92198586]\n",
      " [-0.88838892]\n",
      " ...\n",
      " [-0.18042596]\n",
      " [ 0.07284399]\n",
      " [-1.37520675]]\n",
      "t [[-0.08228998]\n",
      " [-0.92198586]\n",
      " [-0.88838892]\n",
      " ...\n",
      " [-0.18042596]\n",
      " [ 0.07284399]\n",
      " [-1.37520675]]\n",
      "t [[-0.08838564]\n",
      " [-0.99078212]\n",
      " [-0.92944433]\n",
      " ...\n",
      " [-0.18478317]\n",
      " [ 0.07279862]\n",
      " [-1.4559108 ]]\n",
      "loss=34111.85316436017\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.12245457]\n",
      " [-0.52311188]\n",
      " [-0.32879465]\n",
      " ...\n",
      " [-0.23829462]\n",
      " [ 0.0515921 ]\n",
      " [-0.13891323]]\n",
      "t [[ 0.12245457]\n",
      " [-0.52311188]\n",
      " [-0.32879465]\n",
      " ...\n",
      " [-0.23829462]\n",
      " [ 0.0515921 ]\n",
      " [-0.13891323]]\n",
      "t [[ 0.1748868 ]\n",
      " [-0.80594405]\n",
      " [-0.4997112 ]\n",
      " ...\n",
      " [-0.34447311]\n",
      " [ 0.05577552]\n",
      " [-0.24141903]]\n",
      "t [[ 0.1748868 ]\n",
      " [-0.80594405]\n",
      " [-0.4997112 ]\n",
      " ...\n",
      " [-0.34447311]\n",
      " [ 0.05577552]\n",
      " [-0.24141903]]\n",
      "Current iteration=2, loss=37112.259535330086\n",
      "t [[ 0.20012654]\n",
      " [-0.99451922]\n",
      " [-0.61399463]\n",
      " ...\n",
      " [-0.40765326]\n",
      " [ 0.04088022]\n",
      " [-0.32336537]]\n",
      "t [[ 0.20012654]\n",
      " [-0.99451922]\n",
      " [-0.61399463]\n",
      " ...\n",
      " [-0.40765326]\n",
      " [ 0.04088022]\n",
      " [-0.32336537]]\n",
      "t [[ 0.212975  ]\n",
      " [-1.13633663]\n",
      " [-0.70294397]\n",
      " ...\n",
      " [-0.45473901]\n",
      " [ 0.01671708]\n",
      " [-0.39131927]]\n",
      "t [[ 0.212975  ]\n",
      " [-1.13633663]\n",
      " [-0.70294397]\n",
      " ...\n",
      " [-0.45473901]\n",
      " [ 0.01671708]\n",
      " [-0.39131927]]\n",
      "Current iteration=4, loss=35811.12359281747\n",
      "t [[ 0.21950044]\n",
      " [-1.25033768]\n",
      " [-0.77793647]\n",
      " ...\n",
      " [-0.49488277]\n",
      " [-0.01228968]\n",
      " [-0.44879902]]\n",
      "t [[ 0.21950044]\n",
      " [-1.25033768]\n",
      " [-0.77793647]\n",
      " ...\n",
      " [-0.49488277]\n",
      " [-0.01228968]\n",
      " [-0.44879902]]\n",
      "t [[ 0.22252963]\n",
      " [-1.34547343]\n",
      " [-0.84366729]\n",
      " ...\n",
      " [-0.53138158]\n",
      " [-0.04374556]\n",
      " [-0.49803651]]\n",
      "t [[ 0.22252963]\n",
      " [-1.34547343]\n",
      " [-0.84366729]\n",
      " ...\n",
      " [-0.53138158]\n",
      " [-0.04374556]\n",
      " [-0.49803651]]\n",
      "Current iteration=6, loss=35038.94831253234\n",
      "t [[ 0.22351579]\n",
      " [-1.42663393]\n",
      " [-0.90233545]\n",
      " ...\n",
      " [-0.56544482]\n",
      " [-0.07621435]\n",
      " [-0.54059313]]\n",
      "t [[ 0.22351579]\n",
      " [-1.42663393]\n",
      " [-0.90233545]\n",
      " ...\n",
      " [-0.56544482]\n",
      " [-0.07621435]\n",
      " [-0.54059313]]\n",
      "t [[ 0.22327101]\n",
      " [-1.49685032]\n",
      " [-0.95514406]\n",
      " ...\n",
      " [-0.59752659]\n",
      " [-0.10878736]\n",
      " [-0.57762722]]\n",
      "t [[ 0.22327101]\n",
      " [-1.49685032]\n",
      " [-0.95514406]\n",
      " ...\n",
      " [-0.59752659]\n",
      " [-0.10878736]\n",
      " [-0.57762722]]\n",
      "Current iteration=8, loss=34535.042287752105\n",
      "t [[ 0.22228065]\n",
      " [-1.55819662]\n",
      " [-1.00287018]\n",
      " ...\n",
      " [-0.62781476]\n",
      " [-0.1408776 ]\n",
      " [-0.61003065]]\n",
      "t [[ 0.22228065]\n",
      " [-1.55819662]\n",
      " [-1.00287018]\n",
      " ...\n",
      " [-0.62781476]\n",
      " [-0.1408776 ]\n",
      " [-0.61003065]]\n",
      "t [[ 0.22084933]\n",
      " [-1.61219314]\n",
      " [-1.04608955]\n",
      " ...\n",
      " [-0.65641048]\n",
      " [-0.17210753]\n",
      " [-0.63850755]]\n",
      "loss=34184.21179614331\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01614853]\n",
      " [-0.09942433]\n",
      " [-0.31170564]\n",
      " ...\n",
      " [-0.23919614]\n",
      " [ 0.04650136]\n",
      " [-0.14190177]]\n",
      "t [[ 0.01614853]\n",
      " [-0.09942433]\n",
      " [-0.31170564]\n",
      " ...\n",
      " [-0.23919614]\n",
      " [ 0.04650136]\n",
      " [-0.14190177]]\n",
      "t [[ 0.00831292]\n",
      " [-0.23561789]\n",
      " [-0.4614034 ]\n",
      " ...\n",
      " [-0.34347505]\n",
      " [ 0.04674433]\n",
      " [-0.24658118]]\n",
      "t [[ 0.00831292]\n",
      " [-0.23561789]\n",
      " [-0.4614034 ]\n",
      " ...\n",
      " [-0.34347505]\n",
      " [ 0.04674433]\n",
      " [-0.24658118]]\n",
      "Current iteration=2, loss=37072.35924734554\n",
      "t [[-0.00469438]\n",
      " [-0.37106133]\n",
      " [-0.55747946]\n",
      " ...\n",
      " [-0.40472289]\n",
      " [ 0.02909143]\n",
      " [-0.33036787]]\n",
      "t [[-0.00469438]\n",
      " [-0.37106133]\n",
      " [-0.55747946]\n",
      " ...\n",
      " [-0.40472289]\n",
      " [ 0.02909143]\n",
      " [-0.33036787]]\n",
      "t [[-0.01782265]\n",
      " [-0.49668982]\n",
      " [-0.63200164]\n",
      " ...\n",
      " [-0.45009271]\n",
      " [ 0.00303856]\n",
      " [-0.39993261]]\n",
      "t [[-0.01782265]\n",
      " [-0.49668982]\n",
      " [-0.63200164]\n",
      " ...\n",
      " [-0.45009271]\n",
      " [ 0.00303856]\n",
      " [-0.39993261]]\n",
      "Current iteration=4, loss=35746.23666974114\n",
      "t [[-0.02967825]\n",
      " [-0.61090489]\n",
      " [-0.69594285]\n",
      " ...\n",
      " [-0.48869693]\n",
      " [-0.02723971]\n",
      " [-0.45883936]]\n",
      "t [[-0.02967825]\n",
      " [-0.61090489]\n",
      " [-0.69594285]\n",
      " ...\n",
      " [-0.48869693]\n",
      " [-0.02723971]\n",
      " [-0.45883936]]\n",
      "t [[-0.03998745]\n",
      " [-0.71425438]\n",
      " [-0.753423  ]\n",
      " ...\n",
      " [-0.52378261]\n",
      " [-0.05952823]\n",
      " [-0.50935068]]\n",
      "t [[-0.03998745]\n",
      " [-0.71425438]\n",
      " [-0.753423  ]\n",
      " ...\n",
      " [-0.52378261]\n",
      " [-0.05952823]\n",
      " [-0.50935068]]\n",
      "Current iteration=6, loss=34959.1021185172\n",
      "t [[-0.04882735]\n",
      " [-0.80784121]\n",
      " [-0.80612152]\n",
      " ...\n",
      " [-0.5565308 ]\n",
      " [-0.0925182 ]\n",
      " [-0.55305185]]\n",
      "t [[-0.04882735]\n",
      " [-0.80784121]\n",
      " [-0.80612152]\n",
      " ...\n",
      " [-0.5565308 ]\n",
      " [-0.0925182 ]\n",
      " [-0.55305185]]\n",
      "t [[-0.05636801]\n",
      " [-0.89281303]\n",
      " [-0.8548162 ]\n",
      " ...\n",
      " [-0.58738312]\n",
      " [-0.12539176]\n",
      " [-0.59112045]]\n",
      "t [[-0.05636801]\n",
      " [-0.89281303]\n",
      " [-0.8548162 ]\n",
      " ...\n",
      " [-0.58738312]\n",
      " [-0.12539176]\n",
      " [-0.59112045]]\n",
      "Current iteration=8, loss=34446.304514100964\n",
      "t [[-0.06278657]\n",
      " [-0.97020986]\n",
      " [-0.89994468]\n",
      " ...\n",
      " [-0.61652278]\n",
      " [-0.15762717]\n",
      " [-0.62446392]]\n",
      "t [[-0.06278657]\n",
      " [-0.97020986]\n",
      " [-0.89994468]\n",
      " ...\n",
      " [-0.61652278]\n",
      " [-0.15762717]\n",
      " [-0.62446392]]\n",
      "t [[-0.06824181]\n",
      " [-1.04093319]\n",
      " [-0.9418121 ]\n",
      " ...\n",
      " [-0.64404917]\n",
      " [-0.1888941 ]\n",
      " [-0.65379914]]\n",
      "loss=34090.06327104874\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01379263]\n",
      " [-0.08726074]\n",
      " [-0.31508152]\n",
      " ...\n",
      " [-0.23519603]\n",
      " [ 0.04999804]\n",
      " [-0.13310447]]\n",
      "t [[ 0.01379263]\n",
      " [-0.08726074]\n",
      " [-0.31508152]\n",
      " ...\n",
      " [-0.23519603]\n",
      " [ 0.04999804]\n",
      " [-0.13310447]]\n",
      "t [[ 0.00382925]\n",
      " [-0.21306971]\n",
      " [-0.4684231 ]\n",
      " ...\n",
      " [-0.33728561]\n",
      " [ 0.05324114]\n",
      " [-0.2302509 ]]\n",
      "t [[ 0.00382925]\n",
      " [-0.21306971]\n",
      " [-0.4684231 ]\n",
      " ...\n",
      " [-0.33728561]\n",
      " [ 0.05324114]\n",
      " [-0.2302509 ]]\n",
      "Current iteration=2, loss=37112.494977825365\n",
      "t [[-0.01096079]\n",
      " [-0.33966719]\n",
      " [-0.56750811]\n",
      " ...\n",
      " [-0.39679907]\n",
      " [ 0.03804879]\n",
      " [-0.307407  ]]\n",
      "t [[-0.01096079]\n",
      " [-0.33966719]\n",
      " [-0.56750811]\n",
      " ...\n",
      " [-0.39679907]\n",
      " [ 0.03804879]\n",
      " [-0.307407  ]]\n",
      "t [[-0.02550017]\n",
      " [-0.45765209]\n",
      " [-0.64448976]\n",
      " ...\n",
      " [-0.44071111]\n",
      " [ 0.01397845]\n",
      " [-0.37107739]]\n",
      "t [[-0.02550017]\n",
      " [-0.45765209]\n",
      " [-0.64448976]\n",
      " ...\n",
      " [-0.44071111]\n",
      " [ 0.01397845]\n",
      " [-0.37107739]]\n",
      "Current iteration=4, loss=35811.40892135943\n",
      "t [[-0.03842603]\n",
      " [-0.5651844 ]\n",
      " [-0.71047105]\n",
      " ...\n",
      " [-0.4781001 ]\n",
      " [-0.01473136]\n",
      " [-0.42471416]]\n",
      "t [[-0.03842603]\n",
      " [-0.5651844 ]\n",
      " [-0.71047105]\n",
      " ...\n",
      " [-0.4781001 ]\n",
      " [-0.01473136]\n",
      " [-0.42471416]]\n",
      "t [[-0.04951394]\n",
      " [-0.66263624]\n",
      " [-0.76967009]\n",
      " ...\n",
      " [-0.51219263]\n",
      " [-0.0458005 ]\n",
      " [-0.47049294]]\n",
      "t [[-0.04951394]\n",
      " [-0.66263624]\n",
      " [-0.76967009]\n",
      " ...\n",
      " [-0.51219263]\n",
      " [-0.0458005 ]\n",
      " [-0.47049294]]\n",
      "Current iteration=6, loss=35040.05530293072\n",
      "t [[-0.05889172]\n",
      " [-0.75097797]\n",
      " [-0.82383576]\n",
      " ...\n",
      " [-0.54414582]\n",
      " [-0.07785773]\n",
      " [-0.50992796]]\n",
      "t [[-0.05889172]\n",
      " [-0.75097797]\n",
      " [-0.82383576]\n",
      " ...\n",
      " [-0.54414582]\n",
      " [-0.07785773]\n",
      " [-0.50992796]]\n",
      "t [[-0.06677506]\n",
      " [-0.8312544 ]\n",
      " [-0.87379543]\n",
      " ...\n",
      " [-0.57437482]\n",
      " [-0.11002877]\n",
      " [-0.54413774]]\n",
      "t [[-0.06677506]\n",
      " [-0.8312544 ]\n",
      " [-0.87379543]\n",
      " ...\n",
      " [-0.57437482]\n",
      " [-0.11002877]\n",
      " [-0.54413774]]\n",
      "Current iteration=8, loss=34537.2165294882\n",
      "t [[-0.07337998]\n",
      " [-0.90442419]\n",
      " [-0.92002304]\n",
      " ...\n",
      " [-0.60303661]\n",
      " [-0.1417433 ]\n",
      " [-0.57398013]]\n",
      "t [[-0.07337998]\n",
      " [-0.90442419]\n",
      " [-0.92002304]\n",
      " ...\n",
      " [-0.60303661]\n",
      " [-0.1417433 ]\n",
      " [-0.57398013]]\n",
      "t [[-0.0788974 ]\n",
      " [-0.97132345]\n",
      " [-0.96285113]\n",
      " ...\n",
      " [-0.63020636]\n",
      " [-0.17263055]\n",
      " [-0.60013009]]\n",
      "loss=34187.39084637695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01148649]\n",
      " [-0.09242418]\n",
      " [-0.31300251]\n",
      " ...\n",
      " [-0.13780696]\n",
      " [-0.01659712]\n",
      " [-0.28114852]]\n",
      "t [[ 0.01148649]\n",
      " [-0.09242418]\n",
      " [-0.31300251]\n",
      " ...\n",
      " [-0.13780696]\n",
      " [-0.01659712]\n",
      " [-0.28114852]]\n",
      "t [[-3.08766096e-04]\n",
      " [-2.23081752e-01]\n",
      " [-4.62975385e-01]\n",
      " ...\n",
      " [-1.71135589e-01]\n",
      " [ 1.41020603e-03]\n",
      " [-5.01060552e-01]]\n",
      "t [[-3.08766096e-04]\n",
      " [-2.23081752e-01]\n",
      " [-4.62975385e-01]\n",
      " ...\n",
      " [-1.71135589e-01]\n",
      " [ 1.41020603e-03]\n",
      " [-5.01060552e-01]]\n",
      "Current iteration=2, loss=37058.02228404287\n",
      "t [[-0.01654126]\n",
      " [-0.35405679]\n",
      " [-0.55873056]\n",
      " ...\n",
      " [-0.17631632]\n",
      " [ 0.02319958]\n",
      " [-0.6838235 ]]\n",
      "t [[-0.01654126]\n",
      " [-0.35405679]\n",
      " [-0.55873056]\n",
      " ...\n",
      " [-0.17631632]\n",
      " [ 0.02319958]\n",
      " [-0.6838235 ]]\n",
      "t [[-0.03224266]\n",
      " [-0.47598048]\n",
      " [-0.63267007]\n",
      " ...\n",
      " [-0.17484183]\n",
      " [ 0.04135835]\n",
      " [-0.8404971 ]]\n",
      "t [[-0.03224266]\n",
      " [-0.47598048]\n",
      " [-0.63267007]\n",
      " ...\n",
      " [-0.17484183]\n",
      " [ 0.04135835]\n",
      " [-0.8404971 ]]\n",
      "Current iteration=4, loss=35732.76469672381\n",
      "t [[-0.04612896]\n",
      " [-0.58705473]\n",
      " [-0.69593857]\n",
      " ...\n",
      " [-0.17314876]\n",
      " [ 0.05467186]\n",
      " [-0.97736832]]\n",
      "t [[-0.04612896]\n",
      " [-0.58705473]\n",
      " [-0.69593857]\n",
      " ...\n",
      " [-0.17314876]\n",
      " [ 0.05467186]\n",
      " [-0.97736832]]\n",
      "t [[-0.05802937]\n",
      " [-0.68769491]\n",
      " [-0.75273771]\n",
      " ...\n",
      " [-0.17294825]\n",
      " [ 0.06364974]\n",
      " [-1.09853456]]\n",
      "t [[-0.05802937]\n",
      " [-0.68769491]\n",
      " [-0.75273771]\n",
      " ...\n",
      " [-0.17294825]\n",
      " [ 0.06364974]\n",
      " [-1.09853456]]\n",
      "Current iteration=6, loss=34945.804268287204\n",
      "t [[-0.06810783]\n",
      " [-0.77891357]\n",
      " [-0.80478259]\n",
      " ...\n",
      " [-0.17441539]\n",
      " [ 0.06917863]\n",
      " [-1.20688991]]\n",
      "t [[-0.06810783]\n",
      " [-0.77891357]\n",
      " [-0.80478259]\n",
      " ...\n",
      " [-0.17441539]\n",
      " [ 0.06917863]\n",
      " [-1.20688991]]\n",
      "t [[-0.07660456]\n",
      " [-0.86179432]\n",
      " [-0.85286447]\n",
      " ...\n",
      " [-0.17727388]\n",
      " [ 0.07210567]\n",
      " [-1.30458435]]\n",
      "t [[-0.07660456]\n",
      " [-0.86179432]\n",
      " [-0.85286447]\n",
      " ...\n",
      " [-0.17727388]\n",
      " [ 0.07210567]\n",
      " [-1.30458435]]\n",
      "Current iteration=8, loss=34432.755041047945\n",
      "t [[-0.08375246]\n",
      " [-0.93733007]\n",
      " [-0.89742427]\n",
      " ...\n",
      " [-0.18116629]\n",
      " [ 0.07313104]\n",
      " [-1.39327024]]\n",
      "t [[-0.08375246]\n",
      " [-0.93733007]\n",
      " [-0.89742427]\n",
      " ...\n",
      " [-0.18116629]\n",
      " [ 0.07313104]\n",
      " [-1.39327024]]\n",
      "t [[-0.08975436]\n",
      " [-1.00638642]\n",
      " [-0.93876605]\n",
      " ...\n",
      " [-0.18577072]\n",
      " [ 0.07280179]\n",
      " [-1.4742491 ]]\n",
      "loss=34076.13680942983\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.12517578]\n",
      " [-0.53473659]\n",
      " [-0.33610119]\n",
      " ...\n",
      " [-0.24359005]\n",
      " [ 0.05273859]\n",
      " [-0.14200019]]\n",
      "t [[ 0.12517578]\n",
      " [-0.53473659]\n",
      " [-0.33610119]\n",
      " ...\n",
      " [-0.24359005]\n",
      " [ 0.05273859]\n",
      " [-0.14200019]]\n",
      "t [[ 0.17724868]\n",
      " [-0.8186287 ]\n",
      " [-0.50738556]\n",
      " ...\n",
      " [-0.34925405]\n",
      " [ 0.05599325]\n",
      " [-0.24598414]]\n",
      "t [[ 0.17724868]\n",
      " [-0.8186287 ]\n",
      " [-0.50738556]\n",
      " ...\n",
      " [-0.34925405]\n",
      " [ 0.05599325]\n",
      " [-0.24598414]]\n",
      "Current iteration=2, loss=37069.52195945039\n",
      "t [[ 0.20184517]\n",
      " [-1.00718914]\n",
      " [-0.62167796]\n",
      " ...\n",
      " [-0.41191545]\n",
      " [ 0.03991538]\n",
      " [-0.32883502]]\n",
      "t [[ 0.20184517]\n",
      " [-1.00718914]\n",
      " [-0.62167796]\n",
      " ...\n",
      " [-0.41191545]\n",
      " [ 0.03991538]\n",
      " [-0.32883502]]\n",
      "t [[ 0.21415158]\n",
      " [-1.14900528]\n",
      " [-0.71089045]\n",
      " ...\n",
      " [-0.45895513]\n",
      " [ 0.01458631]\n",
      " [-0.39735991]]\n",
      "t [[ 0.21415158]\n",
      " [-1.14900528]\n",
      " [-0.71089045]\n",
      " ...\n",
      " [-0.45895513]\n",
      " [ 0.01458631]\n",
      " [-0.39735991]]\n",
      "Current iteration=4, loss=35765.22971982908\n",
      "t [[ 0.22026052]\n",
      " [-1.26304529]\n",
      " [-0.78629444]\n",
      " ...\n",
      " [-0.4993632 ]\n",
      " [-0.01550315]\n",
      " [-0.45518036]]\n",
      "t [[ 0.22026052]\n",
      " [-1.26304529]\n",
      " [-0.78629444]\n",
      " ...\n",
      " [-0.4993632 ]\n",
      " [-0.01550315]\n",
      " [-0.45518036]]\n",
      "t [[ 0.22297033]\n",
      " [-1.35818579]\n",
      " [-0.85244818]\n",
      " ...\n",
      " [-0.53626212]\n",
      " [-0.04793418]\n",
      " [-0.50459133]]\n",
      "t [[ 0.22297033]\n",
      " [-1.35818579]\n",
      " [-0.85244818]\n",
      " ...\n",
      " [-0.53626212]\n",
      " [-0.04793418]\n",
      " [-0.50459133]]\n",
      "Current iteration=6, loss=34996.22598645912\n",
      "t [[ 0.22370815]\n",
      " [-1.43927656]\n",
      " [-0.91147172]\n",
      " ...\n",
      " [-0.5707533 ]\n",
      " [-0.08126116]\n",
      " [-0.54719831]]\n",
      "t [[ 0.22370815]\n",
      " [-1.43927656]\n",
      " [-0.91147172]\n",
      " ...\n",
      " [-0.5707533 ]\n",
      " [-0.08126116]\n",
      " [-0.54719831]]\n",
      "t [[ 0.22326855]\n",
      " [-1.50934196]\n",
      " [-0.96453609]\n",
      " ...\n",
      " [-0.60323592]\n",
      " [-0.11457417]\n",
      " [-0.5841919 ]]\n",
      "t [[ 0.22326855]\n",
      " [-1.50934196]\n",
      " [-0.96453609]\n",
      " ...\n",
      " [-0.60323592]\n",
      " [-0.11457417]\n",
      " [-0.5841919 ]]\n",
      "Current iteration=8, loss=34496.83516921988\n",
      "t [[ 0.2221255 ]\n",
      " [-1.57046586]\n",
      " [-1.01241279]\n",
      " ...\n",
      " [-0.63387442]\n",
      " [-0.14729039]\n",
      " [-0.61648783]]\n",
      "t [[ 0.2221255 ]\n",
      " [-1.57046586]\n",
      " [-1.01241279]\n",
      " ...\n",
      " [-0.63387442]\n",
      " [-0.14729039]\n",
      " [-0.61648783]]\n",
      "t [[ 0.22057572]\n",
      " [-1.62418366]\n",
      " [-1.05568453]\n",
      " ...\n",
      " [-0.6627631 ]\n",
      " [-0.17903996]\n",
      " [-0.64480802]]\n",
      "loss=34150.27645694691\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01650739]\n",
      " [-0.10163376]\n",
      " [-0.31863243]\n",
      " ...\n",
      " [-0.24451161]\n",
      " [ 0.04753473]\n",
      " [-0.14505514]]\n",
      "t [[ 0.01650739]\n",
      " [-0.10163376]\n",
      " [-0.31863243]\n",
      " ...\n",
      " [-0.24451161]\n",
      " [ 0.04753473]\n",
      " [-0.14505514]]\n",
      "t [[ 0.00798607]\n",
      " [-0.24164068]\n",
      " [-0.46813809]\n",
      " ...\n",
      " [-0.34817522]\n",
      " [ 0.04678786]\n",
      " [-0.2512436 ]]\n",
      "t [[ 0.00798607]\n",
      " [-0.24164068]\n",
      " [-0.46813809]\n",
      " ...\n",
      " [-0.34817522]\n",
      " [ 0.04678786]\n",
      " [-0.2512436 ]]\n",
      "Current iteration=2, loss=37028.91854181399\n",
      "t [[-0.00554186]\n",
      " [-0.38006957]\n",
      " [-0.56394755]\n",
      " ...\n",
      " [-0.40885742]\n",
      " [ 0.02794269]\n",
      " [-0.33596084]]\n",
      "t [[-0.00554186]\n",
      " [-0.38006957]\n",
      " [-0.56394755]\n",
      " ...\n",
      " [-0.40885742]\n",
      " [ 0.02794269]\n",
      " [-0.33596084]]\n",
      "t [[-0.01897438]\n",
      " [-0.50784581]\n",
      " [-0.63866278]\n",
      " ...\n",
      " [-0.45415662]\n",
      " [ 0.00073957]\n",
      " [-0.40611732]]\n",
      "t [[-0.01897438]\n",
      " [-0.50784581]\n",
      " [-0.63866278]\n",
      " ...\n",
      " [-0.45415662]\n",
      " [ 0.00073957]\n",
      " [-0.40611732]]\n",
      "Current iteration=4, loss=35699.42742423358\n",
      "t [[-0.03098267]\n",
      " [-0.62358861]\n",
      " [-0.70307036]\n",
      " ...\n",
      " [-0.4930066 ]\n",
      " [-0.03059458]\n",
      " [-0.46538051]]\n",
      "t [[-0.03098267]\n",
      " [-0.62358861]\n",
      " [-0.70307036]\n",
      " ...\n",
      " [-0.4930066 ]\n",
      " [-0.03059458]\n",
      " [-0.46538051]]\n",
      "t [[-0.04135007]\n",
      " [-0.72802758]\n",
      " [-0.76110212]\n",
      " ...\n",
      " [-0.52847517]\n",
      " [-0.06382786]\n",
      " [-0.51607695]]\n",
      "t [[-0.04135007]\n",
      " [-0.72802758]\n",
      " [-0.76110212]\n",
      " ...\n",
      " [-0.52847517]\n",
      " [-0.06382786]\n",
      " [-0.51607695]]\n",
      "Current iteration=6, loss=34915.57267213152\n",
      "t [[-0.05019062]\n",
      " [-0.82239007]\n",
      " [-0.81432837]\n",
      " ...\n",
      " [-0.56163529]\n",
      " [-0.09764603]\n",
      " [-0.55983692]]\n",
      "t [[-0.05019062]\n",
      " [-0.82239007]\n",
      " [-0.81432837]\n",
      " ...\n",
      " [-0.56163529]\n",
      " [-0.09764603]\n",
      " [-0.55983692]]\n",
      "t [[-0.05769634]\n",
      " [-0.90790599]\n",
      " [-0.86347693]\n",
      " ...\n",
      " [-0.59287458]\n",
      " [-0.131232  ]\n",
      " [-0.59787101]]\n",
      "t [[-0.05769634]\n",
      " [-0.90790599]\n",
      " [-0.86347693]\n",
      " ...\n",
      " [-0.59287458]\n",
      " [-0.131232  ]\n",
      " [-0.59787101]]\n",
      "Current iteration=8, loss=34407.45205731731\n",
      "t [[-0.06405737]\n",
      " [-0.98567077]\n",
      " [-0.90896865]\n",
      " ...\n",
      " [-0.62235359]\n",
      " [-0.16406912]\n",
      " [-0.63111113]]\n",
      "t [[-0.06405737]\n",
      " [-0.98567077]\n",
      " [-0.90896865]\n",
      " ...\n",
      " [-0.62235359]\n",
      " [-0.16406912]\n",
      " [-0.63111113]]\n",
      "t [[-0.06944033]\n",
      " [-1.05662478]\n",
      " [-0.95110831]\n",
      " ...\n",
      " [-0.65016519]\n",
      " [-0.19583505]\n",
      " [-0.66029235]]\n",
      "loss=34055.633891689045\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01409913]\n",
      " [-0.08919987]\n",
      " [-0.32208333]\n",
      " ...\n",
      " [-0.24042261]\n",
      " [ 0.05110911]\n",
      " [-0.13606234]]\n",
      "t [[ 0.01409913]\n",
      " [-0.08919987]\n",
      " [-0.32208333]\n",
      " ...\n",
      " [-0.24042261]\n",
      " [ 0.05110911]\n",
      " [-0.13606234]]\n",
      "t [[ 0.00340722]\n",
      " [-0.21863271]\n",
      " [-0.47531667]\n",
      " ...\n",
      " [-0.34188645]\n",
      " [ 0.05341642]\n",
      " [-0.23457867]]\n",
      "t [[ 0.00340722]\n",
      " [-0.21863271]\n",
      " [-0.47531667]\n",
      " ...\n",
      " [-0.34188645]\n",
      " [ 0.05341642]\n",
      " [-0.23457867]]\n",
      "Current iteration=2, loss=37069.77006019396\n",
      "t [[-0.01192775]\n",
      " [-0.34808643]\n",
      " [-0.57417566]\n",
      " ...\n",
      " [-0.40081763]\n",
      " [ 0.03706254]\n",
      " [-0.31255784]]\n",
      "t [[-0.01192775]\n",
      " [-0.34808643]\n",
      " [-0.57417566]\n",
      " ...\n",
      " [-0.40081763]\n",
      " [ 0.03706254]\n",
      " [-0.31255784]]\n",
      "t [[-0.02677785]\n",
      " [-0.46812843]\n",
      " [-0.65136958]\n",
      " ...\n",
      " [-0.444646  ]\n",
      " [ 0.0118542 ]\n",
      " [-0.37673755]]\n",
      "t [[-0.02677785]\n",
      " [-0.46812843]\n",
      " [-0.65136958]\n",
      " ...\n",
      " [-0.444646  ]\n",
      " [ 0.0118542 ]\n",
      " [-0.37673755]]\n",
      "Current iteration=4, loss=35765.53038714416\n",
      "t [[-0.03984965]\n",
      " [-0.57712467]\n",
      " [-0.7178257 ]\n",
      " ...\n",
      " [-0.4822756 ]\n",
      " [-0.01791347]\n",
      " [-0.43066865]]\n",
      "t [[-0.03984965]\n",
      " [-0.57712467]\n",
      " [-0.7178257 ]\n",
      " ...\n",
      " [-0.4822756 ]\n",
      " [-0.01791347]\n",
      " [-0.43066865]]\n",
      "t [[-0.0509804 ]\n",
      " [-0.67562166]\n",
      " [-0.77757889]\n",
      " ...\n",
      " [-0.51675373]\n",
      " [-0.04993909]\n",
      " [-0.47658671]]\n",
      "t [[-0.0509804 ]\n",
      " [-0.67562166]\n",
      " [-0.77757889]\n",
      " ...\n",
      " [-0.51675373]\n",
      " [-0.04993909]\n",
      " [-0.47658671]]\n",
      "Current iteration=6, loss=34997.39958740269\n",
      "t [[-0.06033836]\n",
      " [-0.76470915]\n",
      " [-0.83227119]\n",
      " ...\n",
      " [-0.54912763]\n",
      " [-0.08284199]\n",
      " [-0.51604773]]\n",
      "t [[-0.06033836]\n",
      " [-0.76470915]\n",
      " [-0.83227119]\n",
      " ...\n",
      " [-0.54912763]\n",
      " [-0.08284199]\n",
      " [-0.51604773]]\n",
      "t [[-0.06816375]\n",
      " [-0.84551065]\n",
      " [-0.88268115]\n",
      " ...\n",
      " [-0.57975648]\n",
      " [-0.11574555]\n",
      " [-0.5502006 ]]\n",
      "t [[-0.06816375]\n",
      " [-0.84551065]\n",
      " [-0.88268115]\n",
      " ...\n",
      " [-0.57975648]\n",
      " [-0.11574555]\n",
      " [-0.5502006 ]]\n",
      "Current iteration=8, loss=34499.10420620534\n",
      "t [[-0.07468723]\n",
      " [-0.91903765]\n",
      " [-0.92926666]\n",
      " ...\n",
      " [-0.60877286]\n",
      " [-0.14808236]\n",
      " [-0.57992558]]\n",
      "t [[-0.07468723]\n",
      " [-0.91903765]\n",
      " [-0.92926666]\n",
      " ...\n",
      " [-0.60877286]\n",
      " [-0.14808236]\n",
      " [-0.57992558]]\n",
      "t [[-0.08010871]\n",
      " [-0.98616327]\n",
      " [-0.97236034]\n",
      " ...\n",
      " [-0.63624414]\n",
      " [-0.1794882 ]\n",
      " [-0.60591417]]\n",
      "loss=34153.561932662255\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01174174]\n",
      " [-0.09447805]\n",
      " [-0.31995813]\n",
      " ...\n",
      " [-0.14086934]\n",
      " [-0.01696594]\n",
      " [-0.28739626]]\n",
      "t [[ 0.01174174]\n",
      " [-0.09447805]\n",
      " [-0.31995813]\n",
      " ...\n",
      " [-0.14086934]\n",
      " [-0.01696594]\n",
      " [-0.28739626]]\n",
      "t [[-0.00081218]\n",
      " [-0.22885832]\n",
      " [-0.46972276]\n",
      " ...\n",
      " [-0.17266956]\n",
      " [ 0.00219576]\n",
      " [-0.51084676]]\n",
      "t [[-0.00081218]\n",
      " [-0.22885832]\n",
      " [-0.46972276]\n",
      " ...\n",
      " [-0.17266956]\n",
      " [ 0.00219576]\n",
      " [-0.51084676]]\n",
      "Current iteration=2, loss=37014.57209152647\n",
      "t [[-0.01760441]\n",
      " [-0.36276658]\n",
      " [-0.56517855]\n",
      " ...\n",
      " [-0.17669895]\n",
      " [ 0.02464776]\n",
      " [-0.69602637]]\n",
      "t [[-0.01760441]\n",
      " [-0.36276658]\n",
      " [-0.56517855]\n",
      " ...\n",
      " [-0.17669895]\n",
      " [ 0.02464776]\n",
      " [-0.69602637]]\n",
      "t [[-0.03362353]\n",
      " [-0.48680638]\n",
      " [-0.63928073]\n",
      " ...\n",
      " [-0.17472923]\n",
      " [ 0.04297686]\n",
      " [-0.85444434]]\n",
      "t [[-0.03362353]\n",
      " [-0.48680638]\n",
      " [-0.63928073]\n",
      " ...\n",
      " [-0.17472923]\n",
      " [ 0.04297686]\n",
      " [-0.85444434]]\n",
      "Current iteration=4, loss=35685.98633194591\n",
      "t [[-0.04765901]\n",
      " [-0.59938848]\n",
      " [-0.7029925 ]\n",
      " ...\n",
      " [-0.17296976]\n",
      " [ 0.05615976]\n",
      " [-0.9925977 ]]\n",
      "t [[-0.04765901]\n",
      " [-0.59938848]\n",
      " [-0.7029925 ]\n",
      " ...\n",
      " [-0.17296976]\n",
      " [ 0.05615976]\n",
      " [-0.9925977 ]]\n",
      "t [[-0.05960366]\n",
      " [-0.7011057 ]\n",
      " [-0.76032706]\n",
      " ...\n",
      " [-0.17292612]\n",
      " [ 0.06485756]\n",
      " [-1.11471088]]\n",
      "t [[-0.05960366]\n",
      " [-0.7011057 ]\n",
      " [-0.76032706]\n",
      " ...\n",
      " [-0.17292612]\n",
      " [ 0.06485756]\n",
      " [-1.11471088]]\n",
      "Current iteration=6, loss=34902.27391936733\n",
      "t [[-0.0696628 ]\n",
      " [-0.79309275]\n",
      " [-0.81288866]\n",
      " ...\n",
      " [-0.17464551]\n",
      " [ 0.07005083]\n",
      " [-1.22376374]]\n",
      "t [[-0.0696628 ]\n",
      " [-0.79309275]\n",
      " [-0.81288866]\n",
      " ...\n",
      " [-0.17464551]\n",
      " [ 0.07005083]\n",
      " [-1.22376374]]\n",
      "t [[-0.07810147]\n",
      " [-0.87651418]\n",
      " [-0.86141699]\n",
      " ...\n",
      " [-0.17778186]\n",
      " [ 0.07263928]\n",
      " [-1.32196694]]\n",
      "t [[-0.07810147]\n",
      " [-0.87651418]\n",
      " [-0.86141699]\n",
      " ...\n",
      " [-0.17778186]\n",
      " [ 0.07263928]\n",
      " [-1.32196694]]\n",
      "Current iteration=8, loss=34393.87707544716\n",
      "t [[-0.08516735]\n",
      " [-0.95241742]\n",
      " [-0.90633509]\n",
      " ...\n",
      " [-0.18194266]\n",
      " [ 0.07335039]\n",
      " [-1.41101684]]\n",
      "t [[-0.08516735]\n",
      " [-0.95241742]\n",
      " [-0.90633509]\n",
      " ...\n",
      " [-0.18194266]\n",
      " [ 0.07335039]\n",
      " [-1.41101684]]\n",
      "t [[-0.09107227]\n",
      " [-1.0217063 ]\n",
      " [-0.9479459 ]\n",
      " ...\n",
      " [-0.18678993]\n",
      " [ 0.07274391]\n",
      " [-1.49224743]]\n",
      "loss=34041.667678133905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.12789699]\n",
      " [-0.5463613 ]\n",
      " [-0.34340774]\n",
      " ...\n",
      " [-0.24888549]\n",
      " [ 0.05388508]\n",
      " [-0.14508715]]\n",
      "t [[ 0.12789699]\n",
      " [-0.5463613 ]\n",
      " [-0.34340774]\n",
      " ...\n",
      " [-0.24888549]\n",
      " [ 0.05388508]\n",
      " [-0.14508715]]\n",
      "t [[ 0.17954841]\n",
      " [-0.83110061]\n",
      " [-0.51492041]\n",
      " ...\n",
      " [-0.35391791]\n",
      " [ 0.05616993]\n",
      " [-0.25051618]]\n",
      "t [[ 0.17954841]\n",
      " [-0.83110061]\n",
      " [-0.51492041]\n",
      " ...\n",
      " [-0.35391791]\n",
      " [ 0.05616993]\n",
      " [-0.25051618]]\n",
      "Current iteration=2, loss=37027.46860617189\n",
      "t [[ 0.20348807]\n",
      " [-1.01959261]\n",
      " [-0.62920094]\n",
      " ...\n",
      " [-0.41605566]\n",
      " [ 0.03889674]\n",
      " [-0.33424638]]\n",
      "t [[ 0.20348807]\n",
      " [-1.01959261]\n",
      " [-0.62920094]\n",
      " ...\n",
      " [-0.41605566]\n",
      " [ 0.03889674]\n",
      " [-0.33424638]]\n",
      "t [[ 0.21525838]\n",
      " [-1.16140519]\n",
      " [-0.71869079]\n",
      " ...\n",
      " [-0.46307782]\n",
      " [ 0.01240186]\n",
      " [-0.40332048]]\n",
      "t [[ 0.21525838]\n",
      " [-1.16140519]\n",
      " [-0.71869079]\n",
      " ...\n",
      " [-0.46307782]\n",
      " [ 0.01240186]\n",
      " [-0.40332048]]\n",
      "Current iteration=4, loss=35720.338454474295\n",
      "t [[ 0.22096148]\n",
      " [-1.27548691]\n",
      " [-0.79451848]\n",
      " ...\n",
      " [-0.50377634]\n",
      " [-0.01876343]\n",
      " [-0.46146179]]\n",
      "t [[ 0.22096148]\n",
      " [-1.27548691]\n",
      " [-0.79451848]\n",
      " ...\n",
      " [-0.50377634]\n",
      " [-0.01876343]\n",
      " [-0.46146179]]\n",
      "t [[ 0.2233623 ]\n",
      " [-1.37062823]\n",
      " [-0.86109618]\n",
      " ...\n",
      " [-0.54108975]\n",
      " [-0.05215813]\n",
      " [-0.51102826]]\n",
      "t [[ 0.2233623 ]\n",
      " [-1.37062823]\n",
      " [-0.86109618]\n",
      " ...\n",
      " [-0.54108975]\n",
      " [-0.05215813]\n",
      " [-0.51102826]]\n",
      "Current iteration=6, loss=34954.685797822545\n",
      "t [[ 0.2238609 ]\n",
      " [-1.45163942]\n",
      " [-0.92046621]\n",
      " ...\n",
      " [-0.57601228]\n",
      " [-0.08632824]\n",
      " [-0.5536697 ]]\n",
      "t [[ 0.2238609 ]\n",
      " [-1.45163942]\n",
      " [-0.92046621]\n",
      " ...\n",
      " [-0.57601228]\n",
      " [-0.08632824]\n",
      " [-0.5536697 ]]\n",
      "t [[ 0.22323449]\n",
      " [-1.52154127]\n",
      " [-0.97377116]\n",
      " ...\n",
      " [-0.60889154]\n",
      " [-0.12036349]\n",
      " [-0.59060906]]\n",
      "t [[ 0.22323449]\n",
      " [-1.52154127]\n",
      " [-0.97377116]\n",
      " ...\n",
      " [-0.60889154]\n",
      " [-0.12036349]\n",
      " [-0.59060906]]\n",
      "Current iteration=8, loss=34459.836328997284\n",
      "t [[ 0.22194591]\n",
      " [-1.58243017]\n",
      " [-1.02178019]\n",
      " ...\n",
      " [-0.63987157]\n",
      " [-0.15368607]\n",
      " [-0.62278591]]\n",
      "t [[ 0.22194591]\n",
      " [-1.58243017]\n",
      " [-1.02178019]\n",
      " ...\n",
      " [-0.63987157]\n",
      " [-0.15368607]\n",
      " [-0.62278591]]\n",
      "t [[ 0.22028417]\n",
      " [-1.63585807]\n",
      " [-1.06508507]\n",
      " ...\n",
      " [-0.669042  ]\n",
      " [-0.18593467]\n",
      " [-0.65093979]]\n",
      "loss=34117.50258380073\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01686624]\n",
      " [-0.10384318]\n",
      " [-0.32555923]\n",
      " ...\n",
      " [-0.24982708]\n",
      " [ 0.04856809]\n",
      " [-0.14820851]]\n",
      "t [[ 0.01686624]\n",
      " [-0.10384318]\n",
      " [-0.32555923]\n",
      " ...\n",
      " [-0.24982708]\n",
      " [ 0.04856809]\n",
      " [-0.14820851]]\n",
      "t [[ 0.00763898]\n",
      " [-0.24769473]\n",
      " [-0.47472975]\n",
      " ...\n",
      " [-0.35275606]\n",
      " [ 0.04679151]\n",
      " [-0.25587227]]\n",
      "t [[ 0.00763898]\n",
      " [-0.24769473]\n",
      " [-0.47472975]\n",
      " ...\n",
      " [-0.35275606]\n",
      " [ 0.04679151]\n",
      " [-0.25587227]]\n",
      "Current iteration=2, loss=36986.163544894735\n",
      "t [[-0.00640372]\n",
      " [-0.38907636]\n",
      " [-0.5702632 ]\n",
      " ...\n",
      " [-0.41286966]\n",
      " [ 0.02674341]\n",
      " [-0.34149459]]\n",
      "t [[-0.00640372]\n",
      " [-0.38907636]\n",
      " [-0.5702632 ]\n",
      " ...\n",
      " [-0.41286966]\n",
      " [ 0.02674341]\n",
      " [-0.34149459]]\n",
      "t [[-0.02012707]\n",
      " [-0.51894758]\n",
      " [-0.64519882]\n",
      " ...\n",
      " [-0.45812823]\n",
      " [-0.0016082 ]\n",
      " [-0.41222071]]\n",
      "t [[-0.02012707]\n",
      " [-0.51894758]\n",
      " [-0.64519882]\n",
      " ...\n",
      " [-0.45812823]\n",
      " [-0.0016082 ]\n",
      " [-0.41222071]]\n",
      "Current iteration=4, loss=35653.639666689945\n",
      "t [[-0.03227522]\n",
      " [-0.63616529]\n",
      " [-0.71009571]\n",
      " ...\n",
      " [-0.49725064]\n",
      " [-0.0339904 ]\n",
      " [-0.47182011]]\n",
      "t [[-0.03227522]\n",
      " [-0.63616529]\n",
      " [-0.71009571]\n",
      " ...\n",
      " [-0.49725064]\n",
      " [-0.0339904 ]\n",
      " [-0.47182011]]\n",
      "t [[-0.04269076]\n",
      " [-0.74164687]\n",
      " [-0.76868804]\n",
      " ...\n",
      " [-0.53311663]\n",
      " [-0.06815658]\n",
      " [-0.52268324]]\n",
      "t [[-0.04269076]\n",
      " [-0.74164687]\n",
      " [-0.76868804]\n",
      " ...\n",
      " [-0.53311663]\n",
      " [-0.06815658]\n",
      " [-0.52268324]]\n",
      "Current iteration=6, loss=34873.25199935249\n",
      "t [[-0.05152456]\n",
      " [-0.83674464]\n",
      " [-0.82243879]\n",
      " ...\n",
      " [-0.56669225]\n",
      " [-0.10278796]\n",
      " [-0.56648569]]\n",
      "t [[-0.05152456]\n",
      " [-0.83674464]\n",
      " [-0.82243879]\n",
      " ...\n",
      " [-0.56669225]\n",
      " [-0.10278796]\n",
      " [-0.56648569]]\n",
      "t [[-0.05899001]\n",
      " [-0.92276978]\n",
      " [-0.87202993]\n",
      " ...\n",
      " [-0.59831463]\n",
      " [-0.13706895]\n",
      " [-0.60447118]]\n",
      "t [[-0.05899001]\n",
      " [-0.92276978]\n",
      " [-0.87202993]\n",
      " ...\n",
      " [-0.59831463]\n",
      " [-0.13706895]\n",
      " [-0.60447118]]\n",
      "Current iteration=8, loss=34369.83547997815\n",
      "t [[-0.06528961]\n",
      " [-1.00087212]\n",
      " [-0.91786916]\n",
      " ...\n",
      " [-0.62812471]\n",
      " [-0.17048873]\n",
      " [-0.63759605]]\n",
      "t [[-0.06528961]\n",
      " [-1.00087212]\n",
      " [-0.91786916]\n",
      " ...\n",
      " [-0.62812471]\n",
      " [-0.17048873]\n",
      " [-0.63759605]]\n",
      "t [[-0.07059743]\n",
      " [-1.0720301 ]\n",
      " [-0.96026328]\n",
      " ...\n",
      " [-0.65621094]\n",
      " [-0.20273376]\n",
      " [-0.66661339]]\n",
      "loss=34022.39177769114\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01440564]\n",
      " [-0.091139  ]\n",
      " [-0.32908514]\n",
      " ...\n",
      " [-0.24564918]\n",
      " [ 0.05222018]\n",
      " [-0.13902022]]\n",
      "t [[ 0.01440564]\n",
      " [-0.091139  ]\n",
      " [-0.32908514]\n",
      " ...\n",
      " [-0.24564918]\n",
      " [ 0.05222018]\n",
      " [-0.13902022]]\n",
      "t [[ 0.00296511]\n",
      " [-0.22422877]\n",
      " [-0.48206716]\n",
      " ...\n",
      " [-0.34636945]\n",
      " [ 0.05355121]\n",
      " [-0.23887387]]\n",
      "t [[ 0.00296511]\n",
      " [-0.22422877]\n",
      " [-0.48206716]\n",
      " ...\n",
      " [-0.34636945]\n",
      " [ 0.05355121]\n",
      " [-0.23887387]]\n",
      "Current iteration=2, loss=37027.72667897431\n",
      "t [[-0.01290814]\n",
      " [-0.35650847]\n",
      " [-0.58068916]\n",
      " ...\n",
      " [-0.40471535]\n",
      " [ 0.0360242 ]\n",
      " [-0.31765198]]\n",
      "t [[-0.01290814]\n",
      " [-0.35650847]\n",
      " [-0.58068916]\n",
      " ...\n",
      " [-0.40471535]\n",
      " [ 0.0360242 ]\n",
      " [-0.31765198]]\n",
      "t [[-0.02805437]\n",
      " [-0.47855723]\n",
      " [-0.65812134]\n",
      " ...\n",
      " [-0.44849027]\n",
      " [ 0.00967844]\n",
      " [-0.38232049]]\n",
      "t [[-0.02805437]\n",
      " [-0.47855723]\n",
      " [-0.65812134]\n",
      " ...\n",
      " [-0.44849027]\n",
      " [ 0.00967844]\n",
      " [-0.38232049]]\n",
      "Current iteration=4, loss=35720.65629871094\n",
      "t [[-0.04125815]\n",
      " [-0.58896685]\n",
      " [-0.72507432]\n",
      " ...\n",
      " [-0.48638784]\n",
      " [-0.02114046]\n",
      " [-0.43652738]]\n",
      "t [[-0.04125815]\n",
      " [-0.58896685]\n",
      " [-0.72507432]\n",
      " ...\n",
      " [-0.48638784]\n",
      " [-0.02114046]\n",
      " [-0.43652738]]\n",
      "t [[-0.05242073]\n",
      " [-0.68846417]\n",
      " [-0.78538996]\n",
      " ...\n",
      " [-0.52126691]\n",
      " [-0.05411174]\n",
      " [-0.48256804]]\n",
      "t [[-0.05242073]\n",
      " [-0.68846417]\n",
      " [-0.78538996]\n",
      " ...\n",
      " [-0.52126691]\n",
      " [-0.05411174]\n",
      " [-0.48256804]]\n",
      "Current iteration=6, loss=34955.92737080339\n",
      "t [[-0.06175083]\n",
      " [-0.77825888]\n",
      " [-0.84060516]\n",
      " ...\n",
      " [-0.55406587]\n",
      " [-0.08784603]\n",
      " [-0.52204041]]\n",
      "t [[-0.06175083]\n",
      " [-0.77825888]\n",
      " [-0.84060516]\n",
      " ...\n",
      " [-0.55406587]\n",
      " [-0.08784603]\n",
      " [-0.52204041]]\n",
      "t [[-0.06951252]\n",
      " [-0.85955221]\n",
      " [-0.89145374]\n",
      " ...\n",
      " [-0.58509128]\n",
      " [-0.12146509]\n",
      " [-0.55612385]]\n",
      "t [[-0.06951252]\n",
      " [-0.85955221]\n",
      " [-0.89145374]\n",
      " ...\n",
      " [-0.58509128]\n",
      " [-0.12146509]\n",
      " [-0.55612385]]\n",
      "Current iteration=8, loss=34462.19986144171\n",
      "t [[-0.07595051]\n",
      " [-0.93340747]\n",
      " [-0.93838108]\n",
      " ...\n",
      " [-0.61445436]\n",
      " [-0.15440519]\n",
      " [-0.58572094]]\n",
      "t [[-0.07595051]\n",
      " [-0.93340747]\n",
      " [-0.93838108]\n",
      " ...\n",
      " [-0.61445436]\n",
      " [-0.15440519]\n",
      " [-0.58572094]]\n",
      "t [[-0.08127325]\n",
      " [-1.00073399]\n",
      " [-0.98172232]\n",
      " ...\n",
      " [-0.6422168 ]\n",
      " [-0.18630952]\n",
      " [-0.61153961]]\n",
      "loss=34120.893042234995\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.011997  ]\n",
      " [-0.09653192]\n",
      " [-0.32691374]\n",
      " ...\n",
      " [-0.14393172]\n",
      " [-0.01733477]\n",
      " [-0.29364401]]\n",
      "t [[ 0.011997  ]\n",
      " [-0.09653192]\n",
      " [-0.32691374]\n",
      " ...\n",
      " [-0.14393172]\n",
      " [-0.01733477]\n",
      " [-0.29364401]]\n",
      "t [[-0.00133521]\n",
      " [-0.23466749]\n",
      " [-0.47632624]\n",
      " ...\n",
      " [-0.17411132]\n",
      " [ 0.00301214]\n",
      " [-0.52057707]]\n",
      "t [[-0.00133521]\n",
      " [-0.23466749]\n",
      " [-0.47632624]\n",
      " ...\n",
      " [-0.17411132]\n",
      " [ 0.00301214]\n",
      " [-0.52057707]]\n",
      "Current iteration=2, loss=36971.812904618135\n",
      "t [[-0.01867987]\n",
      " [-0.37147788]\n",
      " [-0.57147253]\n",
      " ...\n",
      " [-0.17700097]\n",
      " [ 0.02610773]\n",
      " [-0.70812529]]\n",
      "t [[-0.01867987]\n",
      " [-0.37147788]\n",
      " [-0.57147253]\n",
      " ...\n",
      " [-0.17700097]\n",
      " [ 0.02610773]\n",
      " [-0.70812529]]\n",
      "t [[-0.03500164]\n",
      " [-0.49758234]\n",
      " [-0.64576482]\n",
      " ...\n",
      " [-0.17457656]\n",
      " [ 0.0445762 ]\n",
      " [-0.86824452]]\n",
      "t [[-0.03500164]\n",
      " [-0.49758234]\n",
      " [-0.64576482]\n",
      " ...\n",
      " [-0.17457656]\n",
      " [ 0.0445762 ]\n",
      " [-0.86824452]]\n",
      "Current iteration=4, loss=35640.22813869985\n",
      "t [[-0.04917199]\n",
      " [-0.61162047]\n",
      " [-0.70994343]\n",
      " ...\n",
      " [-0.17278706]\n",
      " [ 0.05760285]\n",
      " [-1.00764054]]\n",
      "t [[-0.04917199]\n",
      " [-0.61162047]\n",
      " [-0.70994343]\n",
      " ...\n",
      " [-0.17278706]\n",
      " [ 0.05760285]\n",
      " [-1.00764054]]\n",
      "t [[-0.06114969]\n",
      " [-0.71436864]\n",
      " [-0.76782308]\n",
      " ...\n",
      " [-0.17292407]\n",
      " [ 0.06600463]\n",
      " [-1.13066479]]\n",
      "t [[-0.06114969]\n",
      " [-0.71436864]\n",
      " [-0.76782308]\n",
      " ...\n",
      " [-0.17292407]\n",
      " [ 0.06600463]\n",
      " [-1.13066479]]\n",
      "Current iteration=6, loss=34859.950521018465\n",
      "t [[-0.07118132]\n",
      " [-0.80708437]\n",
      " [-0.8208988 ]\n",
      " ...\n",
      " [-0.17490787]\n",
      " [ 0.07085518]\n",
      " [-1.24038299]]\n",
      "t [[-0.07118132]\n",
      " [-0.80708437]\n",
      " [-0.8208988 ]\n",
      " ...\n",
      " [-0.17490787]\n",
      " [ 0.07085518]\n",
      " [-1.24038299]]\n",
      "t [[-0.07955611]\n",
      " [-0.89101219]\n",
      " [-0.86986282]\n",
      " ...\n",
      " [-0.17832585]\n",
      " [ 0.07310461]\n",
      " [-1.3390664 ]]\n",
      "t [[-0.07955611]\n",
      " [-0.89101219]\n",
      " [-0.86986282]\n",
      " ...\n",
      " [-0.17832585]\n",
      " [ 0.07310461]\n",
      " [-1.3390664 ]]\n",
      "Current iteration=8, loss=34356.23392773602\n",
      "t [[-0.08653583]\n",
      " [-0.96725304]\n",
      " [-0.9151239 ]\n",
      " ...\n",
      " [-0.18275335]\n",
      " [ 0.0735056 ]\n",
      " [-1.42845521]]\n",
      "t [[-0.08653583]\n",
      " [-0.96725304]\n",
      " [-0.9151239 ]\n",
      " ...\n",
      " [-0.18275335]\n",
      " [ 0.0735056 ]\n",
      " [-1.42845521]]\n",
      "t [[-0.09234087]\n",
      " [-1.03674819]\n",
      " [-0.95698631]\n",
      " ...\n",
      " [-0.18783849]\n",
      " [ 0.07262901]\n",
      " [-1.50991552]]\n",
      "loss=34008.38552962703\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.13061821]\n",
      " [-0.557986  ]\n",
      " [-0.35071429]\n",
      " ...\n",
      " [-0.25418092]\n",
      " [ 0.05503157]\n",
      " [-0.14817412]]\n",
      "t [[ 0.13061821]\n",
      " [-0.557986  ]\n",
      " [-0.35071429]\n",
      " ...\n",
      " [-0.25418092]\n",
      " [ 0.05503157]\n",
      " [-0.14817412]]\n",
      "t [[ 0.18178632]\n",
      " [-0.84336088]\n",
      " [-0.52231647]\n",
      " ...\n",
      " [-0.35846529]\n",
      " [ 0.05630582]\n",
      " [-0.2550153 ]]\n",
      "t [[ 0.18178632]\n",
      " [-0.84336088]\n",
      " [-0.52231647]\n",
      " ...\n",
      " [-0.35846529]\n",
      " [ 0.05630582]\n",
      " [-0.2550153 ]]\n",
      "Current iteration=2, loss=36986.07690477786\n",
      "t [[ 0.20505761]\n",
      " [-1.03173745]\n",
      " [-0.63656896]\n",
      " ...\n",
      " [-0.42007864]\n",
      " [ 0.03782577]\n",
      " [-0.33960037]]\n",
      "t [[ 0.20505761]\n",
      " [-1.03173745]\n",
      " [-0.63656896]\n",
      " ...\n",
      " [-0.42007864]\n",
      " [ 0.03782577]\n",
      " [-0.33960037]]\n",
      "t [[ 0.21629881]\n",
      " [-1.17354742]\n",
      " [-0.72635224]\n",
      " ...\n",
      " [-0.46711329]\n",
      " [ 0.010166  ]\n",
      " [-0.40920258]]\n",
      "t [[ 0.21629881]\n",
      " [-1.17354742]\n",
      " [-0.72635224]\n",
      " ...\n",
      " [-0.46711329]\n",
      " [ 0.010166  ]\n",
      " [-0.40920258]]\n",
      "Current iteration=4, loss=35676.420083068595\n",
      "t [[ 0.22160691]\n",
      " [-1.28767399]\n",
      " [-0.80261547]\n",
      " ...\n",
      " [-0.50812769]\n",
      " [-0.02206782]\n",
      " [-0.4676455 ]]\n",
      "t [[ 0.22160691]\n",
      " [-1.28767399]\n",
      " [-0.80261547]\n",
      " ...\n",
      " [-0.50812769]\n",
      " [-0.02206782]\n",
      " [-0.4676455 ]]\n",
      "t [[ 0.2237089 ]\n",
      " [-1.3828116 ]\n",
      " [-0.86961699]\n",
      " ...\n",
      " [-0.54586843]\n",
      " [-0.05641449]\n",
      " [-0.51735006]]\n",
      "t [[ 0.2237089 ]\n",
      " [-1.3828116 ]\n",
      " [-0.86961699]\n",
      " ...\n",
      " [-0.54586843]\n",
      " [-0.05641449]\n",
      " [-0.51735006]]\n",
      "Current iteration=6, loss=34914.28447463349\n",
      "t [[ 0.22397704]\n",
      " [-1.4637328 ]\n",
      " [-0.92932358]\n",
      " ...\n",
      " [-0.58122431]\n",
      " [-0.09141258]\n",
      " [-0.56001064]]\n",
      "t [[ 0.22397704]\n",
      " [-1.4637328 ]\n",
      " [-0.92932358]\n",
      " ...\n",
      " [-0.58122431]\n",
      " [-0.09141258]\n",
      " [-0.56001064]]\n",
      "t [[ 0.22317146]\n",
      " [-1.53345846]\n",
      " [-0.98285337]\n",
      " ...\n",
      " [-0.61449498]\n",
      " [-0.12615228]\n",
      " [-0.59688264]]\n",
      "t [[ 0.22317146]\n",
      " [-1.53345846]\n",
      " [-0.98285337]\n",
      " ...\n",
      " [-0.61449498]\n",
      " [-0.12615228]\n",
      " [-0.59688264]]\n",
      "Current iteration=8, loss=34423.993896594984\n",
      "t [[ 0.22174422]\n",
      " [-1.59410002]\n",
      " [-1.03097631]\n",
      " ...\n",
      " [-0.64580719]\n",
      " [-0.16006173]\n",
      " [-0.62892942]]\n",
      "t [[ 0.22174422]\n",
      " [-1.59410002]\n",
      " [-1.03097631]\n",
      " ...\n",
      " [-0.64580719]\n",
      " [-0.16006173]\n",
      " [-0.62892942]]\n",
      "t [[ 0.21997677]\n",
      " [-1.6472273 ]\n",
      " [-1.07429539]\n",
      " ...\n",
      " [-0.67524795]\n",
      " [-0.19278899]\n",
      " [-0.65690799]]\n",
      "loss=34085.83471228776\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0172251 ]\n",
      " [-0.10605261]\n",
      " [-0.33248602]\n",
      " ...\n",
      " [-0.25514255]\n",
      " [ 0.04960145]\n",
      " [-0.15136189]]\n",
      "t [[ 0.0172251 ]\n",
      " [-0.10605261]\n",
      " [-0.33248602]\n",
      " ...\n",
      " [-0.25514255]\n",
      " [ 0.04960145]\n",
      " [-0.15136189]]\n",
      "t [[ 0.00727182]\n",
      " [-0.2537798 ]\n",
      " [-0.48117916]\n",
      " ...\n",
      " [-0.3572182 ]\n",
      " [ 0.04675553]\n",
      " [-0.26046732]]\n",
      "t [[ 0.00727182]\n",
      " [-0.2537798 ]\n",
      " [-0.48117916]\n",
      " ...\n",
      " [-0.3572182 ]\n",
      " [ 0.04675553]\n",
      " [-0.26046732]]\n",
      "Current iteration=2, loss=36944.07164678267\n",
      "t [[-0.00727898]\n",
      " [-0.39807981]\n",
      " [-0.57643216]\n",
      " ...\n",
      " [-0.41676449]\n",
      " [ 0.02549504]\n",
      " [-0.34697006]]\n",
      "t [[-0.00727898]\n",
      " [-0.39807981]\n",
      " [-0.57643216]\n",
      " ...\n",
      " [-0.41676449]\n",
      " [ 0.02549504]\n",
      " [-0.34697006]]\n",
      "t [[-0.02127959]\n",
      " [-0.52999316]\n",
      " [-0.65161719]\n",
      " ...\n",
      " [-0.46201379]\n",
      " [-0.00400256]\n",
      " [-0.41824441]]\n",
      "t [[-0.02127959]\n",
      " [-0.52999316]\n",
      " [-0.65161719]\n",
      " ...\n",
      " [-0.46201379]\n",
      " [-0.00400256]\n",
      " [-0.41824441]]\n",
      "Current iteration=4, loss=35608.84350935723\n",
      "t [[-0.03355508]\n",
      " [-0.64863408]\n",
      " [-0.71702554]\n",
      " ...\n",
      " [-0.50143452]\n",
      " [-0.03742462]\n",
      " [-0.47816035]]\n",
      "t [[-0.03355508]\n",
      " [-0.64863408]\n",
      " [-0.71702554]\n",
      " ...\n",
      " [-0.50143452]\n",
      " [-0.03742462]\n",
      " [-0.47816035]]\n",
      "t [[-0.04400919]\n",
      " [-0.755113  ]\n",
      " [-0.77618577]\n",
      " ...\n",
      " [-0.53771088]\n",
      " [-0.0725117 ]\n",
      " [-0.52917236]]\n",
      "t [[-0.04400919]\n",
      " [-0.755113  ]\n",
      " [-0.77618577]\n",
      " ...\n",
      " [-0.53771088]\n",
      " [-0.0725117 ]\n",
      " [-0.52917236]]\n",
      "Current iteration=6, loss=34832.09596611099\n",
      "t [[-0.05282935]\n",
      " [-0.85090732]\n",
      " [-0.83045632]\n",
      " ...\n",
      " [-0.57170417]\n",
      " [-0.10794124]\n",
      " [-0.57300157]]\n",
      "t [[-0.05282935]\n",
      " [-0.85090732]\n",
      " [-0.83045632]\n",
      " ...\n",
      " [-0.57170417]\n",
      " [-0.10794124]\n",
      " [-0.57300157]]\n",
      "t [[-0.06024959]\n",
      " [-0.93740835]\n",
      " [-0.88047785]\n",
      " ...\n",
      " [-0.60370477]\n",
      " [-0.14289986]\n",
      " [-0.61092498]]\n",
      "t [[-0.06024959]\n",
      " [-0.93740835]\n",
      " [-0.88047785]\n",
      " ...\n",
      " [-0.60370477]\n",
      " [-0.14289986]\n",
      " [-0.61092498]]\n",
      "Current iteration=8, loss=34333.40171146786\n",
      "t [[-0.06648418]\n",
      " [-1.01581922]\n",
      " [-0.92664844]\n",
      " ...\n",
      " [-0.63383706]\n",
      " [-0.17688342]\n",
      " [-0.64392328]]\n",
      "t [[-0.06648418]\n",
      " [-1.01581922]\n",
      " [-0.92664844]\n",
      " ...\n",
      " [-0.63383706]\n",
      " [-0.17688342]\n",
      " [-0.64392328]]\n",
      "t [[-0.07171426]\n",
      " [-1.0871557 ]\n",
      " [-0.96927928]\n",
      " ...\n",
      " [-0.66218716]\n",
      " [-0.20958791]\n",
      " [-0.67276745]]\n",
      "loss=33990.28013863641\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01471214]\n",
      " [-0.09307812]\n",
      " [-0.33608695]\n",
      " ...\n",
      " [-0.25087576]\n",
      " [ 0.05333124]\n",
      " [-0.1419781 ]]\n",
      "t [[ 0.01471214]\n",
      " [-0.09307812]\n",
      " [-0.33608695]\n",
      " ...\n",
      " [-0.25087576]\n",
      " [ 0.05333124]\n",
      " [-0.1419781 ]]\n",
      "t [[ 0.00250307]\n",
      " [-0.22985769]\n",
      " [-0.48867534]\n",
      " ...\n",
      " [-0.35073524]\n",
      " [ 0.05364575]\n",
      " [-0.24313664]]\n",
      "t [[ 0.00250307]\n",
      " [-0.22985769]\n",
      " [-0.48867534]\n",
      " ...\n",
      " [-0.35073524]\n",
      " [ 0.05364575]\n",
      " [-0.24313664]]\n",
      "Current iteration=2, loss=36986.34228084129\n",
      "t [[-0.01390097]\n",
      " [-0.3649314 ]\n",
      " [-0.58705433]\n",
      " ...\n",
      " [-0.40849705]\n",
      " [ 0.03493522]\n",
      " [-0.32269033]]\n",
      "t [[-0.01390097]\n",
      " [-0.3649314 ]\n",
      " [-0.58705433]\n",
      " ...\n",
      " [-0.40849705]\n",
      " [ 0.03493522]\n",
      " [-0.32269033]]\n",
      "t [[-0.02932861]\n",
      " [-0.48893643]\n",
      " [-0.6647525 ]\n",
      " ...\n",
      " [-0.45225017]\n",
      " [ 0.00745337]\n",
      " [-0.38782778]]\n",
      "t [[-0.02932861]\n",
      " [-0.48893643]\n",
      " [-0.6647525 ]\n",
      " ...\n",
      " [-0.45225017]\n",
      " [ 0.00745337]\n",
      " [-0.38782778]]\n",
      "Current iteration=4, loss=35676.75698152449\n",
      "t [[-0.04265073]\n",
      " [-0.60070992]\n",
      " [-0.7322236 ]\n",
      " ...\n",
      " [-0.49044226]\n",
      " [-0.02440975]\n",
      " [-0.44229248]]\n",
      "t [[-0.04265073]\n",
      " [-0.60070992]\n",
      " [-0.7322236 ]\n",
      " ...\n",
      " [-0.49044226]\n",
      " [-0.02440975]\n",
      " [-0.44229248]]\n",
      "t [[-0.05383468]\n",
      " [-0.70116431]\n",
      " [-0.79310841]\n",
      " ...\n",
      " [-0.52573604]\n",
      " [-0.05831565]\n",
      " [-0.48843962]]\n",
      "t [[-0.05383468]\n",
      " [-0.70116431]\n",
      " [-0.79310841]\n",
      " ...\n",
      " [-0.52573604]\n",
      " [-0.05831565]\n",
      " [-0.48843962]]\n",
      "Current iteration=6, loss=34915.59522989633\n",
      "t [[-0.0631294 ]\n",
      " [-0.79162929]\n",
      " [-0.84884135]\n",
      " ...\n",
      " [-0.55896297]\n",
      " [-0.09286696]\n",
      " [-0.52790925]]\n",
      "t [[-0.0631294 ]\n",
      " [-0.79162929]\n",
      " [-0.84884135]\n",
      " ...\n",
      " [-0.55896297]\n",
      " [-0.09286696]\n",
      " [-0.52790925]]\n",
      "t [[-0.07082211]\n",
      " [-0.87338268]\n",
      " [-0.90011598]\n",
      " ...\n",
      " [-0.59038063]\n",
      " [-0.12718447]\n",
      " [-0.5619113 ]]\n",
      "t [[-0.07082211]\n",
      " [-0.87338268]\n",
      " [-0.90011598]\n",
      " ...\n",
      " [-0.59038063]\n",
      " [-0.12718447]\n",
      " [-0.5619113 ]]\n",
      "Current iteration=8, loss=34426.451511996056\n",
      "t [[-0.07717093]\n",
      " [-0.94753854]\n",
      " [-0.94736871]\n",
      " ...\n",
      " [-0.62008193]\n",
      " [-0.16070897]\n",
      " [-0.5913706 ]]\n",
      "t [[-0.07717093]\n",
      " [-0.94753854]\n",
      " [-0.94736871]\n",
      " ...\n",
      " [-0.62008193]\n",
      " [-0.16070897]\n",
      " [-0.5913706 ]]\n",
      "t [[-0.0823924 ]\n",
      " [-1.01504167]\n",
      " [-0.9909395 ]\n",
      " ...\n",
      " [-0.64812492]\n",
      " [-0.19309188]\n",
      " [-0.61701132]]\n",
      "loss=34089.32868209493\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01225225]\n",
      " [-0.09858579]\n",
      " [-0.33386935]\n",
      " ...\n",
      " [-0.14699409]\n",
      " [-0.01770359]\n",
      " [-0.29989175]]\n",
      "t [[ 0.01225225]\n",
      " [-0.09858579]\n",
      " [-0.33386935]\n",
      " ...\n",
      " [-0.14699409]\n",
      " [-0.01770359]\n",
      " [-0.29989175]]\n",
      "t [[-0.00187772]\n",
      " [-0.24050902]\n",
      " [-0.4827866 ]\n",
      " ...\n",
      " [-0.17546137]\n",
      " [ 0.00385919]\n",
      " [-0.53025171]]\n",
      "t [[-0.00187772]\n",
      " [-0.24050902]\n",
      " [-0.4827866 ]\n",
      " ...\n",
      " [-0.17546137]\n",
      " [ 0.00385919]\n",
      " [-0.53025171]]\n",
      "Current iteration=2, loss=36929.72175465302\n",
      "t [[-0.01976669]\n",
      " [-0.38018877]\n",
      " [-0.57761827]\n",
      " ...\n",
      " [-0.17722637]\n",
      " [ 0.0275779 ]\n",
      " [-0.72012175]]\n",
      "t [[-0.01976669]\n",
      " [-0.38018877]\n",
      " [-0.57761827]\n",
      " ...\n",
      " [-0.17722637]\n",
      " [ 0.0275779 ]\n",
      " [-0.72012175]]\n",
      "t [[-0.03637588]\n",
      " [-0.50830631]\n",
      " [-0.65212986]\n",
      " ...\n",
      " [-0.17438873]\n",
      " [ 0.04615468]\n",
      " [-0.88190035]]\n",
      "t [[-0.03637588]\n",
      " [-0.50830631]\n",
      " [-0.65212986]\n",
      " ...\n",
      " [-0.17438873]\n",
      " [ 0.04615468]\n",
      " [-0.88190035]]\n",
      "Current iteration=4, loss=35595.460157673246\n",
      "t [[-0.05066715]\n",
      " [-0.6237497 ]\n",
      " [-0.71679807]\n",
      " ...\n",
      " [-0.17260451]\n",
      " [ 0.05900046]\n",
      " [-1.0225007 ]]\n",
      "t [[-0.05066715]\n",
      " [-0.6237497 ]\n",
      " [-0.71679807]\n",
      " ...\n",
      " [-0.17260451]\n",
      " [ 0.05900046]\n",
      " [-1.0225007 ]]\n",
      "t [[-0.06266724]\n",
      " [-0.72748431]\n",
      " [-0.77523086]\n",
      " ...\n",
      " [-0.17294415]\n",
      " [ 0.0670916 ]\n",
      " [-1.14640126]]\n",
      "t [[-0.06266724]\n",
      " [-0.72748431]\n",
      " [-0.77523086]\n",
      " ...\n",
      " [-0.17294415]\n",
      " [ 0.0670916 ]\n",
      " [-1.14640126]]\n",
      "Current iteration=6, loss=34818.790037895866\n",
      "t [[-0.07266374]\n",
      " [-0.82089062]\n",
      " [-0.82881662]\n",
      " ...\n",
      " [-0.17520284]\n",
      " [ 0.07159354]\n",
      " [-1.25675374]]\n",
      "t [[-0.07266374]\n",
      " [-0.82089062]\n",
      " [-0.82881662]\n",
      " ...\n",
      " [-0.17520284]\n",
      " [ 0.07159354]\n",
      " [-1.25675374]]\n",
      "t [[-0.08096928]\n",
      " [-0.90529206]\n",
      " [-0.87820464]\n",
      " ...\n",
      " [-0.17890492]\n",
      " [ 0.07350446]\n",
      " [-1.35588996]]\n",
      "t [[-0.08096928]\n",
      " [-0.90529206]\n",
      " [-0.87820464]\n",
      " ...\n",
      " [-0.17890492]\n",
      " [ 0.07350446]\n",
      " [-1.35588996]]\n",
      "Current iteration=8, loss=34319.77263655777\n",
      "t [[-0.08785908]\n",
      " [-0.981842  ]\n",
      " [-0.92379296]\n",
      " ...\n",
      " [-0.1835966 ]\n",
      " [ 0.07360014]\n",
      " [-1.44559363]]\n",
      "t [[-0.08785908]\n",
      " [-0.981842  ]\n",
      " [-0.92379296]\n",
      " ...\n",
      " [-0.1835966 ]\n",
      " [ 0.07360014]\n",
      " [-1.44559363]]\n",
      "t [[-0.09356163]\n",
      " [-1.05151839]\n",
      " [-0.9658895 ]\n",
      " ...\n",
      " [-0.18891413]\n",
      " [ 0.07246096]\n",
      " [-1.5272627 ]]\n",
      "loss=33976.233649941096\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.13333942]\n",
      " [-0.56961071]\n",
      " [-0.35802084]\n",
      " ...\n",
      " [-0.25947636]\n",
      " [ 0.05617806]\n",
      " [-0.15126108]]\n",
      "t [[ 0.13333942]\n",
      " [-0.56961071]\n",
      " [-0.35802084]\n",
      " ...\n",
      " [-0.25947636]\n",
      " [ 0.05617806]\n",
      " [-0.15126108]]\n",
      "t [[ 0.18396272]\n",
      " [-0.85541066]\n",
      " [-0.52957451]\n",
      " ...\n",
      " [-0.36289682]\n",
      " [ 0.05640119]\n",
      " [-0.25948162]]\n",
      "t [[ 0.18396272]\n",
      " [-0.85541066]\n",
      " [-0.52957451]\n",
      " ...\n",
      " [-0.36289682]\n",
      " [ 0.05640119]\n",
      " [-0.25948162]]\n",
      "Current iteration=2, loss=36945.32517553136\n",
      "t [[ 0.20655613]\n",
      " [-1.0436314 ]\n",
      " [-0.64378736]\n",
      " ...\n",
      " [-0.42398908]\n",
      " [ 0.03670393]\n",
      " [-0.34489787]]\n",
      "t [[ 0.20655613]\n",
      " [-1.0436314 ]\n",
      " [-0.64378736]\n",
      " ...\n",
      " [-0.42398908]\n",
      " [ 0.03670393]\n",
      " [-0.34489787]]\n",
      "t [[ 0.2172762 ]\n",
      " [-1.18544259]\n",
      " [-0.73388178]\n",
      " ...\n",
      " [-0.47106747]\n",
      " [ 0.00788089]\n",
      " [-0.41500774]]\n",
      "t [[ 0.2172762 ]\n",
      " [-1.18544259]\n",
      " [-0.73388178]\n",
      " ...\n",
      " [-0.47106747]\n",
      " [ 0.00788089]\n",
      " [-0.41500774]]\n",
      "Current iteration=4, loss=35633.446276297866\n",
      "t [[ 0.22220021]\n",
      " [-1.29961732]\n",
      " [-0.81059183]\n",
      " ...\n",
      " [-0.51242229]\n",
      " [-0.02541378]\n",
      " [-0.47373356]]\n",
      "t [[ 0.22220021]\n",
      " [-1.29961732]\n",
      " [-0.81059183]\n",
      " ...\n",
      " [-0.51242229]\n",
      " [-0.02541378]\n",
      " [-0.47373356]]\n",
      "t [[ 0.22401322]\n",
      " [-1.39474601]\n",
      " [-0.87801583]\n",
      " ...\n",
      " [-0.55060169]\n",
      " [-0.06070051]\n",
      " [-0.52355937]]\n",
      "t [[ 0.22401322]\n",
      " [-1.39474601]\n",
      " [-0.87801583]\n",
      " ...\n",
      " [-0.55060169]\n",
      " [-0.06070051]\n",
      " [-0.52355937]]\n",
      "Current iteration=6, loss=34874.980678171676\n",
      "t [[ 0.22405932]\n",
      " [-1.47556633]\n",
      " [-0.93804809]\n",
      " ...\n",
      " [-0.58639158]\n",
      " [-0.09651132]\n",
      " [-0.56622435]]\n",
      "t [[ 0.22405932]\n",
      " [-1.47556633]\n",
      " [-0.93804809]\n",
      " ...\n",
      " [-0.58639158]\n",
      " [-0.09651132]\n",
      " [-0.56622435]]\n",
      "t [[ 0.2230819 ]\n",
      " [-1.54510306]\n",
      " [-0.99178645]\n",
      " ...\n",
      " [-0.62004753]\n",
      " [-0.1319377 ]\n",
      " [-0.60301645]]\n",
      "t [[ 0.2230819 ]\n",
      " [-1.54510306]\n",
      " [-0.99178645]\n",
      " ...\n",
      " [-0.62004753]\n",
      " [-0.1319377 ]\n",
      " [-0.60301645]]\n",
      "Current iteration=8, loss=34389.258711167226\n",
      "t [[ 0.22152258]\n",
      " [-1.60548524]\n",
      " [-1.04000485]\n",
      " ...\n",
      " [-0.65168207]\n",
      " [-0.16641464]\n",
      " [-0.63492273]]\n",
      "t [[ 0.22152258]\n",
      " [-1.60548524]\n",
      " [-1.04000485]\n",
      " ...\n",
      " [-0.65168207]\n",
      " [-0.16641464]\n",
      " [-0.63492273]]\n",
      "t [[ 0.21965543]\n",
      " [-1.65830173]\n",
      " [-1.08331946]\n",
      " ...\n",
      " [-0.68138164]\n",
      " [-0.19960046]\n",
      " [-0.66271752]]\n",
      "loss=34055.22059358888\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01758395]\n",
      " [-0.10826204]\n",
      " [-0.33941281]\n",
      " ...\n",
      " [-0.26045802]\n",
      " [ 0.05063482]\n",
      " [-0.15451526]]\n",
      "t [[ 0.01758395]\n",
      " [-0.10826204]\n",
      " [-0.33941281]\n",
      " ...\n",
      " [-0.26045802]\n",
      " [ 0.05063482]\n",
      " [-0.15451526]]\n",
      "t [[ 0.00688474]\n",
      " [-0.25989568]\n",
      " [-0.4874871 ]\n",
      " ...\n",
      " [-0.36156229]\n",
      " [ 0.04668019]\n",
      " [-0.26502889]]\n",
      "t [[ 0.00688474]\n",
      " [-0.25989568]\n",
      " [-0.4874871 ]\n",
      " ...\n",
      " [-0.36156229]\n",
      " [ 0.04668019]\n",
      " [-0.26502889]]\n",
      "Current iteration=2, loss=36902.62115098648\n",
      "t [[-0.00816671]\n",
      " [-0.40707808]\n",
      " [-0.58246009]\n",
      " ...\n",
      " [-0.42054672]\n",
      " [ 0.02419904]\n",
      " [-0.35238816]]\n",
      "t [[-0.00816671]\n",
      " [-0.40707808]\n",
      " [-0.58246009]\n",
      " ...\n",
      " [-0.42054672]\n",
      " [ 0.02419904]\n",
      " [-0.35238816]]\n",
      "t [[-0.02243085]\n",
      " [-0.54098069]\n",
      " [-0.65792499]\n",
      " ...\n",
      " [-0.46581931]\n",
      " [-0.00644142]\n",
      " [-0.42418998]]\n",
      "t [[-0.02243085]\n",
      " [-0.54098069]\n",
      " [-0.65792499]\n",
      " ...\n",
      " [-0.46581931]\n",
      " [-0.00644142]\n",
      " [-0.42418998]]\n",
      "Current iteration=4, loss=35565.01042767132\n",
      "t [[-0.03482152]\n",
      " [-0.66099426]\n",
      " [-0.72386598]\n",
      " ...\n",
      " [-0.50556328]\n",
      " [-0.04089483]\n",
      " [-0.48440337]]\n",
      "t [[-0.03482152]\n",
      " [-0.66099426]\n",
      " [-0.72386598]\n",
      " ...\n",
      " [-0.50556328]\n",
      " [-0.04089483]\n",
      " [-0.48440337]]\n",
      "t [[-0.04530513]\n",
      " [-0.76842686]\n",
      " [-0.78359979]\n",
      " ...\n",
      " [-0.54226141]\n",
      " [-0.07689066]\n",
      " [-0.53554697]]\n",
      "t [[-0.04530513]\n",
      " [-0.76842686]\n",
      " [-0.78359979]\n",
      " ...\n",
      " [-0.54226141]\n",
      " [-0.07689066]\n",
      " [-0.53554697]]\n",
      "Current iteration=6, loss=34792.06239351568\n",
      "t [[-0.05410521]\n",
      " [-0.86488058]\n",
      " [-0.83838411]\n",
      " ...\n",
      " [-0.57667318]\n",
      " [-0.11310326]\n",
      " [-0.57938782]]\n",
      "t [[-0.05410521]\n",
      " [-0.86488058]\n",
      " [-0.83838411]\n",
      " ...\n",
      " [-0.57667318]\n",
      " [-0.11310326]\n",
      " [-0.57938782]]\n",
      "t [[-0.06147568]\n",
      " [-0.9518256 ]\n",
      " [-0.88882302]\n",
      " ...\n",
      " [-0.60904623]\n",
      " [-0.14872219]\n",
      " [-0.61723627]]\n",
      "t [[-0.06147568]\n",
      " [-0.9518256 ]\n",
      " [-0.88882302]\n",
      " ...\n",
      " [-0.60904623]\n",
      " [-0.14872219]\n",
      " [-0.61723627]]\n",
      "Current iteration=8, loss=34298.100446214725\n",
      "t [[-0.06764199]\n",
      " [-1.03051728]\n",
      " [-0.93530854]\n",
      " ...\n",
      " [-0.63949145]\n",
      " [-0.18325076]\n",
      " [-0.65009728]]\n",
      "t [[-0.06764199]\n",
      " [-1.03051728]\n",
      " [-0.93530854]\n",
      " ...\n",
      " [-0.63949145]\n",
      " [-0.18325076]\n",
      " [-0.65009728]]\n",
      "t [[-0.07279197]\n",
      " [-1.10200797]\n",
      " [-0.97815841]\n",
      " ...\n",
      " [-0.66809453]\n",
      " [-0.21639534]\n",
      " [-0.67875955]]\n",
      "loss=33959.245475239935\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01501864]\n",
      " [-0.09501725]\n",
      " [-0.34308876]\n",
      " ...\n",
      " [-0.25610234]\n",
      " [ 0.05444231]\n",
      " [-0.14493597]]\n",
      "t [[ 0.01501864]\n",
      " [-0.09501725]\n",
      " [-0.34308876]\n",
      " ...\n",
      " [-0.25610234]\n",
      " [ 0.05444231]\n",
      " [-0.14493597]]\n",
      "t [[ 0.00202124]\n",
      " [-0.23551923]\n",
      " [-0.49514196]\n",
      " ...\n",
      " [-0.35498443]\n",
      " [ 0.05370029]\n",
      " [-0.24736713]]\n",
      "t [[ 0.00202124]\n",
      " [-0.23551923]\n",
      " [-0.49514196]\n",
      " ...\n",
      " [-0.35498443]\n",
      " [ 0.05370029]\n",
      " [-0.24736713]]\n",
      "Current iteration=2, loss=36945.595216980626\n",
      "t [[-0.0149053 ]\n",
      " [-0.37335335]\n",
      " [-0.59327679]\n",
      " ...\n",
      " [-0.41216749]\n",
      " [ 0.03379706]\n",
      " [-0.32767379]]\n",
      "t [[-0.0149053 ]\n",
      " [-0.37335335]\n",
      " [-0.59327679]\n",
      " ...\n",
      " [-0.41216749]\n",
      " [ 0.03379706]\n",
      " [-0.32767379]]\n",
      "t [[-0.03059947]\n",
      " [-0.49926409]\n",
      " [-0.6712702 ]\n",
      " ...\n",
      " [-0.45593164]\n",
      " [ 0.00518113]\n",
      " [-0.39326094]]\n",
      "t [[-0.03059947]\n",
      " [-0.49926409]\n",
      " [-0.6712702 ]\n",
      " ...\n",
      " [-0.45593164]\n",
      " [ 0.00518113]\n",
      " [-0.39326094]]\n",
      "Current iteration=4, loss=35633.80412842245\n",
      "t [[-0.04402667]\n",
      " [-0.61235303]\n",
      " [-0.73927974]\n",
      " ...\n",
      " [-0.49444387]\n",
      " [-0.02771888]\n",
      " [-0.44796601]]\n",
      "t [[-0.04402667]\n",
      " [-0.61235303]\n",
      " [-0.73927974]\n",
      " ...\n",
      " [-0.49444387]\n",
      " [-0.02771888]\n",
      " [-0.44796601]]\n",
      "t [[-0.05522206]\n",
      " [-0.71372275]\n",
      " [-0.80073884]\n",
      " ...\n",
      " [-0.53016457]\n",
      " [-0.06254821]\n",
      " [-0.49420401]]\n",
      "t [[-0.05522206]\n",
      " [-0.71372275]\n",
      " [-0.80073884]\n",
      " ...\n",
      " [-0.53016457]\n",
      " [-0.06254821]\n",
      " [-0.49420401]]\n",
      "Current iteration=6, loss=34876.361680625356\n",
      "t [[-0.0644744 ]\n",
      " [-0.80482258]\n",
      " [-0.85698305]\n",
      " ...\n",
      " [-0.563821  ]\n",
      " [-0.09790205]\n",
      " [-0.53365737]]\n",
      "t [[-0.0644744 ]\n",
      " [-0.80482258]\n",
      " [-0.85698305]\n",
      " ...\n",
      " [-0.563821  ]\n",
      " [-0.09790205]\n",
      " [-0.53365737]]\n",
      "t [[-0.07209328]\n",
      " [-0.88700564]\n",
      " [-0.90867038]\n",
      " ...\n",
      " [-0.59562568]\n",
      " [-0.13290093]\n",
      " [-0.56756662]]\n",
      "t [[-0.07209328]\n",
      " [-0.88700564]\n",
      " [-0.90867038]\n",
      " ...\n",
      " [-0.59562568]\n",
      " [-0.13290093]\n",
      " [-0.56756662]]\n",
      "Current iteration=8, loss=34391.80989706628\n",
      "t [[-0.07834959]\n",
      " [-0.96143568]\n",
      " [-0.95623178]\n",
      " ...\n",
      " [-0.62565623]\n",
      " [-0.16699102]\n",
      " [-0.59687876]]\n",
      "t [[-0.07834959]\n",
      " [-0.96143568]\n",
      " [-0.95623178]\n",
      " ...\n",
      " [-0.62565623]\n",
      " [-0.16699102]\n",
      " [-0.59687876]]\n",
      "t [[-0.08346754]\n",
      " [-1.02909227]\n",
      " [-1.0000142 ]\n",
      " ...\n",
      " [-0.653969  ]\n",
      " [-0.19983288]\n",
      " [-0.62233401]]\n",
      "loss=34058.81658510983\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01250751]\n",
      " [-0.10063966]\n",
      " [-0.34082496]\n",
      " ...\n",
      " [-0.15005647]\n",
      " [-0.01807242]\n",
      " [-0.30613949]]\n",
      "t [[ 0.01250751]\n",
      " [-0.10063966]\n",
      " [-0.34082496]\n",
      " ...\n",
      " [-0.15005647]\n",
      " [-0.01807242]\n",
      " [-0.30613949]]\n",
      "t [[-0.00243954]\n",
      " [-0.24638269]\n",
      " [-0.48910464]\n",
      " ...\n",
      " [-0.17672023]\n",
      " [ 0.00473674]\n",
      " [-0.53987089]]\n",
      "t [[-0.00243954]\n",
      " [-0.24638269]\n",
      " [-0.48910464]\n",
      " ...\n",
      " [-0.17672023]\n",
      " [ 0.00473674]\n",
      " [-0.53987089]]\n",
      "Current iteration=2, loss=36888.276598570636\n",
      "t [[-0.02086391]\n",
      " [-0.38889741]\n",
      " [-0.58362146]\n",
      " ...\n",
      " [-0.1773791 ]\n",
      " [ 0.02905669]\n",
      " [-0.73201722]]\n",
      "t [[-0.02086391]\n",
      " [-0.38889741]\n",
      " [-0.58362146]\n",
      " ...\n",
      " [-0.1773791 ]\n",
      " [ 0.02905669]\n",
      " [-0.73201722]]\n",
      "t [[-0.03774518]\n",
      " [-0.51897636]\n",
      " [-0.658383  ]\n",
      " ...\n",
      " [-0.17417047]\n",
      " [ 0.04771073]\n",
      " [-0.89541446]]\n",
      "t [[-0.03774518]\n",
      " [-0.51897636]\n",
      " [-0.658383  ]\n",
      " ...\n",
      " [-0.17417047]\n",
      " [ 0.04771073]\n",
      " [-0.89541446]]\n",
      "Current iteration=4, loss=35551.65381186029\n",
      "t [[-0.05214382]\n",
      " [-0.63577537]\n",
      " [-0.72356265]\n",
      " ...\n",
      " [-0.17242559]\n",
      " [ 0.060352  ]\n",
      " [-1.03718189]]\n",
      "t [[-0.05214382]\n",
      " [-0.63577537]\n",
      " [-0.72356265]\n",
      " ...\n",
      " [-0.17242559]\n",
      " [ 0.060352  ]\n",
      " [-1.03718189]]\n",
      "t [[-0.06415621]\n",
      " [-0.74045345]\n",
      " [-0.78255496]\n",
      " ...\n",
      " [-0.17298806]\n",
      " [ 0.0681192 ]\n",
      " [-1.1619251 ]]\n",
      "t [[-0.06415621]\n",
      " [-0.74045345]\n",
      " [-0.78255496]\n",
      " ...\n",
      " [-0.17298806]\n",
      " [ 0.0681192 ]\n",
      " [-1.1619251 ]]\n",
      "Current iteration=6, loss=34778.75038762944\n",
      "t [[-0.07411048]\n",
      " [-0.83451382]\n",
      " [-0.83664535]\n",
      " ...\n",
      " [-0.1755305 ]\n",
      " [ 0.07226784]\n",
      " [-1.27288189]]\n",
      "t [[-0.07411048]\n",
      " [-0.83451382]\n",
      " [-0.83664535]\n",
      " ...\n",
      " [-0.1755305 ]\n",
      " [ 0.07226784]\n",
      " [-1.27288189]]\n",
      "t [[-0.08234182]\n",
      " [-0.91935752]\n",
      " [-0.88644483]\n",
      " ...\n",
      " [-0.17951799]\n",
      " [ 0.07384164]\n",
      " [-1.37244456]]\n",
      "t [[-0.08234182]\n",
      " [-0.91935752]\n",
      " [-0.88644483]\n",
      " ...\n",
      " [-0.17951799]\n",
      " [ 0.07384164]\n",
      " [-1.37244456]]\n",
      "Current iteration=8, loss=34284.442997626786\n",
      "t [[-0.08913829]\n",
      " [-0.99618928]\n",
      " [-0.9323443 ]\n",
      " ...\n",
      " [-0.18447056]\n",
      " [ 0.0736374 ]\n",
      " [-1.4624401 ]]\n",
      "t [[-0.08913829]\n",
      " [-0.99618928]\n",
      " [-0.9323443 ]\n",
      " ...\n",
      " [-0.18447056]\n",
      " [ 0.0736374 ]\n",
      " [-1.4624401 ]]\n",
      "t [[-0.094736  ]\n",
      " [-1.06602303]\n",
      " [-0.97465758]\n",
      " ...\n",
      " [-0.19001462]\n",
      " [ 0.07224346]\n",
      " [-1.54429794]]\n",
      "loss=33945.158607453144\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=53749.404969340525\n",
      "t [[-0.03779159]\n",
      " [-0.0029329 ]\n",
      " [ 0.00123944]\n",
      " ...\n",
      " [-0.0029329 ]\n",
      " [ 0.0026682 ]\n",
      " [-0.00153612]]\n",
      "t [[-0.03779159]\n",
      " [-0.0029329 ]\n",
      " [ 0.00123944]\n",
      " ...\n",
      " [-0.0029329 ]\n",
      " [ 0.0026682 ]\n",
      " [-0.00153612]]\n",
      "t [[-0.07493605]\n",
      " [-0.005848  ]\n",
      " [ 0.00247142]\n",
      " ...\n",
      " [-0.005848  ]\n",
      " [ 0.00530351]\n",
      " [-0.00306339]]\n",
      "t [[-0.07493605]\n",
      " [-0.005848  ]\n",
      " [ 0.00247142]\n",
      " ...\n",
      " [-0.005848  ]\n",
      " [ 0.00530351]\n",
      " [-0.00306339]]\n",
      "Current iteration=2, loss=53596.156021112605\n",
      "t [[-0.11144353]\n",
      " [-0.00874549]\n",
      " [ 0.00369592]\n",
      " ...\n",
      " [-0.00874549]\n",
      " [ 0.00790632]\n",
      " [-0.00458191]]\n",
      "t [[-0.11144353]\n",
      " [-0.00874549]\n",
      " [ 0.00369592]\n",
      " ...\n",
      " [-0.00874549]\n",
      " [ 0.00790632]\n",
      " [-0.00458191]]\n",
      "t [[-0.14732413]\n",
      " [-0.01162557]\n",
      " [ 0.00491288]\n",
      " ...\n",
      " [-0.01162557]\n",
      " [ 0.01047702]\n",
      " [-0.00609179]]\n",
      "t [[-0.14732413]\n",
      " [-0.01162557]\n",
      " [ 0.00491288]\n",
      " ...\n",
      " [-0.01162557]\n",
      " [ 0.01047702]\n",
      " [-0.00609179]]\n",
      "Current iteration=4, loss=53447.89623502396\n",
      "t [[-0.18258789]\n",
      " [-0.01448843]\n",
      " [ 0.00612227]\n",
      " ...\n",
      " [-0.01448843]\n",
      " [ 0.01301601]\n",
      " [-0.00759311]]\n",
      "t [[-0.18258789]\n",
      " [-0.01448843]\n",
      " [ 0.00612227]\n",
      " ...\n",
      " [-0.01448843]\n",
      " [ 0.01301601]\n",
      " [-0.00759311]]\n",
      "t [[-0.21724478]\n",
      " [-0.01733424]\n",
      " [ 0.00732407]\n",
      " ...\n",
      " [-0.01733424]\n",
      " [ 0.01552369]\n",
      " [-0.00908598]]\n",
      "t [[-0.21724478]\n",
      " [-0.01733424]\n",
      " [ 0.00732407]\n",
      " ...\n",
      " [-0.01733424]\n",
      " [ 0.01552369]\n",
      " [-0.00908598]]\n",
      "Current iteration=6, loss=53304.38756613169\n",
      "t [[-0.25130467]\n",
      " [-0.02016319]\n",
      " [ 0.00851824]\n",
      " ...\n",
      " [-0.02016319]\n",
      " [ 0.01800043]\n",
      " [-0.0105705 ]]\n",
      "t [[-0.25130467]\n",
      " [-0.02016319]\n",
      " [ 0.00851824]\n",
      " ...\n",
      " [-0.02016319]\n",
      " [ 0.01800043]\n",
      " [-0.0105705 ]]\n",
      "t [[-0.28477736]\n",
      " [-0.02297547]\n",
      " [ 0.00970475]\n",
      " ...\n",
      " [-0.02297547]\n",
      " [ 0.02044661]\n",
      " [-0.01204676]]\n",
      "t [[-0.28477736]\n",
      " [-0.02297547]\n",
      " [ 0.00970475]\n",
      " ...\n",
      " [-0.02297547]\n",
      " [ 0.02044661]\n",
      " [-0.01204676]]\n",
      "Current iteration=8, loss=53165.404086882125\n",
      "t [[-0.31767256]\n",
      " [-0.02577126]\n",
      " [ 0.01088357]\n",
      " ...\n",
      " [-0.02577126]\n",
      " [ 0.02286263]\n",
      " [-0.01351486]]\n",
      "t [[-0.31767256]\n",
      " [-0.02577126]\n",
      " [ 0.01088357]\n",
      " ...\n",
      " [-0.02577126]\n",
      " [ 0.02286263]\n",
      " [-0.01351486]]\n",
      "t [[-0.34999986]\n",
      " [-0.02855072]\n",
      " [ 0.01205468]\n",
      " ...\n",
      " [-0.02855072]\n",
      " [ 0.02524884]\n",
      " [-0.01497489]]\n",
      "t [[-0.34999986]\n",
      " [-0.02855072]\n",
      " [ 0.01205468]\n",
      " ...\n",
      " [-0.02855072]\n",
      " [ 0.02524884]\n",
      " [-0.01497489]]\n",
      "Current iteration=10, loss=53030.731512952625\n",
      "t [[-0.38176878]\n",
      " [-0.03131405]\n",
      " [ 0.01321806]\n",
      " ...\n",
      " [-0.03131405]\n",
      " [ 0.02760563]\n",
      " [-0.01642694]]\n",
      "t [[-0.38176878]\n",
      " [-0.03131405]\n",
      " [ 0.01321806]\n",
      " ...\n",
      " [-0.03131405]\n",
      " [ 0.02760563]\n",
      " [-0.01642694]]\n",
      "t [[-0.41298869]\n",
      " [-0.03406141]\n",
      " [ 0.01437368]\n",
      " ...\n",
      " [-0.03406141]\n",
      " [ 0.02993337]\n",
      " [-0.01787112]]\n",
      "t [[-0.41298869]\n",
      " [-0.03406141]\n",
      " [ 0.01437368]\n",
      " ...\n",
      " [-0.03406141]\n",
      " [ 0.02993337]\n",
      " [-0.01787112]]\n",
      "Current iteration=12, loss=52900.166720809655\n",
      "t [[-0.4436689 ]\n",
      " [-0.03679297]\n",
      " [ 0.01552152]\n",
      " ...\n",
      " [-0.03679297]\n",
      " [ 0.0322324 ]\n",
      " [-0.0193075 ]]\n",
      "t [[-0.4436689 ]\n",
      " [-0.03679297]\n",
      " [ 0.01552152]\n",
      " ...\n",
      " [-0.03679297]\n",
      " [ 0.0322324 ]\n",
      " [-0.0193075 ]]\n",
      "t [[-0.47381856]\n",
      " [-0.03950892]\n",
      " [ 0.01666156]\n",
      " ...\n",
      " [-0.03950892]\n",
      " [ 0.03450311]\n",
      " [-0.02073618]]\n",
      "t [[-0.47381856]\n",
      " [-0.03950892]\n",
      " [ 0.01666156]\n",
      " ...\n",
      " [-0.03950892]\n",
      " [ 0.03450311]\n",
      " [-0.02073618]]\n",
      "Current iteration=14, loss=52773.517262031055\n",
      "t [[-0.50344672]\n",
      " [-0.04220941]\n",
      " [ 0.01779378]\n",
      " ...\n",
      " [-0.04220941]\n",
      " [ 0.03674583]\n",
      " [-0.02215725]]\n",
      "t [[-0.50344672]\n",
      " [-0.04220941]\n",
      " [ 0.01779378]\n",
      " ...\n",
      " [-0.04220941]\n",
      " [ 0.03674583]\n",
      " [-0.02215725]]\n",
      "t [[-0.53256232]\n",
      " [-0.04489461]\n",
      " [ 0.01891818]\n",
      " ...\n",
      " [-0.04489461]\n",
      " [ 0.03896093]\n",
      " [-0.02357079]]\n",
      "t [[-0.53256232]\n",
      " [-0.04489461]\n",
      " [ 0.01891818]\n",
      " ...\n",
      " [-0.04489461]\n",
      " [ 0.03896093]\n",
      " [-0.02357079]]\n",
      "Current iteration=16, loss=52650.60087874284\n",
      "t [[-0.56117416]\n",
      " [-0.04756469]\n",
      " [ 0.02003473]\n",
      " ...\n",
      " [-0.04756469]\n",
      " [ 0.04114875]\n",
      " [-0.02497689]]\n",
      "t [[-0.56117416]\n",
      " [-0.04756469]\n",
      " [ 0.02003473]\n",
      " ...\n",
      " [-0.04756469]\n",
      " [ 0.04114875]\n",
      " [-0.02497689]]\n",
      "t [[-0.58929093]\n",
      " [-0.05021981]\n",
      " [ 0.02114342]\n",
      " ...\n",
      " [-0.05021981]\n",
      " [ 0.04330963]\n",
      " [-0.02637564]]\n",
      "t [[-0.58929093]\n",
      " [-0.05021981]\n",
      " [ 0.02114342]\n",
      " ...\n",
      " [-0.05021981]\n",
      " [ 0.04330963]\n",
      " [-0.02637564]]\n",
      "Current iteration=18, loss=52531.245023867334\n",
      "t [[-0.61692118]\n",
      " [-0.05286014]\n",
      " [ 0.02224424]\n",
      " ...\n",
      " [-0.05286014]\n",
      " [ 0.04544392]\n",
      " [-0.02776713]]\n",
      "t [[-0.61692118]\n",
      " [-0.05286014]\n",
      " [ 0.02224424]\n",
      " ...\n",
      " [-0.05286014]\n",
      " [ 0.04544392]\n",
      " [-0.02776713]]\n",
      "t [[-0.64407335]\n",
      " [-0.05548582]\n",
      " [ 0.02333718]\n",
      " ...\n",
      " [-0.05548582]\n",
      " [ 0.04755196]\n",
      " [-0.02915142]]\n",
      "t [[-0.64407335]\n",
      " [-0.05548582]\n",
      " [ 0.02333718]\n",
      " ...\n",
      " [-0.05548582]\n",
      " [ 0.04755196]\n",
      " [-0.02915142]]\n",
      "Current iteration=20, loss=52415.286389269786\n",
      "t [[-0.67075574]\n",
      " [-0.05809702]\n",
      " [ 0.02442224]\n",
      " ...\n",
      " [-0.05809702]\n",
      " [ 0.04963406]\n",
      " [-0.03052862]]\n",
      "t [[-0.67075574]\n",
      " [-0.05809702]\n",
      " [ 0.02442224]\n",
      " ...\n",
      " [-0.05809702]\n",
      " [ 0.04963406]\n",
      " [-0.03052862]]\n",
      "t [[-0.69697652]\n",
      " [-0.06069388]\n",
      " [ 0.0254994 ]\n",
      " ...\n",
      " [-0.06069388]\n",
      " [ 0.05169056]\n",
      " [-0.03189879]]\n",
      "t [[-0.69697652]\n",
      " [-0.06069388]\n",
      " [ 0.0254994 ]\n",
      " ...\n",
      " [-0.06069388]\n",
      " [ 0.05169056]\n",
      " [-0.03189879]]\n",
      "Current iteration=22, loss=52302.570444335135\n",
      "t [[-0.72274375]\n",
      " [-0.06327658]\n",
      " [ 0.02656865]\n",
      " ...\n",
      " [-0.06327658]\n",
      " [ 0.05372179]\n",
      " [-0.03326201]]\n",
      "t [[-0.72274375]\n",
      " [-0.06327658]\n",
      " [ 0.02656865]\n",
      " ...\n",
      " [-0.06327658]\n",
      " [ 0.05372179]\n",
      " [-0.03326201]]\n",
      "t [[-0.74806534]\n",
      " [-0.06584525]\n",
      " [ 0.02763   ]\n",
      " ...\n",
      " [-0.06584525]\n",
      " [ 0.05572806]\n",
      " [-0.03461837]]\n",
      "t [[-0.74806534]\n",
      " [-0.06584525]\n",
      " [ 0.02763   ]\n",
      " ...\n",
      " [-0.06584525]\n",
      " [ 0.05572806]\n",
      " [-0.03461837]]\n",
      "Current iteration=24, loss=52192.95098700352\n",
      "t [[-0.77294908]\n",
      " [-0.06840004]\n",
      " [ 0.02868343]\n",
      " ...\n",
      " [-0.06840004]\n",
      " [ 0.05770969]\n",
      " [-0.03596795]]\n",
      "t [[-0.77294908]\n",
      " [-0.06840004]\n",
      " [ 0.02868343]\n",
      " ...\n",
      " [-0.06840004]\n",
      " [ 0.05770969]\n",
      " [-0.03596795]]\n",
      "t [[-0.79740263]\n",
      " [-0.0709411 ]\n",
      " [ 0.02972896]\n",
      " ...\n",
      " [-0.0709411 ]\n",
      " [ 0.05966699]\n",
      " [-0.03731081]]\n",
      "t [[-0.79740263]\n",
      " [-0.0709411 ]\n",
      " [ 0.02972896]\n",
      " ...\n",
      " [-0.0709411 ]\n",
      " [ 0.05966699]\n",
      " [-0.03731081]]\n",
      "Current iteration=26, loss=52086.289708846816\n",
      "t [[-0.82143354]\n",
      " [-0.07346858]\n",
      " [ 0.03076656]\n",
      " ...\n",
      " [-0.07346858]\n",
      " [ 0.06160026]\n",
      " [-0.03864704]]\n",
      "t [[-0.82143354]\n",
      " [-0.07346858]\n",
      " [ 0.03076656]\n",
      " ...\n",
      " [-0.07346858]\n",
      " [ 0.06160026]\n",
      " [-0.03864704]]\n",
      "t [[-0.8450492 ]\n",
      " [-0.07598262]\n",
      " [ 0.03179626]\n",
      " ...\n",
      " [-0.07598262]\n",
      " [ 0.06350981]\n",
      " [-0.03997671]]\n",
      "t [[-0.8450492 ]\n",
      " [-0.07598262]\n",
      " [ 0.03179626]\n",
      " ...\n",
      " [-0.07598262]\n",
      " [ 0.06350981]\n",
      " [-0.03997671]]\n",
      "Current iteration=28, loss=51982.45577537387\n",
      "t [[-0.8682569 ]\n",
      " [-0.07848336]\n",
      " [ 0.03281803]\n",
      " ...\n",
      " [-0.07848336]\n",
      " [ 0.06539594]\n",
      " [-0.04129989]]\n",
      "t [[-0.8682569 ]\n",
      " [-0.07848336]\n",
      " [ 0.03281803]\n",
      " ...\n",
      " [-0.07848336]\n",
      " [ 0.06539594]\n",
      " [-0.04129989]]\n",
      "t [[-0.89106379]\n",
      " [-0.08097094]\n",
      " [ 0.03383189]\n",
      " ...\n",
      " [-0.08097094]\n",
      " [ 0.06725893]\n",
      " [-0.04261666]]\n",
      "t [[-0.89106379]\n",
      " [-0.08097094]\n",
      " [ 0.03383189]\n",
      " ...\n",
      " [-0.08097094]\n",
      " [ 0.06725893]\n",
      " [-0.04261666]]\n",
      "Current iteration=30, loss=51881.325422412716\n",
      "t [[-0.91347692]\n",
      " [-0.08344549]\n",
      " [ 0.03483784]\n",
      " ...\n",
      " [-0.08344549]\n",
      " [ 0.0690991 ]\n",
      " [-0.04392707]]\n",
      "t [[-0.91347692]\n",
      " [-0.08344549]\n",
      " [ 0.03483784]\n",
      " ...\n",
      " [-0.08344549]\n",
      " [ 0.0690991 ]\n",
      " [-0.04392707]]\n",
      "t [[-0.93550319]\n",
      " [-0.08590716]\n",
      " [ 0.03583589]\n",
      " ...\n",
      " [-0.08590716]\n",
      " [ 0.07091671]\n",
      " [-0.04523122]]\n",
      "t [[-0.93550319]\n",
      " [-0.08590716]\n",
      " [ 0.03583589]\n",
      " ...\n",
      " [-0.08590716]\n",
      " [ 0.07091671]\n",
      " [-0.04523122]]\n",
      "Current iteration=32, loss=51782.78156911905\n",
      "t [[-0.95714939]\n",
      " [-0.08835607]\n",
      " [ 0.03682603]\n",
      " ...\n",
      " [-0.08835607]\n",
      " [ 0.07271205]\n",
      " [-0.04652915]]\n",
      "t [[-0.95714939]\n",
      " [-0.08835607]\n",
      " [ 0.03682603]\n",
      " ...\n",
      " [-0.08835607]\n",
      " [ 0.07271205]\n",
      " [-0.04652915]]\n",
      "t [[-0.97842219]\n",
      " [-0.09079236]\n",
      " [ 0.03780827]\n",
      " ...\n",
      " [-0.09079236]\n",
      " [ 0.0744854 ]\n",
      " [-0.04782095]]\n",
      "t [[-0.97842219]\n",
      " [-0.09079236]\n",
      " [ 0.03780827]\n",
      " ...\n",
      " [-0.09079236]\n",
      " [ 0.0744854 ]\n",
      " [-0.04782095]]\n",
      "Current iteration=34, loss=51686.713447916496\n",
      "t [[-0.99932813]\n",
      " [-0.09321616]\n",
      " [ 0.03878262]\n",
      " ...\n",
      " [-0.09321616]\n",
      " [ 0.07623704]\n",
      " [-0.04910668]]\n",
      "t [[-0.99932813]\n",
      " [-0.09321616]\n",
      " [ 0.03878262]\n",
      " ...\n",
      " [-0.09321616]\n",
      " [ 0.07623704]\n",
      " [-0.04910668]]\n",
      "t [[-1.01987367]\n",
      " [-0.0956276 ]\n",
      " [ 0.03974909]\n",
      " ...\n",
      " [-0.0956276 ]\n",
      " [ 0.07796724]\n",
      " [-0.0503864 ]]\n",
      "t [[-1.01987367]\n",
      " [-0.0956276 ]\n",
      " [ 0.03974909]\n",
      " ...\n",
      " [-0.0956276 ]\n",
      " [ 0.07796724]\n",
      " [-0.0503864 ]]\n",
      "Current iteration=36, loss=51593.01625145912\n",
      "t [[-1.0400651 ]\n",
      " [-0.0980268 ]\n",
      " [ 0.04070768]\n",
      " ...\n",
      " [-0.0980268 ]\n",
      " [ 0.07967626]\n",
      " [-0.05166018]]\n",
      "t [[-1.0400651 ]\n",
      " [-0.0980268 ]\n",
      " [ 0.04070768]\n",
      " ...\n",
      " [-0.0980268 ]\n",
      " [ 0.07967626]\n",
      " [-0.05166018]]\n",
      "t [[-1.05990864]\n",
      " [-0.10041389]\n",
      " [ 0.04165841]\n",
      " ...\n",
      " [-0.10041389]\n",
      " [ 0.08136437]\n",
      " [-0.05292808]]\n",
      "t [[-1.05990864]\n",
      " [-0.10041389]\n",
      " [ 0.04165841]\n",
      " ...\n",
      " [-0.10041389]\n",
      " [ 0.08136437]\n",
      " [-0.05292808]]\n",
      "Current iteration=38, loss=51501.590796536795\n",
      "t [[-1.07941039]\n",
      " [-0.10278899]\n",
      " [ 0.04260127]\n",
      " ...\n",
      " [-0.10278899]\n",
      " [ 0.08303183]\n",
      " [-0.05419016]]\n",
      "t [[-1.07941039]\n",
      " [-0.10278899]\n",
      " [ 0.04260127]\n",
      " ...\n",
      " [-0.10278899]\n",
      " [ 0.08303183]\n",
      " [-0.05419016]]\n",
      "t [[-1.09857631]\n",
      " [-0.10515222]\n",
      " [ 0.04353629]\n",
      " ...\n",
      " [-0.10515222]\n",
      " [ 0.08467889]\n",
      " [-0.0554465 ]]\n",
      "t [[-1.09857631]\n",
      " [-0.10515222]\n",
      " [ 0.04353629]\n",
      " ...\n",
      " [-0.10515222]\n",
      " [ 0.08467889]\n",
      " [-0.0554465 ]]\n",
      "Current iteration=40, loss=51412.343204699726\n",
      "t [[-1.11741229]\n",
      " [-0.10750371]\n",
      " [ 0.04446347]\n",
      " ...\n",
      " [-0.10750371]\n",
      " [ 0.08630581]\n",
      " [-0.05669714]]\n",
      "t [[-1.11741229]\n",
      " [-0.10750371]\n",
      " [ 0.04446347]\n",
      " ...\n",
      " [-0.10750371]\n",
      " [ 0.08630581]\n",
      " [-0.05669714]]\n",
      "t [[-1.13592408]\n",
      " [-0.10984357]\n",
      " [ 0.04538283]\n",
      " ...\n",
      " [-0.10984357]\n",
      " [ 0.08791283]\n",
      " [-0.05794215]]\n",
      "t [[-1.13592408]\n",
      " [-0.10984357]\n",
      " [ 0.04538283]\n",
      " ...\n",
      " [-0.10984357]\n",
      " [ 0.08791283]\n",
      " [-0.05794215]]\n",
      "Current iteration=42, loss=51325.184599265616\n",
      "t [[-1.15411736]\n",
      " [-0.11217191]\n",
      " [ 0.04629438]\n",
      " ...\n",
      " [-0.11217191]\n",
      " [ 0.08950021]\n",
      " [-0.05918158]]\n",
      "t [[-1.15411736]\n",
      " [-0.11217191]\n",
      " [ 0.04629438]\n",
      " ...\n",
      " [-0.11217191]\n",
      " [ 0.08950021]\n",
      " [-0.05918158]]\n",
      "t [[-1.17199768]\n",
      " [-0.11448885]\n",
      " [ 0.04719812]\n",
      " ...\n",
      " [-0.11448885]\n",
      " [ 0.09106818]\n",
      " [-0.0604155 ]]\n",
      "t [[-1.17199768]\n",
      " [-0.11448885]\n",
      " [ 0.04719812]\n",
      " ...\n",
      " [-0.11448885]\n",
      " [ 0.09106818]\n",
      " [-0.0604155 ]]\n",
      "Current iteration=44, loss=51240.03081828476\n",
      "t [[-1.18957049]\n",
      " [-0.11679451]\n",
      " [ 0.04809409]\n",
      " ...\n",
      " [-0.11679451]\n",
      " [ 0.09261698]\n",
      " [-0.06164397]]\n",
      "t [[-1.18957049]\n",
      " [-0.11679451]\n",
      " [ 0.04809409]\n",
      " ...\n",
      " [-0.11679451]\n",
      " [ 0.09261698]\n",
      " [-0.06164397]]\n",
      "t [[-1.20684114]\n",
      " [-0.119089  ]\n",
      " [ 0.04898228]\n",
      " ...\n",
      " [-0.119089  ]\n",
      " [ 0.09414685]\n",
      " [-0.06286703]]\n",
      "t [[-1.20684114]\n",
      " [-0.119089  ]\n",
      " [ 0.04898228]\n",
      " ...\n",
      " [-0.119089  ]\n",
      " [ 0.09414685]\n",
      " [-0.06286703]]\n",
      "Current iteration=46, loss=51156.80214296672\n",
      "t [[-1.2238149 ]\n",
      " [-0.12137243]\n",
      " [ 0.04986271]\n",
      " ...\n",
      " [-0.12137243]\n",
      " [ 0.09565802]\n",
      " [-0.06408474]]\n",
      "t [[-1.2238149 ]\n",
      " [-0.12137243]\n",
      " [ 0.04986271]\n",
      " ...\n",
      " [-0.12137243]\n",
      " [ 0.09565802]\n",
      " [-0.06408474]]\n",
      "t [[-1.24049693]\n",
      " [-0.1236449 ]\n",
      " [ 0.05073541]\n",
      " ...\n",
      " [-0.1236449 ]\n",
      " [ 0.09715071]\n",
      " [-0.06529717]]\n",
      "t [[-1.24049693]\n",
      " [-0.1236449 ]\n",
      " [ 0.05073541]\n",
      " ...\n",
      " [-0.1236449 ]\n",
      " [ 0.09715071]\n",
      " [-0.06529717]]\n",
      "Current iteration=48, loss=51075.42304102473\n",
      "t [[-1.25689228]\n",
      " [-0.12590652]\n",
      " [ 0.05160038]\n",
      " ...\n",
      " [-0.12590652]\n",
      " [ 0.09862516]\n",
      " [-0.06650435]]\n",
      "t [[-1.25689228]\n",
      " [-0.12590652]\n",
      " [ 0.05160038]\n",
      " ...\n",
      " [-0.12590652]\n",
      " [ 0.09862516]\n",
      " [-0.06650435]]\n",
      "t [[-1.27300594]\n",
      " [-0.1281574 ]\n",
      " [ 0.05245765]\n",
      " ...\n",
      " [-0.1281574 ]\n",
      " [ 0.10008158]\n",
      " [-0.06770635]]\n",
      "loss=50995.821924356045\n",
      "Cross validation finished: optimal gamma 0.01\n",
      "logistic regression loss 51035.40456105957\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(set2_x.shape[1])\n",
    "gamma_opt = cross_validation(set2_y, set2_x, k_fold, gammas, fonction=4)\n",
    "w_lr2, loss_lr = logistic_regression(set2_y,set2_x, initial_w, max_iters, gamma_opt)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"logistic regression loss {loss}\".format(loss=loss_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(set3_x.shape[1])\n",
    "gamma_opt = cross_validation(set3_y, set3_x, k_fold, gammas, fonction=4)\n",
    "w_lr3, loss_lr = logistic_regression(set3_y, set3_x, initial_w, max_iters, gamma_opt)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"logistic regression loss {loss}\".format(loss=loss_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41/663882173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mk_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtX_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "k_fold = 4\n",
    "max_iters = 5\n",
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "lambdas = np.logspace(-4, 0, 10)\n",
    "tX_log = log_distribution(tX, to_log)\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_log, y, ids)\n",
    "\n",
    "set1_x = outliers(set1_x, -999)\n",
    "set1_x = filtering_with_mean_bis(set1_x, set1_y, lr=1)\n",
    "#set1_x = filtering_with_mean(set1_x)\n",
    "set1_x = std(set1_x)\n",
    "#set1_x = scaling(set1_x)\n",
    "\n",
    "set2_x = outliers(set2_x, -999)\n",
    "set2_x = filtering_with_mean_bis(set2_x, set2_y, lr=1)\n",
    "#set2_x = filtering_with_mean(set2_x)\n",
    "set2_x = std(set2_x)\n",
    "\n",
    "set3_x = outliers(set3_x, -999)\n",
    "set3_x = filtering_with_mean_bis(set3_x, set3_y, lr=1)\n",
    "#set3_x = filtering_with_mean(set3_x)\n",
    "set3_x = std(set3_x)\n",
    "\n",
    "print('')\n",
    "print(\"Preprocessing for regularized logistic regression done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "loss=51940.29082807904\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "loss=51940.290828079065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "loss=51940.29082807906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.26777711568\n",
      "Current iteration=4, loss=47334.206997343295\n",
      "Current iteration=6, loss=45092.787585723265\n",
      "Current iteration=8, loss=42890.69387621431\n",
      "loss=40726.62259090609\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.545104345736\n",
      "Current iteration=4, loss=47332.85115637675\n",
      "Current iteration=6, loss=45090.879869552344\n",
      "Current iteration=8, loss=42888.30747675823\n",
      "loss=40723.82280916766\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.08828615133\n",
      "Current iteration=4, loss=47369.71877251754\n",
      "Current iteration=6, loss=45145.854992835084\n",
      "Current iteration=8, loss=42961.175462938336\n",
      "loss=40814.371713106\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.56431391696\n",
      "Current iteration=4, loss=47358.65182206276\n",
      "Current iteration=6, loss=45129.23380650097\n",
      "Current iteration=8, loss=42938.99602266333\n",
      "loss=40786.63669197123\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.26777728165\n",
      "Current iteration=4, loss=47334.20699810852\n",
      "Current iteration=6, loss=45092.78758750556\n",
      "Current iteration=8, loss=42890.693879416714\n",
      "loss=40726.622593245076\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.54510451176\n",
      "Current iteration=4, loss=47332.8511571422\n",
      "Current iteration=6, loss=45090.8798713351\n",
      "Current iteration=8, loss=42888.30747996143\n",
      "loss=40723.82281150711\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.08828631603\n",
      "Current iteration=4, loss=47369.71877327688\n",
      "Current iteration=6, loss=45145.854994603505\n",
      "Current iteration=8, loss=42961.175466115674\n",
      "loss=40814.37171542635\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.56431408207\n",
      "Current iteration=4, loss=47358.65182282393\n",
      "Current iteration=6, loss=45129.23380827374\n",
      "Current iteration=8, loss=42938.9960258486\n",
      "loss=40786.636694297566\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.26777774352\n",
      "Current iteration=4, loss=47334.20700023783\n",
      "Current iteration=6, loss=45092.7875924648\n",
      "Current iteration=8, loss=42890.693888327616\n",
      "loss=40726.62259975337\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.54510497375\n",
      "Current iteration=4, loss=47332.8511592721\n",
      "Current iteration=6, loss=45090.879876295716\n",
      "Current iteration=8, loss=42888.30748887454\n",
      "loss=40723.822818016764\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.08828677436\n",
      "Current iteration=4, loss=47369.71877538974\n",
      "Current iteration=6, loss=45145.85499952421\n",
      "Current iteration=8, loss=42961.17547495685\n",
      "loss=40814.37172188299\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.56431454148\n",
      "Current iteration=4, loss=47358.65182494193\n",
      "Current iteration=6, loss=45129.23381320656\n",
      "Current iteration=8, loss=42938.996034711825\n",
      "loss=40786.63670077074\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.26777902863\n",
      "Current iteration=4, loss=47334.20700616273\n",
      "Current iteration=6, loss=45092.787606264304\n",
      "Current iteration=8, loss=42890.69391312273\n",
      "loss=40726.62261786308\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.545106259284\n",
      "Current iteration=4, loss=47332.851165198736\n",
      "Current iteration=6, loss=45090.87989009891\n",
      "Current iteration=8, loss=42888.30751367584\n",
      "loss=40723.8228361302\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.08828804962\n",
      "Current iteration=4, loss=47369.71878126888\n",
      "Current iteration=6, loss=45145.85501321641\n",
      "Current iteration=8, loss=42961.17549955789\n",
      "loss=40814.371739848946\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.56431581979\n",
      "Current iteration=4, loss=47358.65183083534\n",
      "Current iteration=6, loss=45129.23382693247\n",
      "Current iteration=8, loss=42938.996059374214\n",
      "loss=40786.63671878273\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.26778260455\n",
      "Current iteration=4, loss=47334.2070226491\n",
      "Current iteration=6, loss=45092.78764466219\n",
      "Current iteration=8, loss=42890.69398211658\n",
      "loss=40726.62266825447\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.54510983628\n",
      "Current iteration=4, loss=47332.85118168986\n",
      "Current iteration=6, loss=45090.87992850711\n",
      "Current iteration=8, loss=42888.30758268688\n",
      "loss=40723.822886532\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.08829159811\n",
      "Current iteration=4, loss=47369.718797627946\n",
      "Current iteration=6, loss=45145.85505131578\n",
      "Current iteration=8, loss=42961.17556801182\n",
      "loss=40814.37178984025\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.5643193768\n",
      "Current iteration=4, loss=47358.65184723415\n",
      "Current iteration=6, loss=45129.23386512563\n",
      "Current iteration=8, loss=42938.99612799881\n",
      "loss=40786.636768902186\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.267792554725\n",
      "Current iteration=4, loss=47334.2070685234\n",
      "Current iteration=6, loss=45092.78775150661\n",
      "Current iteration=8, loss=42890.69417409606\n",
      "loss=40726.6228084715\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.54511978961\n",
      "Current iteration=4, loss=47332.85122757746\n",
      "Current iteration=6, loss=45090.88003538026\n",
      "Current iteration=8, loss=42888.307774714165\n",
      "loss=40723.82302677797\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.08830147205\n",
      "Current iteration=4, loss=47369.718843148075\n",
      "Current iteration=6, loss=45145.85515732951\n",
      "Current iteration=8, loss=42961.17575848885\n",
      "loss=40814.371928944056\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.564329274355\n",
      "Current iteration=4, loss=47358.6518928648\n",
      "Current iteration=6, loss=45129.233971400325\n",
      "Current iteration=8, loss=42938.9963189508\n",
      "loss=40786.63690836251\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.26782024173\n",
      "Current iteration=4, loss=47334.207196171345\n",
      "Current iteration=6, loss=45092.78804880749\n",
      "Current iteration=8, loss=42890.69470829036\n",
      "loss=40726.62319863371\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.54514748524\n",
      "Current iteration=4, loss=47332.851355262435\n",
      "Current iteration=6, loss=45090.88033276107\n",
      "Current iteration=8, loss=42888.30830904154\n",
      "loss=40723.82341702067\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.088328946855\n",
      "Current iteration=4, loss=47369.71896981049\n",
      "Current iteration=6, loss=45145.85545231901\n",
      "Current iteration=8, loss=42961.176288502524\n",
      "loss=40814.372316008696\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.56435681488\n",
      "Current iteration=4, loss=47358.65201983475\n",
      "Current iteration=6, loss=45129.234267116\n",
      "Current iteration=8, loss=42938.996850286\n",
      "loss=40786.637296419154\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.267897282436\n",
      "Current iteration=4, loss=47334.20755135933\n",
      "Current iteration=6, loss=45092.78887606477\n",
      "Current iteration=8, loss=42890.69619471755\n",
      "loss=40726.62428428316\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.54522455002\n",
      "Current iteration=4, loss=47332.851710553434\n",
      "Current iteration=6, loss=45090.88116024081\n",
      "Current iteration=8, loss=42888.30979583894\n",
      "loss=40723.82450289413\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.0884053971\n",
      "Current iteration=4, loss=47369.71932225618\n",
      "Current iteration=6, loss=45145.856273144724\n",
      "Current iteration=8, loss=42961.17776329687\n",
      "loss=40814.37339303891\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.564433448075\n",
      "Current iteration=4, loss=47358.652373136254\n",
      "Current iteration=6, loss=45129.23508996226\n",
      "Current iteration=8, loss=42938.99832875762\n",
      "loss=40786.63837620976\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.26811165279\n",
      "Current iteration=4, loss=47334.20853969086\n",
      "Current iteration=6, loss=45092.79117795671\n",
      "Current iteration=8, loss=42890.70033078782\n",
      "loss=40726.627305166585\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.54543898734\n",
      "Current iteration=4, loss=47332.852699171555\n",
      "Current iteration=6, loss=45090.883462751684\n",
      "Current iteration=8, loss=42888.31393293944\n",
      "loss=40723.82752440091\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.088618124384\n",
      "Current iteration=4, loss=47369.72030295706\n",
      "Current iteration=6, loss=45145.858557140404\n",
      "Current iteration=8, loss=42961.181866998115\n",
      "loss=40814.37638993887\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.56464668447\n",
      "Current iteration=4, loss=47358.65335621849\n",
      "Current iteration=6, loss=45129.237379580336\n",
      "Current iteration=8, loss=42939.00244269107\n",
      "loss=40786.64138079058\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49616.268708150936\n",
      "Current iteration=4, loss=47334.211289780724\n",
      "Current iteration=6, loss=45092.7975831028\n",
      "Current iteration=8, loss=42890.71183963663\n",
      "loss=40726.63571094912\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=49615.54603567181\n",
      "Current iteration=4, loss=47332.85545005896\n",
      "Current iteration=6, loss=45090.88986962002\n",
      "Current iteration=8, loss=42888.325444654896\n",
      "loss=40723.835931917936\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=49634.08921005068\n",
      "Current iteration=4, loss=47369.723031814254\n",
      "Current iteration=6, loss=45145.864912489094\n",
      "Current iteration=8, loss=42961.19328577822\n",
      "loss=40814.38472898596\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=49628.56524002731\n",
      "Current iteration=4, loss=47358.656091701894\n",
      "Current iteration=6, loss=45129.243750573616\n",
      "Current iteration=8, loss=42939.013889942886\n",
      "loss=40786.64974121022\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.95109948395\n",
      "Current iteration=4, loss=42871.40173109817\n",
      "Current iteration=6, loss=38572.05844901902\n",
      "Current iteration=8, loss=34415.6235413749\n",
      "loss=30392.24310437346\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.57432553843\n",
      "Current iteration=4, loss=42868.98088030679\n",
      "Current iteration=6, loss=38568.86133254292\n",
      "Current iteration=8, loss=34411.85752157551\n",
      "loss=30388.061450575973\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.49640169037\n",
      "Current iteration=4, loss=42941.95605724868\n",
      "Current iteration=6, loss=38677.03992424746\n",
      "Current iteration=8, loss=34554.41523102594\n",
      "loss=30564.204009131106\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.432794728455\n",
      "Current iteration=4, loss=42919.77664531125\n",
      "Current iteration=6, loss=38643.75002483816\n",
      "Current iteration=8, loss=34510.068383313686\n",
      "loss=30508.892520043963\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.95110014384\n",
      "Current iteration=4, loss=42871.4017341045\n",
      "Current iteration=6, loss=38572.058455941915\n",
      "Current iteration=8, loss=34415.623553680474\n",
      "loss=30392.243113150173\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.57432619848\n",
      "Current iteration=4, loss=42868.980883313925\n",
      "Current iteration=6, loss=38568.86133946734\n",
      "Current iteration=8, loss=34411.857533883405\n",
      "loss=30388.061459353805\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.49640234516\n",
      "Current iteration=4, loss=42941.95606023157\n",
      "Current iteration=6, loss=38677.03993111564\n",
      "Current iteration=8, loss=34554.41524323318\n",
      "loss=30564.204017836062\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.432795384826\n",
      "Current iteration=4, loss=42919.776648301544\n",
      "Current iteration=6, loss=38643.750031723794\n",
      "Current iteration=8, loss=34510.06839555256\n",
      "loss=30508.892528772427\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.95110197996\n",
      "Current iteration=4, loss=42871.4017424698\n",
      "Current iteration=6, loss=38572.05847520524\n",
      "Current iteration=8, loss=34415.623587921364\n",
      "loss=30392.24313757191\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.57432803518\n",
      "Current iteration=4, loss=42868.9808916814\n",
      "Current iteration=6, loss=38568.861358734925\n",
      "Current iteration=8, loss=34411.85756813081\n",
      "loss=30388.061483778634\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.49640416716\n",
      "Current iteration=4, loss=42941.956068531625\n",
      "Current iteration=6, loss=38677.039950226805\n",
      "Current iteration=8, loss=34554.415277200555\n",
      "loss=30564.20404205808\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.43279721124\n",
      "Current iteration=4, loss=42919.776656622125\n",
      "Current iteration=6, loss=38643.750050883464\n",
      "Current iteration=8, loss=34510.06842960799\n",
      "loss=30508.89255305998\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.95110708906\n",
      "Current iteration=4, loss=42871.40176574676\n",
      "Current iteration=6, loss=38572.05852880663\n",
      "Current iteration=8, loss=34415.62368319875\n",
      "loss=30392.24320552685\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.57433314586\n",
      "Current iteration=4, loss=42868.98091496436\n",
      "Current iteration=6, loss=38568.86141234818\n",
      "Current iteration=8, loss=34411.85766342625\n",
      "loss=30388.06155174222\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.49640923699\n",
      "Current iteration=4, loss=42941.95609162696\n",
      "Current iteration=6, loss=38677.04000340472\n",
      "Current iteration=8, loss=34554.41537171672\n",
      "loss=30564.2041094573\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.43280229324\n",
      "Current iteration=4, loss=42919.77667977472\n",
      "Current iteration=6, loss=38643.750104196384\n",
      "Current iteration=8, loss=34510.06852436921\n",
      "loss=30508.89262064154\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.95112130552\n",
      "Current iteration=4, loss=42871.40183051628\n",
      "Current iteration=6, loss=38572.058677955654\n",
      "Current iteration=8, loss=34415.62394831365\n",
      "loss=30392.243394615456\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.57434736656\n",
      "Current iteration=4, loss=42868.98097975061\n",
      "Current iteration=6, loss=38568.86156153021\n",
      "Current iteration=8, loss=34411.85792859147\n",
      "loss=30388.061740854922\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.49642334405\n",
      "Current iteration=4, loss=42941.95615589115\n",
      "Current iteration=6, loss=38677.040151375426\n",
      "Current iteration=8, loss=34554.415634713616\n",
      "loss=30564.204296999633\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.43281643434\n",
      "Current iteration=4, loss=42919.776744198185\n",
      "Current iteration=6, loss=38643.75025254279\n",
      "Current iteration=8, loss=34510.06878804795\n",
      "loss=30508.892808691213\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.951160863624\n",
      "Current iteration=4, loss=42871.40201074126\n",
      "Current iteration=6, loss=38572.05909297166\n",
      "Current iteration=8, loss=34415.62468601164\n",
      "loss=30392.243920765744\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.57438693656\n",
      "Current iteration=4, loss=42868.981160022166\n",
      "Current iteration=6, loss=38568.86197663807\n",
      "Current iteration=8, loss=34411.858666429434\n",
      "loss=30388.062267072186\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.496462597825\n",
      "Current iteration=4, loss=42941.95633471011\n",
      "Current iteration=6, loss=38677.0405631127\n",
      "Current iteration=8, loss=34554.41636651808\n",
      "loss=30564.204818847335\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.43285578277\n",
      "Current iteration=4, loss=42919.776923460326\n",
      "Current iteration=6, loss=38643.75066532539\n",
      "Current iteration=8, loss=34510.0695217497\n",
      "loss=30508.89333195064\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.95127093642\n",
      "Current iteration=4, loss=42871.40251222802\n",
      "Current iteration=6, loss=38572.060247778296\n",
      "Current iteration=8, loss=34415.6267386999\n",
      "loss=30392.245384810143\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.574497042464\n",
      "Current iteration=4, loss=42868.98166163848\n",
      "Current iteration=6, loss=38568.863131700266\n",
      "Current iteration=8, loss=34411.86071950718\n",
      "loss=30388.063731302915\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.496571823795\n",
      "Current iteration=4, loss=42941.95683228438\n",
      "Current iteration=6, loss=38677.04170879609\n",
      "Current iteration=8, loss=34554.41840280721\n",
      "loss=30564.206270919443\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.43296527208\n",
      "Current iteration=4, loss=42919.77742226778\n",
      "Current iteration=6, loss=38643.751813917406\n",
      "Current iteration=8, loss=34510.071563318095\n",
      "loss=30508.89478795092\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.95157722044\n",
      "Current iteration=4, loss=42871.40390764449\n",
      "Current iteration=6, loss=38572.063461095706\n",
      "Current iteration=8, loss=34415.632450425306\n",
      "loss=30392.24945859999\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.574803418625\n",
      "Current iteration=4, loss=42868.98305741551\n",
      "Current iteration=6, loss=38568.86634572879\n",
      "Current iteration=8, loss=34411.86643231636\n",
      "loss=30388.067805611303\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.49687575147\n",
      "Current iteration=4, loss=42941.95821681424\n",
      "Current iteration=6, loss=38677.04489672744\n",
      "Current iteration=8, loss=34554.42406890116\n",
      "loss=30564.2103113958\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.43326993258\n",
      "Current iteration=4, loss=42919.778810229036\n",
      "Current iteration=6, loss=38643.75500994233\n",
      "Current iteration=8, loss=34510.07724410196\n",
      "loss=30508.898839357636\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.95242947389\n",
      "Current iteration=4, loss=42871.40779047245\n",
      "Current iteration=6, loss=38572.07240233727\n",
      "Current iteration=8, loss=34415.64834362823\n",
      "loss=30392.260794157577\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.57565592848\n",
      "Current iteration=4, loss=42868.9869412467\n",
      "Current iteration=6, loss=38568.87528894905\n",
      "Current iteration=8, loss=34411.88232853495\n",
      "loss=30388.079142611758\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.49772144826\n",
      "Current iteration=4, loss=42941.96206934949\n",
      "Current iteration=6, loss=38677.05376733089\n",
      "Current iteration=8, loss=34554.43983513188\n",
      "loss=30564.22155425656\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.43411766844\n",
      "Current iteration=4, loss=42919.78267231236\n",
      "Current iteration=6, loss=38643.763903066545\n",
      "Current iteration=8, loss=34510.09305120815\n",
      "loss=30508.910112632773\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47323.95480091896\n",
      "Current iteration=4, loss=42871.4185946618\n",
      "Current iteration=6, loss=38572.09728183454\n",
      "Current iteration=8, loss=34415.69256731379\n",
      "loss=30392.292335983482\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=47322.57802808693\n",
      "Current iteration=4, loss=42868.997748227615\n",
      "Current iteration=6, loss=38568.9001739522\n",
      "Current iteration=8, loss=34411.92656061181\n",
      "loss=30388.110688452536\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=47359.50007464903\n",
      "Current iteration=4, loss=42941.972789247724\n",
      "Current iteration=6, loss=38677.07845027375\n",
      "Current iteration=8, loss=34554.483705510545\n",
      "loss=30564.252838148386\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=47348.4364765431\n",
      "Current iteration=4, loss=42919.79341877864\n",
      "Current iteration=6, loss=38643.788648674636\n",
      "Current iteration=8, loss=34510.137035325\n",
      "loss=30508.94148115437\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.802089395664\n",
      "Current iteration=4, loss=38544.57589284858\n",
      "Current iteration=6, loss=32350.226003370823\n",
      "Current iteration=8, loss=26446.57251760454\n",
      "loss=20803.36256100959\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.836372613616\n",
      "Current iteration=4, loss=38541.335700698\n",
      "Current iteration=6, loss=32346.19126958524\n",
      "Current iteration=8, loss=26442.04309593432\n",
      "loss=20798.49605521041\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.97387784016\n",
      "Current iteration=4, loss=38649.67251292341\n",
      "Current iteration=6, loss=32505.867953772802\n",
      "Current iteration=8, loss=26651.303386568776\n",
      "loss=21055.70117641896\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.35825713941\n",
      "Current iteration=4, loss=38616.37549906568\n",
      "Current iteration=6, loss=32456.000170216907\n",
      "Current iteration=8, loss=26585.103214098493\n",
      "loss=20973.490866110944\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.80209087133\n",
      "Current iteration=4, loss=38544.57589949435\n",
      "Current iteration=6, loss=32350.226018512898\n",
      "Current iteration=8, loss=26446.57254426235\n",
      "loss=20803.362579637796\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.83637408973\n",
      "Current iteration=4, loss=38541.335707345286\n",
      "Current iteration=6, loss=32346.191284730085\n",
      "Current iteration=8, loss=26442.043122596027\n",
      "loss=20798.496073840215\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.97387930444\n",
      "Current iteration=4, loss=38649.672519516804\n",
      "Current iteration=6, loss=32505.86796879357\n",
      "Current iteration=8, loss=26651.303413009984\n",
      "loss=21055.701194891517\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.35825860727\n",
      "Current iteration=4, loss=38616.37550567572\n",
      "Current iteration=6, loss=32456.00018527682\n",
      "Current iteration=8, loss=26585.103240610373\n",
      "loss=20973.49088463533\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.802094977465\n",
      "Current iteration=4, loss=38544.57591798656\n",
      "Current iteration=6, loss=32350.226060646648\n",
      "Current iteration=8, loss=26446.57261843928\n",
      "loss=20803.3626314719\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.83637819704\n",
      "Current iteration=4, loss=38541.33572584175\n",
      "Current iteration=6, loss=32346.191326871496\n",
      "Current iteration=8, loss=26442.04319678384\n",
      "loss=20798.496125678757\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.973883378836\n",
      "Current iteration=4, loss=38649.67253786334\n",
      "Current iteration=6, loss=32505.86801058976\n",
      "Current iteration=8, loss=26651.303486584235\n",
      "loss=21055.701246292527\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.35826269157\n",
      "Current iteration=4, loss=38616.3755240686\n",
      "Current iteration=6, loss=32456.00022718193\n",
      "Current iteration=8, loss=26585.10331438128\n",
      "loss=20973.49093618054\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.80210640302\n",
      "Current iteration=4, loss=38544.57596944225\n",
      "Current iteration=6, loss=32350.226177886318\n",
      "Current iteration=8, loss=26446.572824840987\n",
      "loss=20803.362775703346\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.8363896259\n",
      "Current iteration=4, loss=38541.335777309236\n",
      "Current iteration=6, loss=32346.191444132535\n",
      "Current iteration=8, loss=26442.043403215877\n",
      "loss=20798.49626992261\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.973894716175\n",
      "Current iteration=4, loss=38649.67258891365\n",
      "Current iteration=6, loss=32505.868126890135\n",
      "Current iteration=8, loss=26651.30369130895\n",
      "loss=21055.701389318863\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.35827405643\n",
      "Current iteration=4, loss=38616.37557524782\n",
      "Current iteration=6, loss=32456.000343785385\n",
      "Current iteration=8, loss=26585.103519653192\n",
      "loss=20973.491079608197\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.80213819532\n",
      "Current iteration=4, loss=38544.57611262073\n",
      "Current iteration=6, loss=32350.226504112674\n",
      "Current iteration=8, loss=26446.57339916603\n",
      "loss=20803.363177035928\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.83642142739\n",
      "Current iteration=4, loss=38541.33592052058\n",
      "Current iteration=6, loss=32346.1917704183\n",
      "Current iteration=8, loss=26442.043977625184\n",
      "loss=20798.496671289722\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.97392626297\n",
      "Current iteration=4, loss=38649.67273096418\n",
      "Current iteration=6, loss=32505.868450502825\n",
      "Current iteration=8, loss=26651.304260967663\n",
      "loss=21055.701787298203\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.35830567985\n",
      "Current iteration=4, loss=38616.37571765705\n",
      "Current iteration=6, loss=32456.000668241395\n",
      "Current iteration=8, loss=26585.104090834524\n",
      "loss=20973.491478704163\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.8022266592\n",
      "Current iteration=4, loss=38544.57651102336\n",
      "Current iteration=6, loss=32350.22741185688\n",
      "Current iteration=8, loss=26446.57499725949\n",
      "loss=20803.364293767678\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.836509916924\n",
      "Current iteration=4, loss=38541.336319014656\n",
      "Current iteration=6, loss=32346.19267832776\n",
      "Current iteration=8, loss=26442.045575953176\n",
      "loss=20798.4977881175\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.97401404378\n",
      "Current iteration=4, loss=38649.67312622819\n",
      "Current iteration=6, loss=32505.86935097435\n",
      "Current iteration=8, loss=26651.30584607678\n",
      "loss=21055.70289469931\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.358393673865\n",
      "Current iteration=4, loss=38616.37611391917\n",
      "Current iteration=6, loss=32456.001571059503\n",
      "Current iteration=8, loss=26585.105680180353\n",
      "loss=20973.49258921235\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.8024728153\n",
      "Current iteration=4, loss=38544.57761960229\n",
      "Current iteration=6, loss=32350.229937708747\n",
      "Current iteration=8, loss=26446.57944404876\n",
      "loss=20803.36740113986\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.83675614424\n",
      "Current iteration=4, loss=38541.337427848\n",
      "Current iteration=6, loss=32346.195204639545\n",
      "Current iteration=8, loss=26442.050023395095\n",
      "loss=20798.500895756857\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.97425829905\n",
      "Current iteration=4, loss=38649.67422607373\n",
      "Current iteration=6, loss=32505.871856589598\n",
      "Current iteration=8, loss=26651.31025673639\n",
      "loss=21055.705976108402\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.35863852244\n",
      "Current iteration=4, loss=38616.37721654196\n",
      "Current iteration=6, loss=32456.00408320426\n",
      "Current iteration=8, loss=26585.11010262896\n",
      "loss=20973.495679267115\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.80315775918\n",
      "Current iteration=4, loss=38544.580704288455\n",
      "Current iteration=6, loss=32350.236966039512\n",
      "Current iteration=8, loss=26446.59181749893\n",
      "loss=20803.376047585698\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.837441286436\n",
      "Current iteration=4, loss=38541.340513242125\n",
      "Current iteration=6, loss=32346.202234250017\n",
      "Current iteration=8, loss=26442.062398661248\n",
      "loss=20798.509542946107\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.97493795385\n",
      "Current iteration=4, loss=38649.677286458704\n",
      "Current iteration=6, loss=32505.87882861072\n",
      "Current iteration=8, loss=26651.322529653604\n",
      "loss=21055.71455031042\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.359319828116\n",
      "Current iteration=4, loss=38616.38028465487\n",
      "Current iteration=6, loss=32456.011073394162\n",
      "Current iteration=8, loss=26585.122408349685\n",
      "loss=20973.504277526124\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.80506365595\n",
      "Current iteration=4, loss=38544.58928760661\n",
      "Current iteration=6, loss=32350.25652277103\n",
      "Current iteration=8, loss=26446.626247318884\n",
      "loss=20803.400106820285\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.83934773492\n",
      "Current iteration=4, loss=38541.34909853025\n",
      "Current iteration=6, loss=32346.2217945424\n",
      "Current iteration=8, loss=26442.096833534295\n",
      "loss=20798.533604249278\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.9768291334\n",
      "Current iteration=4, loss=38649.68580215742\n",
      "Current iteration=6, loss=32505.898228657483\n",
      "Current iteration=8, loss=26651.35667973501\n",
      "loss=21055.738408522397\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.36121560128\n",
      "Current iteration=4, loss=38616.388821856955\n",
      "Current iteration=6, loss=32456.030523996633\n",
      "Current iteration=8, loss=26585.156649708708\n",
      "loss=20973.52820267818\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45062.81036692439\n",
      "Current iteration=4, loss=38544.61317116586\n",
      "Current iteration=6, loss=32350.31094041179\n",
      "Current iteration=8, loss=26446.722050027758\n",
      "loss=20803.467052957\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=45060.84465253856\n",
      "Current iteration=4, loss=38541.37298757103\n",
      "Current iteration=6, loss=32346.276222091474\n",
      "Current iteration=8, loss=26442.192650303576\n",
      "loss=20798.600556142024\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=45115.982091450336\n",
      "Current iteration=4, loss=38649.70949756177\n",
      "Current iteration=6, loss=32505.952210314663\n",
      "Current iteration=8, loss=26651.451704057243\n",
      "loss=21055.804795302836\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=45099.366490700224\n",
      "Current iteration=4, loss=38616.41257709568\n",
      "Current iteration=6, loss=32456.084646327694\n",
      "Current iteration=8, loss=26585.251928015543\n",
      "loss=20973.594775722966\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.25254224887\n",
      "Current iteration=4, loss=34346.32153346375\n",
      "Current iteration=6, loss=26400.1641711233\n",
      "Current iteration=8, loss=18920.037221043014\n",
      "loss=11842.268276924835\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.75942630515\n",
      "Current iteration=4, loss=34342.4632171766\n",
      "Current iteration=6, loss=26395.5944527685\n",
      "Current iteration=8, loss=18915.049359260065\n",
      "loss=11836.912599722491\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.94996598372\n",
      "Current iteration=4, loss=34485.434375543926\n",
      "Current iteration=6, loss=26605.14996949086\n",
      "Current iteration=8, loss=19188.26477664748\n",
      "loss=12171.156910697347\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.773334550715\n",
      "Current iteration=4, loss=34441.05150290201\n",
      "Current iteration=6, loss=26538.90419005186\n",
      "Current iteration=8, loss=19100.723823236884\n",
      "loss=12062.998479670583\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.252544856354\n",
      "Current iteration=4, loss=34346.321545075756\n",
      "Current iteration=6, loss=26400.16419732279\n",
      "Current iteration=8, loss=18920.037266781343\n",
      "loss=11842.26830834226\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.759428913356\n",
      "Current iteration=4, loss=34342.463228791\n",
      "Current iteration=6, loss=26395.59447897194\n",
      "Current iteration=8, loss=18915.049405003814\n",
      "loss=11836.912631142259\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.94996857099\n",
      "Current iteration=4, loss=34485.43438706364\n",
      "Current iteration=6, loss=26605.149995477863\n",
      "Current iteration=8, loss=19188.26482200908\n",
      "loss=12171.156941848152\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.773337144296\n",
      "Current iteration=4, loss=34441.0515144513\n",
      "Current iteration=6, loss=26538.904216108058\n",
      "Current iteration=8, loss=19100.723868722638\n",
      "loss=12062.998510910926\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.252552111815\n",
      "Current iteration=4, loss=34346.32157738691\n",
      "Current iteration=6, loss=26400.16427022441\n",
      "Current iteration=8, loss=18920.037394050978\n",
      "loss=11842.268395763138\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.75943617084\n",
      "Current iteration=4, loss=34342.46326110869\n",
      "Current iteration=6, loss=26395.59455188459\n",
      "Current iteration=8, loss=18915.04953228854\n",
      "loss=11836.912718569642\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.94997577022\n",
      "Current iteration=4, loss=34485.43441911794\n",
      "Current iteration=6, loss=26605.15006778828\n",
      "Current iteration=8, loss=19188.26494823045\n",
      "loss=12171.157028527145\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.77334436116\n",
      "Current iteration=4, loss=34441.05154658792\n",
      "Current iteration=6, loss=26538.904288611087\n",
      "Current iteration=8, loss=19100.723995289383\n",
      "loss=12062.998597838974\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.25257230054\n",
      "Current iteration=4, loss=34346.321667294564\n",
      "Current iteration=6, loss=26400.164473077486\n",
      "Current iteration=8, loss=18920.037748186274\n",
      "loss=11842.268639016904\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.7594563652\n",
      "Current iteration=4, loss=34342.463351034676\n",
      "Current iteration=6, loss=26395.594754768343\n",
      "Current iteration=8, loss=18915.049886465884\n",
      "loss=11836.912961841488\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.9499958025\n",
      "Current iteration=4, loss=34485.434508310886\n",
      "Current iteration=6, loss=26605.1502689963\n",
      "Current iteration=8, loss=19188.265299448878\n",
      "loss=12171.157269716612\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.77336444244\n",
      "Current iteration=4, loss=34441.05163600994\n",
      "Current iteration=6, loss=26538.904490354988\n",
      "Current iteration=8, loss=19100.72434746891\n",
      "loss=12062.998839721542\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.25262847696\n",
      "Current iteration=4, loss=34346.321917467976\n",
      "Current iteration=6, loss=26400.16503752823\n",
      "Current iteration=8, loss=18920.03873358875\n",
      "loss=11842.269315884922\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.75951255721\n",
      "Current iteration=4, loss=34342.46360125904\n",
      "Current iteration=6, loss=26395.595319304448\n",
      "Current iteration=8, loss=18915.050871985288\n",
      "loss=11836.913638759863\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.95005154351\n",
      "Current iteration=4, loss=34485.43475649555\n",
      "Current iteration=6, loss=26605.15082886954\n",
      "Current iteration=8, loss=19188.266276735056\n",
      "loss=12171.157940840574\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.773420319834\n",
      "Current iteration=4, loss=34441.05188483204\n",
      "Current iteration=6, loss=26538.90505171941\n",
      "Current iteration=8, loss=19100.72532742928\n",
      "loss=12062.999512774077\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.25278479111\n",
      "Current iteration=4, loss=34346.32261359037\n",
      "Current iteration=6, loss=26400.16660814581\n",
      "Current iteration=8, loss=18920.04147552948\n",
      "loss=11842.27119931038\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.75966891486\n",
      "Current iteration=4, loss=34342.46429752317\n",
      "Current iteration=6, loss=26395.596890159646\n",
      "Current iteration=8, loss=18915.053614251443\n",
      "loss=11836.915522325384\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.950206646194\n",
      "Current iteration=4, loss=34485.43544708419\n",
      "Current iteration=6, loss=26605.152386750044\n",
      "Current iteration=8, loss=19188.268996091665\n",
      "loss=12171.159808282806\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.77357580196\n",
      "Current iteration=4, loss=34441.052577194314\n",
      "Current iteration=6, loss=26538.906613749154\n",
      "Current iteration=8, loss=19100.728054227104\n",
      "loss=12063.001385582691\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.25321974449\n",
      "Current iteration=4, loss=34346.324550592006\n",
      "Current iteration=6, loss=26400.17097848194\n",
      "Current iteration=8, loss=18920.049105140904\n",
      "loss=11842.27644005305\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.76010398931\n",
      "Current iteration=4, loss=34342.46623491928\n",
      "Current iteration=6, loss=26395.601261156873\n",
      "Current iteration=8, loss=18915.061244768316\n",
      "loss=11836.92076345782\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.95063822856\n",
      "Current iteration=4, loss=34485.4373686879\n",
      "Current iteration=6, loss=26605.156721644416\n",
      "Current iteration=8, loss=19188.276562861454\n",
      "loss=12171.1650045512\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.77400844025\n",
      "Current iteration=4, loss=34441.05450373322\n",
      "Current iteration=6, loss=26538.910960189103\n",
      "Current iteration=8, loss=19100.73564170245\n",
      "loss=12063.006596783365\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.25443002801\n",
      "Current iteration=4, loss=34346.32994041288\n",
      "Current iteration=6, loss=26400.183139196954\n",
      "Current iteration=8, loss=18920.070334976033\n",
      "loss=11842.291022726691\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.76131460972\n",
      "Current iteration=4, loss=34342.471625837745\n",
      "Current iteration=6, loss=26395.61342371142\n",
      "Current iteration=8, loss=18915.082477122975\n",
      "loss=11836.935347215993\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.951839132045\n",
      "Current iteration=4, loss=34485.442715663055\n",
      "Current iteration=6, loss=26605.16878374071\n",
      "Current iteration=8, loss=19188.297617836146\n",
      "loss=12171.179463472603\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.77521228183\n",
      "Current iteration=4, loss=34441.05986444091\n",
      "Current iteration=6, loss=26538.923054411556\n",
      "Current iteration=8, loss=19100.75675429157\n",
      "loss=12063.021097254685\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.257797713064\n",
      "Current iteration=4, loss=34346.344937899536\n",
      "Current iteration=6, loss=26400.216977071217\n",
      "Current iteration=8, loss=18920.129408161723\n",
      "loss=11842.331599850317\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.76468323209\n",
      "Current iteration=4, loss=34342.486626378464\n",
      "Current iteration=6, loss=26395.647266704273\n",
      "Current iteration=8, loss=18915.14155731935\n",
      "loss=11836.97592735741\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.95518071658\n",
      "Current iteration=4, loss=34485.45759392909\n",
      "Current iteration=6, loss=26605.202347202863\n",
      "Current iteration=8, loss=19188.356204462973\n",
      "loss=12171.21969624851\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.77856204183\n",
      "Current iteration=4, loss=34441.07478091846\n",
      "Current iteration=6, loss=26538.956707266534\n",
      "Current iteration=8, loss=19100.81550123373\n",
      "loss=12063.061445645602\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42832.26716849092\n",
      "Current iteration=4, loss=34346.38666921912\n",
      "Current iteration=6, loss=26400.31113267476\n",
      "Current iteration=8, loss=18920.293782100103\n",
      "loss=11842.444507859116\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=42829.77405661811\n",
      "Current iteration=4, loss=34342.5283661962\n",
      "Current iteration=6, loss=26395.74143655058\n",
      "Current iteration=8, loss=18915.305950765254\n",
      "loss=11837.088843763268\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=42902.96447886822\n",
      "Current iteration=4, loss=34485.498993510766\n",
      "Current iteration=6, loss=26605.29573924073\n",
      "Current iteration=8, loss=19188.51922452825\n",
      "loss=12171.33164609156\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=42880.78788294213\n",
      "Current iteration=4, loss=34441.11628682582\n",
      "Current iteration=6, loss=26539.050348044588\n",
      "Current iteration=8, loss=19100.97896738404\n",
      "loss=12063.173717193546\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.70848536067\n",
      "Current iteration=4, loss=30269.267458989136\n",
      "Current iteration=6, loss=20696.298838539053\n",
      "Current iteration=8, loss=11779.49502628051\n",
      "loss=3410.0101885242257\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.74574453203\n",
      "Current iteration=4, loss=30264.950166793788\n",
      "Current iteration=6, loss=20691.372413114652\n",
      "Current iteration=8, loss=11774.131885182698\n",
      "loss=3404.0662631370215\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.82823106753\n",
      "Current iteration=4, loss=30441.850713190866\n",
      "Current iteration=6, loss=20949.286407329444\n",
      "Current iteration=8, loss=12108.815299830685\n",
      "loss=3811.8448132505073\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.08502905762\n",
      "Current iteration=4, loss=30386.446083681913\n",
      "Current iteration=6, loss=20866.94111288349\n",
      "Current iteration=8, loss=12000.555162078483\n",
      "loss=3678.7875290979428\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.708489410186\n",
      "Current iteration=4, loss=30269.267476829016\n",
      "Current iteration=6, loss=20696.298878430636\n",
      "Current iteration=8, loss=11779.495095418104\n",
      "loss=3410.0102353467955\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.745748582645\n",
      "Current iteration=4, loss=30264.950184636913\n",
      "Current iteration=6, loss=20691.372453011354\n",
      "Current iteration=8, loss=11774.131954327424\n",
      "loss=3404.0663099634476\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.82823508555\n",
      "Current iteration=4, loss=30441.85073088776\n",
      "Current iteration=6, loss=20949.286446894126\n",
      "Current iteration=8, loss=12108.815368392488\n",
      "loss=3811.8448596710878\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.08503308555\n",
      "Current iteration=4, loss=30386.446101424935\n",
      "Current iteration=6, loss=20866.941152555475\n",
      "Current iteration=8, loss=12000.555230831305\n",
      "loss=3678.7875756539506\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.70850067826\n",
      "Current iteration=4, loss=30269.267526469597\n",
      "Current iteration=6, loss=20696.298989431303\n",
      "Current iteration=8, loss=11779.495287797632\n",
      "loss=3410.010365633352\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.74575985371\n",
      "Current iteration=4, loss=30264.95023428647\n",
      "Current iteration=6, loss=20691.372564026286\n",
      "Current iteration=8, loss=11774.132146726739\n",
      "loss=3404.066440260758\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.82824626597\n",
      "Current iteration=4, loss=30441.850780130433\n",
      "Current iteration=6, loss=20949.286556985193\n",
      "Current iteration=8, loss=12108.815559169809\n",
      "loss=3811.844988839104\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.0850442935\n",
      "Current iteration=4, loss=30386.446150796022\n",
      "Current iteration=6, loss=20866.94126294512\n",
      "Current iteration=8, loss=12000.55542214013\n",
      "loss=3678.787705198876\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.708532032244\n",
      "Current iteration=4, loss=30269.267664597453\n",
      "Current iteration=6, loss=20696.299298297323\n",
      "Current iteration=8, loss=11779.495823105026\n",
      "loss=3410.0107281634046\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.7457912161\n",
      "Current iteration=4, loss=30264.950372439318\n",
      "Current iteration=6, loss=20691.37287293194\n",
      "Current iteration=8, loss=11774.132682089232\n",
      "loss=3404.066802820773\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.82827737608\n",
      "Current iteration=4, loss=30441.850917151092\n",
      "Current iteration=6, loss=20949.28686332012\n",
      "Current iteration=8, loss=12108.816090018989\n",
      "loss=3811.845348256771\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.085075480274\n",
      "Current iteration=4, loss=30386.446288173986\n",
      "Current iteration=6, loss=20866.9415701108\n",
      "Current iteration=8, loss=12000.555954468315\n",
      "loss=3678.7880656652706\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.70861927663\n",
      "Current iteration=4, loss=30269.268048946396\n",
      "Current iteration=6, loss=20696.300157735357\n",
      "Current iteration=8, loss=11779.49731262968\n",
      "loss=3410.0117369248037\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.7458784838\n",
      "Current iteration=4, loss=30264.95075685777\n",
      "Current iteration=6, loss=20691.373732480253\n",
      "Current iteration=8, loss=11774.134171767178\n",
      "loss=3404.06781166554\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.82836394185\n",
      "Current iteration=4, loss=30441.8512984192\n",
      "Current iteration=6, loss=20949.287715715214\n",
      "Current iteration=8, loss=12108.817567138303\n",
      "loss=3811.8463483577725\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.08516225931\n",
      "Current iteration=4, loss=30386.446670436333\n",
      "Current iteration=6, loss=20866.942424817527\n",
      "Current iteration=8, loss=12000.55743570309\n",
      "loss=3678.7890686844257\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.70886203932\n",
      "Current iteration=4, loss=30269.26911842012\n",
      "Current iteration=6, loss=20696.30254917257\n",
      "Current iteration=8, loss=11779.50145732006\n",
      "loss=3410.01454386321\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.74612131137\n",
      "Current iteration=4, loss=30264.95182652495\n",
      "Current iteration=6, loss=20691.376124224327\n",
      "Current iteration=8, loss=11774.138316884098\n",
      "loss=3404.0706188358963\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.82860481623\n",
      "Current iteration=4, loss=30441.8523593203\n",
      "Current iteration=6, loss=20949.29008755505\n",
      "Current iteration=8, loss=12108.821677310212\n",
      "loss=3811.849131198098\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.08540372718\n",
      "Current iteration=4, loss=30386.447734103975\n",
      "Current iteration=6, loss=20866.9448030897\n",
      "Current iteration=8, loss=12000.561557326431\n",
      "loss=3678.791859644669\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.70953754077\n",
      "Current iteration=4, loss=30269.272094293985\n",
      "Current iteration=6, loss=20696.30920348747\n",
      "Current iteration=8, loss=11779.512990164432\n",
      "loss=3410.0223543350876\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.74679699349\n",
      "Current iteration=4, loss=30264.954802937085\n",
      "Current iteration=6, loss=20691.382779393112\n",
      "Current iteration=8, loss=11774.149850915337\n",
      "loss=3404.0784299531733\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.82927506344\n",
      "Current iteration=4, loss=30441.85531134032\n",
      "Current iteration=6, loss=20949.296687339098\n",
      "Current iteration=8, loss=12108.833114104831\n",
      "loss=3811.856874615565\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.08607562585\n",
      "Current iteration=4, loss=30386.450693822087\n",
      "Current iteration=6, loss=20866.951420772035\n",
      "Current iteration=8, loss=12000.573025985348\n",
      "loss=3678.7996256564147\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.711417163606\n",
      "Current iteration=4, loss=30269.280374837275\n",
      "Current iteration=6, loss=20696.327719504672\n",
      "Current iteration=8, loss=11779.545080966527\n",
      "loss=3410.0440874294436\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.74867711896\n",
      "Current iteration=4, loss=30264.963084978186\n",
      "Current iteration=6, loss=20691.40129778626\n",
      "Current iteration=8, loss=11774.181945019967\n",
      "loss=3404.1001648434467\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.831140065944\n",
      "Current iteration=4, loss=30441.863525508925\n",
      "Current iteration=6, loss=20949.315051621095\n",
      "Current iteration=8, loss=12108.864937643022\n",
      "loss=3811.878421127207\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.087945223546\n",
      "Current iteration=4, loss=30386.458929411034\n",
      "Current iteration=6, loss=20866.969834857053\n",
      "Current iteration=8, loss=12000.604938187773\n",
      "loss=3678.821235037787\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.716647324334\n",
      "Current iteration=4, loss=30269.303415921495\n",
      "Current iteration=6, loss=20696.379241350696\n",
      "Current iteration=8, loss=11779.634375356301\n",
      "loss=3410.104560996624\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.75390867823\n",
      "Current iteration=4, loss=30264.986130230132\n",
      "Current iteration=6, loss=20691.452826243574\n",
      "Current iteration=8, loss=11774.27124859928\n",
      "loss=3404.160643407842\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.8363295448\n",
      "Current iteration=4, loss=30441.886381901735\n",
      "Current iteration=6, loss=20949.366151255497\n",
      "Current iteration=8, loss=12108.953488356563\n",
      "loss=3811.938375517406\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.09314748882\n",
      "Current iteration=4, loss=30386.481845407234\n",
      "Current iteration=6, loss=20867.02107307118\n",
      "Current iteration=8, loss=12000.693735614355\n",
      "loss=3678.881364366658\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40631.73120054576\n",
      "Current iteration=4, loss=30269.367528957144\n",
      "Current iteration=6, loss=20696.522603392576\n",
      "Current iteration=8, loss=11779.882840958335\n",
      "loss=3410.272831834045\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=40628.76846579127\n",
      "Current iteration=4, loss=30265.050254862726\n",
      "Current iteration=6, loss=20691.596206681596\n",
      "Current iteration=8, loss=11774.519739771535\n",
      "loss=3404.3289281502875\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=40719.85076956658\n",
      "Current iteration=4, loss=30441.949981023856\n",
      "Current iteration=6, loss=20949.508338472933\n",
      "Current iteration=8, loss=12109.199884646465\n",
      "loss=3812.1052017181864\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=40692.10762308957\n",
      "Current iteration=4, loss=30386.54561037886\n",
      "Current iteration=6, loss=20867.163645893383\n",
      "Current iteration=8, loss=12000.940818394189\n",
      "loss=3679.0486773432276\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.553913550895\n",
      "Current iteration=4, loss=26306.196172308802\n",
      "Current iteration=6, loss=15215.05526028548\n",
      "Current iteration=8, loss=4975.844226659513\n",
      "loss=-4574.815995634985\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.17544382359\n",
      "Current iteration=4, loss=26301.54053383832\n",
      "Current iteration=6, loss=15209.852820111664\n",
      "Current iteration=8, loss=4970.046707343354\n",
      "loss=-4581.588461047257\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38565.99031985566\n",
      "Current iteration=4, loss=26511.69072428359\n",
      "Current iteration=6, loss=15514.710508742612\n",
      "Current iteration=8, loss=5363.971563915148\n",
      "loss=-4103.295265262772\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.67843753816\n",
      "Current iteration=4, loss=26445.355739893133\n",
      "Current iteration=6, loss=15416.595431472897\n",
      "Current iteration=8, loss=5235.65852240511\n",
      "loss=-4260.201291323305\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.55391934707\n",
      "Current iteration=4, loss=26306.196197579124\n",
      "Current iteration=6, loss=15215.055316333164\n",
      "Current iteration=8, loss=4975.844323195025\n",
      "loss=-4574.815931007084\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.175449621245\n",
      "Current iteration=4, loss=26301.540559112716\n",
      "Current iteration=6, loss=15209.85287616567\n",
      "Current iteration=8, loss=4970.046803888173\n",
      "loss=-4581.588396412721\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38565.990325606566\n",
      "Current iteration=4, loss=26511.690749349807\n",
      "Current iteration=6, loss=15514.710564326844\n",
      "Current iteration=8, loss=5363.971659639307\n",
      "loss=-4103.295201194822\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.67844330337\n",
      "Current iteration=4, loss=26445.355765025586\n",
      "Current iteration=6, loss=15416.595487210156\n",
      "Current iteration=8, loss=5235.65861839964\n",
      "loss=-4260.201227066638\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.55393547528\n",
      "Current iteration=4, loss=26306.196267895324\n",
      "Current iteration=6, loss=15215.055472289223\n",
      "Current iteration=8, loss=4975.844591810809\n",
      "loss=-4574.8157511760655\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.175465753564\n",
      "Current iteration=4, loss=26301.540629440235\n",
      "Current iteration=6, loss=15209.853032139263\n",
      "Current iteration=8, loss=4970.047072529906\n",
      "loss=-4581.58821656333\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38565.99034160891\n",
      "Current iteration=4, loss=26511.690819098025\n",
      "Current iteration=6, loss=15514.710718993309\n",
      "Current iteration=8, loss=5363.971925997417\n",
      "loss=-4103.295022921957\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.67845934534\n",
      "Current iteration=4, loss=26445.3558349582\n",
      "Current iteration=6, loss=15416.595642302376\n",
      "Current iteration=8, loss=5235.658885510163\n",
      "loss=-4260.201048268704\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.55398035293\n",
      "Current iteration=4, loss=26306.19646355425\n",
      "Current iteration=6, loss=15215.055906246214\n",
      "Current iteration=8, loss=4975.845339250143\n",
      "loss=-4574.815250785505\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.17551064277\n",
      "Current iteration=4, loss=26301.540825130705\n",
      "Current iteration=6, loss=15209.853466145001\n",
      "Current iteration=8, loss=4970.047820041466\n",
      "loss=-4581.587716121707\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38565.99038613622\n",
      "Current iteration=4, loss=26511.69101317659\n",
      "Current iteration=6, loss=15514.711149361969\n",
      "Current iteration=8, loss=5363.972667154685\n",
      "loss=-4103.294526867001\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.67850398317\n",
      "Current iteration=4, loss=26445.35602954982\n",
      "Current iteration=6, loss=15416.59607385568\n",
      "Current iteration=8, loss=5235.659628761083\n",
      "loss=-4260.200550752859\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.55410522771\n",
      "Current iteration=4, loss=26306.197007986848\n",
      "Current iteration=6, loss=15215.057113757288\n",
      "Current iteration=8, loss=4975.847419044419\n",
      "loss=-4574.813858419184\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.175635549575\n",
      "Current iteration=4, loss=26301.541369651106\n",
      "Current iteration=6, loss=15209.854673791722\n",
      "Current iteration=8, loss=4970.0499000366835\n",
      "loss=-4581.5863236131845\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38565.99051003622\n",
      "Current iteration=4, loss=26511.691553211702\n",
      "Current iteration=6, loss=15514.712346888231\n",
      "Current iteration=8, loss=5363.974729468777\n",
      "loss=-4103.293146564756\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.67862819056\n",
      "Current iteration=4, loss=26445.35657101256\n",
      "Current iteration=6, loss=15416.597274678368\n",
      "Current iteration=8, loss=5235.66169690081\n",
      "loss=-4260.199166385486\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.55445269914\n",
      "Current iteration=4, loss=26306.19852290284\n",
      "Current iteration=6, loss=15215.060473728345\n",
      "Current iteration=8, loss=4975.853206194909\n",
      "loss=-4574.809984077314\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.17598311025\n",
      "Current iteration=4, loss=26301.542884811355\n",
      "Current iteration=6, loss=15209.858034140203\n",
      "Current iteration=8, loss=4970.055687746367\n",
      "loss=-4581.582448875728\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38565.99085479531\n",
      "Current iteration=4, loss=26511.693055891425\n",
      "Current iteration=6, loss=15514.715679075991\n",
      "Current iteration=8, loss=5363.980467979576\n",
      "loss=-4103.289305791906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.67897380494\n",
      "Current iteration=4, loss=26445.35807766473\n",
      "Current iteration=6, loss=15416.600616038519\n",
      "Current iteration=8, loss=5235.667451621838\n",
      "loss=-4260.195314301258\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.55541955906\n",
      "Current iteration=4, loss=26306.202738246026\n",
      "Current iteration=6, loss=15215.069823045373\n",
      "Current iteration=8, loss=4975.869309280073\n",
      "loss=-4574.799203492585\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.17695021837\n",
      "Current iteration=4, loss=26301.547100834203\n",
      "Current iteration=6, loss=15209.867384507439\n",
      "Current iteration=8, loss=4970.0717923874345\n",
      "loss=-4581.571667190219\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38565.99181410789\n",
      "Current iteration=4, loss=26511.697237186465\n",
      "Current iteration=6, loss=15514.724951084343\n",
      "Current iteration=8, loss=5363.996435721944\n",
      "loss=-4103.278618614961\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.67993549753\n",
      "Current iteration=4, loss=26445.362270013306\n",
      "Current iteration=6, loss=15416.609913569646\n",
      "Current iteration=8, loss=5235.683464470111\n",
      "loss=-4260.184595649669\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.55810990382\n",
      "Current iteration=4, loss=26306.214467684575\n",
      "Current iteration=6, loss=15215.095838059693\n",
      "Current iteration=8, loss=4975.914117033571\n",
      "loss=-4574.769205887649\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.17964125388\n",
      "Current iteration=4, loss=26301.55883216395\n",
      "Current iteration=6, loss=15209.893402444133\n",
      "Current iteration=8, loss=4970.11660447035\n",
      "loss=-4581.541666522273\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38565.994483451825\n",
      "Current iteration=4, loss=26511.708871884064\n",
      "Current iteration=6, loss=15514.750750982856\n",
      "Current iteration=8, loss=5364.040866876373\n",
      "loss=-4103.248880922591\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.68261146395\n",
      "Current iteration=4, loss=26445.373935468037\n",
      "Current iteration=6, loss=15416.635784486789\n",
      "Current iteration=8, loss=5235.728021134346\n",
      "loss=-4260.154770377303\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.56559594542\n",
      "Current iteration=4, loss=26306.247105511164\n",
      "Current iteration=6, loss=15215.16822626144\n",
      "Current iteration=8, loss=4976.038796978652\n",
      "loss=-4574.685735866537\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.18712921753\n",
      "Current iteration=4, loss=26301.591475252902\n",
      "Current iteration=6, loss=15209.965798777419\n",
      "Current iteration=8, loss=4970.24129646223\n",
      "loss=-4581.458187978256\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38566.00191105731\n",
      "Current iteration=4, loss=26511.74124608861\n",
      "Current iteration=6, loss=15514.822540613019\n",
      "Current iteration=8, loss=5364.164498914714\n",
      "loss=-4103.166134122817\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.69005749687\n",
      "Current iteration=4, loss=26445.406395256025\n",
      "Current iteration=6, loss=15416.707771730227\n",
      "Current iteration=8, loss=5235.852002410236\n",
      "loss=-4260.071779881226\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38460.58642628126\n",
      "Current iteration=4, loss=26306.33792194734\n",
      "Current iteration=6, loss=15215.369649795875\n",
      "Current iteration=8, loss=4976.385724081575\n",
      "loss=-4574.453476322393\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=38457.20796490143\n",
      "Current iteration=4, loss=26301.6823063319\n",
      "Current iteration=6, loss=15210.167244938339\n",
      "Current iteration=8, loss=4970.588257085944\n",
      "loss=-4581.225904718682\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=38566.02257879133\n",
      "Current iteration=4, loss=26511.83132898289\n",
      "Current iteration=6, loss=15515.022298594284\n",
      "Current iteration=8, loss=5364.508510173792\n",
      "loss=-4102.935886978143\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=38532.71077650623\n",
      "Current iteration=4, loss=26445.49671629064\n",
      "Current iteration=6, loss=15416.908079579502\n",
      "Current iteration=8, loss=5236.196985437344\n",
      "loss=-4259.840854639442\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.154474594834\n",
      "Current iteration=4, loss=22450.13250424359\n",
      "Current iteration=6, loss=9935.03734130738\n",
      "Current iteration=8, loss=-1533.013585769547\n",
      "loss=-12178.705197264346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MachineLearning/FinalCopy2/scripts/cross_validation.py:61: RuntimeWarning: invalid value encountered in sqrt\n",
      "  loss_tr = np.sqrt(2*loss_tr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.41022877964\n",
      "Current iteration=4, loss=22445.22488376712\n",
      "Current iteration=6, loss=9929.567244548745\n",
      "Current iteration=8, loss=-1539.3846690476953\n",
      "loss=-12186.5886644606\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.79967624978\n",
      "Current iteration=4, loss=22687.97229738101\n",
      "Current iteration=6, loss=10280.06120033935\n",
      "Current iteration=8, loss=-1088.197289952732\n",
      "loss=-11640.369740404902\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.920427220495\n",
      "Current iteration=4, loss=22610.820501436498\n",
      "Current iteration=6, loss=10166.535208411922\n",
      "Current iteration=8, loss=-1235.8923619109762\n",
      "loss=-11820.117502021414\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.154482436774\n",
      "Current iteration=4, loss=22450.13253809374\n",
      "Current iteration=6, loss=9935.037415832729\n",
      "Current iteration=8, loss=-1533.0134580881281\n",
      "loss=-12178.705112575219\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.41023662352\n",
      "Current iteration=4, loss=22445.224917622138\n",
      "Current iteration=6, loss=9929.567319081709\n",
      "Current iteration=8, loss=-1539.3845413538813\n",
      "loss=-12186.588579760777\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.79968403028\n",
      "Current iteration=4, loss=22687.972330955807\n",
      "Current iteration=6, loss=10280.061274243633\n",
      "Current iteration=8, loss=-1088.1971633527453\n",
      "loss=-11640.369656455347\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.9204350204\n",
      "Current iteration=4, loss=22610.820535101135\n",
      "Current iteration=6, loss=10166.53528252216\n",
      "Current iteration=8, loss=-1235.8922349495924\n",
      "loss=-11820.117417823392\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.1545042574\n",
      "Current iteration=4, loss=22450.132632283774\n",
      "Current iteration=6, loss=9935.037623203907\n",
      "Current iteration=8, loss=-1533.0131028070284\n",
      "loss=-12178.704876922704\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.41025844954\n",
      "Current iteration=4, loss=22445.22501182581\n",
      "Current iteration=6, loss=9929.567526474088\n",
      "Current iteration=8, loss=-1539.3841860382415\n",
      "loss=-12186.588344078504\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.79970567998\n",
      "Current iteration=4, loss=22687.972424379703\n",
      "Current iteration=6, loss=10280.061479886674\n",
      "Current iteration=8, loss=-1088.1968110807509\n",
      "loss=-11640.369422860706\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.920456724176\n",
      "Current iteration=4, loss=22610.82062877506\n",
      "Current iteration=6, loss=10166.535488738262\n",
      "Current iteration=8, loss=-1235.8918816719295\n",
      "loss=-11820.117183537317\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.15456497462\n",
      "Current iteration=4, loss=22450.13289437316\n",
      "Current iteration=6, loss=9935.038200226545\n",
      "Current iteration=8, loss=-1533.0121142162034\n",
      "loss=-12178.704221205538\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.41031918173\n",
      "Current iteration=4, loss=22445.22527395309\n",
      "Current iteration=6, loss=9929.568103555732\n",
      "Current iteration=8, loss=-1539.3831973513543\n",
      "loss=-12186.587688278545\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.799765921605\n",
      "Current iteration=4, loss=22687.972684337277\n",
      "Current iteration=6, loss=10280.062052100622\n",
      "Current iteration=8, loss=-1088.195830862991\n",
      "loss=-11640.36877286977\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.920517116276\n",
      "Current iteration=4, loss=22610.820889428254\n",
      "Current iteration=6, loss=10166.536062546773\n",
      "Current iteration=8, loss=-1235.890898655915\n",
      "loss=-11820.11653162253\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.15473392389\n",
      "Current iteration=4, loss=22450.133623652426\n",
      "Current iteration=6, loss=9935.039805826238\n",
      "Current iteration=8, loss=-1533.009363403666\n",
      "loss=-12178.7023966337\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.41048817262\n",
      "Current iteration=4, loss=22445.226003337815\n",
      "Current iteration=6, loss=9929.569709319587\n",
      "Current iteration=8, loss=-1539.3804462715468\n",
      "loss=-12186.58586347627\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.79993354742\n",
      "Current iteration=4, loss=22687.973407684647\n",
      "Current iteration=6, loss=10280.063644319944\n",
      "Current iteration=8, loss=-1088.193103348958\n",
      "loss=-11640.36696423142\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.92068516076\n",
      "Current iteration=4, loss=22610.821614711243\n",
      "Current iteration=6, loss=10166.537659203032\n",
      "Current iteration=8, loss=-1235.888163355504\n",
      "loss=-11820.114717630844\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.15520403527\n",
      "Current iteration=4, loss=22450.135652915196\n",
      "Current iteration=6, loss=9935.044273502344\n",
      "Current iteration=8, loss=-1533.0017091053737\n",
      "loss=-12178.697319654471\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.4109583998\n",
      "Current iteration=4, loss=22445.22803289399\n",
      "Current iteration=6, loss=9929.574177452547\n",
      "Current iteration=8, loss=-1539.3727912295315\n",
      "loss=-12186.580785855833\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.8003999763\n",
      "Current iteration=4, loss=22687.975420441522\n",
      "Current iteration=6, loss=10280.068074764302\n",
      "Current iteration=8, loss=-1088.185513880142\n",
      "loss=-11640.361931588042\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.92115275456\n",
      "Current iteration=4, loss=22610.823632854142\n",
      "Current iteration=6, loss=10166.54210199354\n",
      "Current iteration=8, loss=-1235.880552220705\n",
      "loss=-11820.109670091588\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.156512148016\n",
      "Current iteration=4, loss=22450.14129945853\n",
      "Current iteration=6, loss=9935.056705073426\n",
      "Current iteration=8, loss=-1532.98041057312\n",
      "loss=-12178.683192660632\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.41226683487\n",
      "Current iteration=4, loss=22445.233680253823\n",
      "Current iteration=6, loss=9929.586610294764\n",
      "Current iteration=8, loss=-1539.3514906278522\n",
      "loss=-12186.56665707782\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.801697842195\n",
      "Current iteration=4, loss=22687.981021056265\n",
      "Current iteration=6, loss=10280.080402735854\n",
      "Current iteration=8, loss=-1088.1643957397484\n",
      "loss=-11640.347927961411\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.92245386205\n",
      "Current iteration=4, loss=22610.82924845577\n",
      "Current iteration=6, loss=10166.55446431887\n",
      "Current iteration=8, loss=-1235.8593737934136\n",
      "loss=-11820.095625016169\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.16015204897\n",
      "Current iteration=4, loss=22450.157011294075\n",
      "Current iteration=6, loss=9935.091296634087\n",
      "Current iteration=8, loss=-1532.9211462000105\n",
      "loss=-12178.643883480028\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.41590763259\n",
      "Current iteration=4, loss=22445.249394361348\n",
      "Current iteration=6, loss=9929.621205392472\n",
      "Current iteration=8, loss=-1539.2922204964239\n",
      "loss=-12186.527342932684\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.80530923067\n",
      "Current iteration=4, loss=22687.996605092845\n",
      "Current iteration=6, loss=10280.114706024904\n",
      "Current iteration=8, loss=-1088.1056333171223\n",
      "loss=-11640.3089620572\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.926074270385\n",
      "Current iteration=4, loss=22610.844874194234\n",
      "Current iteration=6, loss=10166.588863199342\n",
      "Current iteration=8, loss=-1235.8004436191245\n",
      "loss=-11820.056543778388\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.17028028547\n",
      "Current iteration=4, loss=22450.200730358334\n",
      "Current iteration=6, loss=9935.187549518572\n",
      "Current iteration=8, loss=-1532.7562400105196\n",
      "loss=-12178.534503497103\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.426038364516\n",
      "Current iteration=4, loss=22445.29311974742\n",
      "Current iteration=6, loss=9929.717468118974\n",
      "Current iteration=8, loss=-1539.1272982841197\n",
      "loss=-12186.417949135626\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.81535812961\n",
      "Current iteration=4, loss=22688.039968549238\n",
      "Current iteration=6, loss=10280.210156778026\n",
      "Current iteration=8, loss=-1087.942123830962\n",
      "loss=-11640.20053725985\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.93614826765\n",
      "Current iteration=4, loss=22610.888353688493\n",
      "Current iteration=6, loss=10166.684579940747\n",
      "Current iteration=8, loss=-1235.636467355133\n",
      "loss=-11819.947798058893\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36318.198462674125\n",
      "Current iteration=4, loss=22450.322380852445\n",
      "Current iteration=6, loss=9935.455377432314\n",
      "Current iteration=8, loss=-1532.2973822202214\n",
      "loss=-12178.230148335817\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=36314.45422769672\n",
      "Current iteration=4, loss=22445.414787832342\n",
      "Current iteration=6, loss=9929.985323418541\n",
      "Current iteration=8, loss=-1538.6683959098443\n",
      "loss=-12186.113555535885\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=36440.843319757\n",
      "Current iteration=4, loss=22688.16062954663\n",
      "Current iteration=6, loss=10280.475752725772\n",
      "Current iteration=8, loss=-1087.487152421122\n",
      "loss=-11639.89883994864\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=36401.96417973244\n",
      "Current iteration=4, loss=22611.00933756709\n",
      "Current iteration=6, loss=10166.950916012665\n",
      "Current iteration=8, loss=-1235.1801971180714\n",
      "loss=-11819.645207766174\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.86105570042\n",
      "Current iteration=4, loss=18694.40632736735\n",
      "Current iteration=6, loss=4837.025448612707\n",
      "Current iteration=8, loss=-7782.966584933406\n",
      "loss=-19456.030663138576\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.7970198946\n",
      "Current iteration=4, loss=18689.303390377223\n",
      "Current iteration=6, loss=4831.245639093838\n",
      "Current iteration=8, loss=-7790.0860242510935\n",
      "loss=-19465.293071664666\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.60515414577\n",
      "Current iteration=4, loss=18964.0243310059\n",
      "Current iteration=6, loss=5226.173949004361\n",
      "Current iteration=8, loss=-7283.3879336841155\n",
      "loss=-18853.36788859657\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.16321214109\n",
      "Current iteration=4, loss=18876.18647922958\n",
      "Current iteration=6, loss=5097.608605681454\n",
      "Current iteration=8, loss=-7449.811225624493\n",
      "loss=-19055.009081710392\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.861065881894\n",
      "Current iteration=4, loss=18694.40637089909\n",
      "Current iteration=6, loss=4837.025543818359\n",
      "Current iteration=8, loss=-7782.966422555183\n",
      "loss=-19456.03055622908\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.797030078465\n",
      "Current iteration=4, loss=18689.303433914672\n",
      "Current iteration=6, loss=4831.245734308752\n",
      "Current iteration=8, loss=-7790.085861856293\n",
      "loss=-19465.29296473905\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.605164247216\n",
      "Current iteration=4, loss=18964.02437418129\n",
      "Current iteration=6, loss=5226.1740434111925\n",
      "Current iteration=8, loss=-7283.3877726903975\n",
      "loss=-18853.367782627385\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.16322226791\n",
      "Current iteration=4, loss=18876.186522521773\n",
      "Current iteration=6, loss=5097.608700354034\n",
      "Current iteration=8, loss=-7449.811064167507\n",
      "loss=-19055.008975426957\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.861094212436\n",
      "Current iteration=4, loss=18694.406492028775\n",
      "Current iteration=6, loss=4837.025808733757\n",
      "Current iteration=8, loss=-7782.9659707282135\n",
      "loss=-19456.03025874705\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.7970584157\n",
      "Current iteration=4, loss=18689.303555060204\n",
      "Current iteration=6, loss=4831.245999249822\n",
      "Current iteration=8, loss=-7790.085409983105\n",
      "loss=-19465.292667212132\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.605192355055\n",
      "Current iteration=4, loss=18964.024494319394\n",
      "Current iteration=6, loss=5226.174306103821\n",
      "Current iteration=8, loss=-7283.387324715814\n",
      "loss=-18853.36748776184\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.16325044641\n",
      "Current iteration=4, loss=18876.186642984845\n",
      "Current iteration=6, loss=5097.608963786128\n",
      "Current iteration=8, loss=-7449.810614903804\n",
      "loss=-19055.008679687067\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.86117304383\n",
      "Current iteration=4, loss=18694.40682907931\n",
      "Current iteration=6, loss=4837.02654587655\n",
      "Current iteration=8, loss=-7782.964713492826\n",
      "loss=-19456.029430985553\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.79713726571\n",
      "Current iteration=4, loss=18689.30389215482\n",
      "Current iteration=6, loss=4831.246736464137\n",
      "Current iteration=8, loss=-7790.0841526190725\n",
      "loss=-19465.29183932577\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.60527056681\n",
      "Current iteration=4, loss=18964.024828610738\n",
      "Current iteration=6, loss=5226.175037061658\n",
      "Current iteration=8, loss=-7283.386078199938\n",
      "loss=-18853.366667280978\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.1633288548\n",
      "Current iteration=4, loss=18876.186978180474\n",
      "Current iteration=6, loss=5097.609696801559\n",
      "Current iteration=8, loss=-7449.80936480087\n",
      "loss=-19055.007856773176\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.8613923968\n",
      "Current iteration=4, loss=18694.40776694242\n",
      "Current iteration=6, loss=4837.028597020116\n",
      "Current iteration=8, loss=-7782.961215160837\n",
      "loss=-19456.027127690126\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.79735667057\n",
      "Current iteration=4, loss=18689.304830140634\n",
      "Current iteration=6, loss=4831.248787806665\n",
      "Current iteration=8, loss=-7790.080653929172\n",
      "loss=-19465.28953568289\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.60548819566\n",
      "Current iteration=4, loss=18964.025758796313\n",
      "Current iteration=6, loss=5226.177070995195\n",
      "Current iteration=8, loss=-7283.382609695668\n",
      "loss=-18853.364384244298\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.16354703074\n",
      "Current iteration=4, loss=18876.187910882203\n",
      "Current iteration=6, loss=5097.611736460473\n",
      "Current iteration=8, loss=-7449.805886315359\n",
      "loss=-19055.005566966523\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.862002759546\n",
      "Current iteration=4, loss=18694.410376602114\n",
      "Current iteration=6, loss=4837.03430444831\n",
      "Current iteration=8, loss=-7782.95148084572\n",
      "loss=-19456.020718634274\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.797967177525\n",
      "Current iteration=4, loss=18689.3074401417\n",
      "Current iteration=6, loss=4831.254495788472\n",
      "Current iteration=8, loss=-7790.070918618143\n",
      "loss=-19465.28312566023\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.606093760834\n",
      "Current iteration=4, loss=18964.028347092695\n",
      "Current iteration=6, loss=5226.182730535577\n",
      "Current iteration=8, loss=-7283.372958377893\n",
      "loss=-18853.358031559474\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.164154118276\n",
      "Current iteration=4, loss=18876.190506180053\n",
      "Current iteration=6, loss=5097.617411931981\n",
      "Current iteration=8, loss=-7449.7962072242235\n",
      "loss=-19054.99919544396\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.86370113004\n",
      "Current iteration=4, loss=18694.417638133917\n",
      "Current iteration=6, loss=4837.050185701724\n",
      "Current iteration=8, loss=-7782.924394546561\n",
      "loss=-19456.002885059148\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.799665949424\n",
      "Current iteration=4, loss=18689.31470262344\n",
      "Current iteration=6, loss=4831.27037858239\n",
      "Current iteration=8, loss=-7790.043829547808\n",
      "loss=-19465.265289394883\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.60777878183\n",
      "Current iteration=4, loss=18964.03554917988\n",
      "Current iteration=6, loss=5226.198478538216\n",
      "Current iteration=8, loss=-7283.346103023715\n",
      "loss=-18853.340354840206\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.16584337526\n",
      "Current iteration=4, loss=18876.19772774917\n",
      "Current iteration=6, loss=5097.633204264021\n",
      "Current iteration=8, loss=-7449.769274589039\n",
      "loss=-19054.98146630743\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.86842694607\n",
      "Current iteration=4, loss=18694.437843767773\n",
      "Current iteration=6, loss=4837.094376197251\n",
      "Current iteration=8, loss=-7782.849025394598\n",
      "loss=-19455.95326210437\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.8043928823\n",
      "Current iteration=4, loss=18689.334910900463\n",
      "Current iteration=6, loss=4831.3145733643805\n",
      "Current iteration=8, loss=-7789.968452684965\n",
      "loss=-19465.21565895439\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.61246745209\n",
      "Current iteration=4, loss=18964.055589405583\n",
      "Current iteration=6, loss=5226.242298255854\n",
      "Current iteration=8, loss=-7283.271376489154\n",
      "loss=-18853.291168345917\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.170543832486\n",
      "Current iteration=4, loss=18876.217822184513\n",
      "Current iteration=6, loss=5097.677147330733\n",
      "Current iteration=8, loss=-7449.69433301579\n",
      "loss=-19054.93213395904\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.881576803906\n",
      "Current iteration=4, loss=18694.494067068288\n",
      "Current iteration=6, loss=4837.217338601309\n",
      "Current iteration=8, loss=-7782.639306906222\n",
      "loss=-19455.81518349707\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.81754584788\n",
      "Current iteration=4, loss=18689.391141555858\n",
      "Current iteration=6, loss=4831.437547695838\n",
      "Current iteration=8, loss=-7789.75871274062\n",
      "loss=-19465.077559517726\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.62551394965\n",
      "Current iteration=4, loss=18964.111352448814\n",
      "Current iteration=6, loss=5226.364228950806\n",
      "Current iteration=8, loss=-7283.063446116287\n",
      "loss=-18853.1543042138\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.183623127916\n",
      "Current iteration=4, loss=18876.273736069088\n",
      "Current iteration=6, loss=5097.799421251032\n",
      "Current iteration=8, loss=-7449.485804286781\n",
      "loss=-19054.79486397998\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34203.91816701804\n",
      "Current iteration=4, loss=18694.65051115434\n",
      "Current iteration=6, loss=4837.559486667837\n",
      "Current iteration=8, loss=-7782.055757818514\n",
      "loss=-19455.430973211915\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=34199.85414470951\n",
      "Current iteration=4, loss=18689.54760610714\n",
      "Current iteration=6, loss=4831.7797289507325\n",
      "Current iteration=8, loss=-7789.175103950996\n",
      "loss=-19464.69329127403\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=34343.6618165581\n",
      "Current iteration=4, loss=18964.26651584635\n",
      "Current iteration=6, loss=5226.703506243236\n",
      "Current iteration=8, loss=-7282.4848725229\n",
      "loss=-18852.773473262627\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=34299.22001699831\n",
      "Current iteration=4, loss=18876.429319190083\n",
      "Current iteration=6, loss=5098.139653582509\n",
      "Current iteration=8, loss=-7448.905565746216\n",
      "loss=-19054.41290374037\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.01322914704\n",
      "Current iteration=4, loss=15032.692929984101\n",
      "Current iteration=6, loss=-96.13539532000055\n",
      "Current iteration=8, loss=-13804.579481067969\n",
      "loss=-26451.49139276253\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.67143723768\n",
      "Current iteration=4, loss=15027.426223745784\n",
      "Current iteration=6, loss=-102.2994424263629\n",
      "Current iteration=8, loss=-13812.628157252531\n",
      "loss=-26462.358064520056\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.744485488107\n",
      "Current iteration=4, loss=15333.526500240305\n",
      "Current iteration=6, loss=335.96244882278177\n",
      "Current iteration=8, loss=-13251.96808447942\n",
      "loss=-25786.63466632268\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.747788841858\n",
      "Current iteration=4, loss=15235.14608201729\n",
      "Current iteration=6, loss=192.7307116394509\n",
      "Current iteration=8, loss=-13436.492826145713\n",
      "loss=-26009.284242512287\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.013241956673\n",
      "Current iteration=4, loss=15032.692984256752\n",
      "Current iteration=6, loss=-96.135277330559\n",
      "Current iteration=8, loss=-13804.579280598264\n",
      "loss=-26451.491261537893\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.671450050173\n",
      "Current iteration=4, loss=15027.426278024912\n",
      "Current iteration=6, loss=-102.29932442566074\n",
      "Current iteration=8, loss=-13812.627956760742\n",
      "loss=-26462.35793327268\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.744498196695\n",
      "Current iteration=4, loss=15333.526554066038\n",
      "Current iteration=6, loss=335.96256581625045\n",
      "Current iteration=8, loss=-13251.967885729111\n",
      "loss=-25786.634536260135\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.7478015826\n",
      "Current iteration=4, loss=15235.146135989997\n",
      "Current iteration=6, loss=192.73082896501467\n",
      "Current iteration=8, loss=-13436.492626820045\n",
      "loss=-26009.28411206424\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.013277600152\n",
      "Current iteration=4, loss=15032.693135273543\n",
      "Current iteration=6, loss=-96.13494901794606\n",
      "Current iteration=8, loss=-13804.578722779428\n",
      "loss=-26451.490896397445\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.671485701765\n",
      "Current iteration=4, loss=15027.42642905978\n",
      "Current iteration=6, loss=-102.29899608165672\n",
      "Current iteration=8, loss=-13812.627398880335\n",
      "loss=-26462.35756806902\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.744533559086\n",
      "Current iteration=4, loss=15333.526703839289\n",
      "Current iteration=6, loss=335.9628913576044\n",
      "Current iteration=8, loss=-13251.967332694647\n",
      "loss=-25786.63417435343\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.747837034447\n",
      "Current iteration=4, loss=15235.146286172305\n",
      "Current iteration=6, loss=192.7311554303356\n",
      "Current iteration=8, loss=-13436.492072184568\n",
      "loss=-26009.28374908487\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.013376780294\n",
      "Current iteration=4, loss=15032.693555486763\n",
      "Current iteration=6, loss=-96.13403546858419\n",
      "Current iteration=8, loss=-13804.577170615368\n",
      "loss=-26451.489880372566\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.67158490436\n",
      "Current iteration=4, loss=15027.42684932329\n",
      "Current iteration=6, loss=-102.29808244499388\n",
      "Current iteration=8, loss=-13812.625846545085\n",
      "loss=-26462.356551868164\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.744631957023\n",
      "Current iteration=4, loss=15333.52712059228\n",
      "Current iteration=6, loss=335.9637971957315\n",
      "Current iteration=8, loss=-13251.96579384339\n",
      "loss=-25786.633167326523\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.74793568131\n",
      "Current iteration=4, loss=15235.1467040635\n",
      "Current iteration=6, loss=192.73206383957694\n",
      "Current iteration=8, loss=-13436.490528878408\n",
      "loss=-26009.282739073227\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.013652754926\n",
      "Current iteration=4, loss=15032.694724754967\n",
      "Current iteration=6, loss=-96.13149346334775\n",
      "Current iteration=8, loss=-13804.572851626894\n",
      "loss=-26451.487053223012\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.67186094155\n",
      "Current iteration=4, loss=15027.428018731427\n",
      "Current iteration=6, loss=-102.29554019684429\n",
      "Current iteration=8, loss=-13812.621527080264\n",
      "loss=-26462.353724229062\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.744905755164\n",
      "Current iteration=4, loss=15333.528280232204\n",
      "Current iteration=6, loss=335.96631774398327\n",
      "Current iteration=8, loss=-13251.961511898608\n",
      "loss=-25786.630365214376\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.748210172118\n",
      "Current iteration=4, loss=15235.147866870577\n",
      "Current iteration=6, loss=192.73459154203226\n",
      "Current iteration=8, loss=-13436.486234537637\n",
      "loss=-26009.27992865591\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.014420670723\n",
      "Current iteration=4, loss=15032.697978313005\n",
      "Current iteration=6, loss=-96.12442018358304\n",
      "Current iteration=8, loss=-13804.56083378691\n",
      "loss=-26451.479186512057\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.672629031385\n",
      "Current iteration=4, loss=15027.431272678778\n",
      "Current iteration=6, loss=-102.28846624111921\n",
      "Current iteration=8, loss=-13812.609507914707\n",
      "loss=-26462.34585615593\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.745667614694\n",
      "Current iteration=4, loss=15333.531506998977\n",
      "Current iteration=6, loss=335.97333131837064\n",
      "Current iteration=8, loss=-13251.949597134842\n",
      "loss=-25786.62256817156\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.748973959035\n",
      "Current iteration=4, loss=15235.151102450023\n",
      "Current iteration=6, loss=192.74162502351732\n",
      "Current iteration=8, loss=-13436.474285281292\n",
      "loss=-26009.272108503428\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.016557441908\n",
      "Current iteration=4, loss=15032.707031529691\n",
      "Current iteration=6, loss=-96.10473836891974\n",
      "Current iteration=8, loss=-13804.527393448452\n",
      "loss=-26451.457296926434\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.674766286815\n",
      "Current iteration=4, loss=15027.440326978825\n",
      "Current iteration=6, loss=-102.26878254553387\n",
      "Current iteration=8, loss=-13812.576063887884\n",
      "loss=-26462.323962779883\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.747787534016\n",
      "Current iteration=4, loss=15333.540485667414\n",
      "Current iteration=6, loss=335.99284699944513\n",
      "Current iteration=8, loss=-13251.916443611979\n",
      "loss=-25786.60087244155\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.751099241363\n",
      "Current iteration=4, loss=15235.160105640303\n",
      "Current iteration=6, loss=192.76119609713467\n",
      "Current iteration=8, loss=-13436.441035780836\n",
      "loss=-26009.25034846952\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.022503133536\n",
      "Current iteration=4, loss=15032.732222629164\n",
      "Current iteration=6, loss=-96.04997260002374\n",
      "Current iteration=8, loss=-13804.434343837878\n",
      "loss=-26451.39638789207\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.680713325906\n",
      "Current iteration=4, loss=15027.465521092825\n",
      "Current iteration=6, loss=-102.21401154286852\n",
      "Current iteration=8, loss=-13812.483004014173\n",
      "loss=-26462.26304319856\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.753686334305\n",
      "Current iteration=4, loss=15333.565469332076\n",
      "Current iteration=6, loss=336.0471504921946\n",
      "Current iteration=8, loss=-13251.824192081724\n",
      "loss=-25786.540502821612\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.757012964645\n",
      "Current iteration=4, loss=15235.185157538383\n",
      "Current iteration=6, loss=192.8156537228164\n",
      "Current iteration=8, loss=-13436.348517187573\n",
      "loss=-26009.189799920176\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.0390473652\n",
      "Current iteration=4, loss=15032.802318253249\n",
      "Current iteration=6, loss=-95.89758397846491\n",
      "Current iteration=8, loss=-13804.17542867939\n",
      "loss=-26451.226905179297\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.697261306963\n",
      "Current iteration=4, loss=15027.535625104969\n",
      "Current iteration=6, loss=-102.06160835806843\n",
      "Current iteration=8, loss=-13812.224060298007\n",
      "loss=-26462.093531138176\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.770100088084\n",
      "Current iteration=4, loss=15333.634987757361\n",
      "Current iteration=6, loss=336.19825280602413\n",
      "Current iteration=8, loss=-13251.56749762141\n",
      "loss=-25786.372521058744\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.773468242453\n",
      "Current iteration=4, loss=15235.254865826933\n",
      "Current iteration=6, loss=192.9671849197149\n",
      "Current iteration=8, loss=-13436.091079611137\n",
      "loss=-26009.021320276428\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32117.08508260685\n",
      "Current iteration=4, loss=15032.99736266591\n",
      "Current iteration=6, loss=-95.47355655691402\n",
      "Current iteration=8, loss=-13803.45498891917\n",
      "loss=-26450.755311735702\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=32112.7433069814\n",
      "Current iteration=4, loss=15027.73069285786\n",
      "Current iteration=6, loss=-101.63754041381677\n",
      "Current iteration=8, loss=-13811.503541075228\n",
      "loss=-26461.621856033744\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=32273.81577226777\n",
      "Current iteration=4, loss=15333.828426086859\n",
      "Current iteration=6, loss=336.6187010250918\n",
      "Current iteration=8, loss=-13250.853237025025\n",
      "loss=-25785.90510407587\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=32223.81925596508\n",
      "Current iteration=4, loss=15235.44883246008\n",
      "Current iteration=6, loss=193.3888265229682\n",
      "Current iteration=8, loss=-13435.374751269987\n",
      "loss=-26008.55251791711\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30056.94252264471\n",
      "Current iteration=4, loss=11459.035083587843\n",
      "Current iteration=6, loss=-4879.704427672945\n",
      "Current iteration=8, loss=-19623.934425714102\n",
      "loss=-33202.10387217038\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.361105092637\n",
      "Current iteration=4, loss=11453.615419709628\n",
      "Current iteration=6, loss=-4886.345680914602\n",
      "Current iteration=8, loss=-19633.08120740988\n",
      "loss=-33214.746950220244\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.54756881196\n",
      "Current iteration=4, loss=11790.530599545893\n",
      "Current iteration=6, loss=-4405.755874600145\n",
      "Current iteration=8, loss=-19019.82932026417\n",
      "loss=-32476.87187218097\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.007197029623\n",
      "Current iteration=4, loss=11681.759933237987\n",
      "Current iteration=6, loss=-4563.286810125175\n",
      "Current iteration=8, loss=-19221.85963719234\n",
      "loss=-32719.709520615506\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30056.942538366067\n",
      "Current iteration=4, loss=11459.035149622723\n",
      "Current iteration=6, loss=-4879.704284879299\n",
      "Current iteration=8, loss=-19623.934183882742\n",
      "loss=-33202.10371457812\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.361120817426\n",
      "Current iteration=4, loss=11453.615485751843\n",
      "Current iteration=6, loss=-4886.345538107091\n",
      "Current iteration=8, loss=-19633.080965549525\n",
      "loss=-33214.74679259773\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.54758440892\n",
      "Current iteration=4, loss=11790.530665034079\n",
      "Current iteration=6, loss=-4405.755733018315\n",
      "Current iteration=8, loss=-19019.82908051824\n",
      "loss=-32476.871715993824\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.0072126663\n",
      "Current iteration=4, loss=11681.75999890655\n",
      "Current iteration=6, loss=-4563.286668138738\n",
      "Current iteration=8, loss=-19221.859396749307\n",
      "loss=-32719.709363966424\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30056.94258211171\n",
      "Current iteration=4, loss=11459.035333368705\n",
      "Current iteration=6, loss=-4879.703887547476\n",
      "Current iteration=8, loss=-19623.933510972605\n",
      "loss=-33202.10327606833\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.36116457256\n",
      "Current iteration=4, loss=11453.615669518205\n",
      "Current iteration=6, loss=-4886.345140736713\n",
      "Current iteration=8, loss=-19633.080292558774\n",
      "loss=-33214.74635400378\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.547627808413\n",
      "Current iteration=4, loss=11790.530847258851\n",
      "Current iteration=6, loss=-4405.7553390584035\n",
      "Current iteration=8, loss=-19019.828413410924\n",
      "loss=-32476.871281393796\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.00725617626\n",
      "Current iteration=4, loss=11681.760181633243\n",
      "Current iteration=6, loss=-4563.286273052991\n",
      "Current iteration=8, loss=-19221.8587277023\n",
      "loss=-32719.708928081025\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30056.94270383653\n",
      "Current iteration=4, loss=11459.035844652837\n",
      "Current iteration=6, loss=-4879.702781948078\n",
      "Current iteration=8, loss=-19623.931638560236\n",
      "loss=-33202.10205588874\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.36128632382\n",
      "Current iteration=4, loss=11453.616180859037\n",
      "Current iteration=6, loss=-4886.344035030034\n",
      "Current iteration=8, loss=-19633.078419922094\n",
      "loss=-33214.74513359013\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.547748570065\n",
      "Current iteration=4, loss=11790.531354310078\n",
      "Current iteration=6, loss=-4405.754242841627\n",
      "Current iteration=8, loss=-19019.8265571452\n",
      "loss=-32476.87007209343\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.00737724529\n",
      "Current iteration=4, loss=11681.760690081119\n",
      "Current iteration=6, loss=-4563.285173703535\n",
      "Current iteration=8, loss=-19221.856866039365\n",
      "loss=-32719.707715204044\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30056.94304254309\n",
      "Current iteration=4, loss=11459.037267331201\n",
      "Current iteration=6, loss=-4879.699705552247\n",
      "Current iteration=8, loss=-19623.92642846193\n",
      "loss=-33202.09866066667\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.361625103968\n",
      "Current iteration=4, loss=11453.617603695195\n",
      "Current iteration=6, loss=-4886.340958335643\n",
      "Current iteration=8, loss=-19633.073209199607\n",
      "loss=-33214.741737716715\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.548084596576\n",
      "Current iteration=4, loss=11790.532765210206\n",
      "Current iteration=6, loss=-4405.751192553437\n",
      "Current iteration=8, loss=-19019.821391975966\n",
      "loss=-32476.866707143367\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.0077141271\n",
      "Current iteration=4, loss=11681.762104867472\n",
      "Current iteration=6, loss=-4563.282114698489\n",
      "Current iteration=8, loss=-19221.851685851932\n",
      "loss=-32719.704340301934\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30056.943985014124\n",
      "Current iteration=4, loss=11459.041226017982\n",
      "Current iteration=6, loss=-4879.691145299218\n",
      "Current iteration=8, loss=-19623.911931056547\n",
      "loss=-33202.08921326043\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.36256777978\n",
      "Current iteration=4, loss=11453.621562821056\n",
      "Current iteration=6, loss=-4886.332397251869\n",
      "Current iteration=8, loss=-19633.05871005737\n",
      "loss=-33214.73228849807\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.549019610262\n",
      "Current iteration=4, loss=11790.536691123307\n",
      "Current iteration=6, loss=-4405.742704946465\n",
      "Current iteration=8, loss=-19019.807019588265\n",
      "loss=-32476.857343970878\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.0086515207\n",
      "Current iteration=4, loss=11681.766041594206\n",
      "Current iteration=6, loss=-4563.273602836334\n",
      "Current iteration=8, loss=-19221.837271675282\n",
      "loss=-32719.69494943715\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30056.94660749561\n",
      "Current iteration=4, loss=11459.052241296653\n",
      "Current iteration=6, loss=-4879.667325895272\n",
      "Current iteration=8, loss=-19623.87159118529\n",
      "loss=-33202.06292529794\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.365190831086\n",
      "Current iteration=4, loss=11453.632579321502\n",
      "Current iteration=6, loss=-4886.3085755363\n",
      "Current iteration=8, loss=-19633.018365353248\n",
      "loss=-33214.70599549239\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.551621341197\n",
      "Current iteration=4, loss=11790.547615207357\n",
      "Current iteration=6, loss=-4405.719087684466\n",
      "Current iteration=8, loss=-19019.76702758607\n",
      "loss=-32476.83129039372\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.011259873852\n",
      "Current iteration=4, loss=11681.77699576787\n",
      "Current iteration=6, loss=-4563.2499180828345\n",
      "Current iteration=8, loss=-19221.79716339283\n",
      "loss=-32719.668818804574\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30056.953904704635\n",
      "Current iteration=4, loss=11459.082891945307\n",
      "Current iteration=6, loss=-4879.601047055671\n",
      "Current iteration=8, loss=-19623.759343255246\n",
      "loss=-33201.989777531555\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.372489625628\n",
      "Current iteration=4, loss=11453.663233369824\n",
      "Current iteration=6, loss=-4886.24229026447\n",
      "Current iteration=8, loss=-19632.90610397546\n",
      "loss=-33214.632833693104\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.55886081058\n",
      "Current iteration=4, loss=11790.578012101585\n",
      "Current iteration=6, loss=-4405.653371316166\n",
      "Current iteration=8, loss=-19019.655747620825\n",
      "loss=-32476.758794817833\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.01851777002\n",
      "Current iteration=4, loss=11681.807476388212\n",
      "Current iteration=6, loss=-4563.184013915604\n",
      "Current iteration=8, loss=-19221.68555987144\n",
      "loss=-32719.59610881765\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30056.974209610595\n",
      "Current iteration=4, loss=11459.168179051585\n",
      "Current iteration=6, loss=-4879.416622764033\n",
      "Current iteration=8, loss=-19623.447007945113\n",
      "loss=-33201.78623991977\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.39279894341\n",
      "Current iteration=4, loss=11453.748529935801\n",
      "Current iteration=6, loss=-4886.057848074877\n",
      "Current iteration=8, loss=-19632.593731246412\n",
      "loss=-33214.42925703404\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.579005052583\n",
      "Current iteration=4, loss=11790.662593122352\n",
      "Current iteration=6, loss=-4405.470512130038\n",
      "Current iteration=8, loss=-19019.346105719793\n",
      "loss=-32476.55707196125\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.038713285678\n",
      "Current iteration=4, loss=11681.892290381433\n",
      "Current iteration=6, loss=-4563.000632169297\n",
      "Current iteration=8, loss=-19221.375017659597\n",
      "loss=-32719.39378935066\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30057.03070912772\n",
      "Current iteration=4, loss=11459.40549437374\n",
      "Current iteration=6, loss=-4878.903455218235\n",
      "Current iteration=8, loss=-19622.577925860933\n",
      "loss=-33201.21988746984\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=30052.449310736687\n",
      "Current iteration=4, loss=11453.985871579993\n",
      "Current iteration=6, loss=-4885.544630727411\n",
      "Current iteration=8, loss=-19631.724545043133\n",
      "loss=-33213.86279593349\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=30230.635057513493\n",
      "Current iteration=4, loss=11790.897943728869\n",
      "Current iteration=6, loss=-4404.961699549272\n",
      "Current iteration=8, loss=-19018.48451812436\n",
      "loss=-32475.99576914613\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=30175.0949084182\n",
      "Current iteration=4, loss=11682.128289244603\n",
      "Current iteration=6, loss=-4562.490365545012\n",
      "Current iteration=8, loss=-19220.510924922935\n",
      "loss=-32718.830826440397\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28022.97548747236\n",
      "Current iteration=4, loss=7967.850778109765\n",
      "Current iteration=6, loss=-9527.23628231958\n",
      "Current iteration=8, loss=-25263.370339434685\n",
      "loss=-39738.76965242061\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.188750332974\n",
      "Current iteration=4, loss=7962.272280601115\n",
      "Current iteration=6, loss=-9534.4556381555\n",
      "Current iteration=8, loss=-25273.76208861476\n",
      "loss=-39753.307465363396\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.339551251825\n",
      "Current iteration=4, loss=8329.46751107049\n",
      "Current iteration=6, loss=-9012.455487335174\n",
      "Current iteration=8, loss=-24609.132950226194\n",
      "loss=-38954.70659753813\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.269578377553\n",
      "Current iteration=4, loss=8210.464461171901\n",
      "Current iteration=6, loss=-9183.928316622578\n",
      "Current iteration=8, loss=-24828.10517055994\n",
      "loss=-39216.977229458615\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28022.975506384297\n",
      "Current iteration=4, loss=7967.850856894575\n",
      "Current iteration=6, loss=-9527.236112770972\n",
      "Current iteration=8, loss=-25263.37005307155\n",
      "loss=-39738.76946643544\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.188769248834\n",
      "Current iteration=4, loss=7962.272359394126\n",
      "Current iteration=6, loss=-9534.455468589833\n",
      "Current iteration=8, loss=-25273.761802214438\n",
      "loss=-39753.307279339824\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.339570013613\n",
      "Current iteration=4, loss=8329.46758919982\n",
      "Current iteration=6, loss=-9012.455319232478\n",
      "Current iteration=8, loss=-24609.13266634498\n",
      "loss=-38954.70641322247\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.26959718737\n",
      "Current iteration=4, loss=8210.464539518027\n",
      "Current iteration=6, loss=-9183.928148036828\n",
      "Current iteration=8, loss=-24828.104885850833\n",
      "loss=-39216.97704459954\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28022.975559007824\n",
      "Current iteration=4, loss=7967.851076117987\n",
      "Current iteration=6, loss=-9527.23564099199\n",
      "Current iteration=8, loss=-25263.36925624916\n",
      "loss=-39738.76894892059\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.18882188332\n",
      "Current iteration=4, loss=7962.2725786404035\n",
      "Current iteration=6, loss=-9534.45499676334\n",
      "Current iteration=8, loss=-25273.761005288547\n",
      "loss=-39753.30676171816\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.339622219417\n",
      "Current iteration=4, loss=8329.467806599276\n",
      "Current iteration=6, loss=-9012.454851476807\n",
      "Current iteration=8, loss=-24609.131876428706\n",
      "loss=-38954.70590035302\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.26964952685\n",
      "Current iteration=4, loss=8210.464757520842\n",
      "Current iteration=6, loss=-9183.92767893698\n",
      "Current iteration=8, loss=-24828.10409363079\n",
      "loss=-39216.97653021818\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28022.975705435918\n",
      "Current iteration=4, loss=7967.851686120176\n",
      "Current iteration=6, loss=-9527.234328238965\n",
      "Current iteration=8, loss=-25263.367039043533\n",
      "loss=-39738.76750890482\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.18896834193\n",
      "Current iteration=4, loss=7962.27318870621\n",
      "Current iteration=6, loss=-9534.453683878159\n",
      "Current iteration=8, loss=-25273.758787794963\n",
      "loss=-39753.30532140526\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.33976748518\n",
      "Current iteration=4, loss=8329.468411526212\n",
      "Current iteration=6, loss=-9012.4535499188\n",
      "Current iteration=8, loss=-24609.129678439775\n",
      "loss=-38954.70447326344\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.269795164535\n",
      "Current iteration=4, loss=8210.46536412656\n",
      "Current iteration=6, loss=-9183.926373638837\n",
      "Current iteration=8, loss=-24828.101889231555\n",
      "loss=-39216.97509892149\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28022.976112880824\n",
      "Current iteration=4, loss=7967.853383487387\n",
      "Current iteration=6, loss=-9527.23067542583\n",
      "Current iteration=8, loss=-25263.360869537588\n",
      "loss=-39738.763501975525\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.189375871683\n",
      "Current iteration=4, loss=7962.274886250471\n",
      "Current iteration=6, loss=-9534.450030697304\n",
      "Current iteration=8, loss=-25273.752617487815\n",
      "loss=-39753.30131364913\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.340171695727\n",
      "Current iteration=4, loss=8329.470094771274\n",
      "Current iteration=6, loss=-9012.4499282566\n",
      "Current iteration=8, loss=-24609.123562405497\n",
      "loss=-38954.700502302076\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.270200410054\n",
      "Current iteration=4, loss=8210.467052043008\n",
      "Current iteration=6, loss=-9183.922741569384\n",
      "Current iteration=8, loss=-24828.0957553602\n",
      "loss=-39216.971116253575\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28022.977246620387\n",
      "Current iteration=4, loss=7967.858106512063\n",
      "Current iteration=6, loss=-9527.220511257796\n",
      "Current iteration=8, loss=-25263.343702524275\n",
      "loss=-39738.752352457894\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.190509847424\n",
      "Current iteration=4, loss=7962.279609767852\n",
      "Current iteration=6, loss=-9534.43986550604\n",
      "Current iteration=8, loss=-25273.735448245086\n",
      "loss=-39753.29016183075\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.341296435654\n",
      "Current iteration=4, loss=8329.474778500227\n",
      "Current iteration=6, loss=-9012.43985076778\n",
      "Current iteration=8, loss=-24609.106544180184\n",
      "loss=-38954.68945286721\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.271328029714\n",
      "Current iteration=4, loss=8210.47174877033\n",
      "Current iteration=6, loss=-9183.912635121862\n",
      "Current iteration=8, loss=-24828.07868750214\n",
      "loss=-39216.960034244665\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28022.980401317833\n",
      "Current iteration=4, loss=7967.87124860568\n",
      "Current iteration=6, loss=-9527.192228867732\n",
      "Current iteration=8, loss=-25263.295934316666\n",
      "loss=-39738.72132827137\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.193665202016\n",
      "Current iteration=4, loss=7962.292753232359\n",
      "Current iteration=6, loss=-9534.411580268761\n",
      "Current iteration=8, loss=-25273.687673833952\n",
      "loss=-39753.25913124225\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.344426090996\n",
      "Current iteration=4, loss=8329.487811251178\n",
      "Current iteration=6, loss=-9012.411809567679\n",
      "Current iteration=8, loss=-24609.059189983676\n",
      "loss=-38954.658707166906\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.274465698185\n",
      "Current iteration=4, loss=8210.484817689983\n",
      "Current iteration=6, loss=-9183.884513342467\n",
      "Current iteration=8, loss=-24828.03119519977\n",
      "loss=-39216.929197905054\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28022.989179448927\n",
      "Current iteration=4, loss=7967.9078172373265\n",
      "Current iteration=6, loss=-9527.113531524614\n",
      "Current iteration=8, loss=-25263.163016647708\n",
      "loss=-39738.63500169573\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.202445161605\n",
      "Current iteration=4, loss=7962.329325678639\n",
      "Current iteration=6, loss=-9534.332875003214\n",
      "Current iteration=8, loss=-25273.5547389034\n",
      "loss=-39753.17278685283\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.353134540943\n",
      "Current iteration=4, loss=8329.524075630581\n",
      "Current iteration=6, loss=-9012.333783349311\n",
      "Current iteration=8, loss=-24608.92742432351\n",
      "loss=-38954.573155495025\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.283196445092\n",
      "Current iteration=4, loss=8210.521182710845\n",
      "Current iteration=6, loss=-9183.806262907632\n",
      "Current iteration=8, loss=-24827.899045252314\n",
      "loss=-39216.84339402423\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28023.013605104716\n",
      "Current iteration=4, loss=7968.009571437415\n",
      "Current iteration=6, loss=-9526.894552168827\n",
      "Current iteration=8, loss=-25262.79316693533\n",
      "loss=-39738.39479338475\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.226875905333\n",
      "Current iteration=4, loss=7962.4310904930935\n",
      "Current iteration=6, loss=-9534.113873602759\n",
      "Current iteration=8, loss=-25273.184841159808\n",
      "loss=-39752.93252897393\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.377366304918\n",
      "Current iteration=4, loss=8329.62498323227\n",
      "Current iteration=6, loss=-9012.116671432173\n",
      "Current iteration=8, loss=-24608.560780130098\n",
      "loss=-38954.33510339477\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.307490251693\n",
      "Current iteration=4, loss=8210.622370352963\n",
      "Current iteration=6, loss=-9183.588527096716\n",
      "Current iteration=8, loss=-24827.531331761293\n",
      "loss=-39216.604640139\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28023.08157082364\n",
      "Current iteration=4, loss=7968.292707074924\n",
      "Current iteration=6, loss=-9526.28523433891\n",
      "Current iteration=8, loss=-25261.764050506743\n",
      "loss=-39737.72640346727\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=28018.294855781674\n",
      "Current iteration=4, loss=7962.714255665473\n",
      "Current iteration=6, loss=-9533.504494432926\n",
      "Current iteration=8, loss=-25272.155591083196\n",
      "loss=-39752.26400113185\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=28213.44479250927\n",
      "Current iteration=4, loss=8329.905763171604\n",
      "Current iteration=6, loss=-9011.512549816556\n",
      "Current iteration=8, loss=-24607.540583140555\n",
      "loss=-38953.67271322314\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=28152.37508909303\n",
      "Current iteration=4, loss=8210.903929517355\n",
      "Current iteration=6, loss=-9182.982669474435\n",
      "Current iteration=8, loss=-24826.508159422967\n",
      "loss=-39215.940297220404\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.436544721997\n",
      "Current iteration=4, loss=4553.9302265781225\n",
      "Current iteration=6, loss=-14050.77309603217\n",
      "Current iteration=8, loss=-30742.10812727547\n",
      "loss=-46087.49269940223\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.475074738613\n",
      "Current iteration=4, loss=4548.173968666567\n",
      "Current iteration=6, loss=-14058.671813800145\n",
      "Current iteration=8, loss=-30753.865144352923\n",
      "loss=-46103.99425983151\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.443684428286\n",
      "Current iteration=4, loss=4945.143452827374\n",
      "Current iteration=6, loss=-13496.098235461845\n",
      "Current iteration=8, loss=-30038.93800625658\n",
      "loss=-45245.90583123987\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.86101387894\n",
      "Current iteration=4, loss=4816.068762081632\n",
      "Current iteration=6, loss=-13681.167678124333\n",
      "Current iteration=8, loss=-30274.321156885082\n",
      "loss=-45526.91900723375\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.436567098615\n",
      "Current iteration=4, loss=4553.93031907056\n",
      "Current iteration=6, loss=-14050.772897836743\n",
      "Current iteration=8, loss=-30742.107793291343\n",
      "loss=-46087.49248301524\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.475097119688\n",
      "Current iteration=4, loss=4548.174061168187\n",
      "Current iteration=6, loss=-14058.671615583737\n",
      "Current iteration=8, loss=-30753.864810322102\n",
      "loss=-46103.99404339766\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.4437066267\n",
      "Current iteration=4, loss=4945.143544546717\n",
      "Current iteration=6, loss=-13496.09803896421\n",
      "Current iteration=8, loss=-30038.93767518104\n",
      "loss=-45245.90561680849\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.861036134516\n",
      "Current iteration=4, loss=4816.068854057205\n",
      "Current iteration=6, loss=-13681.167481059512\n",
      "Current iteration=8, loss=-30274.32082484206\n",
      "loss=-45526.91879217269\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.436629362885\n",
      "Current iteration=4, loss=4553.930576436189\n",
      "Current iteration=6, loss=-14050.772346346148\n",
      "Current iteration=8, loss=-30742.106863960667\n",
      "loss=-46087.491880905625\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.47515939642\n",
      "Current iteration=4, loss=4548.174318559455\n",
      "Current iteration=6, loss=-14058.671064034806\n",
      "Current iteration=8, loss=-30753.863880861485\n",
      "loss=-46103.99344115759\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.44376839516\n",
      "Current iteration=4, loss=4945.143799761251\n",
      "Current iteration=6, loss=-13496.097492197863\n",
      "Current iteration=8, loss=-30038.936753943613\n",
      "loss=-45245.905020140366\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.861098062043\n",
      "Current iteration=4, loss=4816.06910998473\n",
      "Current iteration=6, loss=-13681.166932714885\n",
      "Current iteration=8, loss=-30274.319900912615\n",
      "loss=-45526.918193752404\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.43680261693\n",
      "Current iteration=4, loss=4553.931292571328\n",
      "Current iteration=6, loss=-14050.770811790871\n",
      "Current iteration=8, loss=-30742.104278042934\n",
      "loss=-46087.49020549983\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.475332685066\n",
      "Current iteration=4, loss=4548.17503476593\n",
      "Current iteration=6, loss=-14058.669529317205\n",
      "Current iteration=8, loss=-30753.861294582202\n",
      "loss=-46103.99176538878\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.443940269593\n",
      "Current iteration=4, loss=4945.144509910864\n",
      "Current iteration=6, loss=-13496.095970788014\n",
      "Current iteration=8, loss=-30038.934190545933\n",
      "loss=-45245.90335987602\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.86127037906\n",
      "Current iteration=4, loss=4816.069822118285\n",
      "Current iteration=6, loss=-13681.165406913493\n",
      "Current iteration=8, loss=-30274.317330024052\n",
      "loss=-45526.916528612506\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.43728470663\n",
      "Current iteration=4, loss=4553.933285259835\n",
      "Current iteration=6, loss=-14050.766541799914\n",
      "Current iteration=8, loss=-30742.097082573804\n",
      "loss=-46087.48554358387\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.475814871043\n",
      "Current iteration=4, loss=4548.1770276529305\n",
      "Current iteration=6, loss=-14058.665258874515\n",
      "Current iteration=8, loss=-30753.854098107022\n",
      "loss=-46103.98710246276\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.44441852042\n",
      "Current iteration=4, loss=4945.146485944191\n",
      "Current iteration=6, loss=-13496.091737375\n",
      "Current iteration=8, loss=-30038.92705774012\n",
      "loss=-45245.898740092\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.86174986135\n",
      "Current iteration=4, loss=4816.071803672131\n",
      "Current iteration=6, loss=-13681.16116128065\n",
      "Current iteration=8, loss=-30274.31017637458\n",
      "loss=-45526.91189526199\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.438626149782\n",
      "Current iteration=4, loss=4553.938830033501\n",
      "Current iteration=6, loss=-14050.754660298275\n",
      "Current iteration=8, loss=-30742.077060757907\n",
      "loss=-46087.472571527236\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.4771565821\n",
      "Current iteration=4, loss=4548.182572978906\n",
      "Current iteration=6, loss=-14058.653376115955\n",
      "Current iteration=8, loss=-30753.834073491704\n",
      "loss=-46103.97412759551\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.445749281644\n",
      "Current iteration=4, loss=4945.151984373856\n",
      "Current iteration=6, loss=-13496.079957653687\n",
      "Current iteration=8, loss=-30038.907210288548\n",
      "loss=-45245.88588526997\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.863084049288\n",
      "Current iteration=4, loss=4816.0773174628775\n",
      "Current iteration=6, loss=-13681.149347556951\n",
      "Current iteration=8, loss=-30274.290270924288\n",
      "loss=-45526.89900269025\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.4423587947\n",
      "Current iteration=4, loss=4553.954258691483\n",
      "Current iteration=6, loss=-14050.721599328568\n",
      "Current iteration=8, loss=-30742.021348899838\n",
      "loss=-46087.43647601999\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.480889972452\n",
      "Current iteration=4, loss=4548.198003173699\n",
      "Current iteration=6, loss=-14058.620311648774\n",
      "Current iteration=8, loss=-30753.778353844085\n",
      "loss=-46103.93802426757\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.449452203455\n",
      "Current iteration=4, loss=4945.167284076985\n",
      "Current iteration=6, loss=-13496.047179893605\n",
      "Current iteration=8, loss=-30038.851983609322\n",
      "loss=-45245.85011597484\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.86679650615\n",
      "Current iteration=4, loss=4816.09265990913\n",
      "Current iteration=6, loss=-13681.116475183302\n",
      "Current iteration=8, loss=-30274.234882860208\n",
      "loss=-45526.863128354475\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.45274509836\n",
      "Current iteration=4, loss=4553.997189817391\n",
      "Current iteration=6, loss=-14050.629605328117\n",
      "Current iteration=8, loss=-30741.866327608426\n",
      "loss=-46087.33603821173\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.491278350346\n",
      "Current iteration=4, loss=4548.24093857586\n",
      "Current iteration=6, loss=-14058.528307916433\n",
      "Current iteration=8, loss=-30753.62331087795\n",
      "loss=-46103.837564697875\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.459755800865\n",
      "Current iteration=4, loss=4945.209856378586\n",
      "Current iteration=6, loss=-13495.955973939803\n",
      "Current iteration=8, loss=-30038.698312354558\n",
      "loss=-45245.75058587028\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.877126635303\n",
      "Current iteration=4, loss=4816.135351145971\n",
      "Current iteration=6, loss=-13681.025005961896\n",
      "Current iteration=8, loss=-30274.080762543304\n",
      "loss=-45526.76330596834\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.481645585325\n",
      "Current iteration=4, loss=4554.116647982127\n",
      "Current iteration=6, loss=-14050.373627421202\n",
      "Current iteration=8, loss=-30741.434973693926\n",
      "loss=-46087.0565647005\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.52018460899\n",
      "Current iteration=4, loss=4548.360408639521\n",
      "Current iteration=6, loss=-14058.272302929963\n",
      "Current iteration=8, loss=-30753.191896652443\n",
      "loss=-46103.5580306342\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.488426152908\n",
      "Current iteration=4, loss=4945.328316095391\n",
      "Current iteration=6, loss=-13495.702188812138\n",
      "Current iteration=8, loss=-30038.270714979386\n",
      "loss=-45245.47363809208\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.905870813524\n",
      "Current iteration=4, loss=4816.2541418064675\n",
      "Current iteration=6, loss=-13680.77048827886\n",
      "Current iteration=8, loss=-30273.65191563175\n",
      "loss=-45526.485544901116\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26014.562062752775\n",
      "Current iteration=4, loss=4554.449045537366\n",
      "Current iteration=6, loss=-14049.661360379225\n",
      "Current iteration=8, loss=-30740.23472157821\n",
      "loss=-46086.27891813664\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=26009.60061783638\n",
      "Current iteration=4, loss=4548.692839303976\n",
      "Current iteration=6, loss=-14057.559960538461\n",
      "Current iteration=8, loss=-30751.991476720093\n",
      "loss=-46102.78021558076\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=26221.568202957453\n",
      "Current iteration=4, loss=4945.657935425627\n",
      "Current iteration=6, loss=-13494.99602325129\n",
      "Current iteration=8, loss=-30037.08091551914\n",
      "loss=-45244.703019480505\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=26154.985853043363\n",
      "Current iteration=4, loss=4816.584682002012\n",
      "Current iteration=6, loss=-13680.062284358153\n",
      "Current iteration=8, loss=-30272.45863930746\n",
      "loss=-45525.712663278966\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.65059677103\n",
      "Current iteration=4, loss=1212.425214320268\n",
      "Current iteration=6, loss=-18461.025846398323\n",
      "Current iteration=8, loss=-36076.76868085347\n",
      "loss=-52270.3221972552\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.54138839118\n",
      "Current iteration=4, loss=1206.4624116765187\n",
      "Current iteration=6, loss=-18469.700352560067\n",
      "Current iteration=8, loss=-36089.98362401176\n",
      "loss=-52288.813969583716\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.183941554114\n",
      "Current iteration=4, loss=1632.7286156009104\n",
      "Current iteration=6, loss=-17867.317045665783\n",
      "Current iteration=8, loss=-35325.719769534764\n",
      "loss=-51372.315361349545\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.108126928393\n",
      "Current iteration=4, loss=1493.743808991826\n",
      "Current iteration=6, loss=-18065.65081181395\n",
      "Current iteration=8, loss=-35577.01580574143\n",
      "loss=-51671.44345167148\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.65062288203\n",
      "Current iteration=4, loss=1212.4253214512678\n",
      "Current iteration=6, loss=-18461.02561771381\n",
      "Current iteration=8, loss=-36076.76829622483\n",
      "loss=-52270.32194846671\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.541414507203\n",
      "Current iteration=4, loss=1206.4625188178868\n",
      "Current iteration=6, loss=-18469.700123849965\n",
      "Current iteration=8, loss=-36089.983239325724\n",
      "loss=-52288.81372073971\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.183967456578\n",
      "Current iteration=4, loss=1632.7287218327008\n",
      "Current iteration=6, loss=-17867.31681894846\n",
      "Current iteration=8, loss=-35325.71938827122\n",
      "loss=-51372.31511482482\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.108152897912\n",
      "Current iteration=4, loss=1493.7439155221514\n",
      "Current iteration=6, loss=-18065.650584439845\n",
      "Current iteration=8, loss=-35577.01542336251\n",
      "loss=-51671.44320442583\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.650695537493\n",
      "Current iteration=4, loss=1212.425619549649\n",
      "Current iteration=6, loss=-18461.024981385573\n",
      "Current iteration=8, loss=-36076.76722597277\n",
      "loss=-52270.32125619791\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.541487176568\n",
      "Current iteration=4, loss=1206.4628169450305\n",
      "Current iteration=6, loss=-18469.699487450536\n",
      "Current iteration=8, loss=-36089.98216891395\n",
      "loss=-52288.81302831645\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.184039531723\n",
      "Current iteration=4, loss=1632.7290174289617\n",
      "Current iteration=6, loss=-17867.316188094068\n",
      "Current iteration=8, loss=-35325.71832738277\n",
      "loss=-51372.31442885516\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.108225159653\n",
      "Current iteration=4, loss=1493.744211949075\n",
      "Current iteration=6, loss=-18065.649951757925\n",
      "Current iteration=8, loss=-35577.014359370376\n",
      "loss=-51671.44251645014\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.650897705622\n",
      "Current iteration=4, loss=1212.4264490260675\n",
      "Current iteration=6, loss=-18461.023210764444\n",
      "Current iteration=8, loss=-36076.76424793294\n",
      "loss=-52270.319329918886\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.541689383423\n",
      "Current iteration=4, loss=1206.4636465015199\n",
      "Current iteration=6, loss=-18469.697716631345\n",
      "Current iteration=8, loss=-36089.979190429774\n",
      "loss=-52288.81110160779\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.18424008504\n",
      "Current iteration=4, loss=1632.729839943082\n",
      "Current iteration=6, loss=-17867.31443270421\n",
      "Current iteration=8, loss=-35325.71537539775\n",
      "loss=-51372.31252010382\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.108426232237\n",
      "Current iteration=4, loss=1493.745036774611\n",
      "Current iteration=6, loss=-18065.648191282933\n",
      "Current iteration=8, loss=-35577.01139874918\n",
      "loss=-51671.44060211691\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.65146025047\n",
      "Current iteration=4, loss=1212.428757093363\n",
      "Current iteration=6, loss=-18461.01828390635\n",
      "Current iteration=8, loss=-36076.75596136089\n",
      "loss=-52270.31396993332\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.542252035986\n",
      "Current iteration=4, loss=1206.4659547916922\n",
      "Current iteration=6, loss=-18469.692789222005\n",
      "Current iteration=8, loss=-36089.97090262131\n",
      "loss=-52288.80574042659\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.184798136543\n",
      "Current iteration=4, loss=1632.7321286374308\n",
      "Current iteration=6, loss=-17867.309548228\n",
      "Current iteration=8, loss=-35325.70716132472\n",
      "loss=-51372.307208890015\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.10898572864\n",
      "Current iteration=4, loss=1493.7473319005865\n",
      "Current iteration=6, loss=-18065.64329265703\n",
      "Current iteration=8, loss=-35577.003160645596\n",
      "loss=-51671.435275371216\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.65302556482\n",
      "Current iteration=4, loss=1212.4351794270538\n",
      "Current iteration=6, loss=-18461.004574633334\n",
      "Current iteration=8, loss=-36076.73290348748\n",
      "loss=-52270.29905545678\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.543817650065\n",
      "Current iteration=4, loss=1206.4723777454774\n",
      "Current iteration=6, loss=-18469.679078415214\n",
      "Current iteration=8, loss=-36089.9478413074\n",
      "loss=-52288.7908226233\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.186350947988\n",
      "Current iteration=4, loss=1632.738497064738\n",
      "Current iteration=6, loss=-17867.29595688511\n",
      "Current iteration=8, loss=-35325.684305184055\n",
      "loss=-51372.29243012389\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.110542560556\n",
      "Current iteration=4, loss=1493.7537182243275\n",
      "Current iteration=6, loss=-18065.629661941683\n",
      "Current iteration=8, loss=-35576.98023763841\n",
      "loss=-51671.42045338667\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.657381144534\n",
      "Current iteration=4, loss=1212.4530499469786\n",
      "Current iteration=6, loss=-18460.966427784857\n",
      "Current iteration=8, loss=-36076.66874362753\n",
      "loss=-52270.25755505374\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.548174063948\n",
      "Current iteration=4, loss=1206.490249990833\n",
      "Current iteration=6, loss=-18469.640927298875\n",
      "Current iteration=8, loss=-36089.883671874115\n",
      "loss=-52288.74931296313\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.190671737622\n",
      "Current iteration=4, loss=1632.756217586957\n",
      "Current iteration=6, loss=-17867.25813818404\n",
      "Current iteration=8, loss=-35325.62070665712\n",
      "loss=-51372.251307342936\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.11487453744\n",
      "Current iteration=4, loss=1493.77148854431\n",
      "Current iteration=6, loss=-18065.591733684556\n",
      "Current iteration=8, loss=-35576.91645305151\n",
      "loss=-51671.37921034789\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.66950080062\n",
      "Current iteration=4, loss=1212.502775690711\n",
      "Current iteration=6, loss=-18460.86028205278\n",
      "Current iteration=8, loss=-36076.49021533514\n",
      "loss=-52270.14207782336\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.560296040963\n",
      "Current iteration=4, loss=1206.5399805357238\n",
      "Current iteration=6, loss=-18469.534769691247\n",
      "Current iteration=8, loss=-36089.70511694329\n",
      "loss=-52288.63380997446\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.202694588217\n",
      "Current iteration=4, loss=1632.8055259535029\n",
      "Current iteration=6, loss=-17867.15290554035\n",
      "Current iteration=8, loss=-35325.44374030425\n",
      "loss=-51372.13688086744\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.126928517184\n",
      "Current iteration=4, loss=1493.8209354760754\n",
      "Current iteration=6, loss=-18065.48619619505\n",
      "Current iteration=8, loss=-35576.73896897678\n",
      "loss=-51671.26444924818\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.70322443831\n",
      "Current iteration=4, loss=1212.6411402205647\n",
      "Current iteration=6, loss=-18460.56492633024\n",
      "Current iteration=8, loss=-36075.99345230535\n",
      "loss=-52269.820756394154\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.594026136845\n",
      "Current iteration=4, loss=1206.6783584249802\n",
      "Current iteration=6, loss=-18469.2393809244\n",
      "Current iteration=8, loss=-36089.208279790975\n",
      "loss=-52288.31241687136\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.23614885916\n",
      "Current iteration=4, loss=1632.9427291090892\n",
      "Current iteration=6, loss=-17866.860090531245\n",
      "Current iteration=8, loss=-35324.95132344144\n",
      "loss=-51371.81848321788\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.16046940675\n",
      "Current iteration=4, loss=1493.9585241967654\n",
      "Current iteration=6, loss=-18065.192532937315\n",
      "Current iteration=8, loss=-35576.245111529395\n",
      "loss=-51670.94512048914\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24030.7970622679\n",
      "Current iteration=4, loss=1213.026145375646\n",
      "Current iteration=6, loss=-18459.74308986713\n",
      "Current iteration=8, loss=-36074.61119939973\n",
      "loss=-52268.92666680906\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=24025.68788193666\n",
      "Current iteration=4, loss=1207.0634007532117\n",
      "Current iteration=6, loss=-18468.41745251453\n",
      "Current iteration=8, loss=-36087.8258206379\n",
      "loss=-52287.41812785065\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=24254.329237161244\n",
      "Current iteration=4, loss=1633.324502691093\n",
      "Current iteration=6, loss=-17866.045323681894\n",
      "Current iteration=8, loss=-35323.58116382955\n",
      "loss=-51370.93252915988\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=24182.253798729864\n",
      "Current iteration=4, loss=1494.341370630015\n",
      "Current iteration=6, loss=-18064.375405809544\n",
      "Current iteration=8, loss=-35574.8709434618\n",
      "loss=-51670.05657558159\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22070.945397253206\n",
      "Current iteration=4, loss=-2061.1667008049994\n",
      "Current iteration=6, loss=-22767.539323287452\n",
      "Current iteration=8, loss=-41281.79770715506\n",
      "loss=-58306.08517203048\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.7119975458\n",
      "Current iteration=4, loss=-2067.3719417051307\n",
      "Current iteration=6, loss=-22777.077848466946\n",
      "Current iteration=8, loss=-41296.53661662809\n",
      "loss=-58326.558466899434\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22310.887389349788\n",
      "Current iteration=4, loss=-1612.2592854122047\n",
      "Current iteration=6, loss=-22135.5823925117\n",
      "Current iteration=8, loss=-40483.79397634644\n",
      "loss=-57352.5883396214\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.340445429996\n",
      "Current iteration=4, loss=-1760.993489652061\n",
      "Current iteration=6, loss=-22346.861561865262\n",
      "Current iteration=8, loss=-40750.5377566603\n",
      "loss=-57669.26353548435\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22070.945427364048\n",
      "Current iteration=4, loss=-2061.1665781281777\n",
      "Current iteration=6, loss=-22767.539062313845\n",
      "Current iteration=8, loss=-41281.797268912014\n",
      "loss=-58306.084888844736\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.712027662164\n",
      "Current iteration=4, loss=-2067.37181901669\n",
      "Current iteration=6, loss=-22777.07758746242\n",
      "Current iteration=8, loss=-41296.53617831591\n",
      "loss=-58326.55818364971\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22310.88741921937\n",
      "Current iteration=4, loss=-1612.259163769145\n",
      "Current iteration=6, loss=-22135.58213379193\n",
      "Current iteration=8, loss=-40483.793541954794\n",
      "loss=-57352.588059030095\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.340475377332\n",
      "Current iteration=4, loss=-1760.9933676654036\n",
      "Current iteration=6, loss=-22346.86130239391\n",
      "Current iteration=8, loss=-40750.53732099723\n",
      "loss=-57669.263254075966\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22070.9455111492\n",
      "Current iteration=4, loss=-2061.1662367726667\n",
      "Current iteration=6, loss=-22767.538336139365\n",
      "Current iteration=8, loss=-41281.796049474804\n",
      "loss=-58306.08410086369\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.712111462723\n",
      "Current iteration=4, loss=-2067.3714776287757\n",
      "Current iteration=6, loss=-22777.076861201844\n",
      "Current iteration=8, loss=-41296.53495868626\n",
      "loss=-58326.55739549057\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22310.887502333302\n",
      "Current iteration=4, loss=-1612.2588252901244\n",
      "Current iteration=6, loss=-22135.58141388887\n",
      "Current iteration=8, loss=-40483.79233323435\n",
      "loss=-57352.58727826818\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.340558707583\n",
      "Current iteration=4, loss=-1760.9930282302482\n",
      "Current iteration=6, loss=-22346.860580399512\n",
      "Current iteration=8, loss=-40750.53610873898\n",
      "loss=-57669.26247104043\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22070.945744286382\n",
      "Current iteration=4, loss=-2061.1652869306854\n",
      "Current iteration=6, loss=-22767.536315515765\n",
      "Current iteration=8, loss=-41281.79265631836\n",
      "loss=-58306.081908259526\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.712344642725\n",
      "Current iteration=4, loss=-2067.370527696638\n",
      "Current iteration=6, loss=-22777.074840338733\n",
      "Current iteration=8, loss=-41296.531564994504\n",
      "loss=-58326.55520239094\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22310.887733602744\n",
      "Current iteration=4, loss=-1612.2578834521655\n",
      "Current iteration=6, loss=-22135.579410715898\n",
      "Current iteration=8, loss=-40483.78896989797\n",
      "loss=-57352.58510575169\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.34079057896\n",
      "Current iteration=4, loss=-1760.9920837317886\n",
      "Current iteration=6, loss=-22346.85857140725\n",
      "Current iteration=8, loss=-40750.53273555843\n",
      "loss=-57669.26029219755\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22070.946393004473\n",
      "Current iteration=4, loss=-2061.1626439389975\n",
      "Current iteration=6, loss=-22767.53069301088\n",
      "Current iteration=8, loss=-41281.78321465989\n",
      "loss=-58306.07580720856\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.712993479956\n",
      "Current iteration=4, loss=-2067.367884454108\n",
      "Current iteration=6, loss=-22777.069217167464\n",
      "Current iteration=8, loss=-41296.52212184648\n",
      "loss=-58326.5490999613\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22310.888377123643\n",
      "Current iteration=4, loss=-1612.25526273215\n",
      "Current iteration=6, loss=-22135.5738367684\n",
      "Current iteration=8, loss=-40483.779611215534\n",
      "loss=-57352.57906059585\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.341435774902\n",
      "Current iteration=4, loss=-1760.9894556088452\n",
      "Current iteration=6, loss=-22346.852981267304\n",
      "Current iteration=8, loss=-40750.523349484065\n",
      "loss=-57669.2542294381\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22070.948198100967\n",
      "Current iteration=4, loss=-2061.1552896585495\n",
      "Current iteration=6, loss=-22767.51504806001\n",
      "Current iteration=8, loss=-41281.75694269117\n",
      "loss=-58306.05883067397\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.714798908037\n",
      "Current iteration=4, loss=-2067.3605294755816\n",
      "Current iteration=6, loss=-22777.053570362197\n",
      "Current iteration=8, loss=-41296.4958457329\n",
      "loss=-58326.53211959041\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22310.890167758764\n",
      "Current iteration=4, loss=-1612.2479704239167\n",
      "Current iteration=6, loss=-22135.558326931332\n",
      "Current iteration=8, loss=-40483.75357013249\n",
      "loss=-57352.56223959282\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.34323107082\n",
      "Current iteration=4, loss=-1760.9821427014244\n",
      "Current iteration=6, loss=-22346.83742637368\n",
      "Current iteration=8, loss=-40750.49723218134\n",
      "loss=-57669.23735945193\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22070.953220888714\n",
      "Current iteration=4, loss=-2061.1348259425886\n",
      "Current iteration=6, loss=-22767.471515077013\n",
      "Current iteration=8, loss=-41281.68383942985\n",
      "loss=-58306.01159247504\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.71982261843\n",
      "Current iteration=4, loss=-2067.3400638172634\n",
      "Current iteration=6, loss=-22777.010032219347\n",
      "Current iteration=8, loss=-41296.422730938226\n",
      "loss=-58326.48487071687\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22310.89515030677\n",
      "Current iteration=4, loss=-1612.2276791493255\n",
      "Current iteration=6, loss=-22135.515169910374\n",
      "Current iteration=8, loss=-40483.6811093238\n",
      "loss=-57352.51543416943\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.3482265878\n",
      "Current iteration=4, loss=-1760.9617941083904\n",
      "Current iteration=6, loss=-22346.794143980303\n",
      "Current iteration=8, loss=-40750.42455928709\n",
      "loss=-57669.19041773012\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22070.96719708988\n",
      "Current iteration=4, loss=-2061.0778844860893\n",
      "Current iteration=6, loss=-22767.350382138033\n",
      "Current iteration=8, loss=-41281.48042566772\n",
      "loss=-58305.880149511926\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.73380138691\n",
      "Current iteration=4, loss=-2067.2831169560864\n",
      "Current iteration=6, loss=-22776.888884922788\n",
      "Current iteration=8, loss=-41296.219285084015\n",
      "loss=-58326.353398051055\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22310.90901453864\n",
      "Current iteration=4, loss=-1612.1712175208463\n",
      "Current iteration=6, loss=-22135.395083106614\n",
      "Current iteration=8, loss=-40483.479483220784\n",
      "loss=-57352.3851954288\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.362126906464\n",
      "Current iteration=4, loss=-1760.905172988137\n",
      "Current iteration=6, loss=-22346.673708320846\n",
      "Current iteration=8, loss=-40750.222343044436\n",
      "loss=-57669.05979973138\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22071.006086668418\n",
      "Current iteration=4, loss=-2060.9194418788647\n",
      "Current iteration=6, loss=-22767.01332387275\n",
      "Current iteration=8, loss=-41280.9144179272\n",
      "loss=-58305.5144026771\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.772698109093\n",
      "Current iteration=4, loss=-2067.12465930999\n",
      "Current iteration=6, loss=-22776.55178670681\n",
      "Current iteration=8, loss=-41295.65318804587\n",
      "loss=-58325.98756856679\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22310.947592556113\n",
      "Current iteration=4, loss=-1612.0141100602907\n",
      "Current iteration=6, loss=-22135.06093576312\n",
      "Current iteration=8, loss=-40482.91844972008\n",
      "loss=-57352.02279940418\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.40080533766\n",
      "Current iteration=4, loss=-1760.7476217333015\n",
      "Current iteration=6, loss=-22346.338590269494\n",
      "Current iteration=8, loss=-40749.65966745424\n",
      "loss=-57668.69634840145\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22071.11429898732\n",
      "Current iteration=4, loss=-2060.4785688383727\n",
      "Current iteration=6, loss=-22766.075449548345\n",
      "Current iteration=8, loss=-41279.33949204676\n",
      "loss=-58304.496698265095\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=22065.880930305648\n",
      "Current iteration=4, loss=-2066.683744423298\n",
      "Current iteration=6, loss=-22775.61380121862\n",
      "Current iteration=8, loss=-41294.07801369292\n",
      "loss=-58324.96963417932\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=22311.054937939767\n",
      "Current iteration=4, loss=-1611.5769521201964\n",
      "Current iteration=6, loss=-22134.13116116213\n",
      "Current iteration=8, loss=-40481.35736474203\n",
      "loss=-57351.014418741564\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=22233.508430127826\n",
      "Current iteration=4, loss=-1760.3092289172557\n",
      "Current iteration=6, loss=-22345.40611464542\n",
      "Current iteration=8, loss=-40748.094013334005\n",
      "loss=-57667.685031311164\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.653678186354\n",
      "Current iteration=4, loss=-5271.021172396593\n",
      "Current iteration=6, loss=-26978.838832014157\n",
      "Current iteration=8, loss=-46369.812764526665\n",
      "loss=-64210.95968145044\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.316345852792\n",
      "Current iteration=4, loss=-5277.509531502912\n",
      "Current iteration=6, loss=-26989.319421953394\n",
      "Current iteration=8, loss=-46386.11727080365\n",
      "loss=-64233.377785711535\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20390.886313816\n",
      "Current iteration=4, loss=-4793.974569440665\n",
      "Current iteration=6, loss=-26309.349823462027\n",
      "Current iteration=8, loss=-45525.66273745534\n",
      "loss=-63202.75461830772\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20307.892521603964\n",
      "Current iteration=4, loss=-4952.299417720612\n",
      "Current iteration=6, loss=-26533.268981940957\n",
      "Current iteration=8, loss=-45807.42164234497\n",
      "loss=-63536.46512391139\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.6537125583\n",
      "Current iteration=4, loss=-5271.021033287926\n",
      "Current iteration=6, loss=-26978.83853698749\n",
      "Current iteration=8, loss=-46369.81226974341\n",
      "loss=-64210.95936187245\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.3163802308\n",
      "Current iteration=4, loss=-5277.509392381069\n",
      "Current iteration=6, loss=-26989.319126889717\n",
      "Current iteration=8, loss=-46386.11677593854\n",
      "loss=-64233.37746606127\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20390.886347911794\n",
      "Current iteration=4, loss=-4793.974431508508\n",
      "Current iteration=6, loss=-26309.349530992924\n",
      "Current iteration=8, loss=-45525.66224703952\n",
      "loss=-63202.754301677574\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20307.892555788927\n",
      "Current iteration=4, loss=-4952.299279397065\n",
      "Current iteration=6, loss=-26533.268688620363\n",
      "Current iteration=8, loss=-45807.4211504938\n",
      "loss=-63536.46480636296\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.653808200266\n",
      "Current iteration=4, loss=-5271.020646209775\n",
      "Current iteration=6, loss=-26978.837716058275\n",
      "Current iteration=8, loss=-46369.81089297963\n",
      "loss=-64210.95847262768\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.316475889635\n",
      "Current iteration=4, loss=-5277.509005266349\n",
      "Current iteration=6, loss=-26989.3183058576\n",
      "Current iteration=8, loss=-46386.115398947215\n",
      "loss=-64233.37657661566\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20390.88644278527\n",
      "Current iteration=4, loss=-4793.974047704173\n",
      "Current iteration=6, loss=-26309.348717180197\n",
      "Current iteration=8, loss=-45525.6608824284\n",
      "loss=-63202.753420635345\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20307.89265091064\n",
      "Current iteration=4, loss=-4952.298894503577\n",
      "Current iteration=6, loss=-26533.2678724384\n",
      "Current iteration=8, loss=-45807.4197818888\n",
      "loss=-63536.463922765586\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.65407432972\n",
      "Current iteration=4, loss=-5271.019569141799\n",
      "Current iteration=6, loss=-26978.835431774092\n",
      "Current iteration=8, loss=-46369.807062052816\n",
      "loss=-64210.95599825136\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.31674206601\n",
      "Current iteration=4, loss=-5277.507928096631\n",
      "Current iteration=6, loss=-26989.316021286875\n",
      "Current iteration=8, loss=-46386.111567387\n",
      "loss=-64233.3741016803\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20390.886706776437\n",
      "Current iteration=4, loss=-4793.972979745774\n",
      "Current iteration=6, loss=-26309.346452698057\n",
      "Current iteration=8, loss=-45525.65708531699\n",
      "loss=-63202.75096908302\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20307.892915592416\n",
      "Current iteration=4, loss=-4952.297823514616\n",
      "Current iteration=6, loss=-26533.265601363717\n",
      "Current iteration=8, loss=-45807.41597366421\n",
      "loss=-63536.4614641035\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.654814850765\n",
      "Current iteration=4, loss=-5271.016572136355\n",
      "Current iteration=6, loss=-26978.82907561802\n",
      "Current iteration=8, loss=-46369.79640227246\n",
      "loss=-64210.949113152565\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.31748271758\n",
      "Current iteration=4, loss=-5277.504930807985\n",
      "Current iteration=6, loss=-26989.30966433375\n",
      "Current iteration=8, loss=-46386.10090584425\n",
      "loss=-64233.36721502609\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20390.88744134749\n",
      "Current iteration=4, loss=-4793.970008088227\n",
      "Current iteration=6, loss=-26309.340151642442\n",
      "Current iteration=8, loss=-45525.646519630085\n",
      "loss=-63202.744147493526\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20307.8936520852\n",
      "Current iteration=4, loss=-4952.29484342434\n",
      "Current iteration=6, loss=-26533.259281963907\n",
      "Current iteration=8, loss=-45807.40537705417\n",
      "loss=-63536.45462273054\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.656875394474\n",
      "Current iteration=4, loss=-5271.008232791604\n",
      "Current iteration=6, loss=-26978.811389239705\n",
      "Current iteration=8, loss=-46369.76674080859\n",
      "loss=-64210.92995495886\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.31954362451\n",
      "Current iteration=4, loss=-5277.496590675286\n",
      "Current iteration=6, loss=-26989.291975737386\n",
      "Current iteration=8, loss=-46386.07123947646\n",
      "loss=-64233.34805250433\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20390.889485335\n",
      "Current iteration=4, loss=-4793.9617392755945\n",
      "Current iteration=6, loss=-26309.32261858432\n",
      "Current iteration=8, loss=-45525.61711998672\n",
      "loss=-63202.72516601814\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20307.895701420035\n",
      "Current iteration=4, loss=-4952.286551147076\n",
      "Current iteration=6, loss=-26533.241697862028\n",
      "Current iteration=8, loss=-45807.3758913653\n",
      "loss=-63536.43558620641\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.662608979088\n",
      "Current iteration=4, loss=-5270.98502807717\n",
      "Current iteration=6, loss=-26978.76217586836\n",
      "Current iteration=8, loss=-46369.68420608687\n",
      "loss=-64210.87664616765\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.325278219792\n",
      "Current iteration=4, loss=-5277.47338376835\n",
      "Current iteration=6, loss=-26989.24275619433\n",
      "Current iteration=8, loss=-46385.98869110922\n",
      "loss=-64233.294731669914\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20390.895172851044\n",
      "Current iteration=4, loss=-4793.93873082084\n",
      "Current iteration=6, loss=-26309.273831835362\n",
      "Current iteration=8, loss=-45525.53531379553\n",
      "loss=-63202.67234895586\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20307.901403815307\n",
      "Current iteration=4, loss=-4952.263477400652\n",
      "Current iteration=6, loss=-26533.192769080877\n",
      "Current iteration=8, loss=-45807.293845747685\n",
      "loss=-63536.38261596794\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.67856301387\n",
      "Current iteration=4, loss=-5270.920459640227\n",
      "Current iteration=6, loss=-26978.62523694831\n",
      "Current iteration=8, loss=-46369.45454881216\n",
      "loss=-64210.728311449646\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.341235066844\n",
      "Current iteration=4, loss=-5277.408809230645\n",
      "Current iteration=6, loss=-26989.105800101053\n",
      "Current iteration=8, loss=-46385.75899586519\n",
      "loss=-64233.146363441076\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20390.910998697324\n",
      "Current iteration=4, loss=-4793.874708487639\n",
      "Current iteration=6, loss=-26309.13808001553\n",
      "Current iteration=8, loss=-45525.30768369593\n",
      "loss=-63202.52538250126\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20307.91727106397\n",
      "Current iteration=4, loss=-4952.1992733897005\n",
      "Current iteration=6, loss=-26533.056622048578\n",
      "Current iteration=8, loss=-45807.06554943124\n",
      "loss=-63536.23522329185\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.722956024794\n",
      "Current iteration=4, loss=-5270.740794588962\n",
      "Current iteration=6, loss=-26978.244197886874\n",
      "Current iteration=8, loss=-46368.81551760681\n",
      "loss=-64210.315562526404\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.385635903076\n",
      "Current iteration=4, loss=-5277.229127203643\n",
      "Current iteration=6, loss=-26988.72471325436\n",
      "Current iteration=8, loss=-46385.11985900845\n",
      "loss=-64232.73352127217\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20390.95503501643\n",
      "Current iteration=4, loss=-4793.69656299852\n",
      "Current iteration=6, loss=-26308.760344116938\n",
      "Current iteration=8, loss=-45524.67429319173\n",
      "loss=-63202.116440839556\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20307.96142258754\n",
      "Current iteration=4, loss=-4952.02062237264\n",
      "Current iteration=6, loss=-26532.67778645243\n",
      "Current iteration=8, loss=-45806.43030514995\n",
      "loss=-63535.82509564706\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20134.846481916233\n",
      "Current iteration=4, loss=-5270.240869472515\n",
      "Current iteration=6, loss=-26977.183946592446\n",
      "Current iteration=8, loss=-46367.03740476747\n",
      "loss=-64209.16707373802\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=20129.509183568745\n",
      "Current iteration=4, loss=-5276.729154851669\n",
      "Current iteration=6, loss=-26987.664328996238\n",
      "Current iteration=8, loss=-46383.34145219238\n",
      "loss=-64231.58477302364\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=20391.077568394026\n",
      "Current iteration=4, loss=-4793.20086612411\n",
      "Current iteration=6, loss=-26307.70928396025\n",
      "Current iteration=8, loss=-45522.911875676065\n",
      "loss=-63200.978545885046\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=20308.08427652762\n",
      "Current iteration=4, loss=-4951.52351884665\n",
      "Current iteration=6, loss=-26531.623666357682\n",
      "Current iteration=8, loss=-45804.66272947368\n",
      "loss=-63534.68390065104\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.115037492058\n",
      "Current iteration=4, loss=-8421.00774718006\n",
      "Current iteration=6, loss=-31102.558799904127\n",
      "Current iteration=8, loss=-51351.88674774582\n",
      "loss=-69998.92716420494\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.690912442402\n",
      "Current iteration=4, loss=-8427.822761776168\n",
      "Current iteration=6, loss=-31114.04832819461\n",
      "Current iteration=8, loss=-51369.77685970363\n",
      "loss=-70023.23176903528\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.520103528634\n",
      "Current iteration=4, loss=-7916.264768780714\n",
      "Current iteration=6, loss=-30396.18891062096\n",
      "Current iteration=8, loss=-50462.29674356097\n",
      "loss=-68936.66958157884\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.105812728074\n",
      "Current iteration=4, loss=-8084.024278079604\n",
      "Current iteration=6, loss=-30632.456207651263\n",
      "Current iteration=8, loss=-50758.66989224528\n",
      "loss=-69286.95535077856\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.115076382495\n",
      "Current iteration=4, loss=-8421.007590772346\n",
      "Current iteration=6, loss=-31102.55846909111\n",
      "Current iteration=8, loss=-51351.88619353272\n",
      "loss=-69998.92680623799\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.690951339435\n",
      "Current iteration=4, loss=-8427.822605353604\n",
      "Current iteration=6, loss=-31114.047997337882\n",
      "Current iteration=8, loss=-51369.77630539531\n",
      "loss=-70023.23141098839\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.52014210571\n",
      "Current iteration=4, loss=-7916.264613700433\n",
      "Current iteration=6, loss=-30396.188582686136\n",
      "Current iteration=8, loss=-50462.29619426106\n",
      "loss=-68936.66922693621\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.105851406563\n",
      "Current iteration=4, loss=-8084.02412255748\n",
      "Current iteration=6, loss=-30632.455878760185\n",
      "Current iteration=8, loss=-50758.669341338435\n",
      "loss=-69286.95499511139\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.11518459747\n",
      "Current iteration=4, loss=-8421.0071555586\n",
      "Current iteration=6, loss=-31102.557548584322\n",
      "Current iteration=8, loss=-51351.88465140185\n",
      "loss=-69998.92581017365\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.6910595727\n",
      "Current iteration=4, loss=-8427.822170098476\n",
      "Current iteration=6, loss=-31114.04707670935\n",
      "Current iteration=8, loss=-51369.774762999456\n",
      "loss=-70023.23041470155\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.520249448688\n",
      "Current iteration=4, loss=-7916.2641821803445\n",
      "Current iteration=6, loss=-30396.18767018797\n",
      "Current iteration=8, loss=-50462.29466580143\n",
      "loss=-68936.66824012205\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.105959031735\n",
      "Current iteration=4, loss=-8084.023689807925\n",
      "Current iteration=6, loss=-30632.454963601285\n",
      "Current iteration=8, loss=-50758.66780840738\n",
      "loss=-69286.95400544629\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.11548571203\n",
      "Current iteration=4, loss=-8421.005944550496\n",
      "Current iteration=6, loss=-31102.55498721959\n",
      "Current iteration=8, loss=-51351.88036033132\n",
      "loss=-69998.92303856557\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.691360738238\n",
      "Current iteration=4, loss=-8427.82095897524\n",
      "Current iteration=6, loss=-31114.04451500579\n",
      "Current iteration=8, loss=-51369.77047119159\n",
      "loss=-70023.22764247432\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.52054813693\n",
      "Current iteration=4, loss=-7916.262981450118\n",
      "Current iteration=6, loss=-30396.18513110776\n",
      "Current iteration=8, loss=-50462.29041277197\n",
      "loss=-68936.66549425319\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.10625850523\n",
      "Current iteration=4, loss=-8084.02248565661\n",
      "Current iteration=6, loss=-30632.452417117318\n",
      "Current iteration=8, loss=-50758.66354293585\n",
      "loss=-69286.95125164452\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.116323581165\n",
      "Current iteration=4, loss=-8421.002574848704\n",
      "Current iteration=6, loss=-31102.547860070546\n",
      "Current iteration=8, loss=-51351.86842017383\n",
      "loss=-69998.91532640182\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.692198749195\n",
      "Current iteration=4, loss=-8427.817588953094\n",
      "Current iteration=6, loss=-31114.037386914024\n",
      "Current iteration=8, loss=-51369.75852898243\n",
      "loss=-70023.2199285878\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.52137925472\n",
      "Current iteration=4, loss=-7916.25964034704\n",
      "Current iteration=6, loss=-30396.17806596679\n",
      "Current iteration=8, loss=-50462.27857846605\n",
      "loss=-68936.65785371025\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.107091807953\n",
      "Current iteration=4, loss=-8084.019135034197\n",
      "Current iteration=6, loss=-30632.44533137496\n",
      "Current iteration=8, loss=-50758.651674009154\n",
      "loss=-69286.94358902788\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.118655001774\n",
      "Current iteration=4, loss=-8420.993198454411\n",
      "Current iteration=6, loss=-31102.52802835918\n",
      "Current iteration=8, loss=-51351.835195986234\n",
      "loss=-69998.89386685137\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.694530564393\n",
      "Current iteration=4, loss=-8427.808211667485\n",
      "Current iteration=6, loss=-31114.017552579433\n",
      "Current iteration=8, loss=-51369.72529908592\n",
      "loss=-70023.19846424353\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.523691889226\n",
      "Current iteration=4, loss=-7916.250343530538\n",
      "Current iteration=6, loss=-30396.15840679647\n",
      "Current iteration=8, loss=-50462.24564881663\n",
      "loss=-68936.63659344893\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.10941052217\n",
      "Current iteration=4, loss=-8084.009811729413\n",
      "Current iteration=6, loss=-30632.42561488013\n",
      "Current iteration=8, loss=-50758.618648025404\n",
      "loss=-69286.92226734513\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.125142317287\n",
      "Current iteration=4, loss=-8420.967108089579\n",
      "Current iteration=6, loss=-31102.472845476597\n",
      "Current iteration=8, loss=-51351.74274778659\n",
      "loss=-69998.83415440237\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.70101897785\n",
      "Current iteration=4, loss=-8427.782118822464\n",
      "Current iteration=6, loss=-31113.96236239761\n",
      "Current iteration=8, loss=-51369.63283500094\n",
      "loss=-70023.13873845554\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.53012693133\n",
      "Current iteration=4, loss=-7916.224474595404\n",
      "Current iteration=6, loss=-30396.103704019373\n",
      "Current iteration=8, loss=-50462.15402018629\n",
      "loss=-68936.57743553362\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.115862481474\n",
      "Current iteration=4, loss=-8083.983869089261\n",
      "Current iteration=6, loss=-30632.37075259417\n",
      "Current iteration=8, loss=-50758.526751339414\n",
      "loss=-69286.86293852108\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.143193652028\n",
      "Current iteration=4, loss=-8420.894510170976\n",
      "Current iteration=6, loss=-31102.319296078345\n",
      "Current iteration=8, loss=-51351.4855057681\n",
      "loss=-69998.66800115912\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.719073367763\n",
      "Current iteration=4, loss=-8427.709514002587\n",
      "Current iteration=6, loss=-31113.808792688906\n",
      "Current iteration=8, loss=-51369.37554878057\n",
      "loss=-70022.97254809567\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.5480328123\n",
      "Current iteration=4, loss=-7916.15249281756\n",
      "Current iteration=6, loss=-30395.95149054083\n",
      "Current iteration=8, loss=-50461.899058662704\n",
      "loss=-68936.41282531116\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.13381543545\n",
      "Current iteration=4, loss=-8083.911682222957\n",
      "Current iteration=6, loss=-30632.218095273656\n",
      "Current iteration=8, loss=-50758.27104393662\n",
      "loss=-69286.6978527355\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.193422516593\n",
      "Current iteration=4, loss=-8420.692502702643\n",
      "Current iteration=6, loss=-31101.89203769627\n",
      "Current iteration=8, loss=-51350.769719127515\n",
      "loss=-69998.20567138479\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.769310733474\n",
      "Current iteration=4, loss=-8427.507487331131\n",
      "Current iteration=6, loss=-31113.3814777919\n",
      "Current iteration=8, loss=-51368.65963914639\n",
      "loss=-70022.51011504239\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.59785694342\n",
      "Current iteration=4, loss=-7915.9521997928005\n",
      "Current iteration=6, loss=-30395.52794941789\n",
      "Current iteration=8, loss=-50461.18961759391\n",
      "loss=-68935.95478906864\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.183770549997\n",
      "Current iteration=4, loss=-8083.710818528912\n",
      "Current iteration=6, loss=-30631.793319139335\n",
      "Current iteration=8, loss=-50757.55952742773\n",
      "loss=-69286.23849321458\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18221.33318695475\n",
      "Current iteration=4, loss=-8420.130409201298\n",
      "Current iteration=6, loss=-31100.70318088645\n",
      "Current iteration=8, loss=-51348.77803556111\n",
      "loss=-69996.91922289928\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=18215.909098826476\n",
      "Current iteration=4, loss=-8426.94534039649\n",
      "Current iteration=6, loss=-31112.192463728123\n",
      "Current iteration=8, loss=-51366.667613348494\n",
      "loss=-70021.22337917851\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=18493.7364951896\n",
      "Current iteration=4, loss=-7915.394876796426\n",
      "Current iteration=6, loss=-30394.349435971104\n",
      "Current iteration=8, loss=-50459.21559064181\n",
      "loss=-68934.68028747186\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=18405.32277326437\n",
      "Current iteration=4, loss=-8083.151907623237\n",
      "Current iteration=6, loss=-30630.611369242975\n",
      "Current iteration=8, loss=-50755.57972554125\n",
      "loss=-69284.96030954916\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.677593898592\n",
      "Current iteration=4, loss=-11514.71115798368\n",
      "Current iteration=6, loss=-35145.55452138075\n",
      "Current iteration=8, loss=-56237.78005348359\n",
      "loss=-75682.13281788115\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.180874182763\n",
      "Current iteration=4, loss=-11521.897648438968\n",
      "Current iteration=6, loss=-35158.10841341204\n",
      "Current iteration=8, loss=-56257.257140584996\n",
      "loss=-75708.24957398017\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.136897891003\n",
      "Current iteration=4, loss=-10982.692340470474\n",
      "Current iteration=6, loss=-34402.89503455508\n",
      "Current iteration=8, loss=-55303.36613832042\n",
      "loss=-74566.37138201787\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.33032979878\n",
      "Current iteration=4, loss=-11159.733828897199\n",
      "Current iteration=6, loss=-34651.2322098863\n",
      "Current iteration=8, loss=-55613.98323565471\n",
      "loss=-74932.81970195484\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.677637561159\n",
      "Current iteration=4, loss=-11514.710983426641\n",
      "Current iteration=6, loss=-35145.55415307454\n",
      "Current iteration=8, loss=-56237.779436980905\n",
      "loss=-75682.1324195256\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.180917852436\n",
      "Current iteration=4, loss=-11521.897473865087\n",
      "Current iteration=6, loss=-35158.10804505465\n",
      "Current iteration=8, loss=-56257.25652397302\n",
      "loss=-75708.2491755374\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.136941200755\n",
      "Current iteration=4, loss=-10982.692167399771\n",
      "Current iteration=6, loss=-34402.894669464484\n",
      "Current iteration=8, loss=-55303.365527306516\n",
      "loss=-74566.37098738652\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.330373222925\n",
      "Current iteration=4, loss=-11159.733655331627\n",
      "Current iteration=6, loss=-34651.23184372997\n",
      "Current iteration=8, loss=-55613.98262285462\n",
      "loss=-74932.81930618732\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.677759054848\n",
      "Current iteration=4, loss=-11514.710497711276\n",
      "Current iteration=6, loss=-35145.55312824066\n",
      "Current iteration=8, loss=-56237.77772152559\n",
      "loss=-75682.13131107748\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.181039365898\n",
      "Current iteration=4, loss=-11521.896988102875\n",
      "Current iteration=6, loss=-35158.10702007838\n",
      "Current iteration=8, loss=-56257.25480821361\n",
      "loss=-75708.24806684669\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.13706171277\n",
      "Current iteration=4, loss=-10982.69168582017\n",
      "Current iteration=6, loss=-34402.89365357822\n",
      "Current iteration=8, loss=-55303.363827124005\n",
      "loss=-74566.3698893013\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.33049405324\n",
      "Current iteration=4, loss=-11159.733172375052\n",
      "Current iteration=6, loss=-34651.23082487814\n",
      "Current iteration=8, loss=-55613.98091770192\n",
      "loss=-74932.81820494082\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.678097118305\n",
      "Current iteration=4, loss=-11514.709146179493\n",
      "Current iteration=6, loss=-35145.55027657961\n",
      "Current iteration=8, loss=-56237.77294816951\n",
      "loss=-75682.12822675504\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.181377484321\n",
      "Current iteration=4, loss=-11521.895636440733\n",
      "Current iteration=6, loss=-35158.10416802111\n",
      "Current iteration=8, loss=-56257.250034011406\n",
      "loss=-75708.24498184913\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.137397044593\n",
      "Current iteration=4, loss=-10982.690345796367\n",
      "Current iteration=6, loss=-34402.89082681443\n",
      "Current iteration=8, loss=-55303.359096265354\n",
      "loss=-74566.366833814\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.33083027075\n",
      "Current iteration=4, loss=-11159.73182851977\n",
      "Current iteration=6, loss=-34651.227989862586\n",
      "Current iteration=8, loss=-55613.97617301346\n",
      "loss=-74932.81514065698\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.679037799888\n",
      "Current iteration=4, loss=-11514.705385462203\n",
      "Current iteration=6, loss=-35145.54234166394\n",
      "Current iteration=8, loss=-56237.759666024125\n",
      "loss=-75682.11964444496\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.18231831887\n",
      "Current iteration=4, loss=-11521.891875360656\n",
      "Current iteration=6, loss=-35158.096232002994\n",
      "Current iteration=8, loss=-56257.23674951169\n",
      "loss=-75708.23639766061\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.13833012528\n",
      "Current iteration=4, loss=-10982.686617100697\n",
      "Current iteration=6, loss=-34402.882961176976\n",
      "Current iteration=8, loss=-55303.34593237165\n",
      "loss=-74566.35833173961\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.331765815943\n",
      "Current iteration=4, loss=-11159.728089162762\n",
      "Current iteration=6, loss=-34651.220101264\n",
      "Current iteration=8, loss=-55613.96297063748\n",
      "loss=-74932.80661410568\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.681655302127\n",
      "Current iteration=4, loss=-11514.694921044365\n",
      "Current iteration=6, loss=-35145.520262294776\n",
      "Current iteration=8, loss=-56237.72270767724\n",
      "loss=-75682.09576366126\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.184936246755\n",
      "Current iteration=4, loss=-11521.881409933309\n",
      "Current iteration=6, loss=-35158.074149566106\n",
      "Current iteration=8, loss=-56257.19978461371\n",
      "loss=-75708.21251164988\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.14092647761\n",
      "Current iteration=4, loss=-10982.676241784884\n",
      "Current iteration=6, loss=-34402.86107457836\n",
      "Current iteration=8, loss=-55303.30930306692\n",
      "loss=-74566.33467421634\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.334369025884\n",
      "Current iteration=4, loss=-11159.717684181132\n",
      "Current iteration=6, loss=-34651.1981507748\n",
      "Current iteration=8, loss=-55613.926234253675\n",
      "loss=-74932.78288847403\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.688938656716\n",
      "Current iteration=4, loss=-11514.665803191032\n",
      "Current iteration=6, loss=-35145.458825176815\n",
      "Current iteration=8, loss=-56237.61986897167\n",
      "loss=-75682.02931399168\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.192220785742\n",
      "Current iteration=4, loss=-11521.85228927106\n",
      "Current iteration=6, loss=-35158.01270391206\n",
      "Current iteration=8, loss=-56257.09692767935\n",
      "loss=-75708.14604743592\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.148150981273\n",
      "Current iteration=4, loss=-10982.647371863262\n",
      "Current iteration=6, loss=-34402.80017385561\n",
      "Current iteration=8, loss=-55303.20737993992\n",
      "loss=-74566.26884578189\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.34161261127\n",
      "Current iteration=4, loss=-11159.688731712617\n",
      "Current iteration=6, loss=-34651.1370722728\n",
      "Current iteration=8, loss=-55613.82401317295\n",
      "loss=-74932.71687052415\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.709205016296\n",
      "Current iteration=4, loss=-11514.584781119325\n",
      "Current iteration=6, loss=-35145.2878730436\n",
      "Current iteration=8, loss=-56237.33371486204\n",
      "loss=-75681.84441406901\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.212490440894\n",
      "Current iteration=4, loss=-11521.771259383333\n",
      "Current iteration=6, loss=-35157.84172802677\n",
      "Current iteration=8, loss=-56256.81072284706\n",
      "loss=-75707.96110704255\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.16825358471\n",
      "Current iteration=4, loss=-10982.567039675494\n",
      "Current iteration=6, loss=-34402.63071427138\n",
      "Current iteration=8, loss=-55302.9237734758\n",
      "loss=-74566.08567448049\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.361768310722\n",
      "Current iteration=4, loss=-11159.608169833406\n",
      "Current iteration=6, loss=-34650.96711800805\n",
      "Current iteration=8, loss=-55613.53957763681\n",
      "loss=-74932.53317188536\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.765597309966\n",
      "Current iteration=4, loss=-11514.359333048515\n",
      "Current iteration=6, loss=-35144.81219087772\n",
      "Current iteration=8, loss=-56236.53747945363\n",
      "loss=-75681.32992083266\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.268891904678\n",
      "Current iteration=4, loss=-11521.545789564021\n",
      "Current iteration=6, loss=-35157.36597976961\n",
      "Current iteration=8, loss=-56256.01434630075\n",
      "loss=-75707.44650119438\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.22419021757\n",
      "Current iteration=4, loss=-10982.343511242061\n",
      "Current iteration=6, loss=-34402.15918519155\n",
      "Current iteration=8, loss=-55302.13462699382\n",
      "loss=-74565.5759912172\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.417852686212\n",
      "Current iteration=4, loss=-11159.38400227183\n",
      "Current iteration=6, loss=-34650.494212456855\n",
      "Current iteration=8, loss=-55612.74812422805\n",
      "loss=-74932.02202127929\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16329.922511784884\n",
      "Current iteration=4, loss=-11513.732015482785\n",
      "Current iteration=6, loss=-35143.488594800256\n",
      "Current iteration=8, loss=-56234.32194894002\n",
      "loss=-75679.89832663737\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=16324.425831895904\n",
      "Current iteration=4, loss=-11520.918411482484\n",
      "Current iteration=6, loss=-35156.04219979175\n",
      "Current iteration=8, loss=-56253.79842306887\n",
      "loss=-75706.01459365143\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=16618.379836792737\n",
      "Current iteration=4, loss=-10981.721535138407\n",
      "Current iteration=6, loss=-34400.84714516636\n",
      "Current iteration=8, loss=-55299.93882146252\n",
      "loss=-74564.15778091527\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=16524.573910362975\n",
      "Current iteration=4, loss=-11158.760247770264\n",
      "Current iteration=6, loss=-34649.17834236914\n",
      "Current iteration=8, loss=-55610.545899655735\n",
      "loss=-74930.5997280475\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.699419184715\n",
      "Current iteration=4, loss=-14555.452463026078\n",
      "Current iteration=6, loss=-39113.998742516036\n",
      "Current iteration=8, loss=-61036.13151882361\n",
      "loss=-81271.17547889093\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.141542411635\n",
      "Current iteration=4, loss=-14563.05527169488\n",
      "Current iteration=6, loss=-39127.66118967474\n",
      "Current iteration=8, loss=-61057.18124967995\n",
      "loss=-81299.01865751536\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.095010741192\n",
      "Current iteration=4, loss=-13996.556148279791\n",
      "Current iteration=6, loss=-38335.58582860882\n",
      "Current iteration=8, loss=-60057.43021636443\n",
      "loss=-80102.3680579885\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14664.92606405837\n",
      "Current iteration=4, loss=-14182.73056013484\n",
      "Current iteration=6, loss=-38595.72820151925\n",
      "Current iteration=8, loss=-60381.95010946363\n",
      "loss=-80484.60919487033\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.699467869475\n",
      "Current iteration=4, loss=-14555.452269484502\n",
      "Current iteration=6, loss=-39113.99833503244\n",
      "Current iteration=8, loss=-61036.1308371964\n",
      "loss=-81271.17503814335\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.14159110399\n",
      "Current iteration=4, loss=-14563.055078134203\n",
      "Current iteration=6, loss=-39127.66078213189\n",
      "Current iteration=8, loss=-61057.18056792893\n",
      "loss=-81299.01821667398\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.095059031479\n",
      "Current iteration=4, loss=-13996.555956391272\n",
      "Current iteration=6, loss=-38335.58542469501\n",
      "Current iteration=8, loss=-60057.42954083144\n",
      "loss=-80102.36762138858\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14664.926112476778\n",
      "Current iteration=4, loss=-14182.730367695938\n",
      "Current iteration=6, loss=-38595.727796425526\n",
      "Current iteration=8, loss=-60381.94943195766\n",
      "loss=-80484.60875701735\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.699603337667\n",
      "Current iteration=4, loss=-14555.451730943516\n",
      "Current iteration=6, loss=-39113.997201185055\n",
      "Current iteration=8, loss=-61036.12894052824\n",
      "loss=-81271.173811737\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.141726593383\n",
      "Current iteration=4, loss=-14563.05453954018\n",
      "Current iteration=6, loss=-39127.659648119785\n",
      "Current iteration=8, loss=-61057.17867091632\n",
      "loss=-81299.01699000665\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.095193402103\n",
      "Current iteration=4, loss=-13996.55542245006\n",
      "Current iteration=6, loss=-38335.584300780836\n",
      "Current iteration=8, loss=-60057.42766112079\n",
      "loss=-80102.36640652336\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14664.926247203926\n",
      "Current iteration=4, loss=-14182.72983222333\n",
      "Current iteration=6, loss=-38595.7266692281\n",
      "Current iteration=8, loss=-60381.94754675716\n",
      "loss=-80484.60753866556\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.699980286017\n",
      "Current iteration=4, loss=-14555.45023242129\n",
      "Current iteration=6, loss=-39113.994046187414\n",
      "Current iteration=8, loss=-61036.123662936676\n",
      "loss=-81271.17039918856\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.142103600687\n",
      "Current iteration=4, loss=-14563.05304087026\n",
      "Current iteration=6, loss=-39127.656492663824\n",
      "Current iteration=8, loss=-61057.173392366116\n",
      "loss=-81299.01357673193\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.095567296383\n",
      "Current iteration=4, loss=-13996.553936726974\n",
      "Current iteration=6, loss=-38335.58117342303\n",
      "Current iteration=8, loss=-60057.42243071444\n",
      "loss=-80102.36302608886\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14664.926622090214\n",
      "Current iteration=4, loss=-14182.728342238941\n",
      "Current iteration=6, loss=-38595.72353273454\n",
      "Current iteration=8, loss=-60381.94230107498\n",
      "loss=-80484.60414852931\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.701029167174\n",
      "Current iteration=4, loss=-14555.446062694415\n",
      "Current iteration=6, loss=-39113.98526721984\n",
      "Current iteration=8, loss=-61036.10897772647\n",
      "loss=-81271.16090357049\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.143152645855\n",
      "Current iteration=4, loss=-14563.048870732477\n",
      "Current iteration=6, loss=-39127.647712420876\n",
      "Current iteration=8, loss=-61057.158704488465\n",
      "loss=-81299.00407909292\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.096607679423\n",
      "Current iteration=4, loss=-13996.549802614381\n",
      "Current iteration=6, loss=-38335.5724713649\n",
      "Current iteration=8, loss=-60057.40787679982\n",
      "loss=-80102.3536198296\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14664.927665233516\n",
      "Current iteration=4, loss=-14182.724196269122\n",
      "Current iteration=6, loss=-38595.71480525556\n",
      "Current iteration=8, loss=-60381.92770465454\n",
      "loss=-80484.59471527454\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.70394774118\n",
      "Current iteration=4, loss=-14555.434460183365\n",
      "Current iteration=6, loss=-39113.96083922689\n",
      "Current iteration=8, loss=-61036.06811527038\n",
      "loss=-81271.1344814536\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.146071676207\n",
      "Current iteration=4, loss=-14563.037267078042\n",
      "Current iteration=6, loss=-39127.6232808792\n",
      "Current iteration=8, loss=-61057.11783461007\n",
      "loss=-81298.97765135285\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.099502606829\n",
      "Current iteration=4, loss=-13996.53829920227\n",
      "Current iteration=6, loss=-38335.54825737695\n",
      "Current iteration=8, loss=-60057.36737968145\n",
      "loss=-80102.32744635898\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14664.930567841537\n",
      "Current iteration=4, loss=-14182.71265986346\n",
      "Current iteration=6, loss=-38595.69052053268\n",
      "Current iteration=8, loss=-60381.88708926115\n",
      "loss=-80484.56846668705\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.7120688456\n",
      "Current iteration=4, loss=-14555.402175520001\n",
      "Current iteration=6, loss=-39113.89286693054\n",
      "Current iteration=8, loss=-61035.95441316468\n",
      "loss=-81271.0609603789\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.154194050454\n",
      "Current iteration=4, loss=-14563.004979233123\n",
      "Current iteration=6, loss=-39127.55529870816\n",
      "Current iteration=8, loss=-61057.004111851355\n",
      "loss=-81298.90411463124\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.107557913252\n",
      "Current iteration=4, loss=-13996.506290287394\n",
      "Current iteration=6, loss=-38335.48088056175\n",
      "Current iteration=8, loss=-60057.25469414861\n",
      "loss=-80102.25461715687\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14664.938644519752\n",
      "Current iteration=4, loss=-14182.680559142278\n",
      "Current iteration=6, loss=-38595.62294689346\n",
      "Current iteration=8, loss=-60381.77407462146\n",
      "loss=-80484.49542846813\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.73466629249\n",
      "Current iteration=4, loss=-14555.312341626182\n",
      "Current iteration=6, loss=-39113.703730326924\n",
      "Current iteration=8, loss=-61035.63803112\n",
      "loss=-81270.85638389306\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.176795030771\n",
      "Current iteration=4, loss=-14562.915136486456\n",
      "Current iteration=6, loss=-39127.366134627766\n",
      "Current iteration=8, loss=-61056.6876723386\n",
      "loss=-81298.6994946069\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.129972273395\n",
      "Current iteration=4, loss=-13996.417223679311\n",
      "Current iteration=6, loss=-38335.293400916846\n",
      "Current iteration=8, loss=-60056.94114077094\n",
      "loss=-80102.05196584467\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14664.961118348177\n",
      "Current iteration=4, loss=-14182.591237077848\n",
      "Current iteration=6, loss=-38595.434919575026\n",
      "Current iteration=8, loss=-60381.45960548666\n",
      "loss=-80484.292195555\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.797544964757\n",
      "Current iteration=4, loss=-14555.062374252486\n",
      "Current iteration=6, loss=-39113.17744919236\n",
      "Current iteration=8, loss=-61034.75768562461\n",
      "loss=-81270.28713977293\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.239683534943\n",
      "Current iteration=4, loss=-14562.665144479357\n",
      "Current iteration=6, loss=-39126.839777037814\n",
      "Current iteration=8, loss=-61055.80716693578\n",
      "loss=-81298.13012933855\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.192341496426\n",
      "Current iteration=4, loss=-13996.169391317278\n",
      "Current iteration=6, loss=-38334.77173034444\n",
      "Current iteration=8, loss=-60056.068666152234\n",
      "loss=-80101.48807861289\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14665.023653045111\n",
      "Current iteration=4, loss=-14182.342693895513\n",
      "Current iteration=6, loss=-38594.9117250762\n",
      "Current iteration=8, loss=-60380.584582737545\n",
      "loss=-80483.7266899898\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14459.972508094288\n",
      "Current iteration=4, loss=-14554.366831165897\n",
      "Current iteration=6, loss=-39111.713061578776\n",
      "Current iteration=8, loss=-61032.30812105932\n",
      "loss=-81268.70320048512\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=14454.414674022242\n",
      "Current iteration=4, loss=-14561.969532849525\n",
      "Current iteration=6, loss=-39125.37517668556\n",
      "Current iteration=8, loss=-61053.35715742619\n",
      "loss=-81296.54585294971\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=14764.36588705723\n",
      "Current iteration=4, loss=-13995.479788976436\n",
      "Current iteration=6, loss=-38333.32017170966\n",
      "Current iteration=8, loss=-60053.64100232492\n",
      "loss=-80099.91904501563\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=14665.197659045485\n",
      "Current iteration=4, loss=-14181.651113671694\n",
      "Current iteration=6, loss=-38593.45592608527\n",
      "Current iteration=8, loss=-60378.149828726404\n",
      "loss=-80482.15315332914\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.549759955391\n",
      "Current iteration=4, loss=-17546.309516000285\n",
      "Current iteration=6, loss=-43013.464913517644\n",
      "Current iteration=8, loss=-65754.61627544324\n",
      "loss=-86775.34299026868\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12604.939586681096\n",
      "Current iteration=4, loss=-17554.372517529995\n",
      "Current iteration=6, loss=-43028.2694126197\n",
      "Current iteration=8, loss=-65777.21137458958\n",
      "loss=-86804.81930725317\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12930.764141921789\n",
      "Current iteration=4, loss=-16960.912203984302\n",
      "Current iteration=6, loss=-42199.78419698636\n",
      "Current iteration=8, loss=-64732.094116789835\n",
      "loss=-85553.87049944217\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.264203564395\n",
      "Current iteration=4, loss=-17156.07428965824\n",
      "Current iteration=6, loss=-42471.48059440917\n",
      "Current iteration=8, loss=-65070.20315568072\n",
      "loss=-85951.57354306013\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.54981390897\n",
      "Current iteration=4, loss=-17546.309302652433\n",
      "Current iteration=6, loss=-43013.46446519189\n",
      "Current iteration=8, loss=-65754.61552587719\n",
      "loss=-86775.34250512166\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12604.93964064278\n",
      "Current iteration=4, loss=-17554.3723041606\n",
      "Current iteration=6, loss=-43028.26896422619\n",
      "Current iteration=8, loss=-65777.21062488481\n",
      "loss=-86804.8188220064\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12930.76419543705\n",
      "Current iteration=4, loss=-16960.911992464076\n",
      "Current iteration=6, loss=-42199.78375260137\n",
      "Current iteration=8, loss=-64732.09337395342\n",
      "loss=-85553.8700188901\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.264257222267\n",
      "Current iteration=4, loss=-17156.07407752972\n",
      "Current iteration=6, loss=-42471.48014872552\n",
      "Current iteration=8, loss=-65070.20241067706\n",
      "loss=-85951.57306113256\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.549964037968\n",
      "Current iteration=4, loss=-17546.308708999422\n",
      "Current iteration=6, loss=-43013.463217698896\n",
      "Current iteration=8, loss=-65754.61344016512\n",
      "loss=-86775.34115517128\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12604.939790794377\n",
      "Current iteration=4, loss=-17554.3717104475\n",
      "Current iteration=6, loss=-43028.267716544564\n",
      "Current iteration=8, loss=-65777.2085387868\n",
      "loss=-86804.81747177846\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12930.764344346495\n",
      "Current iteration=4, loss=-16960.91140389652\n",
      "Current iteration=6, loss=-42199.782516073734\n",
      "Current iteration=8, loss=-64732.09130696696\n",
      "loss=-85553.86868172532\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.264406528475\n",
      "Current iteration=4, loss=-17156.07348726948\n",
      "Current iteration=6, loss=-42471.4789085844\n",
      "Current iteration=8, loss=-65070.20033766036\n",
      "loss=-85951.57172014075\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.550381780818\n",
      "Current iteration=4, loss=-17546.30705712462\n",
      "Current iteration=6, loss=-43013.45974647563\n",
      "Current iteration=8, loss=-65754.60763654772\n",
      "loss=-86775.33739885426\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12604.940208600103\n",
      "Current iteration=4, loss=-17554.37005840559\n",
      "Current iteration=6, loss=-43028.26424479651\n",
      "Current iteration=8, loss=-65777.20273409548\n",
      "loss=-86804.81371468914\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12930.764758695826\n",
      "Current iteration=4, loss=-16960.909766172335\n",
      "Current iteration=6, loss=-42199.77907536237\n",
      "Current iteration=8, loss=-64732.08555545471\n",
      "loss=-85553.86496098514\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.264821981867\n",
      "Current iteration=4, loss=-17156.071844835322\n",
      "Current iteration=6, loss=-42471.475457818015\n",
      "Current iteration=8, loss=-65070.19456936842\n",
      "loss=-85951.56798875117\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.551544175141\n",
      "Current iteration=4, loss=-17546.30246068511\n",
      "Current iteration=6, loss=-43013.45008759157\n",
      "Current iteration=8, loss=-65754.5914876395\n",
      "loss=-86775.32694667968\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12604.941371169361\n",
      "Current iteration=4, loss=-17554.365461501104\n",
      "Current iteration=6, loss=-43028.25458445218\n",
      "Current iteration=8, loss=-65777.18658219907\n",
      "loss=-86804.80326036556\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12930.765911647408\n",
      "Current iteration=4, loss=-16960.905209107754\n",
      "Current iteration=6, loss=-42199.76950137933\n",
      "Current iteration=8, loss=-64732.06955153211\n",
      "loss=-85553.85460780516\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.265978005595\n",
      "Current iteration=4, loss=-17156.067274664958\n",
      "Current iteration=6, loss=-42471.46585585661\n",
      "Current iteration=8, loss=-65070.17851875552\n",
      "loss=-85951.55760593877\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.554778606196\n",
      "Current iteration=4, loss=-17546.289670821054\n",
      "Current iteration=6, loss=-43013.423211179695\n",
      "Current iteration=8, loss=-65754.54655235911\n",
      "loss=-86775.29786288828\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12604.944606087167\n",
      "Current iteration=4, loss=-17554.352670343258\n",
      "Current iteration=6, loss=-43028.227703976976\n",
      "Current iteration=8, loss=-65777.1416386037\n",
      "loss=-86804.7741705945\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12930.769119803552\n",
      "Current iteration=4, loss=-16960.89252880677\n",
      "Current iteration=6, loss=-42199.74286120961\n",
      "Current iteration=8, loss=-64732.025019682726\n",
      "loss=-85553.82579947216\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.269194710067\n",
      "Current iteration=4, loss=-17156.05455789637\n",
      "Current iteration=6, loss=-42471.43913783518\n",
      "Current iteration=8, loss=-65070.13385698743\n",
      "loss=-85951.52871515174\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.563778601432\n",
      "Current iteration=4, loss=-17546.254082279633\n",
      "Current iteration=6, loss=-43013.348426019504\n",
      "Current iteration=8, loss=-65754.4215173951\n",
      "loss=-86775.21693555226\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12604.95360743683\n",
      "Current iteration=4, loss=-17554.317078201737\n",
      "Current iteration=6, loss=-43028.152907510375\n",
      "Current iteration=8, loss=-65777.01658050292\n",
      "loss=-86804.69322661964\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12930.778046687336\n",
      "Current iteration=4, loss=-16960.85724513098\n",
      "Current iteration=6, loss=-42199.66873340678\n",
      "Current iteration=8, loss=-64731.901107288315\n",
      "loss=-85553.74563861493\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.278145380147\n",
      "Current iteration=4, loss=-17156.019172747307\n",
      "Current iteration=6, loss=-42471.36479340564\n",
      "Current iteration=8, loss=-65070.00958308702\n",
      "loss=-85951.4483248615\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.588821612844\n",
      "Current iteration=4, loss=-17546.155055165375\n",
      "Current iteration=6, loss=-43013.140332274954\n",
      "Current iteration=8, loss=-65754.073601133\n",
      "loss=-86774.99175075068\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12604.978654216993\n",
      "Current iteration=4, loss=-17554.21804107003\n",
      "Current iteration=6, loss=-43027.94478230499\n",
      "Current iteration=8, loss=-65776.6685998615\n",
      "loss=-86804.4679955196\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12930.802886261756\n",
      "Current iteration=4, loss=-16960.759066322382\n",
      "Current iteration=6, loss=-42199.46246879441\n",
      "Current iteration=8, loss=-64731.556314634036\n",
      "loss=-85553.52258658278\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.303051141353\n",
      "Current iteration=4, loss=-17155.9207115837\n",
      "Current iteration=6, loss=-42471.15792601766\n",
      "Current iteration=8, loss=-65069.66378452351\n",
      "loss=-85951.22463441915\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.658505202198\n",
      "Current iteration=4, loss=-17545.879507237772\n",
      "Current iteration=6, loss=-43012.56130221215\n",
      "Current iteration=8, loss=-65753.10551084173\n",
      "loss=-86774.36516312708\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12605.04834829316\n",
      "Current iteration=4, loss=-17553.942465268403\n",
      "Current iteration=6, loss=-43027.36566470121\n",
      "Current iteration=8, loss=-65775.70033043217\n",
      "loss=-86803.84127906796\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12930.872003776312\n",
      "Current iteration=4, loss=-16960.48587884804\n",
      "Current iteration=6, loss=-42198.88852837287\n",
      "Current iteration=8, loss=-64730.59691590016\n",
      "loss=-85552.90193349108\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.372352824357\n",
      "Current iteration=4, loss=-17155.646738442272\n",
      "Current iteration=6, loss=-42470.58230834629\n",
      "Current iteration=8, loss=-65068.701586808595\n",
      "loss=-85950.60220492026\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12610.85240332946\n",
      "Current iteration=4, loss=-17545.11278573168\n",
      "Current iteration=6, loss=-43010.95014100017\n",
      "Current iteration=8, loss=-65750.41179920144\n",
      "loss=-86772.62166491622\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=12605.242275600438\n",
      "Current iteration=4, loss=-17553.17566620199\n",
      "Current iteration=6, loss=-43025.754259904585\n",
      "Current iteration=8, loss=-65773.006120338\n",
      "loss=-86802.09742238648\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=12931.064326771513\n",
      "Current iteration=4, loss=-16959.72572538384\n",
      "Current iteration=6, loss=-42197.291529174705\n",
      "Current iteration=8, loss=-64727.927388515556\n",
      "loss=-85551.17494826816\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=12826.565188277664\n",
      "Current iteration=4, loss=-17154.884398832255\n",
      "Current iteration=6, loss=-42468.98064217013\n",
      "Current iteration=8, loss=-65066.02427125625\n",
      "loss=-85948.87027679202\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.610062714166\n",
      "Current iteration=4, loss=-20490.13643944419\n",
      "Current iteration=6, loss=-46848.998883107226\n",
      "Current iteration=8, loss=-70400.07700790823\n",
      "loss=-92202.80501504886\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10775.954059624155\n",
      "Current iteration=4, loss=-20498.70178260398\n",
      "Current iteration=6, loss=-46864.96897950303\n",
      "Current iteration=8, loss=-70424.17975851159\n",
      "loss=-92233.8167107122\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.526390961068\n",
      "Current iteration=4, loss=-19878.593346752194\n",
      "Current iteration=6, loss=-46000.48973174214\n",
      "Current iteration=8, loss=-69334.13901242396\n",
      "loss=-90928.98319029847\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11007.72815354675\n",
      "Current iteration=4, loss=-20082.601746762848\n",
      "Current iteration=6, loss=-46283.50233613061\n",
      "Current iteration=8, loss=-69685.5493025147\n",
      "loss=-91341.85217408801\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.610122179929\n",
      "Current iteration=4, loss=-20490.136205480518\n",
      "Current iteration=6, loss=-46848.998392291556\n",
      "Current iteration=8, loss=-70400.07618760616\n",
      "loss=-92202.80448349097\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10775.954119098542\n",
      "Current iteration=4, loss=-20498.70154861593\n",
      "Current iteration=6, loss=-46864.96848861045\n",
      "Current iteration=8, loss=-70424.17893805564\n",
      "loss=-92233.81617904932\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.526449942523\n",
      "Current iteration=4, loss=-19878.59311479841\n",
      "Current iteration=6, loss=-46000.48924525489\n",
      "Current iteration=8, loss=-69334.1381995171\n",
      "loss=-90928.98266380672\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11007.728212686023\n",
      "Current iteration=4, loss=-20082.601514140413\n",
      "Current iteration=6, loss=-46283.50184822151\n",
      "Current iteration=8, loss=-69685.54848723895\n",
      "loss=-91341.85164609287\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.610287646949\n",
      "Current iteration=4, loss=-20490.13555446268\n",
      "Current iteration=6, loss=-46848.99702656778\n",
      "Current iteration=8, loss=-70400.07390506688\n",
      "loss=-92202.80300439964\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10775.95428458955\n",
      "Current iteration=4, loss=-20498.70089753027\n",
      "Current iteration=6, loss=-46864.9671226728\n",
      "Current iteration=8, loss=-70424.17665508833\n",
      "loss=-92233.81469966576\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.526614061893\n",
      "Current iteration=4, loss=-19878.592469373223\n",
      "Current iteration=6, loss=-46000.48789157515\n",
      "Current iteration=8, loss=-69334.13593755529\n",
      "loss=-90928.98119881217\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11007.728377244508\n",
      "Current iteration=4, loss=-20082.600866854686\n",
      "Current iteration=6, loss=-46283.5004905855\n",
      "Current iteration=8, loss=-69685.54621868582\n",
      "loss=-91341.85017691503\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.610748068746\n",
      "Current iteration=4, loss=-20490.133742966947\n",
      "Current iteration=6, loss=-46848.99322636034\n",
      "Current iteration=8, loss=-70400.06755376613\n",
      "loss=-92202.79888874022\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10775.954745078083\n",
      "Current iteration=4, loss=-20498.69908584581\n",
      "Current iteration=6, loss=-46864.963321870215\n",
      "Current iteration=8, loss=-70424.17030259651\n",
      "loss=-92233.81058319329\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.527070733835\n",
      "Current iteration=4, loss=-19878.590673439354\n",
      "Current iteration=6, loss=-46000.48412488105\n",
      "Current iteration=8, loss=-69334.1296435127\n",
      "loss=-90928.97712237795\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11007.728835138278\n",
      "Current iteration=4, loss=-20082.59906574369\n",
      "Current iteration=6, loss=-46283.49671288273\n",
      "Current iteration=8, loss=-69685.53990630213\n",
      "loss=-91341.84608884045\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.612029219667\n",
      "Current iteration=4, loss=-20490.12870237273\n",
      "Current iteration=6, loss=-46848.98265205848\n",
      "Current iteration=8, loss=-70400.0498808969\n",
      "loss=-92202.78743667417\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10775.956026414786\n",
      "Current iteration=4, loss=-20498.694044726446\n",
      "Current iteration=6, loss=-46864.95274591228\n",
      "Current iteration=8, loss=-70424.15262641305\n",
      "loss=-92233.799128865\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.528341450525\n",
      "Current iteration=4, loss=-19878.585676147057\n",
      "Current iteration=6, loss=-46000.47364383188\n",
      "Current iteration=8, loss=-69334.1121299677\n",
      "loss=-90928.96577945838\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11007.730109254879\n",
      "Current iteration=4, loss=-20082.594054045658\n",
      "Current iteration=6, loss=-46283.486201201384\n",
      "Current iteration=8, loss=-69685.52234172201\n",
      "loss=-91341.8347135309\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.61559409805\n",
      "Current iteration=4, loss=-20490.114676622154\n",
      "Current iteration=6, loss=-46848.9532284432\n",
      "Current iteration=8, loss=-70400.00070510684\n",
      "loss=-92202.75557062638\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10775.959591810053\n",
      "Current iteration=4, loss=-20498.680017514653\n",
      "Current iteration=6, loss=-46864.923317688874\n",
      "Current iteration=8, loss=-70424.1034414009\n",
      "loss=-92233.76725652217\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.53187729508\n",
      "Current iteration=4, loss=-19878.57177088652\n",
      "Current iteration=6, loss=-46000.44447969784\n",
      "Current iteration=8, loss=-69334.06339750653\n",
      "loss=-90928.93421711698\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11007.733654559817\n",
      "Current iteration=4, loss=-20082.58010870038\n",
      "Current iteration=6, loss=-46283.456951831344\n",
      "Current iteration=8, loss=-69685.4734672526\n",
      "loss=-91341.80306106248\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.625513582441\n",
      "Current iteration=4, loss=-20490.07564915537\n",
      "Current iteration=6, loss=-46848.87135554633\n",
      "Current iteration=8, loss=-70399.8638706918\n",
      "loss=-92202.66690150311\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10775.96951273269\n",
      "Current iteration=4, loss=-20498.640985981892\n",
      "Current iteration=6, loss=-46864.841431969704\n",
      "Current iteration=8, loss=-70423.96658132494\n",
      "loss=-92233.67856988273\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.541715991094\n",
      "Current iteration=4, loss=-19878.53307869035\n",
      "Current iteration=6, loss=-46000.36332882234\n",
      "Current iteration=8, loss=-69333.92779667924\n",
      "loss=-90928.84639307446\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11007.7435195799\n",
      "Current iteration=4, loss=-20082.541304966104\n",
      "Current iteration=6, loss=-46283.375563781985\n",
      "Current iteration=8, loss=-69685.33747127946\n",
      "loss=-91341.71498623629\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.653115125353\n",
      "Current iteration=4, loss=-20489.96705304526\n",
      "Current iteration=6, loss=-46848.6435398163\n",
      "Current iteration=8, loss=-70399.48312190373\n",
      "loss=-92202.42017477009\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10775.997118277559\n",
      "Current iteration=4, loss=-20498.532378557964\n",
      "Current iteration=6, loss=-46864.61358056084\n",
      "Current iteration=8, loss=-70423.585761134\n",
      "loss=-92233.43179440989\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.569092735599\n",
      "Current iteration=4, loss=-19878.425415489433\n",
      "Current iteration=6, loss=-46000.13752215546\n",
      "Current iteration=8, loss=-69333.55048041225\n",
      "loss=-90928.60201782477\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11007.770969572719\n",
      "Current iteration=4, loss=-20082.433331404034\n",
      "Current iteration=6, loss=-46283.14909716602\n",
      "Current iteration=8, loss=-69684.95905549888\n",
      "loss=-91341.46991316727\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.729917968065\n",
      "Current iteration=4, loss=-20489.66487895922\n",
      "Current iteration=6, loss=-46848.009632644484\n",
      "Current iteration=8, loss=-70398.42367430012\n",
      "loss=-92201.73364584125\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10776.07393225596\n",
      "Current iteration=4, loss=-20498.230172990738\n",
      "Current iteration=6, loss=-46863.97957411112\n",
      "Current iteration=8, loss=-70422.5261148489\n",
      "loss=-92232.74512985969\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.645270064171\n",
      "Current iteration=4, loss=-19878.125837269796\n",
      "Current iteration=6, loss=-45999.50920528898\n",
      "Current iteration=8, loss=-69332.50058392469\n",
      "loss=-90927.92203200786\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11007.847350718735\n",
      "Current iteration=4, loss=-20082.13288958905\n",
      "Current iteration=6, loss=-46282.51894396233\n",
      "Current iteration=8, loss=-69683.9060995737\n",
      "loss=-91340.78798563454\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=10781.943625742906\n",
      "Current iteration=4, loss=-20488.824069676863\n",
      "Current iteration=6, loss=-46846.24577638737\n",
      "Current iteration=8, loss=-70395.47576434881\n",
      "loss=-92199.82336047309\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=10776.287671016387\n",
      "Current iteration=4, loss=-20497.389276111073\n",
      "Current iteration=6, loss=-46862.21544161139\n",
      "Current iteration=8, loss=-70419.57765206436\n",
      "loss=-92230.83446711797\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=11117.85723731423\n",
      "Current iteration=4, loss=-19877.292251070798\n",
      "Current iteration=6, loss=-45997.76090413895\n",
      "Current iteration=8, loss=-69329.5792499191\n",
      "loss=-90926.02995300415\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=11008.059885101278\n",
      "Current iteration=4, loss=-20081.296900407364\n",
      "Current iteration=6, loss=-46280.7655331752\n",
      "Current iteration=8, loss=-69680.97625268994\n",
      "loss=-91338.89050375359\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.274816993082\n",
      "Current iteration=4, loss=-23389.581911741094\n",
      "Current iteration=6, loss=-50625.18066421323\n",
      "Current iteration=8, loss=-74978.6337554712\n",
      "loss=-97560.77230380697\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.57723803267\n",
      "Current iteration=4, loss=-23398.689455920103\n",
      "Current iteration=6, loss=-50642.33081013418\n",
      "Current iteration=8, loss=-75004.19821829518\n",
      "loss=-97593.21958287529\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9323.777087968747\n",
      "Current iteration=4, loss=-22752.227678713556\n",
      "Current iteration=6, loss=-49742.2401866563\n",
      "Current iteration=8, loss=-73869.63093817123\n",
      "loss=-96234.86170123419\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9208.714375291835\n",
      "Current iteration=4, loss=-22964.94494984443\n",
      "Current iteration=6, loss=-50036.34429577481\n",
      "Current iteration=8, loss=-74234.07855640534\n",
      "loss=-96662.63203331707\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.274882211334\n",
      "Current iteration=4, loss=-23389.581656362872\n",
      "Current iteration=6, loss=-50625.18012927437\n",
      "Current iteration=8, loss=-74978.63286165027\n",
      "loss=-97560.77172382303\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.577303260015\n",
      "Current iteration=4, loss=-23398.68920051444\n",
      "Current iteration=6, loss=-50642.33027510892\n",
      "Current iteration=8, loss=-75004.19732430513\n",
      "loss=-97593.21900278179\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9323.777152654497\n",
      "Current iteration=4, loss=-22752.22742553516\n",
      "Current iteration=6, loss=-49742.23965645025\n",
      "Current iteration=8, loss=-73869.63005244127\n",
      "loss=-96234.86112681165\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9208.714440151336\n",
      "Current iteration=4, loss=-22964.944695934693\n",
      "Current iteration=6, loss=-50036.343764019344\n",
      "Current iteration=8, loss=-74234.07766809748\n",
      "loss=-96662.63145725743\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.27506368495\n",
      "Current iteration=4, loss=-23389.580945757767\n",
      "Current iteration=6, loss=-50625.178640775215\n",
      "Current iteration=8, loss=-74978.63037454053\n",
      "loss=-97560.77010998328\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.577484759018\n",
      "Current iteration=4, loss=-23398.688489832955\n",
      "Current iteration=6, loss=-50642.32878636932\n",
      "Current iteration=8, loss=-75004.19483672488\n",
      "loss=-97593.21738863704\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9323.77733264642\n",
      "Current iteration=4, loss=-22752.226721051222\n",
      "Current iteration=6, loss=-49742.23818112029\n",
      "Current iteration=8, loss=-73869.62758784505\n",
      "loss=-96234.85952844685\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9208.714620626737\n",
      "Current iteration=4, loss=-22964.943989415813\n",
      "Current iteration=6, loss=-50036.34228437812\n",
      "Current iteration=8, loss=-74234.07519632796\n",
      "loss=-96662.62985433728\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.27556864607\n",
      "Current iteration=4, loss=-23389.578968456975\n",
      "Current iteration=6, loss=-50625.17449893805\n",
      "Current iteration=8, loss=-74978.62345401035\n",
      "loss=-97560.76561937833\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.577989790792\n",
      "Current iteration=4, loss=-23398.68651231953\n",
      "Current iteration=6, loss=-50642.32464386314\n",
      "Current iteration=8, loss=-75004.18791488536\n",
      "loss=-97593.21289718375\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9323.77783348464\n",
      "Current iteration=4, loss=-22752.22476078287\n",
      "Current iteration=6, loss=-49742.23407592726\n",
      "Current iteration=8, loss=-73869.62072996002\n",
      "loss=-96234.85508090195\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9208.715122810261\n",
      "Current iteration=4, loss=-22964.942023485077\n",
      "Current iteration=6, loss=-50036.338167188704\n",
      "Current iteration=8, loss=-74234.06831848298\n",
      "loss=-96662.6253941168\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.276973730412\n",
      "Current iteration=4, loss=-23389.573466500275\n",
      "Current iteration=6, loss=-50625.16297403116\n",
      "Current iteration=8, loss=-74978.60419722667\n",
      "loss=-97560.75312400423\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.579395071616\n",
      "Current iteration=4, loss=-23398.68100977136\n",
      "Current iteration=6, loss=-50642.31311709478\n",
      "Current iteration=8, loss=-75004.16865445851\n",
      "loss=-97593.2003994489\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9323.779227096658\n",
      "Current iteration=4, loss=-22752.21930622003\n",
      "Current iteration=6, loss=-49742.22265298503\n",
      "Current iteration=8, loss=-73869.60164749026\n",
      "loss=-96234.84270534496\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9208.716520165708\n",
      "Current iteration=4, loss=-22964.93655316634\n",
      "Current iteration=6, loss=-50036.32671086573\n",
      "Current iteration=8, loss=-74234.04918047339\n",
      "loss=-96662.6129832892\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.280883460786\n",
      "Current iteration=4, loss=-23389.558156981628\n",
      "Current iteration=6, loss=-50625.130905302256\n",
      "Current iteration=8, loss=-74978.55061410334\n",
      "loss=-97560.71835489076\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.583305348784\n",
      "Current iteration=4, loss=-23398.665698606717\n",
      "Current iteration=6, loss=-50642.281043186085\n",
      "Current iteration=8, loss=-75004.11506119753\n",
      "loss=-97593.16562376646\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9323.78310490471\n",
      "Current iteration=4, loss=-22752.204128577512\n",
      "Current iteration=6, loss=-49742.190867978505\n",
      "Current iteration=8, loss=-73869.54854940543\n",
      "loss=-96234.80826962966\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9208.72040839006\n",
      "Current iteration=4, loss=-22964.92133168206\n",
      "Current iteration=6, loss=-50036.29483297539\n",
      "Current iteration=8, loss=-74233.99592784578\n",
      "loss=-96662.57844943138\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.291762516094\n",
      "Current iteration=4, loss=-23389.515557356455\n",
      "Current iteration=6, loss=-50625.04167222852\n",
      "Current iteration=8, loss=-74978.40151604253\n",
      "loss=-97560.62160782208\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.594185925549\n",
      "Current iteration=4, loss=-23398.62309440158\n",
      "Current iteration=6, loss=-50642.1917956993\n",
      "Current iteration=8, loss=-75003.96593492838\n",
      "loss=-97593.06885841937\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9323.793895134222\n",
      "Current iteration=4, loss=-22752.161895905392\n",
      "Current iteration=6, loss=-49742.10242437856\n",
      "Current iteration=8, loss=-73869.4008009917\n",
      "loss=-96234.71245026069\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9208.731227603546\n",
      "Current iteration=4, loss=-22964.878977017706\n",
      "Current iteration=6, loss=-50036.206130921\n",
      "Current iteration=8, loss=-74233.84774940803\n",
      "loss=-96662.48235697518\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.322034120285\n",
      "Current iteration=4, loss=-23389.39702152342\n",
      "Current iteration=6, loss=-50624.79337643788\n",
      "Current iteration=8, loss=-74977.98664309298\n",
      "loss=-97560.35240378299\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.62446176326\n",
      "Current iteration=4, loss=-23398.504545824388\n",
      "Current iteration=6, loss=-50641.94345980359\n",
      "Current iteration=8, loss=-75003.55098348758\n",
      "loss=-97592.79960351945\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9323.823919575541\n",
      "Current iteration=4, loss=-22752.04438113959\n",
      "Current iteration=6, loss=-49741.85632534104\n",
      "Current iteration=8, loss=-73868.98968350382\n",
      "loss=-96234.44582759659\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9208.761332694381\n",
      "Current iteration=4, loss=-22964.761122801705\n",
      "Current iteration=6, loss=-50035.95931271985\n",
      "Current iteration=8, loss=-74233.43543535635\n",
      "loss=-96662.21497443097\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.406266553173\n",
      "Current iteration=4, loss=-23389.067189722267\n",
      "Current iteration=6, loss=-50624.1024828141\n",
      "Current iteration=8, loss=-74976.83224423318\n",
      "loss=-97559.6033308545\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.708705976182\n",
      "Current iteration=4, loss=-23398.17467856216\n",
      "Current iteration=6, loss=-50641.25245458577\n",
      "Current iteration=8, loss=-75002.39636622275\n",
      "loss=-97592.05038906759\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9323.907464263857\n",
      "Current iteration=4, loss=-22751.717390508435\n",
      "Current iteration=6, loss=-49741.17154427603\n",
      "Current iteration=8, loss=-73867.84573434955\n",
      "loss=-96233.70393746215\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9208.845101794574\n",
      "Current iteration=4, loss=-22964.433187633662\n",
      "Current iteration=6, loss=-50035.27253055156\n",
      "Current iteration=8, loss=-74232.28815672029\n",
      "loss=-96661.47096989435\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=8972.640647496415\n",
      "Current iteration=4, loss=-23388.149422400278\n",
      "Current iteration=6, loss=-50622.18006249802\n",
      "Current iteration=8, loss=-74973.62013645053\n",
      "loss=-97557.51901649643\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=8966.94311969789\n",
      "Current iteration=4, loss=-23397.256812568747\n",
      "Current iteration=6, loss=-50639.32972375724\n",
      "Current iteration=8, loss=-74999.1836507262\n",
      "loss=-97589.96568091314\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=9324.139931523361\n",
      "Current iteration=4, loss=-22750.807528831217\n",
      "Current iteration=6, loss=-49739.26613223051\n",
      "Current iteration=8, loss=-73864.66270279989\n",
      "loss=-96231.6396093849\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=9209.078193491427\n",
      "Current iteration=4, loss=-22963.520697752403\n",
      "Current iteration=6, loss=-50033.36155040918\n",
      "Current iteration=8, loss=-74229.09586090602\n",
      "loss=-96659.40075843954\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7181.952231808376\n",
      "Current iteration=4, loss=-26247.106176478417\n",
      "Current iteration=6, loss=-54346.17771910545\n",
      "Current iteration=8, loss=-79495.77632087737\n",
      "loss=-102855.62926103473\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.215295722347\n",
      "Current iteration=4, loss=-26256.793089699862\n",
      "Current iteration=6, loss=-54364.514183838895\n",
      "Current iteration=8, loss=-79522.75026645979\n",
      "loss=-102889.41219458578\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7548.925457312287\n",
      "Current iteration=4, loss=-25584.25567398817\n",
      "Current iteration=6, loss=-53429.16447107699\n",
      "Current iteration=8, loss=-78344.01232117855\n",
      "loss=-101477.84374166133\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7428.633058794903\n",
      "Current iteration=4, loss=-25805.548284820245\n",
      "Current iteration=6, loss=-53734.148172926565\n",
      "Current iteration=8, loss=-78721.25554987772\n",
      "loss=-101920.27895550334\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7181.952303016433\n",
      "Current iteration=4, loss=-26247.105898896716\n",
      "Current iteration=6, loss=-54346.17713842279\n",
      "Current iteration=8, loss=-79495.7753507669\n",
      "loss=-102855.62863060602\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.215366940021\n",
      "Current iteration=4, loss=-26256.792812087308\n",
      "Current iteration=6, loss=-54364.51360305992\n",
      "Current iteration=8, loss=-79522.7492961649\n",
      "loss=-102889.41156404365\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7548.925527937511\n",
      "Current iteration=4, loss=-25584.255398803834\n",
      "Current iteration=6, loss=-53429.1638955482\n",
      "Current iteration=8, loss=-78344.01135988513\n",
      "loss=-101477.84311731355\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7428.63312961056\n",
      "Current iteration=4, loss=-25805.548008839607\n",
      "Current iteration=6, loss=-53734.1475957165\n",
      "Current iteration=8, loss=-78721.25458578985\n",
      "loss=-101920.27832937893\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7181.95250115708\n",
      "Current iteration=4, loss=-26247.105126509094\n",
      "Current iteration=6, loss=-54346.175522638805\n",
      "Current iteration=8, loss=-79495.77265137705\n",
      "loss=-102855.62687640081\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.2155651074545\n",
      "Current iteration=4, loss=-26256.792039613934\n",
      "Current iteration=6, loss=-54364.51198700788\n",
      "Current iteration=8, loss=-79522.74659626182\n",
      "loss=-102889.40980952265\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7548.925724456446\n",
      "Current iteration=4, loss=-25584.254633087017\n",
      "Current iteration=6, loss=-53429.16229410523\n",
      "Current iteration=8, loss=-78344.00868502918\n",
      "loss=-101477.84138002877\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7428.633326659344\n",
      "Current iteration=4, loss=-25805.547240907134\n",
      "Current iteration=6, loss=-53734.145989595105\n",
      "Current iteration=8, loss=-78721.25190315822\n",
      "loss=-101920.27658715048\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7181.953052495218\n",
      "Current iteration=4, loss=-26247.10297729475\n",
      "Current iteration=6, loss=-54346.17102662408\n",
      "Current iteration=8, loss=-79495.76514016473\n",
      "loss=-102855.62199522066\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.2161165200405\n",
      "Current iteration=4, loss=-26256.789890160875\n",
      "Current iteration=6, loss=-54364.50749024728\n",
      "Current iteration=8, loss=-79522.73908362158\n",
      "loss=-102889.40492746397\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7548.926271281965\n",
      "Current iteration=4, loss=-25584.252502434596\n",
      "Current iteration=6, loss=-53429.157837995066\n",
      "Current iteration=8, loss=-78344.00124208396\n",
      "loss=-101477.8365459307\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7428.633874959257\n",
      "Current iteration=4, loss=-25805.545104089364\n",
      "Current iteration=6, loss=-53734.14152046718\n",
      "Current iteration=8, loss=-78721.24443857661\n",
      "loss=-101920.27173929662\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7181.954586626254\n",
      "Current iteration=4, loss=-26247.09699697851\n",
      "Current iteration=6, loss=-54346.158516197356\n",
      "Current iteration=8, loss=-79495.74423977341\n",
      "loss=-102855.60841304803\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.217650858294\n",
      "Current iteration=4, loss=-26256.783909180474\n",
      "Current iteration=6, loss=-54364.49497774509\n",
      "Current iteration=8, loss=-79522.71817925695\n",
      "loss=-102889.3913428468\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7548.927792856465\n",
      "Current iteration=4, loss=-25584.24657376799\n",
      "Current iteration=6, loss=-53429.145438605265\n",
      "Current iteration=8, loss=-78343.98053164988\n",
      "loss=-101477.82309476685\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7428.635400636304\n",
      "Current iteration=4, loss=-25805.53915826746\n",
      "Current iteration=6, loss=-53734.129084854525\n",
      "Current iteration=8, loss=-78721.22366793804\n",
      "loss=-101920.25824985618\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7181.9588554367365\n",
      "Current iteration=4, loss=-26247.08035639631\n",
      "Current iteration=6, loss=-54346.12370520202\n",
      "Current iteration=8, loss=-79495.68608321706\n",
      "loss=-102855.57061985406\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.22192024535\n",
      "Current iteration=4, loss=-26256.76726675017\n",
      "Current iteration=6, loss=-54364.46016097477\n",
      "Current iteration=8, loss=-79522.66001164453\n",
      "loss=-102889.35354285053\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7548.932026727603\n",
      "Current iteration=4, loss=-25584.23007690394\n",
      "Current iteration=6, loss=-53429.110936576675\n",
      "Current iteration=8, loss=-78343.92290366052\n",
      "loss=-101477.78566611234\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7428.63964592312\n",
      "Current iteration=4, loss=-25805.5226136677\n",
      "Current iteration=6, loss=-53734.0944820338\n",
      "Current iteration=8, loss=-78721.16587242633\n",
      "loss=-101920.22071469511\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7181.9707336535475\n",
      "Current iteration=4, loss=-26247.034053010553\n",
      "Current iteration=6, loss=-54346.02684161881\n",
      "Current iteration=8, loss=-79495.52425932999\n",
      "loss=-102855.4654581099\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.233800066479\n",
      "Current iteration=4, loss=-26256.720958222\n",
      "Current iteration=6, loss=-54364.36328132225\n",
      "Current iteration=8, loss=-79522.49815699345\n",
      "loss=-102889.2483621788\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7548.943807723607\n",
      "Current iteration=4, loss=-25584.18417342231\n",
      "Current iteration=6, loss=-53429.01493271107\n",
      "Current iteration=8, loss=-78343.76255054087\n",
      "loss=-101477.68151872035\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7428.651458683859\n",
      "Current iteration=4, loss=-25805.476577358728\n",
      "Current iteration=6, loss=-53733.998197708344\n",
      "Current iteration=8, loss=-78721.00505316608\n",
      "loss=-101920.11627094234\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7182.003785482051\n",
      "Current iteration=4, loss=-26246.905211265388\n",
      "Current iteration=6, loss=-54345.75731355742\n",
      "Current iteration=8, loss=-79495.07397618912\n",
      "loss=-102855.17283979873\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.266856359075\n",
      "Current iteration=4, loss=-26256.59210216777\n",
      "Current iteration=6, loss=-54364.093708547174\n",
      "Current iteration=8, loss=-79522.04778824997\n",
      "loss=-102888.95569120036\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7548.97658902952\n",
      "Current iteration=4, loss=-25584.056444432557\n",
      "Current iteration=6, loss=-53428.74779685948\n",
      "Current iteration=8, loss=-78343.31635988441\n",
      "loss=-101477.39172289919\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7428.684328376991\n",
      "Current iteration=4, loss=-25805.348478769494\n",
      "Current iteration=6, loss=-53733.73028146229\n",
      "Current iteration=8, loss=-78720.55756545003\n",
      "loss=-101919.82565048106\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7182.095754038923\n",
      "Current iteration=4, loss=-26246.546702820106\n",
      "Current iteration=6, loss=-54345.007340460514\n",
      "Current iteration=8, loss=-79493.82104773188\n",
      "loss=-102854.35861575013\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.358837337621\n",
      "Current iteration=4, loss=-26256.23355390681\n",
      "Current iteration=6, loss=-54363.343611032455\n",
      "Current iteration=8, loss=-79520.79462160016\n",
      "loss=-102888.14132060227\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7549.067804842276\n",
      "Current iteration=4, loss=-25583.701032283614\n",
      "Current iteration=6, loss=-53428.004480185926\n",
      "Current iteration=8, loss=-78342.07481890565\n",
      "loss=-101476.58535255636\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7428.775790132122\n",
      "Current iteration=4, loss=-25804.992038191802\n",
      "Current iteration=6, loss=-53732.98479330842\n",
      "Current iteration=8, loss=-78719.31241535785\n",
      "loss=-101919.01698553894\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7182.3516610887655\n",
      "Current iteration=4, loss=-26245.54914233638\n",
      "Current iteration=6, loss=-54342.92053250252\n",
      "Current iteration=8, loss=-79490.33478604807\n",
      "loss=-102852.09301826921\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=7176.614778951279\n",
      "Current iteration=4, loss=-26255.235882634977\n",
      "Current iteration=6, loss=-54361.256456879775\n",
      "Current iteration=8, loss=-79517.30769714371\n",
      "loss=-102885.87531533881\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=7549.3216173442\n",
      "Current iteration=4, loss=-25582.712087337273\n",
      "Current iteration=6, loss=-53425.93619379265\n",
      "Current iteration=8, loss=-78338.62024276373\n",
      "loss=-101474.34160816678\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=7429.030286981039\n",
      "Current iteration=4, loss=-25804.000231611244\n",
      "Current iteration=6, loss=-53730.91046474699\n",
      "Current iteration=8, loss=-78715.8477968899\n",
      "loss=-101916.76685637355\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.064760810033\n",
      "Current iteration=4, loss=-29064.996751099494\n",
      "Current iteration=6, loss=-58015.79102403236\n",
      "Current iteration=8, loss=-83956.44250226239\n",
      "loss=-108093.04505899319\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.288823835579\n",
      "Current iteration=4, loss=-29075.297240363714\n",
      "Current iteration=6, loss=-58035.31281129968\n",
      "Current iteration=8, loss=-83984.7690690916\n",
      "loss=-108128.06504167666\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.3951296932755\n",
      "Current iteration=4, loss=-28376.94594689167\n",
      "Current iteration=6, loss=-57065.02843127728\n",
      "Current iteration=8, loss=-82762.17942512243\n",
      "loss=-106663.55898503796\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5666.908644518136\n",
      "Current iteration=4, loss=-28606.684260694252\n",
      "Current iteration=6, loss=-57380.692205087886\n",
      "Current iteration=8, loss=-83151.99704137018\n",
      "loss=-107120.44780197646\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.064838242453\n",
      "Current iteration=4, loss=-29064.996450534145\n",
      "Current iteration=6, loss=-58015.790395996286\n",
      "Current iteration=8, loss=-83956.44145310193\n",
      "loss=-108093.04437609823\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.2889012781225\n",
      "Current iteration=4, loss=-29075.296939763943\n",
      "Current iteration=6, loss=-58035.31218315694\n",
      "Current iteration=8, loss=-83984.7680197314\n",
      "loss=-108128.06435866492\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.395206490446\n",
      "Current iteration=4, loss=-28376.945648928773\n",
      "Current iteration=6, loss=-57065.02780883289\n",
      "Current iteration=8, loss=-82762.17838553555\n",
      "loss=-106663.55830876753\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5666.908721523094\n",
      "Current iteration=4, loss=-28606.683961867882\n",
      "Current iteration=6, loss=-57380.69158082598\n",
      "Current iteration=8, loss=-83151.9959987649\n",
      "loss=-107120.44712378392\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.065053702768\n",
      "Current iteration=4, loss=-29064.995614193227\n",
      "Current iteration=6, loss=-58015.78864844854\n",
      "Current iteration=8, loss=-83956.43853375051\n",
      "loss=-108093.04247590218\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.289116766551\n",
      "Current iteration=4, loss=-29075.296103327037\n",
      "Current iteration=6, loss=-58035.31043531259\n",
      "Current iteration=8, loss=-83984.76509982428\n",
      "loss=-108128.06245814412\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.395420183043\n",
      "Current iteration=4, loss=-28376.944819829336\n",
      "Current iteration=6, loss=-57065.026076844515\n",
      "Current iteration=8, loss=-82762.17549282324\n",
      "loss=-106663.55642700489\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5666.908935793947\n",
      "Current iteration=4, loss=-28606.683130365833\n",
      "Current iteration=6, loss=-57380.68984378017\n",
      "Current iteration=8, loss=-83151.99309765373\n",
      "loss=-107120.44523667304\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.065653233845\n",
      "Current iteration=4, loss=-29064.99328702502\n",
      "Current iteration=6, loss=-58015.78378579336\n",
      "Current iteration=8, loss=-83956.43041048234\n",
      "loss=-108093.03718849405\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.289716375946\n",
      "Current iteration=4, loss=-29075.29377589179\n",
      "Current iteration=6, loss=-58035.30557183208\n",
      "Current iteration=8, loss=-83984.75697500976\n",
      "loss=-108128.05716983239\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.396014795385\n",
      "Current iteration=4, loss=-28376.942512811063\n",
      "Current iteration=6, loss=-57065.02125748416\n",
      "Current iteration=8, loss=-82762.16744367983\n",
      "loss=-106663.55119088871\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5666.90953201532\n",
      "Current iteration=4, loss=-28606.68081666194\n",
      "Current iteration=6, loss=-57380.68501034731\n",
      "Current iteration=8, loss=-83151.98502514006\n",
      "loss=-107120.439985675\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.067321464643\n",
      "Current iteration=4, loss=-29064.986811541632\n",
      "Current iteration=6, loss=-58015.77025516798\n",
      "Current iteration=8, loss=-83956.40780700956\n",
      "loss=-108093.02247596804\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.291384824682\n",
      "Current iteration=4, loss=-29075.28729966538\n",
      "Current iteration=6, loss=-58035.29203891006\n",
      "Current iteration=8, loss=-83984.73436723417\n",
      "loss=-108128.04245479201\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.397669339547\n",
      "Current iteration=4, loss=-28376.93609339605\n",
      "Current iteration=6, loss=-57065.00784732915\n",
      "Current iteration=8, loss=-82762.14504646376\n",
      "loss=-106663.53662108556\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5666.911191036661\n",
      "Current iteration=4, loss=-28606.67437864396\n",
      "Current iteration=6, loss=-57380.671561034666\n",
      "Current iteration=8, loss=-83151.9625628947\n",
      "loss=-107120.4253744621\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.071963415693\n",
      "Current iteration=4, loss=-29064.968793127806\n",
      "Current iteration=6, loss=-58015.73260541074\n",
      "Current iteration=8, loss=-83956.34491153111\n",
      "loss=-108092.9815374997\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.2960273820245\n",
      "Current iteration=4, loss=-29075.269279184035\n",
      "Current iteration=6, loss=-58035.25438276225\n",
      "Current iteration=8, loss=-83984.67145978287\n",
      "loss=-108128.00150932733\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.402273206686\n",
      "Current iteration=4, loss=-28376.91823099573\n",
      "Current iteration=6, loss=-57064.97053278774\n",
      "Current iteration=8, loss=-82762.08272490649\n",
      "loss=-106663.49607975202\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5666.915807361828\n",
      "Current iteration=4, loss=-28606.656464479725\n",
      "Current iteration=6, loss=-57380.63413753482\n",
      "Current iteration=8, loss=-83151.90006038971\n",
      "loss=-107120.38471790355\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.084879917961\n",
      "Current iteration=4, loss=-29064.918655846788\n",
      "Current iteration=6, loss=-58015.627842814465\n",
      "Current iteration=8, loss=-83956.16990133649\n",
      "loss=-108092.86762385175\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.308945571431\n",
      "Current iteration=4, loss=-29075.219136150015\n",
      "Current iteration=6, loss=-58035.1496023838\n",
      "Current iteration=8, loss=-83984.4964162732\n",
      "loss=-108127.8875762116\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.415083738172\n",
      "Current iteration=4, loss=-28376.86852783136\n",
      "Current iteration=6, loss=-57064.866702948544\n",
      "Current iteration=8, loss=-82761.90931167966\n",
      "loss=-106663.38327115432\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5666.928652558521\n",
      "Current iteration=4, loss=-28606.606617279373\n",
      "Current iteration=6, loss=-57380.53000451262\n",
      "Current iteration=8, loss=-83151.72614366584\n",
      "loss=-107120.27158868575\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.12082083519\n",
      "Current iteration=4, loss=-29064.77914608398\n",
      "Current iteration=6, loss=-58015.33633536136\n",
      "Current iteration=8, loss=-83955.68292670282\n",
      "loss=-108092.55065291651\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.344891183213\n",
      "Current iteration=4, loss=-29075.079610379125\n",
      "Current iteration=6, loss=-58034.858045451096\n",
      "Current iteration=8, loss=-83984.00934893865\n",
      "loss=-108127.57055110614\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.450729785598\n",
      "Current iteration=4, loss=-28376.730226022057\n",
      "Current iteration=6, loss=-57064.57779094138\n",
      "Current iteration=8, loss=-82761.4267806888\n",
      "loss=-106663.06937508119\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5666.964395063914\n",
      "Current iteration=4, loss=-28606.467914681947\n",
      "Current iteration=6, loss=-57380.24024888273\n",
      "Current iteration=8, loss=-83151.24221166909\n",
      "loss=-107119.95680046969\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.2208284365215\n",
      "Current iteration=4, loss=-29064.39095342884\n",
      "Current iteration=6, loss=-58014.52520393309\n",
      "Current iteration=8, loss=-83954.32790346883\n",
      "loss=-108091.668666773\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.444911847401\n",
      "Current iteration=4, loss=-29074.691373180776\n",
      "Current iteration=6, loss=-58034.046776343435\n",
      "Current iteration=8, loss=-83982.65406776071\n",
      "loss=-108126.68841423058\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.549916895334\n",
      "Current iteration=4, loss=-28376.345394556018\n",
      "Current iteration=6, loss=-57063.77388144716\n",
      "Current iteration=8, loss=-82760.08412203827\n",
      "loss=-106662.19594487813\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5667.063850573353\n",
      "Current iteration=4, loss=-28606.08196800351\n",
      "Current iteration=6, loss=-57379.4339919735\n",
      "Current iteration=8, loss=-83149.89565467255\n",
      "loss=-107119.08088783812\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5410.499104476341\n",
      "Current iteration=4, loss=-29063.310796284975\n",
      "Current iteration=6, loss=-58012.268224174484\n",
      "Current iteration=8, loss=-83950.55756847355\n",
      "loss=-108089.21452132636\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=5404.723224235164\n",
      "Current iteration=4, loss=-29073.61109209429\n",
      "Current iteration=6, loss=-58031.789413489976\n",
      "Current iteration=8, loss=-83978.88301503504\n",
      "loss=-108124.23384936321\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=5792.825909876883\n",
      "Current iteration=4, loss=-28375.27459001692\n",
      "Current iteration=6, loss=-57061.536996775365\n",
      "Current iteration=8, loss=-82756.34819132814\n",
      "loss=-106659.76560648739\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=5667.340590390236\n",
      "Current iteration=4, loss=-28605.008060353324\n",
      "Current iteration=6, loss=-57377.19057559888\n",
      "Current iteration=8, loss=-83146.14887686286\n",
      "loss=-107116.6436420379\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.0494912681947\n",
      "Current iteration=4, loss=-31845.382857468234\n",
      "Current iteration=6, loss=-61637.49499456129\n",
      "Current iteration=8, loss=-88365.08470087961\n",
      "loss=-113278.0673622393\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.2332139785476\n",
      "Current iteration=4, loss=-31856.32800620087\n",
      "Current iteration=6, loss=-61658.194736531455\n",
      "Current iteration=8, loss=-88394.70380101405\n",
      "loss=-113314.22820658813\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4053.624517972787\n",
      "Current iteration=4, loss=-31132.40971046139\n",
      "Current iteration=6, loss=-60653.274504242254\n",
      "Current iteration=8, loss=-87128.54825573662\n",
      "loss=-111797.02169904695\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3922.9802093607736\n",
      "Current iteration=4, loss=-31370.467965788237\n",
      "Current iteration=6, loss=-60979.43076247942\n",
      "Current iteration=8, loss=-87530.73790096382\n",
      "loss=-112268.17538557247\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.049575156852\n",
      "Current iteration=4, loss=-31845.382533147054\n",
      "Current iteration=6, loss=-61637.49431757166\n",
      "Current iteration=8, loss=-88365.08356991713\n",
      "loss=-113278.06662485372\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.2332978778236\n",
      "Current iteration=4, loss=-31856.327681841245\n",
      "Current iteration=6, loss=-61658.1940594246\n",
      "Current iteration=8, loss=-88394.70266983657\n",
      "loss=-113314.22746908324\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4053.6246011716166\n",
      "Current iteration=4, loss=-31132.409388955308\n",
      "Current iteration=6, loss=-60653.27383329896\n",
      "Current iteration=8, loss=-87128.54713513488\n",
      "loss=-111797.02096885402\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3922.9802927855017\n",
      "Current iteration=4, loss=-31370.467643349297\n",
      "Current iteration=6, loss=-60979.43008957811\n",
      "Current iteration=8, loss=-87530.7367771123\n",
      "loss=-112268.17465330605\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.049808582023\n",
      "Current iteration=4, loss=-31845.381630704105\n",
      "Current iteration=6, loss=-61637.49243380777\n",
      "Current iteration=8, loss=-88365.08042294694\n",
      "loss=-113278.06457303466\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.2335313325425\n",
      "Current iteration=4, loss=-31856.326779291394\n",
      "Current iteration=6, loss=-61658.19217533473\n",
      "Current iteration=8, loss=-88394.69952226839\n",
      "loss=-113314.22541693211\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4053.62483267728\n",
      "Current iteration=4, loss=-31132.408494345487\n",
      "Current iteration=6, loss=-60653.27196635951\n",
      "Current iteration=8, loss=-87128.54401699391\n",
      "loss=-111797.01893704882\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3922.9805249197575\n",
      "Current iteration=4, loss=-31370.46674614375\n",
      "Current iteration=6, loss=-60979.42821719035\n",
      "Current iteration=8, loss=-87530.73364992863\n",
      "loss=-112268.17261573125\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.0504581014948\n",
      "Current iteration=4, loss=-31845.379119603036\n",
      "Current iteration=6, loss=-61637.48719212304\n",
      "Current iteration=8, loss=-88365.07166631591\n",
      "loss=-113278.05886372636\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.234180934175\n",
      "Current iteration=4, loss=-31856.324267892734\n",
      "Current iteration=6, loss=-61658.18693274287\n",
      "Current iteration=8, loss=-88394.69076397334\n",
      "loss=-113314.21970669986\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4053.6254768555073\n",
      "Current iteration=4, loss=-31132.406005040626\n",
      "Current iteration=6, loss=-60653.266771489834\n",
      "Current iteration=8, loss=-87128.53534058206\n",
      "loss=-111797.01328343048\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3922.9811708471498\n",
      "Current iteration=4, loss=-31370.464249616052\n",
      "Current iteration=6, loss=-60979.423007160316\n",
      "Current iteration=8, loss=-87530.7249483548\n",
      "loss=-112268.16694605857\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.052265427904\n",
      "Current iteration=4, loss=-31845.37213231566\n",
      "Current iteration=6, loss=-61637.47260682551\n",
      "Current iteration=8, loss=-88365.0473004738\n",
      "loss=-113278.04297723813\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.235988489222\n",
      "Current iteration=4, loss=-31856.317279777333\n",
      "Current iteration=6, loss=-61658.17234492136\n",
      "Current iteration=8, loss=-88394.66639350096\n",
      "loss=-113314.20381764088\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4053.6272693196734\n",
      "Current iteration=4, loss=-31132.39907840248\n",
      "Current iteration=6, loss=-60653.252316458005\n",
      "Current iteration=8, loss=-87128.51119795459\n",
      "loss=-111796.9975519029\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3922.9829681784167\n",
      "Current iteration=4, loss=-31370.457302879906\n",
      "Current iteration=6, loss=-60979.40850994398\n",
      "Current iteration=8, loss=-87530.70073571276\n",
      "loss=-112268.15116985887\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.0572944206683\n",
      "Current iteration=4, loss=-31845.352689777243\n",
      "Current iteration=6, loss=-61637.43202238197\n",
      "Current iteration=8, loss=-88364.97950110154\n",
      "loss=-113277.99877215156\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.241018118169\n",
      "Current iteration=4, loss=-31856.29783493485\n",
      "Current iteration=6, loss=-61658.131753454436\n",
      "Current iteration=8, loss=-88394.59858124472\n",
      "loss=-113314.15960540093\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4053.6322569573595\n",
      "Current iteration=4, loss=-31132.37980462411\n",
      "Current iteration=6, loss=-60653.2120944862\n",
      "Current iteration=8, loss=-87128.44401968997\n",
      "loss=-111796.95377800323\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3922.9879693590965\n",
      "Current iteration=4, loss=-31370.43797317772\n",
      "Current iteration=6, loss=-60979.36817059137\n",
      "Current iteration=8, loss=-87530.63336262839\n",
      "loss=-112268.10727165657\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.071287889257\n",
      "Current iteration=4, loss=-31845.29858978824\n",
      "Current iteration=6, loss=-61637.31909385799\n",
      "Current iteration=8, loss=-88364.79084555779\n",
      "loss=-113277.87576895385\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.2550133569916\n",
      "Current iteration=4, loss=-31856.243728534686\n",
      "Current iteration=6, loss=-61658.01880538769\n",
      "Current iteration=8, loss=-88394.40988985059\n",
      "loss=-113314.03658229849\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4053.6461353529467\n",
      "Current iteration=4, loss=-31132.326174219696\n",
      "Current iteration=6, loss=-60653.100174560444\n",
      "Current iteration=8, loss=-87128.25709241292\n",
      "loss=-111796.83197460775\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3923.00188543886\n",
      "Current iteration=4, loss=-31370.384187162086\n",
      "Current iteration=6, loss=-60979.25592404693\n",
      "Current iteration=8, loss=-87530.44589325458\n",
      "loss=-112267.98512238177\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.110225526948\n",
      "Current iteration=4, loss=-31845.148053580622\n",
      "Current iteration=6, loss=-61637.00486431448\n",
      "Current iteration=8, loss=-88364.26590213682\n",
      "loss=-113277.5335058808\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.2939559204283\n",
      "Current iteration=4, loss=-31856.09317448772\n",
      "Current iteration=6, loss=-61657.7045214653\n",
      "Current iteration=8, loss=-88393.88484667405\n",
      "loss=-113313.69426383931\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4053.6847527934224\n",
      "Current iteration=4, loss=-31132.1769446571\n",
      "Current iteration=6, loss=-60652.788751494416\n",
      "Current iteration=8, loss=-87127.73695797967\n",
      "loss=-111796.49305004913\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3923.0406077377\n",
      "Current iteration=4, loss=-31370.23452460265\n",
      "Current iteration=6, loss=-60978.94359214711\n",
      "Current iteration=8, loss=-87529.92425041016\n",
      "loss=-112267.64523539512\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.218571662879\n",
      "Current iteration=4, loss=-31844.729179386024\n",
      "Current iteration=6, loss=-61636.130508003174\n",
      "Current iteration=8, loss=-88362.80523010051\n",
      "loss=-113276.58114343659\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.40231576255\n",
      "Current iteration=4, loss=-31855.67425065421\n",
      "Current iteration=6, loss=-61656.830013842446\n",
      "Current iteration=8, loss=-88392.42389706396\n",
      "loss=-113312.74174727987\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4053.79220796277\n",
      "Current iteration=4, loss=-31131.761706264777\n",
      "Current iteration=6, loss=-60651.922204318944\n",
      "Current iteration=8, loss=-87126.28966710548\n",
      "loss=-111795.54997716703\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3923.1483546813547\n",
      "Current iteration=4, loss=-31369.818081375794\n",
      "Current iteration=6, loss=-60978.0745161048\n",
      "Current iteration=8, loss=-87528.47276233295\n",
      "loss=-112266.69948451394\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3656.5200500304913\n",
      "Current iteration=4, loss=-31843.563650570173\n",
      "Current iteration=6, loss=-61633.69760654436\n",
      "Current iteration=8, loss=-88358.74093345486\n",
      "loss=-113273.9311762059\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=3650.703832268194\n",
      "Current iteration=4, loss=-31854.508583716975\n",
      "Current iteration=6, loss=-61654.396691357055\n",
      "Current iteration=8, loss=-88388.35882806816\n",
      "loss=-113310.09135121417\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=4054.0912071724047\n",
      "Current iteration=4, loss=-31130.60629416762\n",
      "Current iteration=6, loss=-60649.51103182158\n",
      "Current iteration=8, loss=-87122.2626033168\n",
      "loss=-111792.92585828924\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=3923.448165767274\n",
      "Current iteration=4, loss=-31368.659316793186\n",
      "Current iteration=6, loss=-60975.65630701953\n",
      "Current iteration=8, loss=-87524.43401989301\n",
      "loss=-112264.06791405324\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.3584115591648\n",
      "Current iteration=4, loss=-34590.24862620616\n",
      "Current iteration=6, loss=-65214.472190821994\n",
      "Current iteration=8, loss=-92725.72693923436\n",
      "loss=-118415.2018383728\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1913.4989190826186\n",
      "Current iteration=4, loss=-34601.86631735255\n",
      "Current iteration=6, loss=-65236.336998659935\n",
      "Current iteration=8, loss=-92756.576436819\n",
      "loss=-118452.41057609119\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.0670715735037\n",
      "Current iteration=4, loss=-33852.61398502087\n",
      "Current iteration=6, loss=-64197.05616505477\n",
      "Current iteration=8, loss=-91447.1109556038\n",
      "loss=-116882.70932803264\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.301731463217\n",
      "Current iteration=4, loss=-34098.87027855816\n",
      "Current iteration=6, loss=-64533.5287523887\n",
      "Current iteration=8, loss=-91861.48759988106\n",
      "loss=-117367.95932588211\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.3585021334477\n",
      "Current iteration=4, loss=-34590.24827736406\n",
      "Current iteration=6, loss=-65214.471463286936\n",
      "Current iteration=8, loss=-92725.72572372535\n",
      "loss=-118415.20104447057\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1913.4990096680258\n",
      "Current iteration=4, loss=-34601.86596846781\n",
      "Current iteration=6, loss=-65236.33627099694\n",
      "Current iteration=8, loss=-92756.57522108006\n",
      "loss=-118452.40978206763\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.0671614012645\n",
      "Current iteration=4, loss=-33852.613639213974\n",
      "Current iteration=6, loss=-64197.05544403762\n",
      "Current iteration=8, loss=-91447.10975127327\n",
      "loss=-116882.70854191549\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.3018215356956\n",
      "Current iteration=4, loss=-34098.86993174683\n",
      "Current iteration=6, loss=-64533.52802926874\n",
      "Current iteration=8, loss=-91861.48639206185\n",
      "loss=-117367.9585375339\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.3587541617696\n",
      "Current iteration=4, loss=-34590.24730669024\n",
      "Current iteration=6, loss=-65214.46943887751\n",
      "Current iteration=8, loss=-92725.72234149944\n",
      "loss=-118415.19883539047\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1913.4992617272449\n",
      "Current iteration=4, loss=-34601.864997675264\n",
      "Current iteration=6, loss=-65236.33424623147\n",
      "Current iteration=8, loss=-92756.57183821424\n",
      "loss=-118452.40757264987\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.0674113523733\n",
      "Current iteration=4, loss=-33852.61267698577\n",
      "Current iteration=6, loss=-64197.05343776457\n",
      "Current iteration=8, loss=-91447.10640015219\n",
      "loss=-116882.7063544976\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.302072167714\n",
      "Current iteration=4, loss=-34098.8689667238\n",
      "Current iteration=6, loss=-64533.52601714463\n",
      "Current iteration=8, loss=-91861.48303123337\n",
      "loss=-117367.95634390817\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.359455445491\n",
      "Current iteration=4, loss=-34590.244605732696\n",
      "Current iteration=6, loss=-65214.463805838226\n",
      "Current iteration=8, loss=-92725.71293025547\n",
      "loss=-118415.19268849422\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1913.4999630970226\n",
      "Current iteration=4, loss=-34601.862296387415\n",
      "Current iteration=6, loss=-65236.32861220153\n",
      "Current iteration=8, loss=-92756.56242518975\n",
      "loss=-118452.40142481387\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.068106856182\n",
      "Current iteration=4, loss=-33852.60999952877\n",
      "Current iteration=6, loss=-64197.04785519096\n",
      "Current iteration=8, loss=-91447.09707545927\n",
      "loss=-116882.70026787768\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.3027695662336\n",
      "Current iteration=4, loss=-34098.866281489965\n",
      "Current iteration=6, loss=-64533.52041829\n",
      "Current iteration=8, loss=-91861.47367952895\n",
      "loss=-117367.95024001457\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.3614068090574\n",
      "Current iteration=4, loss=-34590.23709015839\n",
      "Current iteration=6, loss=-65214.44813157371\n",
      "Current iteration=8, loss=-92725.6867429146\n",
      "loss=-118415.1755843917\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1913.5019147000367\n",
      "Current iteration=4, loss=-34601.854779894085\n",
      "Current iteration=6, loss=-65236.31293518042\n",
      "Current iteration=8, loss=-92756.53623289437\n",
      "loss=-118452.38431809662\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.0700421367264\n",
      "Current iteration=4, loss=-33852.60254934609\n",
      "Current iteration=6, loss=-64197.03232135004\n",
      "Current iteration=8, loss=-91447.07112895165\n",
      "loss=-116882.68333149771\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.304710118933\n",
      "Current iteration=4, loss=-34098.85880966779\n",
      "Current iteration=6, loss=-64533.504839146255\n",
      "Current iteration=8, loss=-91861.44765786034\n",
      "loss=-117367.93325556937\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.3668365937306\n",
      "Current iteration=4, loss=-34590.21617763068\n",
      "Current iteration=6, loss=-65214.40451701626\n",
      "Current iteration=8, loss=-92725.61387511739\n",
      "loss=-118415.12799122244\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1913.5073451510077\n",
      "Current iteration=4, loss=-34601.83386480909\n",
      "Current iteration=6, loss=-65236.26931295258\n",
      "Current iteration=8, loss=-92756.46335131096\n",
      "loss=-118452.33671765143\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.075427169454\n",
      "Current iteration=4, loss=-33852.58181877445\n",
      "Current iteration=6, loss=-64196.9890975295\n",
      "Current iteration=8, loss=-91446.998931287\n",
      "loss=-116882.63620502618\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.310109821751\n",
      "Current iteration=4, loss=-34098.83801888291\n",
      "Current iteration=6, loss=-64533.46148926777\n",
      "Current iteration=8, loss=-91861.37525105567\n",
      "loss=-117367.88599535359\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.3819452892653\n",
      "Current iteration=4, loss=-34590.157987312785\n",
      "Current iteration=6, loss=-65214.283157032616\n",
      "Current iteration=8, loss=-92725.41111640907\n",
      "loss=-118414.99556049371\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1913.5224557004976\n",
      "Current iteration=4, loss=-34601.77566737531\n",
      "Current iteration=6, loss=-65236.147931625805\n",
      "Current iteration=8, loss=-92756.26055424179\n",
      "loss=-118452.20426667717\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.0904113400634\n",
      "Current iteration=4, loss=-33852.524134759755\n",
      "Current iteration=6, loss=-64196.86882479353\n",
      "Current iteration=8, loss=-91446.79803725991\n",
      "loss=-116882.50507291059\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.325134812744\n",
      "Current iteration=4, loss=-34098.78016732148\n",
      "Current iteration=6, loss=-64533.34086576863\n",
      "Current iteration=8, loss=-91861.17377508506\n",
      "loss=-117367.7544910871\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.4239861096444\n",
      "Current iteration=4, loss=-34589.99606954963\n",
      "Current iteration=6, loss=-65213.94546654713\n",
      "Current iteration=8, loss=-92724.84693031844\n",
      "loss=-118414.62706483617\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1913.5645016796436\n",
      "Current iteration=4, loss=-34601.61372981189\n",
      "Current iteration=6, loss=-65235.81018175175\n",
      "Current iteration=8, loss=-92755.69626141014\n",
      "loss=-118451.83571468538\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.132105662595\n",
      "Current iteration=4, loss=-33852.36362581318\n",
      "Current iteration=6, loss=-64196.53415963133\n",
      "Current iteration=8, loss=-91446.23903973644\n",
      "loss=-116882.14019071378\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.366942720377\n",
      "Current iteration=4, loss=-34098.61919216689\n",
      "Current iteration=6, loss=-64533.00522458945\n",
      "Current iteration=8, loss=-91860.61315827485\n",
      "loss=-117367.38857336008\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.5409670161866\n",
      "Current iteration=4, loss=-34589.54552571446\n",
      "Current iteration=6, loss=-65213.005829508606\n",
      "Current iteration=8, loss=-92723.27706495367\n",
      "loss=-118413.6017092915\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1913.681496940707\n",
      "Current iteration=4, loss=-34601.163130881556\n",
      "Current iteration=6, loss=-65234.8703794621\n",
      "Current iteration=8, loss=-92754.1260990342\n",
      "loss=-118450.81020238722\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.2481224197404\n",
      "Current iteration=4, loss=-33851.91700207679\n",
      "Current iteration=6, loss=-64195.6029406731\n",
      "Current iteration=8, loss=-91444.6836117194\n",
      "loss=-116881.1248897788\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.4832755343305\n",
      "Current iteration=4, loss=-34098.17127118449\n",
      "Current iteration=6, loss=-64532.07128982548\n",
      "Current iteration=8, loss=-91859.05322454206\n",
      "loss=-117366.37039101683\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=1919.8664719941723\n",
      "Current iteration=4, loss=-34588.29187589809\n",
      "Current iteration=6, loss=-65210.391286258426\n",
      "Current iteration=8, loss=-92718.90894478651\n",
      "loss=-118410.74863925563\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=1914.0070418607195\n",
      "Current iteration=4, loss=-34599.90932776165\n",
      "Current iteration=6, loss=-65232.25537639837\n",
      "Current iteration=8, loss=-92749.75715243301\n",
      "loss=-118447.95669617478\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=2332.570944605481\n",
      "Current iteration=4, loss=-33850.674260036365\n",
      "Current iteration=6, loss=-64193.011820755506\n",
      "Current iteration=8, loss=-91440.35566317993\n",
      "loss=-116878.2997968328\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=2196.8069771635182\n",
      "Current iteration=4, loss=-34096.92491952277\n",
      "Current iteration=6, loss=-64529.47261316751\n",
      "Current iteration=8, loss=-91854.71273893457\n",
      "loss=-117363.53728050439\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=199.45857114537327\n",
      "Current iteration=4, loss=-37301.44514414377\n",
      "Current iteration=6, loss=-68749.64357952154\n",
      "Current iteration=8, loss=-97042.01391924577\n",
      "loss=-123508.47996011902\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=193.55160618138402\n",
      "Current iteration=4, loss=-37313.76005090583\n",
      "Current iteration=6, loss=-68772.65583810469\n",
      "Current iteration=8, loss=-97074.0306084696\n",
      "loss=-123546.64740895352\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.1914235794384\n",
      "Current iteration=4, loss=-36539.39363303767\n",
      "Current iteration=6, loss=-67699.26794593294\n",
      "Current iteration=8, loss=-95721.48431096438\n",
      "loss=-121924.62950826217\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.34224779280476\n",
      "Current iteration=4, loss=-36793.7299051997\n",
      "Current iteration=6, loss=-68045.89161012812\n",
      "Current iteration=8, loss=-96147.87881900987\n",
      "loss=-122423.82531343184\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=199.45866863223156\n",
      "Current iteration=4, loss=-37301.44477002213\n",
      "Current iteration=6, loss=-68749.64279985646\n",
      "Current iteration=8, loss=-97042.01261645198\n",
      "loss=-123508.47910767239\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=193.55170367988788\n",
      "Current iteration=4, loss=-37313.75967673704\n",
      "Current iteration=6, loss=-68772.65505830072\n",
      "Current iteration=8, loss=-97074.029305431\n",
      "loss=-123546.6465563841\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.1915202610586\n",
      "Current iteration=4, loss=-36539.39326217884\n",
      "Current iteration=6, loss=-67699.26717327411\n",
      "Current iteration=8, loss=-95721.48302019763\n",
      "loss=-121924.62866421732\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.34234473862284\n",
      "Current iteration=4, loss=-36793.72953326277\n",
      "Current iteration=6, loss=-68045.89083521758\n",
      "Current iteration=8, loss=-96147.87752450812\n",
      "loss=-122423.82446699232\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=199.458939895221\n",
      "Current iteration=4, loss=-37301.44372900649\n",
      "Current iteration=6, loss=-68749.64063039221\n",
      "Current iteration=8, loss=-97042.00899135102\n",
      "loss=-123508.47673568924\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=193.55197497521038\n",
      "Current iteration=4, loss=-37313.75863559021\n",
      "Current iteration=6, loss=-68772.6528884499\n",
      "Current iteration=8, loss=-97074.0256796488\n",
      "loss=-123546.64418405903\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.1917892834027\n",
      "Current iteration=4, loss=-36539.39223024214\n",
      "Current iteration=6, loss=-67699.26502330518\n",
      "Current iteration=8, loss=-95721.47942856258\n",
      "loss=-121924.62631561239\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.3426144961216\n",
      "Current iteration=4, loss=-36793.72849832623\n",
      "Current iteration=6, loss=-68045.88867898301\n",
      "Current iteration=8, loss=-96147.87392248007\n",
      "loss=-122423.82211172419\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=199.4596947006248\n",
      "Current iteration=4, loss=-37301.440832318694\n",
      "Current iteration=6, loss=-68749.63459372932\n",
      "Current iteration=8, loss=-97041.99890429282\n",
      "loss=-123508.47013550547\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=193.55272987059615\n",
      "Current iteration=4, loss=-37313.7557385373\n",
      "Current iteration=6, loss=-68772.64685071124\n",
      "Current iteration=8, loss=-97074.01559069501\n",
      "loss=-123546.63758292378\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.1925378540245\n",
      "Current iteration=4, loss=-36539.389358817105\n",
      "Current iteration=6, loss=-67699.25904088923\n",
      "Current iteration=8, loss=-95721.46943462543\n",
      "loss=-121924.61978047984\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.3433651124306\n",
      "Current iteration=4, loss=-36793.725618553886\n",
      "Current iteration=6, loss=-68045.88267913254\n",
      "Current iteration=8, loss=-96147.8638996238\n",
      "loss=-122423.81555805093\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=199.46179499145092\n",
      "Current iteration=4, loss=-37301.43277211345\n",
      "Current iteration=6, loss=-68749.61779635835\n",
      "Current iteration=8, loss=-97041.97083645916\n",
      "loss=-123508.45177010365\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=193.55483041182922\n",
      "Current iteration=4, loss=-37313.747677316125\n",
      "Current iteration=6, loss=-68772.63005034707\n",
      "Current iteration=8, loss=-97073.98751758684\n",
      "loss=-123546.61921487458\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.194620796215\n",
      "Current iteration=4, loss=-36539.38136890694\n",
      "Current iteration=6, loss=-67699.24239446355\n",
      "Current iteration=8, loss=-95721.44162590652\n",
      "loss=-121924.60159608701\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.34545374679857\n",
      "Current iteration=4, loss=-36793.717605416845\n",
      "Current iteration=6, loss=-68045.8659841944\n",
      "Current iteration=8, loss=-96147.8360104357\n",
      "loss=-122423.7973220677\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=199.46763917503685\n",
      "Current iteration=4, loss=-37301.41034411823\n",
      "Current iteration=6, loss=-68749.57105669213\n",
      "Current iteration=8, loss=-97041.89273608314\n",
      "loss=-123508.40066729557\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=193.56067529217216\n",
      "Current iteration=4, loss=-37313.72524649405\n",
      "Current iteration=6, loss=-68772.58330235198\n",
      "Current iteration=8, loss=-97073.90940253405\n",
      "loss=-123546.56810469992\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.2004167061799\n",
      "Current iteration=4, loss=-36539.359136511885\n",
      "Current iteration=6, loss=-67699.19607481129\n",
      "Current iteration=8, loss=-95721.36424653228\n",
      "loss=-121924.55099694703\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.3512654956587\n",
      "Current iteration=4, loss=-36793.69530839178\n",
      "Current iteration=6, loss=-68045.81952955348\n",
      "Current iteration=8, loss=-96147.75840715131\n",
      "loss=-122423.74657937407\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=199.4839009597436\n",
      "Current iteration=4, loss=-37301.34793692582\n",
      "Current iteration=6, loss=-68749.44100092122\n",
      "Current iteration=8, loss=-97041.67541744455\n",
      "loss=-123508.25847079963\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=193.57693901562982\n",
      "Current iteration=4, loss=-37313.662831435766\n",
      "Current iteration=6, loss=-68772.45322340539\n",
      "Current iteration=8, loss=-97073.6920430565\n",
      "loss=-123546.42588770602\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.2165441667034\n",
      "Current iteration=4, loss=-36539.297273588236\n",
      "Current iteration=6, loss=-67699.067187753\n",
      "Current iteration=8, loss=-95721.14893412108\n",
      "loss=-121924.41020193629\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.36743702882325\n",
      "Current iteration=4, loss=-36793.63326563138\n",
      "Current iteration=6, loss=-68045.69026688146\n",
      "Current iteration=8, loss=-96147.5424716979\n",
      "loss=-122423.60538491738\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=199.52915031652614\n",
      "Current iteration=4, loss=-37301.174285488574\n",
      "Current iteration=6, loss=-68749.07911399323\n",
      "Current iteration=8, loss=-97041.07071772363\n",
      "loss=-123507.86280140195\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=193.6221937670594\n",
      "Current iteration=4, loss=-37313.48915811123\n",
      "Current iteration=6, loss=-68772.09127198996\n",
      "Current iteration=8, loss=-97073.0872296991\n",
      "loss=-123546.03016127163\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.2614197586989\n",
      "Current iteration=4, loss=-36539.12513660868\n",
      "Current iteration=6, loss=-67698.70855282851\n",
      "Current iteration=8, loss=-95720.54981682567\n",
      "loss=-121924.01843224591\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.4124352554962\n",
      "Current iteration=4, loss=-36793.46062824608\n",
      "Current iteration=6, loss=-68045.33058679226\n",
      "Current iteration=8, loss=-96146.9416207568\n",
      "loss=-122423.21250374697\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=199.65505914504163\n",
      "Current iteration=4, loss=-37300.691092238216\n",
      "Current iteration=6, loss=-68748.0721497096\n",
      "Current iteration=8, loss=-97039.38812263828\n",
      "loss=-123506.76183398628\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=193.74811760654168\n",
      "Current iteration=4, loss=-37313.00590395861\n",
      "Current iteration=6, loss=-68771.08412826734\n",
      "Current iteration=8, loss=-97071.40431841598\n",
      "loss=-123544.92903514765\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.3862885659954\n",
      "Current iteration=4, loss=-36538.64615740781\n",
      "Current iteration=6, loss=-67697.71063736899\n",
      "Current iteration=8, loss=-95718.8827550045\n",
      "loss=-121922.92831593083\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.537645300645\n",
      "Current iteration=4, loss=-36792.980256642695\n",
      "Current iteration=6, loss=-68044.32976312145\n",
      "Current iteration=8, loss=-96145.26973501478\n",
      "loss=-122422.1192946905\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=200.00540643247044\n",
      "Current iteration=4, loss=-37299.34659526607\n",
      "Current iteration=6, loss=-68745.27027054284\n",
      "Current iteration=8, loss=-97034.70633974708\n",
      "loss=-123503.69837465507\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=194.09850666247831\n",
      "Current iteration=4, loss=-37311.66123752435\n",
      "Current iteration=6, loss=-68768.28174980945\n",
      "Current iteration=8, loss=-97066.72165570427\n",
      "loss=-123541.8651342002\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=627.7337419447666\n",
      "Current iteration=4, loss=-36537.31338613089\n",
      "Current iteration=6, loss=-67694.93393655976\n",
      "Current iteration=8, loss=-95714.24419305465\n",
      "loss=-121919.89504991456\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=486.88604818992536\n",
      "Current iteration=4, loss=-36791.64361097102\n",
      "Current iteration=6, loss=-68041.5449702097\n",
      "Current iteration=8, loss=-96140.6177506137\n",
      "loss=-122419.07742307684\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1504.1678537631005\n",
      "Current iteration=4, loss=-39980.701423499086\n",
      "Current iteration=6, loss=-72245.69500703417\n",
      "Current iteration=8, loss=-101317.2534328577\n",
      "loss=-128561.51709294386\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1510.127785536079\n",
      "Current iteration=4, loss=-39993.735052775424\n",
      "Current iteration=6, loss=-72269.83310711636\n",
      "Current iteration=8, loss=-101350.37384121276\n",
      "loss=-128600.55823763784\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1061.5185561827614\n",
      "Current iteration=4, loss=-39194.462304660345\n",
      "Current iteration=6, loss=-71162.57168198649\n",
      "Current iteration=8, loss=-99954.9516761402\n",
      "loss=-126926.37748904576\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1207.4140823493788\n",
      "Current iteration=4, loss=-39456.76432594144\n",
      "Current iteration=6, loss=-71519.19152963073\n",
      "Current iteration=8, loss=-100393.20947674947\n",
      "loss=-127439.3847556886\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1504.1677491389191\n",
      "Current iteration=4, loss=-39980.70102334509\n",
      "Current iteration=6, loss=-72245.69417366078\n",
      "Current iteration=8, loss=-101317.25204004627\n",
      "loss=-128561.51617992422\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1510.1276808997716\n",
      "Current iteration=4, loss=-39993.73465256956\n",
      "Current iteration=6, loss=-72269.83227359293\n",
      "Current iteration=8, loss=-101350.37244814192\n",
      "loss=-128600.55732449437\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1061.5184524246392\n",
      "Current iteration=4, loss=-39194.46190800422\n",
      "Current iteration=6, loss=-71162.5708561247\n",
      "Current iteration=8, loss=-99954.95029623524\n",
      "loss=-126926.37658506888\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1207.4139783068847\n",
      "Current iteration=4, loss=-39456.76392813148\n",
      "Current iteration=6, loss=-71519.19070136396\n",
      "Current iteration=8, loss=-100393.20809285557\n",
      "loss=-127439.38384914726\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1504.1674580159236\n",
      "Current iteration=4, loss=-39980.699909892945\n",
      "Current iteration=6, loss=-72245.69185474997\n",
      "Current iteration=8, loss=-101317.24816446596\n",
      "loss=-128561.51363939284\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1510.1273897429976\n",
      "Current iteration=4, loss=-39993.73353897293\n",
      "Current iteration=6, loss=-72269.82995426451\n",
      "Current iteration=8, loss=-101350.36857183964\n",
      "loss=-128600.55478361812\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1061.518163711557\n",
      "Current iteration=4, loss=-39194.46080428506\n",
      "Current iteration=6, loss=-71162.5685581152\n",
      "Current iteration=8, loss=-99954.94645656785\n",
      "loss=-126926.37406969944\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1207.4136888024225\n",
      "Current iteration=4, loss=-39456.7628212016\n",
      "Current iteration=6, loss=-71519.18839666259\n",
      "Current iteration=8, loss=-100393.20424208864\n",
      "loss=-127439.38132664212\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1504.1666479489377\n",
      "Current iteration=4, loss=-39980.69681164622\n",
      "Current iteration=6, loss=-72245.68540224312\n",
      "Current iteration=8, loss=-101317.2373804341\n",
      "loss=-128561.50657021362\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1510.1265795820366\n",
      "Current iteration=4, loss=-39993.73044032426\n",
      "Current iteration=6, loss=-72269.82350059578\n",
      "Current iteration=8, loss=-101350.35778579897\n",
      "loss=-128600.54771347946\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1061.5173603501792\n",
      "Current iteration=4, loss=-39194.45773312105\n",
      "Current iteration=6, loss=-71162.56216376758\n",
      "Current iteration=8, loss=-99954.93577246593\n",
      "loss=-126926.36707053486\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1207.4128832390975\n",
      "Current iteration=4, loss=-39456.75974110362\n",
      "Current iteration=6, loss=-71519.18198369439\n",
      "Current iteration=8, loss=-100393.19352710163\n",
      "loss=-127439.37430762201\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1504.1643938894929\n",
      "Current iteration=4, loss=-39980.68819059132\n",
      "Current iteration=6, loss=-72245.66744776188\n",
      "Current iteration=8, loss=-101317.20737323049\n",
      "loss=-128561.48689980441\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1510.1243252610664\n",
      "Current iteration=4, loss=-39993.72181815094\n",
      "Current iteration=6, loss=-72269.8055428814\n",
      "Current iteration=8, loss=-101350.32777300573\n",
      "loss=-128600.52804040053\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1061.5151249495534\n",
      "Current iteration=4, loss=-39194.44918742537\n",
      "Current iteration=6, loss=-71162.54437111788\n",
      "Current iteration=8, loss=-99954.90604332321\n",
      "loss=-126926.34759494562\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1207.4106417114074\n",
      "Current iteration=4, loss=-39456.751170548654\n",
      "Current iteration=6, loss=-71519.16413923162\n",
      "Current iteration=8, loss=-100393.16371201926\n",
      "loss=-127439.35477678356\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1504.1581218356569\n",
      "Current iteration=4, loss=-39980.66420199925\n",
      "Current iteration=6, loss=-72245.61748836956\n",
      "Current iteration=8, loss=-101317.12387644665\n",
      "loss=-128561.43216573723\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1510.1180524794722\n",
      "Current iteration=4, loss=-39993.69782644672\n",
      "Current iteration=6, loss=-72269.7555744926\n",
      "Current iteration=8, loss=-101350.24426066832\n",
      "loss=-128600.47329890475\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1061.5089048149785\n",
      "Current iteration=4, loss=-39194.42540852467\n",
      "Current iteration=6, loss=-71162.49486203119\n",
      "Current iteration=8, loss=-99954.8233202598\n",
      "loss=-126926.29340297639\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1207.4044045278579\n",
      "Current iteration=4, loss=-39456.72732247556\n",
      "Current iteration=6, loss=-71519.11448597226\n",
      "Current iteration=8, loss=-100393.0807498238\n",
      "loss=-127439.30043108013\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1504.1406694769014\n",
      "Current iteration=4, loss=-39980.59745235752\n",
      "Current iteration=6, loss=-72245.47847353341\n",
      "Current iteration=8, loss=-101316.89154201612\n",
      "loss=-128561.2798650594\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1510.1005980957048\n",
      "Current iteration=4, loss=-39993.631068145194\n",
      "Current iteration=6, loss=-72269.61653462339\n",
      "Current iteration=8, loss=-101350.01188295906\n",
      "loss=-128600.32097755618\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1061.4915969245967\n",
      "Current iteration=4, loss=-39194.35924236129\n",
      "Current iteration=6, loss=-71162.35710019596\n",
      "Current iteration=8, loss=-99954.59313874923\n",
      "loss=-126926.1426107169\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1207.387049197765\n",
      "Current iteration=4, loss=-39456.660963835995\n",
      "Current iteration=6, loss=-71518.97632296842\n",
      "Current iteration=8, loss=-100392.84990291522\n",
      "loss=-127439.1492110466\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1504.092107280031\n",
      "Current iteration=4, loss=-39980.41171782895\n",
      "Current iteration=6, loss=-72245.09165758431\n",
      "Current iteration=8, loss=-101316.24506022007\n",
      "loss=-128560.85608026941\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1510.0520302641658\n",
      "Current iteration=4, loss=-39993.44530952038\n",
      "Current iteration=6, loss=-72269.22964901842\n",
      "Current iteration=8, loss=-101349.36528073781\n",
      "loss=-128599.89713524866\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1061.4434367193728\n",
      "Current iteration=4, loss=-39194.17513139315\n",
      "Current iteration=6, loss=-71161.9737707865\n",
      "Current iteration=8, loss=-99953.95264755706\n",
      "loss=-126925.72302318095\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1207.338756988728\n",
      "Current iteration=4, loss=-39456.4763172923\n",
      "Current iteration=6, loss=-71518.5918772865\n",
      "Current iteration=8, loss=-100392.20756022068\n",
      "loss=-127438.72843320642\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1503.9569802999308\n",
      "Current iteration=4, loss=-39979.89490290607\n",
      "Current iteration=6, loss=-72244.01532766971\n",
      "Current iteration=8, loss=-101314.44620602747\n",
      "loss=-128559.67688083232\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1509.9168876053664\n",
      "Current iteration=4, loss=-39992.928427548635\n",
      "Current iteration=6, loss=-72268.15312528342\n",
      "Current iteration=8, loss=-101347.56609145761\n",
      "loss=-128598.71777576563\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1061.3094283031476\n",
      "Current iteration=4, loss=-39193.66283410232\n",
      "Current iteration=6, loss=-71160.90714229863\n",
      "Current iteration=8, loss=-99952.17046238965\n",
      "loss=-126924.55550277769\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1207.2043812646966\n",
      "Current iteration=4, loss=-39455.962529737684\n",
      "Current iteration=6, loss=-71517.52214272843\n",
      "Current iteration=8, loss=-100390.42022319486\n",
      "loss=-127437.55760073135\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1503.5809831345891\n",
      "Current iteration=4, loss=-39978.45685354412\n",
      "Current iteration=6, loss=-72241.020441104\n",
      "Current iteration=8, loss=-101309.44094073724\n",
      "loss=-128556.39574189045\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-1509.54084681339\n",
      "Current iteration=4, loss=-39991.490191621575\n",
      "Current iteration=6, loss=-72265.15769941027\n",
      "Current iteration=8, loss=-101342.55989378667\n",
      "loss=-128595.4361914849\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-1060.936543594704\n",
      "Current iteration=4, loss=-39192.23735515514\n",
      "Current iteration=6, loss=-71157.93924993432\n",
      "Current iteration=8, loss=-99947.2115782139\n",
      "loss=-126921.30686086227\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-1206.830474504777\n",
      "Current iteration=4, loss=-39454.53290409616\n",
      "Current iteration=6, loss=-71514.54560772605\n",
      "Current iteration=8, loss=-100385.44700410683\n",
      "loss=-127434.29974293507\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3192.023576568063\n",
      "Current iteration=4, loss=-42629.63437505856\n",
      "Current iteration=6, loss=-75705.10043375398\n",
      "Current iteration=8, loss=-105554.45318821905\n",
      "loss=-133577.56246953685\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3198.0430689523578\n",
      "Current iteration=4, loss=-42643.40515130224\n",
      "Current iteration=6, loss=-75730.33943890157\n",
      "Current iteration=8, loss=-105588.61422923616\n",
      "loss=-133617.39669660345\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2734.563789715786\n",
      "Current iteration=4, loss=-41819.42238075125\n",
      "Current iteration=6, loss=-74589.41953428763\n",
      "Current iteration=8, loss=-104150.49937209251\n",
      "loss=-131891.1855428947\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2885.467995656306\n",
      "Current iteration=4, loss=-42089.57973566247\n",
      "Current iteration=6, loss=-74955.89048190924\n",
      "Current iteration=8, loss=-104600.47922913988\n",
      "loss=-132417.8843890351\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3192.0234645840196\n",
      "Current iteration=4, loss=-42629.63394812469\n",
      "Current iteration=6, loss=-75705.09954509951\n",
      "Current iteration=8, loss=-105554.45170266196\n",
      "loss=-133577.56149391475\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3198.04295695566\n",
      "Current iteration=4, loss=-42643.40472431146\n",
      "Current iteration=6, loss=-75730.33855008578\n",
      "Current iteration=8, loss=-105588.61274340516\n",
      "loss=-133617.39572085676\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2734.563678660667\n",
      "Current iteration=4, loss=-41819.42195755776\n",
      "Current iteration=6, loss=-74589.41865366696\n",
      "Current iteration=8, loss=-104150.49790035213\n",
      "loss=-131891.1845769807\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2885.46788429593\n",
      "Current iteration=4, loss=-42089.57931123726\n",
      "Current iteration=6, loss=-74955.8895987262\n",
      "Current iteration=8, loss=-104600.47775314901\n",
      "loss=-132417.8834203806\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3192.0231529817274\n",
      "Current iteration=4, loss=-42629.632760155895\n",
      "Current iteration=6, loss=-75705.09707236562\n",
      "Current iteration=8, loss=-105554.447569011\n",
      "loss=-133577.55877918823\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3198.042645318101\n",
      "Current iteration=4, loss=-42643.40353618428\n",
      "Current iteration=6, loss=-75730.33607690298\n",
      "Current iteration=8, loss=-105588.60860899233\n",
      "loss=-133617.39300578376\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2734.5633696431823\n",
      "Current iteration=4, loss=-41819.42077999674\n",
      "Current iteration=6, loss=-74589.41620328762\n",
      "Current iteration=8, loss=-104150.49380514718\n",
      "loss=-131891.1818892676\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2885.4675744290453\n",
      "Current iteration=4, loss=-42089.578130249\n",
      "Current iteration=6, loss=-74955.88714121703\n",
      "Current iteration=8, loss=-104600.47364611697\n",
      "loss=-132417.8807250419\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3192.0222859298374\n",
      "Current iteration=4, loss=-42629.62945456224\n",
      "Current iteration=6, loss=-75705.09019183701\n",
      "Current iteration=8, loss=-105554.43606688251\n",
      "loss=-133577.55122530067\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3198.041778168107\n",
      "Current iteration=4, loss=-42643.40023014987\n",
      "Current iteration=6, loss=-75730.32919512542\n",
      "Current iteration=8, loss=-105588.5971047438\n",
      "loss=-133617.38545093196\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2734.5625097836846\n",
      "Current iteration=4, loss=-41819.41750336341\n",
      "Current iteration=6, loss=-74589.40938496203\n",
      "Current iteration=8, loss=-104150.48240999703\n",
      "loss=-131891.1744105465\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2885.4667122060246\n",
      "Current iteration=4, loss=-42089.574844078976\n",
      "Current iteration=6, loss=-74955.88030305189\n",
      "Current iteration=8, loss=-104600.46221805707\n",
      "loss=-132417.87322510206\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3192.019873306539\n",
      "Current iteration=4, loss=-42629.62025655233\n",
      "Current iteration=6, loss=-75705.07104636006\n",
      "Current iteration=8, loss=-105554.40406153286\n",
      "loss=-133577.53020616202\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3198.0393652719254\n",
      "Current iteration=4, loss=-42643.39103091355\n",
      "Current iteration=6, loss=-75730.31004617304\n",
      "Current iteration=8, loss=-105588.56509349495\n",
      "loss=-133617.3644291105\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2734.5601171736403\n",
      "Current iteration=4, loss=-41819.40838593719\n",
      "Current iteration=6, loss=-74589.3904125685\n",
      "Current iteration=8, loss=-104150.45070232077\n",
      "loss=-131891.1536005631\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2885.4643130193494\n",
      "Current iteration=4, loss=-42089.56570011655\n",
      "Current iteration=6, loss=-74955.86127545402\n",
      "Current iteration=8, loss=-104600.43041880766\n",
      "loss=-132417.85235607618\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3192.013160039396\n",
      "Current iteration=4, loss=-42629.59466254919\n",
      "Current iteration=6, loss=-75705.01777295335\n",
      "Current iteration=8, loss=-105554.31500479362\n",
      "loss=-133577.47171917674\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3198.0326512453644\n",
      "Current iteration=4, loss=-42643.36543349777\n",
      "Current iteration=6, loss=-75730.25676309566\n",
      "Current iteration=8, loss=-105588.4760203407\n",
      "loss=-133617.30593465985\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2734.5534595945937\n",
      "Current iteration=4, loss=-41819.38301616296\n",
      "Current iteration=6, loss=-74589.33762077657\n",
      "Current iteration=8, loss=-104150.36247387496\n",
      "loss=-131891.09569556417\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2885.4576371404373\n",
      "Current iteration=4, loss=-42089.540256503686\n",
      "Current iteration=6, loss=-74955.80833005259\n",
      "Current iteration=8, loss=-104600.34193555429\n",
      "loss=-132417.7942867885\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3191.994479978879\n",
      "Current iteration=4, loss=-42629.52344576011\n",
      "Current iteration=6, loss=-75704.86953669059\n",
      "Current iteration=8, loss=-105554.06719949246\n",
      "loss=-133577.30897579432\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3198.013969071672\n",
      "Current iteration=4, loss=-42643.294207212886\n",
      "Current iteration=6, loss=-75730.10849992378\n",
      "Current iteration=8, loss=-105588.22816936405\n",
      "loss=-133617.1431705046\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2734.5349344894016\n",
      "Current iteration=4, loss=-41819.31242330382\n",
      "Current iteration=6, loss=-74589.19072463416\n",
      "Current iteration=8, loss=-104150.11697334624\n",
      "loss=-131890.93457159234\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2885.439061114811\n",
      "Current iteration=4, loss=-42089.46945818424\n",
      "Current iteration=6, loss=-74955.6610064831\n",
      "Current iteration=8, loss=-104600.09572600917\n",
      "loss=-132417.63270567343\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3191.9425016320915\n",
      "Current iteration=4, loss=-42629.32528116287\n",
      "Current iteration=6, loss=-75704.45706169169\n",
      "Current iteration=8, loss=-105553.37766935637\n",
      "loss=-133576.85613365946\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3197.961984844936\n",
      "Current iteration=4, loss=-42643.096016193085\n",
      "Current iteration=6, loss=-75729.695950049\n",
      "Current iteration=8, loss=-105587.53851213354\n",
      "loss=-133616.69027056827\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2734.483387314721\n",
      "Current iteration=4, loss=-41819.11599482555\n",
      "Current iteration=6, loss=-74588.78197858876\n",
      "Current iteration=8, loss=-104149.43385634958\n",
      "loss=-131890.48623555282\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2885.3873722511066\n",
      "Current iteration=4, loss=-42089.272458001455\n",
      "Current iteration=6, loss=-74955.25107109996\n",
      "Current iteration=8, loss=-104599.41063614053\n",
      "loss=-132417.18309760885\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3191.79786903636\n",
      "Current iteration=4, loss=-42628.773879099754\n",
      "Current iteration=6, loss=-75703.3093348576\n",
      "Current iteration=8, loss=-105551.4590327394\n",
      "loss=-133575.59608121697\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3197.8173358879408\n",
      "Current iteration=4, loss=-42642.544540607836\n",
      "Current iteration=6, loss=-75728.54801486946\n",
      "Current iteration=8, loss=-105585.61952187162\n",
      "loss=-133615.4300572896\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2734.3399544791787\n",
      "Current iteration=4, loss=-41818.569423593006\n",
      "Current iteration=6, loss=-74587.64462770383\n",
      "Current iteration=8, loss=-104147.53306446751\n",
      "loss=-131889.23872150475\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2885.2435451580054\n",
      "Current iteration=4, loss=-42088.724295974775\n",
      "Current iteration=6, loss=-74954.1104108389\n",
      "Current iteration=8, loss=-104597.50435468792\n",
      "loss=-132415.93204409783\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-3191.395422120243\n",
      "Current iteration=4, loss=-42627.23959101084\n",
      "Current iteration=6, loss=-75700.11578917371\n",
      "Current iteration=8, loss=-105546.1204822993\n",
      "loss=-133572.089970044\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-3197.414843445981\n",
      "Current iteration=4, loss=-42641.010047942145\n",
      "Current iteration=6, loss=-75725.35388946232\n",
      "Current iteration=8, loss=-105580.27998741568\n",
      "loss=-133611.92349857854\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-2733.9408459518922\n",
      "Current iteration=4, loss=-41817.048577393936\n",
      "Current iteration=6, loss=-74584.47995305379\n",
      "Current iteration=8, loss=-104142.24416646313\n",
      "loss=-131885.7674985127\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-2884.843339590499\n",
      "Current iteration=4, loss=-42087.19902335584\n",
      "Current iteration=6, loss=-74950.9365278603\n",
      "Current iteration=8, loss=-104592.20018211524\n",
      "loss=-132412.45097251478\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4864.596287313155\n",
      "Current iteration=4, loss=-45249.75786757678\n",
      "Current iteration=6, loss=-79130.14239226401\n",
      "Current iteration=8, loss=-109756.35291779217\n",
      "loss=-138559.54234769777\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.682904051881\n",
      "Current iteration=4, loss=-45264.281247029554\n",
      "Current iteration=6, loss=-79156.45464071175\n",
      "Current iteration=8, loss=-109791.49241522334\n",
      "loss=-138600.0935663035\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4392.430089674891\n",
      "Current iteration=4, loss=-44415.773998956174\n",
      "Current iteration=6, loss=-77982.07425283188\n",
      "Current iteration=8, loss=-108310.84841917639\n",
      "loss=-136821.96564569278\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4548.30521367699\n",
      "Current iteration=4, loss=-44693.680064202366\n",
      "Current iteration=6, loss=-78358.26048170165\n",
      "Current iteration=8, loss=-108772.42130012563\n",
      "loss=-137362.24913930686\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4864.596167748673\n",
      "Current iteration=4, loss=-45249.75741312022\n",
      "Current iteration=6, loss=-79130.14144676026\n",
      "Current iteration=8, loss=-109756.35133676516\n",
      "loss=-138559.54130744346\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.682784474153\n",
      "Current iteration=4, loss=-45264.28079251086\n",
      "Current iteration=6, loss=-79156.45369503547\n",
      "Current iteration=8, loss=-109791.49083390848\n",
      "loss=-138600.09252592447\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4392.429971104249\n",
      "Current iteration=4, loss=-44415.77354848992\n",
      "Current iteration=6, loss=-77982.07331590133\n",
      "Current iteration=8, loss=-108310.84685290759\n",
      "loss=-136821.96461583627\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4548.30509477955\n",
      "Current iteration=4, loss=-44693.67961242452\n",
      "Current iteration=6, loss=-78358.2595420471\n",
      "Current iteration=8, loss=-108772.41972933723\n",
      "loss=-137362.24810652752\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4864.5958350533565\n",
      "Current iteration=4, loss=-45249.75614856793\n",
      "Current iteration=6, loss=-79130.13881584018\n",
      "Current iteration=8, loss=-109756.34693746387\n",
      "loss=-138559.53841287413\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.682451742101\n",
      "Current iteration=4, loss=-45264.27952778553\n",
      "Current iteration=6, loss=-79156.45106363503\n",
      "Current iteration=8, loss=-109791.48643380597\n",
      "loss=-138600.0896310079\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4392.429641174386\n",
      "Current iteration=4, loss=-44415.7722950409\n",
      "Current iteration=6, loss=-77982.07070883644\n",
      "Current iteration=8, loss=-108310.84249467152\n",
      "loss=-136821.9617501995\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4548.304763940331\n",
      "Current iteration=4, loss=-44693.67835532592\n",
      "Current iteration=6, loss=-78358.25692740253\n",
      "Current iteration=8, loss=-108772.41535852531\n",
      "loss=-137362.2452327577\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4864.594909308949\n",
      "Current iteration=4, loss=-45249.75262987615\n",
      "Current iteration=6, loss=-79130.1314951492\n",
      "Current iteration=8, loss=-109756.33469614734\n",
      "loss=-138559.53035856335\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.681525895407\n",
      "Current iteration=4, loss=-45264.27600861232\n",
      "Current iteration=6, loss=-79156.44374160733\n",
      "Current iteration=8, loss=-109791.47419026034\n",
      "loss=-138600.08157573102\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4392.428723124942\n",
      "Current iteration=4, loss=-44415.768807244625\n",
      "Current iteration=6, loss=-77982.06345452393\n",
      "Current iteration=8, loss=-108310.8303676218\n",
      "loss=-136821.9537763953\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4548.303843360585\n",
      "Current iteration=4, loss=-44693.67485737455\n",
      "Current iteration=6, loss=-78358.24965199914\n",
      "Current iteration=8, loss=-108772.40319648245\n",
      "loss=-137362.23723632286\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4864.592333370195\n",
      "Current iteration=4, loss=-45249.74283890812\n",
      "Current iteration=6, loss=-79130.1111248944\n",
      "Current iteration=8, loss=-109756.3006339637\n",
      "loss=-138559.50794696744\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.678949672037\n",
      "Current iteration=4, loss=-45264.26621630466\n",
      "Current iteration=6, loss=-79156.42336763316\n",
      "Current iteration=8, loss=-109791.44012187386\n",
      "loss=-138600.05916144708\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4392.426168597858\n",
      "Current iteration=4, loss=-44415.75910224501\n",
      "Current iteration=6, loss=-77982.04326897122\n",
      "Current iteration=8, loss=-108310.79662339216\n",
      "loss=-136821.93158881355\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4548.301281792804\n",
      "Current iteration=4, loss=-44693.665124117775\n",
      "Current iteration=6, loss=-78358.22940775975\n",
      "Current iteration=8, loss=-108772.3693548823\n",
      "loss=-137362.21498577035\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4864.585165668153\n",
      "Current iteration=4, loss=-45249.715594964306\n",
      "Current iteration=6, loss=-79130.05444347273\n",
      "Current iteration=8, loss=-109756.20585396672\n",
      "loss=-138559.44558538907\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.67178117806\n",
      "Current iteration=4, loss=-45264.23896863326\n",
      "Current iteration=6, loss=-79156.36667586213\n",
      "Current iteration=8, loss=-109791.34532461721\n",
      "loss=-138599.99679238873\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4392.4190604751375\n",
      "Current iteration=4, loss=-44415.732097513544\n",
      "Current iteration=6, loss=-77981.98710149377\n",
      "Current iteration=8, loss=-108310.70272812039\n",
      "loss=-136821.86985056775\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4548.294154078926\n",
      "Current iteration=4, loss=-44693.63804075899\n",
      "Current iteration=6, loss=-78358.17307698325\n",
      "Current iteration=8, loss=-108772.27518867161\n",
      "loss=-137362.1530723043\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4864.565221116011\n",
      "Current iteration=4, loss=-45249.63978712212\n",
      "Current iteration=6, loss=-79129.89672422216\n",
      "Current iteration=8, loss=-109755.94212339915\n",
      "loss=-138559.27206073524\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.651834422211\n",
      "Current iteration=4, loss=-45264.163150418804\n",
      "Current iteration=6, loss=-79156.20892781374\n",
      "Current iteration=8, loss=-109791.08154602359\n",
      "loss=-138599.82324692153\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4392.399281705816\n",
      "Current iteration=4, loss=-44415.65695529344\n",
      "Current iteration=6, loss=-77981.83081232205\n",
      "Current iteration=8, loss=-108310.44145934955\n",
      "loss=-136821.69806037212\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4548.274320796047\n",
      "Current iteration=4, loss=-44693.56267975391\n",
      "Current iteration=6, loss=-78358.01633342273\n",
      "Current iteration=8, loss=-108772.01316599807\n",
      "loss=-137361.9807945487\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4864.509724249453\n",
      "Current iteration=4, loss=-45249.42884768327\n",
      "Current iteration=6, loss=-79129.45786237202\n",
      "Current iteration=8, loss=-109755.20828055816\n",
      "loss=-138558.78921918073\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.596331423849\n",
      "Current iteration=4, loss=-45263.95218211864\n",
      "Current iteration=6, loss=-79155.76998583226\n",
      "Current iteration=8, loss=-109790.34756954771\n",
      "loss=-138599.34034745258\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4392.344246139652\n",
      "Current iteration=4, loss=-44415.44786798411\n",
      "Current iteration=6, loss=-77981.39592973904\n",
      "Current iteration=8, loss=-108309.71446657463\n",
      "loss=-136821.2200450388\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4548.219133542751\n",
      "Current iteration=4, loss=-44693.35298366343\n",
      "Current iteration=6, loss=-78357.58018647955\n",
      "Current iteration=8, loss=-108771.28407545315\n",
      "loss=-137361.5014225537\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4864.355301189714\n",
      "Current iteration=4, loss=-45248.841899146544\n",
      "Current iteration=6, loss=-79128.23671352917\n",
      "Current iteration=8, loss=-109753.16634349158\n",
      "loss=-138557.44569240996\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.441891301994\n",
      "Current iteration=4, loss=-45263.36515327387\n",
      "Current iteration=6, loss=-79154.54861402053\n",
      "Current iteration=8, loss=-109788.30526063718\n",
      "loss=-138597.9966595313\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4392.1911066734565\n",
      "Current iteration=4, loss=-44414.86607308175\n",
      "Current iteration=6, loss=-77980.1858533488\n",
      "Current iteration=8, loss=-108307.69158999516\n",
      "loss=-136819.88994742237\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4548.065571998823\n",
      "Current iteration=4, loss=-44692.76949479984\n",
      "Current iteration=6, loss=-78356.3665919617\n",
      "Current iteration=8, loss=-108769.25536177376\n",
      "loss=-137360.16754997088\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4863.925611928726\n",
      "Current iteration=4, loss=-45247.208703067095\n",
      "Current iteration=6, loss=-79124.83887427242\n",
      "Current iteration=8, loss=-109747.48471991003\n",
      "loss=-138553.70731545705\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-4870.01215456506\n",
      "Current iteration=4, loss=-45261.73173373575\n",
      "Current iteration=6, loss=-79151.15015435104\n",
      "Current iteration=8, loss=-109782.62260240193\n",
      "loss=-138594.25783416565\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-4391.764989071083\n",
      "Current iteration=4, loss=-44413.247217093165\n",
      "Current iteration=6, loss=-77976.81882311738\n",
      "Current iteration=8, loss=-108302.06300158007\n",
      "loss=-136816.18893717107\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-4547.63827994542\n",
      "Current iteration=4, loss=-44691.145925328965\n",
      "Current iteration=6, loss=-78352.98977255984\n",
      "Current iteration=8, loss=-108763.61053182004\n",
      "loss=-137356.45603584568\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6522.358799993434\n",
      "Current iteration=4, loss=-47842.490953156856\n",
      "Current iteration=6, loss=-82522.93005854276\n",
      "Current iteration=8, loss=-113925.45247869869\n",
      "loss=-143510.09740959154\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6528.520953259516\n",
      "Current iteration=4, loss=-47857.77956028428\n",
      "Current iteration=6, loss=-82550.2857020143\n",
      "Current iteration=8, loss=-113961.50958171953\n",
      "loss=-143551.29408461924\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6035.588318017952\n",
      "Current iteration=4, loss=-46984.92324478838\n",
      "Current iteration=6, loss=-81342.62706824635\n",
      "Current iteration=8, loss=-112438.48230847168\n",
      "loss=-141721.34647152363\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6196.396591872271\n",
      "Current iteration=4, loss=-47270.47515886517\n",
      "Current iteration=6, loss=-81728.40148923863\n",
      "Current iteration=8, loss=-112911.53034510819\n",
      "loss=-142275.11927747136\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6522.358672629882\n",
      "Current iteration=4, loss=-47842.49047043916\n",
      "Current iteration=6, loss=-82522.92905462607\n",
      "Current iteration=8, loss=-113925.45079948142\n",
      "loss=-143510.09630267555\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6528.520825882174\n",
      "Current iteration=4, loss=-47857.77907749891\n",
      "Current iteration=6, loss=-82550.28469791364\n",
      "Current iteration=8, loss=-113961.50790220061\n",
      "loss=-143551.29297757844\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6035.588191715191\n",
      "Current iteration=4, loss=-46984.922766318334\n",
      "Current iteration=6, loss=-81342.62607345916\n",
      "Current iteration=8, loss=-112438.48064498501\n",
      "loss=-141721.3453757194\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6196.3964652204995\n",
      "Current iteration=4, loss=-47270.474679001665\n",
      "Current iteration=6, loss=-81728.40049156157\n",
      "Current iteration=8, loss=-112911.52867682523\n",
      "loss=-142275.11817855568\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6522.358318233208\n",
      "Current iteration=4, loss=-47842.489127248446\n",
      "Current iteration=6, loss=-82522.92626116835\n",
      "Current iteration=8, loss=-113925.44612695977\n",
      "loss=-143510.09322261595\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6528.520471447201\n",
      "Current iteration=4, loss=-47857.777734119874\n",
      "Current iteration=6, loss=-82550.2819039439\n",
      "Current iteration=8, loss=-113961.50322883944\n",
      "loss=-143551.28989717193\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6035.587840270205\n",
      "Current iteration=4, loss=-46984.921434947\n",
      "Current iteration=6, loss=-81342.62330540468\n",
      "Current iteration=8, loss=-112438.47601623445\n",
      "loss=-141721.342326579\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6196.3961128044\n",
      "Current iteration=4, loss=-47270.47334375288\n",
      "Current iteration=6, loss=-81728.39771546597\n",
      "Current iteration=8, loss=-112911.52403472878\n",
      "loss=-142275.11512075734\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6522.35733210343\n",
      "Current iteration=4, loss=-47842.48538974053\n",
      "Current iteration=6, loss=-82522.91848820672\n",
      "Current iteration=8, loss=-113925.43312539169\n",
      "loss=-143510.0846521675\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6528.519485210819\n",
      "Current iteration=4, loss=-47857.77399608808\n",
      "Current iteration=6, loss=-82550.27412955755\n",
      "Current iteration=8, loss=-113961.49022493532\n",
      "loss=-143551.28132575797\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6035.586862353694\n",
      "Current iteration=4, loss=-46984.91773032729\n",
      "Current iteration=6, loss=-81342.61560312922\n",
      "Current iteration=8, loss=-112438.46313646215\n",
      "loss=-141721.33384216516\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6196.395132185641\n",
      "Current iteration=4, loss=-47270.46962834396\n",
      "Current iteration=6, loss=-81728.38999081549\n",
      "Current iteration=8, loss=-112911.5111178208\n",
      "loss=-142275.1066122523\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6522.354588138874\n",
      "Current iteration=4, loss=-47842.474989903705\n",
      "Current iteration=6, loss=-82522.89685948234\n",
      "Current iteration=8, loss=-113925.39694776361\n",
      "loss=-143510.0608043884\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6528.516740949575\n",
      "Current iteration=4, loss=-47857.76359479342\n",
      "Current iteration=6, loss=-82550.25249686894\n",
      "Current iteration=8, loss=-113961.4540408071\n",
      "loss=-143551.25747529214\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6035.58414124296\n",
      "Current iteration=4, loss=-46984.90742200377\n",
      "Current iteration=6, loss=-81342.59417109324\n",
      "Current iteration=8, loss=-112438.42729773789\n",
      "loss=-141721.3102337823\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6196.392403555758\n",
      "Current iteration=4, loss=-47270.45928999882\n",
      "Current iteration=6, loss=-81728.36849651967\n",
      "Current iteration=8, loss=-112911.4751757643\n",
      "loss=-142275.082936834\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6522.346952895082\n",
      "Current iteration=4, loss=-47842.446051746985\n",
      "Current iteration=6, loss=-82522.83667629655\n",
      "Current iteration=8, loss=-113925.29628142173\n",
      "loss=-143509.99444654654\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6528.509104880276\n",
      "Current iteration=4, loss=-47857.7346525803\n",
      "Current iteration=6, loss=-82550.1923026523\n",
      "Current iteration=8, loss=-113961.35335637827\n",
      "loss=-143551.19110997455\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6035.576569591357\n",
      "Current iteration=4, loss=-46984.87873848838\n",
      "Current iteration=6, loss=-81342.53453520428\n",
      "Current iteration=8, loss=-112438.32757441574\n",
      "loss=-141721.24454207486\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6196.384810981695\n",
      "Current iteration=4, loss=-47270.43052294635\n",
      "Current iteration=6, loss=-81728.30868738914\n",
      "Current iteration=8, loss=-112911.37516491406\n",
      "loss=-142275.0170585965\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6522.325707380667\n",
      "Current iteration=4, loss=-47842.36552966166\n",
      "Current iteration=6, loss=-82522.66921319652\n",
      "Current iteration=8, loss=-113925.0161717906\n",
      "loss=-143509.8098020672\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6528.4878570688215\n",
      "Current iteration=4, loss=-47857.65411920777\n",
      "Current iteration=6, loss=-82550.02480885829\n",
      "Current iteration=8, loss=-113961.0731964191\n",
      "loss=-143551.00644469343\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6035.555501025878\n",
      "Current iteration=4, loss=-46984.79892495716\n",
      "Current iteration=6, loss=-81342.36859498853\n",
      "Current iteration=8, loss=-112438.05008878857\n",
      "loss=-141721.06175115219\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6196.363684198307\n",
      "Current iteration=4, loss=-47270.35047696855\n",
      "Current iteration=6, loss=-81728.14226511902\n",
      "Current iteration=8, loss=-112911.09687922434\n",
      "loss=-142274.83374864352\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6522.266590512935\n",
      "Current iteration=4, loss=-47842.14147260009\n",
      "Current iteration=6, loss=-82522.20323864027\n",
      "Current iteration=8, loss=-113924.23675354665\n",
      "loss=-143509.29601905358\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6528.428733809538\n",
      "Current iteration=4, loss=-47857.43003073897\n",
      "Current iteration=6, loss=-82549.55874889449\n",
      "Current iteration=8, loss=-113960.29363813472\n",
      "loss=-143550.49260379767\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6035.496876528885\n",
      "Current iteration=4, loss=-46984.57683948565\n",
      "Current iteration=6, loss=-81341.9068579347\n",
      "Current iteration=8, loss=-112437.27797195884\n",
      "loss=-141720.55312575682\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6196.304897706515\n",
      "Current iteration=4, loss=-47270.12774470178\n",
      "Current iteration=6, loss=-81727.67918672446\n",
      "Current iteration=8, loss=-112910.3225361829\n",
      "loss=-142274.32367901914\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6522.102094612908\n",
      "Current iteration=4, loss=-47841.51802379952\n",
      "Current iteration=6, loss=-82520.9066481178\n",
      "Current iteration=8, loss=-113922.06800264743\n",
      "loss=-143507.86639673263\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6528.264220124675\n",
      "Current iteration=4, loss=-47856.80649454633\n",
      "Current iteration=6, loss=-82548.26192072206\n",
      "Current iteration=8, loss=-113958.12449756834\n",
      "loss=-143549.06282041586\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6035.333750677211\n",
      "Current iteration=4, loss=-46983.95887672259\n",
      "Current iteration=6, loss=-81340.62205841027\n",
      "Current iteration=8, loss=-112435.12953742612\n",
      "loss=-141719.13785471217\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6196.141321095616\n",
      "Current iteration=4, loss=-47269.50798220189\n",
      "Current iteration=6, loss=-81726.39065487222\n",
      "Current iteration=8, loss=-112908.16790716052\n",
      "loss=-142272.90438934925\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6521.644377292144\n",
      "Current iteration=4, loss=-47839.78326589398\n",
      "Current iteration=6, loss=-82517.29889592541\n",
      "Current iteration=8, loss=-113916.03353088486\n",
      "loss=-143503.88846074563\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-6527.806453316955\n",
      "Current iteration=4, loss=-47855.07149347063\n",
      "Current iteration=6, loss=-82544.6535072666\n",
      "Current iteration=8, loss=-113952.08894155979\n",
      "loss=-143545.08443626564\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-6034.879845578918\n",
      "Current iteration=4, loss=-46982.23938381859\n",
      "Current iteration=6, loss=-81337.0471145626\n",
      "Current iteration=8, loss=-112429.15159520521\n",
      "loss=-141715.1998512094\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-6195.686161738768\n",
      "Current iteration=4, loss=-47267.78348149527\n",
      "Current iteration=6, loss=-81722.80532585239\n",
      "Current iteration=8, loss=-112902.17272899735\n",
      "loss=-142268.95520399863\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.769250713902\n",
      "Current iteration=4, loss=-50409.16533444255\n",
      "Current iteration=6, loss=-85885.41526427383\n",
      "Current iteration=8, loss=-118064.03653120607\n",
      "loss=-148431.6152719778\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8172.016087567987\n",
      "Current iteration=4, loss=-50425.22911392739\n",
      "Current iteration=6, loss=-85913.78274701662\n",
      "Current iteration=8, loss=-118100.9520378117\n",
      "loss=-148473.39039018605\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7664.494595024136\n",
      "Current iteration=4, loss=-49528.1895848529\n",
      "Current iteration=6, loss=-84673.01353960289\n",
      "Current iteration=8, loss=-116535.67139207678\n",
      "loss=-146591.70556037404\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7830.1983194780505\n",
      "Current iteration=4, loss=-49821.2882071638\n",
      "Current iteration=6, loss=-85068.25727294898\n",
      "Current iteration=8, loss=-117020.08692762358\n",
      "loss=-147158.8827306939\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.769115334445\n",
      "Current iteration=4, loss=-50409.16482272898\n",
      "Current iteration=6, loss=-85885.41420038395\n",
      "Current iteration=8, loss=-118064.03475108127\n",
      "loss=-148431.6140963708\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8172.015952174161\n",
      "Current iteration=4, loss=-50425.228602140465\n",
      "Current iteration=6, loss=-85913.7816829314\n",
      "Current iteration=8, loss=-118100.95025737163\n",
      "loss=-148473.38921445463\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7664.494460774412\n",
      "Current iteration=4, loss=-49528.189077651856\n",
      "Current iteration=6, loss=-84673.012485416\n",
      "Current iteration=8, loss=-116535.669628686\n",
      "loss=-146591.70439661707\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7830.1981848564465\n",
      "Current iteration=4, loss=-49821.28769848531\n",
      "Current iteration=6, loss=-85068.25621570215\n",
      "Current iteration=8, loss=-117020.0851591522\n",
      "loss=-147158.88156363048\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.76873863307\n",
      "Current iteration=4, loss=-50409.163398855635\n",
      "Current iteration=6, loss=-85885.41124004731\n",
      "Current iteration=8, loss=-118064.0297977782\n",
      "loss=-148431.6108251746\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8172.015575432886\n",
      "Current iteration=4, loss=-50425.22717806297\n",
      "Current iteration=6, loss=-85913.77872205112\n",
      "Current iteration=8, loss=-118100.94530319158\n",
      "loss=-148473.3859429123\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7664.494087216603\n",
      "Current iteration=4, loss=-49528.1876663347\n",
      "Current iteration=6, loss=-84673.00955207829\n",
      "Current iteration=8, loss=-116535.66472194654\n",
      "loss=-146591.7011583942\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7830.197810263888\n",
      "Current iteration=4, loss=-49821.28628305736\n",
      "Current iteration=6, loss=-85068.25327385012\n",
      "Current iteration=8, loss=-117020.08023827554\n",
      "loss=-147158.878316207\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.7676904391055\n",
      "Current iteration=4, loss=-50409.15943684365\n",
      "Current iteration=6, loss=-85885.40300273527\n",
      "Current iteration=8, loss=-118064.01601491954\n",
      "loss=-148431.6017228771\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8172.014527127839\n",
      "Current iteration=4, loss=-50425.22321548291\n",
      "Current iteration=6, loss=-85913.77048322622\n",
      "Current iteration=8, loss=-118100.93151789228\n",
      "loss=-148473.37683965187\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7664.493047769805\n",
      "Current iteration=4, loss=-49528.1837392612\n",
      "Current iteration=6, loss=-84673.00138989258\n",
      "Current iteration=8, loss=-116535.65106865365\n",
      "loss=-146591.6921478471\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7830.196767937817\n",
      "Current iteration=4, loss=-49821.282344544976\n",
      "Current iteration=6, loss=-85068.2450879725\n",
      "Current iteration=8, loss=-117020.06654564498\n",
      "loss=-147158.86928005883\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.764773777306\n",
      "Current iteration=4, loss=-50409.148412310904\n",
      "Current iteration=6, loss=-85885.38008192855\n",
      "Current iteration=8, loss=-118063.97766330457\n",
      "loss=-148431.5763951965\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8172.011610156867\n",
      "Current iteration=4, loss=-50425.212189369464\n",
      "Current iteration=6, loss=-85913.74755820994\n",
      "Current iteration=8, loss=-118100.89315948621\n",
      "loss=-148473.35150929168\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7664.49015544748\n",
      "Current iteration=4, loss=-49528.17281194678\n",
      "Current iteration=6, loss=-84672.97867812925\n",
      "Current iteration=8, loss=-116535.61307756312\n",
      "loss=-146591.66707546753\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7830.193867603691\n",
      "Current iteration=4, loss=-49821.27138540135\n",
      "Current iteration=6, loss=-85068.22231028517\n",
      "Current iteration=8, loss=-117020.02844509516\n",
      "loss=-147158.84413644264\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.756657993245\n",
      "Current iteration=4, loss=-50409.11773590137\n",
      "Current iteration=6, loss=-85885.31630344925\n",
      "Current iteration=8, loss=-118063.87094772069\n",
      "loss=-148431.50591944324\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8172.003493512569\n",
      "Current iteration=4, loss=-50425.181508561574\n",
      "Current iteration=6, loss=-85913.68376801732\n",
      "Current iteration=8, loss=-118100.78642500596\n",
      "loss=-148473.2810260822\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7664.4821073894755\n",
      "Current iteration=4, loss=-49528.14240605308\n",
      "Current iteration=6, loss=-84672.91548132528\n",
      "Current iteration=8, loss=-116535.50736515943\n",
      "loss=-146591.59731010397\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7830.18579725239\n",
      "Current iteration=4, loss=-49821.24089094093\n",
      "Current iteration=6, loss=-85068.15893004394\n",
      "Current iteration=8, loss=-117019.92242811472\n",
      "loss=-147158.7741728592\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.734075347455\n",
      "Current iteration=4, loss=-50409.032377029755\n",
      "Current iteration=6, loss=-85885.13883624975\n",
      "Current iteration=8, loss=-118063.57400575724\n",
      "loss=-148431.3098166466\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8171.980908473113\n",
      "Current iteration=4, loss=-50425.09613745127\n",
      "Current iteration=6, loss=-85913.50626822491\n",
      "Current iteration=8, loss=-118100.48943046229\n",
      "loss=-148473.08490253828\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7664.459713195394\n",
      "Current iteration=4, loss=-49528.05779990712\n",
      "Current iteration=6, loss=-84672.73963267014\n",
      "Current iteration=8, loss=-116535.21321459988\n",
      "loss=-146591.40318400727\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7830.163341025916\n",
      "Current iteration=4, loss=-49821.15603835313\n",
      "Current iteration=6, loss=-85067.98257096426\n",
      "Current iteration=8, loss=-117019.62743005372\n",
      "loss=-147158.57949520423\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.671237836159\n",
      "Current iteration=4, loss=-50408.79486136424\n",
      "Current iteration=6, loss=-85884.64502483483\n",
      "Current iteration=8, loss=-118062.74775088309\n",
      "loss=-148430.7641503082\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8171.918064301356\n",
      "Current iteration=4, loss=-50424.858587730814\n",
      "Current iteration=6, loss=-85913.01236611839\n",
      "Current iteration=8, loss=-118099.66302928088\n",
      "loss=-148472.53917846942\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7664.397400061876\n",
      "Current iteration=4, loss=-49527.82237874141\n",
      "Current iteration=6, loss=-84672.25032493606\n",
      "Current iteration=8, loss=-116534.3947269369\n",
      "loss=-146590.86301793903\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7830.100855283682\n",
      "Current iteration=4, loss=-49820.91993144959\n",
      "Current iteration=6, loss=-85067.4918429482\n",
      "Current iteration=8, loss=-117018.80658417847\n",
      "loss=-147158.0377943965\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.496389054198\n",
      "Current iteration=4, loss=-50408.13396351795\n",
      "Current iteration=6, loss=-85883.27097770051\n",
      "Current iteration=8, loss=-118060.44867686566\n",
      "loss=-148429.2458116997\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8171.743196986246\n",
      "Current iteration=4, loss=-50424.197595125304\n",
      "Current iteration=6, loss=-85911.63806663117\n",
      "Current iteration=8, loss=-118097.3635481585\n",
      "loss=-148471.02067922225\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7664.224010389551\n",
      "Current iteration=4, loss=-49527.167308933575\n",
      "Current iteration=6, loss=-84670.8888094466\n",
      "Current iteration=8, loss=-116532.11726536813\n",
      "loss=-146589.3599840459\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7829.926985318218\n",
      "Current iteration=4, loss=-49820.26295354573\n",
      "Current iteration=6, loss=-85066.12637547548\n",
      "Current iteration=8, loss=-117016.52256082803\n",
      "loss=-147156.53049003027\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-8165.009864455452\n",
      "Current iteration=4, loss=-50406.29500389154\n",
      "Current iteration=6, loss=-85879.44770640942\n",
      "Current iteration=8, loss=-118054.05159331785\n",
      "loss=-148425.02102478774\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-8171.256620818385\n",
      "Current iteration=4, loss=-50422.358371829425\n",
      "Current iteration=6, loss=-85907.81409316638\n",
      "Current iteration=8, loss=-118090.96533184529\n",
      "loss=-148466.7954453216\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-7663.741545829652\n",
      "Current iteration=4, loss=-49525.34456592415\n",
      "Current iteration=6, loss=-84667.1004073133\n",
      "Current iteration=8, loss=-116525.78031759146\n",
      "loss=-146585.17778253654\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-7829.443184320694\n",
      "Current iteration=4, loss=-49818.434901224915\n",
      "Current iteration=6, loss=-85062.32697699\n",
      "Current iteration=8, loss=-117010.16735515546\n",
      "loss=-147152.3364059146\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9795.271338199744\n",
      "Current iteration=4, loss=-52951.032144674165\n",
      "Current iteration=6, loss=-89219.40672738862\n",
      "Current iteration=8, loss=-122174.19628150994\n",
      "loss=-153326.25882704533\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9801.612634883732\n",
      "Current iteration=4, loss=-52967.878523578365\n",
      "Current iteration=6, loss=-89248.75320938087\n",
      "Current iteration=8, loss=-122211.91288500182\n",
      "loss=-153368.54981302307\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9279.590550162946\n",
      "Current iteration=4, loss=-52046.81261386511\n",
      "Current iteration=6, loss=-87975.02763451907\n",
      "Current iteration=8, loss=-120604.4943734771\n",
      "loss=-151435.19736865166\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9450.152161716751\n",
      "Current iteration=4, loss=-52347.36247252038\n",
      "Current iteration=6, loss=-88379.62950807702\n",
      "Current iteration=8, loss=-121100.17909013617\n",
      "loss=-152015.70326068252\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9795.27119458926\n",
      "Current iteration=4, loss=-52951.031603233656\n",
      "Current iteration=6, loss=-89219.40560196867\n",
      "Current iteration=8, loss=-122174.19439776303\n",
      "loss=-153326.25758071878\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9801.612491258302\n",
      "Current iteration=4, loss=-52967.87798205859\n",
      "Current iteration=6, loss=-89248.75208375414\n",
      "Current iteration=8, loss=-122211.91100092654\n",
      "loss=-153368.5485665727\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9279.590407753176\n",
      "Current iteration=4, loss=-52046.812077209346\n",
      "Current iteration=6, loss=-87975.02651939263\n",
      "Current iteration=8, loss=-120604.49250749896\n",
      "loss=-151435.19613493755\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9450.152018911584\n",
      "Current iteration=4, loss=-52347.3619343013\n",
      "Current iteration=6, loss=-88379.62838971638\n",
      "Current iteration=8, loss=-121100.1772187853\n",
      "loss=-152015.7020234606\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9795.270794984592\n",
      "Current iteration=4, loss=-52951.030096643335\n",
      "Current iteration=6, loss=-89219.4024704208\n",
      "Current iteration=8, loss=-122174.18915612552\n",
      "loss=-153326.2541127412\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9801.612091612027\n",
      "Current iteration=4, loss=-52967.87647524764\n",
      "Current iteration=6, loss=-89248.7489516309\n",
      "Current iteration=8, loss=-122211.90575837516\n",
      "loss=-153368.5450982505\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9279.590011489525\n",
      "Current iteration=4, loss=-52046.81058393271\n",
      "Current iteration=6, loss=-87975.0234164872\n",
      "Current iteration=8, loss=-120604.48731530414\n",
      "loss=-151435.19270205486\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9450.15162154774\n",
      "Current iteration=4, loss=-52347.360436674804\n",
      "Current iteration=6, loss=-88379.62527781153\n",
      "Current iteration=8, loss=-121100.17201164049\n",
      "loss=-152015.69858081706\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9795.269683060847\n",
      "Current iteration=4, loss=-52951.025904466354\n",
      "Current iteration=6, loss=-89219.3937567033\n",
      "Current iteration=8, loss=-122174.17457095884\n",
      "loss=-153326.24446288805\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9801.610979572528\n",
      "Current iteration=4, loss=-52967.872282456825\n",
      "Current iteration=6, loss=-89248.74023631254\n",
      "Current iteration=8, loss=-122211.89117066575\n",
      "loss=-153368.5354474385\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9279.588908862406\n",
      "Current iteration=4, loss=-52046.80642880204\n",
      "Current iteration=6, loss=-87975.01478246904\n",
      "Current iteration=8, loss=-120604.47286771477\n",
      "loss=-151435.18314985535\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9450.150515859217\n",
      "Current iteration=4, loss=-52347.35626944038\n",
      "Current iteration=6, loss=-88379.61661875188\n",
      "Current iteration=8, loss=-121100.15752245203\n",
      "loss=-152015.68900145742\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9795.266589067121\n",
      "Current iteration=4, loss=-52951.01423948601\n",
      "Current iteration=6, loss=-89219.36951027057\n",
      "Current iteration=8, loss=-122174.13398687514\n",
      "loss=-153326.21761160152\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9801.607885256659\n",
      "Current iteration=4, loss=-52967.86061576843\n",
      "Current iteration=6, loss=-89248.71598542514\n",
      "Current iteration=8, loss=-122211.8505795066\n",
      "loss=-153368.5085934837\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9279.585840737034\n",
      "Current iteration=4, loss=-52046.794866905206\n",
      "Current iteration=6, loss=-87974.99075780435\n",
      "Current iteration=8, loss=-120604.43266644799\n",
      "loss=-151435.15657029592\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9450.147439215329\n",
      "Current iteration=4, loss=-52347.344673864005\n",
      "Current iteration=6, loss=-88379.5925244079\n",
      "Current iteration=8, loss=-121100.11720543304\n",
      "loss=-152015.6623463232\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9795.25797984652\n",
      "Current iteration=4, loss=-52950.981780993825\n",
      "Current iteration=6, loss=-89219.3020431606\n",
      "Current iteration=8, loss=-122174.02105932044\n",
      "loss=-153326.1428963264\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9801.599275139753\n",
      "Current iteration=4, loss=-52967.8281525235\n",
      "Current iteration=6, loss=-89248.64850591992\n",
      "Current iteration=8, loss=-122211.73763226416\n",
      "loss=-153368.43387078418\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9279.577303496697\n",
      "Current iteration=4, loss=-52046.762695248835\n",
      "Current iteration=6, loss=-87974.92390777684\n",
      "Current iteration=8, loss=-120604.32080410334\n",
      "loss=-151435.0826111174\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9450.138878271702\n",
      "Current iteration=4, loss=-52347.31240849242\n",
      "Current iteration=6, loss=-88379.52548049374\n",
      "Current iteration=8, loss=-121100.00502100142\n",
      "loss=-152015.58817685343\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9795.23402418482\n",
      "Current iteration=4, loss=-52950.89146337694\n",
      "Current iteration=6, loss=-89219.11431214683\n",
      "Current iteration=8, loss=-122173.70683222511\n",
      "loss=-153325.93499682556\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9801.575316983963\n",
      "Current iteration=4, loss=-52967.73782168177\n",
      "Current iteration=6, loss=-89248.46074041548\n",
      "Current iteration=8, loss=-122211.42335038654\n",
      "loss=-153368.22595062442\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9279.553548124266\n",
      "Current iteration=4, loss=-52046.673175769116\n",
      "Current iteration=6, loss=-87974.7378938295\n",
      "Current iteration=8, loss=-120604.00954101309\n",
      "loss=-151434.87681549782\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9450.115056943478\n",
      "Current iteration=4, loss=-52347.22262824471\n",
      "Current iteration=6, loss=-88379.33892704592\n",
      "Current iteration=8, loss=-121099.6928616866\n",
      "loss=-152015.3817960868\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9795.167366179236\n",
      "Current iteration=4, loss=-52950.64014975188\n",
      "Current iteration=6, loss=-89218.59194120963\n",
      "Current iteration=8, loss=-122172.83248079919\n",
      "loss=-153325.35650558854\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9801.508652038488\n",
      "Current iteration=4, loss=-52967.48647125791\n",
      "Current iteration=6, loss=-89247.93827350627\n",
      "Current iteration=8, loss=-122210.54884652603\n",
      "loss=-153367.64740190274\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9279.48744743516\n",
      "Current iteration=4, loss=-52046.42408300369\n",
      "Current iteration=6, loss=-87974.22030071552\n",
      "Current iteration=8, loss=-120603.14343706802\n",
      "loss=-151434.30417841955\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9450.048772728558\n",
      "Current iteration=4, loss=-52346.97280987815\n",
      "Current iteration=6, loss=-88378.81983274473\n",
      "Current iteration=8, loss=-121098.82426395519\n",
      "loss=-152014.80753080646\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9794.981886676796\n",
      "Current iteration=4, loss=-52949.940858602626\n",
      "Current iteration=6, loss=-89217.13842667644\n",
      "Current iteration=8, loss=-122170.39957801503\n",
      "loss=-153323.74683076935\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9801.323153225376\n",
      "Current iteration=4, loss=-52966.78707771434\n",
      "Current iteration=6, loss=-89246.48449192714\n",
      "Current iteration=8, loss=-122208.11551958774\n",
      "loss=-153366.03756712895\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9279.303518695899\n",
      "Current iteration=4, loss=-52045.7309714933\n",
      "Current iteration=6, loss=-87972.78008063327\n",
      "Current iteration=8, loss=-120600.73348308454\n",
      "loss=-151432.7107930203\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9449.86433331881\n",
      "Current iteration=4, loss=-52346.27767935092\n",
      "Current iteration=6, loss=-88377.37543555902\n",
      "Current iteration=8, loss=-121096.40737095533\n",
      "loss=-152013.209614872\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9794.465781712488\n",
      "Current iteration=4, loss=-52947.99506995413\n",
      "Current iteration=6, loss=-89213.09404167255\n",
      "Current iteration=8, loss=-122163.6301292327\n",
      "loss=-153319.26790332128\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-9800.806994528542\n",
      "Current iteration=4, loss=-52964.841004151676\n",
      "Current iteration=6, loss=-89242.4393638664\n",
      "Current iteration=8, loss=-122201.34489060187\n",
      "loss=-153361.55819459536\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-9278.791728799928\n",
      "Current iteration=4, loss=-52043.80237778716\n",
      "Current iteration=6, loss=-87968.7726872545\n",
      "Current iteration=8, loss=-120594.02788834393\n",
      "loss=-151428.2771908863\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-9449.351122459082\n",
      "Current iteration=4, loss=-52344.34346769703\n",
      "Current iteration=6, loss=-88373.35641944394\n",
      "Current iteration=8, loss=-121089.6824687022\n",
      "loss=-152008.76340651774\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11411.294599057905\n",
      "Current iteration=4, loss=-55469.268106458265\n",
      "Current iteration=6, loss=-92526.5827355605\n",
      "Current iteration=8, loss=-126257.84869453369\n",
      "loss=-158195.99101309985\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11417.740662931978\n",
      "Current iteration=4, loss=-55486.90216218661\n",
      "Current iteration=6, loss=-92556.87446419118\n",
      "Current iteration=8, loss=-126296.31116590626\n",
      "loss=-158238.73960827562\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10881.303607193844\n",
      "Current iteration=4, loss=-54541.95818145174\n",
      "Current iteration=6, loss=-91250.33427517972\n",
      "Current iteration=8, loss=-124646.85729917033\n",
      "loss=-156253.7777935487\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11056.68573678647\n",
      "Current iteration=4, loss=-54849.86740991672\n",
      "Current iteration=6, loss=-91664.1903438882\n",
      "Current iteration=8, loss=-125153.72142024772\n",
      "loss=-156847.5451021376\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11411.294447002894\n",
      "Current iteration=4, loss=-55469.267534562874\n",
      "Current iteration=6, loss=-92526.58154705622\n",
      "Current iteration=8, loss=-126257.84670445278\n",
      "loss=-158195.98969402607\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11417.740510861386\n",
      "Current iteration=4, loss=-55486.90159020582\n",
      "Current iteration=6, loss=-92556.8732754688\n",
      "Current iteration=8, loss=-126296.30917548401\n",
      "loss=-158238.73828907867\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10881.303456412497\n",
      "Current iteration=4, loss=-54541.9576146207\n",
      "Current iteration=6, loss=-91250.33309757682\n",
      "Current iteration=8, loss=-124646.85532792418\n",
      "loss=-156253.77648787393\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11056.685585585563\n",
      "Current iteration=4, loss=-54849.86684143455\n",
      "Current iteration=6, loss=-91664.18916287257\n",
      "Current iteration=8, loss=-125153.71944332891\n",
      "loss=-156847.5437927472\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11411.294023900817\n",
      "Current iteration=4, loss=-55469.2659432299\n",
      "Current iteration=6, loss=-92526.57823997259\n",
      "Current iteration=8, loss=-126257.84116693442\n",
      "loss=-158195.98602362484\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11417.740087715936\n",
      "Current iteration=4, loss=-55486.899998635236\n",
      "Current iteration=6, loss=-92556.86996777827\n",
      "Current iteration=8, loss=-126296.30363701597\n",
      "loss=-158238.73461833474\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10881.303036854448\n",
      "Current iteration=4, loss=-54541.9560373797\n",
      "Current iteration=6, loss=-91250.32982082697\n",
      "Current iteration=8, loss=-124646.84984281461\n",
      "loss=-156253.77285475645\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11056.68516486008\n",
      "Current iteration=4, loss=-54849.865259599224\n",
      "Current iteration=6, loss=-91664.18587662645\n",
      "Current iteration=8, loss=-125153.71394243512\n",
      "loss=-156847.54014929075\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11411.292846594124\n",
      "Current iteration=4, loss=-55469.26151525162\n",
      "Current iteration=6, loss=-92526.56903781646\n",
      "Current iteration=8, loss=-126257.82575846197\n",
      "loss=-158195.97581051587\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11417.738910288634\n",
      "Current iteration=4, loss=-55486.89556999586\n",
      "Current iteration=6, loss=-92556.86076393332\n",
      "Current iteration=8, loss=-126296.28822590085\n",
      "loss=-158238.72440427216\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10881.301869409286\n",
      "Current iteration=4, loss=-54541.95164861308\n",
      "Current iteration=6, loss=-91250.32070307634\n",
      "Current iteration=8, loss=-124646.83458017271\n",
      "loss=-156253.76274539152\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11056.68399416638\n",
      "Current iteration=4, loss=-54849.86085804862\n",
      "Current iteration=6, loss=-91664.17673245193\n",
      "Current iteration=8, loss=-125153.69863587272\n",
      "loss=-156847.5300111573\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11411.289570668458\n",
      "Current iteration=4, loss=-55469.24919414021\n",
      "Current iteration=6, loss=-92526.5434322745\n",
      "Current iteration=8, loss=-126257.78288348191\n",
      "loss=-158195.94739193714\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11417.73563402731\n",
      "Current iteration=4, loss=-55486.883247044774\n",
      "Current iteration=6, loss=-92556.83515369224\n",
      "Current iteration=8, loss=-126296.24534356725\n",
      "loss=-158238.6959830397\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10881.298620923866\n",
      "Current iteration=4, loss=-54541.9394366105\n",
      "Current iteration=6, loss=-91250.29533239767\n",
      "Current iteration=8, loss=-124646.79211097461\n",
      "loss=-156253.73461548664\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11056.68073664179\n",
      "Current iteration=4, loss=-54849.848610473804\n",
      "Current iteration=6, loss=-91664.1512882473\n",
      "Current iteration=8, loss=-125153.6560444631\n",
      "loss=-156847.50180120216\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11411.280455211507\n",
      "Current iteration=4, loss=-55469.21490992501\n",
      "Current iteration=6, loss=-92526.47218336498\n",
      "Current iteration=8, loss=-126257.66358137799\n",
      "loss=-158195.86831558033\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11417.726517636424\n",
      "Current iteration=4, loss=-55486.848957710725\n",
      "Current iteration=6, loss=-92556.7638917072\n",
      "Current iteration=8, loss=-126296.12602100223\n",
      "loss=-158238.61689929935\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10881.289581821073\n",
      "Current iteration=4, loss=-54541.90545599703\n",
      "Current iteration=6, loss=-91250.22473700886\n",
      "Current iteration=8, loss=-124646.67393798268\n",
      "loss=-156253.65634238173\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11056.671672386961\n",
      "Current iteration=4, loss=-54849.814530878524\n",
      "Current iteration=6, loss=-91664.08048826827\n",
      "Current iteration=8, loss=-125153.53753141058\n",
      "loss=-156847.42330535277\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11411.255090917564\n",
      "Current iteration=4, loss=-55469.119512131445\n",
      "Current iteration=6, loss=-92526.27392929\n",
      "Current iteration=8, loss=-126257.33161676941\n",
      "loss=-158195.64828112893\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11417.70115074374\n",
      "Current iteration=4, loss=-55486.753545673586\n",
      "Current iteration=6, loss=-92556.56560124867\n",
      "Current iteration=8, loss=-126295.79399945911\n",
      "loss=-158238.3968443023\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10881.264429987046\n",
      "Current iteration=4, loss=-54541.81090299261\n",
      "Current iteration=6, loss=-91250.02830139172\n",
      "Current iteration=8, loss=-124646.34511518937\n",
      "loss=-156253.43854302412\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11056.646450565928\n",
      "Current iteration=4, loss=-54849.71970245153\n",
      "Current iteration=6, loss=-91663.88348336735\n",
      "Current iteration=8, loss=-125153.20776238036\n",
      "loss=-156847.20488619595\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11411.184513313558\n",
      "Current iteration=4, loss=-55468.85406265936\n",
      "Current iteration=6, loss=-92525.72227746963\n",
      "Current iteration=8, loss=-126256.40791003482\n",
      "loss=-158195.03602381097\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11417.630565908565\n",
      "Current iteration=4, loss=-55486.48805656798\n",
      "Current iteration=6, loss=-92556.01384818928\n",
      "Current iteration=8, loss=-126294.87013430143\n",
      "loss=-158237.78452981496\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10881.194443564975\n",
      "Current iteration=4, loss=-54541.5478041916\n",
      "Current iteration=6, loss=-91249.48170952075\n",
      "Current iteration=8, loss=-124645.4301506996\n",
      "loss=-156252.83250496993\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11056.57626940095\n",
      "Current iteration=4, loss=-54849.45583727241\n",
      "Current iteration=6, loss=-91663.33530743586\n",
      "Current iteration=8, loss=-125152.2901649429\n",
      "loss=-156846.59712351835\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11410.988127329301\n",
      "Current iteration=4, loss=-55468.115438035\n",
      "Current iteration=6, loss=-92524.18728837704\n",
      "Current iteration=8, loss=-126253.8376760812\n",
      "loss=-158193.3323939497\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11417.43415980322\n",
      "Current iteration=4, loss=-55485.74932166154\n",
      "Current iteration=6, loss=-92554.47857739504\n",
      "Current iteration=8, loss=-126292.29945953029\n",
      "loss=-158236.0807408761\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10880.999702576344\n",
      "Current iteration=4, loss=-54540.815720411054\n",
      "Current iteration=6, loss=-91247.96079990017\n",
      "Current iteration=8, loss=-124642.884242228\n",
      "loss=-156251.14618044274\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11056.380986529692\n",
      "Current iteration=4, loss=-54848.72162101156\n",
      "Current iteration=6, loss=-91661.80999011552\n",
      "Current iteration=8, loss=-125149.73693023862\n",
      "loss=-156844.90600016172\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11410.441674692362\n",
      "Current iteration=4, loss=-55466.0602044351\n",
      "Current iteration=6, loss=-92519.9162051517\n",
      "Current iteration=8, loss=-126246.68611768048\n",
      "loss=-158188.59203940522\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-11416.887651178704\n",
      "Current iteration=4, loss=-55483.69378119971\n",
      "Current iteration=6, loss=-92550.20671033367\n",
      "Current iteration=8, loss=-126285.14667456156\n",
      "loss=-158231.33994368705\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-10880.457827212967\n",
      "Current iteration=4, loss=-54538.77868680225\n",
      "Current iteration=6, loss=-91243.72889257631\n",
      "Current iteration=8, loss=-124635.8003683449\n",
      "loss=-156246.4539779611\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-11055.837603353528\n",
      "Current iteration=4, loss=-54846.67865374415\n",
      "Current iteration=6, loss=-91657.56581843497\n",
      "Current iteration=8, loss=-125142.63267145613\n",
      "loss=-156840.20044494135\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13014.254711038897\n",
      "Current iteration=4, loss=-57964.981129759275\n",
      "Current iteration=6, loss=-95808.50248200409\n",
      "Current iteration=8, loss=-130316.75351721798\n",
      "loss=-163042.59651696146\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13020.816289920847\n",
      "Current iteration=4, loss=-57983.40576030033\n",
      "Current iteration=6, loss=-95839.70511663014\n",
      "Current iteration=8, loss=-130355.9088343778\n",
      "loss=-163085.74863214768\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12470.04729672474\n",
      "Current iteration=4, loss=-57014.72395912675\n",
      "Current iteration=6, loss=-94500.4805485223\n",
      "Current iteration=8, loss=-128664.51038800196\n",
      "loss=-161049.22566619058\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12650.212820895425\n",
      "Current iteration=4, loss=-57329.90422270671\n",
      "Current iteration=6, loss=-94923.49363692346\n",
      "Current iteration=8, loss=-129182.47194900713\n",
      "loss=-161656.19455782455\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13014.254550327365\n",
      "Current iteration=4, loss=-57964.98052668383\n",
      "Current iteration=6, loss=-95808.50122886384\n",
      "Current iteration=8, loss=-130316.75141809319\n",
      "loss=-163042.59512311366\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13020.816129193054\n",
      "Current iteration=4, loss=-57983.4051571332\n",
      "Current iteration=6, loss=-95839.7038632605\n",
      "Current iteration=8, loss=-130355.9067348991\n",
      "loss=-163085.74723817746\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12470.047137361822\n",
      "Current iteration=4, loss=-57014.7233614028\n",
      "Current iteration=6, loss=-94500.47930690856\n",
      "Current iteration=8, loss=-128664.50830880928\n",
      "loss=-161049.22428655258\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12650.21266108812\n",
      "Current iteration=4, loss=-57329.903623241844\n",
      "Current iteration=6, loss=-94923.49239171408\n",
      "Current iteration=8, loss=-129182.46986383422\n",
      "loss=-161656.19317425677\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13014.254103137964\n",
      "Current iteration=4, loss=-57964.97884859064\n",
      "Current iteration=6, loss=-95808.49774192674\n",
      "Current iteration=8, loss=-130316.74557715403\n",
      "loss=-163042.59124464932\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13020.815681958495\n",
      "Current iteration=4, loss=-57983.40347878489\n",
      "Current iteration=6, loss=-95839.70037568502\n",
      "Current iteration=8, loss=-130355.90089297507\n",
      "loss=-163085.74335937257\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12470.046693925002\n",
      "Current iteration=4, loss=-57014.72169820045\n",
      "Current iteration=6, loss=-94500.4758520445\n",
      "Current iteration=8, loss=-128664.502523332\n",
      "loss=-161049.2204476278\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12650.212216414817\n",
      "Current iteration=4, loss=-57329.90195519523\n",
      "Current iteration=6, loss=-94923.48892684517\n",
      "Current iteration=8, loss=-129182.4640617169\n",
      "loss=-161656.18932439736\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13014.252858806887\n",
      "Current iteration=4, loss=-57964.97417919675\n",
      "Current iteration=6, loss=-95808.48803931769\n",
      "Current iteration=8, loss=-130316.72932439538\n",
      "loss=-163042.5804525924\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13020.814437501716\n",
      "Current iteration=4, loss=-57983.39880868116\n",
      "Current iteration=6, loss=-95839.69067129967\n",
      "Current iteration=8, loss=-130355.88463747615\n",
      "loss=-163085.7325663681\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12470.045460035757\n",
      "Current iteration=4, loss=-57014.71707024126\n",
      "Current iteration=6, loss=-94500.46623868062\n",
      "Current iteration=8, loss=-128664.48642489921\n",
      "loss=-161049.20976559207\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12650.210979085008\n",
      "Current iteration=4, loss=-57329.89731375673\n",
      "Current iteration=6, loss=-94923.47928564211\n",
      "Current iteration=8, loss=-129182.44791698206\n",
      "loss=-161656.17861193515\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13014.249396381892\n",
      "Current iteration=4, loss=-57964.96118633215\n",
      "Current iteration=6, loss=-95808.46104123609\n",
      "Current iteration=8, loss=-130316.68410013944\n",
      "loss=-163042.55042305688\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13020.81097472694\n",
      "Current iteration=4, loss=-57983.385813841436\n",
      "Current iteration=6, loss=-95839.66366827555\n",
      "Current iteration=8, loss=-130355.8394055952\n",
      "loss=-163085.7025341958\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12470.042026665687\n",
      "Current iteration=4, loss=-57014.70419267118\n",
      "Current iteration=6, loss=-94500.43948892906\n",
      "Current iteration=8, loss=-128664.4416300641\n",
      "loss=-161049.18004219682\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12650.207536141334\n",
      "Current iteration=4, loss=-57329.88439867959\n",
      "Current iteration=6, loss=-94923.45245842631\n",
      "Current iteration=8, loss=-129182.40299330905\n",
      "loss=-161656.1488038767\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13014.239761979547\n",
      "Current iteration=4, loss=-57964.92503292476\n",
      "Current iteration=6, loss=-95808.38591750528\n",
      "Current iteration=8, loss=-130316.55826104271\n",
      "loss=-163042.4668641199\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13020.801339351343\n",
      "Current iteration=4, loss=-57983.34965493796\n",
      "Current iteration=6, loss=-95839.58853079163\n",
      "Current iteration=8, loss=-130355.71354528131\n",
      "loss=-163085.6189679216\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12470.032473110525\n",
      "Current iteration=4, loss=-57014.66836007751\n",
      "Current iteration=6, loss=-94500.36505619081\n",
      "Current iteration=8, loss=-128664.3169858556\n",
      "loss=-161049.09733511307\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12650.197955947027\n",
      "Current iteration=4, loss=-57329.84846172026\n",
      "Current iteration=6, loss=-94923.37781013949\n",
      "Current iteration=8, loss=-129182.2779906014\n",
      "loss=-161656.06586121238\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13014.212953689861\n",
      "Current iteration=4, loss=-57964.82443399908\n",
      "Current iteration=6, loss=-95808.17688153149\n",
      "Current iteration=8, loss=-130316.2081069147\n",
      "loss=-163042.23435664174\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13020.774528353519\n",
      "Current iteration=4, loss=-57983.24904071929\n",
      "Current iteration=6, loss=-95839.37945654933\n",
      "Current iteration=8, loss=-130355.36333211529\n",
      "loss=-163085.38644002748\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12470.005889782855\n",
      "Current iteration=4, loss=-57014.56865383446\n",
      "Current iteration=6, loss=-94500.15794294255\n",
      "Current iteration=8, loss=-128663.9701565689\n",
      "loss=-161048.86719796452\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12650.171298494388\n",
      "Current iteration=4, loss=-57329.74846507383\n",
      "Current iteration=6, loss=-94923.17009711513\n",
      "Current iteration=8, loss=-129181.93016377132\n",
      "loss=-161655.83506854787\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13014.138358086519\n",
      "Current iteration=4, loss=-57964.544512117034\n",
      "Current iteration=6, loss=-95807.59522860643\n",
      "Current iteration=8, loss=-130315.23378715885\n",
      "loss=-163041.58739253448\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13020.699925214582\n",
      "Current iteration=4, loss=-57982.96907628357\n",
      "Current iteration=6, loss=-95838.79769714003\n",
      "Current iteration=8, loss=-130354.38884808356\n",
      "loss=-163084.73941911157\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12469.931920149122\n",
      "Current iteration=4, loss=-57014.29121588934\n",
      "Current iteration=6, loss=-94499.581640096\n",
      "Current iteration=8, loss=-128663.00508833458\n",
      "loss=-161048.2268294211\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12650.097122603736\n",
      "Current iteration=4, loss=-57329.47021906577\n",
      "Current iteration=6, loss=-94922.59212536197\n",
      "Current iteration=8, loss=-129180.96231982757\n",
      "loss=-161655.19287599745\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13013.930791816096\n",
      "Current iteration=4, loss=-57963.76561753179\n",
      "Current iteration=6, loss=-95805.97676096599\n",
      "Current iteration=8, loss=-130312.52272254266\n",
      "loss=-163039.7871901194\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13020.49233797604\n",
      "Current iteration=4, loss=-57982.190063290924\n",
      "Current iteration=6, loss=-95837.1789332031\n",
      "Current iteration=8, loss=-130351.67732636449\n",
      "loss=-163082.93905862284\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12469.726095672975\n",
      "Current iteration=4, loss=-57013.51923296477\n",
      "Current iteration=6, loss=-94497.97805921674\n",
      "Current iteration=8, loss=-128660.31976626294\n",
      "loss=-161046.44497940483\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12649.890724206454\n",
      "Current iteration=4, loss=-57328.695987671315\n",
      "Current iteration=6, loss=-94920.9839006973\n",
      "Current iteration=8, loss=-129178.26927428892\n",
      "loss=-161653.4059506142\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-13013.35322964524\n",
      "Current iteration=4, loss=-57961.59833331243\n",
      "Current iteration=6, loss=-95801.47340385147\n",
      "Current iteration=8, loss=-130304.97931827031\n",
      "loss=-163034.77812560575\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-13019.91471746077\n",
      "Current iteration=4, loss=-57980.0224496008\n",
      "Current iteration=6, loss=-95832.67475164261\n",
      "Current iteration=8, loss=-130344.13265021153\n",
      "loss=-163077.92955425798\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-12469.153380121299\n",
      "Current iteration=4, loss=-57011.37118052968\n",
      "Current iteration=6, loss=-94493.51612423695\n",
      "Current iteration=8, loss=-128652.84798933398\n",
      "loss=-161041.48698037487\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-12649.316411693815\n",
      "Current iteration=4, loss=-57326.54167883818\n",
      "Current iteration=6, loss=-94916.50904446775\n",
      "Current iteration=8, loss=-129170.77600720186\n",
      "loss=-161648.43382939146\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14604.553818342134\n",
      "Current iteration=4, loss=-60439.21540433939\n",
      "Current iteration=6, loss=-99066.6162234088\n",
      "Current iteration=8, loss=-134352.52839953263\n",
      "loss=-167867.70083056658\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14611.242016970069\n",
      "Current iteration=4, loss=-60458.43149795678\n",
      "Current iteration=6, loss=-99098.69511712807\n",
      "Current iteration=8, loss=-134392.3258326365\n",
      "loss=-167911.20638028186\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14046.221590253519\n",
      "Current iteration=4, loss=-59466.14450245419\n",
      "Current iteration=6, loss=-97726.90574934964\n",
      "Current iteration=8, loss=-132659.06298193723\n",
      "loss=-165823.16163026067\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14231.133675394307\n",
      "Current iteration=4, loss=-59788.51091617334\n",
      "Current iteration=6, loss=-98158.98501839716\n",
      "Current iteration=8, loss=-133188.04716531583\n",
      "loss=-166443.27896833105\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14604.55364876348\n",
      "Current iteration=4, loss=-60439.21476936138\n",
      "Current iteration=6, loss=-99066.61490408296\n",
      "Current iteration=8, loss=-134352.52618865643\n",
      "loss=-167867.69935991918\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14611.241847374526\n",
      "Current iteration=4, loss=-60458.430862880625\n",
      "Current iteration=6, loss=-99098.6937975616\n",
      "Current iteration=8, loss=-134392.32362139403\n",
      "loss=-167911.20490951286\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14046.22142210041\n",
      "Current iteration=4, loss=-59466.143873122244\n",
      "Current iteration=6, loss=-97726.90444219264\n",
      "Current iteration=8, loss=-132659.06079212143\n",
      "loss=-165823.1601746578\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14231.133506771408\n",
      "Current iteration=4, loss=-59788.51028500871\n",
      "Current iteration=6, loss=-98158.98370745753\n",
      "Current iteration=8, loss=-133188.0449692046\n",
      "loss=-166443.27750857803\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14604.553176900812\n",
      "Current iteration=4, loss=-60439.213002497374\n",
      "Current iteration=6, loss=-99066.61123298066\n",
      "Current iteration=8, loss=-134352.52003676214\n",
      "loss=-167867.69526775548\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14611.24137546478\n",
      "Current iteration=4, loss=-60458.42909574355\n",
      "Current iteration=6, loss=-99098.69012578967\n",
      "Current iteration=8, loss=-134392.31746848067\n",
      "loss=-167911.2008170109\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14046.220954204397\n",
      "Current iteration=4, loss=-59466.1421219687\n",
      "Current iteration=6, loss=-97726.90080495077\n",
      "Current iteration=8, loss=-132659.05469882916\n",
      "loss=-165823.15612435646\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14231.133037568121\n",
      "Current iteration=4, loss=-59788.508528755694\n",
      "Current iteration=6, loss=-98158.98005969024\n",
      "Current iteration=8, loss=-133188.03885839478\n",
      "loss=-166443.2734467285\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14604.551863914976\n",
      "Current iteration=4, loss=-60439.20808609352\n",
      "Current iteration=6, loss=-99066.6010179211\n",
      "Current iteration=8, loss=-134352.50291875235\n",
      "loss=-167867.68388106747\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14611.240062347919\n",
      "Current iteration=4, loss=-60458.42417857978\n",
      "Current iteration=6, loss=-99098.67990886675\n",
      "Current iteration=8, loss=-134392.30034763514\n",
      "loss=-167911.1894293817\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14046.21965225597\n",
      "Current iteration=4, loss=-59466.137249280284\n",
      "Current iteration=6, loss=-97726.89068410984\n",
      "Current iteration=8, loss=-132659.03774388306\n",
      "loss=-165823.1448541531\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14231.131731982152\n",
      "Current iteration=4, loss=-59788.50364187748\n",
      "Current iteration=6, loss=-98158.9699095618\n",
      "Current iteration=8, loss=-133188.02185470494\n",
      "loss=-166443.26214439137\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14604.548210453986\n",
      "Current iteration=4, loss=-60439.194405909155\n",
      "Current iteration=6, loss=-99066.57259391577\n",
      "Current iteration=8, loss=-134352.45528688488\n",
      "loss=-167867.65219693587\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14611.236408522349\n",
      "Current iteration=4, loss=-60458.410496280994\n",
      "Current iteration=6, loss=-99098.65147967677\n",
      "Current iteration=8, loss=-134392.25270787702\n",
      "loss=-167911.15774263107\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14046.216029507252\n",
      "Current iteration=4, loss=-59466.123690736604\n",
      "Current iteration=6, loss=-97726.86252227353\n",
      "Current iteration=8, loss=-132658.99056574964\n",
      "loss=-165823.11349414688\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14231.128099111746\n",
      "Current iteration=4, loss=-59788.49004384999\n",
      "Current iteration=6, loss=-98158.94166623121\n",
      "Current iteration=8, loss=-133187.97454093932\n",
      "loss=-166443.2306949713\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14604.538044482764\n",
      "Current iteration=4, loss=-60439.15633999447\n",
      "Current iteration=6, loss=-99066.49350247036\n",
      "Current iteration=8, loss=-134352.32274847306\n",
      "loss=-167867.56403398962\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14611.226241536664\n",
      "Current iteration=4, loss=-60458.3724244828\n",
      "Current iteration=6, loss=-99098.5723738046\n",
      "Current iteration=8, loss=-134392.12014750906\n",
      "loss=-167911.06957239722\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14046.205948994768\n",
      "Current iteration=4, loss=-59466.08596329436\n",
      "Current iteration=6, loss=-97726.78416032862\n",
      "Current iteration=8, loss=-132658.8592898788\n",
      "loss=-165823.0262330982\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14231.117990435047\n",
      "Current iteration=4, loss=-59788.45220654161\n",
      "Current iteration=6, loss=-98158.86307752384\n",
      "Current iteration=8, loss=-133187.84288766413\n",
      "loss=-166443.14318512328\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14604.50975707168\n",
      "Current iteration=4, loss=-60439.05041941043\n",
      "Current iteration=6, loss=-99066.27342611815\n",
      "Current iteration=8, loss=-134351.95395315663\n",
      "loss=-167867.31871560254\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14611.197951302844\n",
      "Current iteration=4, loss=-60458.2664875275\n",
      "Current iteration=6, loss=-99098.35225730926\n",
      "Current iteration=8, loss=-134391.75129109854\n",
      "loss=-167910.82423373187\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14046.177899377646\n",
      "Current iteration=4, loss=-59465.98098452902\n",
      "Current iteration=6, loss=-97726.56611385221\n",
      "Current iteration=8, loss=-132658.49400765134\n",
      "loss=-165822.7834242919\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14231.08986244934\n",
      "Current iteration=4, loss=-59788.3469220677\n",
      "Current iteration=6, loss=-98158.64440006818\n",
      "Current iteration=8, loss=-133187.47655528857\n",
      "loss=-166442.89968401886\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14604.431045730042\n",
      "Current iteration=4, loss=-60438.755689749574\n",
      "Current iteration=6, loss=-99065.66105286253\n",
      "Current iteration=8, loss=-134350.9277636063\n",
      "loss=-167866.63610453732\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14611.11923210677\n",
      "Current iteration=4, loss=-60457.97171231279\n",
      "Current iteration=6, loss=-99097.7397723532\n",
      "Current iteration=8, loss=-134390.724931551\n",
      "loss=-167910.14156624133\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14046.099849711329\n",
      "Current iteration=4, loss=-59465.688875528904\n",
      "Current iteration=6, loss=-97725.9593888255\n",
      "Current iteration=8, loss=-132657.47759342857\n",
      "loss=-165822.1077962636\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14231.011594718066\n",
      "Current iteration=4, loss=-59788.05396241706\n",
      "Current iteration=6, loss=-98158.03591931086\n",
      "Current iteration=8, loss=-133186.4572189818\n",
      "loss=-166442.22212963566\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14604.21202721112\n",
      "Current iteration=4, loss=-60437.935592040914\n",
      "Current iteration=6, loss=-99063.95710545874\n",
      "Current iteration=8, loss=-134348.07237145316\n",
      "loss=-167864.7367135609\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14610.900191732482\n",
      "Current iteration=4, loss=-60457.15148784847\n",
      "Current iteration=6, loss=-99096.03551413845\n",
      "Current iteration=8, loss=-134387.86906637513\n",
      "loss=-167908.24201825782\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14045.882672339536\n",
      "Current iteration=4, loss=-59464.87606991925\n",
      "Current iteration=6, loss=-97724.27115779073\n",
      "Current iteration=8, loss=-132654.64940130888\n",
      "loss=-165820.22783583918\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14230.793810568655\n",
      "Current iteration=4, loss=-59787.238789836134\n",
      "Current iteration=6, loss=-98156.3428029015\n",
      "Current iteration=8, loss=-133183.62089610897\n",
      "loss=-166440.33680905908\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14603.602598771029\n",
      "Current iteration=4, loss=-60435.653660785436\n",
      "Current iteration=6, loss=-99059.21590651733\n",
      "Current iteration=8, loss=-134340.1273923762\n",
      "loss=-167859.4516604036\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-14610.29070247922\n",
      "Current iteration=4, loss=-60454.86920389317\n",
      "Current iteration=6, loss=-99091.29345036532\n",
      "Current iteration=8, loss=-134379.92277112257\n",
      "loss=-167902.95652821765\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-14045.278366971404\n",
      "Current iteration=4, loss=-59462.614429012014\n",
      "Current iteration=6, loss=-97719.57368931486\n",
      "Current iteration=8, loss=-132646.7801048767\n",
      "loss=-165814.99684808715\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-14230.187816815293\n",
      "Current iteration=4, loss=-59784.97056280264\n",
      "Current iteration=6, loss=-98151.63174097046\n",
      "Current iteration=8, loss=-133175.72897629315\n",
      "loss=-166435.09090671124\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16182.580873744335\n",
      "Current iteration=4, loss=-62892.956036774805\n",
      "Current iteration=6, loss=-102302.27440512278\n",
      "Current iteration=8, loss=-138366.6623567542\n",
      "loss=-172672.7870191484\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16189.407077054098\n",
      "Current iteration=4, loss=-62912.962638905585\n",
      "Current iteration=6, loss=-102335.19484793402\n",
      "Current iteration=8, loss=-138407.05351751714\n",
      "loss=-172716.5997441669\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15610.213250451032\n",
      "Current iteration=4, loss=-61897.195858324\n",
      "Current iteration=6, loss=-100930.95040051806\n",
      "Current iteration=8, loss=-136631.99685878045\n",
      "loss=-170577.06475862794\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15799.835390784063\n",
      "Current iteration=4, loss=-62226.666898066534\n",
      "Current iteration=6, loss=-101372.01093930294\n",
      "Current iteration=8, loss=-137171.93538720947\n",
      "loss=-171210.28341020845\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16182.580695089373\n",
      "Current iteration=4, loss=-62892.955369174015\n",
      "Current iteration=6, loss=-102302.27301806374\n",
      "Current iteration=8, loss=-138366.66003142056\n",
      "loss=-172672.78546967715\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16189.406898381463\n",
      "Current iteration=4, loss=-62912.96197119998\n",
      "Current iteration=6, loss=-102335.19346062318\n",
      "Current iteration=8, loss=-138407.0511918052\n",
      "loss=-172716.59819457488\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15610.213073300483\n",
      "Current iteration=4, loss=-61897.19519667129\n",
      "Current iteration=6, loss=-100930.94902628752\n",
      "Current iteration=8, loss=-136631.9945556672\n",
      "loss=-170577.06322506006\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15799.83521313765\n",
      "Current iteration=4, loss=-62226.66623448746\n",
      "Current iteration=6, loss=-101372.00956109846\n",
      "Current iteration=8, loss=-137171.93307747756\n",
      "loss=-171210.28187226347\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16182.580197971241\n",
      "Current iteration=4, loss=-62892.953511535125\n",
      "Current iteration=6, loss=-102302.26915848968\n",
      "Current iteration=8, loss=-138366.65356104187\n",
      "loss=-172672.7811581814\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16189.406401214288\n",
      "Current iteration=4, loss=-62912.96011326962\n",
      "Current iteration=6, loss=-102335.18960034843\n",
      "Current iteration=8, loss=-138407.0447203738\n",
      "loss=-172716.59388274318\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15610.212580368563\n",
      "Current iteration=4, loss=-61897.193355583324\n",
      "Current iteration=6, loss=-100930.94520240946\n",
      "Current iteration=8, loss=-136631.98814711787\n",
      "loss=-170577.05895781642\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15799.834718825903\n",
      "Current iteration=4, loss=-62226.66438803925\n",
      "Current iteration=6, loss=-101372.00572616264\n",
      "Current iteration=8, loss=-137171.92665051136\n",
      "loss=-171210.2775928402\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16182.578814710623\n",
      "Current iteration=4, loss=-62892.94834254489\n",
      "Current iteration=6, loss=-102302.25841899637\n",
      "Current iteration=8, loss=-138366.63555683038\n",
      "loss=-172672.76916118895\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16189.405017817027\n",
      "Current iteration=4, loss=-62912.95494346819\n",
      "Current iteration=6, loss=-102335.17885890539\n",
      "Current iteration=8, loss=-138407.0267132333\n",
      "loss=-172716.58188481597\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15610.211208756193\n",
      "Current iteration=4, loss=-61897.18823264692\n",
      "Current iteration=6, loss=-100930.9345622424\n",
      "Current iteration=8, loss=-136631.97031495033\n",
      "loss=-170577.04708395814\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15799.833343374194\n",
      "Current iteration=4, loss=-62226.6592501877\n",
      "Current iteration=6, loss=-101371.99505522661\n",
      "Current iteration=8, loss=-137171.90876709792\n",
      "loss=-171210.26568509146\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16182.57496570587\n",
      "Current iteration=4, loss=-62892.933959524016\n",
      "Current iteration=6, loss=-102302.22853572354\n",
      "Current iteration=8, loss=-138366.58545905488\n",
      "loss=-172672.73577884943\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16189.401168432189\n",
      "Current iteration=4, loss=-62912.94055819017\n",
      "Current iteration=6, loss=-102335.14897020732\n",
      "Current iteration=8, loss=-138406.97660730738\n",
      "loss=-172716.54849987512\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15610.20739216346\n",
      "Current iteration=4, loss=-61897.17397777363\n",
      "Current iteration=6, loss=-100930.90495535068\n",
      "Current iteration=8, loss=-136631.92069589705\n",
      "loss=-170577.01404424635\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15799.829516098162\n",
      "Current iteration=4, loss=-62226.64495381214\n",
      "Current iteration=6, loss=-101371.9653627185\n",
      "Current iteration=8, loss=-137171.85900545018\n",
      "loss=-171210.2325510777\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16182.56425562262\n",
      "Current iteration=4, loss=-62892.893937925925\n",
      "Current iteration=6, loss=-102302.14538378261\n",
      "Current iteration=8, loss=-138366.44605911485\n",
      "loss=-172672.64289054117\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16189.3904572913\n",
      "Current iteration=4, loss=-62912.90053031133\n",
      "Current iteration=6, loss=-102335.06580317041\n",
      "Current iteration=8, loss=-138406.8371846885\n",
      "loss=-172716.45560432895\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15610.19677226858\n",
      "Current iteration=4, loss=-61897.1343127537\n",
      "Current iteration=6, loss=-100930.82257245637\n",
      "Current iteration=8, loss=-136631.78262802935\n",
      "loss=-170576.92210932032\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15799.818866476418\n",
      "Current iteration=4, loss=-62226.60517330965\n",
      "Current iteration=6, loss=-101371.88274159169\n",
      "Current iteration=8, loss=-137171.7205408048\n",
      "loss=-171210.1403537506\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16182.534454188211\n",
      "Current iteration=4, loss=-62892.782575543584\n",
      "Current iteration=6, loss=-102301.91400888462\n",
      "Current iteration=8, loss=-138366.05817124576\n",
      "loss=-172672.38442357528\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16189.360652913874\n",
      "Current iteration=4, loss=-62912.789150452576\n",
      "Current iteration=6, loss=-102334.83438626713\n",
      "Current iteration=8, loss=-138406.44923371432\n",
      "loss=-172716.1971172227\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15610.167221788524\n",
      "Current iteration=4, loss=-61897.02394257039\n",
      "Current iteration=6, loss=-100930.59333747339\n",
      "Current iteration=8, loss=-136631.39844672344\n",
      "loss=-170576.66629519378\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15799.789233279607\n",
      "Current iteration=4, loss=-62226.494481789654\n",
      "Current iteration=6, loss=-101371.6528437136\n",
      "Current iteration=8, loss=-137171.3352554437\n",
      "loss=-171209.88380947863\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16182.45152999185\n",
      "Current iteration=4, loss=-62892.472703810316\n",
      "Current iteration=6, loss=-102301.2701969424\n",
      "Current iteration=8, loss=-138364.97885597573\n",
      "loss=-172671.66522597478\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16189.277720528478\n",
      "Current iteration=4, loss=-62912.4792300902\n",
      "Current iteration=6, loss=-102334.19045744294\n",
      "Current iteration=8, loss=-138405.36974285118\n",
      "loss=-172715.47786358083\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15610.084995887066\n",
      "Current iteration=4, loss=-61896.716831683574\n",
      "Current iteration=6, loss=-100929.95547994874\n",
      "Current iteration=8, loss=-136630.32944513005\n",
      "loss=-170575.9544792533\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15799.706777214054\n",
      "Current iteration=4, loss=-62226.18647676637\n",
      "Current iteration=6, loss=-101371.01314165085\n",
      "Current iteration=8, loss=-137170.26318176772\n",
      "loss=-171209.16996187143\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16182.220788994386\n",
      "Current iteration=4, loss=-62891.610472807966\n",
      "Current iteration=6, loss=-102299.47877098103\n",
      "Current iteration=8, loss=-138361.97564177442\n",
      "loss=-172669.66403207634\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16189.04695674456\n",
      "Current iteration=4, loss=-62911.616863775365\n",
      "Current iteration=6, loss=-102332.39870625311\n",
      "Current iteration=8, loss=-138402.36604005704\n",
      "loss=-172713.4765137437\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15609.85619793251\n",
      "Current iteration=4, loss=-61895.86228285145\n",
      "Current iteration=6, loss=-100928.18062233081\n",
      "Current iteration=8, loss=-136627.35492891716\n",
      "loss=-170573.97382508582\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15799.47733881557\n",
      "Current iteration=4, loss=-62225.32943996204\n",
      "Current iteration=6, loss=-101369.23315155068\n",
      "Current iteration=8, loss=-137167.28011743154\n",
      "loss=-171207.1836545191\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-16181.578742370191\n",
      "Current iteration=4, loss=-62889.21130643017\n",
      "Current iteration=6, loss=-102294.49416902744\n",
      "Current iteration=8, loss=-138353.6193655703\n",
      "loss=-172664.0957161981\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-16188.404846716485\n",
      "Current iteration=4, loss=-62909.21732088822\n",
      "Current iteration=6, loss=-102327.41319935114\n",
      "Current iteration=8, loss=-138394.00840435497\n",
      "loss=-172707.9077639559\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-15609.219557908935\n",
      "Current iteration=4, loss=-61893.48449218833\n",
      "Current iteration=6, loss=-100923.24212140372\n",
      "Current iteration=8, loss=-136619.0785032433\n",
      "loss=-170568.46266085858\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-15798.83891672886\n",
      "Current iteration=4, loss=-62222.94472648967\n",
      "Current iteration=6, loss=-101364.28036960907\n",
      "Current iteration=8, loss=-137158.97990709438\n",
      "loss=-171201.6567603462\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17748.711993005894\n",
      "Current iteration=4, loss=-65327.13327734839\n",
      "Current iteration=6, loss=-105516.7358779794\n",
      "Current iteration=8, loss=-142360.52778048976\n",
      "loss=-177459.21050459667\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17755.687795892984\n",
      "Current iteration=4, loss=-65347.927752957694\n",
      "Current iteration=6, loss=-105550.46330526946\n",
      "Current iteration=8, loss=-142401.46664208177\n",
      "loss=-177503.28778774265\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17162.396193121767\n",
      "Current iteration=4, loss=-64308.79976251165\n",
      "Current iteration=6, loss=-104113.86437373936\n",
      "Current iteration=8, loss=-140584.67811172546\n",
      "loss=-175312.28720753893\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17356.692243042162\n",
      "Current iteration=4, loss=-64645.29717136931\n",
      "Current iteration=6, loss=-104563.82681624871\n",
      "Current iteration=8, loss=-141135.50869515745\n",
      "loss=-175958.56542306175\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17748.71180506661\n",
      "Current iteration=4, loss=-65327.132576406635\n",
      "Current iteration=6, loss=-105516.73442164111\n",
      "Current iteration=8, loss=-142360.52533799454\n",
      "loss=-177459.2088742786\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17755.687607935277\n",
      "Current iteration=4, loss=-65347.927051904386\n",
      "Current iteration=6, loss=-105550.46184866827\n",
      "Current iteration=8, loss=-142401.46419919643\n",
      "loss=-177503.28615730465\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17162.396006767758\n",
      "Current iteration=4, loss=-64308.799067827495\n",
      "Current iteration=6, loss=-104113.86293090656\n",
      "Current iteration=8, loss=-140584.67569264173\n",
      "loss=-175312.285594007\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17356.692056165615\n",
      "Current iteration=4, loss=-64645.296474663126\n",
      "Current iteration=6, loss=-104563.82536924635\n",
      "Current iteration=8, loss=-141135.50626912416\n",
      "loss=-175958.56380491957\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17748.71128211437\n",
      "Current iteration=4, loss=-65327.13062599465\n",
      "Current iteration=6, loss=-105516.73036929335\n",
      "Current iteration=8, loss=-142360.51854160667\n",
      "loss=-177459.2043378219\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17755.687084931826\n",
      "Current iteration=4, loss=-65347.9251011821\n",
      "Current iteration=6, loss=-105550.45779558903\n",
      "Current iteration=8, loss=-142401.4574017231\n",
      "loss=-177503.28162051408\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17162.39548822665\n",
      "Current iteration=4, loss=-64308.79713482761\n",
      "Current iteration=6, loss=-104113.8589161387\n",
      "Current iteration=8, loss=-140584.66896139787\n",
      "loss=-175312.2811042588\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17356.69153617047\n",
      "Current iteration=4, loss=-64645.294536037\n",
      "Current iteration=6, loss=-104563.82134287637\n",
      "Current iteration=8, loss=-141135.49951854252\n",
      "loss=-175958.5593023427\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17748.70982696867\n",
      "Current iteration=4, loss=-65327.125198857706\n",
      "Current iteration=6, loss=-105516.71909339578\n",
      "Current iteration=8, loss=-142360.49963025557\n",
      "loss=-177459.19171486213\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17755.685629643704\n",
      "Current iteration=4, loss=-65347.919673181466\n",
      "Current iteration=6, loss=-105550.44651765595\n",
      "Current iteration=8, loss=-142401.43848735155\n",
      "loss=-177503.26899662567\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17162.394045355308\n",
      "Current iteration=4, loss=-64308.79175614083\n",
      "Current iteration=6, loss=-104113.84774480938\n",
      "Current iteration=8, loss=-140584.65023131392\n",
      "loss=-175312.26861126837\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17356.690089253098\n",
      "Current iteration=4, loss=-64645.28914169473\n",
      "Current iteration=6, loss=-104563.81013926347\n",
      "Current iteration=8, loss=-141135.4807346498\n",
      "loss=-175958.54677365566\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17748.705777939584\n",
      "Current iteration=4, loss=-65327.11009752848\n",
      "Current iteration=6, loss=-105516.6877175467\n",
      "Current iteration=8, loss=-142360.44700831114\n",
      "loss=-177459.1565907318\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17755.68158021817\n",
      "Current iteration=4, loss=-65347.90456944908\n",
      "Current iteration=6, loss=-105550.41513614297\n",
      "Current iteration=8, loss=-142401.3858570025\n",
      "loss=-177503.233869911\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17162.39003048018\n",
      "Current iteration=4, loss=-64308.77678962692\n",
      "Current iteration=6, loss=-104113.81665992746\n",
      "Current iteration=8, loss=-140584.5981137556\n",
      "loss=-175312.23384878496\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17356.686063119712\n",
      "Current iteration=4, loss=-64645.27413161859\n",
      "Current iteration=6, loss=-104563.77896455079\n",
      "Current iteration=8, loss=-141135.42846736583\n",
      "loss=-175958.51191184454\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17748.694511276753\n",
      "Current iteration=4, loss=-65327.068077195596\n",
      "Current iteration=6, loss=-105516.60041242716\n",
      "Current iteration=8, loss=-142360.3005847288\n",
      "loss=-177459.05885579035\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17755.670312452243\n",
      "Current iteration=4, loss=-65347.86254242926\n",
      "Current iteration=6, loss=-105550.32781526334\n",
      "Current iteration=8, loss=-142401.2394100339\n",
      "loss=-177503.13612777845\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17162.37885885284\n",
      "Current iteration=4, loss=-64308.73514442569\n",
      "Current iteration=6, loss=-104113.7301644409\n",
      "Current iteration=8, loss=-140584.45309365678\n",
      "loss=-175312.13712014724\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17356.67486016563\n",
      "Current iteration=4, loss=-64645.23236520285\n",
      "Current iteration=6, loss=-104563.6922191049\n",
      "Current iteration=8, loss=-141135.2830306466\n",
      "loss=-175958.41490682162\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17748.663161127217\n",
      "Current iteration=4, loss=-65326.951153222464\n",
      "Current iteration=6, loss=-105516.35748108849\n",
      "Current iteration=8, loss=-142359.893153218\n",
      "loss=-177458.7869028021\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17755.63895923338\n",
      "Current iteration=4, loss=-65347.745599849346\n",
      "Current iteration=6, loss=-105550.08484007143\n",
      "Current iteration=8, loss=-142400.83191344963\n",
      "loss=-177502.86415478066\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17162.3477731452\n",
      "Current iteration=4, loss=-64308.619264277746\n",
      "Current iteration=6, loss=-104113.48948595114\n",
      "Current iteration=8, loss=-140584.04956741436\n",
      "loss=-175311.86796725568\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17356.643687289452\n",
      "Current iteration=4, loss=-64645.11614776856\n",
      "Current iteration=6, loss=-104563.4508450892\n",
      "Current iteration=8, loss=-141134.87834513566\n",
      "loss=-175958.14498487275\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17748.57592754461\n",
      "Current iteration=4, loss=-65326.62580608973\n",
      "Current iteration=6, loss=-105515.68151286153\n",
      "Current iteration=8, loss=-142358.7594570693\n",
      "loss=-177458.03017971833\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17755.551717110066\n",
      "Current iteration=4, loss=-65347.42020094236\n",
      "Current iteration=6, loss=-105549.40874982034\n",
      "Current iteration=8, loss=-142399.69803623066\n",
      "loss=-177502.1073760192\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17162.261275387133\n",
      "Current iteration=4, loss=-64308.29682164373\n",
      "Current iteration=6, loss=-104112.81978638496\n",
      "Current iteration=8, loss=-140582.926737847\n",
      "loss=-175311.1190355827\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17356.55694697996\n",
      "Current iteration=4, loss=-64644.79276661758\n",
      "Current iteration=6, loss=-104562.77921018856\n",
      "Current iteration=8, loss=-141133.75228985262\n",
      "loss=-175957.39391325903\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17748.333195465864\n",
      "Current iteration=4, loss=-65325.720514313805\n",
      "Current iteration=6, loss=-105513.80061165983\n",
      "Current iteration=8, loss=-142355.60492844624\n",
      "loss=-177455.92457029098\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17755.308961266433\n",
      "Current iteration=4, loss=-65346.51476510218\n",
      "Current iteration=6, loss=-105547.52750908218\n",
      "Current iteration=8, loss=-142396.54300377468\n",
      "loss=-177500.00161166533\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17162.02059077955\n",
      "Current iteration=4, loss=-64307.39961175484\n",
      "Current iteration=6, loss=-104110.95632791237\n",
      "Current iteration=8, loss=-140579.8024456664\n",
      "loss=-175309.03510602654\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17356.315587460056\n",
      "Current iteration=4, loss=-64643.892945266474\n",
      "Current iteration=6, loss=-104560.910366592\n",
      "Current iteration=8, loss=-141130.61902206606\n",
      "loss=-175955.3040292436\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17747.65778327151\n",
      "Current iteration=4, loss=-65323.20153221778\n",
      "Current iteration=6, loss=-105508.56705139753\n",
      "Current iteration=8, loss=-142346.82763877322\n",
      "loss=-177450.065722508\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-17754.633482945763\n",
      "Current iteration=4, loss=-65343.99538214504\n",
      "Current iteration=6, loss=-105542.29300406\n",
      "Current iteration=8, loss=-142387.7643122003\n",
      "loss=-177494.14233278975\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-17161.350875760865\n",
      "Current iteration=4, loss=-64304.90311757759\n",
      "Current iteration=6, loss=-104105.77130159271\n",
      "Current iteration=8, loss=-140571.10928708676\n",
      "loss=-175303.23658228648\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-17355.643994468872\n",
      "Current iteration=4, loss=-64641.38918467138\n",
      "Current iteration=6, loss=-104555.71035630131\n",
      "Current iteration=8, loss=-141121.9008894083\n",
      "loss=-175949.48893727665\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19303.310817624\n",
      "Current iteration=4, loss=-67742.62637761059\n",
      "Current iteration=6, loss=-108711.17531372939\n",
      "Current iteration=8, loss=-146335.39117601406\n",
      "loss=-182228.2121229296\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19310.44796084598\n",
      "Current iteration=4, loss=-67764.20456767002\n",
      "Current iteration=6, loss=-108745.67548376323\n",
      "Current iteration=8, loss=-146376.83406911048\n",
      "loss=-182272.51480199437\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18703.131856895445\n",
      "Current iteration=4, loss=-66701.82746828\n",
      "Current iteration=6, loss=-107276.81421721789\n",
      "Current iteration=8, loss=-144518.3677710717\n",
      "loss=-180030.06716393202\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18902.06605831732\n",
      "Current iteration=4, loss=-67045.27615994477\n",
      "Current iteration=6, loss=-107735.60438379344\n",
      "Current iteration=8, loss=-145080.03360296696\n",
      "loss=-180689.36802202452\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19303.31062019362\n",
      "Current iteration=4, loss=-67742.62564261167\n",
      "Current iteration=6, loss=-108711.17378656719\n",
      "Current iteration=8, loss=-146335.38861365448\n",
      "loss=-182228.21040974322\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19310.447763396405\n",
      "Current iteration=4, loss=-67764.20383255264\n",
      "Current iteration=6, loss=-108745.67395632715\n",
      "Current iteration=8, loss=-146376.83150634926\n",
      "loss=-182272.51308868878\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18703.131661133146\n",
      "Current iteration=4, loss=-66701.82673985562\n",
      "Current iteration=6, loss=-107276.81270425564\n",
      "Current iteration=8, loss=-144518.36523334627\n",
      "loss=-180030.06546843855\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18902.065862005165\n",
      "Current iteration=4, loss=-67045.27542940083\n",
      "Current iteration=6, loss=-107735.60286646165\n",
      "Current iteration=8, loss=-145080.03105795296\n",
      "loss=-180689.366321681\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19303.31007083184\n",
      "Current iteration=4, loss=-67742.62359743351\n",
      "Current iteration=6, loss=-108711.16953714767\n",
      "Current iteration=8, loss=-146335.381483737\n",
      "loss=-182228.20564270046\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19310.44721398119\n",
      "Current iteration=4, loss=-67764.20178704485\n",
      "Current iteration=6, loss=-108745.66970614558\n",
      "Current iteration=8, loss=-146376.82437531409\n",
      "loss=-182272.50832131426\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18703.131116412947\n",
      "Current iteration=4, loss=-66701.82471297155\n",
      "Current iteration=6, loss=-107276.8084943483\n",
      "Current iteration=8, loss=-144518.35817197472\n",
      "loss=-180030.06075062734\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18902.065315754906\n",
      "Current iteration=4, loss=-67045.27339661906\n",
      "Current iteration=6, loss=-107735.59864439588\n",
      "Current iteration=8, loss=-145080.02397630073\n",
      "loss=-180689.36159037423\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19303.30854220004\n",
      "Current iteration=4, loss=-67742.61790660406\n",
      "Current iteration=6, loss=-108711.1577128864\n",
      "Current iteration=8, loss=-146335.36164431984\n",
      "loss=-182228.1923781216\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19310.44568520075\n",
      "Current iteration=4, loss=-67764.19609529819\n",
      "Current iteration=6, loss=-108745.65787976368\n",
      "Current iteration=8, loss=-146376.80453278692\n",
      "loss=-182272.49505581206\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18703.129600696597\n",
      "Current iteration=4, loss=-66701.81907304638\n",
      "Current iteration=6, loss=-107276.79678003173\n",
      "Current iteration=8, loss=-144518.33852329067\n",
      "loss=-180030.047623038\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18902.06379578119\n",
      "Current iteration=4, loss=-67045.26774028326\n",
      "Current iteration=6, loss=-107735.58689624793\n",
      "Current iteration=8, loss=-145080.00427118456\n",
      "loss=-180689.34842523286\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19303.304288691397\n",
      "Current iteration=4, loss=-67742.6020715348\n",
      "Current iteration=6, loss=-108711.12481118306\n",
      "Current iteration=8, loss=-146335.30643997772\n",
      "loss=-182228.15546864824\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19310.44143127855\n",
      "Current iteration=4, loss=-67764.18025767677\n",
      "Current iteration=6, loss=-108745.62497215983\n",
      "Current iteration=8, loss=-146376.74931979112\n",
      "loss=-182272.4581437693\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18703.125383126022\n",
      "Current iteration=4, loss=-66701.80337962144\n",
      "Current iteration=6, loss=-107276.76418425616\n",
      "Current iteration=8, loss=-144518.2838496747\n",
      "loss=-180030.01109474606\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18902.059566364125\n",
      "Current iteration=4, loss=-67045.25200119475\n",
      "Current iteration=6, loss=-107735.55420633458\n",
      "Current iteration=8, loss=-145079.94944054255\n",
      "loss=-180689.31179245026\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19303.292453052265\n",
      "Current iteration=4, loss=-67742.55800952787\n",
      "Current iteration=6, loss=-108711.03326028687\n",
      "Current iteration=8, loss=-146335.15283072848\n",
      "loss=-182228.05276588682\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19310.42959448854\n",
      "Current iteration=4, loss=-67764.13618856821\n",
      "Current iteration=6, loss=-108745.53340484499\n",
      "Current iteration=8, loss=-146376.59568646253\n",
      "loss=-182272.35543385864\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18703.11364748661\n",
      "Current iteration=4, loss=-66701.75971174797\n",
      "Current iteration=6, loss=-107276.67348462167\n",
      "Current iteration=8, loss=-144518.13171720147\n",
      "loss=-180029.90945264423\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18902.047797761235\n",
      "Current iteration=4, loss=-67045.20820625963\n",
      "Current iteration=6, loss=-107735.46324475622\n",
      "Current iteration=8, loss=-145079.79687113562\n",
      "loss=-180689.20985959686\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19303.259519693194\n",
      "Current iteration=4, loss=-67742.43540448308\n",
      "Current iteration=6, loss=-108710.77851484952\n",
      "Current iteration=8, loss=-146334.72540473592\n",
      "loss=-182227.76698966828\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19310.396657927176\n",
      "Current iteration=4, loss=-67764.0135637629\n",
      "Current iteration=6, loss=-108745.27861372178\n",
      "Current iteration=8, loss=-146376.1681934679\n",
      "loss=-182272.06963774696\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18703.08099238268\n",
      "Current iteration=4, loss=-66701.63820340212\n",
      "Current iteration=6, loss=-107276.42110786709\n",
      "Current iteration=8, loss=-144517.70840041735\n",
      "loss=-180029.6266277705\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18902.015050934388\n",
      "Current iteration=4, loss=-67045.08634435757\n",
      "Current iteration=6, loss=-107735.21013912819\n",
      "Current iteration=8, loss=-145079.37233855997\n",
      "loss=-180688.92622569087\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19303.167880741596\n",
      "Current iteration=4, loss=-67742.09424949149\n",
      "Current iteration=6, loss=-108710.0696733989\n",
      "Current iteration=8, loss=-146333.5360732433\n",
      "loss=-182226.97180280983\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19310.305010064963\n",
      "Current iteration=4, loss=-67763.67235378642\n",
      "Current iteration=6, loss=-108744.56964514789\n",
      "Current iteration=8, loss=-146374.9786755387\n",
      "loss=-182271.27439553462\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18702.990127691868\n",
      "Current iteration=4, loss=-66701.30010003297\n",
      "Current iteration=6, loss=-107275.71885739028\n",
      "Current iteration=8, loss=-144516.53050297644\n",
      "loss=-180028.83965317762\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18901.92393101937\n",
      "Current iteration=4, loss=-67044.74725719959\n",
      "Current iteration=6, loss=-107734.50586052604\n",
      "Current iteration=8, loss=-145078.19105812634\n",
      "loss=-180688.13699992478\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19302.912890506264\n",
      "Current iteration=4, loss=-67741.14497187657\n",
      "Current iteration=6, loss=-108708.09730211036\n",
      "Current iteration=8, loss=-146330.22673975897\n",
      "loss=-182224.75916707458\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19310.049995035504\n",
      "Current iteration=4, loss=-67762.72292317371\n",
      "Current iteration=6, loss=-108742.59692013433\n",
      "Current iteration=8, loss=-146371.6688232895\n",
      "loss=-182269.06160577384\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18702.737291878453\n",
      "Current iteration=4, loss=-66700.35931368126\n",
      "Current iteration=6, loss=-107273.76482567015\n",
      "Current iteration=8, loss=-144513.2529849168\n",
      "loss=-180026.64986835106\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18901.670385030917\n",
      "Current iteration=4, loss=-67043.80373341526\n",
      "Current iteration=6, loss=-107732.54618549037\n",
      "Current iteration=8, loss=-145074.90412683893\n",
      "loss=-180685.9409511325\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-19302.203369606137\n",
      "Current iteration=4, loss=-67738.50360018863\n",
      "Current iteration=6, loss=-108702.609233367\n",
      "Current iteration=8, loss=-146321.01872569133\n",
      "loss=-182218.60252330345\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-19309.340405145103\n",
      "Current iteration=4, loss=-67760.0811257672\n",
      "Current iteration=6, loss=-108737.10786715217\n",
      "Current iteration=8, loss=-146362.45936577488\n",
      "loss=-182262.9045334182\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-18702.03376574791\n",
      "Current iteration=4, loss=-66697.7415689926\n",
      "Current iteration=6, loss=-107268.32778625784\n",
      "Current iteration=8, loss=-144504.13349526274\n",
      "loss=-180020.5568069819\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-18900.964882808283\n",
      "Current iteration=4, loss=-67041.17837179908\n",
      "Current iteration=6, loss=-107727.09344370972\n",
      "Current iteration=8, loss=-145065.7584454835\n",
      "loss=-180679.83046035093\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20846.728882556876\n",
      "Current iteration=4, loss=-70140.26711524323\n",
      "Current iteration=6, loss=-111886.68991135052\n",
      "Current iteration=8, loss=-150292.4227785282\n",
      "loss=-186980.92967751148\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20854.039194423352\n",
      "Current iteration=4, loss=-70162.62348635211\n",
      "Current iteration=6, loss=-111921.92905516032\n",
      "Current iteration=8, loss=-150334.32836818203\n",
      "loss=-187025.4218582647\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20232.769577263716\n",
      "Current iteration=4, loss=-69077.1032427147\n",
      "Current iteration=6, loss=-110420.88978175474\n",
      "Current iteration=8, loss=-148434.23131880746\n",
      "loss=-184731.5403046905\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20436.306582590438\n",
      "Current iteration=4, loss=-69427.43120352089\n",
      "Current iteration=6, loss=-110888.43834454146\n",
      "Current iteration=8, loss=-149006.68061723557\n",
      "loss=-185403.83121519873\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20846.728675429727\n",
      "Current iteration=4, loss=-70140.26634547248\n",
      "Current iteration=6, loss=-111886.68831182101\n",
      "Current iteration=8, loss=-150292.4200936029\n",
      "loss=-186980.9278794367\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20854.03898727613\n",
      "Current iteration=4, loss=-70162.62271645587\n",
      "Current iteration=6, loss=-111921.92745534604\n",
      "Current iteration=8, loss=-150334.3256828437\n",
      "loss=-187025.42006007128\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20232.76937188939\n",
      "Current iteration=4, loss=-69077.10247984294\n",
      "Current iteration=6, loss=-110420.888197137\n",
      "Current iteration=8, loss=-148434.22865977022\n",
      "loss=-184731.53852523922\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20436.30637663836\n",
      "Current iteration=4, loss=-69427.4304384302\n",
      "Current iteration=6, loss=-110888.43675534998\n",
      "Current iteration=8, loss=-149006.67795056323\n",
      "loss=-185403.8294306514\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20846.72809908604\n",
      "Current iteration=4, loss=-70140.26420353973\n",
      "Current iteration=6, loss=-111886.68386103517\n",
      "Current iteration=8, loss=-150292.41262263892\n",
      "loss=-186980.92287618708\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20854.038410876718\n",
      "Current iteration=4, loss=-70162.62057417385\n",
      "Current iteration=6, loss=-111921.92300376778\n",
      "Current iteration=8, loss=-150334.31821073047\n",
      "loss=-187025.4150564914\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20232.76880042314\n",
      "Current iteration=4, loss=-69077.10035710709\n",
      "Current iteration=6, loss=-110420.88378784394\n",
      "Current iteration=8, loss=-148434.22126084153\n",
      "loss=-184731.53357381045\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20436.305803564403\n",
      "Current iteration=4, loss=-69427.42830951992\n",
      "Current iteration=6, loss=-110888.4323333303\n",
      "Current iteration=8, loss=-149006.670530389\n",
      "loss=-185403.82446504262\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20846.726495375613\n",
      "Current iteration=4, loss=-70140.25824348486\n",
      "Current iteration=6, loss=-111886.67147646015\n",
      "Current iteration=8, loss=-150292.3918342399\n",
      "loss=-186980.90895434836\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20854.036807011147\n",
      "Current iteration=4, loss=-70162.61461314722\n",
      "Current iteration=6, loss=-111921.91061698792\n",
      "Current iteration=8, loss=-150334.29741913365\n",
      "loss=-187025.40113373366\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20232.767210284394\n",
      "Current iteration=4, loss=-69077.09445046875\n",
      "Current iteration=6, loss=-110420.87151872505\n",
      "Current iteration=8, loss=-148434.20067288488\n",
      "loss=-184731.51979616642\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20436.304208952155\n",
      "Current iteration=4, loss=-69427.4223857009\n",
      "Current iteration=6, loss=-110888.42002879882\n",
      "Current iteration=8, loss=-149006.6498833156\n",
      "loss=-185403.8106479418\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20846.72203295622\n",
      "Current iteration=4, loss=-70140.24165928007\n",
      "Current iteration=6, loss=-111886.637015651\n",
      "Current iteration=8, loss=-150292.33398930033\n",
      "loss=-186980.87021601087\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20854.032344160085\n",
      "Current iteration=4, loss=-70162.59802623841\n",
      "Current iteration=6, loss=-111921.87615004371\n",
      "Current iteration=8, loss=-150334.2395652961\n",
      "loss=-187025.36239283922\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20232.76278562904\n",
      "Current iteration=4, loss=-69077.07801489857\n",
      "Current iteration=6, loss=-110420.83737917952\n",
      "Current iteration=8, loss=-148434.14338568793\n",
      "loss=-184731.48145905923\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20436.29977184898\n",
      "Current iteration=4, loss=-69427.4059023245\n",
      "Current iteration=6, loss=-110888.38579071555\n",
      "Current iteration=8, loss=-149006.59243162294\n",
      "loss=-185403.77220104382\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20846.709616010576\n",
      "Current iteration=4, loss=-70140.19551276002\n",
      "Current iteration=6, loss=-111886.541126454\n",
      "Current iteration=8, loss=-150292.1730324405\n",
      "loss=-186980.76242432962\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20854.019926013294\n",
      "Current iteration=4, loss=-70162.5518721943\n",
      "Current iteration=6, loss=-111921.78024377528\n",
      "Current iteration=8, loss=-150334.078583677\n",
      "loss=-187025.25459404316\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20232.75047376401\n",
      "Current iteration=4, loss=-69077.03228196292\n",
      "Current iteration=6, loss=-110420.74238391696\n",
      "Current iteration=8, loss=-148433.983980779\n",
      "loss=-184731.3747838247\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20436.287425347164\n",
      "Current iteration=4, loss=-69427.36003636534\n",
      "Current iteration=6, loss=-110888.29052126627\n",
      "Current iteration=8, loss=-149006.4325689952\n",
      "loss=-185403.66522030998\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20846.675065132364\n",
      "Current iteration=4, loss=-70140.06710744201\n",
      "Current iteration=6, loss=-111886.27430946512\n",
      "Current iteration=8, loss=-150291.72516135153\n",
      "loss=-186980.46248791466\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20853.985371792816\n",
      "Current iteration=4, loss=-70162.42344593994\n",
      "Current iteration=6, loss=-111921.51337928433\n",
      "Current iteration=8, loss=-150333.63064369396\n",
      "loss=-187024.95463783032\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20232.71621527881\n",
      "Current iteration=4, loss=-69076.90502746699\n",
      "Current iteration=6, loss=-110420.47805435004\n",
      "Current iteration=8, loss=-148433.54042807635\n",
      "loss=-184731.077953985\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20436.25307048306\n",
      "Current iteration=4, loss=-69427.23241172385\n",
      "Current iteration=6, loss=-110888.02542875953\n",
      "Current iteration=8, loss=-149005.98774266528\n",
      "loss=-185403.36754040135\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20846.57892534365\n",
      "Current iteration=4, loss=-70139.70981290878\n",
      "Current iteration=6, loss=-111885.53187842366\n",
      "Current iteration=8, loss=-150290.47894067707\n",
      "loss=-186979.6278996629\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20853.889222704132\n",
      "Current iteration=4, loss=-70162.06609315045\n",
      "Current iteration=6, loss=-111920.77081606597\n",
      "Current iteration=8, loss=-150332.38423131855\n",
      "loss=-187024.11999448994\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20232.620889090387\n",
      "Current iteration=4, loss=-69076.55093515672\n",
      "Current iteration=6, loss=-110419.74254467912\n",
      "Current iteration=8, loss=-148432.3062234984\n",
      "loss=-184730.25200993463\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20436.15747611484\n",
      "Current iteration=4, loss=-69426.87728946414\n",
      "Current iteration=6, loss=-110887.28779617211\n",
      "Current iteration=8, loss=-149004.74999416486\n",
      "loss=-185402.53923099165\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20846.311411308365\n",
      "Current iteration=4, loss=-70138.71562655119\n",
      "Current iteration=6, loss=-111883.46604379393\n",
      "Current iteration=8, loss=-150287.01131364188\n",
      "loss=-186977.3056287145\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20853.621682791225\n",
      "Current iteration=4, loss=-70161.07174469228\n",
      "Current iteration=6, loss=-111918.70461364926\n",
      "Current iteration=8, loss=-150328.91607087053\n",
      "loss=-187021.79757025442\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20232.355638940982\n",
      "Current iteration=4, loss=-69075.56565911268\n",
      "Current iteration=6, loss=-110417.69596894998\n",
      "Current iteration=8, loss=-148428.87203142265\n",
      "loss=-184727.95379176582\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20435.89147974086\n",
      "Current iteration=4, loss=-69425.88914754415\n",
      "Current iteration=6, loss=-110885.23531337042\n",
      "Current iteration=8, loss=-149001.30594107643\n",
      "loss=-185400.23443113224\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20845.567042551636\n",
      "Current iteration=4, loss=-70135.94929741764\n",
      "Current iteration=6, loss=-111877.7179208419\n",
      "Current iteration=8, loss=-150277.36286915725\n",
      "loss=-186970.84393009974\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-20852.877242029364\n",
      "Current iteration=4, loss=-70158.30496451141\n",
      "Current iteration=6, loss=-111912.95546733176\n",
      "Current iteration=8, loss=-150319.26614218304\n",
      "loss=-187015.3354451107\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-20231.617569540893\n",
      "Current iteration=4, loss=-69072.82412297573\n",
      "Current iteration=6, loss=-110412.00143329911\n",
      "Current iteration=8, loss=-148419.3166174957\n",
      "loss=-184721.55901970106\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-20435.15133393972\n",
      "Current iteration=4, loss=-69423.13963708951\n",
      "Current iteration=6, loss=-110879.52434146924\n",
      "Current iteration=8, loss=-148991.72308954998\n",
      "loss=-185393.821345603\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22379.30598603951\n",
      "Current iteration=4, loss=-72520.8430190574\n",
      "Current iteration=6, loss=-115044.30547408867\n",
      "Current iteration=8, loss=-154232.70518003125\n",
      "loss=-191718.40817834128\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22386.801329534577\n",
      "Current iteration=4, loss=-72543.97080552648\n",
      "Current iteration=6, loss=-115080.25042087799\n",
      "Current iteration=8, loss=-154275.0344272777\n",
      "loss=-191763.05704988886\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21751.646962078838\n",
      "Current iteration=4, loss=-71435.40756375297\n",
      "Current iteration=6, loss=-113547.11022461014\n",
      "Current iteration=8, loss=-152333.34722610036\n",
      "loss=-189417.7499557887\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21959.75185339337\n",
      "Current iteration=4, loss=-71792.5457546709\n",
      "Current iteration=6, loss=-114023.35239598581\n",
      "Current iteration=8, loss=-152916.53281564853\n",
      "loss=-190103.00221471192\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22379.305769010905\n",
      "Current iteration=4, loss=-72520.84221380173\n",
      "Current iteration=6, loss=-115044.30380064946\n",
      "Current iteration=8, loss=-154232.70236984003\n",
      "loss=-191718.40629335947\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22386.801112485067\n",
      "Current iteration=4, loss=-72543.9700001382\n",
      "Current iteration=6, loss=-115080.24874714328\n",
      "Current iteration=8, loss=-154275.03161666228\n",
      "loss=-191763.05516478897\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21751.646746889797\n",
      "Current iteration=4, loss=-71435.40676572829\n",
      "Current iteration=6, loss=-113547.10856681193\n",
      "Current iteration=8, loss=-152333.3444430826\n",
      "loss=-189417.74809038488\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21959.75163759803\n",
      "Current iteration=4, loss=-71792.54495432589\n",
      "Current iteration=6, loss=-114023.35073340531\n",
      "Current iteration=8, loss=-152916.53002464105\n",
      "loss=-190103.00034395972\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22379.305165115977\n",
      "Current iteration=4, loss=-72520.83997313012\n",
      "Current iteration=6, loss=-115044.29914420571\n",
      "Current iteration=8, loss=-154232.6945503162\n",
      "loss=-191718.401048286\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22386.80050853192\n",
      "Current iteration=4, loss=-72543.9677590974\n",
      "Current iteration=6, loss=-115080.24408987713\n",
      "Current iteration=8, loss=-154275.02379595826\n",
      "loss=-191763.04991938625\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21751.646148113472\n",
      "Current iteration=4, loss=-71435.40454517734\n",
      "Current iteration=6, loss=-113547.10395389017\n",
      "Current iteration=8, loss=-152333.33669917076\n",
      "loss=-189417.7428997879\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21959.751037134673\n",
      "Current iteration=4, loss=-71792.54272731842\n",
      "Current iteration=6, loss=-114023.34610717667\n",
      "Current iteration=8, loss=-152916.52225849702\n",
      "loss=-190102.99513848053\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22379.303484742388\n",
      "Current iteration=4, loss=-72520.8337383286\n",
      "Current iteration=6, loss=-115044.28618737517\n",
      "Current iteration=8, loss=-154232.6727920288\n",
      "loss=-191718.38645355814\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22386.798827996376\n",
      "Current iteration=4, loss=-72543.96152326855\n",
      "Current iteration=6, loss=-115080.23113075821\n",
      "Current iteration=8, loss=-154275.0020343868\n",
      "loss=-191763.03532374266\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21751.64448198284\n",
      "Current iteration=4, loss=-71435.39836636253\n",
      "Current iteration=6, loss=-113547.09111816227\n",
      "Current iteration=8, loss=-152333.31515127822\n",
      "loss=-189417.72845664437\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21959.749366309763\n",
      "Current iteration=4, loss=-71792.53653053816\n",
      "Current iteration=6, loss=-114023.33323442146\n",
      "Current iteration=8, loss=-152916.5006487422\n",
      "loss=-190102.98065392632\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22379.298809003354\n",
      "Current iteration=4, loss=-72520.81638962492\n",
      "Current iteration=6, loss=-115044.25013423161\n",
      "Current iteration=8, loss=-154232.6122483184\n",
      "loss=-191718.34584286704\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22386.794151806615\n",
      "Current iteration=4, loss=-72543.94417170635\n",
      "Current iteration=6, loss=-115080.1950712471\n",
      "Current iteration=8, loss=-154274.94148153826\n",
      "loss=-191762.99471050332\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21751.639845875594\n",
      "Current iteration=4, loss=-71435.38117344533\n",
      "Current iteration=6, loss=-113547.0554019939\n",
      "Current iteration=8, loss=-152333.25519300377\n",
      "loss=-189417.68826774578\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21959.74471714033\n",
      "Current iteration=4, loss=-71792.51928763106\n",
      "Current iteration=6, loss=-114023.29741522246\n",
      "Current iteration=8, loss=-152916.44051833215\n",
      "loss=-190102.9403497999\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22379.285798483186\n",
      "Current iteration=4, loss=-72520.76811584234\n",
      "Current iteration=6, loss=-115044.14981427317\n",
      "Current iteration=8, loss=-154232.44378197743\n",
      "loss=-191718.23284125412\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22386.78114003236\n",
      "Current iteration=4, loss=-72543.89588996985\n",
      "Current iteration=6, loss=-115080.09473357073\n",
      "Current iteration=8, loss=-154274.77298977005\n",
      "loss=-191762.88170179992\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21751.626945633194\n",
      "Current iteration=4, loss=-71435.33333314785\n",
      "Current iteration=6, loss=-113546.95601968851\n",
      "Current iteration=8, loss=-152333.0883556719\n",
      "loss=-189417.5764397948\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21959.731780551683\n",
      "Current iteration=4, loss=-71792.47130823365\n",
      "Current iteration=6, loss=-114023.1977462284\n",
      "Current iteration=8, loss=-152916.27320202359\n",
      "loss=-190102.82820122113\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22379.24959594941\n",
      "Current iteration=4, loss=-72520.63379129853\n",
      "Current iteration=6, loss=-115043.87066845717\n",
      "Current iteration=8, loss=-154231.9750153805\n",
      "loss=-191717.91840792098\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22386.74493400897\n",
      "Current iteration=4, loss=-72543.76154329376\n",
      "Current iteration=6, loss=-115079.81553845337\n",
      "Current iteration=8, loss=-154274.3041524205\n",
      "loss=-191762.56724873697\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21751.591049953826\n",
      "Current iteration=4, loss=-71435.20021480092\n",
      "Current iteration=6, loss=-113546.67948294368\n",
      "Current iteration=8, loss=-152332.6241218798\n",
      "loss=-189417.26527224164\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21959.69578373673\n",
      "Current iteration=4, loss=-71792.3378028332\n",
      "Current iteration=6, loss=-114022.92041175644\n",
      "Current iteration=8, loss=-152915.80763545312\n",
      "loss=-190102.5161415032\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22379.1488603374\n",
      "Current iteration=4, loss=-72520.26002623462\n",
      "Current iteration=6, loss=-115043.0939319509\n",
      "Current iteration=8, loss=-154230.67065225294\n",
      "loss=-191717.04348133737\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22386.644188686903\n",
      "Current iteration=4, loss=-72543.38771664552\n",
      "Current iteration=6, loss=-115079.03866476366\n",
      "Current iteration=8, loss=-154272.99959242003\n",
      "loss=-191761.69226725434\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21751.491168181667\n",
      "Current iteration=4, loss=-71434.82980604269\n",
      "Current iteration=6, loss=-113545.91000630165\n",
      "Current iteration=8, loss=-152331.33237147515\n",
      "loss=-189416.39943285377\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21959.595620549007\n",
      "Current iteration=4, loss=-71791.9663170783\n",
      "Current iteration=6, loss=-114022.14871540094\n",
      "Current iteration=8, loss=-152914.51217653608\n",
      "loss=-190101.6478196224\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22378.868558199636\n",
      "Current iteration=4, loss=-72519.22001016128\n",
      "Current iteration=6, loss=-115040.93264210019\n",
      "Current iteration=8, loss=-154227.0412445581\n",
      "loss=-191714.60896816573\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22386.363859530462\n",
      "Current iteration=4, loss=-72542.34752921114\n",
      "Current iteration=6, loss=-115076.87699319512\n",
      "Current iteration=8, loss=-154269.36963692209\n",
      "loss=-191759.257601323\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21751.213241898404\n",
      "Current iteration=4, loss=-71433.79912902266\n",
      "Current iteration=6, loss=-113543.76891721354\n",
      "Current iteration=8, loss=-152327.7380588471\n",
      "loss=-189413.9902051024\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21959.316911212092\n",
      "Current iteration=4, loss=-71790.93264327226\n",
      "Current iteration=6, loss=-114020.00144990181\n",
      "Current iteration=8, loss=-152910.90754492578\n",
      "loss=-190099.2316842536\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-22378.088606167035\n",
      "Current iteration=4, loss=-72516.32616110417\n",
      "Current iteration=6, loss=-115034.91892305057\n",
      "Current iteration=8, loss=-154216.94266806592\n",
      "loss=-191707.83496113864\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-22385.583832317807\n",
      "Current iteration=4, loss=-72539.4532033395\n",
      "Current iteration=6, loss=-115070.86221201812\n",
      "Current iteration=8, loss=-154259.2695361884\n",
      "loss=-191752.4831692354\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-21750.4399007787\n",
      "Current iteration=4, loss=-71430.93126591905\n",
      "Current iteration=6, loss=-113537.8114061188\n",
      "Current iteration=8, loss=-152317.73713193712\n",
      "loss=-189407.28655438602\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-21958.541391213064\n",
      "Current iteration=4, loss=-71788.05644159716\n",
      "Current iteration=6, loss=-114014.02675314668\n",
      "Current iteration=8, loss=-152900.87790618336\n",
      "loss=-190092.50881320034\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23901.370559054394\n",
      "Current iteration=4, loss=-74885.10032351255\n",
      "Current iteration=6, loss=-118184.98192656104\n",
      "Current iteration=8, loss=-158157.24108087606\n",
      "loss=-196441.60893134447\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23909.06278403917\n",
      "Current iteration=4, loss=-74908.99166147776\n",
      "Current iteration=6, loss=-118221.60020746535\n",
      "Current iteration=8, loss=-158199.95719233688\n",
      "loss=-196486.38458551728\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23260.090266083593\n",
      "Current iteration=4, loss=-73777.48004746219\n",
      "Current iteration=6, loss=-116656.42945995368\n",
      "Current iteration=8, loss=-156216.7146263355\n",
      "loss=-194089.6561132648\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23472.72857111683\n",
      "Current iteration=4, loss=-74141.36230701976\n",
      "Current iteration=6, loss=-117141.30470270671\n",
      "Current iteration=8, loss=-156810.5935570173\n",
      "loss=-194787.84450395074\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23901.3703319207\n",
      "Current iteration=4, loss=-74885.09948206025\n",
      "Current iteration=6, loss=-118184.98017767082\n",
      "Current iteration=8, loss=-158157.23814271978\n",
      "loss=-196441.60695743866\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23909.062556883626\n",
      "Current iteration=4, loss=-74908.99081988551\n",
      "Current iteration=6, loss=-118221.5984582689\n",
      "Current iteration=8, loss=-158199.95425374547\n",
      "loss=-196486.38261149335\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23260.090040878047\n",
      "Current iteration=4, loss=-73777.47921358033\n",
      "Current iteration=6, loss=-116656.42772745117\n",
      "Current iteration=8, loss=-156216.71171666958\n",
      "loss=-194089.65415991496\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23472.72834527588\n",
      "Current iteration=4, loss=-74141.36147071428\n",
      "Current iteration=6, loss=-117141.30296520903\n",
      "Current iteration=8, loss=-156810.59063899895\n",
      "loss=-194787.84254499373\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23901.369699907726\n",
      "Current iteration=4, loss=-74885.09714066926\n",
      "Current iteration=6, loss=-118184.9753112799\n",
      "Current iteration=8, loss=-158157.22996712578\n",
      "loss=-196441.60146492854\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23909.06192480986\n",
      "Current iteration=4, loss=-74908.98847810505\n",
      "Current iteration=6, loss=-118221.59359102587\n",
      "Current iteration=8, loss=-158199.94607694075\n",
      "loss=-196486.37711865467\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23260.089414230322\n",
      "Current iteration=4, loss=-73777.47689325451\n",
      "Current iteration=6, loss=-116656.42290666013\n",
      "Current iteration=8, loss=-156216.70362035176\n",
      "loss=-194089.64872460326\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23472.7277168601\n",
      "Current iteration=4, loss=-74141.3591436447\n",
      "Current iteration=6, loss=-117141.29813051874\n",
      "Current iteration=8, loss=-156810.58251944036\n",
      "loss=-194787.83709408\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23901.367941294106\n",
      "Current iteration=4, loss=-74885.09062560996\n",
      "Current iteration=6, loss=-118184.96177025913\n",
      "Current iteration=8, loss=-158157.207218052\n",
      "loss=-196441.58618169342\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23909.06016602706\n",
      "Current iteration=4, loss=-74908.98196196204\n",
      "Current iteration=6, loss=-118221.58004763401\n",
      "Current iteration=8, loss=-158199.92332449806\n",
      "loss=-196486.36183450551\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23260.08767054582\n",
      "Current iteration=4, loss=-73777.47043681041\n",
      "Current iteration=6, loss=-116656.40949252376\n",
      "Current iteration=8, loss=-156216.68109186864\n",
      "loss=-194089.63360052634\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23472.72596825589\n",
      "Current iteration=4, loss=-74141.35266843559\n",
      "Current iteration=6, loss=-117141.2846777068\n",
      "Current iteration=8, loss=-156810.5599262882\n",
      "loss=-194787.82192658944\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23901.36304784741\n",
      "Current iteration=4, loss=-74885.07249707267\n",
      "Current iteration=6, loss=-118184.92409157213\n",
      "Current iteration=8, loss=-158157.14391742094\n",
      "loss=-196441.54365519076\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23909.05527210963\n",
      "Current iteration=4, loss=-74908.9638304093\n",
      "Current iteration=6, loss=-118221.54236234925\n",
      "Current iteration=8, loss=-158199.86001449267\n",
      "loss=-196486.31930545915\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23260.082818640352\n",
      "Current iteration=4, loss=-73777.45247137347\n",
      "Current iteration=6, loss=-116656.37216689996\n",
      "Current iteration=8, loss=-156216.61840504402\n",
      "loss=-194089.59151689045\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23472.721102661006\n",
      "Current iteration=4, loss=-74141.33465078386\n",
      "Current iteration=6, loss=-117141.24724446613\n",
      "Current iteration=8, loss=-156810.4970595182\n",
      "loss=-194787.77972215242\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23901.349431542938\n",
      "Current iteration=4, loss=-74885.02205335784\n",
      "Current iteration=6, loss=-118184.81924844642\n",
      "Current iteration=8, loss=-158156.96777979407\n",
      "loss=-196441.4253227219\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23909.04165449528\n",
      "Current iteration=4, loss=-74908.91337830397\n",
      "Current iteration=6, loss=-118221.4375008652\n",
      "Current iteration=8, loss=-158199.6838507812\n",
      "loss=-196486.20096591246\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23260.069317926693\n",
      "Current iteration=4, loss=-73777.40248149491\n",
      "Current iteration=6, loss=-116656.26830619326\n",
      "Current iteration=8, loss=-156216.4439753685\n",
      "loss=-194089.4744167243\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23472.707563855776\n",
      "Current iteration=4, loss=-74141.2845156147\n",
      "Current iteration=6, loss=-117141.14308430895\n",
      "Current iteration=8, loss=-156810.32212913438\n",
      "loss=-194787.6622858502\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23901.31154337923\n",
      "Current iteration=4, loss=-74884.88169085782\n",
      "Current iteration=6, loss=-118184.52751667974\n",
      "Current iteration=8, loss=-158156.47766746097\n",
      "loss=-196441.0960559934\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23909.003762686694\n",
      "Current iteration=4, loss=-74908.77299245658\n",
      "Current iteration=6, loss=-118221.14571801512\n",
      "Current iteration=8, loss=-158199.1936658665\n",
      "loss=-196485.87167948915\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23260.031751401188\n",
      "Current iteration=4, loss=-73777.26338181988\n",
      "Current iteration=6, loss=-116655.97930806126\n",
      "Current iteration=8, loss=-156215.95861550138\n",
      "loss=-194089.14857894662\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23472.6698913382\n",
      "Current iteration=4, loss=-74141.14501166018\n",
      "Current iteration=6, loss=-117140.8532529397\n",
      "Current iteration=8, loss=-156809.8353760198\n",
      "loss=-194787.3355127549\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23901.206117408325\n",
      "Current iteration=4, loss=-74884.49112489136\n",
      "Current iteration=6, loss=-118183.71575925795\n",
      "Current iteration=8, loss=-158155.11390911872\n",
      "loss=-196440.1798548199\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23908.89832657384\n",
      "Current iteration=4, loss=-74908.38236152506\n",
      "Current iteration=6, loss=-118220.33381845111\n",
      "Current iteration=8, loss=-158197.82970556224\n",
      "loss=-196484.95542351395\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23259.92722040696\n",
      "Current iteration=4, loss=-73776.87632972989\n",
      "Current iteration=6, loss=-116655.17515710682\n",
      "Current iteration=8, loss=-156214.60808109655\n",
      "loss=-194088.24191899938\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23472.56506541505\n",
      "Current iteration=4, loss=-74140.75683464155\n",
      "Current iteration=6, loss=-117140.04678346333\n",
      "Current iteration=8, loss=-156808.48096484508\n",
      "loss=-194786.42625024018\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23900.912764119694\n",
      "Current iteration=4, loss=-74883.40435984763\n",
      "Current iteration=6, loss=-118181.45702348609\n",
      "Current iteration=8, loss=-158151.31923507908\n",
      "loss=-196437.63049430973\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23908.60494506474\n",
      "Current iteration=4, loss=-74907.29541571316\n",
      "Current iteration=6, loss=-118218.0746871637\n",
      "Current iteration=8, loss=-158194.03446955897\n",
      "loss=-196482.40591051526\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23259.63635743812\n",
      "Current iteration=4, loss=-73775.79934218425\n",
      "Current iteration=6, loss=-116652.93758652317\n",
      "Current iteration=8, loss=-156210.8502028202\n",
      "loss=-194085.7191072575\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23472.273381790823\n",
      "Current iteration=4, loss=-74139.67671693781\n",
      "Current iteration=6, loss=-117137.80276153372\n",
      "Current iteration=8, loss=-156804.71229941337\n",
      "loss=-194783.89619677197\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23900.09649688198\n",
      "Current iteration=4, loss=-74880.3804331746\n",
      "Current iteration=6, loss=-118175.17216974663\n",
      "Current iteration=8, loss=-158140.7608289836\n",
      "loss=-196430.53693058807\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-23907.788599302967\n",
      "Current iteration=4, loss=-74904.27098605037\n",
      "Current iteration=6, loss=-118211.78873290564\n",
      "Current iteration=8, loss=-158183.4744998234\n",
      "loss=-196475.31192248897\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-23258.827019616343\n",
      "Current iteration=4, loss=-73772.8026214251\n",
      "Current iteration=6, loss=-116646.71162417084\n",
      "Current iteration=8, loss=-156200.39417825776\n",
      "loss=-194078.69941504585\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-23471.461760461858\n",
      "Current iteration=4, loss=-74136.67128650333\n",
      "Current iteration=6, loss=-117131.5588485397\n",
      "Current iteration=8, loss=-156794.2262603976\n",
      "loss=-194776.85635458518\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25413.240032411548\n",
      "Current iteration=4, loss=-77233.74667903612\n",
      "Current iteration=6, loss=-121309.61833231007\n",
      "Current iteration=8, loss=-162066.96026517006\n",
      "loss=-201151.41761925668\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25421.140932560473\n",
      "Current iteration=4, loss=-77258.39273237581\n",
      "Current iteration=6, loss=-121346.87826509017\n",
      "Current iteration=8, loss=-162110.02863338427\n",
      "loss=-201196.29287526186\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24758.414762441516\n",
      "Current iteration=4, loss=-76104.0221320334\n",
      "Current iteration=6, loss=-119749.74111582719\n",
      "Current iteration=8, loss=-160085.26022160542\n",
      "loss=-198748.1434659054\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24975.55246783218\n",
      "Current iteration=4, loss=-76474.58508083229\n",
      "Current iteration=6, loss=-120243.19287370644\n",
      "Current iteration=8, loss=-160689.79342123852\n",
      "loss=-199459.24590144082\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25413.239794969977\n",
      "Current iteration=4, loss=-77233.7458006765\n",
      "Current iteration=6, loss=-121309.61650642806\n",
      "Current iteration=8, loss=-162066.95719635068\n",
      "loss=-201151.415554411\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25421.140695096103\n",
      "Current iteration=4, loss=-77258.39185386884\n",
      "Current iteration=6, loss=-121346.87643889136\n",
      "Current iteration=8, loss=-162110.02556411893\n",
      "loss=-201196.29081029812\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24758.414527018667\n",
      "Current iteration=4, loss=-76104.02126159126\n",
      "Current iteration=6, loss=-119749.73930709716\n",
      "Current iteration=8, loss=-160085.25718262471\n",
      "loss=-198748.14142261771\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24975.55223174427\n",
      "Current iteration=4, loss=-76474.58420786141\n",
      "Current iteration=6, loss=-120243.19105976407\n",
      "Current iteration=8, loss=-160689.79037353504\n",
      "loss=-199459.24385228095\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25413.239134274783\n",
      "Current iteration=4, loss=-77233.74335658878\n",
      "Current iteration=6, loss=-121309.61142580313\n",
      "Current iteration=8, loss=-162066.9486571789\n",
      "loss=-201151.4098088554\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25421.140034337375\n",
      "Current iteration=4, loss=-77258.38940937116\n",
      "Current iteration=6, loss=-121346.87135738487\n",
      "Current iteration=8, loss=-162110.01702370628\n",
      "loss=-201196.28506441388\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24758.41387194069\n",
      "Current iteration=4, loss=-76104.01883953433\n",
      "Current iteration=6, loss=-119749.73427419864\n",
      "Current iteration=8, loss=-160085.2487264807\n",
      "loss=-198748.13573704846\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24975.551574815574\n",
      "Current iteration=4, loss=-76474.58177876801\n",
      "Current iteration=6, loss=-120243.18601236184\n",
      "Current iteration=8, loss=-160689.78189311948\n",
      "loss=-199459.23815037226\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25413.23729585116\n",
      "Current iteration=4, loss=-77233.7365557699\n",
      "Current iteration=6, loss=-121309.59728866347\n",
      "Current iteration=8, loss=-162066.92489642842\n",
      "loss=-201151.3938215065\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25421.138195736974\n",
      "Current iteration=4, loss=-77258.38260741133\n",
      "Current iteration=6, loss=-121346.85721779213\n",
      "Current iteration=8, loss=-162109.99325950324\n",
      "loss=-201196.26907615073\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24758.412049147326\n",
      "Current iteration=4, loss=-76104.0121000174\n",
      "Current iteration=6, loss=-119749.7202698605\n",
      "Current iteration=8, loss=-160085.2251967602\n",
      "loss=-198748.11991661508\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24975.54974687246\n",
      "Current iteration=4, loss=-76474.57501967176\n",
      "Current iteration=6, loss=-120243.17196766625\n",
      "Current iteration=8, loss=-160689.75829586186\n",
      "loss=-199459.22228447333\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25413.232180328436\n",
      "Current iteration=4, loss=-77233.71763208965\n",
      "Current iteration=6, loss=-121309.55795124063\n",
      "Current iteration=8, loss=-162066.85878074815\n",
      "loss=-201151.34933576555\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25421.13307972236\n",
      "Current iteration=4, loss=-77258.36368055646\n",
      "Current iteration=6, loss=-121346.81787354371\n",
      "Current iteration=8, loss=-162109.92713421583\n",
      "loss=-201196.22458786558\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24758.406977116694\n",
      "Current iteration=4, loss=-76103.99334691347\n",
      "Current iteration=6, loss=-119749.68130196593\n",
      "Current iteration=8, loss=-160085.1597239341\n",
      "loss=-198748.07589532665\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24975.544660512413\n",
      "Current iteration=4, loss=-76474.55621208722\n",
      "Current iteration=6, loss=-120243.13288747479\n",
      "Current iteration=8, loss=-160689.69263511003\n",
      "loss=-199459.1781366742\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25413.21794608421\n",
      "Current iteration=4, loss=-77233.66497584361\n",
      "Current iteration=6, loss=-121309.44849258829\n",
      "Current iteration=8, loss=-162066.67481008926\n",
      "loss=-201151.22555160374\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25421.118844109496\n",
      "Current iteration=4, loss=-77258.3110154769\n",
      "Current iteration=6, loss=-121346.70839589862\n",
      "Current iteration=8, loss=-162109.74313682437\n",
      "loss=-201196.10079662458\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24758.392863891895\n",
      "Current iteration=4, loss=-76103.94116530604\n",
      "Current iteration=6, loss=-119749.5728715473\n",
      "Current iteration=8, loss=-160084.97754205382\n",
      "loss=-198747.95340353064\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24975.53050741515\n",
      "Current iteration=4, loss=-76474.5038788844\n",
      "Current iteration=6, loss=-120243.02414458356\n",
      "Current iteration=8, loss=-160689.50993031543\n",
      "loss=-199459.05529285496\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25413.17833846737\n",
      "Current iteration=4, loss=-77233.51845685333\n",
      "Current iteration=6, loss=-121309.14391787685\n",
      "Current iteration=8, loss=-162066.16290195583\n",
      "loss=-201150.88111524426\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25421.079232684362\n",
      "Current iteration=4, loss=-77258.16447190671\n",
      "Current iteration=6, loss=-121346.40376833885\n",
      "Current iteration=8, loss=-162109.23115430633\n",
      "loss=-201195.75634056694\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24758.35359301861\n",
      "Current iteration=4, loss=-76103.79596702463\n",
      "Current iteration=6, loss=-119749.27115795249\n",
      "Current iteration=8, loss=-160084.47061129185\n",
      "loss=-198747.6125632514\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24975.491125594453\n",
      "Current iteration=4, loss=-76474.35825878008\n",
      "Current iteration=6, loss=-120242.72156151652\n",
      "Current iteration=8, loss=-160689.00154451662\n",
      "loss=-199458.71347305123\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25413.068128023002\n",
      "Current iteration=4, loss=-77233.11076015995\n",
      "Current iteration=6, loss=-121308.29642444917\n",
      "Current iteration=8, loss=-162064.73849609456\n",
      "loss=-201149.9227038968\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25420.969011643163\n",
      "Current iteration=4, loss=-77257.75670681846\n",
      "Current iteration=6, loss=-121345.55612785813\n",
      "Current iteration=8, loss=-162107.8065414662\n",
      "loss=-201194.79787440796\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24758.24431958233\n",
      "Current iteration=4, loss=-76103.39194527213\n",
      "Current iteration=6, loss=-119748.43162571598\n",
      "Current iteration=8, loss=-160083.06005517518\n",
      "loss=-198746.6641581745\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24975.381543440613\n",
      "Current iteration=4, loss=-76473.95306328345\n",
      "Current iteration=6, loss=-120241.87960993273\n",
      "Current iteration=8, loss=-160687.5869396995\n",
      "loss=-199457.76234239966\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25412.761461706494\n",
      "Current iteration=4, loss=-77231.97632841463\n",
      "Current iteration=6, loss=-121305.93825306035\n",
      "Current iteration=8, loss=-162060.7750713008\n",
      "loss=-201147.25589280916\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25420.662315840502\n",
      "Current iteration=4, loss=-77256.62208476134\n",
      "Current iteration=6, loss=-121343.19754728921\n",
      "Current iteration=8, loss=-162103.8425407496\n",
      "loss=-201192.13091080502\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24757.940260540057\n",
      "Current iteration=4, loss=-76102.26773919098\n",
      "Current iteration=6, loss=-119746.09560653665\n",
      "Current iteration=8, loss=-160079.13516744453\n",
      "loss=-198744.0251898497\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24975.076625375543\n",
      "Current iteration=4, loss=-76472.8255912139\n",
      "Current iteration=6, loss=-120239.53685886055\n",
      "Current iteration=8, loss=-160683.6507864195\n",
      "loss=-199455.11579007885\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-25411.908150594365\n",
      "Current iteration=4, loss=-77228.81977067693\n",
      "Current iteration=6, loss=-121299.37672884815\n",
      "Current iteration=8, loss=-162049.74714159052\n",
      "loss=-201139.83552934625\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-25419.8089226825\n",
      "Current iteration=4, loss=-77253.46499747889\n",
      "Current iteration=6, loss=-121336.63488453763\n",
      "Current iteration=8, loss=-162092.8130085606\n",
      "loss=-201184.7101229639\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-24757.09420427313\n",
      "Current iteration=4, loss=-76099.13963437549\n",
      "Current iteration=6, loss=-119739.5957200104\n",
      "Current iteration=8, loss=-160068.21446418253\n",
      "loss=-198736.68229836467\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-24974.228178842768\n",
      "Current iteration=4, loss=-76469.68839877969\n",
      "Current iteration=6, loss=-120233.01824109792\n",
      "Current iteration=8, loss=-160672.6987376477\n",
      "loss=-199447.75179627468\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26915.22119973678\n",
      "Current iteration=4, loss=-79567.45364164015\n",
      "Current iteration=6, loss=-124419.0574645748\n",
      "Current iteration=8, loss=-165962.72588650955\n",
      "loss=-205848.65149669634\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26923.34247387196\n",
      "Current iteration=4, loss=-79592.8447196377\n",
      "Current iteration=6, loss=-124456.92822157194\n",
      "Current iteration=8, loss=-166006.1140232585\n",
      "loss=-205893.60173190874\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26246.925109590233\n",
      "Current iteration=4, loss=-78415.69954216147\n",
      "Current iteration=6, loss=-122827.88304995146\n",
      "Current iteration=8, loss=-163939.8445080301\n",
      "loss=-203394.02854078778\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26468.52867189758\n",
      "Current iteration=4, loss=-78792.882489384\n",
      "Current iteration=6, loss=-123329.85849713575\n",
      "Current iteration=8, loss=-164554.99646482946\n",
      "loss=-204118.02574305696\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26915.22095178543\n",
      "Current iteration=4, loss=-79567.45272566372\n",
      "Current iteration=6, loss=-124419.05556016108\n",
      "Current iteration=8, loss=-165962.72268432987\n",
      "loss=-205848.64933889676\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26923.34222589674\n",
      "Current iteration=4, loss=-79592.84380350646\n",
      "Current iteration=6, loss=-124456.92631683088\n",
      "Current iteration=8, loss=-166006.11082062224\n",
      "loss=-205893.59957399074\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26246.924863750108\n",
      "Current iteration=4, loss=-78415.69863445703\n",
      "Current iteration=6, loss=-122827.88116347145\n",
      "Current iteration=8, loss=-163939.84133706873\n",
      "loss=-203394.02640557158\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26468.528425362067\n",
      "Current iteration=4, loss=-78792.88157904375\n",
      "Current iteration=6, loss=-123329.85660522191\n",
      "Current iteration=8, loss=-164554.99328476703\n",
      "loss=-204118.02360169744\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26915.220261846178\n",
      "Current iteration=4, loss=-79567.45017690494\n",
      "Current iteration=6, loss=-124419.0502610169\n",
      "Current iteration=8, loss=-165962.713774075\n",
      "loss=-205848.64333469083\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26923.341535891082\n",
      "Current iteration=4, loss=-79592.84125431681\n",
      "Current iteration=6, loss=-124456.92101677606\n",
      "Current iteration=8, loss=-166006.10190909693\n",
      "loss=-205893.59356945546\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26246.92417968535\n",
      "Current iteration=4, loss=-78415.69610871558\n",
      "Current iteration=6, loss=-122827.875914229\n",
      "Current iteration=8, loss=-163939.83251368097\n",
      "loss=-203394.02046420588\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26468.527739362333\n",
      "Current iteration=4, loss=-78792.87904596793\n",
      "Current iteration=6, loss=-123329.85134085912\n",
      "Current iteration=8, loss=-164554.98443605492\n",
      "loss=-204118.01764323725\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26915.218342049142\n",
      "Current iteration=4, loss=-79567.44308483259\n",
      "Current iteration=6, loss=-124419.03551583462\n",
      "Current iteration=8, loss=-165962.6889807643\n",
      "loss=-205848.62662763233\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26923.339615909303\n",
      "Current iteration=4, loss=-79592.83416104555\n",
      "Current iteration=6, loss=-124456.90626905962\n",
      "Current iteration=8, loss=-166006.07711225087\n",
      "loss=-205893.57686148048\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26246.922276234618\n",
      "Current iteration=4, loss=-78415.68908069032\n",
      "Current iteration=6, loss=-122827.86130790117\n",
      "Current iteration=8, loss=-163939.80796208288\n",
      "loss=-203394.0039320038\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26468.52583052727\n",
      "Current iteration=4, loss=-78792.87199753431\n",
      "Current iteration=6, loss=-123329.83669245824\n",
      "Current iteration=8, loss=-164554.9598139904\n",
      "loss=-204118.00106346887\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26915.213000100124\n",
      "Current iteration=4, loss=-79567.42335072251\n",
      "Current iteration=6, loss=-124418.99448649782\n",
      "Current iteration=8, loss=-165962.61999192502\n",
      "loss=-205848.58013925722\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26923.334273446177\n",
      "Current iteration=4, loss=-79592.81442359957\n",
      "Current iteration=6, loss=-124456.86523267138\n",
      "Current iteration=8, loss=-166006.0081135744\n",
      "loss=-205893.53037055512\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26246.916979770038\n",
      "Current iteration=4, loss=-78415.669524795\n",
      "Current iteration=6, loss=-122827.82066493503\n",
      "Current iteration=8, loss=-163939.7396458231\n",
      "loss=-203393.95793017698\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26468.5205190806\n",
      "Current iteration=4, loss=-78792.85238485162\n",
      "Current iteration=6, loss=-123329.79593242136\n",
      "Current iteration=8, loss=-164554.89130165364\n",
      "loss=-204117.95492928606\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26915.198135811468\n",
      "Current iteration=4, loss=-79567.36843940859\n",
      "Current iteration=6, loss=-124418.8803199985\n",
      "Current iteration=8, loss=-165962.42802654076\n",
      "loss=-205848.45078265044\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26923.319407727053\n",
      "Current iteration=4, loss=-79592.75950300318\n",
      "Current iteration=6, loss=-124456.75104655119\n",
      "Current iteration=8, loss=-166005.81612081756\n",
      "loss=-205893.40100685225\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26246.902242044587\n",
      "Current iteration=4, loss=-78415.61510937408\n",
      "Current iteration=6, loss=-122827.7075735344\n",
      "Current iteration=8, loss=-163939.54955192996\n",
      "loss=-203393.82992741914\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26468.505739666627\n",
      "Current iteration=4, loss=-78792.79781141646\n",
      "Current iteration=6, loss=-123329.68251526475\n",
      "Current iteration=8, loss=-164554.70066216495\n",
      "loss=-204117.8265582402\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26915.1567750595\n",
      "Current iteration=4, loss=-79567.21564556775\n",
      "Current iteration=6, loss=-124418.56264545712\n",
      "Current iteration=8, loss=-165961.89387268882\n",
      "loss=-205848.0908406627\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26923.27804299463\n",
      "Current iteration=4, loss=-79592.6066833334\n",
      "Current iteration=6, loss=-124456.43331741361\n",
      "Current iteration=8, loss=-166005.2818908001\n",
      "loss=-205893.04104511923\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26246.861233462078\n",
      "Current iteration=4, loss=-78415.4636953835\n",
      "Current iteration=6, loss=-122827.39289051405\n",
      "Current iteration=8, loss=-163939.02060560082\n",
      "loss=-203393.47375259126\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26468.46461508325\n",
      "Current iteration=4, loss=-78792.64595774244\n",
      "Current iteration=6, loss=-123329.36692581032\n",
      "Current iteration=8, loss=-164554.1701976877\n",
      "loss=-204117.4693586302\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26915.04168641969\n",
      "Current iteration=4, loss=-79566.79048880597\n",
      "Current iteration=6, loss=-124417.67870123681\n",
      "Current iteration=8, loss=-165960.40756741122\n",
      "loss=-205847.0892842186\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26923.162943279076\n",
      "Current iteration=4, loss=-79592.18145470133\n",
      "Current iteration=6, loss=-124455.5492212765\n",
      "Current iteration=8, loss=-166003.79537358842\n",
      "loss=-205892.03943373286\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26246.74712475392\n",
      "Current iteration=4, loss=-78415.04237812663\n",
      "Current iteration=6, loss=-122826.51727034\n",
      "Current iteration=8, loss=-163937.54879047177\n",
      "loss=-203392.48267845472\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26468.350183596074\n",
      "Current iteration=4, loss=-78792.22341704348\n",
      "Current iteration=6, loss=-123328.48878344144\n",
      "Current iteration=8, loss=-164552.69415824857\n",
      "loss=-204116.4754329872\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26914.72144629096\n",
      "Current iteration=4, loss=-79565.60747397227\n",
      "Current iteration=6, loss=-124415.21910538197\n",
      "Current iteration=8, loss=-165956.27190859083\n",
      "loss=-205844.3024211579\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26922.84267233158\n",
      "Current iteration=4, loss=-79590.99823988523\n",
      "Current iteration=6, loss=-124453.08920270806\n",
      "Current iteration=8, loss=-165999.65912505737\n",
      "loss=-205889.2524177927\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26246.42961133621\n",
      "Current iteration=4, loss=-78413.87004686076\n",
      "Current iteration=6, loss=-122824.08083634083\n",
      "Current iteration=8, loss=-163933.45345062844\n",
      "loss=-203389.72498273794\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26468.031772028902\n",
      "Current iteration=4, loss=-78791.04768150343\n",
      "Current iteration=6, loss=-123326.04533137588\n",
      "Current iteration=8, loss=-164548.5870642261\n",
      "loss=-204113.70980286758\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26913.830365675713\n",
      "Current iteration=4, loss=-79562.31573546604\n",
      "Current iteration=6, loss=-124408.37537728364\n",
      "Current iteration=8, loss=-165944.76476445078\n",
      "loss=-205836.54802004914\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-26921.951505962526\n",
      "Current iteration=4, loss=-79587.70594492591\n",
      "Current iteration=6, loss=-124446.24429841465\n",
      "Current iteration=8, loss=-165988.15034007732\n",
      "loss=-205881.49759129464\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-26245.546117902497\n",
      "Current iteration=4, loss=-78410.60803537854\n",
      "Current iteration=6, loss=-122817.30155517596\n",
      "Current iteration=8, loss=-163922.05849084636\n",
      "loss=-203382.05173917708\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-26467.145779458424\n",
      "Current iteration=4, loss=-78787.77619762713\n",
      "Current iteration=6, loss=-123319.24652272518\n",
      "Current iteration=8, loss=-164537.15939940128\n",
      "loss=-204106.01448200372\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28407.61057496788\n",
      "Current iteration=4, loss=-81886.85896284717\n",
      "Current iteration=6, loss=-127514.08997653345\n",
      "Current iteration=8, loss=-169845.3401397156\n",
      "loss=-210534.06580583338\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28415.9637924683\n",
      "Current iteration=4, loss=-81912.98462967106\n",
      "Current iteration=6, loss=-127552.54163797968\n",
      "Current iteration=8, loss=-169889.01760422578\n",
      "loss=-210579.06879332097\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27725.915712050304\n",
      "Current iteration=4, loss=-80713.14455496903\n",
      "Current iteration=6, loss=-125891.641470213\n",
      "Current iteration=8, loss=-167781.26739458618\n",
      "loss=-208028.06607683253\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27951.952066923805\n",
      "Current iteration=4, loss=-81096.88940705617\n",
      "Current iteration=6, loss=-126402.09127823253\n",
      "Current iteration=8, loss=-168407.00586700498\n",
      "loss=-208764.94128821557\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28407.61031630562\n",
      "Current iteration=4, loss=-81886.85800854528\n",
      "Current iteration=6, loss=-127514.08799204856\n",
      "Current iteration=8, loss=-169845.33680147922\n",
      "loss=-210534.06355306666\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28415.96353378113\n",
      "Current iteration=4, loss=-81912.98367520668\n",
      "Current iteration=6, loss=-127552.53965315707\n",
      "Current iteration=8, loss=-169889.01426552213\n",
      "loss=-210579.06654043528\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27725.915455593673\n",
      "Current iteration=4, loss=-80713.14360930125\n",
      "Current iteration=6, loss=-125891.63950446113\n",
      "Current iteration=8, loss=-167781.26408897934\n",
      "loss=-208028.06384769865\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27951.95180974088\n",
      "Current iteration=4, loss=-81096.88845864356\n",
      "Current iteration=6, loss=-126402.08930682074\n",
      "Current iteration=8, loss=-168407.00255191064\n",
      "loss=-208764.9390526607\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28407.6095965626\n",
      "Current iteration=4, loss=-81886.85535314359\n",
      "Current iteration=6, loss=-127514.08247010157\n",
      "Current iteration=8, loss=-169845.3275126383\n",
      "loss=-210534.0572846096\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28415.96281396866\n",
      "Current iteration=4, loss=-81912.9810193531\n",
      "Current iteration=6, loss=-127552.53413027049\n",
      "Current iteration=8, loss=-169889.0049753814\n",
      "loss=-210579.06027164726\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27725.914741987817\n",
      "Current iteration=4, loss=-80713.14097792443\n",
      "Current iteration=6, loss=-125891.63403463988\n",
      "Current iteration=8, loss=-167781.25489093224\n",
      "loss=-208028.05764500133\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27951.951094114127\n",
      "Current iteration=4, loss=-81096.88581962915\n",
      "Current iteration=6, loss=-126402.08382125058\n",
      "Current iteration=8, loss=-168406.9933274644\n",
      "loss=-208764.93283209647\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28407.607593834924\n",
      "Current iteration=4, loss=-81886.84796433101\n",
      "Current iteration=6, loss=-127514.06710495718\n",
      "Current iteration=8, loss=-169845.30166588968\n",
      "loss=-210534.03984225646\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28415.960811047793\n",
      "Current iteration=4, loss=-81912.97362928299\n",
      "Current iteration=6, loss=-127552.51876251173\n",
      "Current iteration=8, loss=-169888.9791250157\n",
      "loss=-210579.04282837326\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27725.912756337246\n",
      "Current iteration=4, loss=-80713.13365596256\n",
      "Current iteration=6, loss=-125891.61881453873\n",
      "Current iteration=8, loss=-167781.22929682257\n",
      "loss=-208028.04038562858\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27951.949102840197\n",
      "Current iteration=4, loss=-81096.8784764152\n",
      "Current iteration=6, loss=-126402.06855732686\n",
      "Current iteration=8, loss=-168406.96765989737\n",
      "loss=-208764.91552300807\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28407.602021126357\n",
      "Current iteration=4, loss=-81886.82740452388\n",
      "Current iteration=6, loss=-127514.02435053981\n",
      "Current iteration=8, loss=-169845.22974579822\n",
      "loss=-210533.99130788105\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28415.955237801776\n",
      "Current iteration=4, loss=-81912.95306597657\n",
      "Current iteration=6, loss=-127552.47600081925\n",
      "Current iteration=8, loss=-169888.90719485938\n",
      "loss=-210578.9942914353\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27725.90723114682\n",
      "Current iteration=4, loss=-80713.11328217131\n",
      "Current iteration=6, loss=-125891.57646371229\n",
      "Current iteration=8, loss=-167781.15807971367\n",
      "loss=-208027.9923604065\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27951.94356200244\n",
      "Current iteration=4, loss=-81096.85804348886\n",
      "Current iteration=6, loss=-126402.02608456173\n",
      "Current iteration=8, loss=-168406.89623838934\n",
      "loss=-208764.86735944945\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28407.5865147358\n",
      "Current iteration=4, loss=-81886.7701956601\n",
      "Current iteration=6, loss=-127513.90538390586\n",
      "Current iteration=8, loss=-169845.02962404146\n",
      "loss=-210533.85625816107\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28415.93972991561\n",
      "Current iteration=4, loss=-81912.89584737598\n",
      "Current iteration=6, loss=-127552.3570139421\n",
      "Current iteration=8, loss=-169888.70704509664\n",
      "loss=-210578.85923458502\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27725.89185697817\n",
      "Current iteration=4, loss=-80713.05659090762\n",
      "Current iteration=6, loss=-125891.45862009349\n",
      "Current iteration=8, loss=-167780.95991404605\n",
      "loss=-208027.8587274354\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27951.928144294157\n",
      "Current iteration=4, loss=-81096.80118767833\n",
      "Current iteration=6, loss=-126401.90790164168\n",
      "Current iteration=8, loss=-168406.69750396942\n",
      "loss=-208764.73334154897\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28407.543367298054\n",
      "Current iteration=4, loss=-81886.61100876056\n",
      "Current iteration=6, loss=-127513.57435274003\n",
      "Current iteration=8, loss=-169844.47277468175\n",
      "loss=-210533.48047477915\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28415.896578316304\n",
      "Current iteration=4, loss=-81912.73663338325\n",
      "Current iteration=6, loss=-127552.02592644855\n",
      "Current iteration=8, loss=-169888.15011780872\n",
      "loss=-210578.4834313626\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27725.84907745563\n",
      "Current iteration=4, loss=-80712.89884425973\n",
      "Current iteration=6, loss=-125891.13071377872\n",
      "Current iteration=8, loss=-167780.40850760764\n",
      "loss=-208027.4868862363\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27951.885243620127\n",
      "Current iteration=4, loss=-81096.64298316944\n",
      "Current iteration=6, loss=-126401.5790512025\n",
      "Current iteration=8, loss=-168406.14451494755\n",
      "loss=-208764.36042926216\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28407.42330710652\n",
      "Current iteration=4, loss=-81886.16806301268\n",
      "Current iteration=6, loss=-127512.65324319231\n",
      "Current iteration=8, loss=-169842.9233184495\n",
      "loss=-210532.4348389608\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28415.77650654507\n",
      "Current iteration=4, loss=-81912.29361224703\n",
      "Current iteration=6, loss=-127551.10466016612\n",
      "Current iteration=8, loss=-169886.60044473794\n",
      "loss=-210577.4377403371\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27725.73004100913\n",
      "Current iteration=4, loss=-80712.45990608625\n",
      "Current iteration=6, loss=-125890.21829927372\n",
      "Current iteration=8, loss=-167778.87419652785\n",
      "loss=-208026.45221973426\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27951.76587006267\n",
      "Current iteration=4, loss=-81096.20277097408\n",
      "Current iteration=6, loss=-126400.66400962761\n",
      "Current iteration=8, loss=-168404.6058002649\n",
      "loss=-208763.32278240653\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28407.08923339877\n",
      "Current iteration=4, loss=-81884.9355498835\n",
      "Current iteration=6, loss=-127510.09023472575\n",
      "Current iteration=8, loss=-169838.61194333414\n",
      "loss=-210529.52532432796\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28415.442400616088\n",
      "Current iteration=4, loss=-81911.06088934654\n",
      "Current iteration=6, loss=-127548.54121557991\n",
      "Current iteration=8, loss=-169882.28846626522\n",
      "loss=-210574.52807208805\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27725.39881592518\n",
      "Current iteration=4, loss=-80711.23854418246\n",
      "Current iteration=6, loss=-125887.6794849614\n",
      "Current iteration=8, loss=-167774.6049629262\n",
      "loss=-208023.5732275549\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27951.433706949814\n",
      "Current iteration=4, loss=-81094.9778640566\n",
      "Current iteration=6, loss=-126398.117885434\n",
      "Current iteration=8, loss=-168400.3243136056\n",
      "loss=-208760.43549730565\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-28406.1596604837\n",
      "Current iteration=4, loss=-81881.50608419171\n",
      "Current iteration=6, loss=-127502.95877129941\n",
      "Current iteration=8, loss=-169826.61589677676\n",
      "loss=-210521.4296526823\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-28414.512738044916\n",
      "Current iteration=4, loss=-81907.63083996395\n",
      "Current iteration=6, loss=-127541.40853865736\n",
      "Current iteration=8, loss=-169870.29074089945\n",
      "loss=-210566.43197300442\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-27724.477169417438\n",
      "Current iteration=4, loss=-80707.84010675538\n",
      "Current iteration=6, loss=-125880.61534074065\n",
      "Current iteration=8, loss=-167762.72617165523\n",
      "loss=-208015.5624839674\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-27950.50945033971\n",
      "Current iteration=4, loss=-81091.56956263309\n",
      "Current iteration=6, loss=-126391.03340178302\n",
      "Current iteration=8, loss=-168388.41142924535\n",
      "loss=-208752.40167886083\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29890.694743221233\n",
      "Current iteration=4, loss=-84192.5686987217\n",
      "Current iteration=6, loss=-130595.45821168658\n",
      "Current iteration=8, loss=-173715.54938496853\n",
      "loss=-215208.35950519875\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29899.291313199163\n",
      "Current iteration=4, loss=-84219.4178749001\n",
      "Current iteration=6, loss=-130634.46180624676\n",
      "Current iteration=8, loss=-173759.4877085485\n",
      "loss=-215253.39525835757\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29195.671074091286\n",
      "Current iteration=4, loss=-82996.95808637915\n",
      "Current iteration=6, loss=-128941.75470009098\n",
      "Current iteration=8, loss=-171610.27328095812\n",
      "loss=-212650.95471783847\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29426.10764394443\n",
      "Current iteration=4, loss=-83387.2092579028\n",
      "Current iteration=6, loss=-129460.63282076977\n",
      "Current iteration=8, loss=-172246.56903209083\n",
      "loss=-213400.69344198002\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29890.694473647705\n",
      "Current iteration=4, loss=-84192.56770538646\n",
      "Current iteration=6, loss=-130595.45614559145\n",
      "Current iteration=8, loss=-173715.54590797954\n",
      "loss=-215208.35715545353\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29899.291043599544\n",
      "Current iteration=4, loss=-84219.4168813948\n",
      "Current iteration=6, loss=-130634.45973980363\n",
      "Current iteration=8, loss=-173759.4842310819\n",
      "loss=-215253.39290849247\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29195.67080681966\n",
      "Current iteration=4, loss=-82996.95710204767\n",
      "Current iteration=6, loss=-128941.75265354577\n",
      "Current iteration=8, loss=-171610.26983804142\n",
      "loss=-212650.95239279885\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29426.10737591501\n",
      "Current iteration=4, loss=-83387.20827071565\n",
      "Current iteration=6, loss=-129460.63076833423\n",
      "Current iteration=8, loss=-172246.56557929242\n",
      "loss=-213400.69111023555\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29890.693723543318\n",
      "Current iteration=4, loss=-84192.56494137224\n",
      "Current iteration=6, loss=-130595.45039655924\n",
      "Current iteration=8, loss=-173715.5362330515\n",
      "loss=-215208.35061714807\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29899.29029342253\n",
      "Current iteration=4, loss=-84219.41411690733\n",
      "Current iteration=6, loss=-130634.4539898032\n",
      "Current iteration=8, loss=-173759.47455482473\n",
      "loss=-215253.3863698536\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29195.670063120448\n",
      "Current iteration=4, loss=-82996.9543630869\n",
      "Current iteration=6, loss=-128941.74695891226\n",
      "Current iteration=8, loss=-171610.26025792124\n",
      "loss=-212650.94592323832\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29426.106630107348\n",
      "Current iteration=4, loss=-83387.2055238088\n",
      "Current iteration=6, loss=-129460.6250573109\n",
      "Current iteration=8, loss=-172246.55597167625\n",
      "loss=-213400.68462201837\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29890.691636333308\n",
      "Current iteration=4, loss=-84192.5572503389\n",
      "Current iteration=6, loss=-130595.43439953687\n",
      "Current iteration=8, loss=-173715.5093119926\n",
      "loss=-215208.33242392592\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29899.28820601054\n",
      "Current iteration=4, loss=-84219.40642455702\n",
      "Current iteration=6, loss=-130634.43799008674\n",
      "Current iteration=8, loss=-173759.44763006762\n",
      "loss=-215253.36817570362\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29195.66799373323\n",
      "Current iteration=4, loss=-82996.94674176637\n",
      "Current iteration=6, loss=-128941.73111325769\n",
      "Current iteration=8, loss=-171610.2336006712\n",
      "loss=-212650.92792130302\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29426.10455485325\n",
      "Current iteration=4, loss=-83387.19788037779\n",
      "Current iteration=6, loss=-129460.60916605043\n",
      "Current iteration=8, loss=-172246.52923791652\n",
      "loss=-213400.66656816966\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29890.685828547812\n",
      "Current iteration=4, loss=-84192.53584958454\n",
      "Current iteration=6, loss=-130595.3898868818\n",
      "Current iteration=8, loss=-173715.4344025704\n",
      "loss=-215208.2818002131\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29899.282397662937\n",
      "Current iteration=4, loss=-84219.38502013832\n",
      "Current iteration=6, loss=-130634.39346993505\n",
      "Current iteration=8, loss=-173759.3727103547\n",
      "loss=-215253.31754940923\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29195.66223554067\n",
      "Current iteration=4, loss=-82996.92553499209\n",
      "Current iteration=6, loss=-128941.68702179215\n",
      "Current iteration=8, loss=-171610.15942531233\n",
      "loss=-212650.877829857\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29426.098780335684\n",
      "Current iteration=4, loss=-83387.17661207974\n",
      "Current iteration=6, loss=-129460.56494768405\n",
      "Current iteration=8, loss=-172246.45484966514\n",
      "loss=-213400.6163322716\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29890.669668041726\n",
      "Current iteration=4, loss=-84192.47630073667\n",
      "Current iteration=6, loss=-130595.266027852\n",
      "Current iteration=8, loss=-173715.22596283437\n",
      "loss=-215208.14093679198\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29899.26623559282\n",
      "Current iteration=4, loss=-84219.32546109414\n",
      "Current iteration=6, loss=-130634.26959004546\n",
      "Current iteration=8, loss=-173759.16424198446\n",
      "loss=-215253.17667880474\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29195.646213029904\n",
      "Current iteration=4, loss=-82996.86652590497\n",
      "Current iteration=6, loss=-128941.56433474667\n",
      "Current iteration=8, loss=-171609.95302814967\n",
      "loss=-212650.73844749943\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29426.08271239959\n",
      "Current iteration=4, loss=-83387.11743179924\n",
      "Current iteration=6, loss=-129460.44190752943\n",
      "Current iteration=8, loss=-172246.24786011697\n",
      "loss=-213400.4765479678\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29890.624700489774\n",
      "Current iteration=4, loss=-84192.31060270299\n",
      "Current iteration=6, loss=-130594.92138334074\n",
      "Current iteration=8, loss=-173714.64596829005\n",
      "loss=-215207.74897647445\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29899.22126368879\n",
      "Current iteration=4, loss=-84219.1597346887\n",
      "Current iteration=6, loss=-130633.92488749076\n",
      "Current iteration=8, loss=-173758.58416776356\n",
      "loss=-215252.78469849925\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29195.60162945796\n",
      "Current iteration=4, loss=-82996.70232978619\n",
      "Current iteration=6, loss=-128941.2229513458\n",
      "Current iteration=8, loss=-171609.37871717315\n",
      "loss=-212650.3506083233\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29426.03800242904\n",
      "Current iteration=4, loss=-83386.95275932514\n",
      "Current iteration=6, loss=-129460.09954158321\n",
      "Current iteration=8, loss=-172245.67190079656\n",
      "loss=-213400.08759035415\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29890.499575730355\n",
      "Current iteration=4, loss=-84191.84953942063\n",
      "Current iteration=6, loss=-130593.96239413641\n",
      "Current iteration=8, loss=-173713.03210987878\n",
      "loss=-215206.6583276291\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29899.096126819462\n",
      "Current iteration=4, loss=-84218.69859246025\n",
      "Current iteration=6, loss=-130632.9657367776\n",
      "Current iteration=8, loss=-173756.9700876489\n",
      "loss=-215251.69399403597\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29195.47757314464\n",
      "Current iteration=4, loss=-82996.24544565905\n",
      "Current iteration=6, loss=-128940.27303633087\n",
      "Current iteration=8, loss=-171607.78067352035\n",
      "loss=-212649.27142675442\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29425.913594404457\n",
      "Current iteration=4, loss=-83386.4945497148\n",
      "Current iteration=6, loss=-129459.14689259088\n",
      "Current iteration=8, loss=-172244.06927056046\n",
      "loss=-213399.0052966786\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29890.151409622886\n",
      "Current iteration=4, loss=-84190.5666138167\n",
      "Current iteration=6, loss=-130591.29398548542\n",
      "Current iteration=8, loss=-173708.5415370767\n",
      "loss=-215203.62356356505\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29898.747927015742\n",
      "Current iteration=4, loss=-84217.41544718575\n",
      "Current iteration=6, loss=-130630.29687872312\n",
      "Current iteration=8, loss=-173752.47889795332\n",
      "loss=-215248.65907521284\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29195.132380043982\n",
      "Current iteration=4, loss=-82994.97414870866\n",
      "Current iteration=6, loss=-128937.62987681056\n",
      "Current iteration=8, loss=-171603.33410539856\n",
      "loss=-212646.26857072552\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29425.567422648925\n",
      "Current iteration=4, loss=-83385.21956455836\n",
      "Current iteration=6, loss=-129456.49612571916\n",
      "Current iteration=8, loss=-172239.60994024228\n",
      "loss=-213395.9937811236\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29889.182624244415\n",
      "Current iteration=4, loss=-84186.99687738666\n",
      "Current iteration=6, loss=-130583.86925689851\n",
      "Current iteration=8, loss=-173696.046902588\n",
      "loss=-215195.17939334686\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-29897.779047876706\n",
      "Current iteration=4, loss=-84213.84509952001\n",
      "Current iteration=6, loss=-130622.87089967844\n",
      "Current iteration=8, loss=-173739.98254699574\n",
      "loss=-215240.21447437903\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-29194.171867172023\n",
      "Current iteration=4, loss=-82991.4367689689\n",
      "Current iteration=6, loss=-128930.27540279664\n",
      "Current iteration=8, loss=-171590.96191016168\n",
      "loss=-212637.9131838588\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-29424.604186631474\n",
      "Current iteration=4, loss=-83381.67192239531\n",
      "Current iteration=6, loss=-129449.12048459623\n",
      "Current iteration=8, loss=-172227.20223532539\n",
      "loss=-213387.61429934856\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31364.750704119084\n",
      "Current iteration=4, loss=-86485.15915484064\n",
      "Current iteration=6, loss=-133663.8596902464\n",
      "Current iteration=8, loss=-177574.0487827557\n",
      "loss=-219872.18039229797\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31373.601848071285\n",
      "Current iteration=4, loss=-86512.72021098262\n",
      "Current iteration=6, loss=-133703.38722446765\n",
      "Current iteration=8, loss=-177618.22139115052\n",
      "loss=-219917.23101679696\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30656.466145390157\n",
      "Current iteration=4, loss=-85267.71161482972\n",
      "Current iteration=6, loss=-131978.91662449582\n",
      "Current iteration=8, loss=-175427.55565203057\n",
      "loss=-217263.3421047256\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30891.270845865525\n",
      "Current iteration=4, loss=-85664.4159414811\n",
      "Current iteration=6, loss=-132506.18008755604\n",
      "Current iteration=8, loss=-176074.38220617082\n",
      "loss=-218025.93187322663\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31364.750423434514\n",
      "Current iteration=4, loss=-86485.15812176498\n",
      "Current iteration=6, loss=-133663.85754100245\n",
      "Current iteration=8, loss=-177574.04516431902\n",
      "loss=-219872.17794356402\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31373.601567359437\n",
      "Current iteration=4, loss=-86512.71917772912\n",
      "Current iteration=6, loss=-133703.38507486548\n",
      "Current iteration=8, loss=-177618.21777222573\n",
      "loss=-219917.22856794205\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30656.46586710574\n",
      "Current iteration=4, loss=-85267.71059113504\n",
      "Current iteration=6, loss=-131978.91449563619\n",
      "Current iteration=8, loss=-175427.55206914013\n",
      "loss=-217263.33968179373\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30891.270566791307\n",
      "Current iteration=4, loss=-85664.41491481794\n",
      "Current iteration=6, loss=-132506.17795257145\n",
      "Current iteration=8, loss=-176074.37861299657\n",
      "loss=-218025.92944329983\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31364.749642413044\n",
      "Current iteration=4, loss=-86485.15524717043\n",
      "Current iteration=6, loss=-133663.8515606035\n",
      "Current iteration=8, loss=-177574.03509580423\n",
      "loss=-219872.1711298166\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31373.600786262014\n",
      "Current iteration=4, loss=-86512.7163026398\n",
      "Current iteration=6, loss=-133703.37909347002\n",
      "Current iteration=8, loss=-177618.20770235293\n",
      "loss=-219917.22175385768\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30656.4650927627\n",
      "Current iteration=4, loss=-85267.70774264373\n",
      "Current iteration=6, loss=-131978.90857195793\n",
      "Current iteration=8, loss=-175427.54209953523\n",
      "loss=-217263.33293984193\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30891.26979025073\n",
      "Current iteration=4, loss=-85664.4120580668\n",
      "Current iteration=6, loss=-132506.17201185017\n",
      "Current iteration=8, loss=-176074.36861477644\n",
      "loss=-218025.9226818841\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31364.7474691744\n",
      "Current iteration=4, loss=-86485.14724844095\n",
      "Current iteration=6, loss=-133663.83491978972\n",
      "Current iteration=8, loss=-177574.00707956712\n",
      "loss=-219872.15217016073\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31373.598612812093\n",
      "Current iteration=4, loss=-86512.70830253343\n",
      "Current iteration=6, loss=-133703.3624498831\n",
      "Current iteration=8, loss=-177618.1796823368\n",
      "loss=-219917.2027932646\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30656.462938107292\n",
      "Current iteration=4, loss=-85267.69981654795\n",
      "Current iteration=6, loss=-131978.89208897259\n",
      "Current iteration=8, loss=-175427.51435852045\n",
      "loss=-217263.31417996183\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30891.267629480404\n",
      "Current iteration=4, loss=-85664.40410898735\n",
      "Current iteration=6, loss=-132506.15548144173\n",
      "Current iteration=8, loss=-176074.340794138\n",
      "loss=-218025.9038678445\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31364.741422009127\n",
      "Current iteration=4, loss=-86485.12499150411\n",
      "Current iteration=6, loss=-133663.7886157475\n",
      "Current iteration=8, loss=-177573.92912274797\n",
      "loss=-219872.09941380125\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31373.592565058865\n",
      "Current iteration=4, loss=-86512.68604176538\n",
      "Current iteration=6, loss=-133703.3161381246\n",
      "Current iteration=8, loss=-177618.10171500256\n",
      "loss=-219917.15003429726\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30656.45694265092\n",
      "Current iteration=4, loss=-85267.67776171851\n",
      "Current iteration=6, loss=-131978.84622409727\n",
      "Current iteration=8, loss=-175427.43716752334\n",
      "loss=-217263.2619794901\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30891.261617008982\n",
      "Current iteration=4, loss=-85664.38199020464\n",
      "Current iteration=6, loss=-132506.10948460863\n",
      "Current iteration=8, loss=-176074.26338158388\n",
      "loss=-218025.85151667084\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31364.72459541476\n",
      "Current iteration=4, loss=-86485.06306027903\n",
      "Current iteration=6, loss=-133663.6597720815\n",
      "Current iteration=8, loss=-177573.71220346072\n",
      "loss=-219871.95261616883\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31373.57573682846\n",
      "Current iteration=4, loss=-86512.62409987969\n",
      "Current iteration=6, loss=-133703.18727298756\n",
      "Current iteration=8, loss=-177617.88476645635\n",
      "loss=-219917.0032294081\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30656.440259939638\n",
      "Current iteration=4, loss=-85267.61639286917\n",
      "Current iteration=6, loss=-131978.71860243854\n",
      "Current iteration=8, loss=-175427.2223791799\n",
      "loss=-217263.1167286476\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30891.244886952325\n",
      "Current iteration=4, loss=-85664.32044340149\n",
      "Current iteration=6, loss=-132505.98149576984\n",
      "Current iteration=8, loss=-176074.04797674494\n",
      "loss=-218025.7058464913\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31364.677774433596\n",
      "Current iteration=4, loss=-86484.89073315049\n",
      "Current iteration=6, loss=-133663.30125756245\n",
      "Current iteration=8, loss=-177573.10861415177\n",
      "loss=-219871.5441435904\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31373.52891129496\n",
      "Current iteration=4, loss=-86512.45174308747\n",
      "Current iteration=6, loss=-133702.82869872416\n",
      "Current iteration=8, loss=-177617.28109573282\n",
      "loss=-219916.5947366376\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30656.3938393216\n",
      "Current iteration=4, loss=-85267.44563058282\n",
      "Current iteration=6, loss=-131978.36348822125\n",
      "Current iteration=8, loss=-175426.62471933372\n",
      "loss=-217262.71256009786\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30891.198334592897\n",
      "Current iteration=4, loss=-85664.14918594867\n",
      "Current iteration=6, loss=-132505.62535985417\n",
      "Current iteration=8, loss=-176073.44860146786\n",
      "loss=-218025.3005111134\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31364.547492405545\n",
      "Current iteration=4, loss=-86484.41122410395\n",
      "Current iteration=6, loss=-133662.3036745347\n",
      "Current iteration=8, loss=-177571.4291026066\n",
      "loss=-219870.40754866647\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31373.398616599854\n",
      "Current iteration=4, loss=-86511.97215150016\n",
      "Current iteration=6, loss=-133701.83094945498\n",
      "Current iteration=8, loss=-177615.60135764803\n",
      "loss=-219915.45808552825\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30656.26467132667\n",
      "Current iteration=4, loss=-85266.97047578977\n",
      "Current iteration=6, loss=-131977.3753666884\n",
      "Current iteration=8, loss=-175424.9617067573\n",
      "loss=-217261.58794134232\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30891.068800020228\n",
      "Current iteration=4, loss=-85663.67265332968\n",
      "Current iteration=6, loss=-132504.63439539893\n",
      "Current iteration=8, loss=-176071.78081563688\n",
      "loss=-218024.17264560203\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31364.184975955974\n",
      "Current iteration=4, loss=-86483.0769727344\n",
      "Current iteration=6, loss=-133659.52787858105\n",
      "Current iteration=8, loss=-177566.75585148117\n",
      "loss=-219867.24493898696\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31373.036064903536\n",
      "Current iteration=4, loss=-86510.63767045739\n",
      "Current iteration=6, loss=-133699.0546909292\n",
      "Current iteration=8, loss=-177610.9274761726\n",
      "loss=-219912.29531951103\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30655.90525473182\n",
      "Current iteration=4, loss=-85265.64834028872\n",
      "Current iteration=6, loss=-131974.62589754348\n",
      "Current iteration=8, loss=-175420.33436411328\n",
      "loss=-217258.45865569712\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30890.70836340377\n",
      "Current iteration=4, loss=-85662.34668397748\n",
      "Current iteration=6, loss=-132501.8770157634\n",
      "Current iteration=8, loss=-176067.14019138788\n",
      "loss=-218021.0343257677\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-31363.17626039463\n",
      "Current iteration=4, loss=-86479.36442448881\n",
      "Current iteration=6, loss=-133651.8043562787\n",
      "Current iteration=8, loss=-177553.75294568084\n",
      "loss=-219858.44504683482\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-31372.027251267562\n",
      "Current iteration=4, loss=-86506.92448314396\n",
      "Current iteration=6, loss=-133691.3298815287\n",
      "Current iteration=8, loss=-177597.92281646456\n",
      "loss=-219903.49499235212\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-30654.90516463461\n",
      "Current iteration=4, loss=-85261.96950438975\n",
      "Current iteration=6, loss=-131966.97562834274\n",
      "Current iteration=8, loss=-175407.4591945808\n",
      "loss=-217249.75148682078\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-30889.70543505717\n",
      "Current iteration=4, loss=-85658.65718040381\n",
      "Current iteration=6, loss=-132494.20473600258\n",
      "Current iteration=8, loss=-176054.2280670132\n",
      "loss=-218012.30201951048\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32830.046206863444\n",
      "Current iteration=4, loss=-88765.17868228687\n",
      "Current iteration=6, loss=-136719.9503032538\n",
      "Current iteration=8, loss=-181421.48649115468\n",
      "loss=-224526.1296904503\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32839.16293452327\n",
      "Current iteration=4, loss=-88793.43952535783\n",
      "Current iteration=6, loss=-136759.97478141403\n",
      "Current iteration=8, loss=-181465.8686256682\n",
      "loss=-224571.17924355302\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32108.566658016225\n",
      "Current iteration=4, loss=-87525.94895743343\n",
      "Current iteration=6, loss=-135003.77984735483\n",
      "Current iteration=8, loss=-179233.76123983346\n",
      "loss=-221865.8294366083\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32347.707903475\n",
      "Current iteration=4, loss=-87929.05561099699\n",
      "Current iteration=6, loss=-135539.38857142313\n",
      "Current iteration=8, loss=-179891.09465903882\n",
      "loss=-222641.25959887044\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32830.04591486876\n",
      "Current iteration=4, loss=-88765.1776087641\n",
      "Current iteration=6, loss=-136719.9480693226\n",
      "Current iteration=8, loss=-181421.4827285755\n",
      "loss=-224526.12714071863\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32839.16264249999\n",
      "Current iteration=4, loss=-88793.43845164943\n",
      "Current iteration=6, loss=-136759.97254711448\n",
      "Current iteration=8, loss=-181465.8648625905\n",
      "loss=-224571.17669369874\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32108.5663685218\n",
      "Current iteration=4, loss=-87525.9478936765\n",
      "Current iteration=6, loss=-135003.7776346599\n",
      "Current iteration=8, loss=-179233.7575143062\n",
      "loss=-221865.8269137986\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32347.7076131582\n",
      "Current iteration=4, loss=-87929.05454415696\n",
      "Current iteration=6, loss=-135539.38635236418\n",
      "Current iteration=8, loss=-179891.09092281768\n",
      "loss=-222641.2570687695\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32830.04510237625\n",
      "Current iteration=4, loss=-88765.17462162332\n",
      "Current iteration=6, loss=-136719.9418532764\n",
      "Current iteration=8, loss=-181421.47225897587\n",
      "loss=-224526.120045939\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32839.16182992804\n",
      "Current iteration=4, loss=-88793.43546399205\n",
      "Current iteration=6, loss=-136759.96633004362\n",
      "Current iteration=8, loss=-181465.8543916039\n",
      "loss=-224571.16959857792\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32108.565562986274\n",
      "Current iteration=4, loss=-87525.94493370969\n",
      "Current iteration=6, loss=-135003.77147770516\n",
      "Current iteration=8, loss=-179233.74714780573\n",
      "loss=-221865.8198939308\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32347.706805334405\n",
      "Current iteration=4, loss=-87929.05157561117\n",
      "Current iteration=6, loss=-135539.38017770107\n",
      "Current iteration=8, loss=-179891.08052656072\n",
      "loss=-222641.2500286133\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32830.04284156749\n",
      "Current iteration=4, loss=-88765.16630972698\n",
      "Current iteration=6, loss=-136719.92455675994\n",
      "Current iteration=8, loss=-181421.44312669634\n",
      "loss=-224526.1003042945\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32839.15956889823\n",
      "Current iteration=4, loss=-88793.4271506583\n",
      "Current iteration=6, loss=-136759.94903067572\n",
      "Current iteration=8, loss=-181465.8252554651\n",
      "loss=-224571.14985598394\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32108.563321535956\n",
      "Current iteration=4, loss=-87525.9366974266\n",
      "Current iteration=6, loss=-135003.75434561408\n",
      "Current iteration=8, loss=-179233.71830240564\n",
      "loss=-221865.80036073303\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32347.704557516787\n",
      "Current iteration=4, loss=-87929.04331545666\n",
      "Current iteration=6, loss=-135539.36299633543\n",
      "Current iteration=8, loss=-179891.05159836155\n",
      "loss=-222641.23043896182\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32830.036550733195\n",
      "Current iteration=4, loss=-88765.143181385\n",
      "Current iteration=6, loss=-136719.87642818675\n",
      "Current iteration=8, loss=-181421.36206442467\n",
      "loss=-224526.04537200596\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32839.15327744884\n",
      "Current iteration=4, loss=-88793.40401831664\n",
      "Current iteration=6, loss=-136759.9008941684\n",
      "Current iteration=8, loss=-181465.74418245465\n",
      "loss=-224571.0949210534\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32108.55708456757\n",
      "Current iteration=4, loss=-87525.913779483\n",
      "Current iteration=6, loss=-135003.70667456428\n",
      "Current iteration=8, loss=-179233.6380383928\n",
      "loss=-221865.74600845965\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32347.698302831068\n",
      "Current iteration=4, loss=-87929.02033108924\n",
      "Current iteration=6, loss=-135539.3151881762\n",
      "Current iteration=8, loss=-179890.97110395538\n",
      "loss=-222641.17592960253\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32830.019046115456\n",
      "Current iteration=4, loss=-88765.0788254248\n",
      "Current iteration=6, loss=-136719.7425076601\n",
      "Current iteration=8, loss=-181421.13650404353\n",
      "loss=-224525.8925197263\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32839.13577111949\n",
      "Current iteration=4, loss=-88793.33965122711\n",
      "Current iteration=6, loss=-136759.7669515644\n",
      "Current iteration=8, loss=-181465.51859219203\n",
      "loss=-224570.94206142216\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32108.539729834913\n",
      "Current iteration=4, loss=-87525.8500089686\n",
      "Current iteration=6, loss=-135003.57402712267\n",
      "Current iteration=8, loss=-179233.4146992121\n",
      "loss=-221865.5947701058\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32347.680898798808\n",
      "Current iteration=4, loss=-87928.95637574675\n",
      "Current iteration=6, loss=-135539.18215921987\n",
      "Current iteration=8, loss=-179890.7471236922\n",
      "loss=-222641.0242541482\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32829.97033849513\n",
      "Current iteration=4, loss=-88764.89975133848\n",
      "Current iteration=6, loss=-136719.3698665149\n",
      "Current iteration=8, loss=-181420.50887047136\n",
      "loss=-224525.4671997683\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32839.087058736506\n",
      "Current iteration=4, loss=-88793.16054617267\n",
      "Current iteration=6, loss=-136759.3942489879\n",
      "Current iteration=8, loss=-181464.89087547356\n",
      "loss=-224570.51672100826\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32108.491439278598\n",
      "Current iteration=4, loss=-87525.67256391798\n",
      "Current iteration=6, loss=-135003.2049284056\n",
      "Current iteration=8, loss=-179232.7932462468\n",
      "loss=-221865.17394098575\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32347.63247106343\n",
      "Current iteration=4, loss=-87928.77841640174\n",
      "Current iteration=6, loss=-135538.81199891743\n",
      "Current iteration=8, loss=-179890.12388688157\n",
      "loss=-222640.6022087719\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32829.834806790204\n",
      "Current iteration=4, loss=-88764.40146857109\n",
      "Current iteration=6, loss=-136718.33297562046\n",
      "Current iteration=8, loss=-181418.76245506475\n",
      "loss=-224524.2837262897\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32838.95151377927\n",
      "Current iteration=4, loss=-88792.66217723493\n",
      "Current iteration=6, loss=-136758.35718715796\n",
      "Current iteration=8, loss=-181463.1442287086\n",
      "loss=-224569.33319061005\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32108.357068077814\n",
      "Current iteration=4, loss=-87525.17881402555\n",
      "Current iteration=6, loss=-135002.1778944784\n",
      "Current iteration=8, loss=-179231.0640286248\n",
      "loss=-221864.00296348025\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32347.497718154344\n",
      "Current iteration=4, loss=-87928.2832354586\n",
      "Current iteration=6, loss=-135537.78201108056\n",
      "Current iteration=8, loss=-179888.3897056395\n",
      "loss=-222639.42784697507\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32829.45768286968\n",
      "Current iteration=4, loss=-88763.01497890402\n",
      "Current iteration=6, loss=-136715.44780559084\n",
      "Current iteration=8, loss=-181413.90304561573\n",
      "loss=-224520.9906764136\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32838.57435298362\n",
      "Current iteration=4, loss=-88791.27544779523\n",
      "Current iteration=6, loss=-136755.4715414949\n",
      "Current iteration=8, loss=-181458.28417550193\n",
      "loss=-224566.0399823536\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32107.983173319983\n",
      "Current iteration=4, loss=-87523.80493724537\n",
      "Current iteration=6, loss=-134999.32015165922\n",
      "Current iteration=8, loss=-179226.25247209665\n",
      "loss=-221860.74468400227\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32347.12276127327\n",
      "Current iteration=4, loss=-87926.90537672861\n",
      "Current iteration=6, loss=-135534.9160489493\n",
      "Current iteration=8, loss=-179883.5643378202\n",
      "loss=-222636.16015061413\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32828.408321670235\n",
      "Current iteration=4, loss=-88759.15707988206\n",
      "Current iteration=6, loss=-136707.41996199312\n",
      "Current iteration=8, loss=-181400.3821869289\n",
      "loss=-224511.8278434407\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "Current iteration=2, loss=-32837.52488917852\n",
      "Current iteration=4, loss=-88787.41688160418\n",
      "Current iteration=6, loss=-136747.44237445717\n",
      "Current iteration=8, loss=-181444.76152560447\n",
      "loss=-224556.8767086925\n",
      "Current iteration=0, loss=51940.290828079065\n",
      "Current iteration=2, loss=-32106.942797387495\n",
      "Current iteration=4, loss=-87519.98213350291\n",
      "Current iteration=6, loss=-134991.36862291384\n",
      "Current iteration=8, loss=-179212.86475975407\n",
      "loss=-221851.67859871345\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-32346.079429942492\n",
      "Current iteration=4, loss=-87923.07149323369\n",
      "Current iteration=6, loss=-135526.94165038373\n",
      "Current iteration=8, loss=-179870.13819687953\n",
      "loss=-222627.06786312792\n",
      "Current iteration=0, loss=51940.29082807906\n",
      "Current iteration=2, loss=-34286.84007651326\n",
      "Current iteration=4, loss=-91033.14933820271\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(set1_x.shape[1])\n",
    "gamma_opt, lambda_opt = best_param_selection(set1_y, set1_x, k_fold, gammas, lambdas, fonction=5)\n",
    "w_rlr1, loss_rlr = reg_logistic_regression(set1_y, set1_x, lambda_opt, initial_w, max_iters, gamma_opt)\n",
    "print(\"Cross validation finished: optimal lambda {l} and gamma {g}\".format(l=lambda_opt, g=gamma_opt))\n",
    "print(\"regularized logistic regression loss {loss}\".format(loss=loss_rlr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(set2_x.shape[1])\n",
    "gamma_opt = cross_validation(set2_y, set2_x, k_fold, gammas, lambdas, fonction=5)\n",
    "w_rlr2, loss_rlr = reg_logistic_regression(set2_y, set2_x, lambda_opt, initial_w, max_iters, gamma_opt)\n",
    "print(\"Cross validation finished: optimal lambda {l} and gamma {g}\".format(l=lambda_opt, g=gamma_opt))\n",
    "print(\"regularized logistic regression loss {loss}\".format(loss=loss_rlr))\n",
    "# Best gamma and lambda model\n",
    "lambda_rlr_set2 = lambda_opt\n",
    "gamma_rlr_set2 = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(set3_x.shape[1])\n",
    "gamma_opt = cross_validation(set3_y, set3_x, k_fold, gammas, lambdas, fonction=5)\n",
    "w_rlr3, loss_rlr = reg_logistic_regression(set3_y, set3_x, lambda_opt, initial_w, max_iters, gamma_opt)\n",
    "print(\"Cross validation finished: optimal lambda {l} and gamma {g}\".format(l=lambda_opt, g=gamma_opt))\n",
    "print(\"regularized logistic regression loss {loss}\".format(loss=loss_rlr))\n",
    "# Best gamma and lambda model\n",
    "lambda_rlr_set3 = lambda_opt\n",
    "gamma_rlr_set3 = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "OUTPUT_PATH = '../data/pred_ls.csv' \n",
    "tX_test_ls = cut(std(filtering_with_mean(tX_test)),[15,18,20])\n",
    "tX_test_ls = build_poly(tX_test_ls, degree_ls)\n",
    "y_pred = predict_labels(w_ls, tX_test_ls)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_set_ls.csv' \n",
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y_test , tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test = cut(tX_test, [15,18,20])\n",
    "\n",
    "set1_x, _, set1_ids, set2_x, _, set2_ids, set3_x, _, set3_ids = separate_sets(tX_test, y_test, ids_test, 22-3)\n",
    "\n",
    "def filtering_test_ls (set_x, degree_rr):\n",
    "    print(set_x.shape)\n",
    "    set_x = outliers(set_x, -999)\n",
    "    print(set_x.shape)\n",
    "    set_x = std(filtering_with_mean(set_x))\n",
    "    set_x = build_poly(set_x, degree_rr)\n",
    "    return set_x\n",
    "\n",
    "\n",
    "set1_x = filtering_test_ls(set1_x, degree_set1_ls)\n",
    "set2_x = filtering_test_ls(set2_x, degree_set2_ls)\n",
    "set3_x = filtering_test_ls(set3_x, degree_set3_ls)\n",
    "\n",
    "y_pred1 = predict_labels(w_set1_ls, set1_x)\n",
    "y_pred2 = predict_labels(w_set2_ls, set2_x)\n",
    "y_pred3 = predict_labels(w_set3_ls, set3_x)\n",
    "\n",
    "y_pred_ls, ids_ls = concatenate_sets(y_pred1, set1_ids, y_pred2, set2_ids, y_pred3, set3_ids)\n",
    "create_csv_submission(ids_ls, y_pred_ls, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_set_rr.csv' \n",
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y_test , tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "set1_x, _, set1_ids, set2_x, _, set2_ids, set3_x, _, set3_ids = separate_sets(tX_test, y_test, ids_test)\n",
    "\n",
    "def filtering_test_rr (set_x, degree_rr):\n",
    "    set_x = outliers(set_x, -999)\n",
    "    set_x = build_poly(set_x, degree_rr)\n",
    "    return set_x\n",
    "\n",
    "set1_x = filtering_test_rr(set1_x, degree_rr_set1)\n",
    "set2_x = filtering_test_rr(set2_x, degree_rr_set2)\n",
    "set3_x = filtering_test_rr(set3_x, degree_rr_set3)\n",
    "y_pred1 = predict_labels(w_rr_set1, set1_x)\n",
    "y_pred2 = predict_labels(w_rr_set2, set2_x)\n",
    "y_pred3 = predict_labels(w_rr_set3, set3_x)\n",
    "\n",
    "y_pred_rr, ids_rr = concatenate_sets(y_pred1, set1_ids, y_pred2, set2_ids, y_pred3, set3_ids)\n",
    "create_csv_submission(ids_rr, y_pred_rr, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_log = log_distribution(tX_test, to_log)\n",
    "test_set1_x, _, test_set1_ids, test_set2_x, _, test_set2_ids, test_set3_x, _, test_set3_ids = separate_sets(tX_test_log, _, ids_test)\n",
    "\n",
    "test_set1_x = outliers(test_set1_x, -999)\n",
    "test_set1_x = filtering_with_mean(test_set1_x)\n",
    "test_set1_x = std(test_set1_x)\n",
    "\n",
    "test_set2_x = outliers(test_set2_x, -999)\n",
    "test_set2_x = filtering_with_mean(test_set2_x)\n",
    "test_set2_x = std(test_set2_x)\n",
    "\n",
    "test_set3_x = outliers(test_set3_x, -999)\n",
    "test_set3_x = filtering_with_mean(test_set3_x)\n",
    "test_set3_x = std(test_set3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_gd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred1 = predict_labels(w_gd1, test_set1_x)\n",
    "y_pred2 = predict_labels(w_gd2, test_set2_x)\n",
    "y_pred3 = predict_labels(w_gd3, test_set3_x)\n",
    "y_pred = np.concatenate((y_pred1, y_pred2, y_pred3), axis=0)\n",
    "ids_test = np.concatenate((test_set1_ids, test_set2_ids, test_set3_ids), axis=0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_log = log_distribution(tX_test, to_log)\n",
    "test_set1_x, _, test_set1_ids, test_set2_x, _, test_set2_ids, test_set3_x, _, test_set3_ids = separate_sets(tX_test_log, _, ids_test)\n",
    "\n",
    "test_set1_x = outliers(test_set1_x, -999)\n",
    "test_set1_x = filtering_with_mean(test_set1_x)\n",
    "test_set1_x = std(test_set1_x)\n",
    "\n",
    "test_set2_x = outliers(test_set2_x, -999)\n",
    "test_set2_x = filtering_with_mean(test_set2_x)\n",
    "test_set2_x = std(test_set2_x)\n",
    "\n",
    "test_set3_x = outliers(test_set3_x, -999)\n",
    "test_set3_x = filtering_with_mean(test_set3_x)\n",
    "test_set3_x = std(test_set3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_gd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred1 = predict_labels(w_gd1, test_set1_x)\n",
    "y_pred2 = predict_labels(w_gd2, test_set2_x)\n",
    "y_pred3 = predict_labels(w_gd3, test_set3_x)\n",
    "y_pred = np.concatenate((y_pred1, y_pred2, y_pred3), axis=0)\n",
    "ids_test = np.concatenate((test_set1_ids, test_set2_ids, test_set3_ids), axis=0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Logistic regressions submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers ratio for each feature [0.26054480387588036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.09834148900979822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.060335344108509326, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "tX_test_log = log_distribution(tX_test, to_log)\n",
    "test_set1_x, _, test_set1_ids, test_set2_x, _, test_set2_ids, test_set3_x, _, test_set3_ids = separate_sets(tX_test_log, _, ids_test)\n",
    "\n",
    "test_set1_x = outliers(test_set1_x, -999)\n",
    "test_set1_x = np.c_[np.ones((test_set1_x .shape[0], 1)), test_set1_x ]\n",
    "test_set1_x = filtering_with_mean(test_set1_x)\n",
    "test_set1_x = std(test_set1_x)\n",
    "\n",
    "test_set2_x = np.c_[np.ones((test_set2_x .shape[0], 1)), test_set2_x ]\n",
    "test_set2_x = outliers(test_set2_x, -999)\n",
    "test_set2_x = filtering_with_mean(test_set2_x)\n",
    "test_set2_x = std(test_set2_x)\n",
    "\n",
    "test_set3_x = np.c_[np.ones((test_set3_x .shape[0], 1)), test_set3_x ]\n",
    "test_set3_x = outliers(test_set3_x, -999)\n",
    "test_set3_x = filtering_with_mean(test_set3_x)\n",
    "test_set3_x = std(test_set3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_lr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred1 = predict_labels(w_lr1, test_set1_x)\n",
    "y_pred2 = predict_labels(w_lr2, test_set2_x)\n",
    "y_pred3 = predict_labels(w_lr3, test_set3_x)\n",
    "y_pred = np.concatenate((y_pred1, y_pred2, y_pred3), axis=0)\n",
    "ids_test = np.concatenate((test_set1_ids, test_set2_ids, test_set3_ids), axis=0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regularized logistic regressions submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_log = log_distribution(tX_test, to_log)\n",
    "test_set1_x, _, test_set1_ids, test_set2_x, _, test_set2_ids, test_set3_x, _, test_set3_ids = separate_sets(tX_test_log, _, ids_test)\n",
    "\n",
    "test_set1_x = outliers(test_set1_x, -999)\n",
    "test_set1_x = np.c_[np.ones((test_set1_x .shape[0], 1)), test_set1_x ]\n",
    "test_set1_x = filtering_with_mean(test_set1_x) # BIS ????\n",
    "test_set1_x = std(test_set1_x)\n",
    "\n",
    "test_set2_x = np.c_[np.ones((test_set2_x .shape[0], 1)), test_set2_x ]\n",
    "test_set2_x = outliers(test_set2_x, -999)\n",
    "test_set2_x = filtering_with_mean(test_set2_x)\n",
    "test_set2_x = std(test_set2_x)\n",
    "\n",
    "test_set3_x = np.c_[np.ones((test_set3_x .shape[0], 1)), test_set3_x ]\n",
    "test_set3_x = outliers(test_set3_x, -999)\n",
    "test_set3_x = filtering_with_mean(test_set3_x)\n",
    "test_set3_x = std(test_set3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_rlr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred1 = predict_labels(w_rlr1, test_set1_x)\n",
    "y_pred2 = predict_labels(w_rlr2, test_set2_x)\n",
    "y_pred3 = predict_labels(w_rlr3, test_set3_x)\n",
    "y_pred = np.concatenate((y_pred1, y_pred2, y_pred3), axis=0)\n",
    "ids_test = np.concatenate((test_set1_ids, test_set2_ids, test_set3_ids), axis=0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
