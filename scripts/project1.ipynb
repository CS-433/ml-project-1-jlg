{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = Path(\"../data/\")\n",
    "DATA_TRAIN_PATH = \"../data/train.csv\"\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "(250000,)\n",
      "(250000, 30)\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(np.shape(y))\n",
    "print(np.shape(tX))\n",
    "print(tX.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 DER_mass_MMC\n",
      "1 DER_mass_transverse_met_lep\n",
      "2 DER_mass_vis\n",
      "3 DER_pt_h\n",
      "4 DER_deltaeta_jet_jet\n",
      "5 DER_mass_jet_jet\n",
      "6 DER_prodeta_jet_jet\n",
      "7 DER_deltar_tau_lep\n",
      "8 DER_pt_tot\n",
      "9 DER_sum_pt\n",
      "10 DER_pt_ratio_lep_tau\n",
      "11 DER_met_phi_centrality\n",
      "12 DER_lep_eta_centrality\n",
      "13 PRI_tau_pt\n",
      "14 PRI_tau_eta\n",
      "15 PRI_tau_phi\n",
      "16 PRI_lep_pt\n",
      "17 PRI_lep_eta\n",
      "18 PRI_lep_phi\n",
      "19 PRI_met\n",
      "20 PRI_met_phi\n",
      "21 PRI_met_sumet\n",
      "22 PRI_jet_num\n",
      "23 PRI_jet_leading_pt\n",
      "24 PRI_jet_leading_eta\n",
      "25 PRI_jet_leading_phi\n",
      "26 PRI_jet_subleading_pt\n",
      "27 PRI_jet_subleading_eta\n",
      "28 PRI_jet_subleading_phi\n",
      "29 PRI_jet_all_pt\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', \n",
    "                 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', \n",
    "                 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', \n",
    "                 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', \n",
    "                 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi',\n",
    "                 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(i, feature_names[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set is composed of : \n",
    "* a y vector of length 250'000 and type float\n",
    "* a tX float matrix of 250'000 rows and 30 columns\n",
    "\n",
    "It means that our data set is composed of 250'000 different obsevations of 30 different features. In the rest of the notebook, we name the features by their index nummer. So, it means from the feature 0 from the feature  29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEICAYAAABBBrPDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdFUlEQVR4nO3df5DU9Z3n8ecrTMxqDMiP0ZAZNkMiyQa4pKJzSJK9rDlyQswP2CqtGi+u5JYqKqzJJanL5mRzF3OxuJK9bNxYu1DHCisYS6SIq2x2WcPhelZ2ER1NFFEJY1CZQGQiiHgbScD3/fH9zNZ3mp7P9HRPz4i8HlVd/e339/P59Kd7mnnx/TH9VURgZmY2mDeN9QTMzOz1zUFhZmZZDgozM8tyUJiZWZaDwszMshwUZmaW5aCwNzxJuyVdOtbzGEuSfl/SfkmvSPrgWM/HTi8OCjutSXpW0scrap+T9KP+xxExKyLuH2KcDkkhqaVJUx1r3wa+EBHnRsSPx3oydnpxUJiNgtdBAL0T2D3Gc7DTlIPC3vDKWx2S5kjqlvSypBckfSc1eyDdv5R2z3xI0psk/TdJz0k6JGmDpAmlca9J616U9N8rnuebkjZL+p6kl4HPpefeIeklSQcl/YWks0rjhaQ/krRX0jFJN0h6d+rzsqRN5fYVr7HqXCW9RdIrwDjgMUnPVOn7l5L+rKL2t5K+3MDbbm8gDgo703wX+G5EjAfeDWxK9Y+m+/PS7pkdwOfS7WPAu4Bzgb8AkDQTWAV8FpgKTADaKp5rIbAZOA+4HTgJfAWYAnwImAf8UUWfBcDFwFzga8Ca9BzTgNnAVYO8rqpzjYjjEXFuavOBiHh3lb7rgaskvSm9tilpbncM8lx2hnFQ2BvB3el/6S9JeoniF/hgfgNcKGlKRLwSEQ9m2n4W+E5E/CwiXgGWA11pN9IVwN9GxI8i4tfAN4DKL07bERF3R8RrEfGriHgkIh6MiBMR8Szwv4Hfq+izMiJejojdwBPAD9PzHwW2AoMdiM7NNSsiHgKOUoQDQBdwf0S8MFRfOzM4KOyNYFFEnNd/49T/pZctAd4DPC3pYUmfyrR9B/Bc6fFzQAtwQVq3v39FRPwL8GJF//3lB5LeI+kHkn6Rdkf9T4qti7LyL+dfVXl8LtXl5lqL9cDVaflq4LYa+9kZwEFhZ5SI2BsRVwHnAyuBzZLeyqlbAwAHKA4C9/tt4ATFL++DQHv/CklnA5Mrn67i8WrgaWBG2vX1J4DqfzU1z7UW3wMWSvoA8D7g7hGal70BOCjsjCLpakmtEfEa8FIqnwT6gNco9u/3uwP4iqTpks6l2AK4MyJOUBx7+LSkD6cDzP+DoX/pvw14GXhF0u8Ay0bsheXnOqSI6AUeptiS+H5E/GoE52anOQeFnWkWALvTmUDfBboi4tW062gF8E/pWMdcYB3FL84HgH3Aq8AXAdIxhC8CGym2Lo4Bh4Djmef+KvAfU9u/Au4cwdc16FyHYT3wb/BuJ6sgX7jIrHHpf/EvUexW2jfW86mHpI9S7ILqSFtcZoC3KMzqJunTks5Jxzi+DewCnh3bWdVH0puBLwG3OCSskoPCrH4LKQ4iHwBmUOzGOu020SW9j2JraCrw52M8HXsd8q4nMzPL8haFmZlljfUXlY24KVOmREdHx1hPw8zstPLII4/8MiJaq617wwVFR0cH3d3dYz0NM7PTiqTnBlvnXU9mZpbloDAzsywHhZmZZTkozMwsy0FhZmZZDgozM8tyUJiZWZaDwszMshwUZmaW9Yb7y+xGdVz3d2PyvM/e+MkxeV4zs6F4i8LMzLKGDApJ6yQdkvRERf2LkvZI2i3pT0v15ZJ60rr5pfrFknaldTdLUqq/RdKdqb5TUkepz2JJe9Nt8Ui8YDMzG55atihupbjO8L+S9DGKi7a8PyJmUVzdC0kzgS5gVuqzStK41G01sJTiAi8zSmMuAY5ExIXATcDKNNYk4HrgEmAOcL2kiXW9SjMzq9uQQRERDwCHK8rLgBsj4nhqcyjVFwIbI+J4um5wDzBH0lRgfETsSFcA2wAsKvVZn5Y3A/PS1sZ8YFtEHI6II8A2KgLLzMyar95jFO8B/l3aVfR/Jf3bVG8D9pfa9aZaW1qurA/oExEngKPA5MxYp5C0VFK3pO6+vr46X5KZmVVTb1C0ABOBucAfA5vSVoCqtI1MnTr7DCxGrImIzojobG2tet0NMzOrU71B0QvcFYWHgNeAKak+rdSuneLC871pubJOuY+kFmACxa6uwcYyM7NRVG9Q3A38ewBJ7wHOAn4JbAG60plM0ykOWj8UEQeBY5Lmpi2Pa4B70lhbgP4zmq4A7kvHMe4FLpM0MR3EvizVzMxsFA35B3eS7gAuBaZI6qU4E2kdsC6dMvtrYHH65b5b0ibgSeAEcG1EnExDLaM4g+psYGu6AawFbpPUQ7El0QUQEYcl3QA8nNp9KyIqD6qbmVmTDRkUEXHVIKuuHqT9CmBFlXo3MLtK/VXgykHGWkcRSmZmNkb8l9lmZpbloDAzsywHhZmZZTkozMwsy0FhZmZZDgozM8tyUJiZWZaDwszMshwUZmaW5aAwM7MsB4WZmWU5KMzMLMtBYWZmWQ4KMzPLclCYmVmWg8LMzLKGDApJ6yQdSlezq1z3VUkhaUqptlxSj6Q9kuaX6hdL2pXW3ZwuiUq6bOqdqb5TUkepz2JJe9NtMWZmNupq2aK4FVhQWZQ0DfgPwPOl2kyKS5nOSn1WSRqXVq8GllJcR3tGacwlwJGIuBC4CViZxppEcdnVS4A5wPXp2tlmZjaKhgyKiHiA4lrWlW4CvgZEqbYQ2BgRxyNiH9ADzJE0FRgfETvStbU3AItKfdan5c3AvLS1MR/YFhGHI+IIsI0qgWVmZs1V1zEKSZ8Bfh4Rj1WsagP2lx73plpbWq6sD+gTESeAo8DkzFjV5rNUUrek7r6+vnpekpmZDWLYQSHpHODrwDeqra5Si0y93j4DixFrIqIzIjpbW1urNTEzszrVs0XxbmA68JikZ4F24FFJb6f4X/+0Utt24ECqt1epU+4jqQWYQLGra7CxzMxsFA07KCJiV0ScHxEdEdFB8Qv9ooj4BbAF6EpnMk2nOGj9UEQcBI5JmpuOP1wD3JOG3AL0n9F0BXBfOo5xL3CZpInpIPZlqWZmZqOoZagGku4ALgWmSOoFro+ItdXaRsRuSZuAJ4ETwLURcTKtXkZxBtXZwNZ0A1gL3Caph2JLoiuNdVjSDcDDqd23IqLaQXUzM2uiIYMiIq4aYn1HxeMVwIoq7bqB2VXqrwJXDjL2OmDdUHM0M7Pm8V9mm5lZloPCzMyyHBRmZpbloDAzsywHhZmZZTkozMwsy0FhZmZZDgozM8tyUJiZWZaDwszMshwUZmaW5aAwM7MsB4WZmWU5KMzMLMtBYWZmWQ4KMzPLGjIoJK2TdEjSE6Xa/5L0tKTHJf2NpPNK65ZL6pG0R9L8Uv1iSbvSupvTJVFJl029M9V3Suoo9VksaW+69V8u1czMRlEtWxS3AgsqatuA2RHxfuCnwHIASTMpLmU6K/VZJWlc6rMaWEpxHe0ZpTGXAEci4kLgJmBlGmsScD1wCTAHuD5dO9vMzEbRkEEREQ9QXMu6XPthRJxIDx8E2tPyQmBjRByPiH1ADzBH0lRgfETsiIgANgCLSn3Wp+XNwLy0tTEf2BYRhyPiCEU4VQaWmZk12Ugco/hDYGtabgP2l9b1plpbWq6sD+iTwucoMDkzlpmZjaKGgkLS14ETwO39pSrNIlOvt0/lPJZK6pbU3dfXl5+0mZkNS91BkQ4ufwr4bNqdBMX/+qeVmrUDB1K9vUp9QB9JLcAEil1dg411iohYExGdEdHZ2tpa70syM7Mq6goKSQuA/wp8JiL+pbRqC9CVzmSaTnHQ+qGIOAgckzQ3HX+4Brin1Kf/jKYrgPtS8NwLXCZpYjqIfVmqmZnZKGoZqoGkO4BLgSmSeinORFoOvAXYls5yfTAiPh8RuyVtAp6k2CV1bUScTEMtoziD6myKYxr9xzXWArdJ6qHYkugCiIjDkm4AHk7tvhURAw6qm5lZ8w0ZFBFxVZXy2kz7FcCKKvVuYHaV+qvAlYOMtQ5YN9QczcysefyX2WZmluWgMDOzLAeFmZllOSjMzCzLQWFmZlkOCjMzy3JQmJlZloPCzMyyHBRmZpbloDAzsywHhZmZZTkozMwsy0FhZmZZDgozM8tyUJiZWZaDwszMshwUZmaWNWRQSFon6ZCkJ0q1SZK2Sdqb7ieW1i2X1CNpj6T5pfrFknaldTena2eTrq99Z6rvlNRR6rM4PcdeSf3X1TYzs1FUyxbFrcCCitp1wPaImAFsT4+RNJPimtezUp9VksalPquBpcCMdOsfcwlwJCIuBG4CVqaxJlFcn/sSYA5wfTmQzMxsdAwZFBHxAHC4orwQWJ+W1wOLSvWNEXE8IvYBPcAcSVOB8RGxIyIC2FDRp3+szcC8tLUxH9gWEYcj4giwjVMDy8zMmqzeYxQXRMRBgHR/fqq3AftL7XpTrS0tV9YH9ImIE8BRYHJmrFNIWiqpW1J3X19fnS/JzMyqGemD2apSi0y93j4DixFrIqIzIjpbW1trmqiZmdWm3qB4Ie1OIt0fSvVeYFqpXTtwINXbq9QH9JHUAkyg2NU12FhmZjaK6g2KLUD/WUiLgXtK9a50JtN0ioPWD6XdU8ckzU3HH66p6NM/1hXAfek4xr3AZZImpoPYl6WamZmNopahGki6A7gUmCKpl+JMpBuBTZKWAM8DVwJExG5Jm4AngRPAtRFxMg21jOIMqrOBrekGsBa4TVIPxZZEVxrrsKQbgIdTu29FROVBdTMza7IhgyIirhpk1bxB2q8AVlSpdwOzq9RfJQVNlXXrgHVDzdHMzJrHf5ltZmZZDgozM8tyUJiZWZaDwszMshwUZmaW5aAwM7MsB4WZmWU5KMzMLMtBYWZmWQ4KMzPLclCYmVmWg8LMzLKG/FJAMzMbno7r/m5MnvfZGz/ZlHG9RWFmZlkOCjMzy3JQmJlZVkNBIekrknZLekLSHZJ+S9IkSdsk7U33E0vtl0vqkbRH0vxS/WJJu9K6m9PlUkmXVL0z1XdK6mhkvmZmNnx1B4WkNuA/A50RMRsYR3EZ0+uA7RExA9ieHiNpZlo/C1gArJI0Lg23GlhKcY3tGWk9wBLgSERcCNwErKx3vmZmVp9Gdz21AGdLagHOAQ4AC4H1af16YFFaXghsjIjjEbEP6AHmSJoKjI+IHRERwIaKPv1jbQbm9W9tmJnZ6Kg7KCLi58C3geeBg8DRiPghcEFEHExtDgLnpy5twP7SEL2p1paWK+sD+kTECeAoMLlyLpKWSuqW1N3X11fvSzIzsyoa2fU0keJ//NOBdwBvlXR1rkuVWmTquT4DCxFrIqIzIjpbW1vzEzczs2FpZNfTx4F9EdEXEb8B7gI+DLyQdieR7g+l9r3AtFL/dopdVb1pubI+oE/avTUBONzAnM3MbJgaCYrngbmSzknHDeYBTwFbgMWpzWLgnrS8BehKZzJNpzho/VDaPXVM0tw0zjUVffrHugK4Lx3HMDOzUVL3V3hExE5Jm4FHgRPAj4E1wLnAJklLKMLkytR+t6RNwJOp/bURcTINtwy4FTgb2JpuAGuB2yT1UGxJdNU7XzMzq09D3/UUEdcD11eUj1NsXVRrvwJYUaXeDcyuUn+VFDRmZjY2/JfZZmaW5aAwM7MsB4WZmWU5KMzMLMtBYWZmWQ4KMzPLclCYmVmWg8LMzLIcFGZmluWgMDOzLAeFmZllOSjMzCzLQWFmZlkOCjMzy3JQmJlZloPCzMyyGgoKSedJ2izpaUlPSfqQpEmStknam+4nltovl9QjaY+k+aX6xZJ2pXU3p0uiki6bemeq75TU0ch8zcxs+Brdovgu8A8R8TvAByiumX0dsD0iZgDb02MkzaS4lOksYAGwStK4NM5qYCnFdbRnpPUAS4AjEXEhcBOwssH5mpnZMNUdFJLGAx+luK41EfHriHgJWAisT83WA4vS8kJgY0Qcj4h9QA8wR9JUYHxE7IiIADZU9OkfazMwr39rw8zMRkcjWxTvAvqAv5b0Y0m3SHorcEFEHARI9+en9m3A/lL/3lRrS8uV9QF9IuIEcBSYXDkRSUsldUvq7uvra+AlmZlZpUaCogW4CFgdER8E/h9pN9Mgqm0JRKae6zOwELEmIjojorO1tTU/azMzG5ZGgqIX6I2InenxZorgeCHtTiLdHyq1n1bq3w4cSPX2KvUBfSS1ABOAww3M2czMhqnuoIiIXwD7Jb03leYBTwJbgMWpthi4Jy1vAbrSmUzTKQ5aP5R2Tx2TNDcdf7imok//WFcA96XjGGZmNkpaGuz/ReB2SWcBPwP+E0X4bJK0BHgeuBIgInZL2kQRJieAayPiZBpnGXArcDawNd2gOFB+m6Qeii2Jrgbna2Zmw9RQUETET4DOKqvmDdJ+BbCiSr0bmF2l/iopaMzMbGz4L7PNzCzLQWFmZlkOCjMzy3JQmJlZloPCzMyyHBRmZpbloDAzsywHhZmZZTkozMwsy0FhZmZZDgozM8tyUJiZWZaDwszMshwUZmaW5aAwM7MsB4WZmWU1HBSSxkn6saQfpMeTJG2TtDfdTyy1XS6pR9IeSfNL9Ysl7Urrbk6XRCVdNvXOVN8pqaPR+ZqZ2fCMxBbFl4CnSo+vA7ZHxAxge3qMpJkUlzKdBSwAVkkal/qsBpZSXEd7RloPsAQ4EhEXAjcBK0dgvmZmNgwNBYWkduCTwC2l8kJgfVpeDywq1TdGxPGI2Af0AHMkTQXGR8SOiAhgQ0Wf/rE2A/P6tzbMzGx0NLpF8efA14DXSrULIuIgQLo/P9XbgP2ldr2p1paWK+sD+kTECeAoMLlyEpKWSuqW1N3X19fgSzIzs7K6g0LSp4BDEfFIrV2q1CJTz/UZWIhYExGdEdHZ2tpa43TMzKwWLQ30/QjwGUmXA78FjJf0PeAFSVMj4mDarXQote8FppX6twMHUr29Sr3cp1dSCzABONzAnM3MbJjq3qKIiOUR0R4RHRQHqe+LiKuBLcDi1GwxcE9a3gJ0pTOZplMctH4o7Z46JmluOv5wTUWf/rGuSM9xyhaFmZk1TyNbFIO5EdgkaQnwPHAlQETslrQJeBI4AVwbESdTn2XArcDZwNZ0A1gL3Caph2JLoqsJ8zUzs4wRCYqIuB+4Py2/CMwbpN0KYEWVejcwu0r9VVLQmJnZ2PBfZpuZWZaDwszMshwUZmaW5aAwM7MsB4WZmWU5KMzMLMtBYWZmWQ4KMzPLclCYmVmWg8LMzLIcFGZmluWgMDOzLAeFmZllOSjMzCzLQWFmZlkOCjMzy6o7KCRNk/SPkp6StFvSl1J9kqRtkvam+4mlPssl9UjaI2l+qX6xpF1p3c3pkqiky6bemeo7JXXU/1LNzKwejWxRnAD+S0S8D5gLXCtpJnAdsD0iZgDb02PSui5gFrAAWCVpXBprNbCU4jraM9J6gCXAkYi4ELgJWNnAfM3MrA51B0VEHIyIR9PyMeApoA1YCKxPzdYDi9LyQmBjRByPiH1ADzBH0lRgfETsiIgANlT06R9rMzCvf2vDzMxGx4gco0i7hD4I7AQuiIiDUIQJcH5q1gbsL3XrTbW2tFxZH9AnIk4AR4HJIzFnMzOrTcNBIelc4PvAlyPi5VzTKrXI1HN9KuewVFK3pO6+vr6hpmxmZsPQUFBIejNFSNweEXel8gtpdxLp/lCq9wLTSt3bgQOp3l6lPqCPpBZgAnC4ch4RsSYiOiOis7W1tZGXZGZmFRo560nAWuCpiPhOadUWYHFaXgzcU6p3pTOZplMctH4o7Z46JmluGvOaij79Y10B3JeOY5iZ2ShpaaDvR4A/AHZJ+kmq/QlwI7BJ0hLgeeBKgIjYLWkT8CTFGVPXRsTJ1G8ZcCtwNrA13aAIotsk9VBsSXQ1MF8zM6tD3UERET+i+jEEgHmD9FkBrKhS7wZmV6m/SgoaMzMbG/7LbDMzy3JQmJlZloPCzMyyHBRmZpbloDAzsywHhZmZZTkozMwsy0FhZmZZDgozM8tyUJiZWZaDwszMshwUZmaW5aAwM7MsB4WZmWU5KMzMLMtBYWZmWQ4KMzPLOi2CQtICSXsk9Ui6bqznY2Z2JnndB4WkccBfAp8AZgJXSZo5trMyMztzvO6DApgD9ETEzyLi18BGYOEYz8nM7IzRMtYTqEEbsL/0uBe4pNxA0lJgaXr4iqQ9DTzfFOCXDfSvi1YO2WRM5lUDz2t4PK/h8byGQSsbmtc7B1txOgSFqtRiwIOINcCaEXkyqTsiOkdirJHkeQ2P5zU8ntfwnGnzOh12PfUC00qP24EDYzQXM7MzzukQFA8DMyRNl3QW0AVsGeM5mZmdMV73u54i4oSkLwD3AuOAdRGxu4lPOSK7sJrA8xoez2t4PK/hOaPmpYgYupWZmZ2xToddT2ZmNoYcFGZmlnXGBYWkKyXtlvSapEFPIxvsa0MkTZK0TdLedD9xhOY15LiS3ivpJ6Xby5K+nNZ9U9LPS+suH615pXbPStqVnrt7uP2bNTdJ0yT9o6Sn0s/9S6V1I/aeDfU1MyrcnNY/LumiWvs2ooZ5fTbN53FJ/yzpA6V1VX+mozSvSyUdLf1svlFr3ybP649Lc3pC0klJk9K6Zr5f6yQdkvTEIOub+/mKiDPqBrwPeC9wP9A5SJtxwDPAu4CzgMeAmWndnwLXpeXrgJUjNK9hjZvm+AvgnenxN4GvNuH9qmlewLPAlEZf10jPDZgKXJSW3wb8tPSzHJH3LPd5KbW5HNhK8XdBc4GdtfZt8rw+DExMy5/on1fuZzpK87oU+EE9fZs5r4r2nwbua/b7lcb+KHAR8MQg65v6+Trjtigi4qmIGOovt3NfG7IQWJ+W1wOLRmhqwx13HvBMRDw3Qs8/mEZfb7Per5rGjoiDEfFoWj4GPEXx1/4jqZavmVkIbIjCg8B5kqbW2Ldp84qIf46II+nhgxR/p9RsjbzmMX2/KlwF3DFCz50VEQ8AhzNNmvr5OuOCokbVvjak/5fLBRFxEIpfQsD5I/Scwx23i1M/pF9Im53rRnAXT63zCuCHkh5R8ZUqw+3fzLkBIKkD+CCws1Qeifcs93kZqk0tfes13LGXUPyvtN9gP9PRmteHJD0maaukWcPs28x5IekcYAHw/VK5We9XLZr6+Xrd/x1FPST9H+DtVVZ9PSLuqWWIKrWGzyPOzWuY45wFfAZYXiqvBm6gmOcNwJ8BfziK8/pIRByQdD6wTdLT6X9BDRnB9+xcin/UX46Il1O57vescvgqtcrPy2BtmvJZG+I5T20ofYwiKH63VG7Kz7TGeT1KsVv1lXTs6G5gRo19mzmvfp8G/ikiyv/Lb9b7VYumfr7ekEERER9vcIjc14a8IGlqRBxMm3aHRmJekoYz7ieARyPihdLY/7os6a+AH4zmvCLiQLo/JOlvKDZ5H6CB92uk5ibpzRQhcXtE3FUau+73rEItXzMzWJuzauhbr5q+/kbS+4FbgE9ExIv99czPtOnzKoU5EfH3klZJmlJL32bOq+SULfomvl+1aOrny7ueqst9bcgWYHFaXgzUsoVSi+GMe8q+0fSLst/vA1XPjmjGvCS9VdLb+peBy0rP36z3q9a5CVgLPBUR36lYN1LvWS1fM7MFuCadnTIXOJp2lzXzK2qGHFvSbwN3AX8QET8t1XM/09GY19vTzw5Jcyh+V71YS99mzivNZwLwe5Q+b01+v2rR3M9XM47Qv55vFL8QeoHjwAvAvan+DuDvS+0upzhD5hmKXVb99cnAdmBvup80QvOqOm6VeZ1D8Q9mQkX/24BdwOPpgzB1tOZFcUbFY+m2ezTer2HM7XcpNrUfB36SbpeP9HtW7fMCfB74fFoWxQW4nknP2ZnrO4Lv0VDzugU4Unpvuof6mY7SvL6QnvcxioPsH349vF/p8eeAjRX9mv1+3QEcBH5D8ftryWh+vvwVHmZmluVdT2ZmluWgMDOzLAeFmZllOSjMzCzLQWFmZlkOCjMzy3JQmJlZ1v8H8gWv63t5NxEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y)\n",
    "plt.title('Histogram of y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more y = -1 than y = 1 in the data, so there is more y = 'b' than y = 's'. So, we have to pay attention to normalize the data in order to compare them in the next plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "ind = np.arange(30)\n",
    "plt.boxplot(tX[:,], labels = ind)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many outliers depending on the feature. There are also feature that has a long interquantile range. Maybe we have to treat these feature in order to be more efficient in our futur predictions. Let's do more plots to be have a better idea :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind_1 = np.where(y == 1)\n",
    "ind_2 = np.where(y == -1)\n",
    "tX_1 = tX[ind_1[0],:]\n",
    "tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "fig, axs = plt.subplots(5, 6, figsize=(20,20))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].hist(tX_2[:,n], alpha=0.4, density=True, label=['y=-1', 'y=1'])\n",
    "        axs[i,j].hist(tX_1[:,n], alpha=0.4, density=True)\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the histograms of the features with a color for each y, we can see that there are useless features as they have almost the same distribution for y=1 than for y = -1. We can cut feature 15, 18, 20. \n",
    "\n",
    "\n",
    "There are also features that are very inequally distributed with value that are about -1000 and values around 0 ; it can be problematic for the prediction with such a large gap between values of a single distribution. Moreover, there is not a big difference in the distribution of y=1 and y=-1. Maybe it can be useful to put off these big negative values of these features. The features in question are : 0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(20,20))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].scatter(tX[:,n], y)\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()\n",
    "\n",
    "#meme constat comment faire pour se debarrasser de ces valeurs ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots confirm the obsevations that we made in the previous plot ; no difference of the distribution of y for features 15, 18, 20 and very large gap in the distributions of features : 0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of -999 in the entire matrix :\n",
      "[(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
      "(0,)\n",
      "number of -999 in the rows where y = 1 :\n",
      "[(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
      "(0,)\n",
      "number of -999 in the rows where y = -1 :\n",
      "[(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "def nb_outliers(tX, outlier) : \n",
    "    sum = 0\n",
    "    nb_outliers = []\n",
    "    for col in range(tX.shape[1]) :\n",
    "        sum = np.where(tX[:,col] == outlier)[0].shape\n",
    "        nb_outliers.append(sum)   \n",
    "    print(nb_outliers)\n",
    "    print(np.where(tX==outlier)[0].shape)\n",
    "\n",
    "out = -999\n",
    "\n",
    "print('number of -999 in the entire matrix :')\n",
    "nb_outliers(tX, out)\n",
    "\n",
    "ind_1 = np.where(y == 1)\n",
    "ind_2 = np.where(y == -1)\n",
    "tX_1 = tX[ind_1[0],:]\n",
    "tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "print('number of -999 in the rows where y = 1 :')\n",
    "nb_outliers(tX_1, out)\n",
    "print('number of -999 in the rows where y = -1 :')\n",
    "nb_outliers(tX_2, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem with features  0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28. They are inequally distributed; they have a lot of -999 values and the rest is values around 0. So, here we can see how much of these -999 there are. We can see that the -999 appear only in the features that we identified with the histograms. It seems that there is a correlation between features as many features have the same number of -999. We can also see that there is more -999 in the obsevations where y=-1, so we have to take this into account when we filter the data. As there are many -999, we can't delete the rows where there is -999 because we will loose to much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changer par la valeur da la mean du feature en question sans les compter dedans\n",
    "def filtering_with_mean(tX):\n",
    "    index = [0,4,5,6,12,23,24,25,26,27,28]\n",
    "    tX_filtered = np.copy(tX)\n",
    "    arr = []\n",
    "    for ind in index :\n",
    "        arr = np.delete(tX_filtered[:,ind], np.where(tX_filtered[:,ind]==-999))\n",
    "        mean = np.mean(arr)\n",
    "        tX_filtered[np.where(tX_filtered[:,ind]==-999), ind] = mean\n",
    "    return tX_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_with_mean_bis(tX, y):\n",
    "    #index = [0,4,5,6,12,23,24,25,26,27,28]\n",
    "    index = np.arange(tX.shape[1])\n",
    "    tX_filtered = np.copy(tX)\n",
    "    \n",
    "    ind_1 = np.where(y == 1)[0]\n",
    "    ind_2 = np.where(y == -1)[0]\n",
    "    tX_1 = tX[ind_1,:]\n",
    "    tX_2 = tX[ind_2,:]\n",
    "    \n",
    "    ind_3 = np.where(tX[:,0]==-999)[0]\n",
    "    new_ind_1 = np.intersect1d(ind_3, ind_1)\n",
    "    new_ind_2 = np.intersect1d(ind_3, ind_2)\n",
    "    \n",
    "    arr_1 = []\n",
    "    arr_2 = []\n",
    "    for ind in index :\n",
    "        arr_1 = np.delete(tX_1[:,ind], np.where(tX_1[:,ind]==-999))\n",
    "        mean_1 = np.mean(arr_1)\n",
    "        arr_2 = np.delete(tX_2[:,ind], np.where(tX_2[:,ind]==-999))\n",
    "        mean_2 = np.mean(arr_2)\n",
    "        tX_filtered[new_ind_1, ind] = mean_1\n",
    "        tX_filtered[new_ind_2, ind] = mean_2\n",
    "    return tX_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(tX):\n",
    "    mean = np.mean(tX, axis = 0)\n",
    "    std = np.std(tX, axis = 0)\n",
    "    tX[:, std>0] = (tX[:, std>0] - mean[std>0])/std[std>0]\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to filter the data, we want to get rif of these -999, but we can't just delete the rows. So, we have the idea to replace the -999 by the mean of the rest of values of the feature. As there is a significant difference of amount of -999 in between y=1 and y=-1 in certain features, we calculate the mean for the rows where y = 1 and y = -1 separatly.\n",
    "\n",
    "Then, we can also standardize the data. It can be a good idea because the features are not all in the same range of values and it can create disproportionality between the importance of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_distribution(tX, to_log):\n",
    "    tX_log = np.copy(tX)\n",
    "    index = np.arange(tX.shape[1])\n",
    "    for i in range(tX.shape[1]):\n",
    "        for j in range(len(to_log)):\n",
    "            if index[i] == to_log[j]:\n",
    "                tX_log[:, i] = np.log(1+tX[:, to_log[j]], where=np.all(tX[:, i]>0))  \n",
    "    return tX_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sets(tX, y, ids):\n",
    "    index1 = np.where(tX[:, 22]==0)\n",
    "    index2 = np.where(tX[:, 22]==1)\n",
    "    index3 = np.where(tX[:, 22]>1)\n",
    "    \n",
    "    set1_x = tX[index1]\n",
    "    set1_y = y[index1]\n",
    "    set1_ids = ids[index1]\n",
    "    \n",
    "    set2_x = tX[index2]\n",
    "    set2_y = y[index2]\n",
    "    set2_ids = ids[index2]\n",
    "    \n",
    "    set3_x = tX[index3]\n",
    "    set3_y = y[index3]\n",
    "    set3_ids = ids[index3]\n",
    "    \n",
    "    return set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids\n",
    "\n",
    "def concatenate_sets(set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids):\n",
    "    tX = np.concatenate((set1_x, set2_x, set3_x), axis = 0)\n",
    "    y = np.concatenate((set1_y, set2_y, set3_y), axis = 0)\n",
    "    ids = np.concatenate((set1_ids, set2_ids, set3_ids), axis = 0)\n",
    "    return tX, y, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(tX, outlier):\n",
    "    outliers = []\n",
    "    M = np.squeeze(tX.shape[0])\n",
    "    for col in range(tX.shape[1]) :\n",
    "        out_col = np.nonzero(tX[:,col] == outlier)[0].shape\n",
    "        out_col = np.squeeze(out_col)\n",
    "        outliers.append(out_col/M)\n",
    "    print('outliers ratio for each feature', outliers)\n",
    "    \n",
    "    index_full = np.arange(tX.shape[1])\n",
    "    index = index_full[~(outliers==np.ones(len(outliers)))]\n",
    "    index = index.reshape(-1)\n",
    "    X_without_outliers = tX[:, index]\n",
    "    return X_without_outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(tX, to_cut):\n",
    "    cut_index = 100*np.ones(tX.shape[1])\n",
    "    index_full = np.arange(tX.shape[1])\n",
    "    for i in range(tX.shape[1]):\n",
    "        for j in range(len(to_cut)):\n",
    "            if index_full[i] == to_cut[j]:\n",
    "                cut_index[i] = to_cut[j]\n",
    "    index = index_full[~(index_full == cut_index)]\n",
    "    index = index.reshape(-1)\n",
    "    tX_cut = tX[:, index]\n",
    "    return tX_cut\n",
    "\n",
    "def keep(tX, to_keep):\n",
    "    keep_index = 100*np.ones(tX.shape[1])\n",
    "    index_full = np.arange(tX.shape[1])\n",
    "    for i in range(tX.shape[1]):\n",
    "        for j in range(len(to_keep)):\n",
    "            if index_full[i] == to_keep[j]:\n",
    "                keep_index[i] = to_keep[j]\n",
    "    index = index_full[index_full == keep_index]\n",
    "    index = index.reshape(-1)\n",
    "    tX_kept = tX[:, index]\n",
    "    return tX_kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above in the histograms, some features seem to be useless as they have a similar distribution between the y = 1 and y = -1. So, it is useful to have function that cut or keep some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_mean = filtering_with_mean_bis(tX, y)\n",
    "tX_std = std(tX_mean)\n",
    "\n",
    "to_cut = [15,18,20]\n",
    "tX_mean_cut = cut(tX_std, to_cut)\n",
    "\n",
    "to_cut1 = [0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "tX_cut = cut(tX_std, to_cut1)\n",
    "\n",
    "to_keep = [13, 14, 17]\n",
    "tX_kept = keep(tX_std, to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 2\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 3\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "to_log = [1, 2, 5, 9, 10, 13, 16, 19, 21, 23, 26, 29]\n",
    "\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX, y, ids)\n",
    "\n",
    "print('Set 1')\n",
    "set1_x = outliers(set1_x, -999)\n",
    "set1_x = log_distribution(set1_x, to_log)\n",
    "set1_x = filtering_with_mean_bis(set1_x, set1_y)\n",
    "set1_x = std(set1_x)\n",
    "_ = outliers(set1_x, -999)\n",
    "\n",
    "print('\\nSet 2')\n",
    "set2_x = outliers(set2_x, -999)\n",
    "set2_x = log_distribution(set2_x, to_log)\n",
    "set2_x = filtering_with_mean_bis(set2_x, set2_y)\n",
    "set2_x = std(set2_x)\n",
    "_ = outliers(set2_x, -999)\n",
    "\n",
    "print('\\nSet 3')\n",
    "set3_x = outliers(set3_x, -999)\n",
    "set3_x = log_distribution(set3_x, to_log)\n",
    "set3_x = filtering_with_mean_bis(set3_x, set3_y)\n",
    "set3_x = std(set3_x)\n",
    "_ = outliers(set3_x, -999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose x and generate test and train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tX_mean\n",
    "x2 = tX_kept\n",
    "x3 = tX_std\n",
    "x4 = tX_mean_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation to get parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "degrees = np.arange(1,11)\n",
    "degree_opt, _ = best_degree_selection(y, tX_log, degrees, k_fold=4, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "tX_poly = build_poly(tX_log, degree_opt)\n",
    "w_ls, loss_ls = least_squares(y, tX_poly)\n",
    "print(\"Least square loss {loss}\".format(loss=loss_ls))\n",
    "degree_ls = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "degrees = np.arange(1,11)\n",
    "degree_opt, _ = best_degree_selection(set1_y, set1_x, degrees, k_fold=4, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "tX_poly = build_poly(set1_x, degree_opt)\n",
    "w_ls1, loss_ls1 = least_squares(set1_y, tX_poly)\n",
    "print(\"Least square loss {loss}\".format(loss=loss_ls1))\n",
    "degree_ls1 = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#je commente sinon pas le bon w_ls pour la submission\n",
    "\"\"\"\"w_ls, loss_ls = least_squares(y, x1)\n",
    "print(\"Least square loss {loss}\".format(loss=loss_ls))\n",
    "w_ls, loss_ls = least_squares(y, build_poly(x1, 8))\n",
    "print(\"Least square mse loss {loss} with degree 8\".format(loss=loss_ls))\n",
    "w_ls, loss_ls = least_squares(y, build_poly(x1, 8))\n",
    "loss_ls = np.sqrt(2*loss_ls)\n",
    "print(\"Least square rmse loss {loss} with degree 8\".format(loss=loss_ls))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "degrees = np.arange(1,11)\n",
    "degree_opt, _ = best_degree_selection(y, x4, degrees, k_fold=4, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "tX2 = build_poly(x4, degree_opt)\n",
    "w_ls2, loss_ls2 = least_squares(y, tX2)\n",
    "print(\"Least square loss {loss}\".format(loss=loss_ls))\n",
    "degree_ls2 = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation to find the oprimal lambda and degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(1,11)\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x4, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "tX = build_poly(x1, degree_opt)\n",
    "w_rr, loss_rr = ridge_regression(y, tX, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "degree_rr = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(1,11)\n",
    "degree_opt, lambda_opt = best_degree_selection(set1_y, set1_x, degrees, k_fold=10, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "tX_poly = build_poly(x1, degree_opt)\n",
    "w_rr1, loss_rr1 = ridge_regression(set1_y, tX_poly, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr1))\n",
    "degree_rr1 = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV to find best gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49421607051756417\n",
      "Gradient Descent(2/49): loss=0.48901332979372475\n",
      "Gradient Descent(3/49): loss=0.48431862629636574\n",
      "Gradient Descent(4/49): loss=0.48006856168784173\n",
      "Gradient Descent(5/49): loss=0.47620816601436416\n",
      "Gradient Descent(6/49): loss=0.4726897543075113\n",
      "Gradient Descent(7/49): loss=0.46947193965277456\n",
      "Gradient Descent(8/49): loss=0.46651878121807516\n",
      "Gradient Descent(9/49): loss=0.4637990486987297\n",
      "Gradient Descent(10/49): loss=0.46128558718995133\n",
      "Gradient Descent(11/49): loss=0.4589547687001185\n",
      "Gradient Descent(12/49): loss=0.4567860184163917\n",
      "Gradient Descent(13/49): loss=0.4547614054707779\n",
      "Gradient Descent(14/49): loss=0.45286528936555676\n",
      "Gradient Descent(15/49): loss=0.45108401443327945\n",
      "Gradient Descent(16/49): loss=0.449405645755163\n",
      "Gradient Descent(17/49): loss=0.4478197408657794\n",
      "Gradient Descent(18/49): loss=0.4463171523514386\n",
      "Gradient Descent(19/49): loss=0.4448898571217587\n",
      "Gradient Descent(20/49): loss=0.4435308087134438\n",
      "Gradient Descent(21/49): loss=0.4422338094850068\n",
      "Gradient Descent(22/49): loss=0.4409933999921008\n",
      "Gradient Descent(23/49): loss=0.4398047632047317\n",
      "Gradient Descent(24/49): loss=0.43866364154811494\n",
      "Gradient Descent(25/49): loss=0.4375662650253359\n",
      "Gradient Descent(26/49): loss=0.4365092889183783\n",
      "Gradient Descent(27/49): loss=0.43548973976970906\n",
      "Gradient Descent(28/49): loss=0.4345049685239935\n",
      "Gradient Descent(29/49): loss=0.4335526098625257\n",
      "Gradient Descent(30/49): loss=0.43263054689497565\n",
      "Gradient Descent(31/49): loss=0.4317368804869526\n",
      "Gradient Descent(32/49): loss=0.43086990260015856\n",
      "Gradient Descent(33/49): loss=0.43002807310671876\n",
      "Gradient Descent(34/49): loss=0.42920999961245954\n",
      "Gradient Descent(35/49): loss=0.42841441988707674\n",
      "Gradient Descent(36/49): loss=0.42764018655366354\n",
      "Gradient Descent(37/49): loss=0.4268862537371351\n",
      "Gradient Descent(38/49): loss=0.42615166541173216\n",
      "Gradient Descent(39/49): loss=0.42543554522287164\n",
      "Gradient Descent(40/49): loss=0.4247370875889248\n",
      "Gradient Descent(41/49): loss=0.4240555499146701\n",
      "Gradient Descent(42/49): loss=0.42339024577078727\n",
      "Gradient Descent(43/49): loss=0.42274053891328917\n",
      "Gradient Descent(44/49): loss=0.4221058380336722\n",
      "Gradient Descent(45/49): loss=0.4214855921451553\n",
      "Gradient Descent(46/49): loss=0.42087928652298945\n",
      "Gradient Descent(47/49): loss=0.4202864391277276\n",
      "Gradient Descent(48/49): loss=0.41970659744977173\n",
      "Gradient Descent(49/49): loss=0.4191393357216744\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4941459958994773\n",
      "Gradient Descent(2/49): loss=0.48888088532340834\n",
      "Gradient Descent(3/49): loss=0.48413001618688634\n",
      "Gradient Descent(4/49): loss=0.47982877687029796\n",
      "Gradient Descent(5/49): loss=0.47592122010308174\n",
      "Gradient Descent(6/49): loss=0.472358876968231\n",
      "Gradient Descent(7/49): loss=0.46909973465505567\n",
      "Gradient Descent(8/49): loss=0.46610735525396974\n",
      "Gradient Descent(9/49): loss=0.46335011604301485\n",
      "Gradient Descent(10/49): loss=0.4608005544325581\n",
      "Gradient Descent(11/49): loss=0.45843480307326884\n",
      "Gradient Descent(12/49): loss=0.45623210264575964\n",
      "Gradient Descent(13/49): loss=0.45417438158346135\n",
      "Gradient Descent(14/49): loss=0.45224589347241134\n",
      "Gradient Descent(15/49): loss=0.45043290415622467\n",
      "Gradient Descent(16/49): loss=0.44872342168047946\n",
      "Gradient Descent(17/49): loss=0.44710696316295606\n",
      "Gradient Descent(18/49): loss=0.44557435349602165\n",
      "Gradient Descent(19/49): loss=0.44411755149335463\n",
      "Gradient Descent(20/49): loss=0.4427294997010505\n",
      "Gradient Descent(21/49): loss=0.441403994616537\n",
      "Gradient Descent(22/49): loss=0.44013557450945023\n",
      "Gradient Descent(23/49): loss=0.4389194224267557\n",
      "Gradient Descent(24/49): loss=0.437751282298667\n",
      "Gradient Descent(25/49): loss=0.4366273863497985\n",
      "Gradient Descent(26/49): loss=0.43554439226794384\n",
      "Gradient Descent(27/49): loss=0.43449932879644226\n",
      "Gradient Descent(28/49): loss=0.4334895486000634\n",
      "Gradient Descent(29/49): loss=0.43251268741282284\n",
      "Gradient Descent(30/49): loss=0.43156662861266926\n",
      "Gradient Descent(31/49): loss=0.4306494724856175\n",
      "Gradient Descent(32/49): loss=0.4297595095432525\n",
      "Gradient Descent(33/49): loss=0.4288951973448729\n",
      "Gradient Descent(34/49): loss=0.42805514035080383\n",
      "Gradient Descent(35/49): loss=0.4272380723982827\n",
      "Gradient Descent(36/49): loss=0.42644284144723427\n",
      "Gradient Descent(37/49): loss=0.42566839629144976\n",
      "Gradient Descent(38/49): loss=0.4249137749722489\n",
      "Gradient Descent(39/49): loss=0.4241780946675246\n",
      "Gradient Descent(40/49): loss=0.4234605428599785\n",
      "Gradient Descent(41/49): loss=0.4227603696150043\n",
      "Gradient Descent(42/49): loss=0.42207688082166256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=0.42140943227002703\n",
      "Gradient Descent(44/49): loss=0.42075742445529885\n",
      "Gradient Descent(45/49): loss=0.4201202980138515\n",
      "Gradient Descent(46/49): loss=0.419497529709125\n",
      "Gradient Descent(47/49): loss=0.4188886288962927\n",
      "Gradient Descent(48/49): loss=0.41829313440413457\n",
      "Gradient Descent(49/49): loss=0.417710611780759\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4942006895458212\n",
      "Gradient Descent(2/49): loss=0.48898703804521687\n",
      "Gradient Descent(3/49): loss=0.48428481971309384\n",
      "Gradient Descent(4/49): loss=0.4800297835864654\n",
      "Gradient Descent(5/49): loss=0.47616628706228203\n",
      "Gradient Descent(6/49): loss=0.47264611821008007\n",
      "Gradient Descent(7/49): loss=0.4694274806670227\n",
      "Gradient Descent(8/49): loss=0.46647411856583815\n",
      "Gradient Descent(9/49): loss=0.46375456208173654\n",
      "Gradient Descent(10/49): loss=0.4612414768832993\n",
      "Gradient Descent(11/49): loss=0.4589111030954597\n",
      "Gradient Descent(12/49): loss=0.4567427713824055\n",
      "Gradient Descent(13/49): loss=0.45471848547958305\n",
      "Gradient Descent(14/49): loss=0.45282256198577836\n",
      "Gradient Descent(15/49): loss=0.45104131950185505\n",
      "Gradient Descent(16/49): loss=0.4493628103008694\n",
      "Gradient Descent(17/49): loss=0.44777658865968356\n",
      "Gradient Descent(18/49): loss=0.4462735107961273\n",
      "Gradient Descent(19/49): loss=0.4448455620565277\n",
      "Gradient Descent(20/49): loss=0.4434857076017936\n",
      "Gradient Descent(21/49): loss=0.4421877633597652\n",
      "Gradient Descent(22/49): loss=0.4409462844588859\n",
      "Gradient Descent(23/49): loss=0.43975646874348506\n",
      "Gradient Descent(24/49): loss=0.43861407330269386\n",
      "Gradient Descent(25/49): loss=0.4375153422307237\n",
      "Gradient Descent(26/49): loss=0.4364569440822965\n",
      "Gradient Descent(27/49): loss=0.43543591769895545\n",
      "Gradient Descent(28/49): loss=0.4344496252645478\n",
      "Gradient Descent(29/49): loss=0.4334957116054274\n",
      "Gradient Descent(30/49): loss=0.43257206888641525\n",
      "Gradient Descent(31/49): loss=0.43167680597027625\n",
      "Gradient Descent(32/49): loss=0.4308082218090546\n",
      "Gradient Descent(33/49): loss=0.42996478232227536\n",
      "Gradient Descent(34/49): loss=0.4291451002917191\n",
      "Gradient Descent(35/49): loss=0.42834791786684734\n",
      "Gradient Descent(36/49): loss=0.4275720913304564\n",
      "Gradient Descent(37/49): loss=0.4268165778219749\n",
      "Gradient Descent(38/49): loss=0.4260804237570688\n",
      "Gradient Descent(39/49): loss=0.42536275471778767\n",
      "Gradient Descent(40/49): loss=0.4246627666181632\n",
      "Gradient Descent(41/49): loss=0.4239797179766317\n",
      "Gradient Descent(42/49): loss=0.42331292314947905\n",
      "Gradient Descent(43/49): loss=0.4226617463992072\n",
      "Gradient Descent(44/49): loss=0.4220255966887191\n",
      "Gradient Descent(45/49): loss=0.42140392310689223\n",
      "Gradient Descent(46/49): loss=0.42079621084378316\n",
      "Gradient Descent(47/49): loss=0.42020197764464307\n",
      "Gradient Descent(48/49): loss=0.41962077068137543\n",
      "Gradient Descent(49/49): loss=0.4190521637882332\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4941315234633629\n",
      "Gradient Descent(2/49): loss=0.488852278147595\n",
      "Gradient Descent(3/49): loss=0.48408789502310834\n",
      "Gradient Descent(4/49): loss=0.4797739623999058\n",
      "Gradient Descent(5/49): loss=0.4758546669581367\n",
      "Gradient Descent(6/49): loss=0.47228162178951283\n",
      "Gradient Descent(7/49): loss=0.46901285560582867\n",
      "Gradient Descent(8/49): loss=0.4660119408505675\n",
      "Gradient Descent(9/49): loss=0.4632472415328005\n",
      "Gradient Descent(10/49): loss=0.4606912642582469\n",
      "Gradient Descent(11/49): loss=0.4583200982197674\n",
      "Gradient Descent(12/49): loss=0.4561129318798493\n",
      "Gradient Descent(13/49): loss=0.45405163577479835\n",
      "Gradient Descent(14/49): loss=0.4521204023322816\n",
      "Gradient Descent(15/49): loss=0.45030543485321356\n",
      "Gradient Descent(16/49): loss=0.44859467889383337\n",
      "Gradient Descent(17/49): loss=0.4469775902184062\n",
      "Gradient Descent(18/49): loss=0.44544493429812554\n",
      "Gradient Descent(19/49): loss=0.4439886130254534\n",
      "Gradient Descent(20/49): loss=0.44260151491077365\n",
      "Gradient Descent(21/49): loss=0.4412773855431583\n",
      "Gradient Descent(22/49): loss=0.44001071554071913\n",
      "Gradient Descent(23/49): loss=0.4387966435983369\n",
      "Gradient Descent(24/49): loss=0.4376308725699895\n",
      "Gradient Descent(25/49): loss=0.43650959680681506\n",
      "Gradient Descent(26/49): loss=0.4354294392167073\n",
      "Gradient Descent(27/49): loss=0.43438739672211807\n",
      "Gradient Descent(28/49): loss=0.4333807929744961\n",
      "Gradient Descent(29/49): loss=0.43240723734045694\n",
      "Gradient Descent(30/49): loss=0.431464589309836\n",
      "Gradient Descent(31/49): loss=0.43055092759220465\n",
      "Gradient Descent(32/49): loss=0.42966452326881377\n",
      "Gradient Descent(33/49): loss=0.4288038164534842\n",
      "Gradient Descent(34/49): loss=0.42796739599060135\n",
      "Gradient Descent(35/49): loss=0.4271539817827428\n",
      "Gradient Descent(36/49): loss=0.42636240939598574\n",
      "Gradient Descent(37/49): loss=0.42559161663883055\n",
      "Gradient Descent(38/49): loss=0.42484063185199594\n",
      "Gradient Descent(39/49): loss=0.4241085636819851\n",
      "Gradient Descent(40/49): loss=0.4233945921420854\n",
      "Gradient Descent(41/49): loss=0.4226979607910141\n",
      "Gradient Descent(42/49): loss=0.4220179698823371\n",
      "Gradient Descent(43/49): loss=0.4213539703575728\n",
      "Gradient Descent(44/49): loss=0.42070535857298147\n",
      "Gradient Descent(45/49): loss=0.42007157166478526\n",
      "Gradient Descent(46/49): loss=0.41945208347032126\n",
      "Gradient Descent(47/49): loss=0.4188464009336299\n",
      "Gradient Descent(48/49): loss=0.41825406093350614\n",
      "Gradient Descent(49/49): loss=0.4176746274802627\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48874270882801446\n",
      "Gradient Descent(2/49): loss=0.4796560143651844\n",
      "Gradient Descent(3/49): loss=0.4722153063724159\n",
      "Gradient Descent(4/49): loss=0.46603046716968727\n",
      "Gradient Descent(5/49): loss=0.46081072460902744\n",
      "Gradient Descent(6/49): loss=0.4563387670656144\n",
      "Gradient Descent(7/49): loss=0.45245166908727963\n",
      "Gradient Descent(8/49): loss=0.44902682672498806\n",
      "Gradient Descent(9/49): loss=0.44597157914681473\n",
      "Gradient Descent(10/49): loss=0.4432155438670975\n",
      "Gradient Descent(11/49): loss=0.44070495052662284\n",
      "Gradient Descent(12/49): loss=0.43839844739277734\n",
      "Gradient Descent(13/49): loss=0.43626399378010666\n",
      "Gradient Descent(14/49): loss=0.43427655375898666\n",
      "Gradient Descent(15/49): loss=0.43241638161394247\n",
      "Gradient Descent(16/49): loss=0.4306677447214458\n",
      "Gradient Descent(17/49): loss=0.4290179701171031\n",
      "Gradient Descent(18/49): loss=0.4274567308890376\n",
      "Gradient Descent(19/49): loss=0.4259755105135382\n",
      "Gradient Descent(20/49): loss=0.4245671994305657\n",
      "Gradient Descent(21/49): loss=0.4232257900755726\n",
      "Gradient Descent(22/49): loss=0.42194614536802433\n",
      "Gradient Descent(23/49): loss=0.420723822134622\n",
      "Gradient Descent(24/49): loss=0.4195549357255041\n",
      "Gradient Descent(25/49): loss=0.4184360556122699\n",
      "Gradient Descent(26/49): loss=0.41736412436665765\n",
      "Gradient Descent(27/49): loss=0.41633639435021363\n",
      "Gradient Descent(28/49): loss=0.41535037787638246\n",
      "Gradient Descent(29/49): loss=0.4144038076682431\n",
      "Gradient Descent(30/49): loss=0.41349460522411313\n",
      "Gradient Descent(31/49): loss=0.41262085529056547\n",
      "Gradient Descent(32/49): loss=0.4117807850804501\n",
      "Gradient Descent(33/49): loss=0.41097274720096194\n",
      "Gradient Descent(34/49): loss=0.41019520550216537\n",
      "Gradient Descent(35/49): loss=0.4094467232407655\n",
      "Gradient Descent(36/49): loss=0.4087259530928966\n",
      "Gradient Descent(37/49): loss=0.40803162865480064\n",
      "Gradient Descent(38/49): loss=0.40736255715006603\n",
      "Gradient Descent(39/49): loss=0.406717613122919\n",
      "Gradient Descent(40/49): loss=0.406095732943625\n",
      "Gradient Descent(41/49): loss=0.4054959099878859\n",
      "Gradient Descent(42/49): loss=0.40491719037982293\n",
      "Gradient Descent(43/49): loss=0.40435866920966945\n",
      "Gradient Descent(44/49): loss=0.40381948715414356\n",
      "Gradient Descent(45/49): loss=0.40329882744071593\n",
      "Gradient Descent(46/49): loss=0.4027959131074719\n",
      "Gradient Descent(47/49): loss=0.40231000451861926\n",
      "Gradient Descent(48/49): loss=0.401840397102385\n",
      "Gradient Descent(49/49): loss=0.40138641928344065\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48860683843071817\n",
      "Gradient Descent(2/49): loss=0.4794118547050772\n",
      "Gradient Descent(3/49): loss=0.47188023938378915\n",
      "Gradient Descent(4/49): loss=0.4656154336966647\n",
      "Gradient Descent(5/49): loss=0.46032268843507185\n",
      "Gradient Descent(6/49): loss=0.4557823083126269\n",
      "Gradient Descent(7/49): loss=0.45182999667994894\n",
      "Gradient Descent(8/49): loss=0.44834240706434186\n",
      "Gradient Descent(9/49): loss=0.44522651432807103\n",
      "Gradient Descent(10/49): loss=0.44241178896609773\n",
      "Gradient Descent(11/49): loss=0.4398444295411564\n",
      "Gradient Descent(12/49): loss=0.4374831070790947\n",
      "Gradient Descent(13/49): loss=0.43529582088623525\n",
      "Gradient Descent(14/49): loss=0.4332575719498344\n",
      "Gradient Descent(15/49): loss=0.4313486382699524\n",
      "Gradient Descent(16/49): loss=0.42955329377883794\n",
      "Gradient Descent(17/49): loss=0.4278588545194402\n",
      "Gradient Descent(18/49): loss=0.4262549665682617\n",
      "Gradient Descent(19/49): loss=0.4247330727941727\n",
      "Gradient Descent(20/49): loss=0.4232860121368697\n",
      "Gradient Descent(21/49): loss=0.4219077172722836\n",
      "Gradient Descent(22/49): loss=0.4205929854835942\n",
      "Gradient Descent(23/49): loss=0.4193373041372405\n",
      "Gradient Descent(24/49): loss=0.41813671700471394\n",
      "Gradient Descent(25/49): loss=0.4169877212356498\n",
      "Gradient Descent(26/49): loss=0.41588718741491365\n",
      "Gradient Descent(27/49): loss=0.41483229707470054\n",
      "Gradient Descent(28/49): loss=0.4138204934644951\n",
      "Gradient Descent(29/49): loss=0.41284944244093513\n",
      "Gradient Descent(30/49): loss=0.41191700112439733\n",
      "Gradient Descent(31/49): loss=0.411021192551651\n",
      "Gradient Descent(32/49): loss=0.4101601849872091\n",
      "Gradient Descent(33/49): loss=0.4093322748790372\n",
      "Gradient Descent(34/49): loss=0.4085358726857416\n",
      "Gradient Descent(35/49): loss=0.40776949098338466\n",
      "Gradient Descent(36/49): loss=0.40703173439622453\n",
      "Gradient Descent(37/49): loss=0.406321290998465\n",
      "Gradient Descent(38/49): loss=0.40563692491199216\n",
      "Gradient Descent(39/49): loss=0.4049774698843817\n",
      "Gradient Descent(40/49): loss=0.4043418236768134\n",
      "Gradient Descent(41/49): loss=0.4037289431264056\n",
      "Gradient Descent(42/49): loss=0.4031378397744383\n",
      "Gradient Descent(43/49): loss=0.4025675759729008\n",
      "Gradient Descent(44/49): loss=0.40201726139819965\n",
      "Gradient Descent(45/49): loss=0.40148604991377634\n",
      "Gradient Descent(46/49): loss=0.4009731367336236\n",
      "Gradient Descent(47/49): loss=0.4004777558468517\n",
      "Gradient Descent(48/49): loss=0.3999991776700202\n",
      "Gradient Descent(49/49): loss=0.39953670689925086\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48871449042697457\n",
      "Gradient Descent(2/49): loss=0.4796151261885868\n",
      "Gradient Descent(3/49): loss=0.4721700991954064\n",
      "Gradient Descent(4/49): loss=0.46598495888480207\n",
      "Gradient Descent(5/49): loss=0.46076642627096237\n",
      "Gradient Descent(6/49): loss=0.45629582429820426\n",
      "Gradient Descent(7/49): loss=0.4524095573835051\n",
      "Gradient Descent(8/49): loss=0.4489847600812122\n",
      "Gradient Descent(9/49): loss=0.4459287377011064\n",
      "Gradient Descent(10/49): loss=0.44317118986089643\n",
      "Gradient Descent(11/49): loss=0.44065847749691966\n",
      "Gradient Descent(12/49): loss=0.4383493912341736\n",
      "Gradient Descent(13/49): loss=0.43621202357527583\n",
      "Gradient Descent(14/49): loss=0.43422145326345124\n",
      "Gradient Descent(15/49): loss=0.4323580277657548\n",
      "Gradient Descent(16/49): loss=0.4306060866898868\n",
      "Gradient Descent(17/49): loss=0.42895301063903024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=0.4273885105847707\n",
      "Gradient Descent(19/49): loss=0.42590409527085404\n",
      "Gradient Descent(20/49): loss=0.4244926706264284\n",
      "Gradient Descent(21/49): loss=0.4231482372599215\n",
      "Gradient Descent(22/49): loss=0.42186566099072387\n",
      "Gradient Descent(23/49): loss=0.42064049791008795\n",
      "Gradient Descent(24/49): loss=0.41946886027130253\n",
      "Gradient Descent(25/49): loss=0.4183473130511151\n",
      "Gradient Descent(26/49): loss=0.41727279363591874\n",
      "Gradient Descent(27/49): loss=0.41624254901400765\n",
      "Gradient Descent(28/49): loss=0.4152540862801422\n",
      "Gradient Descent(29/49): loss=0.41430513331349683\n",
      "Gradient Descent(30/49): loss=0.4133936072722528\n",
      "Gradient Descent(31/49): loss=0.4125175891292214\n",
      "Gradient Descent(32/49): loss=0.41167530290557525\n",
      "Gradient Descent(33/49): loss=0.4108650985827107\n",
      "Gradient Descent(34/49): loss=0.41008543791396374\n",
      "Gradient Descent(35/49): loss=0.4093348825393487\n",
      "Gradient Descent(36/49): loss=0.4086120839431477\n",
      "Gradient Descent(37/49): loss=0.40791577489751074\n",
      "Gradient Descent(38/49): loss=0.4072447621136528\n",
      "Gradient Descent(39/49): loss=0.4065979198820423\n",
      "Gradient Descent(40/49): loss=0.40597418452879336\n",
      "Gradient Descent(41/49): loss=0.40537254955075797\n",
      "Gradient Descent(42/49): loss=0.4047920613191338\n",
      "Gradient Descent(43/49): loss=0.4042318152626839\n",
      "Gradient Descent(44/49): loss=0.4036909524583286\n",
      "Gradient Descent(45/49): loss=0.4031686565700153\n",
      "Gradient Descent(46/49): loss=0.4026641510871971\n",
      "Gradient Descent(47/49): loss=0.40217669682257307\n",
      "Gradient Descent(48/49): loss=0.40170558963543584\n",
      "Gradient Descent(49/49): loss=0.4012501583523835\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4885779764846337\n",
      "Gradient Descent(2/49): loss=0.4793561172476302\n",
      "Gradient Descent(3/49): loss=0.47180135115738664\n",
      "Gradient Descent(4/49): loss=0.4655178082766941\n",
      "Gradient Descent(5/49): loss=0.46021080330244074\n",
      "Gradient Descent(6/49): loss=0.4556603550105019\n",
      "Gradient Descent(7/49): loss=0.4517017057265058\n",
      "Gradient Descent(8/49): loss=0.4482109809101283\n",
      "Gradient Descent(9/49): loss=0.4450946245197343\n",
      "Gradient Descent(10/49): loss=0.4422816091400743\n",
      "Gradient Descent(11/49): loss=0.4397176862413276\n",
      "Gradient Descent(12/49): loss=0.43736113728179415\n",
      "Gradient Descent(13/49): loss=0.4351796296386311\n",
      "Gradient Descent(14/49): loss=0.4331478864517991\n",
      "Gradient Descent(15/49): loss=0.43124595658245896\n",
      "Gradient Descent(16/49): loss=0.4294579274839643\n",
      "Gradient Descent(17/49): loss=0.4277709653330085\n",
      "Gradient Descent(18/49): loss=0.4261745972811333\n",
      "Gradient Descent(19/49): loss=0.42466017310283616\n",
      "Gradient Descent(20/49): loss=0.4232204599914722\n",
      "Gradient Descent(21/49): loss=0.42184933636866573\n",
      "Gradient Descent(22/49): loss=0.42054155948611255\n",
      "Gradient Descent(23/49): loss=0.4192925881607353\n",
      "Gradient Descent(24/49): loss=0.4180984468188869\n",
      "Gradient Descent(25/49): loss=0.41695562059039054\n",
      "Gradient Descent(26/49): loss=0.4158609738246394\n",
      "Gradient Descent(27/49): loss=0.4148116863454091\n",
      "Gradient Descent(28/49): loss=0.41380520319963593\n",
      "Gradient Descent(29/49): loss=0.41283919472129815\n",
      "Gradient Descent(30/49): loss=0.41191152452255564\n",
      "Gradient Descent(31/49): loss=0.41102022361242263\n",
      "Gradient Descent(32/49): loss=0.410163469281415\n",
      "Gradient Descent(33/49): loss=0.4093395677178413\n",
      "Gradient Descent(34/49): loss=0.4085469395664098\n",
      "Gradient Descent(35/49): loss=0.40778410782382124\n",
      "Gradient Descent(36/49): loss=0.4070496876046349\n",
      "Gradient Descent(37/49): loss=0.40634237741551726\n",
      "Gradient Descent(38/49): loss=0.4056609516555524\n",
      "Gradient Descent(39/49): loss=0.4050042541209646\n",
      "Gradient Descent(40/49): loss=0.40437119233908225\n",
      "Gradient Descent(41/49): loss=0.40376073259215445\n",
      "Gradient Descent(42/49): loss=0.40317189551933114\n",
      "Gradient Descent(43/49): loss=0.4026037522066897\n",
      "Gradient Descent(44/49): loss=0.4020554206920748\n",
      "Gradient Descent(45/49): loss=0.40152606282483577\n",
      "Gradient Descent(46/49): loss=0.40101488143110176\n",
      "Gradient Descent(47/49): loss=0.40052111774366234\n",
      "Gradient Descent(48/49): loss=0.400044049062295\n",
      "Gradient Descent(49/49): loss=0.3995829866158581\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4835799149313509\n",
      "Gradient Descent(2/49): loss=0.47170948143934616\n",
      "Gradient Descent(3/49): loss=0.46280774163127797\n",
      "Gradient Descent(4/49): loss=0.4558772872696406\n",
      "Gradient Descent(5/49): loss=0.45028456197342653\n",
      "Gradient Descent(6/49): loss=0.4456235947655406\n",
      "Gradient Descent(7/49): loss=0.4416313375758788\n",
      "Gradient Descent(8/49): loss=0.4381349521993736\n",
      "Gradient Descent(9/49): loss=0.4350189022646919\n",
      "Gradient Descent(10/49): loss=0.4322043407459503\n",
      "Gradient Descent(11/49): loss=0.42963614537728056\n",
      "Gradient Descent(12/49): loss=0.4272747223059414\n",
      "Gradient Descent(13/49): loss=0.4250907912751967\n",
      "Gradient Descent(14/49): loss=0.4230620418536895\n",
      "Gradient Descent(15/49): loss=0.42117096905015977\n",
      "Gradient Descent(16/49): loss=0.4194034563827364\n",
      "Gradient Descent(17/49): loss=0.41774783579704533\n",
      "Gradient Descent(18/49): loss=0.4161942542256773\n",
      "Gradient Descent(19/49): loss=0.4147342392129861\n",
      "Gradient Descent(20/49): loss=0.41336039521562395\n",
      "Gradient Descent(21/49): loss=0.41206618679526774\n",
      "Gradient Descent(22/49): loss=0.4108457804384209\n",
      "Gradient Descent(23/49): loss=0.4096939265771922\n",
      "Gradient Descent(24/49): loss=0.4086058696627617\n",
      "Gradient Descent(25/49): loss=0.4075772781790952\n",
      "Gradient Descent(26/49): loss=0.40660418910201157\n",
      "Gradient Descent(27/49): loss=0.40568296302378104\n",
      "Gradient Descent(28/49): loss=0.4048102473002968\n",
      "Gradient Descent(29/49): loss=0.40398294534124585\n",
      "Gradient Descent(30/49): loss=0.40319819068358165\n",
      "Gradient Descent(31/49): loss=0.4024533248480103\n",
      "Gradient Descent(32/49): loss=0.4017458782305859\n",
      "Gradient Descent(33/49): loss=0.4010735534615677\n",
      "Gradient Descent(34/49): loss=0.40043421079420005\n",
      "Gradient Descent(35/49): loss=0.39982585518209435\n",
      "Gradient Descent(36/49): loss=0.399246624775592\n",
      "Gradient Descent(37/49): loss=0.39869478062172475\n",
      "Gradient Descent(38/49): loss=0.3981686973939519\n",
      "Gradient Descent(39/49): loss=0.39766685501005156\n",
      "Gradient Descent(40/49): loss=0.3971878310217487\n",
      "Gradient Descent(41/49): loss=0.39673029367958296\n",
      "Gradient Descent(42/49): loss=0.3962929955923936\n",
      "Gradient Descent(43/49): loss=0.39587476791354964\n",
      "Gradient Descent(44/49): loss=0.3954745149963727\n",
      "Gradient Descent(45/49): loss=0.39509120946961107\n",
      "Gradient Descent(46/49): loss=0.394723887690718\n",
      "Gradient Descent(47/49): loss=0.3943716455403841\n",
      "Gradient Descent(48/49): loss=0.3940336345265014\n",
      "Gradient Descent(49/49): loss=0.39370905816969254\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4833825275937226\n",
      "Gradient Descent(2/49): loss=0.47136986984351426\n",
      "Gradient Descent(3/49): loss=0.4623521455090635\n",
      "Gradient Descent(4/49): loss=0.45531838018856985\n",
      "Gradient Descent(5/49): loss=0.44962917157722276\n",
      "Gradient Descent(6/49): loss=0.44487639455203914\n",
      "Gradient Descent(7/49): loss=0.4407964734558119\n",
      "Gradient Descent(8/49): loss=0.4372166531539856\n",
      "Gradient Descent(9/49): loss=0.43402162276875766\n",
      "Gradient Descent(10/49): loss=0.4311327114133032\n",
      "Gradient Descent(11/49): loss=0.4284948645450745\n",
      "Gradient Descent(12/49): loss=0.4260684471281774\n",
      "Gradient Descent(13/49): loss=0.42382405011669383\n",
      "Gradient Descent(14/49): loss=0.42173917261678295\n",
      "Gradient Descent(15/49): loss=0.41979608089519177\n",
      "Gradient Descent(16/49): loss=0.41798040998722025\n",
      "Gradient Descent(17/49): loss=0.4162802371700758\n",
      "Gradient Descent(18/49): loss=0.41468545781646127\n",
      "Gradient Descent(19/49): loss=0.41318735699059106\n",
      "Gradient Descent(20/49): loss=0.41177830927735226\n",
      "Gradient Descent(21/49): loss=0.4104515637873595\n",
      "Gradient Descent(22/49): loss=0.40920108663097104\n",
      "Gradient Descent(23/49): loss=0.40802144284449576\n",
      "Gradient Descent(24/49): loss=0.4069077059100927\n",
      "Gradient Descent(25/49): loss=0.40585538695600876\n",
      "Gradient Descent(26/49): loss=0.40486037827515314\n",
      "Gradient Descent(27/49): loss=0.40391890746814146\n",
      "Gradient Descent(28/49): loss=0.4030274996213161\n",
      "Gradient Descent(29/49): loss=0.40218294567162927\n",
      "Gradient Descent(30/49): loss=0.4013822756155697\n",
      "Gradient Descent(31/49): loss=0.4006227355693038\n",
      "Gradient Descent(32/49): loss=0.39990176793367627\n",
      "Gradient Descent(33/49): loss=0.39921699409423367\n",
      "Gradient Descent(34/49): loss=0.3985661992149459\n",
      "Gradient Descent(35/49): loss=0.39794731877934036\n",
      "Gradient Descent(36/49): loss=0.39735842660410764\n",
      "Gradient Descent(37/49): loss=0.39679772410451264\n",
      "Gradient Descent(38/49): loss=0.39626353063275727\n",
      "Gradient Descent(39/49): loss=0.3957542747430163\n",
      "Gradient Descent(40/49): loss=0.39526848626249145\n",
      "Gradient Descent(41/49): loss=0.3948047890681829\n",
      "Gradient Descent(42/49): loss=0.3943618944853628\n",
      "Gradient Descent(43/49): loss=0.39393859523688374\n",
      "Gradient Descent(44/49): loss=0.3935337598831194\n",
      "Gradient Descent(45/49): loss=0.39314632770106717\n",
      "Gradient Descent(46/49): loss=0.3927753039583231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=0.3924197555435792\n",
      "Gradient Descent(48/49): loss=0.3920788069202446\n",
      "Gradient Descent(49/49): loss=0.3917516363739356\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4835414026434604\n",
      "Gradient Descent(2/49): loss=0.4716624985851935\n",
      "Gradient Descent(3/49): loss=0.46276226456544034\n",
      "Gradient Descent(4/49): loss=0.45583477067212647\n",
      "Gradient Descent(5/49): loss=0.4502434312868956\n",
      "Gradient Descent(6/49): loss=0.44558166961130935\n",
      "Gradient Descent(7/49): loss=0.44158676071040365\n",
      "Gradient Descent(8/49): loss=0.4380864348379369\n",
      "Gradient Descent(9/49): loss=0.43496568908684036\n",
      "Gradient Descent(10/49): loss=0.43214608513078173\n",
      "Gradient Descent(11/49): loss=0.4295727775584244\n",
      "Gradient Descent(12/49): loss=0.4272063407316924\n",
      "Gradient Descent(13/49): loss=0.42501758363839887\n",
      "Gradient Descent(14/49): loss=0.4229842326585492\n",
      "Gradient Descent(15/49): loss=0.4210887876989345\n",
      "Gradient Descent(16/49): loss=0.4193171197804246\n",
      "Gradient Descent(17/49): loss=0.41765754053085996\n",
      "Gradient Descent(18/49): loss=0.4161001746338781\n",
      "Gradient Descent(19/49): loss=0.4146365287749078\n",
      "Gradient Descent(20/49): loss=0.41325918957085606\n",
      "Gradient Descent(21/49): loss=0.41196160733786485\n",
      "Gradient Descent(22/49): loss=0.41073793787223134\n",
      "Gradient Descent(23/49): loss=0.4095829241085425\n",
      "Gradient Descent(24/49): loss=0.4084918056894576\n",
      "Gradient Descent(25/49): loss=0.407460248443807\n",
      "Gradient Descent(26/49): loss=0.4064842883386895\n",
      "Gradient Descent(27/49): loss=0.4055602861555142\n",
      "Gradient Descent(28/49): loss=0.40468489025805904\n",
      "Gradient Descent(29/49): loss=0.4038550055732306\n",
      "Gradient Descent(30/49): loss=0.4030677674193914\n",
      "Gradient Descent(31/49): loss=0.4023205191739734\n",
      "Gradient Descent(32/49): loss=0.4016107930237571\n",
      "Gradient Descent(33/49): loss=0.4009362932215659\n",
      "Gradient Descent(34/49): loss=0.4002948814044404\n",
      "Gradient Descent(35/49): loss=0.39968456362539057\n",
      "Gradient Descent(36/49): loss=0.39910347882356495\n",
      "Gradient Descent(37/49): loss=0.39854988851288525\n",
      "Gradient Descent(38/49): loss=0.3980221675116161\n",
      "Gradient Descent(39/49): loss=0.3975187955682828\n",
      "Gradient Descent(40/49): loss=0.3970383497651683\n",
      "Gradient Descent(41/49): loss=0.39657949760105504\n",
      "Gradient Descent(42/49): loss=0.3961409906711604\n",
      "Gradient Descent(43/49): loss=0.39572165887529714\n",
      "Gradient Descent(44/49): loss=0.39532040509588245\n",
      "Gradient Descent(45/49): loss=0.3949362002960248\n",
      "Gradient Descent(46/49): loss=0.39456807899499224\n",
      "Gradient Descent(47/49): loss=0.3942151350841825\n",
      "Gradient Descent(48/49): loss=0.3938765179515429\n",
      "Gradient Descent(49/49): loss=0.3935514288864282\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4833393590638122\n",
      "Gradient Descent(2/49): loss=0.4712893160854463\n",
      "Gradient Descent(3/49): loss=0.462244281132413\n",
      "Gradient Descent(4/49): loss=0.45519347229461665\n",
      "Gradient Descent(5/49): loss=0.4494960290446543\n",
      "Gradient Descent(6/49): loss=0.4447419099494959\n",
      "Gradient Descent(7/49): loss=0.44066570038706865\n",
      "Gradient Descent(8/49): loss=0.4370930868428402\n",
      "Gradient Descent(9/49): loss=0.4339075248699908\n",
      "Gradient Descent(10/49): loss=0.43102940989309035\n",
      "Gradient Descent(11/49): loss=0.4284030043162346\n",
      "Gradient Descent(12/49): loss=0.42598818815343875\n",
      "Gradient Descent(13/49): loss=0.4237552184870593\n",
      "Gradient Descent(14/49): loss=0.4216813728905692\n",
      "Gradient Descent(15/49): loss=0.41974877800967053\n",
      "Gradient Descent(16/49): loss=0.417942987986468\n",
      "Gradient Descent(17/49): loss=0.4162520406305724\n",
      "Gradient Descent(18/49): loss=0.41466582055173523\n",
      "Gradient Descent(19/49): loss=0.4131756215100369\n",
      "Gradient Descent(20/49): loss=0.4117738395888408\n",
      "Gradient Descent(21/49): loss=0.4104537534498302\n",
      "Gradient Descent(22/49): loss=0.4092093634479407\n",
      "Gradient Descent(23/49): loss=0.4080352712069743\n",
      "Gradient Descent(24/49): loss=0.4069265875164684\n",
      "Gradient Descent(25/49): loss=0.4058788604315541\n",
      "Gradient Descent(26/49): loss=0.40488801806502056\n",
      "Gradient Descent(27/49): loss=0.40395032227000277\n",
      "Gradient Descent(28/49): loss=0.4030623305459741\n",
      "Gradient Descent(29/49): loss=0.4022208642637648\n",
      "Gradient Descent(30/49): loss=0.40142298182630726\n",
      "Gradient Descent(31/49): loss=0.4006659557431477\n",
      "Gradient Descent(32/49): loss=0.3999472528514468\n",
      "Gradient Descent(33/49): loss=0.39926451709865857\n",
      "Gradient Descent(34/49): loss=0.3986155544349019\n",
      "Gradient Descent(35/49): loss=0.3979983194612023\n",
      "Gradient Descent(36/49): loss=0.39741090355339875\n",
      "Gradient Descent(37/49): loss=0.3968515242374236\n",
      "Gradient Descent(38/49): loss=0.3963185156346755\n",
      "Gradient Descent(39/49): loss=0.3958103198296306\n",
      "Gradient Descent(40/49): loss=0.39532547903808307\n",
      "Gradient Descent(41/49): loss=0.39486262847519715\n",
      "Gradient Descent(42/49): loss=0.3944204898391488\n",
      "Gradient Descent(43/49): loss=0.3939978653394939\n",
      "Gradient Descent(44/49): loss=0.39359363221021376\n",
      "Gradient Descent(45/49): loss=0.3932067376562149\n",
      "Gradient Descent(46/49): loss=0.39283619418929183\n",
      "Gradient Descent(47/49): loss=0.3924810753155446\n",
      "Gradient Descent(48/49): loss=0.3921405115411963\n",
      "Gradient Descent(49/49): loss=0.3918136866679064\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47872768882757355\n",
      "Gradient Descent(2/49): loss=0.4649720456915953\n",
      "Gradient Descent(3/49): loss=0.4554010044781728\n",
      "Gradient Descent(4/49): loss=0.44825377577849135\n",
      "Gradient Descent(5/49): loss=0.44258460522540843\n",
      "Gradient Descent(6/49): loss=0.43787378132937\n",
      "Gradient Descent(7/49): loss=0.4338267003572616\n",
      "Gradient Descent(8/49): loss=0.4302694686380035\n",
      "Gradient Descent(9/49): loss=0.4270942296951046\n",
      "Gradient Descent(10/49): loss=0.42423022229440194\n",
      "Gradient Descent(11/49): loss=0.4216282369821714\n",
      "Gradient Descent(12/49): loss=0.41925210849889233\n",
      "Gradient Descent(13/49): loss=0.41707394464488917\n",
      "Gradient Descent(14/49): loss=0.4150713688750406\n",
      "Gradient Descent(15/49): loss=0.413225868904596\n",
      "Gradient Descent(16/49): loss=0.41152176728321266\n",
      "Gradient Descent(17/49): loss=0.4099455518069716\n",
      "Gradient Descent(18/49): loss=0.40848542103149377\n",
      "Gradient Descent(19/49): loss=0.4071309630554639\n",
      "Gradient Descent(20/49): loss=0.4058729200076905\n",
      "Gradient Descent(21/49): loss=0.4047030097139293\n",
      "Gradient Descent(22/49): loss=0.4036137868614341\n",
      "Gradient Descent(23/49): loss=0.4025985323244339\n",
      "Gradient Descent(24/49): loss=0.4016511631412966\n",
      "Gradient Descent(25/49): loss=0.40076615801681736\n",
      "Gradient Descent(26/49): loss=0.39993849475331267\n",
      "Gradient Descent(27/49): loss=0.39916359702642773\n",
      "Gradient Descent(28/49): loss=0.3984372886093627\n",
      "Gradient Descent(29/49): loss=0.39775575362780397\n",
      "Gradient Descent(30/49): loss=0.3971155017678018\n",
      "Gradient Descent(31/49): loss=0.39651333760465773\n",
      "Gradient Descent(32/49): loss=0.3959463334014474\n",
      "Gradient Descent(33/49): loss=0.39541180486027305\n",
      "Gradient Descent(34/49): loss=0.39490728941075975\n",
      "Gradient Descent(35/49): loss=0.3944305266977082\n",
      "Gradient Descent(36/49): loss=0.3939794409895574\n",
      "Gradient Descent(37/49): loss=0.3935521252759299\n",
      "Gradient Descent(38/49): loss=0.39314682685931124\n",
      "Gradient Descent(39/49): loss=0.39276193427524275\n",
      "Gradient Descent(40/49): loss=0.3923959653990502\n",
      "Gradient Descent(41/49): loss=0.39204755661638196\n",
      "Gradient Descent(42/49): loss=0.3917154529506817\n",
      "Gradient Descent(43/49): loss=0.3913984990539016\n",
      "Gradient Descent(44/49): loss=0.3910956309778153\n",
      "Gradient Descent(45/49): loss=0.3908058686526718\n",
      "Gradient Descent(46/49): loss=0.3905283090079344\n",
      "Gradient Descent(47/49): loss=0.3902621196767544\n",
      "Gradient Descent(48/49): loss=0.3900065332318148\n",
      "Gradient Descent(49/49): loss=0.38976084190541593\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47847306338849066\n",
      "Gradient Descent(2/49): loss=0.4645493005435854\n",
      "Gradient Descent(3/49): loss=0.45483978148288406\n",
      "Gradient Descent(4/49): loss=0.4475654485166112\n",
      "Gradient Descent(5/49): loss=0.4417762700578958\n",
      "Gradient Descent(6/49): loss=0.43695254621443047\n",
      "Gradient Descent(7/49): loss=0.4328004680763072\n",
      "Gradient Descent(8/49): loss=0.4291466616673046\n",
      "Gradient Descent(9/49): loss=0.4258833422873068\n",
      "Gradient Descent(10/49): loss=0.42293946709248603\n",
      "Gradient Descent(11/49): loss=0.4202653289187834\n",
      "Gradient Descent(12/49): loss=0.4178241604560131\n",
      "Gradient Descent(13/49): loss=0.4155874381207534\n",
      "Gradient Descent(14/49): loss=0.4135321692080802\n",
      "Gradient Descent(15/49): loss=0.4116392639350111\n",
      "Gradient Descent(16/49): loss=0.4098925161805308\n",
      "Gradient Descent(17/49): loss=0.4082779363328928\n",
      "Gradient Descent(18/49): loss=0.4067832950789353\n",
      "Gradient Descent(19/49): loss=0.40539779846870533\n",
      "Gradient Descent(20/49): loss=0.404111847931801\n",
      "Gradient Descent(21/49): loss=0.4029168573957768\n",
      "Gradient Descent(22/49): loss=0.4018051101616359\n",
      "Gradient Descent(23/49): loss=0.4007696443448799\n",
      "Gradient Descent(24/49): loss=0.39980415941377834\n",
      "Gradient Descent(25/49): loss=0.39890293868582827\n",
      "Gradient Descent(26/49): loss=0.3980607841490843\n",
      "Gradient Descent(27/49): loss=0.3972729609784492\n",
      "Gradient Descent(28/49): loss=0.39653514980416876\n",
      "Gradient Descent(29/49): loss=0.3958434052716141\n",
      "Gradient Descent(30/49): loss=0.3951941197762164\n",
      "Gradient Descent(31/49): loss=0.39458399150843193\n",
      "Gradient Descent(32/49): loss=0.3940099961291108\n",
      "Gradient Descent(33/49): loss=0.3934693615345374\n",
      "Gradient Descent(34/49): loss=0.39295954527566707\n",
      "Gradient Descent(35/49): loss=0.39247821427674456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=0.39202322656095295\n",
      "Gradient Descent(37/49): loss=0.39159261473964274\n",
      "Gradient Descent(38/49): loss=0.39118457106034515\n",
      "Gradient Descent(39/49): loss=0.39079743383965754\n",
      "Gradient Descent(40/49): loss=0.3904296751320113\n",
      "Gradient Descent(41/49): loss=0.3900798895056499\n",
      "Gradient Descent(42/49): loss=0.38974678381386896\n",
      "Gradient Descent(43/49): loss=0.3894291678634731\n",
      "Gradient Descent(44/49): loss=0.389125945894073\n",
      "Gradient Descent(45/49): loss=0.3888361087917227\n",
      "Gradient Descent(46/49): loss=0.3885587269688267\n",
      "Gradient Descent(47/49): loss=0.3882929438495089\n",
      "Gradient Descent(48/49): loss=0.3880379699059173\n",
      "Gradient Descent(49/49): loss=0.3877930771964355\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4786814261952782\n",
      "Gradient Descent(2/49): loss=0.46492468255621927\n",
      "Gradient Descent(3/49): loss=0.45535907000948744\n",
      "Gradient Descent(4/49): loss=0.44821369049856224\n",
      "Gradient Descent(5/49): loss=0.44254201305182606\n",
      "Gradient Descent(6/49): loss=0.43782582591514163\n",
      "Gradient Descent(7/49): loss=0.4337720346667148\n",
      "Gradient Descent(8/49): loss=0.4302077483800845\n",
      "Gradient Descent(9/49): loss=0.42702564450592834\n",
      "Gradient Descent(10/49): loss=0.42415518299832533\n",
      "Gradient Descent(11/49): loss=0.4215472011330186\n",
      "Gradient Descent(12/49): loss=0.419165498907035\n",
      "Gradient Descent(13/49): loss=0.41698212172992344\n",
      "Gradient Descent(14/49): loss=0.4149746302988353\n",
      "Gradient Descent(15/49): loss=0.41312446068960706\n",
      "Gradient Descent(16/49): loss=0.41141589795692957\n",
      "Gradient Descent(17/49): loss=0.4098354055652484\n",
      "Gradient Descent(18/49): loss=0.40837116838659554\n",
      "Gradient Descent(19/49): loss=0.40701276868976605\n",
      "Gradient Descent(20/49): loss=0.40575094811542484\n",
      "Gradient Descent(21/49): loss=0.4045774273073463\n",
      "Gradient Descent(22/49): loss=0.40348476553325197\n",
      "Gradient Descent(23/49): loss=0.40246624889853505\n",
      "Gradient Descent(24/49): loss=0.40151579956085715\n",
      "Gradient Descent(25/49): loss=0.4006279007379248\n",
      "Gradient Descent(26/49): loss=0.39979753384234096\n",
      "Gradient Descent(27/49): loss=0.39902012510341034\n",
      "Gradient Descent(28/49): loss=0.39829149973649625\n",
      "Gradient Descent(29/49): loss=0.3976078422100008\n",
      "Gradient Descent(30/49): loss=0.3969656615086511\n",
      "Gradient Descent(31/49): loss=0.3963617605442409\n",
      "Gradient Descent(32/49): loss=0.3957932090505081\n",
      "Gradient Descent(33/49): loss=0.3952573194369584\n",
      "Gradient Descent(34/49): loss=0.3947516251805287\n",
      "Gradient Descent(35/49): loss=0.3942738614132945\n",
      "Gradient Descent(36/49): loss=0.3938219474255331\n",
      "Gradient Descent(37/49): loss=0.3933939708510383\n",
      "Gradient Descent(38/49): loss=0.3929881733390341\n",
      "Gradient Descent(39/49): loss=0.392602937546826\n",
      "Gradient Descent(40/49): loss=0.39223677531128515\n",
      "Gradient Descent(41/49): loss=0.39188831687671727\n",
      "Gradient Descent(42/49): loss=0.3915563010726514\n",
      "Gradient Descent(43/49): loss=0.39123956634833135\n",
      "Gradient Descent(44/49): loss=0.3909370425817929\n",
      "Gradient Descent(45/49): loss=0.390647743590795\n",
      "Gradient Descent(46/49): loss=0.3903707602808695\n",
      "Gradient Descent(47/49): loss=0.3901052543726446\n",
      "Gradient Descent(48/49): loss=0.3898504526565503\n",
      "Gradient Descent(49/49): loss=0.3896056417282237\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47841567120089845\n",
      "Gradient Descent(2/49): loss=0.46444692637039914\n",
      "Gradient Descent(3/49): loss=0.45471172318706327\n",
      "Gradient Descent(4/49): loss=0.4474281848822595\n",
      "Gradient Descent(5/49): loss=0.44164141096068615\n",
      "Gradient Descent(6/49): loss=0.4368273481594281\n",
      "Gradient Descent(7/49): loss=0.4326889495058558\n",
      "Gradient Descent(8/49): loss=0.4290506441617947\n",
      "Gradient Descent(9/49): loss=0.42580324690622406\n",
      "Gradient Descent(10/49): loss=0.4228748709901667\n",
      "Gradient Descent(11/49): loss=0.42021533481406004\n",
      "Gradient Descent(12/49): loss=0.4177876333342859\n",
      "Gradient Descent(13/49): loss=0.4155631524158271\n",
      "Gradient Descent(14/49): loss=0.4135188974375974\n",
      "Gradient Descent(15/49): loss=0.4116358278192405\n",
      "Gradient Descent(16/49): loss=0.40989781409931114\n",
      "Gradient Descent(17/49): loss=0.4082909560878808\n",
      "Gradient Descent(18/49): loss=0.4068031177020576\n",
      "Gradient Descent(19/49): loss=0.405423596725196\n",
      "Gradient Descent(20/49): loss=0.4041428818216461\n",
      "Gradient Descent(21/49): loss=0.40295246809723845\n",
      "Gradient Descent(22/49): loss=0.4018447133112204\n",
      "Gradient Descent(23/49): loss=0.4008127231975558\n",
      "Gradient Descent(24/49): loss=0.39985025820383\n",
      "Gradient Descent(25/49): loss=0.39895165636680224\n",
      "Gradient Descent(26/49): loss=0.39811176860176267\n",
      "Gradient Descent(27/49): loss=0.39732590372006127\n",
      "Gradient Descent(28/49): loss=0.39658978119814087\n",
      "Gradient Descent(29/49): loss=0.3958994902173249\n",
      "Gradient Descent(30/49): loss=0.3952514538474217\n",
      "Gradient Descent(31/49): loss=0.394642397503965\n",
      "Gradient Descent(32/49): loss=0.39406932099798664\n",
      "Gradient Descent(33/49): loss=0.39352947363828733\n",
      "Gradient Descent(34/49): loss=0.3930203319526841\n",
      "Gradient Descent(35/49): loss=0.3925395796760207\n",
      "Gradient Descent(36/49): loss=0.39208508971548134\n",
      "Gradient Descent(37/49): loss=0.3916549078526765\n",
      "Gradient Descent(38/49): loss=0.3912472379805301\n",
      "Gradient Descent(39/49): loss=0.3908604287036974\n",
      "Gradient Descent(40/49): loss=0.3904929611559449\n",
      "Gradient Descent(41/49): loss=0.3901434379080086\n",
      "Gradient Descent(42/49): loss=0.3898105728559368\n",
      "Gradient Descent(43/49): loss=0.38949318199361155\n",
      "Gradient Descent(44/49): loss=0.3891901749846033\n",
      "Gradient Descent(45/49): loss=0.38890054745820973\n",
      "Gradient Descent(46/49): loss=0.38862337396279323\n",
      "Gradient Descent(47/49): loss=0.38835780151664634\n",
      "Gradient Descent(48/49): loss=0.38810304370277293\n",
      "Gradient Descent(49/49): loss=0.38785837525935096\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47418603051668234\n",
      "Gradient Descent(2/49): loss=0.4592589087477345\n",
      "Gradient Descent(3/49): loss=0.44945816676918876\n",
      "Gradient Descent(4/49): loss=0.44227063343702466\n",
      "Gradient Descent(5/49): loss=0.4365608237761697\n",
      "Gradient Descent(6/49): loss=0.43178779932049477\n",
      "Gradient Descent(7/49): loss=0.42767398949929775\n",
      "Gradient Descent(8/49): loss=0.4240630995533821\n",
      "Gradient Descent(9/49): loss=0.4208578124297736\n",
      "Gradient Descent(10/49): loss=0.41799160247764877\n",
      "Gradient Descent(11/49): loss=0.4154154131379386\n",
      "Gradient Descent(12/49): loss=0.41309099146687683\n",
      "Gradient Descent(13/49): loss=0.4109873184149777\n",
      "Gradient Descent(14/49): loss=0.4090785508267919\n",
      "Gradient Descent(15/49): loss=0.4073427461776777\n",
      "Gradient Descent(16/49): loss=0.4057610194791869\n",
      "Gradient Descent(17/49): loss=0.40431695460906103\n",
      "Gradient Descent(18/49): loss=0.40299617453160963\n",
      "Gradient Descent(19/49): loss=0.4017860159506792\n",
      "Gradient Descent(20/49): loss=0.4006752756140972\n",
      "Gradient Descent(21/49): loss=0.39965400758092345\n",
      "Gradient Descent(22/49): loss=0.39871335786749673\n",
      "Gradient Descent(23/49): loss=0.3978454272547648\n",
      "Gradient Descent(24/49): loss=0.39704315582517213\n",
      "Gradient Descent(25/49): loss=0.3963002246295768\n",
      "Gradient Descent(26/49): loss=0.395610971120271\n",
      "Gradient Descent(27/49): loss=0.3949703158374596\n",
      "Gradient Descent(28/49): loss=0.39437369843433817\n",
      "Gradient Descent(29/49): loss=0.39381702155327086\n",
      "Gradient Descent(30/49): loss=0.39329660137642686\n",
      "Gradient Descent(31/49): loss=0.392809123904248\n",
      "Gradient Descent(32/49): loss=0.39235160618827747\n",
      "Gradient Descent(33/49): loss=0.3919213618774509\n",
      "Gradient Descent(34/49): loss=0.3915159705401953\n",
      "Gradient Descent(35/49): loss=0.3911332503063813\n",
      "Gradient Descent(36/49): loss=0.3907712334388658\n",
      "Gradient Descent(37/49): loss=0.39042814449793545\n",
      "Gradient Descent(38/49): loss=0.3901023808062467\n",
      "Gradient Descent(39/49): loss=0.38979249495891105\n",
      "Gradient Descent(40/49): loss=0.38949717915470344\n",
      "Gradient Descent(41/49): loss=0.3892152511511024\n",
      "Gradient Descent(42/49): loss=0.38894564166887813\n",
      "Gradient Descent(43/49): loss=0.3886873830918608\n",
      "Gradient Descent(44/49): loss=0.3884395993248772\n",
      "Gradient Descent(45/49): loss=0.38820149668803067\n",
      "Gradient Descent(46/49): loss=0.3879723557388517\n",
      "Gradient Descent(47/49): loss=0.3877515239256109\n",
      "Gradient Descent(48/49): loss=0.3875384089854945\n",
      "Gradient Descent(49/49): loss=0.38733247301056434\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4738784458150223\n",
      "Gradient Descent(2/49): loss=0.45876192471651517\n",
      "Gradient Descent(3/49): loss=0.44879926521680796\n",
      "Gradient Descent(4/49): loss=0.4414602010209498\n",
      "Gradient Descent(5/49): loss=0.4356094455585658\n",
      "Gradient Descent(6/49): loss=0.4307081700142481\n",
      "Gradient Descent(7/49): loss=0.4264796933061042\n",
      "Gradient Descent(8/49): loss=0.42276740462589213\n",
      "Gradient Descent(9/49): loss=0.4194729920284723\n",
      "Gradient Descent(10/49): loss=0.4165286712710228\n",
      "Gradient Descent(11/49): loss=0.4138841013083226\n",
      "Gradient Descent(12/49): loss=0.4114998313925016\n",
      "Gradient Descent(13/49): loss=0.40934377287442975\n",
      "Gradient Descent(14/49): loss=0.40738914731997355\n",
      "Gradient Descent(15/49): loss=0.4056132018912688\n",
      "Gradient Descent(16/49): loss=0.40399635153310054\n",
      "Gradient Descent(17/49): loss=0.4025215749161926\n",
      "Gradient Descent(18/49): loss=0.4011739705389232\n",
      "Gradient Descent(19/49): loss=0.3999404191460661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/49): loss=0.3988093197153606\n",
      "Gradient Descent(21/49): loss=0.3977703781272129\n",
      "Gradient Descent(22/49): loss=0.3968144346735483\n",
      "Gradient Descent(23/49): loss=0.3959333209342411\n",
      "Gradient Descent(24/49): loss=0.3951197393668342\n",
      "Gradient Descent(25/49): loss=0.39436716082478235\n",
      "Gradient Descent(26/49): loss=0.3936697364900807\n",
      "Gradient Descent(27/49): loss=0.39302222158741623\n",
      "Gradient Descent(28/49): loss=0.39241990886931827\n",
      "Gradient Descent(29/49): loss=0.3918585703087615\n",
      "Gradient Descent(30/49): loss=0.391334405762001\n",
      "Gradient Descent(31/49): loss=0.390843997606572\n",
      "Gradient Descent(32/49): loss=0.3903842705420114\n",
      "Gradient Descent(33/49): loss=0.3899524558808608\n",
      "Gradient Descent(34/49): loss=0.3895460597665591\n",
      "Gradient Descent(35/49): loss=0.38916283484111824\n",
      "Gradient Descent(36/49): loss=0.3888007549547846\n",
      "Gradient Descent(37/49): loss=0.38845799256634733\n",
      "Gradient Descent(38/49): loss=0.3881328985293515\n",
      "Gradient Descent(39/49): loss=0.38782398399839657\n",
      "Gradient Descent(40/49): loss=0.38752990422254285\n",
      "Gradient Descent(41/49): loss=0.38724944402083933\n",
      "Gradient Descent(42/49): loss=0.3869815047590178\n",
      "Gradient Descent(43/49): loss=0.3867250926671841\n",
      "Gradient Descent(44/49): loss=0.38647930835641736\n",
      "Gradient Descent(45/49): loss=0.3862433374079969\n",
      "Gradient Descent(46/49): loss=0.3860164419228491\n",
      "Gradient Descent(47/49): loss=0.38579795293103736\n",
      "Gradient Descent(48/49): loss=0.38558726357190665\n",
      "Gradient Descent(49/49): loss=0.3853838229650714\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47413456108242863\n",
      "Gradient Descent(2/49): loss=0.4592144985889454\n",
      "Gradient Descent(3/49): loss=0.4494193118830266\n",
      "Gradient Descent(4/49): loss=0.4422290754503287\n",
      "Gradient Descent(5/49): loss=0.4365117357288613\n",
      "Gradient Descent(6/49): loss=0.4317296230468769\n",
      "Gradient Descent(7/49): loss=0.4276068608790672\n",
      "Gradient Descent(8/49): loss=0.42398775118757037\n",
      "Gradient Descent(9/49): loss=0.42077504089699774\n",
      "Gradient Descent(10/49): loss=0.4179020784222328\n",
      "Gradient Descent(11/49): loss=0.4153196527881339\n",
      "Gradient Descent(12/49): loss=0.4129893874873865\n",
      "Gradient Descent(13/49): loss=0.41088018272583576\n",
      "Gradient Descent(14/49): loss=0.40896615137786724\n",
      "Gradient Descent(15/49): loss=0.407225333173987\n",
      "Gradient Descent(16/49): loss=0.40563884183427584\n",
      "Gradient Descent(17/49): loss=0.4041902689247411\n",
      "Gradient Descent(18/49): loss=0.40286524892088543\n",
      "Gradient Descent(19/49): loss=0.40165113056474105\n",
      "Gradient Descent(20/49): loss=0.4005367212208524\n",
      "Gradient Descent(21/49): loss=0.3995120831118946\n",
      "Gradient Descent(22/49): loss=0.3985683675286192\n",
      "Gradient Descent(23/49): loss=0.39769767757137836\n",
      "Gradient Descent(24/49): loss=0.3968929528396115\n",
      "Gradient Descent(25/49): loss=0.39614787137018515\n",
      "Gradient Descent(26/49): loss=0.3954567653969583\n",
      "Gradient Descent(27/49): loss=0.3948145483792373\n",
      "Gradient Descent(28/49): loss=0.3942166513603999\n",
      "Gradient Descent(29/49): loss=0.39365896715560406\n",
      "Gradient Descent(30/49): loss=0.3931378011849548\n",
      "Gradient Descent(31/49): loss=0.39264982800269105\n",
      "Gradient Descent(32/49): loss=0.39219205274868124\n",
      "Gradient Descent(33/49): loss=0.3917617768826312\n",
      "Gradient Descent(34/49): loss=0.39135656766550714\n",
      "Gradient Descent(35/49): loss=0.39097423093480776\n",
      "Gradient Descent(36/49): loss=0.3906127867861588\n",
      "Gradient Descent(37/49): loss=0.3902704478272534\n",
      "Gradient Descent(38/49): loss=0.38994559971432596\n",
      "Gradient Descent(39/49): loss=0.3896367837182137\n",
      "Gradient Descent(40/49): loss=0.3893426810981786\n",
      "Gradient Descent(41/49): loss=0.3890620990881771\n",
      "Gradient Descent(42/49): loss=0.3887939583230476\n",
      "Gradient Descent(43/49): loss=0.3885372815518021\n",
      "Gradient Descent(44/49): loss=0.3882911835023645\n",
      "Gradient Descent(45/49): loss=0.3880548617771149\n",
      "Gradient Descent(46/49): loss=0.3878275886717837\n",
      "Gradient Descent(47/49): loss=0.387608703821869\n",
      "Gradient Descent(48/49): loss=0.3873976075910261\n",
      "Gradient Descent(49/49): loss=0.38719375512499066\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47380691289589233\n",
      "Gradient Descent(2/49): loss=0.4586412527358599\n",
      "Gradient Descent(3/49): loss=0.44865971720388786\n",
      "Gradient Descent(4/49): loss=0.44132284283050566\n",
      "Gradient Descent(5/49): loss=0.43548593175999917\n",
      "Gradient Descent(6/49): loss=0.4306037228287712\n",
      "Gradient Descent(7/49): loss=0.4263957485709105\n",
      "Gradient Descent(8/49): loss=0.4227033749885233\n",
      "Gradient Descent(9/49): loss=0.41942731799762456\n",
      "Gradient Descent(10/49): loss=0.41649941036391275\n",
      "Gradient Descent(11/49): loss=0.41386924284244314\n",
      "Gradient Descent(12/49): loss=0.4114974553741254\n",
      "Gradient Descent(13/49): loss=0.4093521226429235\n",
      "Gradient Descent(14/49): loss=0.40740665556147465\n",
      "Gradient Descent(15/49): loss=0.4056384921882127\n",
      "Gradient Descent(16/49): loss=0.40402822774427083\n",
      "Gradient Descent(17/49): loss=0.4025590051491314\n",
      "Gradient Descent(18/49): loss=0.40121606937939636\n",
      "Gradient Descent(19/49): loss=0.3999864300728006\n",
      "Gradient Descent(20/49): loss=0.3988585986546236\n",
      "Gradient Descent(21/49): loss=0.39782237855900676\n",
      "Gradient Descent(22/49): loss=0.39686869440360756\n",
      "Gradient Descent(23/49): loss=0.3959894504886778\n",
      "Gradient Descent(24/49): loss=0.395177411889081\n",
      "Gradient Descent(25/49): loss=0.3944261033226478\n",
      "Gradient Descent(26/49): loss=0.3937297222739858\n",
      "Gradient Descent(27/49): loss=0.39308306374737934\n",
      "Gradient Descent(28/49): loss=0.3924814546511177\n",
      "Gradient Descent(29/49): loss=0.3919206962650053\n",
      "Gradient Descent(30/49): loss=0.3913970135693956\n",
      "Gradient Descent(31/49): loss=0.39090701045535964\n",
      "Gradient Descent(32/49): loss=0.3904476300168282\n",
      "Gradient Descent(33/49): loss=0.39001611926396323\n",
      "Gradient Descent(34/49): loss=0.38960999770449856\n",
      "Gradient Descent(35/49): loss=0.3892270293246223\n",
      "Gradient Descent(36/49): loss=0.3888651975689879\n",
      "Gradient Descent(37/49): loss=0.3885226829747671\n",
      "Gradient Descent(38/49): loss=0.3881978431602907\n",
      "Gradient Descent(39/49): loss=0.3878891949069215\n",
      "Gradient Descent(40/49): loss=0.3875953981049624\n",
      "Gradient Descent(41/49): loss=0.38731524136180756\n",
      "Gradient Descent(42/49): loss=0.38704762909409557\n",
      "Gradient Descent(43/49): loss=0.386791569946006\n",
      "Gradient Descent(44/49): loss=0.3865461663935796\n",
      "Gradient Descent(45/49): loss=0.38631060541046897\n",
      "Gradient Descent(46/49): loss=0.3860841500841617\n",
      "Gradient Descent(47/49): loss=0.38586613208374293\n",
      "Gradient Descent(48/49): loss=0.3856559448908935\n",
      "Gradient Descent(49/49): loss=0.3854530377152369\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4699549399986775\n",
      "Gradient Descent(2/49): loss=0.45440215918398474\n",
      "Gradient Descent(3/49): loss=0.44457346596066505\n",
      "Gradient Descent(4/49): loss=0.4373613569957343\n",
      "Gradient Descent(5/49): loss=0.43158387207946103\n",
      "Gradient Descent(6/49): loss=0.42673794145708616\n",
      "Gradient Descent(7/49): loss=0.4225746717313133\n",
      "Gradient Descent(8/49): loss=0.4189495519518647\n",
      "Gradient Descent(9/49): loss=0.41576659619166406\n",
      "Gradient Descent(10/49): loss=0.41295572832089583\n",
      "Gradient Descent(11/49): loss=0.41046259034981414\n",
      "Gradient Descent(12/49): loss=0.4082433691300952\n",
      "Gradient Descent(13/49): loss=0.4062618581681782\n",
      "Gradient Descent(14/49): loss=0.4044876257678807\n",
      "Gradient Descent(15/49): loss=0.4028947873360875\n",
      "Gradient Descent(16/49): loss=0.4014611357060578\n",
      "Gradient Descent(17/49): loss=0.4001674977275577\n",
      "Gradient Descent(18/49): loss=0.39899724136244424\n",
      "Gradient Descent(19/49): loss=0.39793588722944445\n",
      "Gradient Descent(20/49): loss=0.39697079534776475\n",
      "Gradient Descent(21/49): loss=0.39609090781153167\n",
      "Gradient Descent(22/49): loss=0.3952865342816881\n",
      "Gradient Descent(23/49): loss=0.39454917109344767\n",
      "Gradient Descent(24/49): loss=0.393871347330697\n",
      "Gradient Descent(25/49): loss=0.3932464929281588\n",
      "Gradient Descent(26/49): loss=0.3926688250355859\n",
      "Gradient Descent(27/49): loss=0.39213324970435215\n",
      "Gradient Descent(28/49): loss=0.39163527655322206\n",
      "Gradient Descent(29/49): loss=0.39117094451155765\n",
      "Gradient Descent(30/49): loss=0.3907367570729436\n",
      "Gradient Descent(31/49): loss=0.3903296257517637\n",
      "Gradient Descent(32/49): loss=0.3899468206406825\n",
      "Gradient Descent(33/49): loss=0.38958592713252554\n",
      "Gradient Descent(34/49): loss=0.38924480800555256\n",
      "Gradient Descent(35/49): loss=0.3889215701834871\n",
      "Gradient Descent(36/49): loss=0.38861453557587616\n",
      "Gradient Descent(37/49): loss=0.38832221548403417\n",
      "Gradient Descent(38/49): loss=0.3880432881257019\n",
      "Gradient Descent(39/49): loss=0.38777657888970396\n",
      "Gradient Descent(40/49): loss=0.3875210429819265\n",
      "Gradient Descent(41/49): loss=0.38727575016716\n",
      "Gradient Descent(42/49): loss=0.38703987134878565\n",
      "Gradient Descent(43/49): loss=0.3868126667607764\n",
      "Gradient Descent(44/49): loss=0.38659347557474333\n",
      "Gradient Descent(45/49): loss=0.38638170674937183\n",
      "Gradient Descent(46/49): loss=0.38617683097104244\n",
      "Gradient Descent(47/49): loss=0.38597837355316705\n",
      "Gradient Descent(48/49): loss=0.3857859081781244\n",
      "Gradient Descent(49/49): loss=0.3855990513799771\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.46959867487331763\n",
      "Gradient Descent(2/49): loss=0.4538369283798866\n",
      "Gradient Descent(3/49): loss=0.443820950511342\n",
      "Gradient Descent(4/49): loss=0.43643430768038316\n",
      "Gradient Descent(5/49): loss=0.4305000980275004\n",
      "Gradient Descent(6/49): loss=0.4255174794987491\n",
      "Gradient Descent(7/49): loss=0.4212368260645323\n",
      "Gradient Descent(8/49): loss=0.4175115728019179\n",
      "Gradient Descent(9/49): loss=0.4142433832810241\n",
      "Gradient Descent(10/49): loss=0.4113599784825293\n",
      "Gradient Descent(11/49): loss=0.40880508467645116\n",
      "Gradient Descent(12/49): loss=0.406533271067579\n",
      "Gradient Descent(13/49): loss=0.40450697821339315\n",
      "Gradient Descent(14/49): loss=0.4026946432910863\n",
      "Gradient Descent(15/49): loss=0.40106943255584226\n",
      "Gradient Descent(16/49): loss=0.3996083382404764\n",
      "Gradient Descent(17/49): loss=0.3982915081986789\n",
      "Gradient Descent(18/49): loss=0.397101731569196\n",
      "Gradient Descent(19/49): loss=0.396024033295714\n",
      "Gradient Descent(20/49): loss=0.39504534727652935\n",
      "Gradient Descent(21/49): loss=0.39415424809456995\n",
      "Gradient Descent(22/49): loss=0.3933407276128837\n",
      "Gradient Descent(23/49): loss=0.3925960067784062\n",
      "Gradient Descent(24/49): loss=0.39191237564254827\n",
      "Gradient Descent(25/49): loss=0.3912830564008345\n",
      "Gradient Descent(26/49): loss=0.3907020854897455\n",
      "Gradient Descent(27/49): loss=0.390164211651112\n",
      "Gradient Descent(28/49): loss=0.3896648075049197\n",
      "Gradient Descent(29/49): loss=0.3891997926381752\n",
      "Gradient Descent(30/49): loss=0.3887655665711171\n",
      "Gradient Descent(31/49): loss=0.38835895023588457\n",
      "Gradient Descent(32/49): loss=0.3879771348190366\n",
      "Gradient Descent(33/49): loss=0.3876176369932365\n",
      "Gradient Descent(34/49): loss=0.3872782597054398\n",
      "Gradient Descent(35/49): loss=0.38695705780646\n",
      "Gradient Descent(36/49): loss=0.38665230790512045\n",
      "Gradient Descent(37/49): loss=0.38636248191322803\n",
      "Gradient Descent(38/49): loss=0.3860862238182284\n",
      "Gradient Descent(39/49): loss=0.38582232928082216\n",
      "Gradient Descent(40/49): loss=0.3855697277067742\n",
      "Gradient Descent(41/49): loss=0.3853274664869728\n",
      "Gradient Descent(42/49): loss=0.385094697138605\n",
      "Gradient Descent(43/49): loss=0.3848706631139787\n",
      "Gradient Descent(44/49): loss=0.3846546890727929\n",
      "Gradient Descent(45/49): loss=0.3844461714391429\n",
      "Gradient Descent(46/49): loss=0.384244570086767\n",
      "Gradient Descent(47/49): loss=0.3840494010154229\n",
      "Gradient Descent(48/49): loss=0.3838602298982247\n",
      "Gradient Descent(49/49): loss=0.3836766663945673\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46990080730491124\n",
      "Gradient Descent(2/49): loss=0.4543620603367525\n",
      "Gradient Descent(3/49): loss=0.4445351890332355\n",
      "Gradient Descent(4/49): loss=0.43731456644888883\n",
      "Gradient Descent(5/49): loss=0.43152565047689323\n",
      "Gradient Descent(6/49): loss=0.426668793697819\n",
      "Gradient Descent(7/49): loss=0.42249591841500217\n",
      "Gradient Descent(8/49): loss=0.4188623661424679\n",
      "Gradient Descent(9/49): loss=0.4156718186719071\n",
      "Gradient Descent(10/49): loss=0.41285393725440156\n",
      "Gradient Descent(11/49): loss=0.41035421115433784\n",
      "Gradient Descent(12/49): loss=0.40812876002634796\n",
      "Gradient Descent(13/49): loss=0.4061413628542494\n",
      "Gradient Descent(14/49): loss=0.40436160051409165\n",
      "Gradient Descent(15/49): loss=0.4027636115264931\n",
      "Gradient Descent(16/49): loss=0.40132521280514477\n",
      "Gradient Descent(17/49): loss=0.4000272512851756\n",
      "Gradient Descent(18/49): loss=0.39885310886383435\n",
      "Gradient Descent(19/49): loss=0.39778831339741993\n",
      "Gradient Descent(20/49): loss=0.39682022575945214\n",
      "Gradient Descent(21/49): loss=0.3959377832485958\n",
      "Gradient Descent(22/49): loss=0.3951312859772621\n",
      "Gradient Descent(23/49): loss=0.3943922168967684\n",
      "Gradient Descent(24/49): loss=0.39371308873549943\n",
      "Gradient Descent(25/49): loss=0.39308731287520804\n",
      "Gradient Descent(26/49): loss=0.3925090863865935\n",
      "Gradient Descent(27/49): loss=0.3919732942839716\n",
      "Gradient Descent(28/49): loss=0.3914754246619584\n",
      "Gradient Descent(29/49): loss=0.3910114948217848\n",
      "Gradient Descent(30/49): loss=0.39057798683078154\n",
      "Gradient Descent(31/49): loss=0.3901717912181725\n",
      "Gradient Descent(32/49): loss=0.3897901577151578\n",
      "Gradient Descent(33/49): loss=0.38943065211191535\n",
      "Gradient Descent(34/49): loss=0.3890911184386383\n",
      "Gradient Descent(35/49): loss=0.38876964578905815\n",
      "Gradient Descent(36/49): loss=0.3884645391981353\n",
      "Gradient Descent(37/49): loss=0.3881742940643723\n",
      "Gradient Descent(38/49): loss=0.38789757367426797\n",
      "Gradient Descent(39/49): loss=0.3876331894438691\n",
      "Gradient Descent(40/49): loss=0.38738008354179315\n",
      "Gradient Descent(41/49): loss=0.38713731360078146\n",
      "Gradient Descent(42/49): loss=0.38690403926181943\n",
      "Gradient Descent(43/49): loss=0.38667951032696607\n",
      "Gradient Descent(44/49): loss=0.3864630563249708\n",
      "Gradient Descent(45/49): loss=0.38625407731809236\n",
      "Gradient Descent(46/49): loss=0.38605203579976516\n",
      "Gradient Descent(47/49): loss=0.3858564495512992\n",
      "Gradient Descent(48/49): loss=0.38566688534200433\n",
      "Gradient Descent(49/49): loss=0.38548295337129757\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46951308414879417\n",
      "Gradient Descent(2/49): loss=0.45370185273921493\n",
      "Gradient Descent(3/49): loss=0.44367779988573347\n",
      "Gradient Descent(4/49): loss=0.43630575171587394\n",
      "Gradient Descent(5/49): loss=0.4303951420330227\n",
      "Gradient Descent(6/49): loss=0.4254378975341559\n",
      "Gradient Descent(7/49): loss=0.4211811136603411\n",
      "Gradient Descent(8/49): loss=0.4174769820443314\n",
      "Gradient Descent(9/49): loss=0.41422688579282096\n",
      "Gradient Descent(10/49): loss=0.4113586888209127\n",
      "Gradient Descent(11/49): loss=0.4088164243726382\n",
      "Gradient Descent(12/49): loss=0.40655501178202275\n",
      "Gradient Descent(13/49): loss=0.4045372309513642\n",
      "Gradient Descent(14/49): loss=0.40273182578627326\n",
      "Gradient Descent(15/49): loss=0.4011122300544991\n",
      "Gradient Descent(16/49): loss=0.3996556646928846\n",
      "Gradient Descent(17/49): loss=0.39834247083107716\n",
      "Gradient Descent(18/49): loss=0.3971555998403052\n",
      "Gradient Descent(19/49): loss=0.3960802123062281\n",
      "Gradient Descent(20/49): loss=0.39510335528590496\n",
      "Gradient Descent(21/49): loss=0.39421369764755776\n",
      "Gradient Descent(22/49): loss=0.3934013097545621\n",
      "Gradient Descent(23/49): loss=0.3926574778714746\n",
      "Gradient Descent(24/49): loss=0.391974546358482\n",
      "Gradient Descent(25/49): loss=0.39134578251927155\n",
      "Gradient Descent(26/49): loss=0.39076526019974117\n",
      "Gradient Descent(27/49): loss=0.3902277591002423\n",
      "Gradient Descent(28/49): loss=0.38972867738675687\n",
      "Gradient Descent(29/49): loss=0.3892639556457398\n",
      "Gradient Descent(30/49): loss=0.38883001057438826\n",
      "Gradient Descent(31/49): loss=0.38842367706632225\n",
      "Gradient Descent(32/49): loss=0.3880421575643055\n",
      "Gradient Descent(33/49): loss=0.38768297772177945\n",
      "Gradient Descent(34/49): loss=0.3873439475539619\n",
      "Gradient Descent(35/49): loss=0.3870231273743465\n",
      "Gradient Descent(36/49): loss=0.3867187979088085\n",
      "Gradient Descent(37/49): loss=0.38642943406097374\n",
      "Gradient Descent(38/49): loss=0.38615368187185467\n",
      "Gradient Descent(39/49): loss=0.3858903382761516\n",
      "Gradient Descent(40/49): loss=0.3856383333087319\n",
      "Gradient Descent(41/49): loss=0.3853967144589382\n",
      "Gradient Descent(42/49): loss=0.38516463290862524\n",
      "Gradient Descent(43/49): loss=0.3849413314230184\n",
      "Gradient Descent(44/49): loss=0.3847261336923751\n",
      "Gradient Descent(45/49): loss=0.38451843494758764\n",
      "Gradient Descent(46/49): loss=0.3843176936948102\n",
      "Gradient Descent(47/49): loss=0.3841234244333527\n",
      "Gradient Descent(48/49): loss=0.38393519123781905\n",
      "Gradient Descent(49/49): loss=0.38375260210010903\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4660344172735586\n",
      "Gradient Descent(2/49): loss=0.45025077252698453\n",
      "Gradient Descent(3/49): loss=0.4404480089591826\n",
      "Gradient Descent(4/49): loss=0.4331755723407041\n",
      "Gradient Descent(5/49): loss=0.4273142306458453\n",
      "Gradient Descent(6/49): loss=0.42241327383023625\n",
      "Gradient Descent(7/49): loss=0.4182405879310299\n",
      "Gradient Descent(8/49): loss=0.4146506440043766\n",
      "Gradient Descent(9/49): loss=0.41154029159033945\n",
      "Gradient Descent(10/49): loss=0.4088311158027189\n",
      "Gradient Descent(11/49): loss=0.40646100321144474\n",
      "Gradient Descent(12/49): loss=0.40437946753515025\n",
      "Gradient Descent(13/49): loss=0.40254476664856026\n",
      "Gradient Descent(14/49): loss=0.4009219824128019\n",
      "Gradient Descent(15/49): loss=0.39948166531259727\n",
      "Gradient Descent(16/49): loss=0.3981988330482052\n",
      "Gradient Descent(17/49): loss=0.397052203349322\n",
      "Gradient Descent(18/49): loss=0.396023589317632\n",
      "Gradient Descent(19/49): loss=0.3950974123719917\n",
      "Gradient Descent(20/49): loss=0.394260303423169\n",
      "Gradient Descent(21/49): loss=0.393500772274423\n",
      "Gradient Descent(22/49): loss=0.39280893108671383\n",
      "Gradient Descent(23/49): loss=0.39217626151936025\n",
      "Gradient Descent(24/49): loss=0.39159541767985817\n",
      "Gradient Descent(25/49): loss=0.3910600587647049\n",
      "Gradient Descent(26/49): loss=0.3905647065266161\n",
      "Gradient Descent(27/49): loss=0.3901046236315777\n",
      "Gradient Descent(28/49): loss=0.3896757096761914\n",
      "Gradient Descent(29/49): loss=0.38927441218777287\n",
      "Gradient Descent(30/49): loss=0.38889765036953267\n",
      "Gradient Descent(31/49): loss=0.3885427497094757\n",
      "Gradient Descent(32/49): loss=0.3882073858640424\n",
      "Gradient Descent(33/49): loss=0.38788953646985613\n",
      "Gradient Descent(34/49): loss=0.3875874397393496\n",
      "Gradient Descent(35/49): loss=0.38729955886607786\n",
      "Gradient Descent(36/49): loss=0.3870245514090027\n",
      "Gradient Descent(37/49): loss=0.3867612429464985\n",
      "Gradient Descent(38/49): loss=0.3865086043939265\n",
      "Gradient Descent(39/49): loss=0.38626573246630347\n",
      "Gradient Descent(40/49): loss=0.3860318328422691\n",
      "Gradient Descent(41/49): loss=0.38580620564924245\n",
      "Gradient Descent(42/49): loss=0.3855882329440223\n",
      "Gradient Descent(43/49): loss=0.3853773679095353\n",
      "Gradient Descent(44/49): loss=0.385173125528143\n",
      "Gradient Descent(45/49): loss=0.38497507452589075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=0.3847828304111626\n",
      "Gradient Descent(47/49): loss=0.3845960494561041\n",
      "Gradient Descent(48/49): loss=0.3844144234905145\n",
      "Gradient Descent(49/49): loss=0.38423767539620135\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4656337505633766\n",
      "Gradient Descent(2/49): loss=0.44962090565764096\n",
      "Gradient Descent(3/49): loss=0.43960425686151605\n",
      "Gradient Descent(4/49): loss=0.43213796275706645\n",
      "Gradient Descent(5/49): loss=0.4261102555305721\n",
      "Gradient Descent(6/49): loss=0.42107022193514937\n",
      "Gradient Descent(7/49): loss=0.41678232989922065\n",
      "Gradient Descent(8/49): loss=0.41309712452784814\n",
      "Gradient Descent(9/49): loss=0.40990795388623347\n",
      "Gradient Descent(10/49): loss=0.40713350908286977\n",
      "Gradient Descent(11/49): loss=0.40470933647132357\n",
      "Gradient Descent(12/49): loss=0.40258306039150754\n",
      "Gradient Descent(13/49): loss=0.40071140433536884\n",
      "Gradient Descent(14/49): loss=0.3990581940028588\n",
      "Gradient Descent(15/49): loss=0.39759294321341715\n",
      "Gradient Descent(16/49): loss=0.39628980803235037\n",
      "Gradient Descent(17/49): loss=0.3951267857511647\n",
      "Gradient Descent(18/49): loss=0.3940850841887943\n",
      "Gradient Descent(19/49): loss=0.3931486143093324\n",
      "Gradient Descent(20/49): loss=0.3923035752978173\n",
      "Gradient Descent(21/49): loss=0.39153811103419595\n",
      "Gradient Descent(22/49): loss=0.390842023049877\n",
      "Gradient Descent(23/49): loss=0.3902065290333886\n",
      "Gradient Descent(24/49): loss=0.3896240586206914\n",
      "Gradient Descent(25/49): loss=0.3890880800560538\n",
      "Gradient Descent(26/49): loss=0.3885929526350745\n",
      "Gradient Descent(27/49): loss=0.38813380082104815\n",
      "Gradient Descent(28/49): loss=0.3877064066702577\n",
      "Gradient Descent(29/49): loss=0.38730711778133453\n",
      "Gradient Descent(30/49): loss=0.38693276844436997\n",
      "Gradient Descent(31/49): loss=0.3865806120375748\n",
      "Gradient Descent(32/49): loss=0.386248263023967\n",
      "Gradient Descent(33/49): loss=0.3859336471526467\n",
      "Gradient Descent(34/49): loss=0.3856349586794534\n",
      "Gradient Descent(35/49): loss=0.385350623598218\n",
      "Gradient Descent(36/49): loss=0.38507926802256465\n",
      "Gradient Descent(37/49): loss=0.38481969098405855\n",
      "Gradient Descent(38/49): loss=0.38457084101928085\n",
      "Gradient Descent(39/49): loss=0.38433179600918266\n",
      "Gradient Descent(40/49): loss=0.3841017458113937\n",
      "Gradient Descent(41/49): loss=0.38387997729207834\n",
      "Gradient Descent(42/49): loss=0.3836658614202178\n",
      "Gradient Descent(43/49): loss=0.3834588421352747\n",
      "Gradient Descent(44/49): loss=0.3832584267403068\n",
      "Gradient Descent(45/49): loss=0.3830641776077707\n",
      "Gradient Descent(46/49): loss=0.3828757050153569\n",
      "Gradient Descent(47/49): loss=0.3826926609549907\n",
      "Gradient Descent(48/49): loss=0.3825147337802162\n",
      "Gradient Descent(49/49): loss=0.38234164357612865\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4659801648627261\n",
      "Gradient Descent(2/49): loss=0.45021477461911963\n",
      "Gradient Descent(3/49): loss=0.4404073093426169\n",
      "Gradient Descent(4/49): loss=0.4331212506859385\n",
      "Gradient Descent(5/49): loss=0.4272464597616901\n",
      "Gradient Descent(6/49): loss=0.42233414401123376\n",
      "Gradient Descent(7/49): loss=0.4181518022160321\n",
      "Gradient Descent(8/49): loss=0.4145532701625206\n",
      "Gradient Descent(9/49): loss=0.41143499432049474\n",
      "Gradient Descent(10/49): loss=0.4087183827612235\n",
      "Gradient Descent(11/49): loss=0.40634128079714743\n",
      "Gradient Descent(12/49): loss=0.4042532246784261\n",
      "Gradient Descent(13/49): loss=0.4024125158784066\n",
      "Gradient Descent(14/49): loss=0.4007842791132555\n",
      "Gradient Descent(15/49): loss=0.39933909742403906\n",
      "Gradient Descent(16/49): loss=0.3980520077359602\n",
      "Gradient Descent(17/49): loss=0.3969017338543697\n",
      "Gradient Descent(18/49): loss=0.39587008341404145\n",
      "Gradient Descent(19/49): loss=0.39494146292568444\n",
      "Gradient Descent(20/49): loss=0.39410248108386964\n",
      "Gradient Descent(21/49): loss=0.393341620118388\n",
      "Gradient Descent(22/49): loss=0.3926489609435742\n",
      "Gradient Descent(23/49): loss=0.39201595169866127\n",
      "Gradient Descent(24/49): loss=0.39143521182761726\n",
      "Gradient Descent(25/49): loss=0.39090036560939234\n",
      "Gradient Descent(26/49): loss=0.3904059003077719\n",
      "Gradient Descent(27/49): loss=0.3899470450378769\n",
      "Gradient Descent(28/49): loss=0.38951966715073755\n",
      "Gradient Descent(29/49): loss=0.38912018348575805\n",
      "Gradient Descent(30/49): loss=0.38874548427688055\n",
      "Gradient Descent(31/49): loss=0.3883928678508447\n",
      "Gradient Descent(32/49): loss=0.3880599845449372\n",
      "Gradient Descent(33/49): loss=0.38774478851099353\n",
      "Gradient Descent(34/49): loss=0.3874454962722603\n",
      "Gradient Descent(35/49): loss=0.38716055106761343\n",
      "Gradient Descent(36/49): loss=0.3868885921592997\n",
      "Gradient Descent(37/49): loss=0.38662842840035955\n",
      "Gradient Descent(38/49): loss=0.3863790154597735\n",
      "Gradient Descent(39/49): loss=0.38613943619007\n",
      "Gradient Descent(40/49): loss=0.3859088836960272\n",
      "Gradient Descent(41/49): loss=0.38568664672615055\n",
      "Gradient Descent(42/49): loss=0.38547209706247687\n",
      "Gradient Descent(43/49): loss=0.38526467863030733\n",
      "Gradient Descent(44/49): loss=0.3850638980888734\n",
      "Gradient Descent(45/49): loss=0.3848693166976747\n",
      "Gradient Descent(46/49): loss=0.3846805432821229\n",
      "Gradient Descent(47/49): loss=0.38449722814689413\n",
      "Gradient Descent(48/49): loss=0.3843190578066235\n",
      "Gradient Descent(49/49): loss=0.38414575042179694\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4655341849596037\n",
      "Gradient Descent(2/49): loss=0.4494755368618656\n",
      "Gradient Descent(3/49): loss=0.4394641189746057\n",
      "Gradient Descent(4/49): loss=0.4320238683578328\n",
      "Gradient Descent(5/49): loss=0.42602675060437284\n",
      "Gradient Descent(6/49): loss=0.42101533246977624\n",
      "Gradient Descent(7/49): loss=0.41675202949930007\n",
      "Gradient Descent(8/49): loss=0.4130871285651265\n",
      "Gradient Descent(9/49): loss=0.4099143680595669\n",
      "Gradient Descent(10/49): loss=0.4071530134927355\n",
      "Gradient Descent(11/49): loss=0.4047391886834796\n",
      "Gradient Descent(12/49): loss=0.4026210340969153\n",
      "Gradient Descent(13/49): loss=0.4007557113915186\n",
      "Gradient Descent(14/49): loss=0.39910740882528895\n",
      "Gradient Descent(15/49): loss=0.39764593621508304\n",
      "Gradient Descent(16/49): loss=0.3963456896029156\n",
      "Gradient Descent(17/49): loss=0.39518486016820625\n",
      "Gradient Descent(18/49): loss=0.3941448121321444\n",
      "Gradient Descent(19/49): loss=0.3932095825288437\n",
      "Gradient Descent(20/49): loss=0.392365472105352\n",
      "Gradient Descent(21/49): loss=0.39160070648895856\n",
      "Gradient Descent(22/49): loss=0.3909051529106228\n",
      "Gradient Descent(23/49): loss=0.3902700817333466\n",
      "Gradient Descent(24/49): loss=0.38968796467315847\n",
      "Gradient Descent(25/49): loss=0.38915230342116525\n",
      "Gradient Descent(26/49): loss=0.3886574836751633\n",
      "Gradient Descent(27/49): loss=0.3881986505479419\n",
      "Gradient Descent(28/49): loss=0.3877716020471978\n",
      "Gradient Descent(29/49): loss=0.38737269788863277\n",
      "Gradient Descent(30/49): loss=0.38699878135439775\n",
      "Gradient Descent(31/49): loss=0.38664711227351595\n",
      "Gradient Descent(32/49): loss=0.3863153094997104\n",
      "Gradient Descent(33/49): loss=0.38600130150958817\n",
      "Gradient Descent(34/49): loss=0.38570328395082404\n",
      "Gradient Descent(35/49): loss=0.3854196831436167\n",
      "Gradient Descent(36/49): loss=0.38514912468523194\n",
      "Gradient Descent(37/49): loss=0.38489040643152905\n",
      "Gradient Descent(38/49): loss=0.38464247523472894\n",
      "Gradient Descent(39/49): loss=0.3844044069063114\n",
      "Gradient Descent(40/49): loss=0.38417538895030334\n",
      "Gradient Descent(41/49): loss=0.3839547056773772\n",
      "Gradient Descent(42/49): loss=0.38374172536581647\n",
      "Gradient Descent(43/49): loss=0.38353588918296205\n",
      "Gradient Descent(44/49): loss=0.3833367016214225\n",
      "Gradient Descent(45/49): loss=0.3831437222391391\n",
      "Gradient Descent(46/49): loss=0.3829565585221929\n",
      "Gradient Descent(47/49): loss=0.3827748597147755\n",
      "Gradient Descent(48/49): loss=0.3825983114826203\n",
      "Gradient Descent(49/49): loss=0.3824266312949511\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.462424462341326\n",
      "Gradient Descent(2/49): loss=0.44667061125378993\n",
      "Gradient Descent(3/49): loss=0.43686801367216266\n",
      "Gradient Descent(4/49): loss=0.42950197545975627\n",
      "Gradient Descent(5/49): loss=0.4235664273228076\n",
      "Gradient Descent(6/49): loss=0.41864665932742035\n",
      "Gradient Descent(7/49): loss=0.41450962970169175\n",
      "Gradient Descent(8/49): loss=0.4109989340005608\n",
      "Gradient Descent(9/49): loss=0.4079996887677359\n",
      "Gradient Descent(10/49): loss=0.4054232462855989\n",
      "Gradient Descent(11/49): loss=0.403199142927253\n",
      "Gradient Descent(12/49): loss=0.4012702985337323\n",
      "Gradient Descent(13/49): loss=0.39958989562928493\n",
      "Gradient Descent(14/49): loss=0.398119212578077\n",
      "Gradient Descent(15/49): loss=0.39682604001229577\n",
      "Gradient Descent(16/49): loss=0.39568347749702715\n",
      "Gradient Descent(17/49): loss=0.39466899243343\n",
      "Gradient Descent(18/49): loss=0.39376366866369206\n",
      "Gradient Descent(19/49): loss=0.39295159769684407\n",
      "Gradient Descent(20/49): loss=0.3922193803961606\n",
      "Gradient Descent(21/49): loss=0.3915557161377126\n",
      "Gradient Descent(22/49): loss=0.39095106236008864\n",
      "Gradient Descent(23/49): loss=0.3903973514183771\n",
      "Gradient Descent(24/49): loss=0.38988775447356266\n",
      "Gradient Descent(25/49): loss=0.38941648421516956\n",
      "Gradient Descent(26/49): loss=0.38897862977997366\n",
      "Gradient Descent(27/49): loss=0.3885700184453105\n",
      "Gradient Descent(28/49): loss=0.38818709963856385\n",
      "Gradient Descent(29/49): loss=0.38782684757857205\n",
      "Gradient Descent(30/49): loss=0.38748667949368953\n",
      "Gradient Descent(31/49): loss=0.3871643868762921\n",
      "Gradient Descent(32/49): loss=0.386858077657637\n",
      "Gradient Descent(33/49): loss=0.38656612753765585\n",
      "Gradient Descent(34/49): loss=0.38628713899506284\n",
      "Gradient Descent(35/49): loss=0.3860199067448516\n",
      "Gradient Descent(36/49): loss=0.38576338861147635\n",
      "Gradient Descent(37/49): loss=0.38551668095375063\n",
      "Gradient Descent(38/49): loss=0.3852789979174902\n",
      "Gradient Descent(39/49): loss=0.38504965390885054\n",
      "Gradient Descent(40/49): loss=0.3848280487790616\n",
      "Gradient Descent(41/49): loss=0.3846136552930213\n",
      "Gradient Descent(42/49): loss=0.384406008522662\n",
      "Gradient Descent(43/49): loss=0.3842046968633218\n",
      "Gradient Descent(44/49): loss=0.38400935441939843\n",
      "Gradient Descent(45/49): loss=0.38381965454584377\n",
      "Gradient Descent(46/49): loss=0.38363530436585475\n",
      "Gradient Descent(47/49): loss=0.3834560401134805\n",
      "Gradient Descent(48/49): loss=0.383281623173693\n",
      "Gradient Descent(49/49): loss=0.38311183671248433\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46198367288519904\n",
      "Gradient Descent(2/49): loss=0.445977858780078\n",
      "Gradient Descent(3/49): loss=0.43593516417046135\n",
      "Gradient Descent(4/49): loss=0.4283610849725085\n",
      "Gradient Descent(5/49): loss=0.4222551978754906\n",
      "Gradient Descent(6/49): loss=0.41719834219877644\n",
      "Gradient Descent(7/49): loss=0.4129512507047256\n",
      "Gradient Descent(8/49): loss=0.40935206091676707\n",
      "Gradient Descent(9/49): loss=0.4062815614705703\n",
      "Gradient Descent(10/49): loss=0.4036477397359355\n",
      "Gradient Descent(11/49): loss=0.4013775041588477\n",
      "Gradient Descent(12/49): loss=0.3994116993365605\n",
      "Gradient Descent(13/49): loss=0.3977018497527897\n",
      "Gradient Descent(14/49): loss=0.39620789541230605\n",
      "Gradient Descent(15/49): loss=0.3948965376111645\n",
      "Gradient Descent(16/49): loss=0.3937399835407798\n",
      "Gradient Descent(17/49): loss=0.39271496605375406\n",
      "Gradient Descent(18/49): loss=0.39180196225007613\n",
      "Gradient Descent(19/49): loss=0.3909845612518165\n",
      "Gradient Descent(20/49): loss=0.3902489472781119\n",
      "Gradient Descent(21/49): loss=0.3895834738377664\n",
      "Gradient Descent(22/49): loss=0.3889783111196729\n",
      "Gradient Descent(23/49): loss=0.3884251528884964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=0.3879169721693087\n",
      "Gradient Descent(25/49): loss=0.38744781718057447\n",
      "Gradient Descent(26/49): loss=0.38701264061677926\n",
      "Gradient Descent(27/49): loss=0.386607156653295\n",
      "Gradient Descent(28/49): loss=0.38622772105035574\n",
      "Gradient Descent(29/49): loss=0.385871230538463\n",
      "Gradient Descent(30/49): loss=0.38553503832083014\n",
      "Gradient Descent(31/49): loss=0.38521688306272966\n",
      "Gradient Descent(32/49): loss=0.38491482917715697\n",
      "Gradient Descent(33/49): loss=0.3846272165794296\n",
      "Gradient Descent(34/49): loss=0.3843526183844374\n",
      "Gradient Descent(35/49): loss=0.3840898052704631\n",
      "Gradient Descent(36/49): loss=0.38383771544177453\n",
      "Gradient Descent(37/49): loss=0.3835954292958295\n",
      "Gradient Descent(38/49): loss=0.3833621480458428\n",
      "Gradient Descent(39/49): loss=0.3831371756705216\n",
      "Gradient Descent(40/49): loss=0.3829199036639695\n",
      "Gradient Descent(41/49): loss=0.3827097981434241\n",
      "Gradient Descent(42/49): loss=0.38250638894336736\n",
      "Gradient Descent(43/49): loss=0.38230926038389823\n",
      "Gradient Descent(44/49): loss=0.3821180434510151\n",
      "Gradient Descent(45/49): loss=0.3819324091681571\n",
      "Gradient Descent(46/49): loss=0.38175206297335096\n",
      "Gradient Descent(47/49): loss=0.3815767399456743\n",
      "Gradient Descent(48/49): loss=0.3814062007494114\n",
      "Gradient Descent(49/49): loss=0.3812402281849916\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4623726337558733\n",
      "Gradient Descent(2/49): loss=0.44663734142162526\n",
      "Gradient Descent(3/49): loss=0.43682234249373897\n",
      "Gradient Descent(4/49): loss=0.4294393829600147\n",
      "Gradient Descent(5/49): loss=0.42348990454965657\n",
      "Gradient Descent(6/49): loss=0.418558873871426\n",
      "Gradient Descent(7/49): loss=0.4144120636626351\n",
      "Gradient Descent(8/49): loss=0.4108923962782781\n",
      "Gradient Descent(9/49): loss=0.4078847544914924\n",
      "Gradient Descent(10/49): loss=0.40530047628847293\n",
      "Gradient Descent(11/49): loss=0.40306916318121416\n",
      "Gradient Descent(12/49): loss=0.40113381152036204\n",
      "Gradient Descent(13/49): loss=0.39944766398504644\n",
      "Gradient Descent(14/49): loss=0.3979720344957192\n",
      "Gradient Descent(15/49): loss=0.3966747249661455\n",
      "Gradient Descent(16/49): loss=0.39552882560244856\n",
      "Gradient Descent(17/49): loss=0.39451177833197926\n",
      "Gradient Descent(18/49): loss=0.39360462977107896\n",
      "Gradient Descent(19/49): loss=0.3927914262408096\n",
      "Gradient Descent(20/49): loss=0.3920587185607516\n",
      "Gradient Descent(21/49): loss=0.39139515365371513\n",
      "Gradient Descent(22/49): loss=0.39079113595891424\n",
      "Gradient Descent(23/49): loss=0.3902385456602702\n",
      "Gradient Descent(24/49): loss=0.38973050355282224\n",
      "Gradient Descent(25/49): loss=0.38926117442748603\n",
      "Gradient Descent(26/49): loss=0.38882560240739117\n",
      "Gradient Descent(27/49): loss=0.3884195728726203\n",
      "Gradient Descent(28/49): loss=0.38803949656213665\n",
      "Gradient Descent(29/49): loss=0.3876823122061991\n",
      "Gradient Descent(30/49): loss=0.3873454046634626\n",
      "Gradient Descent(31/49): loss=0.38702653604535475\n",
      "Gradient Descent(32/49): loss=0.3867237877290592\n",
      "Gradient Descent(33/49): loss=0.3864355115068097\n",
      "Gradient Descent(34/49): loss=0.3861602884066192\n",
      "Gradient Descent(35/49): loss=0.3858968939586101\n",
      "Gradient Descent(36/49): loss=0.38564426888028874\n",
      "Gradient Descent(37/49): loss=0.3854014943202727\n",
      "Gradient Descent(38/49): loss=0.3851677709387818\n",
      "Gradient Descent(39/49): loss=0.3849424012192331\n",
      "Gradient Descent(40/49): loss=0.3847247745023651\n",
      "Gradient Descent(41/49): loss=0.3845143543155987\n",
      "Gradient Descent(42/49): loss=0.3843106676384431\n",
      "Gradient Descent(43/49): loss=0.38411329580183584\n",
      "Gradient Descent(44/49): loss=0.38392186676719275\n",
      "Gradient Descent(45/49): loss=0.3837360485711257\n",
      "Gradient Descent(46/49): loss=0.38355554375552264\n",
      "Gradient Descent(47/49): loss=0.3833800846310346\n",
      "Gradient Descent(48/49): loss=0.38320942924583495\n",
      "Gradient Descent(49/49): loss=0.3830433579515533\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46187021532832084\n",
      "Gradient Descent(2/49): loss=0.4458263685092284\n",
      "Gradient Descent(3/49): loss=0.43580316651277373\n",
      "Gradient Descent(4/49): loss=0.42826442734184633\n",
      "Gradient Descent(5/49): loss=0.422193279448212\n",
      "Gradient Descent(6/49): loss=0.417165891769207\n",
      "Gradient Descent(7/49): loss=0.4129424123656\n",
      "Gradient Descent(8/49): loss=0.40936163252119684\n",
      "Gradient Descent(9/49): loss=0.4063052649675642\n",
      "Gradient Descent(10/49): loss=0.40368217649119836\n",
      "Gradient Descent(11/49): loss=0.40142002218666994\n",
      "Gradient Descent(12/49): loss=0.3994602514339033\n",
      "Gradient Descent(13/49): loss=0.39775486807491295\n",
      "Gradient Descent(14/49): loss=0.39626418826998316\n",
      "Gradient Descent(15/49): loss=0.3949552072378264\n",
      "Gradient Descent(16/49): loss=0.3938003614991268\n",
      "Gradient Descent(17/49): loss=0.3927765627590391\n",
      "Gradient Descent(18/49): loss=0.39186442753601336\n",
      "Gradient Descent(19/49): loss=0.39104765349597864\n",
      "Gradient Descent(20/49): loss=0.3903125091518311\n",
      "Gradient Descent(21/49): loss=0.3896474131968533\n",
      "Gradient Descent(22/49): loss=0.3890425859041087\n",
      "Gradient Descent(23/49): loss=0.3884897591664099\n",
      "Gradient Descent(24/49): loss=0.38798193466115866\n",
      "Gradient Descent(25/49): loss=0.3875131817497979\n",
      "Gradient Descent(26/49): loss=0.3870784683262784\n",
      "Gradient Descent(27/49): loss=0.38667351907289\n",
      "Gradient Descent(28/49): loss=0.3862946965659481\n",
      "Gradient Descent(29/49): loss=0.38593890146439136\n",
      "Gradient Descent(30/49): loss=0.3856034886565144\n",
      "Gradient Descent(31/49): loss=0.38528619676591647\n",
      "Gradient Descent(32/49): loss=0.3849850888508701\n",
      "Gradient Descent(33/49): loss=0.3846985024895494\n",
      "Gradient Descent(34/49): loss=0.38442500774076876\n",
      "Gradient Descent(35/49): loss=0.38416337171700227\n",
      "Gradient Descent(36/49): loss=0.3839125287122985\n",
      "Gradient Descent(37/49): loss=0.3836715549993685\n",
      "Gradient Descent(38/49): loss=0.3834396475534596\n",
      "Gradient Descent(39/49): loss=0.38321610608039003\n",
      "Gradient Descent(40/49): loss=0.383000317826281\n",
      "Gradient Descent(41/49): loss=0.38279174473033417\n",
      "Gradient Descent(42/49): loss=0.3825899125521804\n",
      "Gradient Descent(43/49): loss=0.3823944016641247\n",
      "Gradient Descent(44/49): loss=0.38220483924789395\n",
      "Gradient Descent(45/49): loss=0.3820208926768301\n",
      "Gradient Descent(46/49): loss=0.3818422638991596\n",
      "Gradient Descent(47/49): loss=0.38166868466708037\n",
      "Gradient Descent(48/49): loss=0.38149991248087284\n",
      "Gradient Descent(49/49): loss=0.381335727137788\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4591250752019797\n",
      "Gradient Descent(2/49): loss=0.44354442479187506\n",
      "Gradient Descent(3/49): loss=0.4336855882590747\n",
      "Gradient Descent(4/49): loss=0.4262147298072996\n",
      "Gradient Descent(5/49): loss=0.42023348177748954\n",
      "Gradient Descent(6/49): loss=0.41533597738097866\n",
      "Gradient Descent(7/49): loss=0.4112747114406275\n",
      "Gradient Descent(8/49): loss=0.40787720838267527\n",
      "Gradient Descent(9/49): loss=0.40501507510640666\n",
      "Gradient Descent(10/49): loss=0.4025890651943319\n",
      "Gradient Descent(11/49): loss=0.40052068605248653\n",
      "Gradient Descent(12/49): loss=0.39874696624839906\n",
      "Gradient Descent(13/49): loss=0.39721692394907354\n",
      "Gradient Descent(14/49): loss=0.3958890360685189\n",
      "Gradient Descent(15/49): loss=0.3947293442562941\n",
      "Gradient Descent(16/49): loss=0.3937099943105059\n",
      "Gradient Descent(17/49): loss=0.3928080868169707\n",
      "Gradient Descent(18/49): loss=0.3920047604324896\n",
      "Gradient Descent(19/49): loss=0.3912844541385559\n",
      "Gradient Descent(20/49): loss=0.3906343099399322\n",
      "Gradient Descent(21/49): loss=0.390043687276178\n",
      "Gradient Descent(22/49): loss=0.38950376710807666\n",
      "Gradient Descent(23/49): loss=0.3890072284363059\n",
      "Gradient Descent(24/49): loss=0.3885479835751727\n",
      "Gradient Descent(25/49): loss=0.38812096123007195\n",
      "Gradient Descent(26/49): loss=0.3877219285534217\n",
      "Gradient Descent(27/49): loss=0.38734734503555474\n",
      "Gradient Descent(28/49): loss=0.38699424243020997\n",
      "Gradient Descent(29/49): loss=0.3866601259942143\n",
      "Gradient Descent(30/49): loss=0.3863428931932612\n",
      "Gradient Descent(31/49): loss=0.3860407667325998\n",
      "Gradient Descent(32/49): loss=0.3857522393456435\n",
      "Gradient Descent(33/49): loss=0.3854760282407605\n",
      "Gradient Descent(34/49): loss=0.3852110374872274\n",
      "Gradient Descent(35/49): loss=0.38495632693191606\n",
      "Gradient Descent(36/49): loss=0.3847110864918943\n",
      "Gradient Descent(37/49): loss=0.38447461487538753\n",
      "Gradient Descent(38/49): loss=0.3842463019530773\n",
      "Gradient Descent(39/49): loss=0.38402561414047076\n",
      "Gradient Descent(40/49): loss=0.383812082265743\n",
      "Gradient Descent(41/49): loss=0.3836052914906151\n",
      "Gradient Descent(42/49): loss=0.38340487292825387\n",
      "Gradient Descent(43/49): loss=0.38321049666490015\n",
      "Gradient Descent(44/49): loss=0.38302186594345156\n",
      "Gradient Descent(45/49): loss=0.3828387123095626\n",
      "Gradient Descent(46/49): loss=0.3826607915556474\n",
      "Gradient Descent(47/49): loss=0.38248788032681963\n",
      "Gradient Descent(48/49): loss=0.38231977327640376\n",
      "Gradient Descent(49/49): loss=0.3821562806780874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4586484418387851\n",
      "Gradient Descent(2/49): loss=0.44278919808385625\n",
      "Gradient Descent(3/49): loss=0.43266637237099376\n",
      "Gradient Descent(4/49): loss=0.42497882051108443\n",
      "Gradient Descent(5/49): loss=0.4188276943669743\n",
      "Gradient Descent(6/49): loss=0.4137978952841824\n",
      "Gradient Descent(7/49): loss=0.4096333608325927\n",
      "Gradient Descent(8/49): loss=0.40615504745735215\n",
      "Gradient Descent(9/49): loss=0.40322965475694633\n",
      "Gradient Descent(10/49): loss=0.40075423580734604\n",
      "Gradient Descent(11/49): loss=0.3986474615255549\n",
      "Gradient Descent(12/49): loss=0.39684415228645603\n",
      "Gradient Descent(13/49): loss=0.395291585331812\n",
      "Gradient Descent(14/49): loss=0.39394685182593475\n",
      "Gradient Descent(15/49): loss=0.39277488271283856\n",
      "Gradient Descent(16/49): loss=0.39174692924952287\n",
      "Gradient Descent(17/49): loss=0.39083936928240653\n",
      "Gradient Descent(18/49): loss=0.3900327563896161\n",
      "Gradient Descent(19/49): loss=0.3893110554116642\n",
      "Gradient Descent(20/49): loss=0.38866102396134444\n",
      "Gradient Descent(21/49): loss=0.3880717098748087\n",
      "Gradient Descent(22/49): loss=0.38753404163065985\n",
      "Gradient Descent(23/49): loss=0.38704049380447936\n",
      "Gradient Descent(24/49): loss=0.38658481335898043\n",
      "Gradient Descent(25/49): loss=0.3861617954138388\n",
      "Gradient Descent(26/49): loss=0.38576709935132847\n",
      "Gradient Descent(27/49): loss=0.3853970978601058\n",
      "Gradient Descent(28/49): loss=0.3850487529122262\n",
      "Gradient Descent(29/49): loss=0.38471951378728264\n",
      "Gradient Descent(30/49): loss=0.3844072331607779\n",
      "Gradient Descent(31/49): loss=0.3841100980055955\n",
      "Gradient Descent(32/49): loss=0.3838265726497692\n",
      "Gradient Descent(33/49): loss=0.38355535181737616\n",
      "Gradient Descent(34/49): loss=0.3832953218734855\n",
      "Gradient Descent(35/49): loss=0.38304552881562587\n",
      "Gradient Descent(36/49): loss=0.38280515181681635\n",
      "Gradient Descent(37/49): loss=0.38257348133981267\n",
      "Gradient Descent(38/49): loss=0.3823499010177735\n",
      "Gradient Descent(39/49): loss=0.38213387264023946\n",
      "Gradient Descent(40/49): loss=0.38192492370101905\n",
      "Gradient Descent(41/49): loss=0.3817226370610461\n",
      "Gradient Descent(42/49): loss=0.3815266423583905\n",
      "Gradient Descent(43/49): loss=0.3813366088625401\n",
      "Gradient Descent(44/49): loss=0.38115223952338245\n",
      "Gradient Descent(45/49): loss=0.38097326600913206\n",
      "Gradient Descent(46/49): loss=0.38079944456346004\n",
      "Gradient Descent(47/49): loss=0.3806305525417154\n",
      "Gradient Descent(48/49): loss=0.3804663855105128\n",
      "Gradient Descent(49/49): loss=0.3803067548150481\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45907821398435267\n",
      "Gradient Descent(2/49): loss=0.4435117538959465\n",
      "Gradient Descent(3/49): loss=0.43363331895508994\n",
      "Gradient Descent(4/49): loss=0.4261442308465016\n",
      "Gradient Descent(5/49): loss=0.420149327396394\n",
      "Gradient Descent(6/49): loss=0.4152405645505774\n",
      "Gradient Descent(7/49): loss=0.41116912333701766\n",
      "Gradient Descent(8/49): loss=0.4077621241573465\n",
      "Gradient Descent(9/49): loss=0.40489117290667387\n",
      "Gradient Descent(10/49): loss=0.40245714041770986\n",
      "Gradient Descent(11/49): loss=0.40038165240561785\n",
      "Gradient Descent(12/49): loss=0.3986018183721512\n",
      "Gradient Descent(13/49): loss=0.397066693754892\n",
      "Gradient Descent(14/49): loss=0.3957347544804245\n",
      "Gradient Descent(15/49): loss=0.39457201161012484\n",
      "Gradient Descent(16/49): loss=0.39355055934530014\n",
      "Gradient Descent(17/49): loss=0.3926474330282171\n",
      "Gradient Descent(18/49): loss=0.39184369828510346\n",
      "Gradient Descent(19/49): loss=0.3911237177219746\n",
      "Gradient Descent(20/49): loss=0.3904745568565676\n",
      "Gradient Descent(21/49): loss=0.38988550078751294\n",
      "Gradient Descent(22/49): loss=0.3893476597786799\n",
      "Gradient Descent(23/49): loss=0.3888536467003937\n",
      "Gradient Descent(24/49): loss=0.38839731280052986\n",
      "Gradient Descent(25/49): loss=0.3879735309728263\n",
      "Gradient Descent(26/49): loss=0.38757801778886025\n",
      "Gradient Descent(27/49): loss=0.3872071872197113\n",
      "Gradient Descent(28/49): loss=0.3868580302988383\n",
      "Gradient Descent(29/49): loss=0.38652801604382886\n",
      "Gradient Descent(30/49): loss=0.3862150098163624\n",
      "Gradient Descent(31/49): loss=0.38591720599855517\n",
      "Gradient Descent(32/49): loss=0.38563307243199574\n",
      "Gradient Descent(33/49): loss=0.38536130452852085\n",
      "Gradient Descent(34/49): loss=0.3851007873392097\n",
      "Gradient Descent(35/49): loss=0.38485056417627905\n",
      "Gradient Descent(36/49): loss=0.38460981063447625\n",
      "Gradient Descent(37/49): loss=0.38437781306465774\n",
      "Gradient Descent(38/49): loss=0.38415395072097125\n",
      "Gradient Descent(39/49): loss=0.3839376809413075\n",
      "Gradient Descent(40/49): loss=0.38372852683403635\n",
      "Gradient Descent(41/49): loss=0.38352606703704617\n",
      "Gradient Descent(42/49): loss=0.3833299271914559\n",
      "Gradient Descent(43/49): loss=0.38313977283510503\n",
      "Gradient Descent(44/49): loss=0.3829553034724926\n",
      "Gradient Descent(45/49): loss=0.38277624762026113\n",
      "Gradient Descent(46/49): loss=0.38260235866223974\n",
      "Gradient Descent(47/49): loss=0.3824334113768236\n",
      "Gradient Descent(48/49): loss=0.3822691990231689\n",
      "Gradient Descent(49/49): loss=0.3821095308922358\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4585211752549459\n",
      "Gradient Descent(2/49): loss=0.44263566401073506\n",
      "Gradient Descent(3/49): loss=0.432546131146119\n",
      "Gradient Descent(4/49): loss=0.42490059146307996\n",
      "Gradient Descent(5/49): loss=0.41878597119943045\n",
      "Gradient Descent(6/49): loss=0.4137848696478661\n",
      "Gradient Descent(7/49): loss=0.4096420137184389\n",
      "Gradient Descent(8/49): loss=0.406179747724088\n",
      "Gradient Descent(9/49): loss=0.40326607941229936\n",
      "Gradient Descent(10/49): loss=0.40079913447777826\n",
      "Gradient Descent(11/49): loss=0.3986984192055748\n",
      "Gradient Descent(12/49): loss=0.39689939090735155\n",
      "Gradient Descent(13/49): loss=0.3953498084909385\n",
      "Gradient Descent(14/49): loss=0.394007126496746\n",
      "Gradient Descent(15/49): loss=0.39283654995200634\n",
      "Gradient Descent(16/49): loss=0.391809536836813\n",
      "Gradient Descent(17/49): loss=0.3909026207367397\n",
      "Gradient Descent(18/49): loss=0.39009647219724125\n",
      "Gradient Descent(19/49): loss=0.38937514340471574\n",
      "Gradient Descent(20/49): loss=0.3887254566040514\n",
      "Gradient Descent(21/49): loss=0.3881365068068186\n",
      "Gradient Descent(22/49): loss=0.38759925624161257\n",
      "Gradient Descent(23/49): loss=0.38710620291936637\n",
      "Gradient Descent(24/49): loss=0.38665110933553465\n",
      "Gradient Descent(25/49): loss=0.3862287801162223\n",
      "Gradient Descent(26/49): loss=0.38583487958596024\n",
      "Gradient Descent(27/49): loss=0.3854657819513575\n",
      "Gradient Descent(28/49): loss=0.3851184481660151\n",
      "Gradient Descent(29/49): loss=0.38479032464490687\n",
      "Gradient Descent(30/49): loss=0.38447925988765697\n",
      "Gradient Descent(31/49): loss=0.38418343579273523\n",
      "Gradient Descent(32/49): loss=0.38390131103187053\n",
      "Gradient Descent(33/49): loss=0.38363157433210415\n",
      "Gradient Descent(34/49): loss=0.3833731059027064\n",
      "Gradient Descent(35/49): loss=0.38312494556231214\n",
      "Gradient Descent(36/49): loss=0.38288626638151907\n",
      "Gradient Descent(37/49): loss=0.38265635286867355\n",
      "Gradient Descent(38/49): loss=0.3824345829004207\n",
      "Gradient Descent(39/49): loss=0.38222041274094326\n",
      "Gradient Descent(40/49): loss=0.3820133646104402\n",
      "Gradient Descent(41/49): loss=0.3818130163590143\n",
      "Gradient Descent(42/49): loss=0.3816189928805846\n",
      "Gradient Descent(43/49): loss=0.3814309589658326\n",
      "Gradient Descent(44/49): loss=0.38124861334608284\n",
      "Gradient Descent(45/49): loss=0.3810716837234959\n",
      "Gradient Descent(46/49): loss=0.3808999226186998\n",
      "Gradient Descent(47/49): loss=0.38073310389640935\n",
      "Gradient Descent(48/49): loss=0.38057101985380576\n",
      "Gradient Descent(49/49): loss=0.38041347877640513\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4561362558555194\n",
      "Gradient Descent(2/49): loss=0.4407718495191318\n",
      "Gradient Descent(3/49): loss=0.4308020480832512\n",
      "Gradient Descent(4/49): loss=0.42323789341064144\n",
      "Gradient Descent(5/49): loss=0.4172474473644171\n",
      "Gradient Descent(6/49): loss=0.4124101893680993\n",
      "Gradient Descent(7/49): loss=0.4084566939045824\n",
      "Gradient Descent(8/49): loss=0.4051962656834749\n",
      "Gradient Descent(9/49): loss=0.4024866256638495\n",
      "Gradient Descent(10/49): loss=0.4002183604962916\n",
      "Gradient Descent(11/49): loss=0.39830582437401335\n",
      "Gradient Descent(12/49): loss=0.3966812555164868\n",
      "Gradient Descent(13/49): loss=0.3952906810632068\n",
      "Gradient Descent(14/49): loss=0.39409091942698693\n",
      "Gradient Descent(15/49): loss=0.39304731277918836\n",
      "Gradient Descent(16/49): loss=0.39213197575907643\n",
      "Gradient Descent(17/49): loss=0.39132242505423487\n",
      "Gradient Descent(18/49): loss=0.3906004980824696\n",
      "Gradient Descent(19/49): loss=0.38995149519280903\n",
      "Gradient Descent(20/49): loss=0.3893634967313631\n",
      "Gradient Descent(21/49): loss=0.388826817951353\n",
      "Gradient Descent(22/49): loss=0.388333573126368\n",
      "Gradient Descent(23/49): loss=0.38787732646863515\n",
      "Gradient Descent(24/49): loss=0.38745281221282496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=0.3870557099089474\n",
      "Gradient Descent(26/49): loss=0.38668246384684785\n",
      "Gradient Descent(27/49): loss=0.386330137799923\n",
      "Gradient Descent(28/49): loss=0.38599629806556995\n",
      "Gradient Descent(29/49): loss=0.3856789191985204\n",
      "Gradient Descent(30/49): loss=0.38537630796001554\n",
      "Gradient Descent(31/49): loss=0.3850870419022736\n",
      "Gradient Descent(32/49): loss=0.38480991972190887\n",
      "Gradient Descent(33/49): loss=0.3845439210856138\n",
      "Gradient Descent(34/49): loss=0.3842881740862209\n",
      "Gradient Descent(35/49): loss=0.3840419288507352\n",
      "Gradient Descent(36/49): loss=0.3838045361126725\n",
      "Gradient Descent(37/49): loss=0.38357542979380027\n",
      "Gradient Descent(38/49): loss=0.3833541128268968\n",
      "Gradient Descent(39/49): loss=0.3831401456007201\n",
      "Gradient Descent(40/49): loss=0.38293313652842914\n",
      "Gradient Descent(41/49): loss=0.38273273433714056\n",
      "Gradient Descent(42/49): loss=0.3825386217538286\n",
      "Gradient Descent(43/49): loss=0.3823505103251566\n",
      "Gradient Descent(44/49): loss=0.3821681361590551\n",
      "Gradient Descent(45/49): loss=0.381991256416333\n",
      "Gradient Descent(46/49): loss=0.38181964641325616\n",
      "Gradient Descent(47/49): loss=0.3816530972223662\n",
      "Gradient Descent(48/49): loss=0.3814914136800999\n",
      "Gradient Descent(49/49): loss=0.38133441272696256\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45562805742413487\n",
      "Gradient Descent(2/49): loss=0.4399537420119925\n",
      "Gradient Descent(3/49): loss=0.42970012595520296\n",
      "Gradient Descent(4/49): loss=0.4219156430879253\n",
      "Gradient Descent(5/49): loss=0.4157588604769042\n",
      "Gradient Descent(6/49): loss=0.4107957721296134\n",
      "Gradient Descent(7/49): loss=0.40674667084525057\n",
      "Gradient Descent(8/49): loss=0.40341349600510806\n",
      "Gradient Descent(9/49): loss=0.40064863134509743\n",
      "Gradient Descent(10/49): loss=0.39833870802235105\n",
      "Gradient Descent(11/49): loss=0.3963950882805551\n",
      "Gradient Descent(12/49): loss=0.3947477111700463\n",
      "Gradient Descent(13/49): loss=0.3933408168948509\n",
      "Gradient Descent(14/49): loss=0.3921298243154098\n",
      "Gradient Descent(15/49): loss=0.3910789740114516\n",
      "Gradient Descent(16/49): loss=0.39015951098154333\n",
      "Gradient Descent(17/49): loss=0.38934826431553277\n",
      "Gradient Descent(18/49): loss=0.38862652746277665\n",
      "Gradient Descent(19/49): loss=0.3879791704902672\n",
      "Gradient Descent(20/49): loss=0.3873939336080875\n",
      "Gradient Descent(21/49): loss=0.3868608634702497\n",
      "Gradient Descent(22/49): loss=0.38637186252768563\n",
      "Gradient Descent(23/49): loss=0.38592032821745964\n",
      "Gradient Descent(24/49): loss=0.3855008637184786\n",
      "Gradient Descent(25/49): loss=0.385109045824656\n",
      "Gradient Descent(26/49): loss=0.3847412384694021\n",
      "Gradient Descent(27/49): loss=0.38439444278059287\n",
      "Gradient Descent(28/49): loss=0.38406617639784\n",
      "Gradient Descent(29/49): loss=0.383754376252116\n",
      "Gradient Descent(30/49): loss=0.38345732017405204\n",
      "Gradient Descent(31/49): loss=0.38317356362521343\n",
      "Gradient Descent(32/49): loss=0.38290188858605984\n",
      "Gradient Descent(33/49): loss=0.38264126222410955\n",
      "Gradient Descent(34/49): loss=0.38239080343679155\n",
      "Gradient Descent(35/49): loss=0.3821497557398612\n",
      "Gradient Descent(36/49): loss=0.38191746527334736\n",
      "Gradient Descent(37/49): loss=0.3816933629380245\n",
      "Gradient Descent(38/49): loss=0.38147694986853253\n",
      "Gradient Descent(39/49): loss=0.3812677856041013\n",
      "Gradient Descent(40/49): loss=0.38106547844209737\n",
      "Gradient Descent(41/49): loss=0.38086967755939255\n",
      "Gradient Descent(42/49): loss=0.3806800665667363\n",
      "Gradient Descent(43/49): loss=0.38049635822580946\n",
      "Gradient Descent(44/49): loss=0.3803182901105383\n",
      "Gradient Descent(45/49): loss=0.3801456210360512\n",
      "Gradient Descent(46/49): loss=0.37997812811235415\n",
      "Gradient Descent(47/49): loss=0.3798156043069776\n",
      "Gradient Descent(48/49): loss=0.37965785642278194\n",
      "Gradient Descent(49/49): loss=0.37950470341482656\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4560969055481644\n",
      "Gradient Descent(2/49): loss=0.44073729835985936\n",
      "Gradient Descent(3/49): loss=0.43074257518804715\n",
      "Gradient Descent(4/49): loss=0.4231603787988023\n",
      "Gradient Descent(5/49): loss=0.41715660839856816\n",
      "Gradient Descent(6/49): loss=0.41230777848792244\n",
      "Gradient Descent(7/49): loss=0.40834353110328153\n",
      "Gradient Descent(8/49): loss=0.40507312083173064\n",
      "Gradient Descent(9/49): loss=0.4023544509417558\n",
      "Gradient Descent(10/49): loss=0.4000782888886843\n",
      "Gradient Descent(11/49): loss=0.39815910374898644\n",
      "Gradient Descent(12/49): loss=0.3965291776170514\n",
      "Gradient Descent(13/49): loss=0.3951345234422625\n",
      "Gradient Descent(14/49): loss=0.3939319036099303\n",
      "Gradient Descent(15/49): loss=0.39288657699999124\n",
      "Gradient Descent(16/49): loss=0.3919705593013563\n",
      "Gradient Descent(17/49): loss=0.3911612612328556\n",
      "Gradient Descent(18/49): loss=0.3904404133391472\n",
      "Gradient Descent(19/49): loss=0.38979321230679415\n",
      "Gradient Descent(20/49): loss=0.3892076406340333\n",
      "Gradient Descent(21/49): loss=0.3886739230403671\n",
      "Gradient Descent(22/49): loss=0.38818409129622033\n",
      "Gradient Descent(23/49): loss=0.38773163531926524\n",
      "Gradient Descent(24/49): loss=0.38731122307977645\n",
      "Gradient Descent(25/49): loss=0.3869184754907787\n",
      "Gradient Descent(26/49): loss=0.38654978529968853\n",
      "Gradient Descent(27/49): loss=0.38620217123475953\n",
      "Gradient Descent(28/49): loss=0.3858731604285413\n",
      "Gradient Descent(29/49): loss=0.38556069354393185\n",
      "Gradient Descent(30/49): loss=0.3852630481442924\n",
      "Gradient Descent(31/49): loss=0.38497877673787834\n",
      "Gradient Descent(32/49): loss=0.38470665663572295\n",
      "Gradient Descent(33/49): loss=0.3844456493281381\n",
      "Gradient Descent(34/49): loss=0.38419486753743076\n",
      "Gradient Descent(35/49): loss=0.383953548466413\n",
      "Gradient Descent(36/49): loss=0.3837210320521431\n",
      "Gradient Descent(37/49): loss=0.3834967432666559\n",
      "Gradient Descent(38/49): loss=0.3832801776927891\n",
      "Gradient Descent(39/49): loss=0.3830708897528163\n",
      "Gradient Descent(40/49): loss=0.3828684830878011\n",
      "Gradient Descent(41/49): loss=0.3826726026822381\n",
      "Gradient Descent(42/49): loss=0.38248292840632964\n",
      "Gradient Descent(43/49): loss=0.3822991697108918\n",
      "Gradient Descent(44/49): loss=0.38212106126037815\n",
      "Gradient Descent(45/49): loss=0.38194835933023635\n",
      "Gradient Descent(46/49): loss=0.38178083882769726\n",
      "Gradient Descent(47/49): loss=0.3816182908216554\n",
      "Gradient Descent(48/49): loss=0.3814605204887856\n",
      "Gradient Descent(49/49): loss=0.3813073454004088\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45548706473947875\n",
      "Gradient Descent(2/49): loss=0.4398019926198328\n",
      "Gradient Descent(3/49): loss=0.42959386946972455\n",
      "Gradient Descent(4/49): loss=0.42185552176885466\n",
      "Gradient Descent(5/49): loss=0.415735242959353\n",
      "Gradient Descent(6/49): loss=0.41079908062576453\n",
      "Gradient Descent(7/49): loss=0.40676926155338805\n",
      "Gradient Descent(8/49): loss=0.4034496631018383\n",
      "Gradient Descent(9/49): loss=0.40069423093966827\n",
      "Gradient Descent(10/49): loss=0.3983907727456735\n",
      "Gradient Descent(11/49): loss=0.3964515155517728\n",
      "Gradient Descent(12/49): loss=0.39480702919253613\n",
      "Gradient Descent(13/49): loss=0.3934020135051073\n",
      "Gradient Descent(14/49): loss=0.3921922225346137\n",
      "Gradient Descent(15/49): loss=0.3911421412812702\n",
      "Gradient Descent(16/49): loss=0.3902231925449118\n",
      "Gradient Descent(17/49): loss=0.38941233399261865\n",
      "Gradient Descent(18/49): loss=0.3886909510843024\n",
      "Gradient Descent(19/49): loss=0.38804397864338025\n",
      "Gradient Descent(20/49): loss=0.38745920130358\n",
      "Gradient Descent(21/49): loss=0.38692669499597426\n",
      "Gradient Descent(22/49): loss=0.386438380210696\n",
      "Gradient Descent(23/49): loss=0.3859876641425293\n",
      "Gradient Descent(24/49): loss=0.38556915368597955\n",
      "Gradient Descent(25/49): loss=0.38517842500405075\n",
      "Gradient Descent(26/49): loss=0.3848118383340052\n",
      "Gradient Descent(27/49): loss=0.38446638900698843\n",
      "Gradient Descent(28/49): loss=0.3841395874877411\n",
      "Gradient Descent(29/49): loss=0.38382936269141604\n",
      "Gradient Descent(30/49): loss=0.3835339839875856\n",
      "Gradient Descent(31/49): loss=0.38325199821943184\n",
      "Gradient Descent(32/49): loss=0.3829821787977811\n",
      "Gradient Descent(33/49): loss=0.3827234845134893\n",
      "Gradient Descent(34/49): loss=0.3824750261780306\n",
      "Gradient Descent(35/49): loss=0.38223603957497393\n",
      "Gradient Descent(36/49): loss=0.38200586350334864\n",
      "Gradient Descent(37/49): loss=0.38178392193279853\n",
      "Gradient Descent(38/49): loss=0.38156970948188706\n",
      "Gradient Descent(39/49): loss=0.38136277958448456\n",
      "Gradient Descent(40/49): loss=0.381162734832439\n",
      "Gradient Descent(41/49): loss=0.38096921908175435\n",
      "Gradient Descent(42/49): loss=0.38078191098911235\n",
      "Gradient Descent(43/49): loss=0.380600518709619\n",
      "Gradient Descent(44/49): loss=0.3804247755382229\n",
      "Gradient Descent(45/49): loss=0.3802544363188048\n",
      "Gradient Descent(46/49): loss=0.3800892744784359\n",
      "Gradient Descent(47/49): loss=0.37992907957133765\n",
      "Gradient Descent(48/49): loss=0.379773655238906\n",
      "Gradient Descent(49/49): loss=0.3796228175098015\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45345800430194527\n",
      "Gradient Descent(2/49): loss=0.43826940876386977\n",
      "Gradient Descent(3/49): loss=0.4281537703643116\n",
      "Gradient Descent(4/49): loss=0.4205231738448645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=0.4145601606094862\n",
      "Gradient Descent(6/49): loss=0.4098150961325335\n",
      "Gradient Descent(7/49): loss=0.4059933011207836\n",
      "Gradient Descent(8/49): loss=0.40288518617988356\n",
      "Gradient Descent(9/49): loss=0.40033497814112934\n",
      "Gradient Descent(10/49): loss=0.39822407670962934\n",
      "Gradient Descent(11/49): loss=0.3964609494952168\n",
      "Gradient Descent(12/49): loss=0.3949743846525212\n",
      "Gradient Descent(13/49): loss=0.3937086958665531\n",
      "Gradient Descent(14/49): loss=0.3926201816726198\n",
      "Gradient Descent(15/49): loss=0.3916744504669995\n",
      "Gradient Descent(16/49): loss=0.39084437186115323\n",
      "Gradient Descent(17/49): loss=0.3901084947795514\n",
      "Gradient Descent(18/49): loss=0.3894498196769143\n",
      "Gradient Descent(19/49): loss=0.38885484237260837\n",
      "Gradient Descent(20/49): loss=0.3883128076271631\n",
      "Gradient Descent(21/49): loss=0.3878151253794845\n",
      "Gradient Descent(22/49): loss=0.38735491349969364\n",
      "Gradient Descent(23/49): loss=0.38692663915301506\n",
      "Gradient Descent(24/49): loss=0.38652583715296107\n",
      "Gradient Descent(25/49): loss=0.3861488885075766\n",
      "Gradient Descent(26/49): loss=0.3857928460861453\n",
      "Gradient Descent(27/49): loss=0.3854552972161791\n",
      "Gradient Descent(28/49): loss=0.3851342552567945\n",
      "Gradient Descent(29/49): loss=0.3848280739325814\n",
      "Gradient Descent(30/49): loss=0.3845353795647833\n",
      "Gradient Descent(31/49): loss=0.38425501739077816\n",
      "Gradient Descent(32/49): loss=0.3839860089853433\n",
      "Gradient Descent(33/49): loss=0.38372751843962905\n",
      "Gradient Descent(34/49): loss=0.3834788254561141\n",
      "Gradient Descent(35/49): loss=0.3832393039110379\n",
      "Gradient Descent(36/49): loss=0.3830084047439129\n",
      "Gradient Descent(37/49): loss=0.38278564227540135\n",
      "Gradient Descent(38/49): loss=0.38257058324458243\n",
      "Gradient Descent(39/49): loss=0.38236283800577703\n",
      "Gradient Descent(40/49): loss=0.3821620534424148\n",
      "Gradient Descent(41/49): loss=0.38196790724781815\n",
      "Gradient Descent(42/49): loss=0.3817801032956039\n",
      "Gradient Descent(43/49): loss=0.3815983678798578\n",
      "Gradient Descent(44/49): loss=0.3814224466506121\n",
      "Gradient Descent(45/49): loss=0.38125210210603044\n",
      "Gradient Descent(46/49): loss=0.3810871115310753\n",
      "Gradient Descent(47/49): loss=0.3809272652949148\n",
      "Gradient Descent(48/49): loss=0.38077236543714144\n",
      "Gradient Descent(49/49): loss=0.38062222448700717\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45292251964124824\n",
      "Gradient Descent(2/49): loss=0.4373877171138617\n",
      "Gradient Descent(3/49): loss=0.4269737034120472\n",
      "Gradient Descent(4/49): loss=0.4191231202586596\n",
      "Gradient Descent(5/49): loss=0.4129992917361809\n",
      "Gradient Descent(6/49): loss=0.4081358269938311\n",
      "Gradient Descent(7/49): loss=0.4042265110497553\n",
      "Gradient Descent(8/49): loss=0.4010537820145398\n",
      "Gradient Descent(9/49): loss=0.39845619040374125\n",
      "Gradient Descent(10/49): loss=0.39631098230001993\n",
      "Gradient Descent(11/49): loss=0.3945235228999732\n",
      "Gradient Descent(12/49): loss=0.3930202503179318\n",
      "Gradient Descent(13/49): loss=0.3917436819861797\n",
      "Gradient Descent(14/49): loss=0.39064873603674544\n",
      "Gradient Descent(15/49): loss=0.3896999569434211\n",
      "Gradient Descent(16/49): loss=0.3888693933028714\n",
      "Gradient Descent(17/49): loss=0.38813496040322415\n",
      "Gradient Descent(18/49): loss=0.3874791699978299\n",
      "Gradient Descent(19/49): loss=0.3868881414427763\n",
      "Gradient Descent(20/49): loss=0.3863508299703689\n",
      "Gradient Descent(21/49): loss=0.3858584232991099\n",
      "Gradient Descent(22/49): loss=0.3854038691476992\n",
      "Gradient Descent(23/49): loss=0.38498150476681575\n",
      "Gradient Descent(24/49): loss=0.38458676611013287\n",
      "Gradient Descent(25/49): loss=0.3842159592610762\n",
      "Gradient Descent(26/49): loss=0.38386608058541555\n",
      "Gradient Descent(27/49): loss=0.38353467506280325\n",
      "Gradient Descent(28/49): loss=0.3832197245649936\n",
      "Gradient Descent(29/49): loss=0.38291955964767443\n",
      "Gradient Descent(30/49): loss=0.38263278982341475\n",
      "Gradient Descent(31/49): loss=0.38235824837484866\n",
      "Gradient Descent(32/49): loss=0.38209494861898946\n",
      "Gradient Descent(33/49): loss=0.3818420491988945\n",
      "Gradient Descent(34/49): loss=0.3815988264991122\n",
      "Gradient Descent(35/49): loss=0.38136465268848807\n",
      "Gradient Descent(36/49): loss=0.3811389782128702\n",
      "Gradient Descent(37/49): loss=0.38092131781036237\n",
      "Gradient Descent(38/49): loss=0.3807112393180741\n",
      "Gradient Descent(39/49): loss=0.38050835469353433\n",
      "Gradient Descent(40/49): loss=0.38031231279519034\n",
      "Gradient Descent(41/49): loss=0.38012279356184514\n",
      "Gradient Descent(42/49): loss=0.37993950330606097\n",
      "Gradient Descent(43/49): loss=0.3797621708958202\n",
      "Gradient Descent(44/49): loss=0.3795905446455059\n",
      "Gradient Descent(45/49): loss=0.37942438977419973\n",
      "Gradient Descent(46/49): loss=0.37926348631849377\n",
      "Gradient Descent(47/49): loss=0.3791076274101149\n",
      "Gradient Descent(48/49): loss=0.3789566178469544\n",
      "Gradient Descent(49/49): loss=0.378810272900594\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4534287084473084\n",
      "Gradient Descent(2/49): loss=0.4382305542972393\n",
      "Gradient Descent(3/49): loss=0.42808734507495944\n",
      "Gradient Descent(4/49): loss=0.4204395964444537\n",
      "Gradient Descent(5/49): loss=0.41446326049158855\n",
      "Gradient Descent(6/49): loss=0.40970603843746795\n",
      "Gradient Descent(7/49): loss=0.4058728980787992\n",
      "Gradient Descent(8/49): loss=0.4027544946456447\n",
      "Gradient Descent(9/49): loss=0.40019533544932245\n",
      "Gradient Descent(10/49): loss=0.3980769982177856\n",
      "Gradient Descent(11/49): loss=0.39630801623477063\n",
      "Gradient Descent(12/49): loss=0.3948171547309457\n",
      "Gradient Descent(13/49): loss=0.3935486438991969\n",
      "Gradient Descent(14/49): loss=0.3924586620633663\n",
      "Gradient Descent(15/49): loss=0.39151267869905526\n",
      "Gradient Descent(16/49): loss=0.39068341846456844\n",
      "Gradient Descent(17/49): loss=0.3899492877121391\n",
      "Gradient Descent(18/49): loss=0.3892931519332129\n",
      "Gradient Descent(19/49): loss=0.3887013825452349\n",
      "Gradient Descent(20/49): loss=0.3881631118537465\n",
      "Gradient Descent(21/49): loss=0.38766964963557354\n",
      "Gradient Descent(22/49): loss=0.3872140255777069\n",
      "Gradient Descent(23/49): loss=0.3867906299336718\n",
      "Gradient Descent(24/49): loss=0.38639493095802685\n",
      "Gradient Descent(25/49): loss=0.38602325244439506\n",
      "Gradient Descent(26/49): loss=0.38567259837298357\n",
      "Gradient Descent(27/49): loss=0.3853405145259355\n",
      "Gradient Descent(28/49): loss=0.3850249791445661\n",
      "Gradient Descent(29/49): loss=0.38472431642673305\n",
      "Gradient Descent(30/49): loss=0.38443712800625107\n",
      "Gradient Descent(31/49): loss=0.38416223860470894\n",
      "Gradient Descent(32/49): loss=0.38389865286508434\n",
      "Gradient Descent(33/49): loss=0.3836455210170825\n",
      "Gradient Descent(34/49): loss=0.3834021115255832\n",
      "Gradient Descent(35/49): loss=0.38316778926656797\n",
      "Gradient Descent(36/49): loss=0.3829419980831835\n",
      "Gradient Descent(37/49): loss=0.3827242468166977\n",
      "Gradient Descent(38/49): loss=0.3825140980973875\n",
      "Gradient Descent(39/49): loss=0.3823111593301339\n",
      "Gradient Descent(40/49): loss=0.38211507542741496\n",
      "Gradient Descent(41/49): loss=0.3819255229353574\n",
      "Gradient Descent(42/49): loss=0.3817422052718583\n",
      "Gradient Descent(43/49): loss=0.3815648488537383\n",
      "Gradient Descent(44/49): loss=0.38139319993569265\n",
      "Gradient Descent(45/49): loss=0.3812270220200618\n",
      "Gradient Descent(46/49): loss=0.3810660937251566\n",
      "Gradient Descent(47/49): loss=0.38091020702264466\n",
      "Gradient Descent(48/49): loss=0.38075916577256985\n",
      "Gradient Descent(49/49): loss=0.3806127844989332\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4527678837819193\n",
      "Gradient Descent(2/49): loss=0.4372411765139835\n",
      "Gradient Descent(3/49): loss=0.42688249641570847\n",
      "Gradient Descent(4/49): loss=0.4190800187011038\n",
      "Gradient Descent(5/49): loss=0.41299148198623786\n",
      "Gradient Descent(6/49): loss=0.40815261070761316\n",
      "Gradient Descent(7/49): loss=0.40426002803113165\n",
      "Gradient Descent(8/49): loss=0.40109849822746724\n",
      "Gradient Descent(9/49): loss=0.39850828061117666\n",
      "Gradient Descent(10/49): loss=0.39636783418741983\n",
      "Gradient Descent(11/49): loss=0.3945833774191838\n",
      "Gradient Descent(12/49): loss=0.39308194871971025\n",
      "Gradient Descent(13/49): loss=0.39180648863715745\n",
      "Gradient Descent(14/49): loss=0.3907122135305747\n",
      "Gradient Descent(15/49): loss=0.3897638772506084\n",
      "Gradient Descent(16/49): loss=0.3889336740240793\n",
      "Gradient Descent(17/49): loss=0.3881996187665616\n",
      "Gradient Descent(18/49): loss=0.3875442895765352\n",
      "Gradient Descent(19/49): loss=0.3869538480970174\n",
      "Gradient Descent(20/49): loss=0.38641727453210356\n",
      "Gradient Descent(21/49): loss=0.38592576920985566\n",
      "Gradient Descent(22/49): loss=0.38547228374099574\n",
      "Gradient Descent(23/49): loss=0.38505115323083666\n",
      "Gradient Descent(24/49): loss=0.3846578074153645\n",
      "Gradient Descent(25/49): loss=0.3842885435214355\n",
      "Gradient Descent(26/49): loss=0.38394034745730654\n",
      "Gradient Descent(27/49): loss=0.3836107528881979\n",
      "Gradient Descent(28/49): loss=0.3832977300406641\n",
      "Gradient Descent(29/49): loss=0.3829995978596995\n",
      "Gradient Descent(30/49): loss=0.38271495452879467\n",
      "Gradient Descent(31/49): loss=0.38244262244402705\n",
      "Gradient Descent(32/49): loss=0.38218160457694994\n",
      "Gradient Descent(33/49): loss=0.3819310498202656\n",
      "Gradient Descent(34/49): loss=0.38169022542588366\n",
      "Gradient Descent(35/49): loss=0.38145849504865387\n",
      "Gradient Descent(36/49): loss=0.38123530122543065\n",
      "Gradient Descent(37/49): loss=0.3810201513672873\n",
      "Gradient Descent(38/49): loss=0.38081260653755505\n",
      "Gradient Descent(39/49): loss=0.3806122724414952\n",
      "Gradient Descent(40/49): loss=0.380418792173872\n",
      "Gradient Descent(41/49): loss=0.380231840365544\n",
      "Gradient Descent(42/49): loss=0.38005111844493017\n",
      "Gradient Descent(43/49): loss=0.37987635078917004\n",
      "Gradient Descent(44/49): loss=0.3797072815863418\n",
      "Gradient Descent(45/49): loss=0.37954367226688357\n",
      "Gradient Descent(46/49): loss=0.3793852993914579\n",
      "Gradient Descent(47/49): loss=0.379231952905525\n",
      "Gradient Descent(48/49): loss=0.3790834346891428\n",
      "Gradient Descent(49/49): loss=0.37893955734497875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4510903205412574\n",
      "Gradient Descent(2/49): loss=0.43597051280481636\n",
      "Gradient Descent(3/49): loss=0.4257005865311936\n",
      "Gradient Descent(4/49): loss=0.4180370339877004\n",
      "Gradient Descent(5/49): loss=0.4121343838873269\n",
      "Gradient Descent(6/49): loss=0.40750708363880095\n",
      "Gradient Descent(7/49): loss=0.4038337901814712\n",
      "Gradient Descent(8/49): loss=0.4008861338198135\n",
      "Gradient Descent(9/49): loss=0.3984958204544568\n",
      "Gradient Descent(10/49): loss=0.3965364638938803\n",
      "Gradient Descent(11/49): loss=0.39491215960570386\n",
      "Gradient Descent(12/49): loss=0.39354967912294203\n",
      "Gradient Descent(13/49): loss=0.3923928739383794\n",
      "Gradient Descent(14/49): loss=0.391398552886531\n",
      "Gradient Descent(15/49): loss=0.3905333984099073\n",
      "Gradient Descent(16/49): loss=0.389771639690114\n",
      "Gradient Descent(17/49): loss=0.38909328763347084\n",
      "Gradient Descent(18/49): loss=0.38848279146923076\n",
      "Gradient Descent(19/49): loss=0.3879280137510585\n",
      "Gradient Descent(20/49): loss=0.38741944678489454\n",
      "Gradient Descent(21/49): loss=0.3869496126194048\n",
      "Gradient Descent(22/49): loss=0.386512602892803\n",
      "Gradient Descent(23/49): loss=0.38610372541852206\n",
      "Gradient Descent(24/49): loss=0.3857192323579107\n",
      "Gradient Descent(25/49): loss=0.3853561108429421\n",
      "Gradient Descent(26/49): loss=0.3850119214654503\n",
      "Gradient Descent(27/49): loss=0.38468467350351143\n",
      "Gradient Descent(28/49): loss=0.384372728380073\n",
      "Gradient Descent(29/49): loss=0.38407472484601535\n",
      "Gradient Descent(30/49): loss=0.38378952090159374\n",
      "Gradient Descent(31/49): loss=0.3835161486313273\n",
      "Gradient Descent(32/49): loss=0.38325377901447405\n",
      "Gradient Descent(33/49): loss=0.38300169445180227\n",
      "Gradient Descent(34/49): loss=0.38275926726908654\n",
      "Gradient Descent(35/49): loss=0.3825259428562958\n",
      "Gradient Descent(36/49): loss=0.38230122640742575\n",
      "Gradient Descent(37/49): loss=0.38208467246113464\n",
      "Gradient Descent(38/49): loss=0.3818758766233538\n",
      "Gradient Descent(39/49): loss=0.3816744689925129\n",
      "Gradient Descent(40/49): loss=0.3814801089156028\n",
      "Gradient Descent(41/49): loss=0.3812924807863842\n",
      "Gradient Descent(42/49): loss=0.38111129066128463\n",
      "Gradient Descent(43/49): loss=0.3809362635182474\n",
      "Gradient Descent(44/49): loss=0.3807671410223221\n",
      "Gradient Descent(45/49): loss=0.3806036796916793\n",
      "Gradient Descent(46/49): loss=0.3804456493809387\n",
      "Gradient Descent(47/49): loss=0.38029283201675657\n",
      "Gradient Descent(48/49): loss=0.3801450205346586\n",
      "Gradient Descent(49/49): loss=0.380002017977062\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45053182849012513\n",
      "Gradient Descent(2/49): loss=0.43502475804519797\n",
      "Gradient Descent(3/49): loss=0.42444757157295265\n",
      "Gradient Descent(4/49): loss=0.416567190089096\n",
      "Gradient Descent(5/49): loss=0.41051047976474453\n",
      "Gradient Descent(6/49): loss=0.40577275602795654\n",
      "Gradient Descent(7/49): loss=0.4020201777774547\n",
      "Gradient Descent(8/49): loss=0.3990159113563219\n",
      "Gradient Descent(9/49): loss=0.39658571182979646\n",
      "Gradient Descent(10/49): loss=0.39459889057307557\n",
      "Gradient Descent(11/49): loss=0.39295637432947744\n",
      "Gradient Descent(12/49): loss=0.3915825743123779\n",
      "Gradient Descent(13/49): loss=0.39041957262320287\n",
      "Gradient Descent(14/49): loss=0.3894228475389049\n",
      "Gradient Descent(15/49): loss=0.38855807989547736\n",
      "Gradient Descent(16/49): loss=0.3877987451080239\n",
      "Gradient Descent(17/49): loss=0.38712428748851196\n",
      "Gradient Descent(18/49): loss=0.3865187311253749\n",
      "Gradient Descent(19/49): loss=0.3859696202998012\n",
      "Gradient Descent(20/49): loss=0.38546720970972953\n",
      "Gradient Descent(21/49): loss=0.38500384460273857\n",
      "Gradient Descent(22/49): loss=0.38457348558383797\n",
      "Gradient Descent(23/49): loss=0.3841713438238368\n",
      "Gradient Descent(24/49): loss=0.38379360063674395\n",
      "Gradient Descent(25/49): loss=0.38343719161898787\n",
      "Gradient Descent(26/49): loss=0.38309964025622406\n",
      "Gradient Descent(27/49): loss=0.38277892947933656\n",
      "Gradient Descent(28/49): loss=0.3824734023687564\n",
      "Gradient Descent(29/49): loss=0.38218168527438307\n",
      "Gradient Descent(30/49): loss=0.38190262819443266\n",
      "Gradient Descent(31/49): loss=0.38163525845903884\n",
      "Gradient Descent(32/49): loss=0.3813787446830234\n",
      "Gradient Descent(33/49): loss=0.38113236865478883\n",
      "Gradient Descent(34/49): loss=0.38089550336619815\n",
      "Gradient Descent(35/49): loss=0.3806675958006285\n",
      "Gradient Descent(36/49): loss=0.3804481534128096\n",
      "Gradient Descent(37/49): loss=0.38023673347713327\n",
      "Gradient Descent(38/49): loss=0.3800329346680788\n",
      "Gradient Descent(39/49): loss=0.37983639038032635\n",
      "Gradient Descent(40/49): loss=0.3796467634070716\n",
      "Gradient Descent(41/49): loss=0.379463741680648\n",
      "Gradient Descent(42/49): loss=0.3792870348456755\n",
      "Gradient Descent(43/49): loss=0.37911637148607163\n",
      "Gradient Descent(44/49): loss=0.37895149686683005\n",
      "Gradient Descent(45/49): loss=0.3787921710821306\n",
      "Gradient Descent(46/49): loss=0.37863816752512386\n",
      "Gradient Descent(47/49): loss=0.3784892716132062\n",
      "Gradient Descent(48/49): loss=0.37834527971695375\n",
      "Gradient Descent(49/49): loss=0.3782059982520574\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4510736226817848\n",
      "Gradient Descent(2/49): loss=0.43592539435806027\n",
      "Gradient Descent(3/49): loss=0.4256279975683457\n",
      "Gradient Descent(4/49): loss=0.4179481465059447\n",
      "Gradient Descent(5/49): loss=0.4120317679146151\n",
      "Gradient Descent(6/49): loss=0.4073916007787771\n",
      "Gradient Descent(7/49): loss=0.4037064898494621\n",
      "Gradient Descent(8/49): loss=0.4007484963033361\n",
      "Gradient Descent(9/49): loss=0.3983496228034012\n",
      "Gradient Descent(10/49): loss=0.3963836044529791\n",
      "Gradient Descent(11/49): loss=0.3947545196663744\n",
      "Gradient Descent(12/49): loss=0.3933890312840218\n",
      "Gradient Descent(13/49): loss=0.39223082933644315\n",
      "Gradient Descent(14/49): loss=0.3912365366823151\n",
      "Gradient Descent(15/49): loss=0.3903726442494373\n",
      "Gradient Descent(16/49): loss=0.3896131960504071\n",
      "Gradient Descent(17/49): loss=0.38893803100157787\n",
      "Gradient Descent(18/49): loss=0.38833144292608646\n",
      "Gradient Descent(19/49): loss=0.3877811567308833\n",
      "Gradient Descent(20/49): loss=0.3872775446280992\n",
      "Gradient Descent(21/49): loss=0.3868130251159012\n",
      "Gradient Descent(22/49): loss=0.38638160139689887\n",
      "Gradient Descent(23/49): loss=0.38597850636399134\n",
      "Gradient Descent(24/49): loss=0.38559992915527724\n",
      "Gradient Descent(25/49): loss=0.38524280423129714\n",
      "Gradient Descent(26/49): loss=0.3849046484397991\n",
      "Gradient Descent(27/49): loss=0.38458343496058645\n",
      "Gradient Descent(28/49): loss=0.38427749563083896\n",
      "Gradient Descent(29/49): loss=0.38398544513846233\n",
      "Gradient Descent(30/49): loss=0.3837061220873013\n",
      "Gradient Descent(31/49): loss=0.383438543096531\n",
      "Gradient Descent(32/49): loss=0.3831818669827664\n",
      "Gradient Descent(33/49): loss=0.382935366752225\n",
      "Gradient Descent(34/49): loss=0.38269840765082586\n",
      "Gradient Descent(35/49): loss=0.38247042991978497\n",
      "Gradient Descent(36/49): loss=0.38225093521150577\n",
      "Gradient Descent(37/49): loss=0.3820394758570165\n",
      "Gradient Descent(38/49): loss=0.38183564635840966\n",
      "Gradient Descent(39/49): loss=0.38163907662028634\n",
      "Gradient Descent(40/49): loss=0.3814494265427614\n",
      "Gradient Descent(41/49): loss=0.381266381682522\n",
      "Gradient Descent(42/49): loss=0.38108964975340043\n",
      "Gradient Descent(43/49): loss=0.38091895778828344\n",
      "Gradient Descent(44/49): loss=0.3807540498232442\n",
      "Gradient Descent(45/49): loss=0.38059468499513566\n",
      "Gradient Descent(46/49): loss=0.3804406359674836\n",
      "Gradient Descent(47/49): loss=0.380291687617895\n",
      "Gradient Descent(48/49): loss=0.3801476359345237\n",
      "Gradient Descent(49/49): loss=0.38000828708031387\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4503636323822676\n",
      "Gradient Descent(2/49): loss=0.43488629079466434\n",
      "Gradient Descent(3/49): loss=0.4243715946832973\n",
      "Gradient Descent(4/49): loss=0.41653964166325264\n",
      "Gradient Descent(5/49): loss=0.41051622941781085\n",
      "Gradient Descent(6/49): loss=0.40580050106934124\n",
      "Gradient Descent(7/49): loss=0.4020621435733589\n",
      "Gradient Descent(8/49): loss=0.39906689928589767\n",
      "Gradient Descent(9/49): loss=0.39664229484892655\n",
      "Gradient Descent(10/49): loss=0.39465884292823594\n",
      "Gradient Descent(11/49): loss=0.39301828538070976\n",
      "Gradient Descent(12/49): loss=0.39164558838023017\n",
      "Gradient Descent(13/49): loss=0.39048321208844183\n",
      "Gradient Descent(14/49): loss=0.38948689102960404\n",
      "Gradient Descent(15/49): loss=0.3886224775173685\n",
      "Gradient Descent(16/49): loss=0.3878635590746365\n",
      "Gradient Descent(17/49): loss=0.38718965046946785\n",
      "Gradient Descent(18/49): loss=0.38658481711168846\n",
      "Gradient Descent(19/49): loss=0.3860366243811125\n",
      "Gradient Descent(20/49): loss=0.3855353342190202\n",
      "Gradient Descent(21/49): loss=0.38507328981018035\n",
      "Gradient Descent(22/49): loss=0.3846444436305141\n",
      "Gradient Descent(23/49): loss=0.3842439949491479\n",
      "Gradient Descent(24/49): loss=0.38386811101518176\n",
      "Gradient Descent(25/49): loss=0.38351371231208603\n",
      "Gradient Descent(26/49): loss=0.3831783069240015\n",
      "Gradient Descent(27/49): loss=0.3828598625965455\n",
      "Gradient Descent(28/49): loss=0.3825567077648406\n",
      "Gradient Descent(29/49): loss=0.3822674548695567\n",
      "Gradient Descent(30/49): loss=0.3819909408430723\n",
      "Gradient Descent(31/49): loss=0.3817261808395534\n",
      "Gradient Descent(32/49): loss=0.3814723321934333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=0.38122866628753477\n",
      "Gradient Descent(34/49): loss=0.3809945465457735\n",
      "Gradient Descent(35/49): loss=0.3807694111746566\n",
      "Gradient Descent(36/49): loss=0.3805527595920008\n",
      "Gradient Descent(37/49): loss=0.38034414172279807\n",
      "Gradient Descent(38/49): loss=0.3801431495279777\n",
      "Gradient Descent(39/49): loss=0.37994941027495316\n",
      "Gradient Descent(40/49): loss=0.3797625811692299\n",
      "Gradient Descent(41/49): loss=0.379582345051558\n",
      "Gradient Descent(42/49): loss=0.3794084069309774\n",
      "Gradient Descent(43/49): loss=0.379240491175052\n",
      "Gradient Descent(44/49): loss=0.37907833921805406\n",
      "Gradient Descent(45/49): loss=0.37892170767846184\n",
      "Gradient Descent(46/49): loss=0.3787703668008826\n",
      "Gradient Descent(47/49): loss=0.3786240991559743\n",
      "Gradient Descent(48/49): loss=0.37848269854629535\n",
      "Gradient Descent(49/49): loss=0.37834596907720447\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4490332045734558\n",
      "Gradient Descent(2/49): loss=0.4338254588711167\n",
      "Gradient Descent(3/49): loss=0.42341671227579286\n",
      "Gradient Descent(4/49): loss=0.4157538962501686\n",
      "Gradient Descent(5/49): loss=0.4099397760056725\n",
      "Gradient Descent(6/49): loss=0.40544997010630984\n",
      "Gradient Descent(7/49): loss=0.40193590508873256\n",
      "Gradient Descent(8/49): loss=0.39915117646386\n",
      "Gradient Descent(9/49): loss=0.39691643113530345\n",
      "Gradient Descent(10/49): loss=0.3950991892799436\n",
      "Gradient Descent(11/49): loss=0.39360079760120686\n",
      "Gradient Descent(12/49): loss=0.3923474153323637\n",
      "Gradient Descent(13/49): loss=0.3912835631592642\n",
      "Gradient Descent(14/49): loss=0.3903674200212648\n",
      "Gradient Descent(15/49): loss=0.38956735976036816\n",
      "Gradient Descent(16/49): loss=0.3888593860816853\n",
      "Gradient Descent(17/49): loss=0.3882252258858589\n",
      "Gradient Descent(18/49): loss=0.38765090827295234\n",
      "Gradient Descent(19/49): loss=0.3871257032608797\n",
      "Gradient Descent(20/49): loss=0.38664132766903975\n",
      "Gradient Descent(21/49): loss=0.38619134985938314\n",
      "Gradient Descent(22/49): loss=0.3857707427672295\n",
      "Gradient Descent(23/49): loss=0.3853755477017659\n",
      "Gradient Descent(24/49): loss=0.38500262102405913\n",
      "Gradient Descent(25/49): loss=0.38464944293205233\n",
      "Gradient Descent(26/49): loss=0.3843139728602191\n",
      "Gradient Descent(27/49): loss=0.3839945399203748\n",
      "Gradient Descent(28/49): loss=0.3836897597244605\n",
      "Gradient Descent(29/49): loss=0.38339847110083747\n",
      "Gradient Descent(30/49): loss=0.3831196878349473\n",
      "Gradient Descent(31/49): loss=0.38285256177502003\n",
      "Gradient Descent(32/49): loss=0.38259635454871216\n",
      "Gradient Descent(33/49): loss=0.3823504158148443\n",
      "Gradient Descent(34/49): loss=0.38211416648337887\n",
      "Gradient Descent(35/49): loss=0.38188708571926167\n",
      "Gradient Descent(36/49): loss=0.38166870083356036\n",
      "Gradient Descent(37/49): loss=0.38145857938222866\n",
      "Gradient Descent(38/49): loss=0.3812563229564847\n",
      "Gradient Descent(39/49): loss=0.3810615622724615\n",
      "Gradient Descent(40/49): loss=0.380873953261363\n",
      "Gradient Descent(41/49): loss=0.38069317393225405\n",
      "Gradient Descent(42/49): loss=0.38051892183340835\n",
      "Gradient Descent(43/49): loss=0.380350911978995\n",
      "Gradient Descent(44/49): loss=0.38018887513898286\n",
      "Gradient Descent(45/49): loss=0.38003255641381495\n",
      "Gradient Descent(46/49): loss=0.37988171403347615\n",
      "Gradient Descent(47/49): loss=0.37973611833437976\n",
      "Gradient Descent(48/49): loss=0.3795955508780529\n",
      "Gradient Descent(49/49): loss=0.37945980368369386\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44845598397076564\n",
      "Gradient Descent(2/49): loss=0.4328159075680932\n",
      "Gradient Descent(3/49): loss=0.42209620486541827\n",
      "Gradient Descent(4/49): loss=0.41422156724355813\n",
      "Gradient Descent(5/49): loss=0.4082609190438718\n",
      "Gradient Descent(6/49): loss=0.40366895358116756\n",
      "Gradient Descent(7/49): loss=0.4000838123734275\n",
      "Gradient Descent(8/49): loss=0.3972502155118323\n",
      "Gradient Descent(9/49): loss=0.3949826396491663\n",
      "Gradient Descent(10/49): loss=0.39314420249621657\n",
      "Gradient Descent(11/49): loss=0.3916330610259099\n",
      "Gradient Descent(12/49): loss=0.39037304430104897\n",
      "Gradient Descent(13/49): loss=0.38930696433897444\n",
      "Gradient Descent(14/49): loss=0.3883917458270628\n",
      "Gradient Descent(15/49): loss=0.3875948424695323\n",
      "Gradient Descent(16/49): loss=0.3868915840492126\n",
      "Gradient Descent(17/49): loss=0.386263205044094\n",
      "Gradient Descent(18/49): loss=0.38569537582146435\n",
      "Gradient Descent(19/49): loss=0.38517710600253985\n",
      "Gradient Descent(20/49): loss=0.38469992421413357\n",
      "Gradient Descent(21/49): loss=0.384257263537037\n",
      "Gradient Descent(22/49): loss=0.3838440003164029\n",
      "Gradient Descent(23/49): loss=0.3834561074999541\n",
      "Gradient Descent(24/49): loss=0.3830903936340552\n",
      "Gradient Descent(25/49): loss=0.38274430602004733\n",
      "Gradient Descent(26/49): loss=0.3824157819985963\n",
      "Gradient Descent(27/49): loss=0.38210313638826865\n",
      "Gradient Descent(28/49): loss=0.3818049761229483\n",
      "Gradient Descent(29/49): loss=0.38152013538087953\n",
      "Gradient Descent(30/49): loss=0.381247626174991\n",
      "Gradient Descent(31/49): loss=0.3809866006266403\n",
      "Gradient Descent(32/49): loss=0.38073632208170594\n",
      "Gradient Descent(33/49): loss=0.3804961429295508\n",
      "Gradient Descent(34/49): loss=0.38026548751154715\n",
      "Gradient Descent(35/49): loss=0.3800438389009631\n",
      "Gradient Descent(36/49): loss=0.3798307286330974\n",
      "Gradient Descent(37/49): loss=0.37962572868821937\n",
      "Gradient Descent(38/49): loss=0.3794284451984847\n",
      "Gradient Descent(39/49): loss=0.3792385134772665\n",
      "Gradient Descent(40/49): loss=0.3790555940655286\n",
      "Gradient Descent(41/49): loss=0.37887936956265195\n",
      "Gradient Descent(42/49): loss=0.37870954206428015\n",
      "Gradient Descent(43/49): loss=0.37854583107158857\n",
      "Gradient Descent(44/49): loss=0.3783879717681668\n",
      "Gradient Descent(45/49): loss=0.3782357135848861\n",
      "Gradient Descent(46/49): loss=0.37808881899153796\n",
      "Gradient Descent(47/49): loss=0.3779470624680801\n",
      "Gradient Descent(48/49): loss=0.3778102296190582\n",
      "Gradient Descent(49/49): loss=0.37767811640298055\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44903164825159336\n",
      "Gradient Descent(2/49): loss=0.4337729843583952\n",
      "Gradient Descent(3/49): loss=0.42333892056120503\n",
      "Gradient Descent(4/49): loss=0.4156601779420088\n",
      "Gradient Descent(5/49): loss=0.40983161909579596\n",
      "Gradient Descent(6/49): loss=0.40532825828048624\n",
      "Gradient Descent(7/49): loss=0.40180211164194\n",
      "Gradient Descent(8/49): loss=0.3990072841665161\n",
      "Gradient Descent(9/49): loss=0.3967646676238115\n",
      "Gradient Descent(10/49): loss=0.39494180571437404\n",
      "Gradient Descent(11/49): loss=0.3934399228760617\n",
      "Gradient Descent(12/49): loss=0.39218497419941595\n",
      "Gradient Descent(13/49): loss=0.3911212398001699\n",
      "Gradient Descent(14/49): loss=0.39020665105048014\n",
      "Gradient Descent(15/49): loss=0.38940934479077965\n",
      "Gradient Descent(16/49): loss=0.3887051077257992\n",
      "Gradient Descent(17/49): loss=0.38807547392419095\n",
      "Gradient Descent(18/49): loss=0.3875063047435699\n",
      "Gradient Descent(19/49): loss=0.3869867265880622\n",
      "Gradient Descent(20/49): loss=0.38650833483622804\n",
      "Gradient Descent(21/49): loss=0.3860645961892628\n",
      "Gradient Descent(22/49): loss=0.3856503992088111\n",
      "Gradient Descent(23/49): loss=0.3852617157172394\n",
      "Gradient Descent(24/49): loss=0.3848953452692591\n",
      "Gradient Descent(25/49): loss=0.3845487219684232\n",
      "Gradient Descent(26/49): loss=0.38421976814610753\n",
      "Gradient Descent(27/49): loss=0.38390678332000694\n",
      "Gradient Descent(28/49): loss=0.3836083597534302\n",
      "Gradient Descent(29/49): loss=0.383323318103038\n",
      "Gradient Descent(30/49): loss=0.3830506582610835\n",
      "Gradient Descent(31/49): loss=0.3827895217090315\n",
      "Gradient Descent(32/49): loss=0.38253916260664406\n",
      "Gradient Descent(33/49): loss=0.382298925521325\n",
      "Gradient Descent(34/49): loss=0.38206822821399616\n",
      "Gradient Descent(35/49): loss=0.3818465482826433\n",
      "Gradient Descent(36/49): loss=0.3816334127546783\n",
      "Gradient Descent(37/49): loss=0.38142838993808437\n",
      "Gradient Descent(38/49): loss=0.3812310830066605\n",
      "Gradient Descent(39/49): loss=0.3810411249197906\n",
      "Gradient Descent(40/49): loss=0.3808581743719546\n",
      "Gradient Descent(41/49): loss=0.3806819125391248\n",
      "Gradient Descent(42/49): loss=0.3805120404438355\n",
      "Gradient Descent(43/49): loss=0.3803482768022937\n",
      "Gradient Descent(44/49): loss=0.38019035624857306\n",
      "Gradient Descent(45/49): loss=0.3800380278551103\n",
      "Gradient Descent(46/49): loss=0.3798910538871864\n",
      "Gradient Descent(47/49): loss=0.3797492087432146\n",
      "Gradient Descent(48/49): loss=0.3796122780434861\n",
      "Gradient Descent(49/49): loss=0.3794800578383428\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4482743105405237\n",
      "Gradient Descent(2/49): loss=0.43268766348736787\n",
      "Gradient Descent(3/49): loss=0.4220350432111448\n",
      "Gradient Descent(4/49): loss=0.4142079701591035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=0.40827814983433697\n",
      "Gradient Descent(6/49): loss=0.40370551225913603\n",
      "Gradient Descent(7/49): loss=0.400132226136037\n",
      "Gradient Descent(8/49): loss=0.39730572250354346\n",
      "Gradient Descent(9/49): loss=0.3950422498807286\n",
      "Gradient Descent(10/49): loss=0.39320608338233265\n",
      "Gradient Descent(11/49): loss=0.3916961402252751\n",
      "Gradient Descent(12/49): loss=0.39043675001125316\n",
      "Gradient Descent(13/49): loss=0.3893710526188831\n",
      "Gradient Descent(14/49): loss=0.38845618421375067\n",
      "Gradient Descent(15/49): loss=0.38765973093822614\n",
      "Gradient Descent(16/49): loss=0.3869571012702602\n",
      "Gradient Descent(17/49): loss=0.3863295720226372\n",
      "Gradient Descent(18/49): loss=0.38576283157398655\n",
      "Gradient Descent(19/49): loss=0.38524589158770917\n",
      "Gradient Descent(20/49): loss=0.38477027255016905\n",
      "Gradient Descent(21/49): loss=0.38432939320396864\n",
      "Gradient Descent(22/49): loss=0.3839181120766915\n",
      "Gradient Descent(23/49): loss=0.383532382648177\n",
      "Gradient Descent(24/49): loss=0.3831689935535428\n",
      "Gradient Descent(25/49): loss=0.38282537251383725\n",
      "Gradient Descent(26/49): loss=0.38249943809632514\n",
      "Gradient Descent(27/49): loss=0.3821894874254955\n",
      "Gradient Descent(28/49): loss=0.3818941109561712\n",
      "Gradient Descent(29/49): loss=0.3816121276482314\n",
      "Gradient Descent(30/49): loss=0.38134253554505015\n",
      "Gradient Descent(31/49): loss=0.3810844740001278\n",
      "Gradient Descent(32/49): loss=0.38083719472600525\n",
      "Gradient Descent(33/49): loss=0.38060003953613336\n",
      "Gradient Descent(34/49): loss=0.38037242317301173\n",
      "Gradient Descent(35/49): loss=0.38015382000858744\n",
      "Gradient Descent(36/49): loss=0.3799437536983298\n",
      "Gradient Descent(37/49): loss=0.37974178909293393\n",
      "Gradient Descent(38/49): loss=0.37954752587947743\n",
      "Gradient Descent(39/49): loss=0.37936059355063795\n",
      "Gradient Descent(40/49): loss=0.37918064739646806\n",
      "Gradient Descent(41/49): loss=0.37900736528583223\n",
      "Gradient Descent(42/49): loss=0.37884044505967307\n",
      "Gradient Descent(43/49): loss=0.37867960240007137\n",
      "Gradient Descent(44/49): loss=0.3785245690708549\n",
      "Gradient Descent(45/49): loss=0.3783750914497017\n",
      "Gradient Descent(46/49): loss=0.37823092929013297\n",
      "Gradient Descent(47/49): loss=0.3780918546658765\n",
      "Gradient Descent(48/49): loss=0.377957651060851\n",
      "Gradient Descent(49/49): loss=0.3778281125762642\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44728665639854015\n",
      "Gradient Descent(2/49): loss=0.431801431142334\n",
      "Gradient Descent(3/49): loss=0.42128421530721105\n",
      "Gradient Descent(4/49): loss=0.41365291776307733\n",
      "Gradient Descent(5/49): loss=0.40795088283116093\n",
      "Gradient Descent(6/49): loss=0.4036131322715128\n",
      "Gradient Descent(7/49): loss=0.4002639323373329\n",
      "Gradient Descent(8/49): loss=0.39764017840381927\n",
      "Gradient Descent(9/49): loss=0.3955532969710854\n",
      "Gradient Descent(10/49): loss=0.3938665640626562\n",
      "Gradient Descent(11/49): loss=0.3924801902676299\n",
      "Gradient Descent(12/49): loss=0.39132100781596774\n",
      "Gradient Descent(13/49): loss=0.39033516133980684\n",
      "Gradient Descent(14/49): loss=0.38948286270462873\n",
      "Gradient Descent(15/49): loss=0.3887346016702205\n",
      "Gradient Descent(16/49): loss=0.3880683970519085\n",
      "Gradient Descent(17/49): loss=0.3874677968482688\n",
      "Gradient Descent(18/49): loss=0.3869204197923835\n",
      "Gradient Descent(19/49): loss=0.3864168894570686\n",
      "Gradient Descent(20/49): loss=0.3859500536514202\n",
      "Gradient Descent(21/49): loss=0.38551441158778316\n",
      "Gradient Descent(22/49): loss=0.3851056926585597\n",
      "Gradient Descent(23/49): loss=0.384720546051664\n",
      "Gradient Descent(24/49): loss=0.3843563115481567\n",
      "Gradient Descent(25/49): loss=0.3840108498900516\n",
      "Gradient Descent(26/49): loss=0.383682416939978\n",
      "Gradient Descent(27/49): loss=0.3833695700928051\n",
      "Gradient Descent(28/49): loss=0.3830710984843301\n",
      "Gradient Descent(29/49): loss=0.38278597079161275\n",
      "Gradient Descent(30/49): loss=0.3825132960626627\n",
      "Gradient Descent(31/49): loss=0.3822522942154724\n",
      "Gradient Descent(32/49): loss=0.38200227372762496\n",
      "Gradient Descent(33/49): loss=0.3817626146846929\n",
      "Gradient Descent(34/49): loss=0.38153275583144464\n",
      "Gradient Descent(35/49): loss=0.3813121846203596\n",
      "Gradient Descent(36/49): loss=0.3811004295105425\n",
      "Gradient Descent(37/49): loss=0.3808970539612159\n",
      "Gradient Descent(38/49): loss=0.380701651705422\n",
      "Gradient Descent(39/49): loss=0.38051384299441665\n",
      "Gradient Descent(40/49): loss=0.38033327158110714\n",
      "Gradient Descent(41/49): loss=0.38015960226878365\n",
      "Gradient Descent(42/49): loss=0.3799925188945418\n",
      "Gradient Descent(43/49): loss=0.3798317226489703\n",
      "Gradient Descent(44/49): loss=0.37967693065774133\n",
      "Gradient Descent(45/49): loss=0.3795278747687453\n",
      "Gradient Descent(46/49): loss=0.3793843005019282\n",
      "Gradient Descent(47/49): loss=0.37924596612913175\n",
      "Gradient Descent(48/49): loss=0.37911264185888927\n",
      "Gradient Descent(49/49): loss=0.37898410910688946\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4466949860831698\n",
      "Gradient Descent(2/49): loss=0.43072961655099806\n",
      "Gradient Descent(3/49): loss=0.4199015694746258\n",
      "Gradient Descent(4/49): loss=0.41206467249119233\n",
      "Gradient Descent(5/49): loss=0.40622413779133204\n",
      "Gradient Descent(6/49): loss=0.4017926042288334\n",
      "Gradient Descent(7/49): loss=0.3983803831233179\n",
      "Gradient Descent(8/49): loss=0.39571514726443807\n",
      "Gradient Descent(9/49): loss=0.39360198719310074\n",
      "Gradient Descent(10/49): loss=0.39189972707708515\n",
      "Gradient Descent(11/49): loss=0.39050541264563027\n",
      "Gradient Descent(12/49): loss=0.38934361453196903\n",
      "Gradient Descent(13/49): loss=0.3883588589823186\n",
      "Gradient Descent(14/49): loss=0.38751020064256464\n",
      "Gradient Descent(15/49): loss=0.38676730351148053\n",
      "Gradient Descent(16/49): loss=0.3861075988946643\n",
      "Gradient Descent(17/49): loss=0.3855142183318439\n",
      "Gradient Descent(18/49): loss=0.384974486673375\n",
      "Gradient Descent(19/49): loss=0.3844788212517871\n",
      "Gradient Descent(20/49): loss=0.38401992614630664\n",
      "Gradient Descent(21/49): loss=0.38359220130759325\n",
      "Gradient Descent(22/49): loss=0.38319130841236143\n",
      "Gradient Descent(23/49): loss=0.3828138512466113\n",
      "Gradient Descent(24/49): loss=0.38245713992422115\n",
      "Gradient Descent(25/49): loss=0.3821190165787015\n",
      "Gradient Descent(26/49): loss=0.38179772620817204\n",
      "Gradient Descent(27/49): loss=0.38149182074355914\n",
      "Gradient Descent(28/49): loss=0.3812000876048391\n",
      "Gradient Descent(29/49): loss=0.38092149633907435\n",
      "Gradient Descent(30/49): loss=0.3806551586344329\n",
      "Gradient Descent(31/49): loss=0.3804002982479343\n",
      "Gradient Descent(32/49): loss=0.38015622829551765\n",
      "Gradient Descent(33/49): loss=0.37992233402122755\n",
      "Gradient Descent(34/49): loss=0.3796980596532596\n",
      "Gradient Descent(35/49): loss=0.3794828983158626\n",
      "Gradient Descent(36/49): loss=0.3792763842323351\n",
      "Gradient Descent(37/49): loss=0.3790780866508461\n",
      "Gradient Descent(38/49): loss=0.3788876050700653\n",
      "Gradient Descent(39/49): loss=0.37870456544911096\n",
      "Gradient Descent(40/49): loss=0.3785286171660499\n",
      "Gradient Descent(41/49): loss=0.37835943054838533\n",
      "Gradient Descent(42/49): loss=0.37819669484299456\n",
      "Gradient Descent(43/49): loss=0.37804011652578207\n",
      "Gradient Descent(44/49): loss=0.3778894178757855\n",
      "Gradient Descent(45/49): loss=0.3777443357567659\n",
      "Gradient Descent(46/49): loss=0.37760462056301164\n",
      "Gradient Descent(47/49): loss=0.37747003529636775\n",
      "Gradient Descent(48/49): loss=0.3773403547492246\n",
      "Gradient Descent(49/49): loss=0.3772153647740241\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4473027851567343\n",
      "Gradient Descent(2/49): loss=0.4317417832804161\n",
      "Gradient Descent(3/49): loss=0.4212020509784438\n",
      "Gradient Descent(4/49): loss=0.41355462007114857\n",
      "Gradient Descent(5/49): loss=0.40783728500181365\n",
      "Gradient Descent(6/49): loss=0.4034854173650137\n",
      "Gradient Descent(7/49): loss=0.4001241232805664\n",
      "Gradient Descent(8/49): loss=0.39749079369319273\n",
      "Gradient Descent(9/49): loss=0.39539699383265203\n",
      "Gradient Descent(10/49): loss=0.39370589599378414\n",
      "Gradient Descent(11/49): loss=0.39231747049875465\n",
      "Gradient Descent(12/49): loss=0.39115824812194006\n",
      "Gradient Descent(13/49): loss=0.3901740585335925\n",
      "Gradient Descent(14/49): loss=0.3893248125944979\n",
      "Gradient Descent(15/49): loss=0.3885807270504577\n",
      "Gradient Descent(16/49): loss=0.3879195813591947\n",
      "Gradient Descent(17/49): loss=0.38732471857038214\n",
      "Gradient Descent(18/49): loss=0.38678358495106213\n",
      "Gradient Descent(19/49): loss=0.3862866608751252\n",
      "Gradient Descent(20/49): loss=0.38582667654224967\n",
      "Gradient Descent(21/49): loss=0.385398035475045\n",
      "Gradient Descent(22/49): loss=0.3849963898804583\n",
      "Gradient Descent(23/49): loss=0.3846183272160842\n",
      "Gradient Descent(24/49): loss=0.3842611383381616\n",
      "Gradient Descent(25/49): loss=0.3839226456090221\n",
      "Gradient Descent(26/49): loss=0.3836010751534483\n",
      "Gradient Descent(27/49): loss=0.3832949616825991\n",
      "Gradient Descent(28/49): loss=0.3830030773872516\n",
      "Gradient Descent(29/49): loss=0.38272437865362685\n",
      "Gradient Descent(30/49): loss=0.3824579660021408\n",
      "Gradient Descent(31/49): loss=0.3822030538563801\n",
      "Gradient Descent(32/49): loss=0.3819589476355307\n",
      "Gradient Descent(33/49): loss=0.3817250263148744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(34/49): loss=0.3815007290786865\n",
      "Gradient Descent(35/49): loss=0.38128554504374895\n",
      "Gradient Descent(36/49): loss=0.3810790052931709\n",
      "Gradient Descent(37/49): loss=0.3808806766537216\n",
      "Gradient Descent(38/49): loss=0.38069015679333396\n",
      "Gradient Descent(39/49): loss=0.38050707032194986\n",
      "Gradient Descent(40/49): loss=0.38033106565809527\n",
      "Gradient Descent(41/49): loss=0.38016181248258785\n",
      "Gradient Descent(42/49): loss=0.3799989996448145\n",
      "Gradient Descent(43/49): loss=0.37984233341993645\n",
      "Gradient Descent(44/49): loss=0.3796915360400281\n",
      "Gradient Descent(45/49): loss=0.3795463444406497\n",
      "Gradient Descent(46/49): loss=0.3794065091782536\n",
      "Gradient Descent(47/49): loss=0.37927179348429013\n",
      "Gradient Descent(48/49): loss=0.3791419724297871\n",
      "Gradient Descent(49/49): loss=0.3790168321801433\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4464999182566875\n",
      "Gradient Descent(2/49): loss=0.43061287554160144\n",
      "Gradient Descent(3/49): loss=0.4198544646918903\n",
      "Gradient Descent(4/49): loss=0.4120634254744756\n",
      "Gradient Descent(5/49): loss=0.40625098967513384\n",
      "Gradient Descent(6/49): loss=0.40183617512640074\n",
      "Gradient Descent(7/49): loss=0.3984336492293191\n",
      "Gradient Descent(8/49): loss=0.3957738399868683\n",
      "Gradient Descent(9/49): loss=0.39366356629999183\n",
      "Gradient Descent(10/49): loss=0.39196274501546957\n",
      "Gradient Descent(11/49): loss=0.39056911668966127\n",
      "Gradient Descent(12/49): loss=0.3894076923172243\n",
      "Gradient Descent(13/49): loss=0.3884232727537279\n",
      "Gradient Descent(14/49): loss=0.38757507819327663\n",
      "Gradient Descent(15/49): loss=0.38683286631669783\n",
      "Gradient Descent(16/49): loss=0.38617411506452454\n",
      "Gradient Descent(17/49): loss=0.3855819722613497\n",
      "Gradient Descent(18/49): loss=0.3850437599408121\n",
      "Gradient Descent(19/49): loss=0.3845498810654994\n",
      "Gradient Descent(20/49): loss=0.38409301881392877\n",
      "Gradient Descent(21/49): loss=0.3836675490027815\n",
      "Gradient Descent(22/49): loss=0.3832691080646946\n",
      "Gradient Descent(23/49): loss=0.3828942747599532\n",
      "Gradient Descent(24/49): loss=0.38254033519046565\n",
      "Gradient Descent(25/49): loss=0.3822051089336455\n",
      "Gradient Descent(26/49): loss=0.38188682009930114\n",
      "Gradient Descent(27/49): loss=0.3815840014631698\n",
      "Gradient Descent(28/49): loss=0.38129542299829816\n",
      "Gradient Descent(29/49): loss=0.3810200384355861\n",
      "Gradient Descent(30/49): loss=0.3807569451723416\n",
      "Gradient Descent(31/49): loss=0.38050535408245567\n",
      "Gradient Descent(32/49): loss=0.38026456668669445\n",
      "Gradient Descent(33/49): loss=0.38003395780582966\n",
      "Gradient Descent(34/49): loss=0.3798129623076381\n",
      "Gradient Descent(35/49): loss=0.379601064918364\n",
      "Gradient Descent(36/49): loss=0.3793977923343925\n",
      "Gradient Descent(37/49): loss=0.3792027070657364\n",
      "Gradient Descent(38/49): loss=0.3790154025878111\n",
      "Gradient Descent(39/49): loss=0.37883549948531436\n",
      "Gradient Descent(40/49): loss=0.37866264235167557\n",
      "Gradient Descent(41/49): loss=0.37849649726673873\n",
      "Gradient Descent(42/49): loss=0.37833674971941234\n",
      "Gradient Descent(43/49): loss=0.37818310287487716\n",
      "Gradient Descent(44/49): loss=0.3780352761104888\n",
      "Gradient Descent(45/49): loss=0.3778930037628765\n",
      "Gradient Descent(46/49): loss=0.3777560340425061\n",
      "Gradient Descent(47/49): loss=0.3776241280823183\n",
      "Gradient Descent(48/49): loss=0.3774970590948339\n",
      "Gradient Descent(49/49): loss=0.37737461161799074\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4458506760165108\n",
      "Gradient Descent(2/49): loss=0.42988250074844864\n",
      "Gradient Descent(3/49): loss=0.4192890208066116\n",
      "Gradient Descent(4/49): loss=0.41171653378382467\n",
      "Gradient Descent(5/49): loss=0.40614594028978707\n",
      "Gradient Descent(6/49): loss=0.40197026945633907\n",
      "Gradient Descent(7/49): loss=0.39878738161439636\n",
      "Gradient Descent(8/49): loss=0.3963193177320294\n",
      "Gradient Descent(9/49): loss=0.39437037125233565\n",
      "Gradient Descent(10/49): loss=0.39280145945711603\n",
      "Gradient Descent(11/49): loss=0.3915131664889057\n",
      "Gradient Descent(12/49): loss=0.39043410627972774\n",
      "Gradient Descent(13/49): loss=0.3895127997314927\n",
      "Gradient Descent(14/49): loss=0.38871195672581854\n",
      "Gradient Descent(15/49): loss=0.38800443139653107\n",
      "Gradient Descent(16/49): loss=0.3873703514117326\n",
      "Gradient Descent(17/49): loss=0.38679507493457393\n",
      "Gradient Descent(18/49): loss=0.38626773302150413\n",
      "Gradient Descent(19/49): loss=0.3857801872141098\n",
      "Gradient Descent(20/49): loss=0.38532628228758287\n",
      "Gradient Descent(21/49): loss=0.3849013092959287\n",
      "Gradient Descent(22/49): loss=0.3845016187809018\n",
      "Gradient Descent(23/49): loss=0.3841243414385748\n",
      "Gradient Descent(24/49): loss=0.3837671858484997\n",
      "Gradient Descent(25/49): loss=0.3834282915868013\n",
      "Gradient Descent(26/49): loss=0.3831061222291718\n",
      "Gradient Descent(27/49): loss=0.3827993871472444\n",
      "Gradient Descent(28/49): loss=0.3825069841350987\n",
      "Gradient Descent(29/49): loss=0.3822279571396974\n",
      "Gradient Descent(30/49): loss=0.38196146496942407\n",
      "Gradient Descent(31/49): loss=0.3817067580020526\n",
      "Gradient Descent(32/49): loss=0.3814631607373695\n",
      "Gradient Descent(33/49): loss=0.38123005863252407\n",
      "Gradient Descent(34/49): loss=0.38100688808559957\n",
      "Gradient Descent(35/49): loss=0.3807931287416217\n",
      "Gradient Descent(36/49): loss=0.3805882975186224\n",
      "Gradient Descent(37/49): loss=0.3803919439133508\n",
      "Gradient Descent(38/49): loss=0.3802036462638657\n",
      "Gradient Descent(39/49): loss=0.3800230087318586\n",
      "Gradient Descent(40/49): loss=0.3798496588299723\n",
      "Gradient Descent(41/49): loss=0.3796832453649866\n",
      "Gradient Descent(42/49): loss=0.3795234367011132\n",
      "Gradient Descent(43/49): loss=0.37936991927213026\n",
      "Gradient Descent(44/49): loss=0.37922239628908305\n",
      "Gradient Descent(45/49): loss=0.3790805866035378\n",
      "Gradient Descent(46/49): loss=0.37894422369617325\n",
      "Gradient Descent(47/49): loss=0.37881305476774496\n",
      "Gradient Descent(48/49): loss=0.37868683991485247\n",
      "Gradient Descent(49/49): loss=0.37856535137694736\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4452488348273376\n",
      "Gradient Descent(2/49): loss=0.42875174396872173\n",
      "Gradient Descent(3/49): loss=0.4178492724130568\n",
      "Gradient Descent(4/49): loss=0.4100782545701899\n",
      "Gradient Descent(5/49): loss=0.4043774961161452\n",
      "Gradient Descent(6/49): loss=0.40011640616084415\n",
      "Gradient Descent(7/49): loss=0.39687830613704644\n",
      "Gradient Descent(8/49): loss=0.39437572645364993\n",
      "Gradient Descent(9/49): loss=0.3924065148564253\n",
      "Gradient Descent(10/49): loss=0.390827139776153\n",
      "Gradient Descent(11/49): loss=0.3895350900578799\n",
      "Gradient Descent(12/49): loss=0.38845682100426154\n",
      "Gradient Descent(13/49): loss=0.3875393491511277\n",
      "Gradient Descent(14/49): loss=0.38674433847446493\n",
      "Gradient Descent(15/49): loss=0.3860439184683894\n",
      "Gradient Descent(16/49): loss=0.38541771694249316\n",
      "Gradient Descent(17/49): loss=0.3848507491104965\n",
      "Gradient Descent(18/49): loss=0.384331912311503\n",
      "Gradient Descent(19/49): loss=0.3838529101854625\n",
      "Gradient Descent(20/49): loss=0.3834074820619454\n",
      "Gradient Descent(21/49): loss=0.3829908497222665\n",
      "Gradient Descent(22/49): loss=0.38259931929199753\n",
      "Gradient Descent(23/49): loss=0.38222999406702407\n",
      "Gradient Descent(24/49): loss=0.38188056682748894\n",
      "Gradient Descent(25/49): loss=0.38154916922256443\n",
      "Gradient Descent(26/49): loss=0.3812342622145688\n",
      "Gradient Descent(27/49): loss=0.3809345561243959\n",
      "Gradient Descent(28/49): loss=0.3806489520632597\n",
      "Gradient Descent(29/49): loss=0.3803764988498404\n",
      "Gradient Descent(30/49): loss=0.3801163611661869\n",
      "Gradient Descent(31/49): loss=0.37986779589049097\n",
      "Gradient Descent(32/49): loss=0.37963013439487214\n",
      "Gradient Descent(33/49): loss=0.3794027692072623\n",
      "Gradient Descent(34/49): loss=0.37918514387639296\n",
      "Gradient Descent(35/49): loss=0.37897674519617436\n",
      "Gradient Descent(36/49): loss=0.37877709717503655\n",
      "Gradient Descent(37/49): loss=0.3785857563017472\n",
      "Gradient Descent(38/49): loss=0.3784023077795745\n",
      "Gradient Descent(39/49): loss=0.3782263624880766\n",
      "Gradient Descent(40/49): loss=0.37805755449543677\n",
      "Gradient Descent(41/49): loss=0.37789553899065476\n",
      "Gradient Descent(42/49): loss=0.3777399905388154\n",
      "Gradient Descent(43/49): loss=0.37759060158747476\n",
      "Gradient Descent(44/49): loss=0.37744708117042475\n",
      "Gradient Descent(45/49): loss=0.3773091537684979\n",
      "Gradient Descent(46/49): loss=0.3771765582969599\n",
      "Gradient Descent(47/49): loss=0.3770490471963446\n",
      "Gradient Descent(48/49): loss=0.3769263856090163\n",
      "Gradient Descent(49/49): loss=0.37680835062777657\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4458870333972075\n",
      "Gradient Descent(2/49): loss=0.429817543272394\n",
      "Gradient Descent(3/49): loss=0.4192030510894151\n",
      "Gradient Descent(4/49): loss=0.41161376360515334\n",
      "Gradient Descent(5/49): loss=0.40602698841413276\n",
      "Gradient Descent(6/49): loss=0.40183682914859503\n",
      "Gradient Descent(7/49): loss=0.39864210036208936\n",
      "Gradient Descent(8/49): loss=0.3961652480505674\n",
      "Gradient Descent(9/49): loss=0.394210556414148\n",
      "Gradient Descent(10/49): loss=0.3926386926249888\n",
      "Gradient Descent(11/49): loss=0.3913498781986264\n",
      "Gradient Descent(12/49): loss=0.3902723327223693\n",
      "Gradient Descent(13/49): loss=0.3893541966383155\n",
      "Gradient Descent(14/49): loss=0.38855783590734744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=0.3878558060802728\n",
      "Gradient Descent(16/49): loss=0.3872279825631922\n",
      "Gradient Descent(17/49): loss=0.3866595145155764\n",
      "Gradient Descent(18/49): loss=0.38613936238452296\n",
      "Gradient Descent(19/49): loss=0.3856592501065639\n",
      "Gradient Descent(20/49): loss=0.3852129126210401\n",
      "Gradient Descent(21/49): loss=0.3847955541630845\n",
      "Gradient Descent(22/49): loss=0.38440345732859216\n",
      "Gradient Descent(23/49): loss=0.38403370021965605\n",
      "Gradient Descent(24/49): loss=0.38368395123378085\n",
      "Gradient Descent(25/49): loss=0.38335231975201767\n",
      "Gradient Descent(26/49): loss=0.38303724715888776\n",
      "Gradient Descent(27/49): loss=0.3827374270268706\n",
      "Gradient Descent(28/49): loss=0.3824517464383045\n",
      "Gradient Descent(29/49): loss=0.3821792426630238\n",
      "Gradient Descent(30/49): loss=0.38191907101897354\n",
      "Gradient Descent(31/49): loss=0.3816704808981328\n",
      "Gradient Descent(32/49): loss=0.381432797770947\n",
      "Gradient Descent(33/49): loss=0.3812054095812826\n",
      "Gradient Descent(34/49): loss=0.3809877563763153\n",
      "Gradient Descent(35/49): loss=0.38077932232857314\n",
      "Gradient Descent(36/49): loss=0.38057962953410845\n",
      "Gradient Descent(37/49): loss=0.38038823313543463\n",
      "Gradient Descent(38/49): loss=0.3802047174376861\n",
      "Gradient Descent(39/49): loss=0.38002869277380996\n",
      "Gradient Descent(40/49): loss=0.3798597929384136\n",
      "Gradient Descent(41/49): loss=0.3796976730566058\n",
      "Gradient Descent(42/49): loss=0.37954200778843106\n",
      "Gradient Descent(43/49): loss=0.3793924897946978\n",
      "Gradient Descent(44/49): loss=0.37924882840855656\n",
      "Gradient Descent(45/49): loss=0.3791107484709067\n",
      "Gradient Descent(46/49): loss=0.3789779892978653\n",
      "Gradient Descent(47/49): loss=0.37885030375607776\n",
      "Gradient Descent(48/49): loss=0.3787274574272739\n",
      "Gradient Descent(49/49): loss=0.37860922784767725\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44504045553075916\n",
      "Gradient Descent(2/49): loss=0.42864676083088804\n",
      "Gradient Descent(3/49): loss=0.4178152921289618\n",
      "Gradient Descent(4/49): loss=0.4100878276955108\n",
      "Gradient Descent(5/49): loss=0.4044123387078669\n",
      "Gradient Descent(6/49): loss=0.40016549729194323\n",
      "Gradient Descent(7/49): loss=0.39693516435234016\n",
      "Gradient Descent(8/49): loss=0.39443660229545013\n",
      "Gradient Descent(9/49): loss=0.3924693144389651\n",
      "Gradient Descent(10/49): loss=0.39089078374013425\n",
      "Gradient Descent(11/49): loss=0.38959912148615095\n",
      "Gradient Descent(12/49): loss=0.3885211594616058\n",
      "Gradient Descent(13/49): loss=0.3876041340977693\n",
      "Gradient Descent(14/49): loss=0.3868098292555924\n",
      "Gradient Descent(15/49): loss=0.38611043089272984\n",
      "Gradient Descent(16/49): loss=0.3854855837550501\n",
      "Gradient Descent(17/49): loss=0.3849202960852066\n",
      "Gradient Descent(18/49): loss=0.3844034444991564\n",
      "Gradient Descent(19/49): loss=0.38392670469097223\n",
      "Gradient Descent(20/49): loss=0.3834837849473392\n",
      "Gradient Descent(21/49): loss=0.38306987544981813\n",
      "Gradient Descent(22/49): loss=0.38268125167022765\n",
      "Gradient Descent(23/49): loss=0.38231498802912084\n",
      "Gradient Descent(24/49): loss=0.38196875061602764\n",
      "Gradient Descent(25/49): loss=0.3816406467160721\n",
      "Gradient Descent(26/49): loss=0.3813291152374904\n",
      "Gradient Descent(27/49): loss=0.38103284665067194\n",
      "Gradient Descent(28/49): loss=0.38075072426747036\n",
      "Gradient Descent(29/49): loss=0.3804817809870951\n",
      "Gradient Descent(30/49): loss=0.38022516727833117\n",
      "Gradient Descent(31/49): loss=0.3799801273455633\n",
      "Gradient Descent(32/49): loss=0.3797459812716276\n",
      "Gradient Descent(33/49): loss=0.379522111538677\n",
      "Gradient Descent(34/49): loss=0.37930795276646956\n",
      "Gradient Descent(35/49): loss=0.37910298382382557\n",
      "Gradient Descent(36/49): loss=0.3789067216977655\n",
      "Gradient Descent(37/49): loss=0.3787187166705792\n",
      "Gradient Descent(38/49): loss=0.37853854847537566\n",
      "Gradient Descent(39/49): loss=0.3783658231881359\n",
      "Gradient Descent(40/49): loss=0.3782001706780286\n",
      "Gradient Descent(41/49): loss=0.37804124248427196\n",
      "Gradient Descent(42/49): loss=0.3778887100218558\n",
      "Gradient Descent(43/49): loss=0.3777422630433946\n",
      "Gradient Descent(44/49): loss=0.3776016083027067\n",
      "Gradient Descent(45/49): loss=0.3774664683792215\n",
      "Gradient Descent(46/49): loss=0.37733658063228653\n",
      "Gradient Descent(47/49): loss=0.3772116962618327\n",
      "Gradient Descent(48/49): loss=0.3770915794573394\n",
      "Gradient Descent(49/49): loss=0.3769760066211376\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4447252634273677\n",
      "Gradient Descent(2/49): loss=0.42806962576985946\n",
      "Gradient Descent(3/49): loss=0.4174194545826845\n",
      "Gradient Descent(4/49): loss=0.40992969137226476\n",
      "Gradient Descent(5/49): loss=0.4045060595456595\n",
      "Gradient Descent(6/49): loss=0.4004985490725668\n",
      "Gradient Descent(7/49): loss=0.39748004825961436\n",
      "Gradient Descent(8/49): loss=0.3951599875867261\n",
      "Gradient Descent(9/49): loss=0.3933377392387235\n",
      "Gradient Descent(10/49): loss=0.391873683528381\n",
      "Gradient Descent(11/49): loss=0.39067012097271986\n",
      "Gradient Descent(12/49): loss=0.3896583497264255\n",
      "Gradient Descent(13/49): loss=0.3887898143550065\n",
      "Gradient Descent(14/49): loss=0.3880300061504227\n",
      "Gradient Descent(15/49): loss=0.38735424292738596\n",
      "Gradient Descent(16/49): loss=0.3867447399508473\n",
      "Gradient Descent(17/49): loss=0.38618857116873\n",
      "Gradient Descent(18/49): loss=0.3856762462156469\n",
      "Gradient Descent(19/49): loss=0.385200714469826\n",
      "Gradient Descent(20/49): loss=0.3847566660526424\n",
      "Gradient Descent(21/49): loss=0.3843400398257706\n",
      "Gradient Descent(22/49): loss=0.38394767604816055\n",
      "Gradient Descent(23/49): loss=0.38357707038103106\n",
      "Gradient Descent(24/49): loss=0.38322619907464756\n",
      "Gradient Descent(25/49): loss=0.38289339427586894\n",
      "Gradient Descent(26/49): loss=0.3825772547174913\n",
      "Gradient Descent(27/49): loss=0.38227658145050386\n",
      "Gradient Descent(28/49): loss=0.38199033134999955\n",
      "Gradient Descent(29/49): loss=0.3817175832718701\n",
      "Gradient Descent(30/49): loss=0.38145751324166516\n",
      "Gradient Descent(31/49): loss=0.3812093761135581\n",
      "Gradient Descent(32/49): loss=0.3809724918811265\n",
      "Gradient Descent(33/49): loss=0.38074623534635943\n",
      "Gradient Descent(34/49): loss=0.38053002822427356\n",
      "Gradient Descent(35/49): loss=0.38032333302336435\n",
      "Gradient Descent(36/49): loss=0.3801256482287566\n",
      "Gradient Descent(37/49): loss=0.37993650444773147\n",
      "Gradient Descent(38/49): loss=0.37975546127203025\n",
      "Gradient Descent(39/49): loss=0.37958210467904463\n",
      "Gradient Descent(40/49): loss=0.3794160448425202\n",
      "Gradient Descent(41/49): loss=0.37925691425824387\n",
      "Gradient Descent(42/49): loss=0.3791043661152817\n",
      "Gradient Descent(43/49): loss=0.37895807286145733\n",
      "Gradient Descent(44/49): loss=0.3788177249248883\n",
      "Gradient Descent(45/49): loss=0.3786830295629356\n",
      "Gradient Descent(46/49): loss=0.3785537098168846\n",
      "Gradient Descent(47/49): loss=0.37842950355577065\n",
      "Gradient Descent(48/49): loss=0.3783101625965147\n",
      "Gradient Descent(49/49): loss=0.37819545189030834\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4441175302032689\n",
      "Gradient Descent(2/49): loss=0.4268855569024318\n",
      "Gradient Descent(3/49): loss=0.41592737549811176\n",
      "Gradient Descent(4/49): loss=0.408246648290478\n",
      "Gradient Descent(5/49): loss=0.40270135349640734\n",
      "Gradient Descent(6/49): loss=0.39861668080826357\n",
      "Gradient Descent(7/49): loss=0.3955504617905075\n",
      "Gradient Descent(8/49): loss=0.39320238854409845\n",
      "Gradient Descent(9/49): loss=0.39136533698123976\n",
      "Gradient Descent(10/49): loss=0.3898952942528553\n",
      "Gradient Descent(11/49): loss=0.3886915765670792\n",
      "Gradient Descent(12/49): loss=0.3876834551203542\n",
      "Gradient Descent(13/49): loss=0.38682100124087576\n",
      "Gradient Descent(14/49): loss=0.38606877951677654\n",
      "Gradient Descent(15/49): loss=0.3854014855262556\n",
      "Gradient Descent(16/49): loss=0.38480091932555216\n",
      "Gradient Descent(17/49): loss=0.384253879970447\n",
      "Gradient Descent(18/49): loss=0.38375069697089814\n",
      "Gradient Descent(19/49): loss=0.3832842033461207\n",
      "Gradient Descent(20/49): loss=0.3828490155949643\n",
      "Gradient Descent(21/49): loss=0.38244102747900316\n",
      "Gradient Descent(22/49): loss=0.38205705310688487\n",
      "Gradient Descent(23/49): loss=0.3816945745164195\n",
      "Gradient Descent(24/49): loss=0.38135156256784264\n",
      "Gradient Descent(25/49): loss=0.38102634939175145\n",
      "Gradient Descent(26/49): loss=0.38071753718042695\n",
      "Gradient Descent(27/49): loss=0.3804239326641488\n",
      "Gradient Descent(28/49): loss=0.38014449978800074\n",
      "Gradient Descent(29/49): loss=0.3798783253219495\n",
      "Gradient Descent(30/49): loss=0.37962459368922563\n",
      "Gradient Descent(31/49): loss=0.3793825683869858\n",
      "Gradient Descent(32/49): loss=0.37915157813872247\n",
      "Gradient Descent(33/49): loss=0.3789310064571103\n",
      "Gradient Descent(34/49): loss=0.37872028367659116\n",
      "Gradient Descent(35/49): loss=0.3785188807842004\n",
      "Gradient Descent(36/49): loss=0.37832630456794925\n",
      "Gradient Descent(37/49): loss=0.3781420937376036\n",
      "Gradient Descent(38/49): loss=0.37796581576916743\n",
      "Gradient Descent(39/49): loss=0.3777970642932103\n",
      "Gradient Descent(40/49): loss=0.37763545689639444\n",
      "Gradient Descent(41/49): loss=0.37748063324084496\n",
      "Gradient Descent(42/49): loss=0.37733225343137555\n",
      "Gradient Descent(43/49): loss=0.3771899965788681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=0.37705355952133274\n",
      "Gradient Descent(45/49): loss=0.37692265567377764\n",
      "Gradient Descent(46/49): loss=0.37679701398501625\n",
      "Gradient Descent(47/49): loss=0.3766763779846558\n",
      "Gradient Descent(48/49): loss=0.3765605049072891\n",
      "Gradient Descent(49/49): loss=0.3764491648836883\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44478439297301287\n",
      "Gradient Descent(2/49): loss=0.42800330964869876\n",
      "Gradient Descent(3/49): loss=0.41733013104157274\n",
      "Gradient Descent(4/49): loss=0.4098224846863665\n",
      "Gradient Descent(5/49): loss=0.4043818609072079\n",
      "Gradient Descent(6/49): loss=0.40035971669267967\n",
      "Gradient Descent(7/49): loss=0.3973298889683652\n",
      "Gradient Descent(8/49): loss=0.39500205986888537\n",
      "Gradient Descent(9/49): loss=0.3931754136900322\n",
      "Gradient Descent(10/49): loss=0.3917099244512562\n",
      "Gradient Descent(11/49): loss=0.39050741040584186\n",
      "Gradient Descent(12/49): loss=0.3894986916001193\n",
      "Gradient Descent(13/49): loss=0.3886347781538481\n",
      "Gradient Descent(14/49): loss=0.38788078606907733\n",
      "Gradient Descent(15/49): loss=0.38721171938054244\n",
      "Gradient Descent(16/49): loss=0.38660953688769606\n",
      "Gradient Descent(17/49): loss=0.3860611064248099\n",
      "Gradient Descent(18/49): loss=0.38555677419319295\n",
      "Gradient Descent(19/49): loss=0.38508936147998185\n",
      "Gradient Descent(20/49): loss=0.3846534591172238\n",
      "Gradient Descent(21/49): loss=0.38424492988511133\n",
      "Gradient Descent(22/49): loss=0.3838605565081167\n",
      "Gradient Descent(23/49): loss=0.3834977918435055\n",
      "Gradient Descent(24/49): loss=0.38315458097982824\n",
      "Gradient Descent(25/49): loss=0.3828292340657045\n",
      "Gradient Descent(26/49): loss=0.3825203350206944\n",
      "Gradient Descent(27/49): loss=0.3822266756943192\n",
      "Gradient Descent(28/49): loss=0.38194720812402755\n",
      "Gradient Descent(29/49): loss=0.38168100970348573\n",
      "Gradient Descent(30/49): loss=0.38142725758928275\n",
      "Gradient Descent(31/49): loss=0.38118520974125547\n",
      "Gradient Descent(32/49): loss=0.3809541907440972\n",
      "Gradient Descent(33/49): loss=0.3807335810896682\n",
      "Gradient Descent(34/49): loss=0.38052280897605334\n",
      "Gradient Descent(35/49): loss=0.38032134394674966\n",
      "Gradient Descent(36/49): loss=0.3801286918835627\n",
      "Gradient Descent(37/49): loss=0.3799443910024065\n",
      "Gradient Descent(38/49): loss=0.3797680085981283\n",
      "Gradient Descent(39/49): loss=0.3795991383539247\n",
      "Gradient Descent(40/49): loss=0.3794373980807891\n",
      "Gradient Descent(41/49): loss=0.3792824277883461\n",
      "Gradient Descent(42/49): loss=0.3791338880143673\n",
      "Gradient Descent(43/49): loss=0.37899145835904446\n",
      "Gradient Descent(44/49): loss=0.3788548361837517\n",
      "Gradient Descent(45/49): loss=0.3787237354439786\n",
      "Gradient Descent(46/49): loss=0.37859788563340446\n",
      "Gradient Descent(47/49): loss=0.37847703082144624\n",
      "Gradient Descent(48/49): loss=0.3783609287705663\n",
      "Gradient Descent(49/49): loss=0.3782493501225738\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44389592236273856\n",
      "Gradient Descent(2/49): loss=0.4267914061527655\n",
      "Gradient Descent(3/49): loss=0.4159054544356194\n",
      "Gradient Descent(4/49): loss=0.408265618680727\n",
      "Gradient Descent(5/49): loss=0.4027427783572759\n",
      "Gradient Descent(6/49): loss=0.398670067767631\n",
      "Gradient Descent(7/49): loss=0.39560992523996813\n",
      "Gradient Descent(8/49): loss=0.39326470414558945\n",
      "Gradient Descent(9/49): loss=0.3914288440423541\n",
      "Gradient Descent(10/49): loss=0.3899592594040828\n",
      "Gradient Descent(11/49): loss=0.38875581166413026\n",
      "Gradient Descent(12/49): loss=0.3877480824907858\n",
      "Gradient Descent(13/49): loss=0.386886308385267\n",
      "Gradient Descent(14/49): loss=0.3861351297263901\n",
      "Gradient Descent(15/49): loss=0.3854692639138673\n",
      "Gradient Descent(16/49): loss=0.3848705013208685\n",
      "Gradient Descent(17/49): loss=0.38432561397413506\n",
      "Gradient Descent(18/49): loss=0.38382489581374835\n",
      "Gradient Descent(19/49): loss=0.3833611411092844\n",
      "Gradient Descent(20/49): loss=0.3829289275899786\n",
      "Gradient Descent(21/49): loss=0.3825241119955295\n",
      "Gradient Descent(22/49): loss=0.38214347406130933\n",
      "Gradient Descent(23/49): loss=0.3817844644733087\n",
      "Gradient Descent(24/49): loss=0.3814450258229456\n",
      "Gradient Descent(25/49): loss=0.38112346494214355\n",
      "Gradient Descent(26/49): loss=0.38081836149246023\n",
      "Gradient Descent(27/49): loss=0.3805285022015\n",
      "Gradient Descent(28/49): loss=0.3802528332923849\n",
      "Gradient Descent(29/49): loss=0.37999042585587633\n",
      "Gradient Descent(30/49): loss=0.3797404504586637\n",
      "Gradient Descent(31/49): loss=0.3795021583652526\n",
      "Gradient Descent(32/49): loss=0.37927486751345924\n",
      "Gradient Descent(33/49): loss=0.3790579519211445\n",
      "Gradient Descent(34/49): loss=0.378850833581647\n",
      "Gradient Descent(35/49): loss=0.37865297617429233\n",
      "Gradient Descent(36/49): loss=0.3784638801071491\n",
      "Gradient Descent(37/49): loss=0.3782830785448629\n",
      "Gradient Descent(38/49): loss=0.37811013417108275\n",
      "Gradient Descent(39/49): loss=0.37794463650404586\n",
      "Gradient Descent(40/49): loss=0.3777861996333349\n",
      "Gradient Descent(41/49): loss=0.3776344602813151\n",
      "Gradient Descent(42/49): loss=0.37748907611830956\n",
      "Gradient Descent(43/49): loss=0.37734972427901786\n",
      "Gradient Descent(44/49): loss=0.3772161000410415\n",
      "Gradient Descent(45/49): loss=0.377087915636098\n",
      "Gradient Descent(46/49): loss=0.3769648991715861\n",
      "Gradient Descent(47/49): loss=0.3768467936453694\n",
      "Gradient Descent(48/49): loss=0.3767333560404678\n",
      "Gradient Descent(49/49): loss=0.37662435648919074\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4439104186311106\n",
      "Gradient Descent(2/49): loss=0.42638065123738256\n",
      "Gradient Descent(3/49): loss=0.41566732392771727\n",
      "Gradient Descent(4/49): loss=0.4082794201687417\n",
      "Gradient Descent(5/49): loss=0.403014653397476\n",
      "Gradient Descent(6/49): loss=0.39917799751186855\n",
      "Gradient Descent(7/49): loss=0.3963193168435626\n",
      "Gradient Descent(8/49): loss=0.3941379454090466\n",
      "Gradient Descent(9/49): loss=0.39243056029716017\n",
      "Gradient Descent(10/49): loss=0.3910586876183471\n",
      "Gradient Descent(11/49): loss=0.3899274821269258\n",
      "Gradient Descent(12/49): loss=0.3889716149434743\n",
      "Gradient Descent(13/49): loss=0.38814581006469856\n",
      "Gradient Descent(14/49): loss=0.38741846723303014\n",
      "Gradient Descent(15/49): loss=0.38676734868586665\n",
      "Gradient Descent(16/49): loss=0.38617665220399044\n",
      "Gradient Descent(17/49): loss=0.38563501857993315\n",
      "Gradient Descent(18/49): loss=0.3851341708954832\n",
      "Gradient Descent(19/49): loss=0.38466798227608395\n",
      "Gradient Descent(20/49): loss=0.3842318350783296\n",
      "Gradient Descent(21/49): loss=0.38382217887293263\n",
      "Gradient Descent(22/49): loss=0.3834362244233323\n",
      "Gradient Descent(23/49): loss=0.383071730967672\n",
      "Gradient Descent(24/49): loss=0.38272685770099013\n",
      "Gradient Descent(25/49): loss=0.3824000595639722\n",
      "Gradient Descent(26/49): loss=0.3820900137031365\n",
      "Gradient Descent(27/49): loss=0.3817955672318942\n",
      "Gradient Descent(28/49): loss=0.3815156998354838\n",
      "Gradient Descent(29/49): loss=0.38124949675849845\n",
      "Gradient Descent(30/49): loss=0.38099612908423086\n",
      "Gradient Descent(31/49): loss=0.38075483915860076\n",
      "Gradient Descent(32/49): loss=0.3805249296626354\n",
      "Gradient Descent(33/49): loss=0.38030575528804056\n",
      "Gradient Descent(34/49): loss=0.38009671628292196\n",
      "Gradient Descent(35/49): loss=0.37989725335202806\n",
      "Gradient Descent(36/49): loss=0.3797068435473838\n",
      "Gradient Descent(37/49): loss=0.3795249968910762\n",
      "Gradient Descent(38/49): loss=0.3793512535461683\n",
      "Gradient Descent(39/49): loss=0.37918518140388063\n",
      "Gradient Descent(40/49): loss=0.3790263739919608\n",
      "Gradient Descent(41/49): loss=0.37887444863517256\n",
      "Gradient Descent(42/49): loss=0.37872904481730635\n",
      "Gradient Descent(43/49): loss=0.378589822707274\n",
      "Gradient Descent(44/49): loss=0.3784564618212794\n",
      "Gradient Descent(45/49): loss=0.3783286597998335\n",
      "Gradient Descent(46/49): loss=0.37820613128329644\n",
      "Gradient Descent(47/49): loss=0.3780886068732042\n",
      "Gradient Descent(48/49): loss=0.37797583216926733\n",
      "Gradient Descent(49/49): loss=0.37786756687387263\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4433010722109639\n",
      "Gradient Descent(2/49): loss=0.4251517305396542\n",
      "Gradient Descent(3/49): loss=0.41412787423773784\n",
      "Gradient Descent(4/49): loss=0.4065563876960534\n",
      "Gradient Descent(5/49): loss=0.4011784795318303\n",
      "Gradient Descent(6/49): loss=0.3972727347167351\n",
      "Gradient Descent(7/49): loss=0.39437346342851265\n",
      "Gradient Descent(8/49): loss=0.39217009474061043\n",
      "Gradient Descent(9/49): loss=0.3904528203253498\n",
      "Gradient Descent(10/49): loss=0.3890788797471421\n",
      "Gradient Descent(11/49): loss=0.38795059071607635\n",
      "Gradient Descent(12/49): loss=0.3870007510459367\n",
      "Gradient Descent(13/49): loss=0.3861828530161804\n",
      "Gradient Descent(14/49): loss=0.3854644909945076\n",
      "Gradient Descent(15/49): loss=0.38482290425785237\n",
      "Gradient Descent(16/49): loss=0.3842419539981074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=0.3837100669067433\n",
      "Gradient Descent(18/49): loss=0.3832188321168064\n",
      "Gradient Descent(19/49): loss=0.38276204101296624\n",
      "Gradient Descent(20/49): loss=0.38233502805008335\n",
      "Gradient Descent(21/49): loss=0.3819342167130038\n",
      "Gradient Descent(22/49): loss=0.3815568056597006\n",
      "Gradient Descent(23/49): loss=0.3812005509191019\n",
      "Gradient Descent(24/49): loss=0.3808636140883056\n",
      "Gradient Descent(25/49): loss=0.38054445600712417\n",
      "Gradient Descent(26/49): loss=0.3802417618620186\n",
      "Gradient Descent(27/49): loss=0.37995438807903237\n",
      "Gradient Descent(28/49): loss=0.3796813243733128\n",
      "Gradient Descent(29/49): loss=0.3794216663805747\n",
      "Gradient Descent(30/49): loss=0.3791745957069414\n",
      "Gradient Descent(31/49): loss=0.37893936520350635\n",
      "Gradient Descent(32/49): loss=0.3787152879402032\n",
      "Gradient Descent(33/49): loss=0.37850172881506156\n",
      "Gradient Descent(34/49): loss=0.37829809805439724\n",
      "Gradient Descent(35/49): loss=0.37810384608117603\n",
      "Gradient Descent(36/49): loss=0.37791845938302326\n",
      "Gradient Descent(37/49): loss=0.3777414571189217\n",
      "Gradient Descent(38/49): loss=0.3775723882788832\n",
      "Gradient Descent(39/49): loss=0.37741082926365754\n",
      "Gradient Descent(40/49): loss=0.37725638178868265\n",
      "Gradient Descent(41/49): loss=0.37710867104270474\n",
      "Gradient Descent(42/49): loss=0.37696734405009374\n",
      "Gradient Descent(43/49): loss=0.37683206819910503\n",
      "Gradient Descent(44/49): loss=0.3767025299078182\n",
      "Gradient Descent(45/49): loss=0.3765784334062853\n",
      "Gradient Descent(46/49): loss=0.3764594996183572\n",
      "Gradient Descent(47/49): loss=0.37634546513025324\n",
      "Gradient Descent(48/49): loss=0.3762360812355806\n",
      "Gradient Descent(49/49): loss=0.3761311130484757\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4439948638841506\n",
      "Gradient Descent(2/49): loss=0.42631942088979935\n",
      "Gradient Descent(3/49): loss=0.4155755176152406\n",
      "Gradient Descent(4/49): loss=0.4081678381884549\n",
      "Gradient Descent(5/49): loss=0.4028853547786826\n",
      "Gradient Descent(6/49): loss=0.3990341571260695\n",
      "Gradient Descent(7/49): loss=0.396164907347082\n",
      "Gradient Descent(8/49): loss=0.3939769838735067\n",
      "Gradient Descent(9/49): loss=0.39226667672892185\n",
      "Gradient Descent(10/49): loss=0.3908949479760838\n",
      "Gradient Descent(11/49): loss=0.389766358807189\n",
      "Gradient Descent(12/49): loss=0.38881503076724305\n",
      "Gradient Descent(13/49): loss=0.3879952122178149\n",
      "Gradient Descent(14/49): loss=0.38727490779231666\n",
      "Gradient Descent(15/49): loss=0.38663156035311474\n",
      "Gradient Descent(16/49): loss=0.3860491145466202\n",
      "Gradient Descent(17/49): loss=0.38551601358180226\n",
      "Gradient Descent(18/49): loss=0.3850238283217407\n",
      "Gradient Descent(19/49): loss=0.3845663160702011\n",
      "Gradient Descent(20/49): loss=0.3841387722091288\n",
      "Gradient Descent(21/49): loss=0.3837375819988174\n",
      "Gradient Descent(22/49): loss=0.38335990958357496\n",
      "Gradient Descent(23/49): loss=0.38300348132142004\n",
      "Gradient Descent(24/49): loss=0.38266643415016705\n",
      "Gradient Descent(25/49): loss=0.3823472089322729\n",
      "Gradient Descent(26/49): loss=0.38204447500478217\n",
      "Gradient Descent(27/49): loss=0.38175707645034784\n",
      "Gradient Descent(28/49): loss=0.3814839935412358\n",
      "Gradient Descent(29/49): loss=0.3812243148228606\n",
      "Gradient Descent(30/49): loss=0.38097721668940304\n",
      "Gradient Descent(31/49): loss=0.38074194826002034\n",
      "Gradient Descent(32/49): loss=0.3805178200251872\n",
      "Gradient Descent(33/49): loss=0.3803041951909648\n",
      "Gradient Descent(34/49): loss=0.38010048296750043\n",
      "Gradient Descent(35/49): loss=0.3799061332700179\n",
      "Gradient Descent(36/49): loss=0.37972063245564286\n",
      "Gradient Descent(37/49): loss=0.37954349982807345\n",
      "Gradient Descent(38/49): loss=0.37937428471846596\n",
      "Gradient Descent(39/49): loss=0.3792125640047251\n",
      "Gradient Descent(40/49): loss=0.379057939969453\n",
      "Gradient Descent(41/49): loss=0.3789100384238176\n",
      "Gradient Descent(42/49): loss=0.37876850704384835\n",
      "Gradient Descent(43/49): loss=0.3786330138794295\n",
      "Gradient Descent(44/49): loss=0.37850324600616375\n",
      "Gradient Descent(45/49): loss=0.3783789082974282\n",
      "Gradient Descent(46/49): loss=0.3782597222991485\n",
      "Gradient Descent(47/49): loss=0.3781454251936258\n",
      "Gradient Descent(48/49): loss=0.37803576884156004\n",
      "Gradient Descent(49/49): loss=0.3779305188935077\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4430663187526256\n",
      "Gradient Descent(2/49): loss=0.42506615122878677\n",
      "Gradient Descent(3/49): loss=0.4141166810762428\n",
      "Gradient Descent(4/49): loss=0.4065834374270561\n",
      "Gradient Descent(5/49): loss=0.4012252811670695\n",
      "Gradient Descent(6/49): loss=0.3973294197385498\n",
      "Gradient Descent(7/49): loss=0.3944347662001683\n",
      "Gradient Descent(8/49): loss=0.39223330953799906\n",
      "Gradient Descent(9/49): loss=0.39051670045075215\n",
      "Gradient Descent(10/49): loss=0.3891430120504872\n",
      "Gradient Descent(11/49): loss=0.38801502677397803\n",
      "Gradient Descent(12/49): loss=0.3870657862417107\n",
      "Gradient Descent(13/49): loss=0.38624889455581285\n",
      "Gradient Descent(14/49): loss=0.3855319804643399\n",
      "Gradient Descent(15/49): loss=0.38489227375920415\n",
      "Gradient Descent(16/49): loss=0.38431360281678506\n",
      "Gradient Descent(17/49): loss=0.38378435055524696\n",
      "Gradient Descent(18/49): loss=0.38329605865268634\n",
      "Gradient Descent(19/49): loss=0.3828424714756776\n",
      "Gradient Descent(20/49): loss=0.3824188790847271\n",
      "Gradient Descent(21/49): loss=0.38202166422127043\n",
      "Gradient Descent(22/49): loss=0.38164798880068573\n",
      "Gradient Descent(23/49): loss=0.38129557608076947\n",
      "Gradient Descent(24/49): loss=0.3809625586318064\n",
      "Gradient Descent(25/49): loss=0.3806473716942128\n",
      "Gradient Descent(26/49): loss=0.38034867793812355\n",
      "Gradient Descent(27/49): loss=0.3800653140187274\n",
      "Gradient Descent(28/49): loss=0.3797962523121438\n",
      "Gradient Descent(29/49): loss=0.37954057326440876\n",
      "Gradient Descent(30/49): loss=0.3792974451915527\n",
      "Gradient Descent(31/49): loss=0.3790661093356447\n",
      "Gradient Descent(32/49): loss=0.37884586864849645\n",
      "Gradient Descent(33/49): loss=0.3786360792356869\n",
      "Gradient Descent(34/49): loss=0.37843614371304707\n",
      "Gradient Descent(35/49): loss=0.37824550594967854\n",
      "Gradient Descent(36/49): loss=0.3780636468261838\n",
      "Gradient Descent(37/49): loss=0.3778900807447506\n",
      "Gradient Descent(38/49): loss=0.3777243527033523\n",
      "Gradient Descent(39/49): loss=0.37756603579944437\n",
      "Gradient Descent(40/49): loss=0.3774147290659718\n",
      "Gradient Descent(41/49): loss=0.377270055568973\n",
      "Gradient Descent(42/49): loss=0.37713166071486776\n",
      "Gradient Descent(43/49): loss=0.3769992107289113\n",
      "Gradient Descent(44/49): loss=0.37687239127590466\n",
      "Gradient Descent(45/49): loss=0.3767509062011712\n",
      "Gradient Descent(46/49): loss=0.3766344763748232\n",
      "Gradient Descent(47/49): loss=0.37652283862601366\n",
      "Gradient Descent(48/49): loss=0.37641574475656187\n",
      "Gradient Descent(49/49): loss=0.3763129606253563\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44340614162773995\n",
      "Gradient Descent(2/49): loss=0.4248503091322522\n",
      "Gradient Descent(3/49): loss=0.4140315361742767\n",
      "Gradient Descent(4/49): loss=0.4067551118917242\n",
      "Gradient Descent(5/49): loss=0.40165709842687847\n",
      "Gradient Descent(6/49): loss=0.39799105859175693\n",
      "Gradient Descent(7/49): loss=0.3952856228514454\n",
      "Gradient Descent(8/49): loss=0.39323263147523324\n",
      "Gradient Descent(9/49): loss=0.3916282098358024\n",
      "Gradient Descent(10/49): loss=0.3903365211913349\n",
      "Gradient Descent(11/49): loss=0.3892664907623677\n",
      "Gradient Descent(12/49): loss=0.3883566458550112\n",
      "Gradient Descent(13/49): loss=0.3875651767672626\n",
      "Gradient Descent(14/49): loss=0.3868633915034336\n",
      "Gradient Descent(15/49): loss=0.38623138897306625\n",
      "Gradient Descent(16/49): loss=0.38565518830929657\n",
      "Gradient Descent(17/49): loss=0.3851248173711906\n",
      "Gradient Descent(18/49): loss=0.3846330352564523\n",
      "Gradient Descent(19/49): loss=0.3841744752962558\n",
      "Gradient Descent(20/49): loss=0.3837450678442693\n",
      "Gradient Descent(21/49): loss=0.3833416498597288\n",
      "Gradient Descent(22/49): loss=0.3829616996097465\n",
      "Gradient Descent(23/49): loss=0.38260315546046114\n",
      "Gradient Descent(24/49): loss=0.3822642913755586\n",
      "Gradient Descent(25/49): loss=0.3819436307928966\n",
      "Gradient Descent(26/49): loss=0.38163988657196773\n",
      "Gradient Descent(27/49): loss=0.38135191872316315\n",
      "Gradient Descent(28/49): loss=0.38107870431888935\n",
      "Gradient Descent(29/49): loss=0.3808193157914479\n",
      "Gradient Descent(30/49): loss=0.3805729050374127\n",
      "Gradient Descent(31/49): loss=0.3803386915682841\n",
      "Gradient Descent(32/49): loss=0.38011595350229677\n",
      "Gradient Descent(33/49): loss=0.3799040205691083\n",
      "Gradient Descent(34/49): loss=0.37970226855566636\n",
      "Gradient Descent(35/49): loss=0.37951011479675534\n",
      "Gradient Descent(36/49): loss=0.37932701443374206\n",
      "Gradient Descent(37/49): loss=0.37915245724750857\n",
      "Gradient Descent(38/49): loss=0.37898596492844583\n",
      "Gradient Descent(39/49): loss=0.3788270886857496\n",
      "Gradient Descent(40/49): loss=0.37867540712563164\n",
      "Gradient Descent(41/49): loss=0.3785305243471767\n",
      "Gradient Descent(42/49): loss=0.378392068217992\n",
      "Gradient Descent(43/49): loss=0.3782596888012848\n",
      "Gradient Descent(44/49): loss=0.3781330569127401\n",
      "Gradient Descent(45/49): loss=0.37801186279041493\n",
      "Gradient Descent(46/49): loss=0.3778958148643657\n",
      "Gradient Descent(47/49): loss=0.3777846386152892\n",
      "Gradient Descent(48/49): loss=0.37767807551336\n",
      "Gradient Descent(49/49): loss=0.3775758820298742\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4427994608504225\n",
      "Gradient Descent(2/49): loss=0.4235883481742735\n",
      "Gradient Descent(3/49): loss=0.4124508416240597\n",
      "Gradient Descent(4/49): loss=0.4049966680487476\n",
      "Gradient Descent(5/49): loss=0.3997937308631518\n",
      "Gradient Descent(6/49): loss=0.39606639906223595\n",
      "Gradient Descent(7/49): loss=0.3933270930442245\n",
      "Gradient Descent(8/49): loss=0.3912576209640408\n",
      "Gradient Descent(9/49): loss=0.3896476925977919\n",
      "Gradient Descent(10/49): loss=0.3883573379817471\n",
      "Gradient Descent(11/49): loss=0.38729282225166417\n",
      "Gradient Descent(12/49): loss=0.3863909679547141\n",
      "Gradient Descent(13/49): loss=0.3856088793524247\n",
      "Gradient Descent(14/49): loss=0.3849171777550225\n",
      "Gradient Descent(15/49): loss=0.3842955320244607\n",
      "Gradient Descent(16/49): loss=0.3837296954129914\n",
      "Gradient Descent(17/49): loss=0.38320953438601557\n",
      "Gradient Descent(18/49): loss=0.38272771279841644\n",
      "Gradient Descent(19/49): loss=0.38227881038096834\n",
      "Gradient Descent(20/49): loss=0.3818587299385522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=0.3814642970653287\n",
      "Gradient Descent(22/49): loss=0.3810929886329139\n",
      "Gradient Descent(23/49): loss=0.3807427476874045\n",
      "Gradient Descent(24/49): loss=0.3804118565184077\n",
      "Gradient Descent(25/49): loss=0.3800988490252373\n",
      "Gradient Descent(26/49): loss=0.37980244972710986\n",
      "Gradient Descent(27/49): loss=0.3795215309104162\n",
      "Gradient Descent(28/49): loss=0.3792550821768606\n",
      "Gradient Descent(29/49): loss=0.3790021885128171\n",
      "Gradient Descent(30/49): loss=0.3787620142476163\n",
      "Gradient Descent(31/49): loss=0.37853379110881213\n",
      "Gradient Descent(32/49): loss=0.3783168091501362\n",
      "Gradient Descent(33/49): loss=0.3781104097123754\n",
      "Gradient Descent(34/49): loss=0.37791397983862524\n",
      "Gradient Descent(35/49): loss=0.3777269477433454\n",
      "Gradient Descent(36/49): loss=0.37754877905628387\n",
      "Gradient Descent(37/49): loss=0.37737897364574813\n",
      "Gradient Descent(38/49): loss=0.3772170628831128\n",
      "Gradient Descent(39/49): loss=0.3770626072501196\n",
      "Gradient Descent(40/49): loss=0.3769151942180628\n",
      "Gradient Descent(41/49): loss=0.3767744363471549\n",
      "Gradient Descent(42/49): loss=0.37663996956785256\n",
      "Gradient Descent(43/49): loss=0.3765114516154306\n",
      "Gradient Descent(44/49): loss=0.37638856059587705\n",
      "Gradient Descent(45/49): loss=0.3762709936660395\n",
      "Gradient Descent(46/49): loss=0.37615846581448353\n",
      "Gradient Descent(47/49): loss=0.3760507087321194\n",
      "Gradient Descent(48/49): loss=0.3759474697635688\n",
      "Gradient Descent(49/49): loss=0.3758485109317098\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44351844613062064\n",
      "Gradient Descent(2/49): loss=0.4248035086422634\n",
      "Gradient Descent(3/49): loss=0.4139395691994949\n",
      "Gradient Descent(4/49): loss=0.40663951670643145\n",
      "Gradient Descent(5/49): loss=0.40152292908569814\n",
      "Gradient Descent(6/49): loss=0.3978426407526909\n",
      "Gradient Descent(7/49): loss=0.3951276088998324\n",
      "Gradient Descent(8/49): loss=0.3930694397163591\n",
      "Gradient Descent(9/49): loss=0.39146365790715126\n",
      "Gradient Descent(10/49): loss=0.3901737099438606\n",
      "Gradient Descent(11/49): loss=0.3891078285988572\n",
      "Gradient Descent(12/49): loss=0.38820393423513033\n",
      "Gradient Descent(13/49): loss=0.3874197133545101\n",
      "Gradient Descent(14/49): loss=0.38672606987423824\n",
      "Gradient Descent(15/49): loss=0.3861027861533483\n",
      "Gradient Descent(16/49): loss=0.3855356377139124\n",
      "Gradient Descent(17/49): loss=0.38501446769064523\n",
      "Gradient Descent(18/49): loss=0.3845318970402645\n",
      "Gradient Descent(19/49): loss=0.3840824573023993\n",
      "Gradient Descent(20/49): loss=0.383662005133277\n",
      "Gradient Descent(21/49): loss=0.3832673253574407\n",
      "Gradient Descent(22/49): loss=0.38289586056711983\n",
      "Gradient Descent(23/49): loss=0.38254552595875563\n",
      "Gradient Descent(24/49): loss=0.38221458178258844\n",
      "Gradient Descent(25/49): loss=0.38190154487597106\n",
      "Gradient Descent(26/49): loss=0.381605126812996\n",
      "Gradient Descent(27/49): loss=0.3813241902555985\n",
      "Gradient Descent(28/49): loss=0.381057717808576\n",
      "Gradient Descent(29/49): loss=0.38080478950827285\n",
      "Gradient Descent(30/49): loss=0.38056456630708174\n",
      "Gradient Descent(31/49): loss=0.3803362777495206\n",
      "Gradient Descent(32/49): loss=0.3801192126011622\n",
      "Gradient Descent(33/49): loss=0.3799127115764746\n",
      "Gradient Descent(34/49): loss=0.3797161615742466\n",
      "Gradient Descent(35/49): loss=0.3795289910090678\n",
      "Gradient Descent(36/49): loss=0.37935066595083394\n",
      "Gradient Descent(37/49): loss=0.37918068686937834\n",
      "Gradient Descent(38/49): loss=0.3790185858402297\n",
      "Gradient Descent(39/49): loss=0.37886392410841396\n",
      "Gradient Descent(40/49): loss=0.3787162899357699\n",
      "Gradient Descent(41/49): loss=0.37857529667727413\n",
      "Gradient Descent(42/49): loss=0.378440581045991\n",
      "Gradient Descent(43/49): loss=0.3783118015362878\n",
      "Gradient Descent(44/49): loss=0.37818863698212357\n",
      "Gradient Descent(45/49): loss=0.37807078523238324\n",
      "Gradient Descent(46/49): loss=0.3779579619289923\n",
      "Gradient Descent(47/49): loss=0.3778498993763148\n",
      "Gradient Descent(48/49): loss=0.37774634549239555\n",
      "Gradient Descent(49/49): loss=0.37764706283416577\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4425516447004206\n",
      "Gradient Descent(2/49): loss=0.42350758870452015\n",
      "Gradient Descent(3/49): loss=0.41244842574986\n",
      "Gradient Descent(4/49): loss=0.4050304900932781\n",
      "Gradient Descent(5/49): loss=0.39984486943442343\n",
      "Gradient Descent(6/49): loss=0.3961255720275628\n",
      "Gradient Descent(7/49): loss=0.39338964696251727\n",
      "Gradient Descent(8/49): loss=0.3913213529546482\n",
      "Gradient Descent(9/49): loss=0.38971174651935464\n",
      "Gradient Descent(10/49): loss=0.3884215926172589\n",
      "Gradient Descent(11/49): loss=0.38735753879889134\n",
      "Gradient Descent(12/49): loss=0.386456585569692\n",
      "Gradient Descent(13/49): loss=0.38567589827422494\n",
      "Gradient Descent(14/49): loss=0.384986094729812\n",
      "Gradient Descent(15/49): loss=0.384366806797726\n",
      "Gradient Descent(16/49): loss=0.3838037353950659\n",
      "Gradient Descent(17/49): loss=0.38328668967356866\n",
      "Gradient Descent(18/49): loss=0.3828082768531456\n",
      "Gradient Descent(19/49): loss=0.3823630235816054\n",
      "Gradient Descent(20/49): loss=0.38194678439202595\n",
      "Gradient Descent(21/49): loss=0.38155634177115755\n",
      "Gradient Descent(22/49): loss=0.38118913451814224\n",
      "Gradient Descent(23/49): loss=0.38084307227764536\n",
      "Gradient Descent(24/49): loss=0.380516408152268\n",
      "Gradient Descent(25/49): loss=0.38020765059698974\n",
      "Gradient Descent(26/49): loss=0.379915501982124\n",
      "Gradient Descent(27/49): loss=0.3796388153355715\n",
      "Gradient Descent(28/49): loss=0.3793765635336335\n",
      "Gradient Descent(29/49): loss=0.37912781705974485\n",
      "Gradient Descent(30/49): loss=0.37889172769477125\n",
      "Gradient Descent(31/49): loss=0.37866751634168605\n",
      "Gradient Descent(32/49): loss=0.3784544637549661\n",
      "Gradient Descent(33/49): loss=0.3782519033299651\n",
      "Gradient Descent(34/49): loss=0.37805921536933257\n",
      "Gradient Descent(35/49): loss=0.37787582242218815\n",
      "Gradient Descent(36/49): loss=0.3777011854140148\n",
      "Gradient Descent(37/49): loss=0.37753480036921294\n",
      "Gradient Descent(38/49): loss=0.3773761955861318\n",
      "Gradient Descent(39/49): loss=0.37722492916446093\n",
      "Gradient Descent(40/49): loss=0.37708058681271456\n",
      "Gradient Descent(41/49): loss=0.3769427798830061\n",
      "Gradient Descent(42/49): loss=0.3768111435939871\n",
      "Gradient Descent(43/49): loss=0.3766853354125075\n",
      "Gradient Descent(44/49): loss=0.37656503357145227\n",
      "Gradient Descent(45/49): loss=0.3764499357061737\n",
      "Gradient Descent(46/49): loss=0.37633975759554633\n",
      "Gradient Descent(47/49): loss=0.3762342319963259\n",
      "Gradient Descent(48/49): loss=0.37613310756146334\n",
      "Gradient Descent(49/49): loss=0.3760361478345258\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4432124324172553\n",
      "Gradient Descent(2/49): loss=0.4235302183861202\n",
      "Gradient Descent(3/49): loss=0.4125242549524964\n",
      "Gradient Descent(4/49): loss=0.4053506049568127\n",
      "Gradient Descent(5/49): loss=0.40042094368026415\n",
      "Gradient Descent(6/49): loss=0.3969223382937759\n",
      "Gradient Descent(7/49): loss=0.3943620341774127\n",
      "Gradient Descent(8/49): loss=0.3924266111648928\n",
      "Gradient Descent(9/49): loss=0.3909135732336985\n",
      "Gradient Descent(10/49): loss=0.3896909809645675\n",
      "Gradient Descent(11/49): loss=0.3886722233541452\n",
      "Gradient Descent(12/49): loss=0.38779997999327165\n",
      "Gradient Descent(13/49): loss=0.38703595267159047\n",
      "Gradient Descent(14/49): loss=0.38635425942688373\n",
      "Gradient Descent(15/49): loss=0.38573716674880265\n",
      "Gradient Descent(16/49): loss=0.3851723209641023\n",
      "Gradient Descent(17/49): loss=0.38465094468661154\n",
      "Gradient Descent(18/49): loss=0.38416665689637036\n",
      "Gradient Descent(19/49): loss=0.38371469753869597\n",
      "Gradient Descent(20/49): loss=0.3832914155054269\n",
      "Gradient Descent(21/49): loss=0.38289392875077116\n",
      "Gradient Descent(22/49): loss=0.3825198973358132\n",
      "Gradient Descent(23/49): loss=0.38216737084944724\n",
      "Gradient Descent(24/49): loss=0.38183468501420464\n",
      "Gradient Descent(25/49): loss=0.3815203909585234\n",
      "Gradient Descent(26/49): loss=0.3812232062863627\n",
      "Gradient Descent(27/49): loss=0.38094198076715896\n",
      "Gradient Descent(28/49): loss=0.3806756718900311\n",
      "Gradient Descent(29/49): loss=0.3804233271186687\n",
      "Gradient Descent(30/49): loss=0.3801840707343427\n",
      "Gradient Descent(31/49): loss=0.3799570938503306\n",
      "Gradient Descent(32/49): loss=0.3797416466432759\n",
      "Gradient Descent(33/49): loss=0.37953703215506535\n",
      "Gradient Descent(34/49): loss=0.3793426012248394\n",
      "Gradient Descent(35/49): loss=0.3791577482490439\n",
      "Gradient Descent(36/49): loss=0.37898190756062394\n",
      "Gradient Descent(37/49): loss=0.3788145502815256\n",
      "Gradient Descent(38/49): loss=0.378655181545546\n",
      "Gradient Descent(39/49): loss=0.37850333801788616\n",
      "Gradient Descent(40/49): loss=0.3783585856579094\n",
      "Gradient Descent(41/49): loss=0.37822051768557086\n",
      "Gradient Descent(42/49): loss=0.37808875272172415\n",
      "Gradient Descent(43/49): loss=0.3779629330793834\n",
      "Gradient Descent(44/49): loss=0.37784272318791295\n",
      "Gradient Descent(45/49): loss=0.3777278081356541\n",
      "Gradient Descent(46/49): loss=0.377617892319088\n",
      "Gradient Descent(47/49): loss=0.3775126981885668\n",
      "Gradient Descent(48/49): loss=0.3774119650821079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=0.3773154481398839\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4426126961216446\n",
      "Gradient Descent(2/49): loss=0.4222509012065325\n",
      "Gradient Descent(3/49): loss=0.4109112368350171\n",
      "Gradient Descent(4/49): loss=0.40356192533582436\n",
      "Gradient Descent(5/49): loss=0.39853440711429483\n",
      "Gradient Descent(6/49): loss=0.3949817934449296\n",
      "Gradient Descent(7/49): loss=0.39239386961478584\n",
      "Gradient Descent(8/49): loss=0.3904469784950635\n",
      "Gradient Descent(9/49): loss=0.38893231040842546\n",
      "Gradient Descent(10/49): loss=0.38771398234486976\n",
      "Gradient Descent(11/49): loss=0.38670292174928217\n",
      "Gradient Descent(12/49): loss=0.3858402799243281\n",
      "Gradient Descent(13/49): loss=0.3850868170835751\n",
      "Gradient Descent(14/49): loss=0.38441607695278057\n",
      "Gradient Descent(15/49): loss=0.3838099804142054\n",
      "Gradient Descent(16/49): loss=0.38325596980543186\n",
      "Gradient Descent(17/49): loss=0.38274515091657946\n",
      "Gradient Descent(18/49): loss=0.3822710792073789\n",
      "Gradient Descent(19/49): loss=0.3818289634681742\n",
      "Gradient Descent(20/49): loss=0.3814151409298371\n",
      "Gradient Descent(21/49): loss=0.3810267295134691\n",
      "Gradient Descent(22/49): loss=0.38066139609477234\n",
      "Gradient Descent(23/49): loss=0.38031720103481964\n",
      "Gradient Descent(24/49): loss=0.3799924930450615\n",
      "Gradient Descent(25/49): loss=0.37968583741269246\n",
      "Gradient Descent(26/49): loss=0.379395966439377\n",
      "Gradient Descent(27/49): loss=0.3791217447482162\n",
      "Gradient Descent(28/49): loss=0.37886214460209944\n",
      "Gradient Descent(29/49): loss=0.37861622801011213\n",
      "Gradient Descent(30/49): loss=0.3783831334743597\n",
      "Gradient Descent(31/49): loss=0.3781620659401373\n",
      "Gradient Descent(32/49): loss=0.3779522889832371\n",
      "Gradient Descent(33/49): loss=0.37775311858125776\n",
      "Gradient Descent(34/49): loss=0.37756391802464623\n",
      "Gradient Descent(35/49): loss=0.3773840936630814\n",
      "Gradient Descent(36/49): loss=0.3772130912768599\n",
      "Gradient Descent(37/49): loss=0.37705039292646325\n",
      "Gradient Descent(38/49): loss=0.3768955141766098\n",
      "Gradient Descent(39/49): loss=0.3767480016205262\n",
      "Gradient Descent(40/49): loss=0.3766074306504072\n",
      "Gradient Descent(41/49): loss=0.3764734034340364\n",
      "Gradient Descent(42/49): loss=0.3763455470673314\n",
      "Gradient Descent(43/49): loss=0.3762235118794809\n",
      "Gradient Descent(44/49): loss=0.37610696987228376\n",
      "Gradient Descent(45/49): loss=0.37599561327886843\n",
      "Gradient Descent(46/49): loss=0.37588915322960714\n",
      "Gradient Descent(47/49): loss=0.3757873185150079\n",
      "Gradient Descent(48/49): loss=0.37568985443686576\n",
      "Gradient Descent(49/49): loss=0.3755965217401263\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4433551397124229\n",
      "Gradient Descent(2/49): loss=0.4235104977187579\n",
      "Gradient Descent(3/49): loss=0.4124375369891609\n",
      "Gradient Descent(4/49): loss=0.40523244182768003\n",
      "Gradient Descent(5/49): loss=0.40028242454219376\n",
      "Gradient Descent(6/49): loss=0.3967698545536179\n",
      "Gradient Descent(7/49): loss=0.3942010733730625\n",
      "Gradient Descent(8/49): loss=0.3922619598489089\n",
      "Gradient Descent(9/49): loss=0.3907491706532166\n",
      "Gradient Descent(10/49): loss=0.3895299023333193\n",
      "Gradient Descent(11/49): loss=0.38851676679017166\n",
      "Gradient Descent(12/49): loss=0.3876517939415387\n",
      "Gradient Descent(13/49): loss=0.3868961660888553\n",
      "Gradient Descent(14/49): loss=0.38622359788779687\n",
      "Gradient Descent(15/49): loss=0.3856160491473245\n",
      "Gradient Descent(16/49): loss=0.3850609369452975\n",
      "Gradient Descent(17/49): loss=0.3845493151534665\n",
      "Gradient Descent(18/49): loss=0.3840746805452404\n",
      "Gradient Descent(19/49): loss=0.3836321862656744\n",
      "Gradient Descent(20/49): loss=0.38321812113923087\n",
      "Gradient Descent(21/49): loss=0.3828295631180715\n",
      "Gradient Descent(22/49): loss=0.38246414724498984\n",
      "Gradient Descent(23/49): loss=0.38211990922179645\n",
      "Gradient Descent(24/49): loss=0.38179517910314725\n",
      "Gradient Descent(25/49): loss=0.38148850837110027\n",
      "Gradient Descent(26/49): loss=0.38119861934709764\n",
      "Gradient Descent(27/49): loss=0.38092436963181825\n",
      "Gradient Descent(28/49): loss=0.38066472671664925\n",
      "Gradient Descent(29/49): loss=0.3804187495278523\n",
      "Gradient Descent(30/49): loss=0.38018557473425474\n",
      "Gradient Descent(31/49): loss=0.37996440635924494\n",
      "Gradient Descent(32/49): loss=0.3797545077106328\n",
      "Gradient Descent(33/49): loss=0.3795551949578812\n",
      "Gradient Descent(34/49): loss=0.3793658318981396\n",
      "Gradient Descent(35/49): loss=0.3791858255952078\n",
      "Gradient Descent(36/49): loss=0.3790146226720527\n",
      "Gradient Descent(37/49): loss=0.3788517061030413\n",
      "Gradient Descent(38/49): loss=0.37869659239680126\n",
      "Gradient Descent(39/49): loss=0.37854882909134335\n",
      "Gradient Descent(40/49): loss=0.37840799250430607\n",
      "Gradient Descent(41/49): loss=0.3782736856959586\n",
      "Gradient Descent(42/49): loss=0.3781455366129736\n",
      "Gradient Descent(43/49): loss=0.378023196388329\n",
      "Gradient Descent(44/49): loss=0.3779063377779723\n",
      "Gradient Descent(45/49): loss=0.3777946537187023\n",
      "Gradient Descent(46/49): loss=0.3776878559945469\n",
      "Gradient Descent(47/49): loss=0.37758567400101917\n",
      "Gradient Descent(48/49): loss=0.3774878535982418\n",
      "Gradient Descent(49/49): loss=0.3773941560451766\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4423519002061231\n",
      "Gradient Descent(2/49): loss=0.42216956414954904\n",
      "Gradient Descent(3/49): loss=0.41091440911591914\n",
      "Gradient Descent(4/49): loss=0.40360091276742077\n",
      "Gradient Descent(5/49): loss=0.39858890055973034\n",
      "Gradient Descent(6/49): loss=0.39504278140304705\n",
      "Gradient Descent(7/49): loss=0.39245722604897454\n",
      "Gradient Descent(8/49): loss=0.3905109690378682\n",
      "Gradient Descent(9/49): loss=0.3889964404914301\n",
      "Gradient Descent(10/49): loss=0.3877783922750244\n",
      "Gradient Descent(11/49): loss=0.3867680520098742\n",
      "Gradient Descent(12/49): loss=0.38590668538606165\n",
      "Gradient Descent(13/49): loss=0.3851550665886479\n",
      "Gradient Descent(14/49): loss=0.38448670207193936\n",
      "Gradient Descent(15/49): loss=0.3838834521665293\n",
      "Gradient Descent(16/49): loss=0.3833326908698978\n",
      "Gradient Descent(17/49): loss=0.3828254560586685\n",
      "Gradient Descent(18/49): loss=0.38235523973860097\n",
      "Gradient Descent(19/49): loss=0.3819171933909554\n",
      "Gradient Descent(20/49): loss=0.38150760349722207\n",
      "Gradient Descent(21/49): loss=0.3811235435561015\n",
      "Gradient Descent(22/49): loss=0.3807626418203015\n",
      "Gradient Descent(23/49): loss=0.3804229251981808\n",
      "Gradient Descent(24/49): loss=0.38010271348869523\n",
      "Gradient Descent(25/49): loss=0.3798005470235793\n",
      "Gradient Descent(26/49): loss=0.3795151365883559\n",
      "Gradient Descent(27/49): loss=0.3792453282802253\n",
      "Gradient Descent(28/49): loss=0.3789900784416108\n",
      "Gradient Descent(29/49): loss=0.37874843543855063\n",
      "Gradient Descent(30/49): loss=0.37851952612804485\n",
      "Gradient Descent(31/49): loss=0.3783025455694371\n",
      "Gradient Descent(32/49): loss=0.3780967490066902\n",
      "Gradient Descent(33/49): loss=0.3779014454625459\n",
      "Gradient Descent(34/49): loss=0.377715992495455\n",
      "Gradient Descent(35/49): loss=0.3775397918109577\n",
      "Gradient Descent(36/49): loss=0.37737228551401825\n",
      "Gradient Descent(37/49): loss=0.3772129528529723\n",
      "Gradient Descent(38/49): loss=0.37706130734937426\n",
      "Gradient Descent(39/49): loss=0.37691689423786856\n",
      "Gradient Descent(40/49): loss=0.3767792881607525\n",
      "Gradient Descent(41/49): loss=0.3766480910761486\n",
      "Gradient Descent(42/49): loss=0.3765229303486842\n",
      "Gradient Descent(43/49): loss=0.3764034569986244\n",
      "Gradient Descent(44/49): loss=0.3762893440904595\n",
      "Gradient Descent(45/49): loss=0.37618028524561364\n",
      "Gradient Descent(46/49): loss=0.376075993266634\n",
      "Gradient Descent(47/49): loss=0.37597619886225714\n",
      "Gradient Descent(48/49): loss=0.3758806494642844\n",
      "Gradient Descent(49/49): loss=0.37578910812841015\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4433292909996569\n",
      "Gradient Descent(2/49): loss=0.42248888488105624\n",
      "Gradient Descent(3/49): loss=0.4111795941479742\n",
      "Gradient Descent(4/49): loss=0.4040698954032178\n",
      "Gradient Descent(5/49): loss=0.39929780815937366\n",
      "Gradient Descent(6/49): loss=0.39595891462962385\n",
      "Gradient Descent(7/49): loss=0.39353403314975893\n",
      "Gradient Descent(8/49): loss=0.39170513755769854\n",
      "Gradient Descent(9/49): loss=0.3902724655018727\n",
      "Gradient Descent(10/49): loss=0.3891089168275548\n",
      "Gradient Descent(11/49): loss=0.3881328104103928\n",
      "Gradient Descent(12/49): loss=0.38729110814503426\n",
      "Gradient Descent(13/49): loss=0.3865489544223069\n",
      "Gradient Descent(14/49): loss=0.38588310790079966\n",
      "Gradient Descent(15/49): loss=0.38527779526038136\n",
      "Gradient Descent(16/49): loss=0.3847220810372601\n",
      "Gradient Descent(17/49): loss=0.38420819105353693\n",
      "Gradient Descent(18/49): loss=0.3837304383099951\n",
      "Gradient Descent(19/49): loss=0.3832845311456999\n",
      "Gradient Descent(20/49): loss=0.3828671249871225\n",
      "Gradient Descent(21/49): loss=0.38247552998855355\n",
      "Gradient Descent(22/49): loss=0.38210751888028605\n",
      "Gradient Descent(23/49): loss=0.38176119952807197\n",
      "Gradient Descent(24/49): loss=0.38143492948677865\n",
      "Gradient Descent(25/49): loss=0.3811272579524472\n",
      "Gradient Descent(26/49): loss=0.38083688569750906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=0.3805626368908596\n",
      "Gradient Descent(28/49): loss=0.3803034388360064\n",
      "Gradient Descent(29/49): loss=0.38005830703525706\n",
      "Gradient Descent(30/49): loss=0.3798263338778274\n",
      "Gradient Descent(31/49): loss=0.3796066798279599\n",
      "Gradient Descent(32/49): loss=0.3793985663662717\n",
      "Gradient Descent(33/49): loss=0.379201270184515\n",
      "Gradient Descent(34/49): loss=0.3790141182963553\n",
      "Gradient Descent(35/49): loss=0.37883648383407986\n",
      "Gradient Descent(36/49): loss=0.3786677823724254\n",
      "Gradient Descent(37/49): loss=0.37850746866831586\n",
      "Gradient Descent(38/49): loss=0.3783550337373203\n",
      "Gradient Descent(39/49): loss=0.37821000220933093\n",
      "Gradient Descent(40/49): loss=0.3780719299208071\n",
      "Gradient Descent(41/49): loss=0.37794040171118043\n",
      "Gradient Descent(42/49): loss=0.37781502939820377\n",
      "Gradient Descent(43/49): loss=0.3776954499121266\n",
      "Gradient Descent(44/49): loss=0.3775813235722676\n",
      "Gradient Descent(45/49): loss=0.37747233249227324\n",
      "Gradient Descent(46/49): loss=0.3773681791023937\n",
      "Gradient Descent(47/49): loss=0.37726858477868297\n",
      "Gradient Descent(48/49): loss=0.37717328857026416\n",
      "Gradient Descent(49/49): loss=0.3770820460167991\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4427407780246304\n",
      "Gradient Descent(2/49): loss=0.42121228914303277\n",
      "Gradient Descent(3/49): loss=0.40954837884400663\n",
      "Gradient Descent(4/49): loss=0.4022585769444352\n",
      "Gradient Descent(5/49): loss=0.3973926936909728\n",
      "Gradient Descent(6/49): loss=0.39400582567657855\n",
      "Gradient Descent(7/49): loss=0.3915588762715698\n",
      "Gradient Descent(8/49): loss=0.38972297465874667\n",
      "Gradient Descent(9/49): loss=0.3882920619251583\n",
      "Gradient Descent(10/49): loss=0.3871352806840831\n",
      "Gradient Descent(11/49): loss=0.38616869249322994\n",
      "Gradient Descent(12/49): loss=0.38533790716008015\n",
      "Gradient Descent(13/49): loss=0.3846072659738413\n",
      "Gradient Descent(14/49): loss=0.38395305673074104\n",
      "Gradient Descent(15/49): loss=0.3833592353223488\n",
      "Gradient Descent(16/49): loss=0.38281471522792737\n",
      "Gradient Descent(17/49): loss=0.382311642356769\n",
      "Gradient Descent(18/49): loss=0.38184429173694373\n",
      "Gradient Descent(19/49): loss=0.3814083582361855\n",
      "Gradient Descent(20/49): loss=0.3810004979675531\n",
      "Gradient Descent(21/49): loss=0.380618029834858\n",
      "Gradient Descent(22/49): loss=0.380258739810252\n",
      "Gradient Descent(23/49): loss=0.37992075141084963\n",
      "Gradient Descent(24/49): loss=0.37960243903903323\n",
      "Gradient Descent(25/49): loss=0.37930236922519833\n",
      "Gradient Descent(26/49): loss=0.3790192601438613\n",
      "Gradient Descent(27/49): loss=0.37875195318110216\n",
      "Gradient Descent(28/49): loss=0.37849939251584475\n",
      "Gradient Descent(29/49): loss=0.37826061008303935\n",
      "Gradient Descent(30/49): loss=0.37803471419438317\n",
      "Gradient Descent(31/49): loss=0.37782088068037567\n",
      "Gradient Descent(32/49): loss=0.3776183458001339\n",
      "Gradient Descent(33/49): loss=0.3774264004153173\n",
      "Gradient Descent(34/49): loss=0.37724438508849023\n",
      "Gradient Descent(35/49): loss=0.37707168587435924\n",
      "Gradient Descent(36/49): loss=0.376907730643982\n",
      "Gradient Descent(37/49): loss=0.3767519858298554\n",
      "Gradient Descent(38/49): loss=0.3766039535119143\n",
      "Gradient Descent(39/49): loss=0.3764631687862318\n",
      "Gradient Descent(40/49): loss=0.3763291973731191\n",
      "Gradient Descent(41/49): loss=0.376201633431632\n",
      "Gradient Descent(42/49): loss=0.37608009755473665\n",
      "Gradient Descent(43/49): loss=0.375964234924548\n",
      "Gradient Descent(44/49): loss=0.37585371361080466\n",
      "Gradient Descent(45/49): loss=0.3757482229985188\n",
      "Gradient Descent(46/49): loss=0.3756474723328374\n",
      "Gradient Descent(47/49): loss=0.37555118937077475\n",
      "Gradient Descent(48/49): loss=0.3754591191307574\n",
      "Gradient Descent(49/49): loss=0.37537102273196016\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4435049446295576\n",
      "Gradient Descent(2/49): loss=0.42251260609804836\n",
      "Gradient Descent(3/49): loss=0.41110897240292515\n",
      "Gradient Descent(4/49): loss=0.40395352444172583\n",
      "Gradient Descent(5/49): loss=0.39915654359691777\n",
      "Gradient Descent(6/49): loss=0.39580321005488556\n",
      "Gradient Descent(7/49): loss=0.3933708570350592\n",
      "Gradient Descent(8/49): loss=0.3915397735716906\n",
      "Gradient Descent(9/49): loss=0.39010895794818795\n",
      "Gradient Descent(10/49): loss=0.38895027347356914\n",
      "Gradient Descent(11/49): loss=0.3879811838978024\n",
      "Gradient Descent(12/49): loss=0.38714797094943454\n",
      "Gradient Descent(13/49): loss=0.3864152547403615\n",
      "Gradient Descent(14/49): loss=0.38575939943058357\n",
      "Gradient Descent(15/49): loss=0.38516434050194726\n",
      "Gradient Descent(16/49): loss=0.3846189311782315\n",
      "Gradient Descent(17/49): loss=0.3841152466007962\n",
      "Gradient Descent(18/49): loss=0.3836474945381846\n",
      "Gradient Descent(19/49): loss=0.38321131187146323\n",
      "Gradient Descent(20/49): loss=0.38280330750560215\n",
      "Gradient Descent(21/49): loss=0.3824207633827143\n",
      "Gradient Descent(22/49): loss=0.38206143738839254\n",
      "Gradient Descent(23/49): loss=0.3817234322371875\n",
      "Gradient Descent(24/49): loss=0.38140510729862664\n",
      "Gradient Descent(25/49): loss=0.38110501852502876\n",
      "Gradient Descent(26/49): loss=0.3808218768844165\n",
      "Gradient Descent(27/49): loss=0.380554519065609\n",
      "Gradient Descent(28/49): loss=0.3803018863892001\n",
      "Gradient Descent(29/49): loss=0.38006300925890135\n",
      "Gradient Descent(30/49): loss=0.3798369953968355\n",
      "Gradient Descent(31/49): loss=0.3796230206986836\n",
      "Gradient Descent(32/49): loss=0.37942032193207376\n",
      "Gradient Descent(33/49): loss=0.3792281907561789\n",
      "Gradient Descent(34/49): loss=0.3790459687085116\n",
      "Gradient Descent(35/49): loss=0.37887304291635665\n",
      "Gradient Descent(36/49): loss=0.37870884236461255\n",
      "Gradient Descent(37/49): loss=0.3785528346017061\n",
      "Gradient Descent(38/49): loss=0.37840452279895465\n",
      "Gradient Descent(39/49): loss=0.3782634431017351\n",
      "Gradient Descent(40/49): loss=0.37812916222661236\n",
      "Gradient Descent(41/49): loss=0.3780012752695781\n",
      "Gradient Descent(42/49): loss=0.37787940369828704\n",
      "Gradient Descent(43/49): loss=0.3777631935067102\n",
      "Gradient Descent(44/49): loss=0.3776523135146442\n",
      "Gradient Descent(45/49): loss=0.37754645379748675\n",
      "Gradient Descent(46/49): loss=0.3774453242339347\n",
      "Gradient Descent(47/49): loss=0.3773486531609868\n",
      "Gradient Descent(48/49): loss=0.377256186126995\n",
      "Gradient Descent(49/49): loss=0.37716768473460116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4424670852697336\n",
      "Gradient Descent(2/49): loss=0.421123176057472\n",
      "Gradient Descent(3/49): loss=0.4095517805623022\n",
      "Gradient Descent(4/49): loss=0.4023000808878617\n",
      "Gradient Descent(5/49): loss=0.3974492743338219\n",
      "Gradient Descent(6/49): loss=0.3940679587000261\n",
      "Gradient Descent(7/49): loss=0.3916226677626926\n",
      "Gradient Descent(8/49): loss=0.38978705336659975\n",
      "Gradient Descent(9/49): loss=0.38835624435727434\n",
      "Gradient Descent(10/49): loss=0.3871999323943259\n",
      "Gradient Descent(11/49): loss=0.38623440235188883\n",
      "Gradient Descent(12/49): loss=0.38540531927833915\n",
      "Gradient Descent(13/49): loss=0.3846769958902954\n",
      "Gradient Descent(14/49): loss=0.38402565334712524\n",
      "Gradient Descent(15/49): loss=0.3834351673557347\n",
      "Gradient Descent(16/49): loss=0.3828943703118212\n",
      "Gradient Descent(17/49): loss=0.38239533220136124\n",
      "Gradient Descent(18/49): loss=0.3819322597654769\n",
      "Gradient Descent(19/49): loss=0.38150078782062063\n",
      "Gradient Descent(20/49): loss=0.3810975203420135\n",
      "Gradient Descent(21/49): loss=0.3807197312880836\n",
      "Gradient Descent(22/49): loss=0.3803651680341601\n",
      "Gradient Descent(23/49): loss=0.38003192101936895\n",
      "Gradient Descent(24/49): loss=0.37971833633222113\n",
      "Gradient Descent(25/49): loss=0.37942295629408485\n",
      "Gradient Descent(26/49): loss=0.37914447841168647\n",
      "Gradient Descent(27/49): loss=0.3788817264677575\n",
      "Gradient Descent(28/49): loss=0.3786336297003012\n",
      "Gradient Descent(29/49): loss=0.37839920742632893\n",
      "Gradient Descent(30/49): loss=0.37817755737463565\n",
      "Gradient Descent(31/49): loss=0.377967846581983\n",
      "Gradient Descent(32/49): loss=0.37776930409135767\n",
      "Gradient Descent(33/49): loss=0.3775812149424185\n",
      "Gradient Descent(34/49): loss=0.37740291510950685\n",
      "Gradient Descent(35/49): loss=0.3772337871517495\n",
      "Gradient Descent(36/49): loss=0.37707325641228057\n",
      "Gradient Descent(37/49): loss=0.3769207876520619\n",
      "Gradient Descent(38/49): loss=0.3767758820364064\n",
      "Gradient Descent(39/49): loss=0.37663807441445507\n",
      "Gradient Descent(40/49): loss=0.37650693084704817\n",
      "Gradient Descent(41/49): loss=0.37638204634897204\n",
      "Gradient Descent(42/49): loss=0.376263042818969\n",
      "Gradient Descent(43/49): loss=0.3761495671362004\n",
      "Gradient Descent(44/49): loss=0.3760412894056937\n",
      "Gradient Descent(45/49): loss=0.3759379013381697\n",
      "Gradient Descent(46/49): loss=0.37583911475180176\n",
      "Gradient Descent(47/49): loss=0.3757446601851397\n",
      "Gradient Descent(48/49): loss=0.3756542856117518\n",
      "Gradient Descent(49/49): loss=0.3755677552482175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4437567173749445\n",
      "Gradient Descent(2/49): loss=0.4218117014495478\n",
      "Gradient Descent(3/49): loss=0.4100648495602761\n",
      "Gradient Descent(4/49): loss=0.4029390200981478\n",
      "Gradient Descent(5/49): loss=0.39828979062972586\n",
      "Gradient Descent(6/49): loss=0.3950927918624172\n",
      "Gradient Descent(7/49): loss=0.3927901411376729\n",
      "Gradient Descent(8/49): loss=0.3910560458145663\n",
      "Gradient Descent(9/49): loss=0.38969322691227914\n",
      "Gradient Descent(10/49): loss=0.3885796869155115\n",
      "Gradient Descent(11/49): loss=0.387638817429845\n",
      "Gradient Descent(12/49): loss=0.3868218190000945\n",
      "Gradient Descent(13/49): loss=0.386097112899793\n",
      "Gradient Descent(14/49): loss=0.3854438786897198\n",
      "Gradient Descent(15/49): loss=0.38484807547132843\n",
      "Gradient Descent(16/49): loss=0.3842999755532793\n",
      "Gradient Descent(17/49): loss=0.3837926261612078\n",
      "Gradient Descent(18/49): loss=0.383320884036376\n",
      "Gradient Descent(19/49): loss=0.38288080556133186\n",
      "Gradient Descent(20/49): loss=0.3824692586362754\n",
      "Gradient Descent(21/49): loss=0.38208367356446676\n",
      "Gradient Descent(22/49): loss=0.3817218815322235\n",
      "Gradient Descent(23/49): loss=0.38138200859125115\n",
      "Gradient Descent(24/49): loss=0.381062405022263\n",
      "Gradient Descent(25/49): loss=0.380761597407676\n",
      "Gradient Descent(26/49): loss=0.38047825539551383\n",
      "Gradient Descent(27/49): loss=0.3802111680569013\n",
      "Gradient Descent(28/49): loss=0.37995922657929626\n",
      "Gradient Descent(29/49): loss=0.3797214112014587\n",
      "Gradient Descent(30/49): loss=0.37949678103553147\n",
      "Gradient Descent(31/49): loss=0.37928446589336584\n",
      "Gradient Descent(32/49): loss=0.37908365953663714\n",
      "Gradient Descent(33/49): loss=0.3788936139651018\n",
      "Gradient Descent(34/49): loss=0.3787136344835469\n",
      "Gradient Descent(35/49): loss=0.37854307537022397\n",
      "Gradient Descent(36/49): loss=0.37838133602355095\n",
      "Gradient Descent(37/49): loss=0.3782278574995998\n",
      "Gradient Descent(38/49): loss=0.3780821193767693\n",
      "Gradient Descent(39/49): loss=0.37794363690019683\n",
      "Gradient Descent(40/49): loss=0.37781195836953535\n",
      "Gradient Descent(41/49): loss=0.37768666274144047\n",
      "Gradient Descent(42/49): loss=0.37756735742359515\n",
      "Gradient Descent(43/49): loss=0.3774536762410763\n",
      "Gradient Descent(44/49): loss=0.3773452775588237\n",
      "Gradient Descent(45/49): loss=0.37724184254622245\n",
      "Gradient Descent(46/49): loss=0.3771430735715699\n",
      "Gradient Descent(47/49): loss=0.3770486927156023\n",
      "Gradient Descent(48/49): loss=0.37695844039441767\n",
      "Gradient Descent(49/49): loss=0.37687207408309165\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44318370655937983\n",
      "Gradient Descent(2/49): loss=0.420562819596734\n",
      "Gradient Descent(3/49): loss=0.40843808493752964\n",
      "Gradient Descent(4/49): loss=0.40111874208656667\n",
      "Gradient Descent(5/49): loss=0.39637356325222667\n",
      "Gradient Descent(6/49): loss=0.39313144973427533\n",
      "Gradient Descent(7/49): loss=0.39081072649994725\n",
      "Gradient Descent(8/49): loss=0.38907322701609953\n",
      "Gradient Descent(9/49): loss=0.3877149921033269\n",
      "Gradient Descent(10/49): loss=0.3866103061667927\n",
      "Gradient Descent(11/49): loss=0.3856804549640669\n",
      "Gradient Descent(12/49): loss=0.3848754399622692\n",
      "Gradient Descent(13/49): loss=0.3841630028732676\n",
      "Gradient Descent(14/49): loss=0.3835219435948936\n",
      "Gradient Descent(15/49): loss=0.38293801476296196\n",
      "Gradient Descent(16/49): loss=0.3824013818549675\n",
      "Gradient Descent(17/49): loss=0.3819050421983553\n",
      "Gradient Descent(18/49): loss=0.3814438348795725\n",
      "Gradient Descent(19/49): loss=0.38101381667865286\n",
      "Gradient Descent(20/49): loss=0.3806118658432806\n",
      "Gradient Descent(21/49): loss=0.3802354283777612\n",
      "Gradient Descent(22/49): loss=0.3798823539279345\n",
      "Gradient Descent(23/49): loss=0.3795507883002786\n",
      "Gradient Descent(24/49): loss=0.3792391019965237\n",
      "Gradient Descent(25/49): loss=0.378945841809996\n",
      "Gradient Descent(26/49): loss=0.37866969730856903\n",
      "Gradient Descent(27/49): loss=0.378409477020044\n",
      "Gradient Descent(28/49): loss=0.3781640910151774\n",
      "Gradient Descent(29/49): loss=0.37793253776925956\n",
      "Gradient Descent(30/49): loss=0.3777138939343101\n",
      "Gradient Descent(31/49): loss=0.37750730613191813\n",
      "Gradient Descent(32/49): loss=0.3773119841823252\n",
      "Gradient Descent(33/49): loss=0.37712719538172446\n",
      "Gradient Descent(34/49): loss=0.3769522595666751\n",
      "Gradient Descent(35/49): loss=0.37678654478713414\n",
      "Gradient Descent(36/49): loss=0.3766294634637579\n",
      "Gradient Descent(37/49): loss=0.3764804689409698\n",
      "Gradient Descent(38/49): loss=0.3763390523712515\n",
      "Gradient Descent(39/49): loss=0.3762047398823578\n",
      "Gradient Descent(40/49): loss=0.3760770899903104\n",
      "Gradient Descent(41/49): loss=0.3759556912288383\n",
      "Gradient Descent(42/49): loss=0.3758401599715081\n",
      "Gradient Descent(43/49): loss=0.375730138426846\n",
      "Gradient Descent(44/49): loss=0.37562529278979684\n",
      "Gradient Descent(45/49): loss=0.3755253115351899\n",
      "Gradient Descent(46/49): loss=0.37542990384070046\n",
      "Gradient Descent(47/49): loss=0.3753387981282711\n",
      "Gradient Descent(48/49): loss=0.3752517407141545\n",
      "Gradient Descent(49/49): loss=0.3751684945587514\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4439678608820245\n",
      "Gradient Descent(2/49): loss=0.4218993449249996\n",
      "Gradient Descent(3/49): loss=0.41002978072255986\n",
      "Gradient Descent(4/49): loss=0.4028354010124953\n",
      "Gradient Descent(5/49): loss=0.3981508218081532\n",
      "Gradient Descent(6/49): loss=0.3949361620104692\n",
      "Gradient Descent(7/49): loss=0.3926259996273974\n",
      "Gradient Descent(8/49): loss=0.3908908502892794\n",
      "Gradient Descent(9/49): loss=0.3895313423586917\n",
      "Gradient Descent(10/49): loss=0.3884241027175444\n",
      "Gradient Descent(11/49): loss=0.38749154185104673\n",
      "Gradient Descent(12/49): loss=0.38668414190620476\n",
      "Gradient Descent(13/49): loss=0.3859697983479107\n",
      "Gradient Descent(14/49): loss=0.38532730986141456\n",
      "Gradient Descent(15/49): loss=0.38474236350816915\n",
      "Gradient Descent(16/49): loss=0.38420504050881954\n",
      "Gradient Descent(17/49): loss=0.38370825663744984\n",
      "Gradient Descent(18/49): loss=0.3832467807705709\n",
      "Gradient Descent(19/49): loss=0.3828166130785446\n",
      "Gradient Descent(20/49): loss=0.38241458810470264\n",
      "Gradient Descent(21/49): loss=0.38203811920059927\n",
      "Gradient Descent(22/49): loss=0.3816850322903104\n",
      "Gradient Descent(23/49): loss=0.38135345640919266\n",
      "Gradient Descent(24/49): loss=0.38104175055332357\n",
      "Gradient Descent(25/49): loss=0.3807484539161325\n",
      "Gradient Descent(26/49): loss=0.38047225131129975\n",
      "Gradient Descent(27/49): loss=0.38021194855133034\n",
      "Gradient Descent(28/49): loss=0.37996645442742094\n",
      "Gradient Descent(29/49): loss=0.3797347671264482\n",
      "Gradient Descent(30/49): loss=0.37951596367931406\n",
      "Gradient Descent(31/49): loss=0.37930919152038584\n",
      "Gradient Descent(32/49): loss=0.379113661550108\n",
      "Gradient Descent(33/49): loss=0.37892864229487255\n",
      "Gradient Descent(34/49): loss=0.3787534548896674\n",
      "Gradient Descent(35/49): loss=0.3785874686950926\n",
      "Gradient Descent(36/49): loss=0.37843009741713673\n",
      "Gradient Descent(37/49): loss=0.37828079563591194\n",
      "Gradient Descent(38/49): loss=0.3781390556749715\n",
      "Gradient Descent(39/49): loss=0.37800440476013647\n",
      "Gradient Descent(40/49): loss=0.37787640242869713\n",
      "Gradient Descent(41/49): loss=0.3777546381582264\n",
      "Gradient Descent(42/49): loss=0.37763872919022223\n",
      "Gradient Descent(43/49): loss=0.37752831852815383\n",
      "Gradient Descent(44/49): loss=0.3774230730927344\n",
      "Gradient Descent(45/49): loss=0.3773226820197181\n",
      "Gradient Descent(46/49): loss=0.37722685508745624\n",
      "Gradient Descent(47/49): loss=0.37713532126298627\n",
      "Gradient Descent(48/49): loss=0.3770478273566949\n",
      "Gradient Descent(49/49): loss=0.3769641367766383\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44289719989125176\n",
      "Gradient Descent(2/49): loss=0.4204567758459027\n",
      "Gradient Descent(3/49): loss=0.4084328990155814\n",
      "Gradient Descent(4/49): loss=0.4011575750510333\n",
      "Gradient Descent(5/49): loss=0.3964297056796943\n",
      "Gradient Descent(6/49): loss=0.3931935812025837\n",
      "Gradient Descent(7/49): loss=0.3908744586622941\n",
      "Gradient Descent(8/49): loss=0.3891372223254825\n",
      "Gradient Descent(9/49): loss=0.3877792336145102\n",
      "Gradient Descent(10/49): loss=0.38667531413882755\n",
      "Gradient Descent(11/49): loss=0.3857469253977635\n",
      "Gradient Descent(12/49): loss=0.38494407822542115\n",
      "Gradient Descent(13/49): loss=0.3842344508209869\n",
      "Gradient Descent(14/49): loss=0.3835967523437765\n",
      "Gradient Descent(15/49): loss=0.38301663955735404\n",
      "Gradient Descent(16/49): loss=0.3824841870996852\n",
      "Gradient Descent(17/49): loss=0.38199231059222827\n",
      "Gradient Descent(18/49): loss=0.38153577762793783\n",
      "Gradient Descent(19/49): loss=0.3811105833608011\n",
      "Gradient Descent(20/49): loss=0.3807135533422603\n",
      "Gradient Descent(21/49): loss=0.38034208869968933\n",
      "Gradient Descent(22/49): loss=0.3799940009372423\n",
      "Gradient Descent(23/49): loss=0.3796674034806839\n",
      "Gradient Descent(24/49): loss=0.3793606393721074\n",
      "Gradient Descent(25/49): loss=0.37907223215747127\n",
      "Gradient Descent(26/49): loss=0.3788008517769906\n",
      "Gradient Descent(27/49): loss=0.3785452902560686\n",
      "Gradient Descent(28/49): loss=0.3783044438744636\n",
      "Gradient Descent(29/49): loss=0.3780772996792796\n",
      "Gradient Descent(30/49): loss=0.37786292496115564\n",
      "Gradient Descent(31/49): loss=0.3776604587935024\n",
      "Gradient Descent(32/49): loss=0.3774691050423689\n",
      "Gradient Descent(33/49): loss=0.37728812645266724\n",
      "Gradient Descent(34/49): loss=0.377116839544815\n",
      "Gradient Descent(35/49): loss=0.37695461013952947\n",
      "Gradient Descent(36/49): loss=0.3768008493834909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=0.3766550101850491\n",
      "Gradient Descent(38/49): loss=0.3765165839935813\n",
      "Gradient Descent(39/49): loss=0.37638509787269253\n",
      "Gradient Descent(40/49): loss=0.37626011182887703\n",
      "Gradient Descent(41/49): loss=0.3761412163652671\n",
      "Gradient Descent(42/49): loss=0.3760280302358192\n",
      "Gradient Descent(43/49): loss=0.37592019837947627\n",
      "Gradient Descent(44/49): loss=0.3758173900169735\n",
      "Gradient Descent(45/49): loss=0.37571929689535694\n",
      "Gradient Descent(46/49): loss=0.37562563166717255\n",
      "Gradient Descent(47/49): loss=0.3755361263928048\n",
      "Gradient Descent(48/49): loss=0.37545053115569216\n",
      "Gradient Descent(49/49): loss=0.37536861278119266\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4444947115431185\n",
      "Gradient Descent(2/49): loss=0.4216009478745001\n",
      "Gradient Descent(3/49): loss=0.40929426826204973\n",
      "Gradient Descent(4/49): loss=0.4020273829738747\n",
      "Gradient Descent(5/49): loss=0.3974260807233959\n",
      "Gradient Descent(6/49): loss=0.3943302741533414\n",
      "Gradient Descent(7/49): loss=0.3921262356965656\n",
      "Gradient Descent(8/49): loss=0.39047137854379865\n",
      "Gradient Descent(9/49): loss=0.3891670966509491\n",
      "Gradient Descent(10/49): loss=0.38809498975927825\n",
      "Gradient Descent(11/49): loss=0.3871828541495087\n",
      "Gradient Descent(12/49): loss=0.3863857252647304\n",
      "Gradient Descent(13/49): loss=0.38567497993976385\n",
      "Gradient Descent(14/49): loss=0.3850319372095258\n",
      "Gradient Descent(15/49): loss=0.38444404357299244\n",
      "Gradient Descent(16/49): loss=0.3839025727756361\n",
      "Gradient Descent(17/49): loss=0.3834012255762021\n",
      "Gradient Descent(18/49): loss=0.38293526987056714\n",
      "Gradient Descent(19/49): loss=0.38250100792968467\n",
      "Gradient Descent(20/49): loss=0.38209544307235954\n",
      "Gradient Descent(21/49): loss=0.38171606873708464\n",
      "Gradient Descent(22/49): loss=0.3813607331720924\n",
      "Gradient Descent(23/49): loss=0.3810275511704413\n",
      "Gradient Descent(24/49): loss=0.3807148453021155\n",
      "Gradient Descent(25/49): loss=0.38042110580756067\n",
      "Gradient Descent(26/49): loss=0.38014496242447376\n",
      "Gradient Descent(27/49): loss=0.37988516394517413\n",
      "Gradient Descent(28/49): loss=0.379640562862146\n",
      "Gradient Descent(29/49): loss=0.3794101034279268\n",
      "Gradient Descent(30/49): loss=0.37919281205980193\n",
      "Gradient Descent(31/49): loss=0.3789877893987666\n",
      "Gradient Descent(32/49): loss=0.37879420357129584\n",
      "Gradient Descent(33/49): loss=0.37861128435425434\n",
      "Gradient Descent(34/49): loss=0.3784383180403497\n",
      "Gradient Descent(35/49): loss=0.37827464286416135\n",
      "Gradient Descent(36/49): loss=0.37811964488957506\n",
      "Gradient Descent(37/49): loss=0.3779727542863828\n",
      "Gradient Descent(38/49): loss=0.3778334419418099\n",
      "Gradient Descent(39/49): loss=0.3777012163650168\n",
      "Gradient Descent(40/49): loss=0.3775756208511376\n",
      "Gradient Descent(41/49): loss=0.3774562308774989\n",
      "Gradient Descent(42/49): loss=0.37734265170908116\n",
      "Gradient Descent(43/49): loss=0.37723451619362225\n",
      "Gradient Descent(44/49): loss=0.37713148272931996\n",
      "Gradient Descent(45/49): loss=0.3770332333901342\n",
      "Gradient Descent(46/49): loss=0.376939472195343\n",
      "Gradient Descent(47/49): loss=0.3768499235113902\n",
      "Gradient Descent(48/49): loss=0.37676433057522946\n",
      "Gradient Descent(49/49): loss=0.37668245412938567\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44394148172589293\n",
      "Gradient Descent(2/49): loss=0.4204102082869549\n",
      "Gradient Descent(3/49): loss=0.4077074741408454\n",
      "Gradient Descent(4/49): loss=0.4002245354987325\n",
      "Gradient Descent(5/49): loss=0.3955148046500287\n",
      "Gradient Descent(6/49): loss=0.3923696321387892\n",
      "Gradient Descent(7/49): loss=0.3901474344076754\n",
      "Gradient Descent(8/49): loss=0.3884905882490526\n",
      "Gradient Descent(9/49): loss=0.3871925444145891\n",
      "Gradient Descent(10/49): loss=0.3861307217520627\n",
      "Gradient Descent(11/49): loss=0.38523070965919315\n",
      "Gradient Descent(12/49): loss=0.3844463761916134\n",
      "Gradient Descent(13/49): loss=0.3837484830391696\n",
      "Gradient Descent(14/49): loss=0.3831180300528249\n",
      "Gradient Descent(15/49): loss=0.3825423041125985\n",
      "Gradient Descent(16/49): loss=0.3820125055590492\n",
      "Gradient Descent(17/49): loss=0.38152230823612804\n",
      "Gradient Descent(18/49): loss=0.38106697803286005\n",
      "Gradient Descent(19/49): loss=0.380642828395045\n",
      "Gradient Descent(20/49): loss=0.38024688063849194\n",
      "Gradient Descent(21/49): loss=0.3798766495780839\n",
      "Gradient Descent(22/49): loss=0.3795300063523035\n",
      "Gradient Descent(23/49): loss=0.3792050891399002\n",
      "Gradient Descent(24/49): loss=0.3789002438256502\n",
      "Gradient Descent(25/49): loss=0.37861398356831283\n",
      "Gradient Descent(26/49): loss=0.3783449604311693\n",
      "Gradient Descent(27/49): loss=0.3780919448146502\n",
      "Gradient Descent(28/49): loss=0.37785381001909296\n",
      "Gradient Descent(29/49): loss=0.37762952024880103\n",
      "Gradient Descent(30/49): loss=0.3774181209801253\n",
      "Gradient Descent(31/49): loss=0.37721873099878056\n",
      "Gradient Descent(32/49): loss=0.3770305356522947\n",
      "Gradient Descent(33/49): loss=0.3768527810159702\n",
      "Gradient Descent(34/49): loss=0.37668476876813856\n",
      "Gradient Descent(35/49): loss=0.3765258516332558\n",
      "Gradient Descent(36/49): loss=0.37637542929231466\n",
      "Gradient Descent(37/49): loss=0.3762329446870818\n",
      "Gradient Descent(38/49): loss=0.3760978806628073\n",
      "Gradient Descent(39/49): loss=0.3759697569064574\n",
      "Gradient Descent(40/49): loss=0.3758481271461839\n",
      "Gradient Descent(41/49): loss=0.37573257658394665\n",
      "Gradient Descent(42/49): loss=0.3756227195377527\n",
      "Gradient Descent(43/49): loss=0.37551819727341956\n",
      "Gradient Descent(44/49): loss=0.37541867600843126\n",
      "Gradient Descent(45/49): loss=0.3753238450725795\n",
      "Gradient Descent(46/49): loss=0.3752334152118158\n",
      "Gradient Descent(47/49): loss=0.3751471170231762\n",
      "Gradient Descent(48/49): loss=0.3750646995098691\n",
      "Gradient Descent(49/49): loss=0.37498592874666403\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44474388846982366\n",
      "Gradient Descent(2/49): loss=0.42177751851057527\n",
      "Gradient Descent(3/49): loss=0.40932692095326323\n",
      "Gradient Descent(4/49): loss=0.4019607229028046\n",
      "Gradient Descent(5/49): loss=0.39730377692628277\n",
      "Gradient Descent(6/49): loss=0.39418032299266553\n",
      "Gradient Descent(7/49): loss=0.3919650116268782\n",
      "Gradient Descent(8/49): loss=0.3903084019430362\n",
      "Gradient Descent(9/49): loss=0.3890080172383789\n",
      "Gradient Descent(10/49): loss=0.38794321822006306\n",
      "Gradient Descent(11/49): loss=0.3870404414035301\n",
      "Gradient Descent(12/49): loss=0.38625385604565027\n",
      "Gradient Descent(13/49): loss=0.38555426797864567\n",
      "Gradient Descent(14/49): loss=0.3849226129196241\n",
      "Gradient Descent(15/49): loss=0.3843460790194335\n",
      "Gradient Descent(16/49): loss=0.3838157673567281\n",
      "Gradient Descent(17/49): loss=0.38332526572712455\n",
      "Gradient Descent(18/49): loss=0.38286977098331654\n",
      "Gradient Descent(19/49): loss=0.3824455438498347\n",
      "Gradient Descent(20/49): loss=0.38204956681852076\n",
      "Gradient Descent(21/49): loss=0.3816793269785111\n",
      "Gradient Descent(22/49): loss=0.38133267624942935\n",
      "Gradient Descent(23/49): loss=0.3810077399252971\n",
      "Gradient Descent(24/49): loss=0.38070285561733724\n",
      "Gradient Descent(25/49): loss=0.38041653150354005\n",
      "Gradient Descent(26/49): loss=0.3801474169751049\n",
      "Gradient Descent(27/49): loss=0.37989428134793496\n",
      "Gradient Descent(28/49): loss=0.37965599790461546\n",
      "Gradient Descent(29/49): loss=0.3794315315270166\n",
      "Gradient Descent(30/49): loss=0.37921992880242816\n",
      "Gradient Descent(31/49): loss=0.3790203098782736\n",
      "Gradient Descent(32/49): loss=0.3788318615888894\n",
      "Gradient Descent(33/49): loss=0.378653831536362\n",
      "Gradient Descent(34/49): loss=0.37848552290932347\n",
      "Gradient Descent(35/49): loss=0.3783262898897335\n",
      "Gradient Descent(36/49): loss=0.37817553354104283\n",
      "Gradient Descent(37/49): loss=0.3780326980999252\n",
      "Gradient Descent(38/49): loss=0.3778972676131677\n",
      "Gradient Descent(39/49): loss=0.3777687628746181\n",
      "Gradient Descent(40/49): loss=0.3776467386263833\n",
      "Gradient Descent(41/49): loss=0.37753078099513104\n",
      "Gradient Descent(42/49): loss=0.3774205051392143\n",
      "Gradient Descent(43/49): loss=0.37731555308600523\n",
      "Gradient Descent(44/49): loss=0.37721559174164737\n",
      "Gradient Descent(45/49): loss=0.37712031105767224\n",
      "Gradient Descent(46/49): loss=0.37702942234073467\n",
      "Gradient Descent(47/49): loss=0.3769426566932207\n",
      "Gradient Descent(48/49): loss=0.376859763573743\n",
      "Gradient Descent(49/49): loss=0.3767805094676232\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44364224407067765\n",
      "Gradient Descent(2/49): loss=0.42027596785647003\n",
      "Gradient Descent(3/49): loss=0.4076797327935177\n",
      "Gradient Descent(4/49): loss=0.4002502687617975\n",
      "Gradient Descent(5/49): loss=0.3955643277146962\n",
      "Gradient Descent(6/49): loss=0.39242855430883383\n",
      "Gradient Descent(7/49): loss=0.39020960322759574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8/49): loss=0.388553883802974\n",
      "Gradient Descent(9/49): loss=0.38725667184891266\n",
      "Gradient Descent(10/49): loss=0.38619613072570563\n",
      "Gradient Descent(11/49): loss=0.38529809087658096\n",
      "Gradient Descent(12/49): loss=0.3845164386731498\n",
      "Gradient Descent(13/49): loss=0.38382186374511684\n",
      "Gradient Descent(14/49): loss=0.38319526421692623\n",
      "Gradient Descent(15/49): loss=0.3826238223322597\n",
      "Gradient Descent(16/49): loss=0.3820986417611084\n",
      "Gradient Descent(17/49): loss=0.38161331122777503\n",
      "Gradient Descent(18/49): loss=0.3811630234178201\n",
      "Gradient Descent(19/49): loss=0.3807440295522259\n",
      "Gradient Descent(20/49): loss=0.38035329834675696\n",
      "Gradient Descent(21/49): loss=0.3799883002627051\n",
      "Gradient Descent(22/49): loss=0.3796468690846401\n",
      "Gradient Descent(23/49): loss=0.3793271115679212\n",
      "Gradient Descent(24/49): loss=0.3790273472105909\n",
      "Gradient Descent(25/49): loss=0.37874606708174885\n",
      "Gradient Descent(26/49): loss=0.3784819048410104\n",
      "Gradient Descent(27/49): loss=0.37823361566407343\n",
      "Gradient Descent(28/49): loss=0.3780000603814731\n",
      "Gradient Descent(29/49): loss=0.37778019312469613\n",
      "Gradient Descent(30/49): loss=0.3775730513890153\n",
      "Gradient Descent(31/49): loss=0.3773777478079338\n",
      "Gradient Descent(32/49): loss=0.3771934631772288\n",
      "Gradient Descent(33/49): loss=0.37701944042093954\n",
      "Gradient Descent(34/49): loss=0.3768549792904339\n",
      "Gradient Descent(35/49): loss=0.3766994316515142\n",
      "Gradient Descent(36/49): loss=0.3765521972562204\n",
      "Gradient Descent(37/49): loss=0.37641271992358766\n",
      "Gradient Descent(38/49): loss=0.3762804840721819\n",
      "Gradient Descent(39/49): loss=0.37615501155995196\n",
      "Gradient Descent(40/49): loss=0.37603585879583584\n",
      "Gradient Descent(41/49): loss=0.37592261409394334\n",
      "Gradient Descent(42/49): loss=0.37581489524582684\n",
      "Gradient Descent(43/49): loss=0.3757123472899064\n",
      "Gradient Descent(44/49): loss=0.37561464045987286\n",
      "Gradient Descent(45/49): loss=0.3755214682960898\n",
      "Gradient Descent(46/49): loss=0.3754325459058158\n",
      "Gradient Descent(47/49): loss=0.37534760835956477\n",
      "Gradient Descent(48/49): loss=0.3752664092121971\n",
      "Gradient Descent(49/49): loss=0.37518871913843127\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4455432735041786\n",
      "Gradient Descent(2/49): loss=0.42197579088923604\n",
      "Gradient Descent(3/49): loss=0.40904535565874545\n",
      "Gradient Descent(4/49): loss=0.4014825198365809\n",
      "Gradient Descent(5/49): loss=0.3967998445074003\n",
      "Gradient Descent(6/49): loss=0.39372018538712594\n",
      "Gradient Descent(7/49): loss=0.3915634025147236\n",
      "Gradient Descent(8/49): loss=0.3899573336449109\n",
      "Gradient Descent(9/49): loss=0.38869322746969326\n",
      "Gradient Descent(10/49): loss=0.38765113794192685\n",
      "Gradient Descent(11/49): loss=0.38676043954254\n",
      "Gradient Descent(12/49): loss=0.3859784486570763\n",
      "Gradient Descent(13/49): loss=0.3852786070125558\n",
      "Gradient Descent(14/49): loss=0.3846438286574327\n",
      "Gradient Descent(15/49): loss=0.38406269210737415\n",
      "Gradient Descent(16/49): loss=0.3835272285512645\n",
      "Gradient Descent(17/49): loss=0.38303161957429843\n",
      "Gradient Descent(18/49): loss=0.38257142032914937\n",
      "Gradient Descent(19/49): loss=0.38214308996035246\n",
      "Gradient Descent(20/49): loss=0.3817437036578908\n",
      "Gradient Descent(21/49): loss=0.3813707731725342\n",
      "Gradient Descent(22/49): loss=0.38102213274496377\n",
      "Gradient Descent(23/49): loss=0.3806958648919925\n",
      "Gradient Descent(24/49): loss=0.38039025075111216\n",
      "Gradient Descent(25/49): loss=0.38010373575240913\n",
      "Gradient Descent(26/49): loss=0.37983490500386285\n",
      "Gradient Descent(27/49): loss=0.379582464947332\n",
      "Gradient Descent(28/49): loss=0.37934522915470115\n",
      "Gradient Descent(29/49): loss=0.3791221069318437\n",
      "Gradient Descent(30/49): loss=0.3789120938868328\n",
      "Gradient Descent(31/49): loss=0.3787142639202629\n",
      "Gradient Descent(32/49): loss=0.37852776228289003\n",
      "Gradient Descent(33/49): loss=0.37835179946327324\n",
      "Gradient Descent(34/49): loss=0.37818564574252705\n",
      "Gradient Descent(35/49): loss=0.3780286263010407\n",
      "Gradient Descent(36/49): loss=0.3778801167930996\n",
      "Gradient Descent(37/49): loss=0.3777395393259521\n",
      "Gradient Descent(38/49): loss=0.3776063587938137\n",
      "Gradient Descent(39/49): loss=0.3774800795269818\n",
      "Gradient Descent(40/49): loss=0.37736024222315023\n",
      "Gradient Descent(41/49): loss=0.37724642113308826\n",
      "Gradient Descent(42/49): loss=0.377138221476713\n",
      "Gradient Descent(43/49): loss=0.37703527706859263\n",
      "Gradient Descent(44/49): loss=0.37693724813434576\n",
      "Gradient Descent(45/49): loss=0.3768438193014092\n",
      "Gradient Descent(46/49): loss=0.37675469774932174\n",
      "Gradient Descent(47/49): loss=0.3766696115061303\n",
      "Gradient Descent(48/49): loss=0.376588307878777\n",
      "Gradient Descent(49/49): loss=0.37651055200643513\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4450141035241694\n",
      "Gradient Descent(2/49): loss=0.4208795790393718\n",
      "Gradient Descent(3/49): loss=0.4075524355516288\n",
      "Gradient Descent(4/49): loss=0.39974730288128235\n",
      "Gradient Descent(5/49): loss=0.3949308420974694\n",
      "Gradient Descent(6/49): loss=0.3917846168540426\n",
      "Gradient Descent(7/49): loss=0.3895998854823119\n",
      "Gradient Descent(8/49): loss=0.3879869347361671\n",
      "Gradient Descent(9/49): loss=0.38672693864435864\n",
      "Gradient Descent(10/49): loss=0.3856944017200401\n",
      "Gradient Descent(11/49): loss=0.3848157276614981\n",
      "Gradient Descent(12/49): loss=0.38404667858937525\n",
      "Gradient Descent(13/49): loss=0.38335990148486904\n",
      "Gradient Descent(14/49): loss=0.3827379159503727\n",
      "Gradient Descent(15/49): loss=0.3821691142518237\n",
      "Gradient Descent(16/49): loss=0.3816454483935237\n",
      "Gradient Descent(17/49): loss=0.3811610752320892\n",
      "Gradient Descent(18/49): loss=0.38071155254014927\n",
      "Gradient Descent(19/49): loss=0.38029335557133215\n",
      "Gradient Descent(20/49): loss=0.37990358206787656\n",
      "Gradient Descent(21/49): loss=0.37953976920131194\n",
      "Gradient Descent(22/49): loss=0.379199777679799\n",
      "Gradient Descent(23/49): loss=0.37888171659105174\n",
      "Gradient Descent(24/49): loss=0.3785838932423474\n",
      "Gradient Descent(25/49): loss=0.378304778548505\n",
      "Gradient Descent(26/49): loss=0.378042982247203\n",
      "Gradient Descent(27/49): loss=0.3777972344475302\n",
      "Gradient Descent(28/49): loss=0.3775663713566117\n",
      "Gradient Descent(29/49): loss=0.37734932383995345\n",
      "Gradient Descent(30/49): loss=0.3771451079656688\n",
      "Gradient Descent(31/49): loss=0.37695281698668265\n",
      "Gradient Descent(32/49): loss=0.37677161440340695\n",
      "Gradient Descent(33/49): loss=0.37660072786731424\n",
      "Gradient Descent(34/49): loss=0.37643944376049837\n",
      "Gradient Descent(35/49): loss=0.3762871023342414\n",
      "Gradient Descent(36/49): loss=0.3761430933208699\n",
      "Gradient Descent(37/49): loss=0.3760068519539817\n",
      "Gradient Descent(38/49): loss=0.3758778553462699\n",
      "Gradient Descent(39/49): loss=0.37575561918405487\n",
      "Gradient Descent(40/49): loss=0.37563969470472325\n",
      "Gradient Descent(41/49): loss=0.3755296659285264\n",
      "Gradient Descent(42/49): loss=0.37542514712019526\n",
      "Gradient Descent(43/49): loss=0.3753257804589706\n",
      "Gradient Descent(44/49): loss=0.3752312338981815\n",
      "Gradient Descent(45/49): loss=0.37514119919760136\n",
      "Gradient Descent(46/49): loss=0.37505539011356404\n",
      "Gradient Descent(47/49): loss=0.3749735407333388\n",
      "Gradient Descent(48/49): loss=0.3748954039415637\n",
      "Gradient Descent(49/49): loss=0.37482075000768805\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44583302739295505\n",
      "Gradient Descent(2/49): loss=0.4222712243318378\n",
      "Gradient Descent(3/49): loss=0.40919575190511337\n",
      "Gradient Descent(4/49): loss=0.401501346006812\n",
      "Gradient Descent(5/49): loss=0.39673091515665115\n",
      "Gradient Descent(6/49): loss=0.39360107003424594\n",
      "Gradient Descent(7/49): loss=0.3914197022883469\n",
      "Gradient Descent(8/49): loss=0.38980496283411964\n",
      "Gradient Descent(9/49): loss=0.3885416307997125\n",
      "Gradient Descent(10/49): loss=0.38750575287197225\n",
      "Gradient Descent(11/49): loss=0.3866242963275782\n",
      "Gradient Descent(12/49): loss=0.38585314353886097\n",
      "Gradient Descent(13/49): loss=0.38516487917363185\n",
      "Gradient Descent(14/49): loss=0.3845419011253092\n",
      "Gradient Descent(15/49): loss=0.38397247527560774\n",
      "Gradient Descent(16/49): loss=0.3834484439355015\n",
      "Gradient Descent(17/49): loss=0.3829638767264392\n",
      "Gradient Descent(18/49): loss=0.38251426567352714\n",
      "Gradient Descent(19/49): loss=0.3820960383611334\n",
      "Gradient Descent(20/49): loss=0.38170625907105427\n",
      "Gradient Descent(21/49): loss=0.3813424422156484\n",
      "Gradient Descent(22/49): loss=0.3810024335653147\n",
      "Gradient Descent(23/49): loss=0.3806843328547106\n",
      "Gradient Descent(24/49): loss=0.38038644194618504\n",
      "Gradient Descent(25/49): loss=0.3801072289917487\n",
      "Gradient Descent(26/49): loss=0.37984530276827383\n",
      "Gradient Descent(27/49): loss=0.3795993936034931\n",
      "Gradient Descent(28/49): loss=0.37936833866783837\n",
      "Gradient Descent(29/49): loss=0.37915107023477096\n",
      "Gradient Descent(30/49): loss=0.37894660602059177\n",
      "Gradient Descent(31/49): loss=0.37875404102941335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=0.3785725405254484\n",
      "Gradient Descent(33/49): loss=0.3784013338786369\n",
      "Gradient Descent(34/49): loss=0.3782397091085722\n",
      "Gradient Descent(35/49): loss=0.3780870080026658\n",
      "Gradient Descent(36/49): loss=0.3779426217179058\n",
      "Gradient Descent(37/49): loss=0.3778059867978725\n",
      "Gradient Descent(38/49): loss=0.3776765815518746\n",
      "Gradient Descent(39/49): loss=0.37755392275368377\n",
      "Gradient Descent(40/49): loss=0.37743756262494604\n",
      "Gradient Descent(41/49): loss=0.3773270860739523\n",
      "Gradient Descent(42/49): loss=0.3772221081646999\n",
      "Gradient Descent(43/49): loss=0.3771222717944899\n",
      "Gradient Descent(44/49): loss=0.3770272455609578\n",
      "Gradient Descent(45/49): loss=0.3769367218016083\n",
      "Gradient Descent(46/49): loss=0.37685041479074666\n",
      "Gradient Descent(47/49): loss=0.3767680590802403\n",
      "Gradient Descent(48/49): loss=0.37668940797188777\n",
      "Gradient Descent(49/49): loss=0.3766142321103304\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4447022178080113\n",
      "Gradient Descent(2/49): loss=0.42070360935481804\n",
      "Gradient Descent(3/49): loss=0.4074808784998024\n",
      "Gradient Descent(4/49): loss=0.39973975950571866\n",
      "Gradient Descent(5/49): loss=0.39495863684102744\n",
      "Gradient Descent(6/49): loss=0.3918304939867746\n",
      "Gradient Descent(7/49): loss=0.3896546719185809\n",
      "Gradient Descent(8/49): loss=0.3880463415378666\n",
      "Gradient Descent(9/49): loss=0.38678933523869535\n",
      "Gradient Descent(10/49): loss=0.3857594781040027\n",
      "Gradient Descent(11/49): loss=0.3848837582747317\n",
      "Gradient Descent(12/49): loss=0.38411814394184624\n",
      "Gradient Descent(13/49): loss=0.3834353068682516\n",
      "Gradient Descent(14/49): loss=0.3828177128433282\n",
      "Gradient Descent(15/49): loss=0.382253672561417\n",
      "Gradient Descent(16/49): loss=0.38173505237037314\n",
      "Gradient Descent(17/49): loss=0.38125592941026964\n",
      "Gradient Descent(18/49): loss=0.38081179114421315\n",
      "Gradient Descent(19/49): loss=0.38039905242534394\n",
      "Gradient Descent(20/49): loss=0.38001475979712024\n",
      "Gradient Descent(21/49): loss=0.3796564073378113\n",
      "Gradient Descent(22/49): loss=0.3793218196410114\n",
      "Gradient Descent(23/49): loss=0.37900907563877717\n",
      "Gradient Descent(24/49): loss=0.3787164575664621\n",
      "Gradient Descent(25/49): loss=0.3784424156159161\n",
      "Gradient Descent(26/49): loss=0.3781855425376049\n",
      "Gradient Descent(27/49): loss=0.3779445546760712\n",
      "Gradient Descent(28/49): loss=0.37771827726412144\n",
      "Gradient Descent(29/49): loss=0.3775056326153345\n",
      "Gradient Descent(30/49): loss=0.37730563035240816\n",
      "Gradient Descent(31/49): loss=0.37711735911569944\n",
      "Gradient Descent(32/49): loss=0.3769399793870043\n",
      "Gradient Descent(33/49): loss=0.37677271718331107\n",
      "Gradient Descent(34/49): loss=0.37661485845121834\n",
      "Gradient Descent(35/49): loss=0.37646574404159155\n",
      "Gradient Descent(36/49): loss=0.3763247651759942\n",
      "Gradient Descent(37/49): loss=0.37619135933773584\n",
      "Gradient Descent(38/49): loss=0.37606500653490815\n",
      "Gradient Descent(39/49): loss=0.3759452258929393\n",
      "Gradient Descent(40/49): loss=0.3758315725415118\n",
      "Gradient Descent(41/49): loss=0.3757236347661073\n",
      "Gradient Descent(42/49): loss=0.37562103139858793\n",
      "Gradient Descent(43/49): loss=0.37552340942447454\n",
      "Gradient Descent(44/49): loss=0.37543044178722007\n",
      "Gradient Descent(45/49): loss=0.3753418253719475\n",
      "Gradient Descent(46/49): loss=0.37525727915295604\n",
      "Gradient Descent(47/49): loss=0.37517654249087473\n",
      "Gradient Descent(48/49): loss=0.3750993735667051\n",
      "Gradient Descent(49/49): loss=0.37502554794119797\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44690240325812486\n",
      "Gradient Descent(2/49): loss=0.4230722841774964\n",
      "Gradient Descent(3/49): loss=0.4095777202489449\n",
      "Gradient Descent(4/49): loss=0.40158302207042423\n",
      "Gradient Descent(5/49): loss=0.39664169510474145\n",
      "Gradient Descent(6/49): loss=0.3934269619000864\n",
      "Gradient Descent(7/49): loss=0.39120786394278667\n",
      "Gradient Descent(8/49): loss=0.38957764656947075\n",
      "Gradient Descent(9/49): loss=0.3883075076971397\n",
      "Gradient Descent(10/49): loss=0.3872670458507982\n",
      "Gradient Descent(11/49): loss=0.38638069750057924\n",
      "Gradient Descent(12/49): loss=0.3856037265374643\n",
      "Gradient Descent(13/49): loss=0.3849089006595811\n",
      "Gradient Descent(14/49): loss=0.3842790450699642\n",
      "Gradient Descent(15/49): loss=0.38370284893925943\n",
      "Gradient Descent(16/49): loss=0.38317248389465425\n",
      "Gradient Descent(17/49): loss=0.38268223880181873\n",
      "Gradient Descent(18/49): loss=0.3822277285931683\n",
      "Gradient Descent(19/49): loss=0.38180542977660226\n",
      "Gradient Descent(20/49): loss=0.3814124033538063\n",
      "Gradient Descent(21/49): loss=0.3810461262065837\n",
      "Gradient Descent(22/49): loss=0.3807043858928087\n",
      "Gradient Descent(23/49): loss=0.38038521294556105\n",
      "Gradient Descent(24/49): loss=0.38008683566497803\n",
      "Gradient Descent(25/49): loss=0.37980764863259264\n",
      "Gradient Descent(26/49): loss=0.3795461897762104\n",
      "Gradient Descent(27/49): loss=0.3793011229029486\n",
      "Gradient Descent(28/49): loss=0.3790712238405381\n",
      "Gradient Descent(29/49): loss=0.3788553690478865\n",
      "Gradient Descent(30/49): loss=0.37865252598470367\n",
      "Gradient Descent(31/49): loss=0.37846174478754013\n",
      "Gradient Descent(32/49): loss=0.3782821509560088\n",
      "Gradient Descent(33/49): loss=0.3781129388491999\n",
      "Gradient Descent(34/49): loss=0.37795336585244954\n",
      "Gradient Descent(35/49): loss=0.3778027471129193\n",
      "Gradient Descent(36/49): loss=0.37766045076738086\n",
      "Gradient Descent(37/49): loss=0.377525893602277\n",
      "Gradient Descent(38/49): loss=0.3773985370976164\n",
      "Gradient Descent(39/49): loss=0.37727788381445204\n",
      "Gradient Descent(40/49): loss=0.37716347409172957\n",
      "Gradient Descent(41/49): loss=0.37705488302290996\n",
      "Gradient Descent(42/49): loss=0.3769517176864112\n",
      "Gradient Descent(43/49): loss=0.3768536146068741\n",
      "Gradient Descent(44/49): loss=0.37676023742672216\n",
      "Gradient Descent(45/49): loss=0.37667127476957973\n",
      "Gradient Descent(46/49): loss=0.3765864382789242\n",
      "Gradient Descent(47/49): loss=0.376505460816928\n",
      "Gradient Descent(48/49): loss=0.37642809480984823\n",
      "Gradient Descent(49/49): loss=0.3763541107275619\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4464015719542097\n",
      "Gradient Descent(2/49): loss=0.42211346378602027\n",
      "Gradient Descent(3/49): loss=0.4082577615816346\n",
      "Gradient Descent(4/49): loss=0.40000694148280885\n",
      "Gradient Descent(5/49): loss=0.39489906226904276\n",
      "Gradient Descent(6/49): loss=0.3915838975543629\n",
      "Gradient Descent(7/49): loss=0.3893090795915502\n",
      "Gradient Descent(8/49): loss=0.3876517018438728\n",
      "Gradient Descent(9/49): loss=0.38637188590580607\n",
      "Gradient Descent(10/49): loss=0.38533205302397355\n",
      "Gradient Descent(11/49): loss=0.3844521621755455\n",
      "Gradient Descent(12/49): loss=0.38368476135233626\n",
      "Gradient Descent(13/49): loss=0.38300100830482414\n",
      "Gradient Descent(14/49): loss=0.3823827940289508\n",
      "Gradient Descent(15/49): loss=0.38181827811155694\n",
      "Gradient Descent(16/49): loss=0.3812993411042083\n",
      "Gradient Descent(17/49): loss=0.380820119608083\n",
      "Gradient Descent(18/49): loss=0.38037615619495796\n",
      "Gradient Descent(19/49): loss=0.37996390051997053\n",
      "Gradient Descent(20/49): loss=0.37958041233026496\n",
      "Gradient Descent(21/49): loss=0.37922318139352823\n",
      "Gradient Descent(22/49): loss=0.3788900157191462\n",
      "Gradient Descent(23/49): loss=0.3785789700842658\n",
      "Gradient Descent(24/49): loss=0.3782882986541138\n",
      "Gradient Descent(25/49): loss=0.3780164222403026\n",
      "Gradient Descent(26/49): loss=0.3777619046354706\n",
      "Gradient Descent(27/49): loss=0.37752343472123034\n",
      "Gradient Descent(28/49): loss=0.377299812364516\n",
      "Gradient Descent(29/49): loss=0.3770899368920739\n",
      "Gradient Descent(30/49): loss=0.37689279739176307\n",
      "Gradient Descent(31/49): loss=0.37670746436374236\n",
      "Gradient Descent(32/49): loss=0.37653308241054384\n",
      "Gradient Descent(33/49): loss=0.3763688637566942\n",
      "Gradient Descent(34/49): loss=0.37621408245185506\n",
      "Gradient Descent(35/49): loss=0.37606806915166985\n",
      "Gradient Descent(36/49): loss=0.37593020639664704\n",
      "Gradient Descent(37/49): loss=0.37579992432689285\n",
      "Gradient Descent(38/49): loss=0.3756766967825616\n",
      "Gradient Descent(39/49): loss=0.3755600377484902\n",
      "Gradient Descent(40/49): loss=0.375449498107846\n",
      "Gradient Descent(41/49): loss=0.3753446626744675\n",
      "Gradient Descent(42/49): loss=0.37524514747741783\n",
      "Gradient Descent(43/49): loss=0.37515059727438016\n",
      "Gradient Descent(44/49): loss=0.37506068327310654\n",
      "Gradient Descent(45/49): loss=0.37497510104232534\n",
      "Gradient Descent(46/49): loss=0.3748935685953891\n",
      "Gradient Descent(47/49): loss=0.374815824631594\n",
      "Gradient Descent(48/49): loss=0.37474162692153246\n",
      "Gradient Descent(49/49): loss=0.3746707508241263\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4472352776514188\n",
      "Gradient Descent(2/49): loss=0.42352185303194895\n",
      "Gradient Descent(3/49): loss=0.4099200244956367\n",
      "Gradient Descent(4/49): loss=0.40177753811217126\n",
      "Gradient Descent(5/49): loss=0.39671132217012056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=0.3934082355562505\n",
      "Gradient Descent(7/49): loss=0.3911334193214611\n",
      "Gradient Descent(8/49): loss=0.3894719684855746\n",
      "Gradient Descent(9/49): loss=0.38818742929776\n",
      "Gradient Descent(10/49): loss=0.3871435073472957\n",
      "Gradient Descent(11/49): loss=0.3862604947834553\n",
      "Gradient Descent(12/49): loss=0.38549088789573865\n",
      "Gradient Descent(13/49): loss=0.38480566698482693\n",
      "Gradient Descent(14/49): loss=0.38418653053201535\n",
      "Gradient Descent(15/49): loss=0.38362147172818656\n",
      "Gradient Descent(16/49): loss=0.38310224114567804\n",
      "Gradient Descent(17/49): loss=0.38262287964700753\n",
      "Gradient Descent(18/49): loss=0.3821788620860537\n",
      "Gradient Descent(19/49): loss=0.3817665917580765\n",
      "Gradient Descent(20/49): loss=0.38138309763978245\n",
      "Gradient Descent(21/49): loss=0.381025849773974\n",
      "Gradient Descent(22/49): loss=0.3806926440956309\n",
      "Gradient Descent(23/49): loss=0.380381528504501\n",
      "Gradient Descent(24/49): loss=0.3800907537525516\n",
      "Gradient Descent(25/49): loss=0.3798187394988325\n",
      "Gradient Descent(26/49): loss=0.3795640498194615\n",
      "Gradient Descent(27/49): loss=0.37932537475701145\n",
      "Gradient Descent(28/49): loss=0.37910151584278484\n",
      "Gradient Descent(29/49): loss=0.37889137432388814\n",
      "Gradient Descent(30/49): loss=0.37869394130340184\n",
      "Gradient Descent(31/49): loss=0.37850828928884617\n",
      "Gradient Descent(32/49): loss=0.37833356481883723\n",
      "Gradient Descent(33/49): loss=0.3781689819455752\n",
      "Gradient Descent(34/49): loss=0.3780138164182989\n",
      "Gradient Descent(35/49): loss=0.3778674004559121\n",
      "Gradient Descent(36/49): loss=0.37772911802507025\n",
      "Gradient Descent(37/49): loss=0.377598400558817\n",
      "Gradient Descent(38/49): loss=0.37747472306380064\n",
      "Gradient Descent(39/49): loss=0.37735760057331225\n",
      "Gradient Descent(40/49): loss=0.37724658491014196\n",
      "Gradient Descent(41/49): loss=0.3771412617283949\n",
      "Gradient Descent(42/49): loss=0.3770412478074235\n",
      "Gradient Descent(43/49): loss=0.37694618857427553\n",
      "Gradient Descent(44/49): loss=0.37685575583372877\n",
      "Gradient Descent(45/49): loss=0.37676964568723165\n",
      "Gradient Descent(46/49): loss=0.3766875766239942\n",
      "Gradient Descent(47/49): loss=0.3766092877691406\n",
      "Gradient Descent(48/49): loss=0.37653453727529607\n",
      "Gradient Descent(49/49): loss=0.37646310084526957\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44607712110325276\n",
      "Gradient Descent(2/49): loss=0.42187981053060586\n",
      "Gradient Descent(3/49): loss=0.4081111989610398\n",
      "Gradient Descent(4/49): loss=0.3999291203456315\n",
      "Gradient Descent(5/49): loss=0.39487051634615244\n",
      "Gradient Descent(6/49): loss=0.39158859060113355\n",
      "Gradient Descent(7/49): loss=0.3893356230252155\n",
      "Gradient Descent(8/49): loss=0.3876927318861147\n",
      "Gradient Descent(9/49): loss=0.38642297173678036\n",
      "Gradient Descent(10/49): loss=0.38539070709512524\n",
      "Gradient Descent(11/49): loss=0.38451710108517423\n",
      "Gradient Descent(12/49): loss=0.38375540489222626\n",
      "Gradient Descent(13/49): loss=0.3830771603588197\n",
      "Gradient Descent(14/49): loss=0.38246444878312974\n",
      "Gradient Descent(15/49): loss=0.3819055074302284\n",
      "Gradient Descent(16/49): loss=0.3813922322199451\n",
      "Gradient Descent(17/49): loss=0.3809187429080637\n",
      "Gradient Descent(18/49): loss=0.3804805504204877\n",
      "Gradient Descent(19/49): loss=0.38007406768315627\n",
      "Gradient Descent(20/49): loss=0.37969631780782187\n",
      "Gradient Descent(21/49): loss=0.37934475658987366\n",
      "Gradient Descent(22/49): loss=0.3790171618426488\n",
      "Gradient Descent(23/49): loss=0.37871156225100716\n",
      "Gradient Descent(24/49): loss=0.37842618991511284\n",
      "Gradient Descent(25/49): loss=0.3781594473401558\n",
      "Gradient Descent(26/49): loss=0.377909883424884\n",
      "Gradient Descent(27/49): loss=0.37767617520553526\n",
      "Gradient Descent(28/49): loss=0.37745711339961946\n",
      "Gradient Descent(29/49): loss=0.377251590552416\n",
      "Gradient Descent(30/49): loss=0.377058591039524\n",
      "Gradient Descent(31/49): loss=0.37687718244902263\n",
      "Gradient Descent(32/49): loss=0.3767065080307909\n",
      "Gradient Descent(33/49): loss=0.3765457800014274\n",
      "Gradient Descent(34/49): loss=0.37639427355633087\n",
      "Gradient Descent(35/49): loss=0.37625132148077706\n",
      "Gradient Descent(36/49): loss=0.3761163092781511\n",
      "Gradient Descent(37/49): loss=0.3759886707511761\n",
      "Gradient Descent(38/49): loss=0.3758678839842336\n",
      "Gradient Descent(39/49): loss=0.37575346768365675\n",
      "Gradient Descent(40/49): loss=0.3756449778393958\n",
      "Gradient Descent(41/49): loss=0.37554200467645954\n",
      "Gradient Descent(42/49): loss=0.375444169868494\n",
      "Gradient Descent(43/49): loss=0.3753511239890925\n",
      "Gradient Descent(44/49): loss=0.3752625441791061\n",
      "Gradient Descent(45/49): loss=0.3751781320105154\n",
      "Gradient Descent(46/49): loss=0.37509761152938126\n",
      "Gradient Descent(47/49): loss=0.37502072746211085\n",
      "Gradient Descent(48/49): loss=0.37494724357078574\n",
      "Gradient Descent(49/49): loss=0.374876941144632\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44857210080495735\n",
      "Gradient Descent(2/49): loss=0.4250433683734401\n",
      "Gradient Descent(3/49): loss=0.41125445608529904\n",
      "Gradient Descent(4/49): loss=0.40281506434459174\n",
      "Gradient Descent(5/49): loss=0.3974544840856357\n",
      "Gradient Descent(6/49): loss=0.39389994442614384\n",
      "Gradient Descent(7/49): loss=0.3914254581815614\n",
      "Gradient Descent(8/49): loss=0.38961181521230953\n",
      "Gradient Descent(9/49): loss=0.38821394216413296\n",
      "Gradient Descent(10/49): loss=0.38708663068242477\n",
      "Gradient Descent(11/49): loss=0.386142517696389\n",
      "Gradient Descent(12/49): loss=0.38532808209094327\n",
      "Gradient Descent(13/49): loss=0.3846098064607492\n",
      "Gradient Descent(14/49): loss=0.38396612858234797\n",
      "Gradient Descent(15/49): loss=0.38338272012348606\n",
      "Gradient Descent(16/49): loss=0.38284969363228827\n",
      "Gradient Descent(17/49): loss=0.38235993597985984\n",
      "Gradient Descent(18/49): loss=0.3819081048462328\n",
      "Gradient Descent(19/49): loss=0.38149001833318674\n",
      "Gradient Descent(20/49): loss=0.38110227933993424\n",
      "Gradient Descent(21/49): loss=0.38074204114911586\n",
      "Gradient Descent(22/49): loss=0.3804068585981436\n",
      "Gradient Descent(23/49): loss=0.380094591555679\n",
      "Gradient Descent(24/49): loss=0.37980334067008725\n",
      "Gradient Descent(25/49): loss=0.37953140325656776\n",
      "Gradient Descent(26/49): loss=0.3792772419270833\n",
      "Gradient Descent(27/49): loss=0.3790394614234326\n",
      "Gradient Descent(28/49): loss=0.3788167908448935\n",
      "Gradient Descent(29/49): loss=0.37860806951660114\n",
      "Gradient Descent(30/49): loss=0.3784122353909787\n",
      "Gradient Descent(31/49): loss=0.37822831527276374\n",
      "Gradient Descent(32/49): loss=0.3780554164052146\n",
      "Gradient Descent(33/49): loss=0.3778927191095491\n",
      "Gradient Descent(34/49): loss=0.37773947026715576\n",
      "Gradient Descent(35/49): loss=0.37759497749635945\n",
      "Gradient Descent(36/49): loss=0.3774586039158396\n",
      "Gradient Descent(37/49): loss=0.3773297634133885\n",
      "Gradient Descent(38/49): loss=0.3772079163566087\n",
      "Gradient Descent(39/49): loss=0.3770925656945467\n",
      "Gradient Descent(40/49): loss=0.3769832534080811\n",
      "Gradient Descent(41/49): loss=0.3768795572733874\n",
      "Gradient Descent(42/49): loss=0.37678108790772563\n",
      "Gradient Descent(43/49): loss=0.3766874860706834\n",
      "Gradient Descent(44/49): loss=0.37659842019713213\n",
      "Gradient Descent(45/49): loss=0.37651358414075636\n",
      "Gradient Descent(46/49): loss=0.3764326951092092\n",
      "Gradient Descent(47/49): loss=0.37635549177384825\n",
      "Gradient Descent(48/49): loss=0.37628173253865665\n",
      "Gradient Descent(49/49): loss=0.37621119395441166\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4481038870160135\n",
      "Gradient Descent(2/49): loss=0.4242718025652942\n",
      "Gradient Descent(3/49): loss=0.41021994610636575\n",
      "Gradient Descent(4/49): loss=0.4015572241757585\n",
      "Gradient Descent(5/49): loss=0.3960168469480457\n",
      "Gradient Descent(6/49): loss=0.3923241951235066\n",
      "Gradient Descent(7/49): loss=0.38974773701264775\n",
      "Gradient Descent(8/49): loss=0.3878615790260699\n",
      "Gradient Descent(9/49): loss=0.3864142097621119\n",
      "Gradient Descent(10/49): loss=0.38525482978621867\n",
      "Gradient Descent(11/49): loss=0.3842915060194276\n",
      "Gradient Descent(12/49): loss=0.38346712999923976\n",
      "Gradient Descent(13/49): loss=0.38274545008486777\n",
      "Gradient Descent(14/49): loss=0.38210286730968823\n",
      "Gradient Descent(15/49): loss=0.38152356463266296\n",
      "Gradient Descent(16/49): loss=0.3809965843768416\n",
      "Gradient Descent(17/49): loss=0.3805140558828802\n",
      "Gradient Descent(18/49): loss=0.3800701089448284\n",
      "Gradient Descent(19/49): loss=0.37966020002945605\n",
      "Gradient Descent(20/49): loss=0.37928068927381753\n",
      "Gradient Descent(21/49): loss=0.37892857124248525\n",
      "Gradient Descent(22/49): loss=0.3786013008345804\n",
      "Gradient Descent(23/49): loss=0.37829667863572003\n",
      "Gradient Descent(24/49): loss=0.3780127737865612\n",
      "Gradient Descent(25/49): loss=0.37774787079337124\n",
      "Gradient Descent(26/49): loss=0.3775004318108342\n",
      "Gradient Descent(27/49): loss=0.3772690690698107\n",
      "Gradient Descent(28/49): loss=0.3770525240710394\n",
      "Gradient Descent(29/49): loss=0.37684965138195814\n",
      "Gradient Descent(30/49): loss=0.3766594056381878\n",
      "Gradient Descent(31/49): loss=0.37648083083488365\n",
      "Gradient Descent(32/49): loss=0.3763130513013918\n",
      "Gradient Descent(33/49): loss=0.3761552639505417\n",
      "Gradient Descent(34/49): loss=0.3760067315220371\n",
      "Gradient Descent(35/49): loss=0.3758667766231348\n",
      "Gradient Descent(36/49): loss=0.37573477642513015\n",
      "Gradient Descent(37/49): loss=0.37561015791118735\n",
      "Gradient Descent(38/49): loss=0.3754923935962139\n",
      "Gradient Descent(39/49): loss=0.37538099765687827\n",
      "Gradient Descent(40/49): loss=0.37527552242215934\n",
      "Gradient Descent(41/49): loss=0.3751755551836922\n",
      "Gradient Descent(42/49): loss=0.37508071529175935\n",
      "Gradient Descent(43/49): loss=0.3749906515077752\n",
      "Gradient Descent(44/49): loss=0.37490503958802857\n",
      "Gradient Descent(45/49): loss=0.3748235800765703\n",
      "Gradient Descent(46/49): loss=0.3747459962877048\n",
      "Gradient Descent(47/49): loss=0.37467203246068265\n",
      "Gradient Descent(48/49): loss=0.37460145207102064\n",
      "Gradient Descent(49/49): loss=0.37453403628444365\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4489506392452149\n",
      "Gradient Descent(2/49): loss=0.4256880884201693\n",
      "Gradient Descent(3/49): loss=0.41189452027348955\n",
      "Gradient Descent(4/49): loss=0.4033430915796139\n",
      "Gradient Descent(5/49): loss=0.39784503182462594\n",
      "Gradient Descent(6/49): loss=0.39416347134380597\n",
      "Gradient Descent(7/49): loss=0.39158515875032524\n",
      "Gradient Descent(8/49): loss=0.3896927732632103\n",
      "Gradient Descent(9/49): loss=0.38823857822691665\n",
      "Gradient Descent(10/49): loss=0.38707323819298217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=0.38610522075949477\n",
      "Gradient Descent(12/49): loss=0.3852773680295455\n",
      "Gradient Descent(13/49): loss=0.38455322031639483\n",
      "Gradient Descent(14/49): loss=0.38390894280118865\n",
      "Gradient Descent(15/49): loss=0.3833285072987808\n",
      "Gradient Descent(16/49): loss=0.3828007856132509\n",
      "Gradient Descent(17/49): loss=0.382317777213445\n",
      "Gradient Descent(18/49): loss=0.3818735167666639\n",
      "Gradient Descent(19/49): loss=0.3814633930937439\n",
      "Gradient Descent(20/49): loss=0.38108371943760716\n",
      "Gradient Descent(21/49): loss=0.3807314586594058\n",
      "Gradient Descent(22/49): loss=0.380404044822209\n",
      "Gradient Descent(23/49): loss=0.3800992653065357\n",
      "Gradient Descent(24/49): loss=0.3798151813174331\n",
      "Gradient Descent(25/49): loss=0.37955007300382754\n",
      "Gradient Descent(26/49): loss=0.37930240054758135\n",
      "Gradient Descent(27/49): loss=0.3790707757588344\n",
      "Gradient Descent(28/49): loss=0.37885394069566525\n",
      "Gradient Descent(29/49): loss=0.3786507510694644\n",
      "Gradient Descent(30/49): loss=0.37846016298279905\n",
      "Gradient Descent(31/49): loss=0.3782812220459412\n",
      "Gradient Descent(32/49): loss=0.37811305423793906\n",
      "Gradient Descent(33/49): loss=0.37795485808427004\n",
      "Gradient Descent(34/49): loss=0.3778058978570959\n",
      "Gradient Descent(35/49): loss=0.37766549759200463\n",
      "Gradient Descent(36/49): loss=0.37753303577332836\n",
      "Gradient Descent(37/49): loss=0.37740794057916355\n",
      "Gradient Descent(38/49): loss=0.3772896856037665\n",
      "Gradient Descent(39/49): loss=0.37717778599336627\n",
      "Gradient Descent(40/49): loss=0.3770717949443832\n",
      "Gradient Descent(41/49): loss=0.37697130052238564\n",
      "Gradient Descent(42/49): loss=0.37687592276700627\n",
      "Gradient Descent(43/49): loss=0.376785311053265\n",
      "Gradient Descent(44/49): loss=0.37669914168380186\n",
      "Gradient Descent(45/49): loss=0.3766171156897555\n",
      "Gradient Descent(46/49): loss=0.3765389568206577\n",
      "Gradient Descent(47/49): loss=0.3764644097059036\n",
      "Gradient Descent(48/49): loss=0.3763932381722149\n",
      "Gradient Descent(49/49): loss=0.37632522370310534\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.447766953956402\n",
      "Gradient Descent(2/49): loss=0.42396193449750813\n",
      "Gradient Descent(3/49): loss=0.4099540802059736\n",
      "Gradient Descent(4/49): loss=0.40134470506806247\n",
      "Gradient Descent(5/49): loss=0.395858130139185\n",
      "Gradient Descent(6/49): loss=0.3922142043346197\n",
      "Gradient Descent(7/49): loss=0.3896793087916312\n",
      "Gradient Descent(8/49): loss=0.3878274291354085\n",
      "Gradient Descent(9/49): loss=0.38640785610093104\n",
      "Gradient Descent(10/49): loss=0.3852709138321498\n",
      "Gradient Descent(11/49): loss=0.3843258012197046\n",
      "Gradient Descent(12/49): loss=0.38351640800260467\n",
      "Gradient Descent(13/49): loss=0.38280729941363556\n",
      "Gradient Descent(14/49): loss=0.38217551266868355\n",
      "Gradient Descent(15/49): loss=0.3816057082891699\n",
      "Gradient Descent(16/49): loss=0.3810872764960238\n",
      "Gradient Descent(17/49): loss=0.3806125934084159\n",
      "Gradient Descent(18/49): loss=0.38017595943349586\n",
      "Gradient Descent(19/49): loss=0.3797729458722583\n",
      "Gradient Descent(20/49): loss=0.3793999878497735\n",
      "Gradient Descent(21/49): loss=0.37905412713054376\n",
      "Gradient Descent(22/49): loss=0.37873284692387993\n",
      "Gradient Descent(23/49): loss=0.3784339636635589\n",
      "Gradient Descent(24/49): loss=0.3781555544295742\n",
      "Gradient Descent(25/49): loss=0.37789590692217273\n",
      "Gradient Descent(26/49): loss=0.37765348389703723\n",
      "Gradient Descent(27/49): loss=0.37742689702176385\n",
      "Gradient Descent(28/49): loss=0.3772148869881359\n",
      "Gradient Descent(29/49): loss=0.3770163078732796\n",
      "Gradient Descent(30/49): loss=0.37683011446344084\n",
      "Gradient Descent(31/49): loss=0.37665535170532843\n",
      "Gradient Descent(32/49): loss=0.37649114573445025\n",
      "Gradient Descent(33/49): loss=0.3763366961106425\n",
      "Gradient Descent(34/49): loss=0.3761912690068991\n",
      "Gradient Descent(35/49): loss=0.3760541911727075\n",
      "Gradient Descent(36/49): loss=0.3759248445423834\n",
      "Gradient Descent(37/49): loss=0.3758026613917269\n",
      "Gradient Descent(38/49): loss=0.375687119968611\n",
      "Gradient Descent(39/49): loss=0.37557774053854914\n",
      "Gradient Descent(40/49): loss=0.37547408179727393\n",
      "Gradient Descent(41/49): loss=0.37537573761035764\n",
      "Gradient Descent(42/49): loss=0.37528233404593286\n",
      "Gradient Descent(43/49): loss=0.3751935266712176\n",
      "Gradient Descent(44/49): loss=0.3751089980872465\n",
      "Gradient Descent(45/49): loss=0.3750284556792224\n",
      "Gradient Descent(46/49): loss=0.3749516295624109\n",
      "Gradient Descent(47/49): loss=0.37487827070562857\n",
      "Gradient Descent(48/49): loss=0.37480814921620725\n",
      "Gradient Descent(49/49): loss=0.3747410527719169\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.450552366144676\n",
      "Gradient Descent(2/49): loss=0.4280588710616432\n",
      "Gradient Descent(3/49): loss=0.41456606293607273\n",
      "Gradient Descent(4/49): loss=0.4059787062154445\n",
      "Gradient Descent(5/49): loss=0.40024509668528074\n",
      "Gradient Descent(6/49): loss=0.39623243183975837\n",
      "Gradient Descent(7/49): loss=0.3932964328954028\n",
      "Gradient Descent(8/49): loss=0.3910603226153392\n",
      "Gradient Descent(9/49): loss=0.3892970379416372\n",
      "Gradient Descent(10/49): loss=0.38786530375548395\n",
      "Gradient Descent(11/49): loss=0.3866742863183964\n",
      "Gradient Descent(12/49): loss=0.38566364365040307\n",
      "Gradient Descent(13/49): loss=0.38479200836171845\n",
      "Gradient Descent(14/49): loss=0.38403017320097177\n",
      "Gradient Descent(15/49): loss=0.38335695226122624\n",
      "Gradient Descent(16/49): loss=0.38275659825321307\n",
      "Gradient Descent(17/49): loss=0.38221714639928767\n",
      "Gradient Descent(18/49): loss=0.38172932409277016\n",
      "Gradient Descent(19/49): loss=0.3812858149941346\n",
      "Gradient Descent(20/49): loss=0.3808807509354399\n",
      "Gradient Descent(21/49): loss=0.38050935390582075\n",
      "Gradient Descent(22/49): loss=0.38016767921609845\n",
      "Gradient Descent(23/49): loss=0.37985242831423016\n",
      "Gradient Descent(24/49): loss=0.37956081044214884\n",
      "Gradient Descent(25/49): loss=0.3792904390969623\n",
      "Gradient Descent(26/49): loss=0.37903925364030083\n",
      "Gradient Descent(27/49): loss=0.37880545929783277\n",
      "Gradient Descent(28/49): loss=0.378587480748637\n",
      "Gradient Descent(29/49): loss=0.37838392585140973\n",
      "Gradient Descent(30/49): loss=0.3781935569969335\n",
      "Gradient Descent(31/49): loss=0.37801526824479387\n",
      "Gradient Descent(32/49): loss=0.37784806688219147\n",
      "Gradient Descent(33/49): loss=0.37769105839055145\n",
      "Gradient Descent(34/49): loss=0.3775434340598524\n",
      "Gradient Descent(35/49): loss=0.37740446067770106\n",
      "Gradient Descent(36/49): loss=0.3772734718586754\n",
      "Gradient Descent(37/49): loss=0.3771498606825332\n",
      "Gradient Descent(38/49): loss=0.3770330733869292\n",
      "Gradient Descent(39/49): loss=0.376922603918147\n",
      "Gradient Descent(40/49): loss=0.37681798918698084\n",
      "Gradient Descent(41/49): loss=0.37671880490994814\n",
      "Gradient Descent(42/49): loss=0.3766246619411579\n",
      "Gradient Descent(43/49): loss=0.3765352030193773\n",
      "Gradient Descent(44/49): loss=0.3764500998696136\n",
      "Gradient Descent(45/49): loss=0.376369050609946\n",
      "Gradient Descent(46/49): loss=0.37629177742322395\n",
      "Gradient Descent(47/49): loss=0.37621802446020575\n",
      "Gradient Descent(48/49): loss=0.376147555946206\n",
      "Gradient Descent(49/49): loss=0.3760801544676843\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4501210487095809\n",
      "Gradient Descent(2/49): loss=0.427531943521945\n",
      "Gradient Descent(3/49): loss=0.4139726475227485\n",
      "Gradient Descent(4/49): loss=0.40530382030996037\n",
      "Gradient Descent(5/49): loss=0.3994697777069811\n",
      "Gradient Descent(6/49): loss=0.3953442642463673\n",
      "Gradient Descent(7/49): loss=0.39229120011680296\n",
      "Gradient Descent(8/49): loss=0.38994073751051705\n",
      "Gradient Descent(9/49): loss=0.3880706883303927\n",
      "Gradient Descent(10/49): loss=0.38654268811513914\n",
      "Gradient Descent(11/49): loss=0.38526724596435924\n",
      "Gradient Descent(12/49): loss=0.38418423950384367\n",
      "Gradient Descent(13/49): loss=0.3832517863932475\n",
      "Gradient Descent(14/49): loss=0.3824397323728269\n",
      "Gradient Descent(15/49): loss=0.38172573086459405\n",
      "Gradient Descent(16/49): loss=0.3810928084921674\n",
      "Gradient Descent(17/49): loss=0.3805278034210448\n",
      "Gradient Descent(18/49): loss=0.3800203304656427\n",
      "Gradient Descent(19/49): loss=0.37956207362344996\n",
      "Gradient Descent(20/49): loss=0.37914628851340204\n",
      "Gradient Descent(21/49): loss=0.3787674436072307\n",
      "Gradient Descent(22/49): loss=0.3784209559913657\n",
      "Gradient Descent(23/49): loss=0.3781029932799767\n",
      "Gradient Descent(24/49): loss=0.3778103229339514\n",
      "Gradient Descent(25/49): loss=0.3775401962459879\n",
      "Gradient Descent(26/49): loss=0.3772902581040416\n",
      "Gradient Descent(27/49): loss=0.3770584761883568\n",
      "Gradient Descent(28/49): loss=0.3768430849832222\n",
      "Gradient Descent(29/49): loss=0.3766425411861537\n",
      "Gradient Descent(30/49): loss=0.3764554879527276\n",
      "Gradient Descent(31/49): loss=0.37628072603621987\n",
      "Gradient Descent(32/49): loss=0.37611719033905544\n",
      "Gradient Descent(33/49): loss=0.37596393073505346\n",
      "Gradient Descent(34/49): loss=0.3758200962795158\n",
      "Gradient Descent(35/49): loss=0.37568492212055044\n",
      "Gradient Descent(36/49): loss=0.37555771857537157\n",
      "Gradient Descent(37/49): loss=0.3754378619510677\n",
      "Gradient Descent(38/49): loss=0.37532478677881925\n",
      "Gradient Descent(39/49): loss=0.37521797919999156\n",
      "Gradient Descent(40/49): loss=0.37511697129659305\n",
      "Gradient Descent(41/49): loss=0.37502133620080275\n",
      "Gradient Descent(42/49): loss=0.3749306838513256\n",
      "Gradient Descent(43/49): loss=0.3748446572902888\n",
      "Gradient Descent(44/49): loss=0.37476292941483585\n",
      "Gradient Descent(45/49): loss=0.37468520011371675\n",
      "Gradient Descent(46/49): loss=0.3746111937319701\n",
      "Gradient Descent(47/49): loss=0.37454065681696586\n",
      "Gradient Descent(48/49): loss=0.37447335610720245\n",
      "Gradient Descent(49/49): loss=0.37440907673175733\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45097911217434317\n",
      "Gradient Descent(2/49): loss=0.42894590747185773\n",
      "Gradient Descent(3/49): loss=0.4156503361632559\n",
      "Gradient Descent(4/49): loss=0.40710299909369024\n",
      "Gradient Descent(5/49): loss=0.40132161052532206\n",
      "Gradient Descent(6/49): loss=0.39721553986480124\n",
      "Gradient Descent(7/49): loss=0.39416620332902413\n",
      "Gradient Descent(8/49): loss=0.39181234499673184\n",
      "Gradient Descent(9/49): loss=0.3899359753076701\n",
      "Gradient Descent(10/49): loss=0.3884007590512737\n",
      "Gradient Descent(11/49): loss=0.38711816307047936\n",
      "Gradient Descent(12/49): loss=0.3860284932651436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=0.3850900340614641\n",
      "Gradient Descent(14/49): loss=0.3842726693639519\n",
      "Gradient Descent(15/49): loss=0.3835540285132833\n",
      "Gradient Descent(16/49): loss=0.3829170852996555\n",
      "Gradient Descent(17/49): loss=0.3823486134614633\n",
      "Gradient Descent(18/49): loss=0.3818381607197715\n",
      "Gradient Descent(19/49): loss=0.38137734599444784\n",
      "Gradient Descent(20/49): loss=0.3809593642589449\n",
      "Gradient Descent(21/49): loss=0.3805786289276094\n",
      "Gradient Descent(22/49): loss=0.3802305080428782\n",
      "Gradient Descent(23/49): loss=0.3799111261776935\n",
      "Gradient Descent(24/49): loss=0.3796172134827582\n",
      "Gradient Descent(25/49): loss=0.37934598924901475\n",
      "Gradient Descent(26/49): loss=0.37909507117020796\n",
      "Gradient Descent(27/49): loss=0.37886240400962745\n",
      "Gradient Descent(28/49): loss=0.37864620308489155\n",
      "Gradient Descent(29/49): loss=0.3784449091747121\n",
      "Gradient Descent(30/49): loss=0.37825715229878587\n",
      "Gradient Descent(31/49): loss=0.3780817224368556\n",
      "Gradient Descent(32/49): loss=0.3779175457065595\n",
      "Gradient Descent(33/49): loss=0.37776366485873225\n",
      "Gradient Descent(34/49): loss=0.37761922320495567\n",
      "Gradient Descent(35/49): loss=0.37748345128734573\n",
      "Gradient Descent(36/49): loss=0.3773556557503093\n",
      "Gradient Descent(37/49): loss=0.37723520998953736\n",
      "Gradient Descent(38/49): loss=0.37712154624304406\n",
      "Gradient Descent(39/49): loss=0.3770141488587286\n",
      "Gradient Descent(40/49): loss=0.3769125485273167\n",
      "Gradient Descent(41/49): loss=0.3768163173121264\n",
      "Gradient Descent(42/49): loss=0.37672506434053665\n",
      "Gradient Descent(43/49): loss=0.37663843204837116\n",
      "Gradient Descent(44/49): loss=0.37655609288919895\n",
      "Gradient Descent(45/49): loss=0.3764777464370239\n",
      "Gradient Descent(46/49): loss=0.37640311682391603\n",
      "Gradient Descent(47/49): loss=0.37633195046456147\n",
      "Gradient Descent(48/49): loss=0.37626401402805454\n",
      "Gradient Descent(49/49): loss=0.3761990926239492\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.449771716367459\n",
      "Gradient Descent(2/49): loss=0.42712459729321395\n",
      "Gradient Descent(3/49): loss=0.41352630748695485\n",
      "Gradient Descent(4/49): loss=0.4048494957282281\n",
      "Gradient Descent(5/49): loss=0.3990332162033572\n",
      "Gradient Descent(6/49): loss=0.39494286140705104\n",
      "Gradient Descent(7/49): loss=0.39193505999487493\n",
      "Gradient Descent(8/49): loss=0.38963445250293116\n",
      "Gradient Descent(9/49): loss=0.387815022147727\n",
      "Gradient Descent(10/49): loss=0.3863359368675876\n",
      "Gradient Descent(11/49): loss=0.38510625293022516\n",
      "Gradient Descent(12/49): loss=0.3840651102613273\n",
      "Gradient Descent(13/49): loss=0.3831703687307138\n",
      "Gradient Descent(14/49): loss=0.38239192186389903\n",
      "Gradient Descent(15/49): loss=0.3817076524569982\n",
      "Gradient Descent(16/49): loss=0.3811009129714607\n",
      "Gradient Descent(17/49): loss=0.3805589074717255\n",
      "Gradient Descent(18/49): loss=0.3800716208956455\n",
      "Gradient Descent(19/49): loss=0.3796310900885116\n",
      "Gradient Descent(20/49): loss=0.3792308944790034\n",
      "Gradient Descent(21/49): loss=0.378865791975426\n",
      "Gradient Descent(22/49): loss=0.3785314534866031\n",
      "Gradient Descent(23/49): loss=0.37822426607589915\n",
      "Gradient Descent(24/49): loss=0.3779411849123514\n",
      "Gradient Descent(25/49): loss=0.3776796205587676\n",
      "Gradient Descent(26/49): loss=0.3774373522479106\n",
      "Gradient Descent(27/49): loss=0.37721246051936647\n",
      "Gradient Descent(28/49): loss=0.3770032744363311\n",
      "Gradient Descent(29/49): loss=0.3768083298830512\n",
      "Gradient Descent(30/49): loss=0.37662633635067894\n",
      "Gradient Descent(31/49): loss=0.3764561502721377\n",
      "Gradient Descent(32/49): loss=0.3762967534431039\n",
      "Gradient Descent(33/49): loss=0.3761472354180225\n",
      "Gradient Descent(34/49): loss=0.37600677903225227\n",
      "Gradient Descent(35/49): loss=0.3758746483983137\n",
      "Gradient Descent(36/49): loss=0.3757501788729617\n",
      "Gradient Descent(37/49): loss=0.37563276860479206\n",
      "Gradient Descent(38/49): loss=0.3755218713582842\n",
      "Gradient Descent(39/49): loss=0.37541699037619675\n",
      "Gradient Descent(40/49): loss=0.3753176730929851\n",
      "Gradient Descent(41/49): loss=0.375223506551052\n",
      "Gradient Descent(42/49): loss=0.37513411340194286\n",
      "Gradient Descent(43/49): loss=0.37504914839813575\n",
      "Gradient Descent(44/49): loss=0.3749682952994293\n",
      "Gradient Descent(45/49): loss=0.3748912641322963\n",
      "Gradient Descent(46/49): loss=0.3748177887518712\n",
      "Gradient Descent(47/49): loss=0.3747476246651621\n",
      "Gradient Descent(48/49): loss=0.37468054708116383\n",
      "Gradient Descent(49/49): loss=0.3746163491592072\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45284319927728084\n",
      "Gradient Descent(2/49): loss=0.43230550677709995\n",
      "Gradient Descent(3/49): loss=0.4201569041472995\n",
      "Gradient Descent(4/49): loss=0.41233086230019533\n",
      "Gradient Descent(5/49): loss=0.40690273794537796\n",
      "Gradient Descent(6/49): loss=0.40287068180585894\n",
      "Gradient Descent(7/49): loss=0.3996990748780312\n",
      "Gradient Descent(8/49): loss=0.39709344055592677\n",
      "Gradient Descent(9/49): loss=0.3948859794250832\n",
      "Gradient Descent(10/49): loss=0.39297662319995397\n",
      "Gradient Descent(11/49): loss=0.39130231576420016\n",
      "Gradient Descent(12/49): loss=0.38982079747929216\n",
      "Gradient Descent(13/49): loss=0.3885019194701983\n",
      "Gradient Descent(14/49): loss=0.3873229092646515\n",
      "Gradient Descent(15/49): loss=0.3862657333009184\n",
      "Gradient Descent(16/49): loss=0.38531558600006766\n",
      "Gradient Descent(17/49): loss=0.38445999279851284\n",
      "Gradient Descent(18/49): loss=0.38368825363177994\n",
      "Gradient Descent(19/49): loss=0.3829910793742985\n",
      "Gradient Descent(20/49): loss=0.3823603407414774\n",
      "Gradient Descent(21/49): loss=0.38178888510366804\n",
      "Gradient Descent(22/49): loss=0.3812703961243558\n",
      "Gradient Descent(23/49): loss=0.3807992817816136\n",
      "Gradient Descent(24/49): loss=0.3803705822242885\n",
      "Gradient Descent(25/49): loss=0.3799798922215193\n",
      "Gradient Descent(26/49): loss=0.37962329485287594\n",
      "Gradient Descent(27/49): loss=0.37929730418908864\n",
      "Gradient Descent(28/49): loss=0.3789988153753704\n",
      "Gradient Descent(29/49): loss=0.378725060940866\n",
      "Gradient Descent(30/49): loss=0.37847357242433083\n",
      "Gradient Descent(31/49): loss=0.3782421465869857\n",
      "Gradient Descent(32/49): loss=0.3780288156121772\n",
      "Gradient Descent(33/49): loss=0.3778318207873446\n",
      "Gradient Descent(34/49): loss=0.37764958923815584\n",
      "Gradient Descent(35/49): loss=0.37748071334433125\n",
      "Gradient Descent(36/49): loss=0.37732393251577195\n",
      "Gradient Descent(37/49): loss=0.3771781170488242\n",
      "Gradient Descent(38/49): loss=0.3770422538175906\n",
      "Gradient Descent(39/49): loss=0.37691543358536017\n",
      "Gradient Descent(40/49): loss=0.37679683974733835\n",
      "Gradient Descent(41/49): loss=0.37668573833857893\n",
      "Gradient Descent(42/49): loss=0.3765814691608589\n",
      "Gradient Descent(43/49): loss=0.3764834378996028\n",
      "Gradient Descent(44/49): loss=0.3763911091171902\n",
      "Gradient Descent(45/49): loss=0.3763040000223571\n",
      "Gradient Descent(46/49): loss=0.37622167492715636\n",
      "Gradient Descent(47/49): loss=0.37614374031328734\n",
      "Gradient Descent(48/49): loss=0.37606984043871117\n",
      "Gradient Descent(49/49): loss=0.37599965342349256\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.452453057034912\n",
      "Gradient Descent(2/49): loss=0.4320886429070844\n",
      "Gradient Descent(3/49): loss=0.42021481671481137\n",
      "Gradient Descent(4/49): loss=0.4126624815714106\n",
      "Gradient Descent(5/49): loss=0.4074669009279942\n",
      "Gradient Descent(6/49): loss=0.40361300916137766\n",
      "Gradient Descent(7/49): loss=0.40056465897327803\n",
      "Gradient Descent(8/49): loss=0.3980323886453696\n",
      "Gradient Descent(9/49): loss=0.39585539580466866\n",
      "Gradient Descent(10/49): loss=0.3939408882397144\n",
      "Gradient Descent(11/49): loss=0.39223260712990793\n",
      "Gradient Descent(12/49): loss=0.39069432252040764\n",
      "Gradient Descent(13/49): loss=0.3893010832059686\n",
      "Gradient Descent(14/49): loss=0.3880345190952038\n",
      "Gradient Descent(15/49): loss=0.3868802807671269\n",
      "Gradient Descent(16/49): loss=0.385826616868879\n",
      "Gradient Descent(17/49): loss=0.38486356357704077\n",
      "Gradient Descent(18/49): loss=0.38398246721311363\n",
      "Gradient Descent(19/49): loss=0.38317569081389674\n",
      "Gradient Descent(20/49): loss=0.38243642413548307\n",
      "Gradient Descent(21/49): loss=0.3817585532088221\n",
      "Gradient Descent(22/49): loss=0.3811365652607974\n",
      "Gradient Descent(23/49): loss=0.3805654754862568\n",
      "Gradient Descent(24/49): loss=0.3800407679873192\n",
      "Gradient Descent(25/49): loss=0.37955834641241054\n",
      "Gradient Descent(26/49): loss=0.37911449162099736\n",
      "Gradient Descent(27/49): loss=0.3787058247136207\n",
      "Gradient Descent(28/49): loss=0.37832927434937996\n",
      "Gradient Descent(29/49): loss=0.3779820476150287\n",
      "Gradient Descent(30/49): loss=0.377661603916208\n",
      "Gradient Descent(31/49): loss=0.37736563149014085\n",
      "Gradient Descent(32/49): loss=0.37709202622287763\n",
      "Gradient Descent(33/49): loss=0.3768388725112162\n",
      "Gradient Descent(34/49): loss=0.3766044259502081\n",
      "Gradient Descent(35/49): loss=0.37638709765773426\n",
      "Gradient Descent(36/49): loss=0.3761854400715571\n",
      "Gradient Descent(37/49): loss=0.37599813407364757\n",
      "Gradient Descent(38/49): loss=0.37582397731276473\n",
      "Gradient Descent(39/49): loss=0.3756618736100487\n",
      "Gradient Descent(40/49): loss=0.3755108233443278\n",
      "Gradient Descent(41/49): loss=0.3753699147242922\n",
      "Gradient Descent(42/49): loss=0.3752383158639244\n",
      "Gradient Descent(43/49): loss=0.37511526758577335\n",
      "Gradient Descent(44/49): loss=0.3750000768839941\n",
      "Gradient Descent(45/49): loss=0.3748921109856196\n",
      "Gradient Descent(46/49): loss=0.3747907919544255\n",
      "Gradient Descent(47/49): loss=0.3746955917870363\n",
      "Gradient Descent(48/49): loss=0.3746060279556946\n",
      "Gradient Descent(49/49): loss=0.3745216593564081\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45332069643880374\n",
      "Gradient Descent(2/49): loss=0.4334885803284734\n",
      "Gradient Descent(3/49): loss=0.4218828154313581\n",
      "Gradient Descent(4/49): loss=0.4144711204043957\n",
      "Gradient Descent(5/49): loss=0.4093557819509371\n",
      "Gradient Descent(6/49): loss=0.40555261388055464\n",
      "Gradient Descent(7/49): loss=0.4025395112837641\n",
      "Gradient Descent(8/49): loss=0.4000333636054958\n",
      "Gradient Descent(9/49): loss=0.3978762777450441\n",
      "Gradient Descent(10/49): loss=0.3959768737897811\n",
      "Gradient Descent(11/49): loss=0.39427967820149146\n",
      "Gradient Descent(12/49): loss=0.3927489945702678\n",
      "Gradient Descent(13/49): loss=0.39136030611073935\n",
      "Gradient Descent(14/49): loss=0.39009563394453084\n",
      "Gradient Descent(15/49): loss=0.38894099306529223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=0.38788497176002384\n",
      "Gradient Descent(17/49): loss=0.38691791915371876\n",
      "Gradient Descent(18/49): loss=0.3860314658812488\n",
      "Gradient Descent(19/49): loss=0.38521822983156745\n",
      "Gradient Descent(20/49): loss=0.38447162651217087\n",
      "Gradient Descent(21/49): loss=0.3837857398774751\n",
      "Gradient Descent(22/49): loss=0.38315522911088573\n",
      "Gradient Descent(23/49): loss=0.3825752575732655\n",
      "Gradient Descent(24/49): loss=0.3820414360346799\n",
      "Gradient Descent(25/49): loss=0.3815497755884445\n",
      "Gradient Descent(26/49): loss=0.38109664749093136\n",
      "Gradient Descent(27/49): loss=0.3806787482203896\n",
      "Gradient Descent(28/49): loss=0.38029306865487383\n",
      "Gradient Descent(29/49): loss=0.37993686662700143\n",
      "Gradient Descent(30/49): loss=0.37960764232915684\n",
      "Gradient Descent(31/49): loss=0.37930311617707446\n",
      "Gradient Descent(32/49): loss=0.3790212088263696\n",
      "Gradient Descent(33/49): loss=0.37876002309484696\n",
      "Gradient Descent(34/49): loss=0.37851782758441094\n",
      "Gradient Descent(35/49): loss=0.37829304182659473\n",
      "Gradient Descent(36/49): loss=0.3780842227989359\n",
      "Gradient Descent(37/49): loss=0.3778900526779516\n",
      "Gradient Descent(38/49): loss=0.3777093277097129\n",
      "Gradient Descent(39/49): loss=0.37754094809188116\n",
      "Gradient Descent(40/49): loss=0.37738390877211564\n",
      "Gradient Descent(41/49): loss=0.3772372910773816\n",
      "Gradient Descent(42/49): loss=0.3771002550971524\n",
      "Gradient Descent(43/49): loss=0.37697203275100066\n",
      "Gradient Descent(44/49): loss=0.3768519214777511\n",
      "Gradient Descent(45/49): loss=0.37673927848935274\n",
      "Gradient Descent(46/49): loss=0.3766335155379857\n",
      "Gradient Descent(47/49): loss=0.37653409414974726\n",
      "Gradient Descent(48/49): loss=0.37644052128260985\n",
      "Gradient Descent(49/49): loss=0.37635234537026335\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4520914083364237\n",
      "Gradient Descent(2/49): loss=0.43155966787942823\n",
      "Gradient Descent(3/49): loss=0.4195055603436531\n",
      "Gradient Descent(4/49): loss=0.41179523726550205\n",
      "Gradient Descent(5/49): loss=0.4064730825471867\n",
      "Gradient Descent(6/49): loss=0.4025244376188844\n",
      "Gradient Descent(7/49): loss=0.39941034102260903\n",
      "Gradient Descent(8/49): loss=0.396837484777044\n",
      "Gradient Descent(9/49): loss=0.3946411279317478\n",
      "Gradient Descent(10/49): loss=0.39272482745468495\n",
      "Gradient Descent(11/49): loss=0.3910290888610915\n",
      "Gradient Descent(12/49): loss=0.3895148786721116\n",
      "Gradient Descent(13/49): loss=0.3881548464299584\n",
      "Gradient Descent(14/49): loss=0.3869285839717579\n",
      "Gradient Descent(15/49): loss=0.3858200195560812\n",
      "Gradient Descent(16/49): loss=0.38481595266808555\n",
      "Gradient Descent(17/49): loss=0.38390520544541934\n",
      "Gradient Descent(18/49): loss=0.3830781120545343\n",
      "Gradient Descent(19/49): loss=0.38232619647963395\n",
      "Gradient Descent(20/49): loss=0.381641957686281\n",
      "Gradient Descent(21/49): loss=0.3810187177489909\n",
      "Gradient Descent(22/49): loss=0.3804505082773119\n",
      "Gradient Descent(23/49): loss=0.37993198121131916\n",
      "Gradient Descent(24/49): loss=0.3794583359509898\n",
      "Gradient Descent(25/49): loss=0.37902525805457143\n",
      "Gradient Descent(26/49): loss=0.3786288665800397\n",
      "Gradient Descent(27/49): loss=0.37826566819476753\n",
      "Gradient Descent(28/49): loss=0.3779325167917882\n",
      "Gradient Descent(29/49): loss=0.3776265777184696\n",
      "Gradient Descent(30/49): loss=0.3773452959509924\n",
      "Gradient Descent(31/49): loss=0.37708636769474996\n",
      "Gradient Descent(32/49): loss=0.3768477149898171\n",
      "Gradient Descent(33/49): loss=0.3766274629708057\n",
      "Gradient Descent(34/49): loss=0.3764239194825788\n",
      "Gradient Descent(35/49): loss=0.3762355567937592\n",
      "Gradient Descent(36/49): loss=0.37606099518253727\n",
      "Gradient Descent(37/49): loss=0.3758989881962477\n",
      "Gradient Descent(38/49): loss=0.3757484094090104\n",
      "Gradient Descent(39/49): loss=0.3756082405213571\n",
      "Gradient Descent(40/49): loss=0.375477560662841\n",
      "Gradient Descent(41/49): loss=0.3753555367735903\n",
      "Gradient Descent(42/49): loss=0.3752414149539853\n",
      "Gradient Descent(43/49): loss=0.3751345126833207\n",
      "Gradient Descent(44/49): loss=0.37503421181871643\n",
      "Gradient Descent(45/49): loss=0.37493995229478455\n",
      "Gradient Descent(46/49): loss=0.3748512264528131\n",
      "Gradient Descent(47/49): loss=0.3747675739355951\n",
      "Gradient Descent(48/49): loss=0.3746885770906094\n",
      "Gradient Descent(49/49): loss=0.37461385683015763\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45544460020277194\n",
      "Gradient Descent(2/49): loss=0.43798687700522204\n",
      "Gradient Descent(3/49): loss=0.4288542022055414\n",
      "Gradient Descent(4/49): loss=0.42377256048288353\n",
      "Gradient Descent(5/49): loss=0.4207901921131811\n",
      "Gradient Descent(6/49): loss=0.4189334412155697\n",
      "Gradient Descent(7/49): loss=0.4177060104292721\n",
      "Gradient Descent(8/49): loss=0.4168503153590824\n",
      "Gradient Descent(9/49): loss=0.41622975970086085\n",
      "Gradient Descent(10/49): loss=0.415769871711364\n",
      "Gradient Descent(11/49): loss=0.41542853677846725\n",
      "Gradient Descent(12/49): loss=0.41518077619609184\n",
      "Gradient Descent(13/49): loss=0.4150108755814067\n",
      "Gradient Descent(14/49): loss=0.41490826509246376\n",
      "Gradient Descent(15/49): loss=0.4148653342279148\n",
      "Gradient Descent(16/49): loss=0.41487625428470704\n",
      "Gradient Descent(17/49): loss=0.4149363311639954\n",
      "Gradient Descent(18/49): loss=0.41504164045958986\n",
      "Gradient Descent(19/49): loss=0.41518881472063524\n",
      "Gradient Descent(20/49): loss=0.41537491401048315\n",
      "Gradient Descent(21/49): loss=0.41559734293982964\n",
      "Gradient Descent(22/49): loss=0.41585379427640334\n",
      "Gradient Descent(23/49): loss=0.4161422082448705\n",
      "Gradient Descent(24/49): loss=0.4164607414712014\n",
      "Gradient Descent(25/49): loss=0.4168077421506409\n",
      "Gradient Descent(26/49): loss=0.41718172945692117\n",
      "Gradient Descent(27/49): loss=0.41758137600845563\n",
      "Gradient Descent(28/49): loss=0.41800549265664017\n",
      "Gradient Descent(29/49): loss=0.41845301511915317\n",
      "Gradient Descent(30/49): loss=0.4189229921324472\n",
      "Gradient Descent(31/49): loss=0.4194145748890696\n",
      "Gradient Descent(32/49): loss=0.41992700758272594\n",
      "Gradient Descent(33/49): loss=0.4204596189214181\n",
      "Gradient Descent(34/49): loss=0.421011814494579\n",
      "Gradient Descent(35/49): loss=0.4215830698985172\n",
      "Gradient Descent(36/49): loss=0.42217292453830213\n",
      "Gradient Descent(37/49): loss=0.42278097603502623\n",
      "Gradient Descent(38/49): loss=0.42340687517614733\n",
      "Gradient Descent(39/49): loss=0.4240503213538874\n",
      "Gradient Descent(40/49): loss=0.4247110584428444\n",
      "Gradient Descent(41/49): loss=0.4253888710732885\n",
      "Gradient Descent(42/49): loss=0.4260835812612519\n",
      "Gradient Descent(43/49): loss=0.42679504536057516\n",
      "Gradient Descent(44/49): loss=0.42752315130567775\n",
      "Gradient Descent(45/49): loss=0.4282678161169787\n",
      "Gradient Descent(46/49): loss=0.4290289836437516\n",
      "Gradient Descent(47/49): loss=0.4298066225217037\n",
      "Gradient Descent(48/49): loss=0.4306007243248325\n",
      "Gradient Descent(49/49): loss=0.4314113018931404\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4550999119920067\n",
      "Gradient Descent(2/49): loss=0.4381540650781806\n",
      "Gradient Descent(3/49): loss=0.42984148992736987\n",
      "Gradient Descent(4/49): loss=0.4257656360142584\n",
      "Gradient Descent(5/49): loss=0.42391788023264165\n",
      "Gradient Descent(6/49): loss=0.4232978459688467\n",
      "Gradient Descent(7/49): loss=0.42339836854863605\n",
      "Gradient Descent(8/49): loss=0.4239590106247825\n",
      "Gradient Descent(9/49): loss=0.42484434855512976\n",
      "Gradient Descent(10/49): loss=0.42598314183079894\n",
      "Gradient Descent(11/49): loss=0.42733761523462993\n",
      "Gradient Descent(12/49): loss=0.42888778915154063\n",
      "Gradient Descent(13/49): loss=0.4306234038948146\n",
      "Gradient Descent(14/49): loss=0.4325397132228191\n",
      "Gradient Descent(15/49): loss=0.43463526811313913\n",
      "Gradient Descent(16/49): loss=0.43691073433843336\n",
      "Gradient Descent(17/49): loss=0.4393682526574278\n",
      "Gradient Descent(18/49): loss=0.442011087181368\n",
      "Gradient Descent(19/49): loss=0.4448434289679774\n",
      "Gradient Descent(20/49): loss=0.447870284755717\n",
      "Gradient Descent(21/49): loss=0.45109741353975763\n",
      "Gradient Descent(22/49): loss=0.4545312909333666\n",
      "Gradient Descent(23/49): loss=0.4581790904019174\n",
      "Gradient Descent(24/49): loss=0.46204867534923877\n",
      "Gradient Descent(25/49): loss=0.46614859868028374\n",
      "Gradient Descent(26/49): loss=0.4704881079099822\n",
      "Gradient Descent(27/49): loss=0.47507715469007883\n",
      "Gradient Descent(28/49): loss=0.4799264080784768\n",
      "Gradient Descent(29/49): loss=0.4850472711371809\n",
      "Gradient Descent(30/49): loss=0.49045190060058874\n",
      "Gradient Descent(31/49): loss=0.4961532294520906\n",
      "Gradient Descent(32/49): loss=0.5021649923093088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=0.5085017535609608\n",
      "Gradient Descent(34/49): loss=0.5151789382292961\n",
      "Gradient Descent(35/49): loss=0.5222128655559146\n",
      "Gradient Descent(36/49): loss=0.5296207853281771\n",
      "Gradient Descent(37/49): loss=0.5374209169799082\n",
      "Gradient Descent(38/49): loss=0.5456324915147093\n",
      "Gradient Descent(39/49): loss=0.5542757963135181\n",
      "Gradient Descent(40/49): loss=0.5633722229004757\n",
      "Gradient Descent(41/49): loss=0.5729443177529375\n",
      "Gradient Descent(42/49): loss=0.5830158362529164\n",
      "Gradient Descent(43/49): loss=0.5936117998883151\n",
      "Gradient Descent(44/49): loss=0.6047585568233365\n",
      "Gradient Descent(45/49): loss=0.6164838459683297\n",
      "Gradient Descent(46/49): loss=0.6288168646903529\n",
      "Gradient Descent(47/49): loss=0.6417883403167213\n",
      "Gradient Descent(48/49): loss=0.6554306055950198\n",
      "Gradient Descent(49/49): loss=0.6697776782845571\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4559753920385967\n",
      "Gradient Descent(2/49): loss=0.43952667029757325\n",
      "Gradient Descent(3/49): loss=0.43148212487308135\n",
      "Gradient Descent(4/49): loss=0.4275760381454023\n",
      "Gradient Descent(5/49): loss=0.425864807777105\n",
      "Gradient Descent(6/49): loss=0.4253773191343782\n",
      "Gradient Descent(7/49): loss=0.4256190929843679\n",
      "Gradient Descent(8/49): loss=0.4263351798728713\n",
      "Gradient Descent(9/49): loss=0.42739269136619057\n",
      "Gradient Descent(10/49): loss=0.4287218309322601\n",
      "Gradient Descent(11/49): loss=0.4302859645291546\n",
      "Gradient Descent(12/49): loss=0.43206626667760945\n",
      "Gradient Descent(13/49): loss=0.43405375899810533\n",
      "Gradient Descent(14/49): loss=0.43624513673754167\n",
      "Gradient Descent(15/49): loss=0.4386405563079496\n",
      "Gradient Descent(16/49): loss=0.44124244876896945\n",
      "Gradient Descent(17/49): loss=0.4440548761299995\n",
      "Gradient Descent(18/49): loss=0.44708317852710217\n",
      "Gradient Descent(19/49): loss=0.45033377966675736\n",
      "Gradient Descent(20/49): loss=0.4538140800812419\n",
      "Gradient Descent(21/49): loss=0.45753240039520715\n",
      "Gradient Descent(22/49): loss=0.461497954111161\n",
      "Gradient Descent(23/49): loss=0.4657208386788075\n",
      "Gradient Descent(24/49): loss=0.47021203861215244\n",
      "Gradient Descent(25/49): loss=0.47498343714648805\n",
      "Gradient Descent(26/49): loss=0.48004783443460775\n",
      "Gradient Descent(27/49): loss=0.48541897112658483\n",
      "Gradient Descent(28/49): loss=0.491111556660002\n",
      "Gradient Descent(29/49): loss=0.49714130186970446\n",
      "Gradient Descent(30/49): loss=0.5035249556964226\n",
      "Gradient Descent(31/49): loss=0.5102803458804651\n",
      "Gradient Descent(32/49): loss=0.5174264235973205\n",
      "Gradient Descent(33/49): loss=0.5249833120414126\n",
      "Gradient Descent(34/49): loss=0.5329723590011506\n",
      "Gradient Descent(35/49): loss=0.5414161934977693\n",
      "Gradient Descent(36/49): loss=0.5503387865851609\n",
      "Gradient Descent(37/49): loss=0.5597655164297292\n",
      "Gradient Descent(38/49): loss=0.5697232378093493\n",
      "Gradient Descent(39/49): loss=0.5802403561894819\n",
      "Gradient Descent(40/49): loss=0.5913469065527303\n",
      "Gradient Descent(41/49): loss=0.6030746371762212\n",
      "Gradient Descent(42/49): loss=0.6154570985689821\n",
      "Gradient Descent(43/49): loss=0.6285297377995787\n",
      "Gradient Descent(44/49): loss=0.6423299984624681\n",
      "Gradient Descent(45/49): loss=0.656897426550136\n",
      "Gradient Descent(46/49): loss=0.6722737825171557\n",
      "Gradient Descent(47/49): loss=0.6885031598419966\n",
      "Gradient Descent(48/49): loss=0.7056321104126397\n",
      "Gradient Descent(49/49): loss=0.7237097770832068\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45472602986329613\n",
      "Gradient Descent(2/49): loss=0.4374762681418711\n",
      "Gradient Descent(3/49): loss=0.42876052670900844\n",
      "Gradient Descent(4/49): loss=0.4242213596844009\n",
      "Gradient Descent(5/49): loss=0.4218627693267884\n",
      "Gradient Descent(6/49): loss=0.4206881821217098\n",
      "Gradient Descent(7/49): loss=0.4201901133816537\n",
      "Gradient Descent(8/49): loss=0.4201059394841561\n",
      "Gradient Descent(9/49): loss=0.42029720261852677\n",
      "Gradient Descent(10/49): loss=0.4206892141653412\n",
      "Gradient Descent(11/49): loss=0.42124050870859187\n",
      "Gradient Descent(12/49): loss=0.42192723174078917\n",
      "Gradient Descent(13/49): loss=0.42273507308495284\n",
      "Gradient Descent(14/49): loss=0.4236550502766156\n",
      "Gradient Descent(15/49): loss=0.4246812753304419\n",
      "Gradient Descent(16/49): loss=0.425809753378692\n",
      "Gradient Descent(17/49): loss=0.427037723736534\n",
      "Gradient Descent(18/49): loss=0.42836328937979834\n",
      "Gradient Descent(19/49): loss=0.4297852018197891\n",
      "Gradient Descent(20/49): loss=0.43130273107248046\n",
      "Gradient Descent(21/49): loss=0.43291558319332507\n",
      "Gradient Descent(22/49): loss=0.4346238451195979\n",
      "Gradient Descent(23/49): loss=0.4364279457414424\n",
      "Gradient Descent(24/49): loss=0.43832862704588194\n",
      "Gradient Descent(25/49): loss=0.440326921844914\n",
      "Gradient Descent(26/49): loss=0.44242413605990244\n",
      "Gradient Descent(27/49): loss=0.44462183434583863\n",
      "Gradient Descent(28/49): loss=0.4469218282969714\n",
      "Gradient Descent(29/49): loss=0.4493261667391324\n",
      "Gradient Descent(30/49): loss=0.45183712776994644\n",
      "Gradient Descent(31/49): loss=0.4544572123031962\n",
      "Gradient Descent(32/49): loss=0.45718913893381363\n",
      "Gradient Descent(33/49): loss=0.4600358399798178\n",
      "Gradient Descent(34/49): loss=0.4630004585851183\n",
      "Gradient Descent(35/49): loss=0.46608634678720556\n",
      "Gradient Descent(36/49): loss=0.46929706446895264\n",
      "Gradient Descent(37/49): loss=0.4726363791258259\n",
      "Gradient Descent(38/49): loss=0.476108266389581\n",
      "Gradient Descent(39/49): loss=0.47971691125771865\n",
      "Gradient Descent(40/49): loss=0.48346670998495656\n",
      "Gradient Descent(41/49): loss=0.4873622725990119\n",
      "Gradient Descent(42/49): loss=0.49140842600827317\n",
      "Gradient Descent(43/49): loss=0.4956102176735773\n",
      "Gradient Descent(44/49): loss=0.4999729198205049\n",
      "Gradient Descent(45/49): loss=0.5045020341722588\n",
      "Gradient Descent(46/49): loss=0.5092032971865583\n",
      "Gradient Descent(47/49): loss=0.514082685782993\n",
      "Gradient Descent(48/49): loss=0.5191464235499906\n",
      "Gradient Descent(49/49): loss=0.5244009874230683\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4583565689211491\n",
      "Gradient Descent(2/49): loss=0.44532347018183943\n",
      "Gradient Descent(3/49): loss=0.44169957200126064\n",
      "Gradient Descent(4/49): loss=0.44308883239776314\n",
      "Gradient Descent(5/49): loss=0.44763505855778124\n",
      "Gradient Descent(6/49): loss=0.4545483174660022\n",
      "Gradient Descent(7/49): loss=0.46355974785017123\n",
      "Gradient Descent(8/49): loss=0.4746744033881471\n",
      "Gradient Descent(9/49): loss=0.4880564857666129\n",
      "Gradient Descent(10/49): loss=0.5039775076185538\n",
      "Gradient Descent(11/49): loss=0.5227953657212157\n",
      "Gradient Descent(12/49): loss=0.5449492375665786\n",
      "Gradient Descent(13/49): loss=0.5709631662724507\n",
      "Gradient Descent(14/49): loss=0.6014550062867816\n",
      "Gradient Descent(15/49): loss=0.6371492626697636\n",
      "Gradient Descent(16/49): loss=0.6788932926146141\n",
      "Gradient Descent(17/49): loss=0.7276768324464187\n",
      "Gradient Descent(18/49): loss=0.784655098138123\n",
      "Gradient Descent(19/49): loss=0.8511758951582323\n",
      "Gradient Descent(20/49): loss=0.9288113203468673\n",
      "Gradient Descent(21/49): loss=1.0193947734383983\n",
      "Gradient Descent(22/49): loss=1.1250641346598602\n",
      "Gradient Descent(23/49): loss=1.2483121171649005\n",
      "Gradient Descent(24/49): loss=1.3920449757225875\n",
      "Gradient Descent(25/49): loss=1.5596509517786146\n",
      "Gradient Descent(26/49): loss=1.755080065269398\n",
      "Gradient Descent(27/49): loss=1.9829371312566106\n",
      "Gradient Descent(28/49): loss=2.2485901910934483\n",
      "Gradient Descent(29/49): loss=2.5582969108926505\n",
      "Gradient Descent(30/49): loss=2.9193519231500358\n",
      "Gradient Descent(31/49): loss=3.340258580483725\n",
      "Gradient Descent(32/49): loss=3.830929165203121\n",
      "Gradient Descent(33/49): loss=4.402918268366561\n",
      "Gradient Descent(34/49): loss=5.069694832900526\n",
      "Gradient Descent(35/49): loss=5.84695926562199\n",
      "Gradient Descent(36/49): loss=6.753013084064787\n",
      "Gradient Descent(37/49): loss=7.809189800839293\n",
      "Gradient Descent(38/49): loss=9.040357189973772\n",
      "Gradient Descent(39/49): loss=10.475502760235853\n",
      "Gradient Descent(40/49): loss=12.148416219382566\n",
      "Gradient Descent(41/49): loss=14.098484996754735\n",
      "Gradient Descent(42/49): loss=16.371621553374368\n",
      "Gradient Descent(43/49): loss=19.02134431139616\n",
      "Gradient Descent(44/49): loss=22.11003765144164\n",
      "Gradient Descent(45/49): loss=25.710420642164223\n",
      "Gradient Descent(46/49): loss=29.90725908060552\n",
      "Gradient Descent(47/49): loss=34.79936115020334\n",
      "Gradient Descent(48/49): loss=40.501903680560616\n",
      "Gradient Descent(49/49): loss=47.14914377649591\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4580616135808649\n",
      "Gradient Descent(2/49): loss=0.44595778249906165\n",
      "Gradient Descent(3/49): loss=0.44397724654771903\n",
      "Gradient Descent(4/49): loss=0.44772640837456495\n",
      "Gradient Descent(5/49): loss=0.45544968308460304\n",
      "Gradient Descent(6/49): loss=0.46652660708828664\n",
      "Gradient Descent(7/49): loss=0.48092163807073557\n",
      "Gradient Descent(8/49): loss=0.4989399715324968\n",
      "Gradient Descent(9/49): loss=0.5211222035815543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=0.5482070315856433\n",
      "Gradient Descent(11/49): loss=0.5811295481756145\n",
      "Gradient Descent(12/49): loss=0.6210402910816862\n",
      "Gradient Descent(13/49): loss=0.6693386115550612\n",
      "Gradient Descent(14/49): loss=0.7277180747212716\n",
      "Gradient Descent(15/49): loss=0.7982237809016187\n",
      "Gradient Descent(16/49): loss=0.8833227658757263\n",
      "Gradient Descent(17/49): loss=0.9859895125287307\n",
      "Gradient Descent(18/49): loss=1.1098093400413076\n",
      "Gradient Descent(19/49): loss=1.2591031628955318\n",
      "Gradient Descent(20/49): loss=1.4390779081523697\n",
      "Gradient Descent(21/49): loss=1.6560077991717335\n",
      "Gradient Descent(22/49): loss=1.9174528019765527\n",
      "Gradient Descent(23/49): loss=2.232521831068039\n",
      "Gradient Descent(24/49): loss=2.6121898731722957\n",
      "Gradient Descent(25/49): loss=3.0696800661354597\n",
      "Gradient Descent(26/49): loss=3.6209240322196767\n",
      "Gradient Descent(27/49): loss=4.285116489558916\n",
      "Gradient Descent(28/49): loss=5.085383447571559\n",
      "Gradient Descent(29/49): loss=6.049587246064523\n",
      "Gradient Descent(30/49): loss=7.2112964612908845\n",
      "Gradient Descent(31/49): loss=8.610954441194673\n",
      "Gradient Descent(32/49): loss=10.297287146279343\n",
      "Gradient Descent(33/49): loss=12.328999302657916\n",
      "Gradient Descent(34/49): loss=14.77681790985942\n",
      "Gradient Descent(35/49): loss=17.72595423722643\n",
      "Gradient Descent(36/49): loss=21.279070010145087\n",
      "Gradient Descent(37/49): loss=25.559851037965267\n",
      "Gradient Descent(38/49): loss=30.717312680262935\n",
      "Gradient Descent(39/49): loss=36.93098702306095\n",
      "Gradient Descent(40/49): loss=44.41717232858829\n",
      "Gradient Descent(41/49): loss=53.436462299434375\n",
      "Gradient Descent(42/49): loss=64.30281724777184\n",
      "Gradient Descent(43/49): loss=77.39449293338018\n",
      "Gradient Descent(44/49): loss=93.16720749888415\n",
      "Gradient Descent(45/49): loss=112.17000483785556\n",
      "Gradient Descent(46/49): loss=135.06436659312183\n",
      "Gradient Descent(47/49): loss=162.64723806611522\n",
      "Gradient Descent(48/49): loss=195.87876955971302\n",
      "Gradient Descent(49/49): loss=235.91573881927562\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4589431989737219\n",
      "Gradient Descent(2/49): loss=0.44728803385281374\n",
      "Gradient Descent(3/49): loss=0.4455664782207128\n",
      "Gradient Descent(4/49): loss=0.4495240709806175\n",
      "Gradient Descent(5/49): loss=0.4574850973011187\n",
      "Gradient Descent(6/49): loss=0.4688788263162144\n",
      "Gradient Descent(7/49): loss=0.48370940235314636\n",
      "Gradient Descent(8/49): loss=0.5023227762098903\n",
      "Gradient Descent(9/49): loss=0.5253076316829989\n",
      "Gradient Descent(10/49): loss=0.5534624365864016\n",
      "Gradient Descent(11/49): loss=0.5877976673890143\n",
      "Gradient Descent(12/49): loss=0.6295591354903172\n",
      "Gradient Descent(13/49): loss=0.6802664340480626\n",
      "Gradient Descent(14/49): loss=0.7417645663016844\n",
      "Gradient Descent(15/49): loss=0.8162889801670882\n",
      "Gradient Descent(16/49): loss=0.9065455419731555\n",
      "Gradient Descent(17/49): loss=1.015807928254138\n",
      "Gradient Descent(18/49): loss=1.1480357462198203\n",
      "Gradient Descent(19/49): loss=1.308017544218793\n",
      "Gradient Descent(20/49): loss=1.5015438244008374\n",
      "Gradient Descent(21/49): loss=1.7356162801495358\n",
      "Gradient Descent(22/49): loss=2.018700803233454\n",
      "Gradient Descent(23/49): loss=2.3610333940402053\n",
      "Gradient Descent(24/49): loss=2.774990023269262\n",
      "Gradient Descent(25/49): loss=3.2755338058778545\n",
      "Gradient Descent(26/49): loss=3.8807556422797616\n",
      "Gradient Descent(27/49): loss=4.612527859176652\n",
      "Gradient Descent(28/49): loss=5.497294465203512\n",
      "Gradient Descent(29/49): loss=6.567026572420649\n",
      "Gradient Descent(30/49): loss=7.86037750197394\n",
      "Gradient Descent(31/49): loss=9.424079306598337\n",
      "Gradient Descent(32/49): loss=11.314631164718909\n",
      "Gradient Descent(33/49): loss=13.600340645837216\n",
      "Gradient Descent(34/49): loss=16.363791595660476\n",
      "Gradient Descent(35/49): loss=19.704827802631126\n",
      "Gradient Descent(36/49): loss=23.744160242010867\n",
      "Gradient Descent(37/49): loss=28.62772822270582\n",
      "Gradient Descent(38/49): loss=34.53197199952725\n",
      "Gradient Descent(39/49): loss=41.670207343632875\n",
      "Gradient Descent(40/49): loss=50.30033237619245\n",
      "Gradient Descent(41/49): loss=60.73414510325195\n",
      "Gradient Descent(42/49): loss=73.34860828229218\n",
      "Gradient Descent(43/49): loss=88.59946860552887\n",
      "Gradient Descent(44/49): loss=107.03772224339697\n",
      "Gradient Descent(45/49): loss=129.32952162693735\n",
      "Gradient Descent(46/49): loss=156.28024267527368\n",
      "Gradient Descent(47/49): loss=188.8635819858434\n",
      "Gradient Descent(48/49): loss=228.25673523101034\n",
      "Gradient Descent(49/49): loss=275.88292771081166\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4576755809480765\n",
      "Gradient Descent(2/49): loss=0.44510077289027705\n",
      "Gradient Descent(3/49): loss=0.442383636057427\n",
      "Gradient Descent(4/49): loss=0.44510944411930736\n",
      "Gradient Descent(5/49): loss=0.45146074182779333\n",
      "Gradient Descent(6/49): loss=0.4607283236235631\n",
      "Gradient Descent(7/49): loss=0.47276010449000905\n",
      "Gradient Descent(8/49): loss=0.4877133073814909\n",
      "Gradient Descent(9/49): loss=0.5059429734898134\n",
      "Gradient Descent(10/49): loss=0.527956266314402\n",
      "Gradient Descent(11/49): loss=0.5544000740390982\n",
      "Gradient Descent(12/49): loss=0.5860667600494404\n",
      "Gradient Descent(13/49): loss=0.6239111244002833\n",
      "Gradient Descent(14/49): loss=0.6690756266004192\n",
      "Gradient Descent(15/49): loss=0.722922926377841\n",
      "Gradient Descent(16/49): loss=0.7870758744317614\n",
      "Gradient Descent(17/49): loss=0.8634657292686397\n",
      "Gradient Descent(18/49): loss=0.9543898315594367\n",
      "Gradient Descent(19/49): loss=1.0625803553938529\n",
      "Gradient Descent(20/49): loss=1.1912861404706978\n",
      "Gradient Descent(21/49): loss=1.3443700280166746\n",
      "Gradient Descent(22/49): loss=1.5264246014487404\n",
      "Gradient Descent(23/49): loss=1.7429097912427778\n",
      "Gradient Descent(24/49): loss=2.000316462122066\n",
      "Gradient Descent(25/49): loss=2.3063608809156375\n",
      "Gradient Descent(26/49): loss=2.6702158894811343\n",
      "Gradient Descent(27/49): loss=3.102785707116525\n",
      "Gradient Descent(28/49): loss=3.6170325940816292\n",
      "Gradient Descent(29/49): loss=4.228365161488883\n",
      "Gradient Descent(30/49): loss=4.955099959507445\n",
      "Gradient Descent(31/49): loss=5.819010170905949\n",
      "Gradient Descent(32/49): loss=6.845977846208347\n",
      "Gradient Descent(33/49): loss=8.066769218326321\n",
      "Gradient Descent(34/49): loss=9.517956321369333\n",
      "Gradient Descent(35/49): loss=11.24301252086881\n",
      "Gradient Descent(36/49): loss=13.29361477215777\n",
      "Gradient Descent(37/49): loss=15.731191616183217\n",
      "Gradient Descent(38/49): loss=18.628763283082257\n",
      "Gradient Descent(39/49): loss=22.073129023935362\n",
      "Gradient Descent(40/49): loss=26.16746719232461\n",
      "Gradient Descent(41/49): loss=31.03442596124022\n",
      "Gradient Descent(42/49): loss=36.819797257852294\n",
      "Gradient Descent(43/49): loss=43.69688396896482\n",
      "Gradient Descent(44/49): loss=51.87169123687506\n",
      "Gradient Descent(45/49): loss=61.58909735100999\n",
      "Gradient Descent(46/49): loss=73.14018908448507\n",
      "Gradient Descent(47/49): loss=86.8709812057006\n",
      "Gradient Descent(48/49): loss=103.19278135804718\n",
      "Gradient Descent(49/49): loss=122.59451078781208\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46157910543241243\n",
      "Gradient Descent(2/49): loss=0.4545526616932\n",
      "Gradient Descent(3/49): loss=0.4599830917927973\n",
      "Gradient Descent(4/49): loss=0.47425030521952793\n",
      "Gradient Descent(5/49): loss=0.4968323485722882\n",
      "Gradient Descent(6/49): loss=0.5288163084379415\n",
      "Gradient Descent(7/49): loss=0.5724616767680524\n",
      "Gradient Descent(8/49): loss=0.6311532464423036\n",
      "Gradient Descent(9/49): loss=0.7095675893477508\n",
      "Gradient Descent(10/49): loss=0.8139985311096967\n",
      "Gradient Descent(11/49): loss=0.9528377666596276\n",
      "Gradient Descent(12/49): loss=1.1372354060125978\n",
      "Gradient Descent(13/49): loss=1.3819869363016084\n",
      "Gradient Descent(14/49): loss=1.7067145578147698\n",
      "Gradient Descent(15/49): loss=2.1374359689749376\n",
      "Gradient Descent(16/49): loss=2.7086454113881224\n",
      "Gradient Descent(17/49): loss=3.4660731201031614\n",
      "Gradient Descent(18/49): loss=4.470343766364982\n",
      "Gradient Descent(19/49): loss=5.801826481121125\n",
      "Gradient Descent(20/49): loss=7.567064412556288\n",
      "Gradient Descent(21/49): loss=9.907298150692936\n",
      "Gradient Descent(22/49): loss=13.009764867242746\n",
      "Gradient Descent(23/49): loss=17.122677075434975\n",
      "Gradient Descent(24/49): loss=22.575079279090524\n",
      "Gradient Descent(25/49): loss=29.803171002578964\n",
      "Gradient Descent(26/49): loss=39.385201991115565\n",
      "Gradient Descent(27/49): loss=52.087731127219165\n",
      "Gradient Descent(28/49): loss=68.92694968332644\n",
      "Gradient Descent(29/49): loss=91.24997464670314\n",
      "Gradient Descent(30/49): loss=120.84261541781761\n",
      "Gradient Descent(31/49): loss=160.07223499913943\n",
      "Gradient Descent(32/49): loss=212.07713427894248\n",
      "Gradient Descent(33/49): loss=281.0176097673572\n",
      "Gradient Descent(34/49): loss=372.4087688922446\n",
      "Gradient Descent(35/49): loss=493.56172740051227\n",
      "Gradient Descent(36/49): loss=654.1684837578422\n",
      "Gradient Descent(37/49): loss=867.0772593066126\n",
      "Gradient Descent(38/49): loss=1149.3203298141734\n",
      "Gradient Descent(39/49): loss=1523.4765728397442\n",
      "Gradient Descent(40/49): loss=2019.4777319312911\n",
      "Gradient Descent(41/49): loss=2677.0028951052964\n",
      "Gradient Descent(42/49): loss=3548.652741005401\n",
      "Gradient Descent(43/49): loss=4704.157485960265\n",
      "Gradient Descent(44/49): loss=6235.955159125326\n",
      "Gradient Descent(45/49): loss=8266.58645634451\n",
      "Gradient Descent(46/49): loss=10958.497745962943\n",
      "Gradient Descent(47/49): loss=14527.036447066312\n",
      "Gradient Descent(48/49): loss=19257.678383906292\n",
      "Gradient Descent(49/49): loss=25528.86526973896\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4613381618014868\n",
      "Gradient Descent(2/49): loss=0.45574677573991373\n",
      "Gradient Descent(3/49): loss=0.4640123317953235\n",
      "Gradient Descent(4/49): loss=0.4829698597150082\n",
      "Gradient Descent(5/49): loss=0.5128875129461232\n",
      "Gradient Descent(6/49): loss=0.5560523222781627\n",
      "Gradient Descent(7/49): loss=0.6164815224635389\n",
      "Gradient Descent(8/49): loss=0.7000961928381283\n",
      "Gradient Descent(9/49): loss=0.815206376724097\n",
      "Gradient Descent(10/49): loss=0.9732888644416203\n",
      "Gradient Descent(11/49): loss=1.1901052308570692\n",
      "Gradient Descent(12/49): loss=1.487257636100597\n",
      "Gradient Descent(13/49): loss=1.894330671191468\n",
      "Gradient Descent(14/49): loss=2.451829112436237\n",
      "Gradient Descent(15/49): loss=3.2152020984358938\n",
      "Gradient Descent(16/49): loss=4.260352937349765\n",
      "Gradient Descent(17/49): loss=5.691181730051115\n",
      "Gradient Descent(18/49): loss=7.649910164886631\n",
      "Gradient Descent(19/49): loss=10.331214386770364\n",
      "Gradient Descent(20/49): loss=14.001570292884715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=19.025733587497957\n",
      "Gradient Descent(22/49): loss=25.902985921591533\n",
      "Gradient Descent(23/49): loss=35.31674890708652\n",
      "Gradient Descent(24/49): loss=48.202496168303306\n",
      "Gradient Descent(25/49): loss=65.84071187700333\n",
      "Gradient Descent(26/49): loss=89.98413309609529\n",
      "Gradient Descent(27/49): loss=123.03192005188966\n",
      "Gradient Descent(28/49): loss=168.26806170033822\n",
      "Gradient Descent(29/49): loss=230.1877070374079\n",
      "Gradient Descent(30/49): loss=314.943849817228\n",
      "Gradient Descent(31/49): loss=430.95875390394673\n",
      "Gradient Descent(32/49): loss=589.760876820564\n",
      "Gradient Descent(33/49): loss=807.1304568967571\n",
      "Gradient Descent(34/49): loss=1104.6676014330299\n",
      "Gradient Descent(35/49): loss=1511.9386973800179\n",
      "Gradient Descent(36/49): loss=2069.4144341527804\n",
      "Gradient Descent(37/49): loss=2832.4913909893603\n",
      "Gradient Descent(38/49): loss=3876.996815469932\n",
      "Gradient Descent(39/49): loss=5306.723605064956\n",
      "Gradient Descent(40/49): loss=7263.744245602953\n",
      "Gradient Descent(41/49): loss=9942.528606519349\n",
      "Gradient Descent(42/49): loss=13609.268483398266\n",
      "Gradient Descent(43/49): loss=18628.329174712446\n",
      "Gradient Descent(44/49): loss=25498.45659567874\n",
      "Gradient Descent(45/49): loss=34902.337843460846\n",
      "Gradient Descent(46/49): loss=47774.440065325376\n",
      "Gradient Descent(47/49): loss=65393.868803052574\n",
      "Gradient Descent(48/49): loss=89511.47318146528\n",
      "Gradient Descent(49/49): loss=122523.82843043974\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4622241172441793\n",
      "Gradient Descent(2/49): loss=0.45701782063394986\n",
      "Gradient Descent(3/49): loss=0.4655180057727977\n",
      "Gradient Descent(4/49): loss=0.4847291824967902\n",
      "Gradient Descent(5/49): loss=0.5150525047841102\n",
      "Gradient Descent(6/49): loss=0.558912552604462\n",
      "Gradient Descent(7/49): loss=0.6205053927271421\n",
      "Gradient Descent(8/49): loss=0.7060085001719902\n",
      "Gradient Descent(9/49): loss=0.8241090453576729\n",
      "Gradient Descent(10/49): loss=0.9868398668993765\n",
      "Gradient Descent(11/49): loss=1.2107796753725053\n",
      "Gradient Descent(12/49): loss=1.5187263794940877\n",
      "Gradient Descent(13/49): loss=1.9420073957648125\n",
      "Gradient Descent(14/49): loss=2.523658677654388\n",
      "Gradient Descent(15/49): loss=3.322793906177481\n",
      "Gradient Descent(16/49): loss=4.420606842606541\n",
      "Gradient Descent(17/49): loss=5.928616017498817\n",
      "Gradient Descent(18/49): loss=7.999988785724046\n",
      "Gradient Descent(19/49): loss=10.845094542783588\n",
      "Gradient Descent(20/49): loss=14.75286637558522\n",
      "Gradient Descent(21/49): loss=20.120140241896266\n",
      "Gradient Descent(22/49): loss=27.491950838788195\n",
      "Gradient Descent(23/49): loss=37.61687589239159\n",
      "Gradient Descent(24/49): loss=51.52304865475214\n",
      "Gradient Descent(25/49): loss=70.62255709176237\n",
      "Gradient Descent(26/49): loss=96.85483069628826\n",
      "Gradient Descent(27/49): loss=132.8835747536143\n",
      "Gradient Descent(28/49): loss=182.36724921739088\n",
      "Gradient Descent(29/49): loss=250.3305572425771\n",
      "Gradient Descent(30/49): loss=343.674665173922\n",
      "Gradient Descent(31/49): loss=471.87796289849655\n",
      "Gradient Descent(32/49): loss=647.9585213760653\n",
      "Gradient Descent(33/49): loss=889.7959774924875\n",
      "Gradient Descent(34/49): loss=1221.9470734554916\n",
      "Gradient Descent(35/49): loss=1678.1392047664233\n",
      "Gradient Descent(36/49): loss=2304.695177346336\n",
      "Gradient Descent(37/49): loss=3165.2369315076094\n",
      "Gradient Descent(38/49): loss=4347.145859687521\n",
      "Gradient Descent(39/49): loss=5970.435713418489\n",
      "Gradient Descent(40/49): loss=8199.939074851834\n",
      "Gradient Descent(41/49): loss=11262.044835092534\n",
      "Gradient Descent(42/49): loss=15467.686241270645\n",
      "Gradient Descent(43/49): loss=21243.91377135064\n",
      "Gradient Descent(44/49): loss=29177.258818776063\n",
      "Gradient Descent(45/49): loss=40073.29143490308\n",
      "Gradient Descent(46/49): loss=55038.41975893979\n",
      "Gradient Descent(47/49): loss=75592.23723704228\n",
      "Gradient Descent(48/49): loss=103821.82562476849\n",
      "Gradient Descent(49/49): loss=142593.68205353274\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4609400615907644\n",
      "Gradient Descent(2/49): loss=0.4546768098583967\n",
      "Gradient Descent(3/49): loss=0.46172641159521977\n",
      "Gradient Descent(4/49): loss=0.47870274492431936\n",
      "Gradient Descent(5/49): loss=0.5055074725764573\n",
      "Gradient Descent(6/49): loss=0.5438793519768358\n",
      "Gradient Descent(7/49): loss=0.5970312602047138\n",
      "Gradient Descent(8/49): loss=0.6697156170193824\n",
      "Gradient Descent(9/49): loss=0.7685536170761658\n",
      "Gradient Descent(10/49): loss=0.9025906519960374\n",
      "Gradient Descent(11/49): loss=1.0840986804967423\n",
      "Gradient Descent(12/49): loss=1.3296850479162263\n",
      "Gradient Descent(13/49): loss=1.6618025293838132\n",
      "Gradient Descent(14/49): loss=2.11079534210773\n",
      "Gradient Descent(15/49): loss=2.717666338226502\n",
      "Gradient Descent(16/49): loss=3.537817120513527\n",
      "Gradient Descent(17/49): loss=4.646101929957685\n",
      "Gradient Descent(18/49): loss=6.143656170098518\n",
      "Gradient Descent(19/49): loss=8.167122402018009\n",
      "Gradient Descent(20/49): loss=10.901115386687131\n",
      "Gradient Descent(21/49): loss=14.59506324386369\n",
      "Gradient Descent(22/49): loss=19.585961004877944\n",
      "Gradient Descent(23/49): loss=26.329112182138186\n",
      "Gradient Descent(24/49): loss=35.439662663757986\n",
      "Gradient Descent(25/49): loss=47.74871574078828\n",
      "Gradient Descent(26/49): loss=64.37914719577368\n",
      "Gradient Descent(27/49): loss=86.84803646143708\n",
      "Gradient Descent(28/49): loss=117.20505783071968\n",
      "Gradient Descent(29/49): loss=158.21945604883996\n",
      "Gradient Descent(30/49): loss=213.6326625847369\n",
      "Gradient Descent(31/49): loss=288.4995967563287\n",
      "Gradient Descent(32/49): loss=389.6497858977974\n",
      "Gradient Descent(33/49): loss=526.3103688953616\n",
      "Gradient Descent(34/49): loss=710.9478147501537\n",
      "Gradient Descent(35/49): loss=960.4051394592074\n",
      "Gradient Descent(36/49): loss=1297.4383604670886\n",
      "Gradient Descent(37/49): loss=1752.7923472089653\n",
      "Gradient Descent(38/49): loss=2368.0054310815485\n",
      "Gradient Descent(39/49): loss=3199.198617105366\n",
      "Gradient Descent(40/49): loss=4322.195056922992\n",
      "Gradient Descent(41/49): loss=5839.436791940978\n",
      "Gradient Descent(42/49): loss=7889.3297260838035\n",
      "Gradient Descent(43/49): loss=10658.869295705223\n",
      "Gradient Descent(44/49): loss=14400.698576111712\n",
      "Gradient Descent(45/49): loss=19456.15490009501\n",
      "Gradient Descent(46/49): loss=26286.407348194858\n",
      "Gradient Descent(47/49): loss=35514.52553629414\n",
      "Gradient Descent(48/49): loss=47982.31730147005\n",
      "Gradient Descent(49/49): loss=64827.12013615525\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4651122097365619\n",
      "Gradient Descent(2/49): loss=0.4659287138759684\n",
      "Gradient Descent(3/49): loss=0.48527991187095404\n",
      "Gradient Descent(4/49): loss=0.5227862885741618\n",
      "Gradient Descent(5/49): loss=0.5832984032162798\n",
      "Gradient Descent(6/49): loss=0.6762966625033613\n",
      "Gradient Descent(7/49): loss=0.816954458410402\n",
      "Gradient Descent(8/49): loss=1.0284291466941293\n",
      "Gradient Descent(9/49): loss=1.3455809153828557\n",
      "Gradient Descent(10/49): loss=1.8206650552949644\n",
      "Gradient Descent(11/49): loss=2.531906028719451\n",
      "Gradient Descent(12/49): loss=3.5963509912980665\n",
      "Gradient Descent(13/49): loss=5.1891106416113315\n",
      "Gradient Descent(14/49): loss=7.572148349574147\n",
      "Gradient Descent(15/49): loss=11.137349672713242\n",
      "Gradient Descent(16/49): loss=16.47095281753489\n",
      "Gradient Descent(17/49): loss=24.44993284389964\n",
      "Gradient Descent(18/49): loss=36.38618606841939\n",
      "Gradient Descent(19/49): loss=54.24222003428314\n",
      "Gradient Descent(20/49): loss=80.95381070310584\n",
      "Gradient Descent(21/49): loss=120.9126750626173\n",
      "Gradient Descent(22/49): loss=180.68851555018097\n",
      "Gradient Descent(23/49): loss=270.1091479130035\n",
      "Gradient Descent(24/49): loss=403.8762967712611\n",
      "Gradient Descent(25/49): loss=603.9827124967203\n",
      "Gradient Descent(26/49): loss=903.3280085846011\n",
      "Gradient Descent(27/49): loss=1351.1277008225225\n",
      "Gradient Descent(28/49): loss=2021.004750601007\n",
      "Gradient Descent(29/49): loss=3023.093946312301\n",
      "Gradient Descent(30/49): loss=4522.14920913512\n",
      "Gradient Descent(31/49): loss=6764.630852755243\n",
      "Gradient Descent(32/49): loss=10119.226224470667\n",
      "Gradient Descent(33/49): loss=15137.465742854922\n",
      "Gradient Descent(34/49): loss=22644.40023951822\n",
      "Gradient Descent(35/49): loss=33874.247903804535\n",
      "Gradient Descent(36/49): loss=50673.312600064695\n",
      "Gradient Descent(37/49): loss=75803.53544102766\n",
      "Gradient Descent(38/49): loss=113396.58665556084\n",
      "Gradient Descent(39/49): loss=169633.1551804786\n",
      "Gradient Descent(40/49): loss=253759.1182559375\n",
      "Gradient Descent(41/49): loss=379605.65988311043\n",
      "Gradient Descent(42/49): loss=567863.2614413173\n",
      "Gradient Descent(43/49): loss=849483.4321917937\n",
      "Gradient Descent(44/49): loss=1270767.4595711718\n",
      "Gradient Descent(45/49): loss=1900978.8227911072\n",
      "Gradient Descent(46/49): loss=2843730.829776776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=4254021.723489262\n",
      "Gradient Descent(48/49): loss=6363718.006428254\n",
      "Gradient Descent(49/49): loss=9519675.707116116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4649295566538723\n",
      "Gradient Descent(2/49): loss=0.4677854334772806\n",
      "Gradient Descent(3/49): loss=0.4916414443195261\n",
      "Gradient Descent(4/49): loss=0.5376420143903546\n",
      "Gradient Descent(5/49): loss=0.6133563719000039\n",
      "Gradient Descent(6/49): loss=0.7328526409893122\n",
      "Gradient Descent(7/49): loss=0.9189095730819478\n",
      "Gradient Descent(8/49): loss=1.2071759954657233\n",
      "Gradient Descent(9/49): loss=1.6529034097696758\n",
      "Gradient Descent(10/49): loss=2.341475666113425\n",
      "Gradient Descent(11/49): loss=3.404722866322372\n",
      "Gradient Descent(12/49): loss=5.046127937939642\n",
      "Gradient Descent(13/49): loss=7.57974157289294\n",
      "Gradient Descent(14/49): loss=11.490245903255374\n",
      "Gradient Descent(15/49): loss=17.5256534101952\n",
      "Gradient Descent(16/49): loss=26.840369853258686\n",
      "Gradient Descent(17/49): loss=41.21598273008562\n",
      "Gradient Descent(18/49): loss=63.402002425815\n",
      "Gradient Descent(19/49): loss=97.64172501893763\n",
      "Gradient Descent(20/49): loss=150.48379148505964\n",
      "Gradient Descent(21/49): loss=232.03465938828933\n",
      "Gradient Descent(22/49): loss=357.8915306006585\n",
      "Gradient Descent(23/49): loss=552.1254203324658\n",
      "Gradient Descent(24/49): loss=851.8848992659034\n",
      "Gradient Descent(25/49): loss=1314.5010050924136\n",
      "Gradient Descent(26/49): loss=2028.4521850342126\n",
      "Gradient Descent(27/49): loss=3130.286387523514\n",
      "Gradient Descent(28/49): loss=4830.736765601881\n",
      "Gradient Descent(29/49): loss=7455.025793977112\n",
      "Gradient Descent(30/49): loss=11505.066230134185\n",
      "Gradient Descent(31/49): loss=17755.455266927493\n",
      "Gradient Descent(32/49): loss=27401.621396663002\n",
      "Gradient Descent(33/49): loss=42288.45805954757\n",
      "Gradient Descent(34/49): loss=65263.171782098594\n",
      "Gradient Descent(35/49): loss=100719.82935828302\n",
      "Gradient Descent(36/49): loss=155439.75234281854\n",
      "Gradient Descent(37/49): loss=239888.48989176672\n",
      "Gradient Descent(38/49): loss=370217.42463054555\n",
      "Gradient Descent(39/49): loss=571352.8319815298\n",
      "Gradient Descent(40/49): loss=881763.1960886808\n",
      "Gradient Descent(41/49): loss=1360816.5632105554\n",
      "Gradient Descent(42/49): loss=2100135.0753416982\n",
      "Gradient Descent(43/49): loss=3241118.3141206796\n",
      "Gradient Descent(44/49): loss=5001986.911070582\n",
      "Gradient Descent(45/49): loss=7719518.694460015\n",
      "Gradient Descent(46/49): loss=11913459.688400809\n",
      "Gradient Descent(47/49): loss=18385928.99606757\n",
      "Gradient Descent(48/49): loss=28374829.411959596\n",
      "Gradient Descent(49/49): loss=43790604.56291932\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46581814684996903\n",
      "Gradient Descent(2/49): loss=0.4689784734468361\n",
      "Gradient Descent(3/49): loss=0.49302127024450304\n",
      "Gradient Descent(4/49): loss=0.5393202944258662\n",
      "Gradient Descent(5/49): loss=0.6157023501434873\n",
      "Gradient Descent(6/49): loss=0.7366206601067185\n",
      "Gradient Descent(7/49): loss=0.9254992019544892\n",
      "Gradient Descent(8/49): loss=1.2190936125916267\n",
      "Gradient Descent(9/49): loss=1.6745483295236183\n",
      "Gradient Descent(10/49): loss=2.3804592570768475\n",
      "Gradient Descent(11/49): loss=3.474065054680424\n",
      "Gradient Descent(12/49): loss=5.16789476404118\n",
      "Gradient Descent(13/49): loss=7.791042222550417\n",
      "Gradient Descent(14/49): loss=11.853081203183391\n",
      "Gradient Descent(15/49): loss=18.143033664608144\n",
      "Gradient Descent(16/49): loss=27.88261197114637\n",
      "Gradient Descent(17/49): loss=42.9634971384121\n",
      "Gradient Descent(18/49): loss=66.31473652173432\n",
      "Gradient Descent(19/49): loss=102.47161585394797\n",
      "Gradient Descent(20/49): loss=158.45649034872082\n",
      "Gradient Descent(21/49): loss=245.14264741299263\n",
      "Gradient Descent(22/49): loss=379.3660865627393\n",
      "Gradient Descent(23/49): loss=587.1953605214304\n",
      "Gradient Descent(24/49): loss=908.9945368782683\n",
      "Gradient Descent(25/49): loss=1407.2625945324141\n",
      "Gradient Descent(26/49): loss=2178.7718005312986\n",
      "Gradient Descent(27/49): loss=3373.362548832891\n",
      "Gradient Descent(28/49): loss=5223.04494189868\n",
      "Gradient Descent(29/49): loss=8087.05914272663\n",
      "Gradient Descent(30/49): loss=12521.645992597143\n",
      "Gradient Descent(31/49): loss=19388.078548347727\n",
      "Gradient Descent(32/49): loss=30019.936121542338\n",
      "Gradient Descent(33/49): loss=46482.10831455909\n",
      "Gradient Descent(34/49): loss=71971.83209151428\n",
      "Gradient Descent(35/49): loss=111439.65017984316\n",
      "Gradient Descent(36/49): loss=172550.89160277837\n",
      "Gradient Descent(37/49): loss=267174.41039675334\n",
      "Gradient Descent(38/49): loss=413687.72117454564\n",
      "Gradient Descent(39/49): loss=640546.2285020553\n",
      "Gradient Descent(40/49): loss=991809.755813845\n",
      "Gradient Descent(41/49): loss=1535699.7208228037\n",
      "Gradient Descent(42/49): loss=2377848.908046625\n",
      "Gradient Descent(43/49): loss=3681817.1721325214\n",
      "Gradient Descent(44/49): loss=5700857.574375381\n",
      "Gradient Descent(45/49): loss=8827102.482424613\n",
      "Gradient Descent(46/49): loss=13667722.419598091\n",
      "Gradient Descent(47/49): loss=21162849.022047617\n",
      "Gradient Descent(48/49): loss=32768164.769981384\n",
      "Gradient Descent(49/49): loss=50737621.55875283\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4645194717913602\n",
      "Gradient Descent(2/49): loss=0.4664652597039949\n",
      "Gradient Descent(3/49): loss=0.48843744149328194\n",
      "Gradient Descent(4/49): loss=0.530901035752765\n",
      "Gradient Descent(5/49): loss=0.6002267094345162\n",
      "Gradient Descent(6/49): loss=0.7084555248679153\n",
      "Gradient Descent(7/49): loss=0.874978856441151\n",
      "Gradient Descent(8/49): loss=1.129829243268512\n",
      "Gradient Descent(9/49): loss=1.5190010666756537\n",
      "Gradient Descent(10/49): loss=2.112694192354334\n",
      "Gradient Descent(11/49): loss=3.0179378224406235\n",
      "Gradient Descent(12/49): loss=4.397855797717646\n",
      "Gradient Descent(13/49): loss=6.50103706305583\n",
      "Gradient Descent(14/49): loss=9.706297520845249\n",
      "Gradient Descent(15/49): loss=14.590892282647701\n",
      "Gradient Descent(16/49): loss=22.034460561079328\n",
      "Gradient Descent(17/49): loss=33.377419975591586\n",
      "Gradient Descent(18/49): loss=50.66233169658201\n",
      "Gradient Descent(19/49): loss=77.00169819189267\n",
      "Gradient Descent(20/49): loss=117.1384216613321\n",
      "Gradient Descent(21/49): loss=178.2998430202508\n",
      "Gradient Descent(22/49): loss=271.4991453616674\n",
      "Gradient Descent(23/49): loss=413.518461246841\n",
      "Gradient Descent(24/49): loss=629.930775292017\n",
      "Gradient Descent(25/49): loss=959.7047658965462\n",
      "Gradient Descent(26/49): loss=1462.2217644876941\n",
      "Gradient Descent(27/49): loss=2227.968352550538\n",
      "Gradient Descent(28/49): loss=3394.8299764518247\n",
      "Gradient Descent(29/49): loss=5172.919516513621\n",
      "Gradient Descent(30/49): loss=7882.411607314801\n",
      "Gradient Descent(31/49): loss=12011.1954081054\n",
      "Gradient Descent(32/49): loss=18302.727163912765\n",
      "Gradient Descent(33/49): loss=27889.901872717473\n",
      "Gradient Descent(34/49): loss=42499.049879818675\n",
      "Gradient Descent(35/49): loss=64760.791380434224\n",
      "Gradient Descent(36/49): loss=98683.72335593041\n",
      "Gradient Descent(37/49): loss=150376.23415805947\n",
      "Gradient Descent(38/49): loss=229146.42046712534\n",
      "Gradient Descent(39/49): loss=349178.16497351276\n",
      "Gradient Descent(40/49): loss=532085.1804568368\n",
      "Gradient Descent(41/49): loss=810802.9183949931\n",
      "Gradient Descent(42/49): loss=1235519.1450042687\n",
      "Gradient Descent(43/49): loss=1882711.083603356\n",
      "Gradient Descent(44/49): loss=2868916.4111497086\n",
      "Gradient Descent(45/49): loss=4371717.806005453\n",
      "Gradient Descent(46/49): loss=6661719.663912582\n",
      "Gradient Descent(47/49): loss=10151274.921938337\n",
      "Gradient Descent(48/49): loss=15468736.065790692\n",
      "Gradient Descent(49/49): loss=23571600.449807446\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46895588183359777\n",
      "Gradient Descent(2/49): loss=0.47972277601722746\n",
      "Gradient Descent(3/49): loss=0.5194894009241886\n",
      "Gradient Descent(4/49): loss=0.5962398751685011\n",
      "Gradient Descent(5/49): loss=0.7300491973088995\n",
      "Gradient Descent(6/49): loss=0.9572803548120611\n",
      "Gradient Descent(7/49): loss=1.3400659170221954\n",
      "Gradient Descent(8/49): loss=1.983097400470486\n",
      "Gradient Descent(9/49): loss=3.0621342623710524\n",
      "Gradient Descent(10/49): loss=4.871953840974858\n",
      "Gradient Descent(11/49): loss=7.9068075203338495\n",
      "Gradient Descent(12/49): loss=12.995335833875524\n",
      "Gradient Descent(13/49): loss=21.526766883396053\n",
      "Gradient Descent(14/49): loss=35.830142996673786\n",
      "Gradient Descent(15/49): loss=59.81009461804857\n",
      "Gradient Descent(16/49): loss=100.01271212354374\n",
      "Gradient Descent(17/49): loss=167.41247287822148\n",
      "Gradient Descent(18/49): loss=280.4080117446112\n",
      "Gradient Descent(19/49): loss=469.8445085851831\n",
      "Gradient Descent(20/49): loss=787.4336841950561\n",
      "Gradient Descent(21/49): loss=1319.8698617937412\n",
      "Gradient Descent(22/49): loss=2212.4954406116567\n",
      "Gradient Descent(23/49): loss=3708.975888779847\n",
      "Gradient Descent(24/49): loss=6217.8145778691905\n",
      "Gradient Descent(25/49): loss=10423.864415039008\n",
      "Gradient Descent(26/49): loss=17475.276276256966\n",
      "Gradient Descent(27/49): loss=29296.916683072166\n",
      "Gradient Descent(28/49): loss=49115.81023836438\n",
      "Gradient Descent(29/49): loss=82342.04001500548\n",
      "Gradient Descent(30/49): loss=138045.57059477054\n",
      "Gradient Descent(31/49): loss=231432.13105842506\n",
      "Gradient Descent(32/49): loss=387994.01465439424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=650468.8639910988\n",
      "Gradient Descent(34/49): loss=1090506.0233590798\n",
      "Gradient Descent(35/49): loss=1828225.0928092296\n",
      "Gradient Descent(36/49): loss=3065005.7005744\n",
      "Gradient Descent(37/49): loss=5138459.315973172\n",
      "Gradient Descent(38/49): loss=8614589.090444595\n",
      "Gradient Descent(39/49): loss=14442295.15491527\n",
      "Gradient Descent(40/49): loss=24212401.61731342\n",
      "Gradient Descent(41/49): loss=40591913.42355871\n",
      "Gradient Descent(42/49): loss=68052044.79910286\n",
      "Gradient Descent(43/49): loss=114088753.59001741\n",
      "Gradient Descent(44/49): loss=191268958.13142434\n",
      "Gradient Descent(45/49): loss=320661004.8154224\n",
      "Gradient Descent(46/49): loss=537585821.8012205\n",
      "Gradient Descent(47/49): loss=901258686.017059\n",
      "Gradient Descent(48/49): loss=1510953574.802435\n",
      "Gradient Descent(49/49): loss=2533102582.847134\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46883579813802145\n",
      "Gradient Descent(2/49): loss=0.4823555524940659\n",
      "Gradient Descent(3/49): loss=0.5289051887052467\n",
      "Gradient Descent(4/49): loss=0.6201080172640707\n",
      "Gradient Descent(5/49): loss=0.7831962850845188\n",
      "Gradient Descent(6/49): loss=1.0681263759568167\n",
      "Gradient Descent(7/49): loss=1.5624884666748842\n",
      "Gradient Descent(8/49): loss=2.4182237340986874\n",
      "Gradient Descent(9/49): loss=3.898184704135417\n",
      "Gradient Descent(10/49): loss=6.45676877495744\n",
      "Gradient Descent(11/49): loss=10.87934565420595\n",
      "Gradient Descent(12/49): loss=18.523254294902234\n",
      "Gradient Descent(13/49): loss=31.73431807714924\n",
      "Gradient Descent(14/49): loss=54.566689062821816\n",
      "Gradient Descent(15/49): loss=94.0269131796994\n",
      "Gradient Descent(16/49): loss=162.2239719100906\n",
      "Gradient Descent(17/49): loss=280.0850656839102\n",
      "Gradient Descent(18/49): loss=483.777370331475\n",
      "Gradient Descent(19/49): loss=835.8063573296591\n",
      "Gradient Descent(20/49): loss=1444.196326646233\n",
      "Gradient Descent(21/49): loss=2495.638922042485\n",
      "Gradient Descent(22/49): loss=4312.78163676412\n",
      "Gradient Descent(23/49): loss=7453.23595469998\n",
      "Gradient Descent(24/49): loss=12880.686928037805\n",
      "Gradient Descent(25/49): loss=22260.611157821495\n",
      "Gradient Descent(26/49): loss=38471.34768325277\n",
      "Gradient Descent(27/49): loss=66487.34983331445\n",
      "Gradient Descent(28/49): loss=114905.65415645298\n",
      "Gradient Descent(29/49): loss=198583.9811944369\n",
      "Gradient Descent(30/49): loss=343200.0000423339\n",
      "Gradient Descent(31/49): loss=593130.8201153807\n",
      "Gradient Descent(32/49): loss=1025070.6239245327\n",
      "Gradient Descent(33/49): loss=1771565.170048892\n",
      "Gradient Descent(34/49): loss=3061685.002590818\n",
      "Gradient Descent(35/49): loss=5291318.415055748\n",
      "Gradient Descent(36/49): loss=9144654.383153046\n",
      "Gradient Descent(37/49): loss=15804133.919060336\n",
      "Gradient Descent(38/49): loss=27313295.86499674\n",
      "Gradient Descent(39/49): loss=47203860.5830332\n",
      "Gradient Descent(40/49): loss=81579479.47346528\n",
      "Gradient Descent(41/49): loss=140988711.48122683\n",
      "Gradient Descent(42/49): loss=243661971.24129224\n",
      "Gradient Descent(43/49): loss=421105744.0942757\n",
      "Gradient Descent(44/49): loss=727770717.9918232\n",
      "Gradient Descent(45/49): loss=1257760611.1479752\n",
      "Gradient Descent(46/49): loss=2173708993.767947\n",
      "Gradient Descent(47/49): loss=3756685292.8592267\n",
      "Gradient Descent(48/49): loss=6492444218.833941\n",
      "Gradient Descent(49/49): loss=11220485254.790691\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46972528779109113\n",
      "Gradient Descent(2/49): loss=0.48344972826342597\n",
      "Gradient Descent(3/49): loss=0.5301044288391052\n",
      "Gradient Descent(4/49): loss=0.621637282946829\n",
      "Gradient Descent(5/49): loss=0.7857817286180239\n",
      "Gradient Descent(6/49): loss=1.0734574041881466\n",
      "Gradient Descent(7/49): loss=1.574178944234537\n",
      "Gradient Descent(8/49): loss=2.443703263947527\n",
      "Gradient Descent(9/49): loss=3.9523416727195397\n",
      "Gradient Descent(10/49): loss=6.568885479228656\n",
      "Gradient Descent(11/49): loss=11.106188108292914\n",
      "Gradient Descent(12/49): loss=18.97360788239147\n",
      "Gradient Descent(13/49): loss=32.614708093600356\n",
      "Gradient Descent(14/49): loss=56.26614789865499\n",
      "Gradient Descent(15/49): loss=97.27344970855388\n",
      "Gradient Descent(16/49): loss=168.37227652494602\n",
      "Gradient Descent(17/49): loss=291.64370824111467\n",
      "Gradient Descent(18/49): loss=505.37189460236806\n",
      "Gradient Descent(19/49): loss=875.933844557161\n",
      "Gradient Descent(20/49): loss=1518.4139265383412\n",
      "Gradient Descent(21/49): loss=2632.345260290857\n",
      "Gradient Descent(22/49): loss=4563.678092061743\n",
      "Gradient Descent(23/49): loss=7912.220476818823\n",
      "Gradient Descent(24/49): loss=13717.918782741184\n",
      "Gradient Descent(25/49): loss=23783.830569211554\n",
      "Gradient Descent(26/49): loss=41236.094510663315\n",
      "Gradient Descent(27/49): loss=71494.80546681616\n",
      "Gradient Descent(28/49): loss=123957.31632060728\n",
      "Gradient Descent(29/49): loss=214916.74434881992\n",
      "Gradient Descent(30/49): loss=372622.073482657\n",
      "Gradient Descent(31/49): loss=646051.3525249001\n",
      "Gradient Descent(32/49): loss=1120122.65393959\n",
      "Gradient Descent(33/49): loss=1942066.8129119116\n",
      "Gradient Descent(34/49): loss=3367152.44541824\n",
      "Gradient Descent(35/49): loss=5837963.920557879\n",
      "Gradient Descent(36/49): loss=10121853.39802231\n",
      "Gradient Descent(37/49): loss=17549254.978277616\n",
      "Gradient Descent(38/49): loss=30426873.442599993\n",
      "Gradient Descent(39/49): loss=52754070.31223692\n",
      "Gradient Descent(40/49): loss=91464932.99511108\n",
      "Gradient Descent(41/49): loss=158581772.53393424\n",
      "Gradient Descent(42/49): loss=274948854.98779905\n",
      "Gradient Descent(43/49): loss=476705939.6757462\n",
      "Gradient Descent(44/49): loss=826512090.7229143\n",
      "Gradient Descent(45/49): loss=1433005505.8100643\n",
      "Gradient Descent(46/49): loss=2484542940.02309\n",
      "Gradient Descent(47/49): loss=4307697071.700488\n",
      "Gradient Descent(48/49): loss=7468679153.465295\n",
      "Gradient Descent(49/49): loss=12949185462.63027\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46841381154986367\n",
      "Gradient Descent(2/49): loss=0.4807442560088523\n",
      "Gradient Descent(3/49): loss=0.5245029691620129\n",
      "Gradient Descent(4/49): loss=0.6097418034140912\n",
      "Gradient Descent(5/49): loss=0.7606044625602243\n",
      "Gradient Descent(6/49): loss=1.021137604281926\n",
      "Gradient Descent(7/49): loss=1.4677555244892295\n",
      "Gradient Descent(8/49): loss=2.2314484490020408\n",
      "Gradient Descent(9/49): loss=3.536070006311823\n",
      "Gradient Descent(10/49): loss=5.763855295739241\n",
      "Gradient Descent(11/49): loss=9.567330165296877\n",
      "Gradient Descent(12/49): loss=16.060369427176198\n",
      "Gradient Descent(13/49): loss=27.14434230503292\n",
      "Gradient Descent(14/49): loss=46.0648396615177\n",
      "Gradient Descent(15/49): loss=78.36199355673966\n",
      "Gradient Descent(16/49): loss=133.49264507833684\n",
      "Gradient Descent(17/49): loss=227.59933565958895\n",
      "Gradient Descent(18/49): loss=388.23689036406176\n",
      "Gradient Descent(19/49): loss=662.4405501451648\n",
      "Gradient Descent(20/49): loss=1130.498025144144\n",
      "Gradient Descent(21/49): loss=1929.4579653918586\n",
      "Gradient Descent(22/49): loss=3293.2581959769727\n",
      "Gradient Descent(23/49): loss=5621.223378027064\n",
      "Gradient Descent(24/49): loss=9594.98840537678\n",
      "Gradient Descent(25/49): loss=16378.083039875164\n",
      "Gradient Descent(26/49): loss=27956.616733949682\n",
      "Gradient Descent(27/49): loss=47720.817125004796\n",
      "Gradient Descent(28/49): loss=81457.69832605936\n",
      "Gradient Descent(29/49): loss=139045.51511015528\n",
      "Gradient Descent(30/49): loss=237346.14399085278\n",
      "Gradient Descent(31/49): loss=405142.2886018516\n",
      "Gradient Descent(32/49): loss=691565.1371467314\n",
      "Gradient Descent(33/49): loss=1180480.1139788378\n",
      "Gradient Descent(34/49): loss=2015042.914270796\n",
      "Gradient Descent(35/49): loss=3439615.898534408\n",
      "Gradient Descent(36/49): loss=5871318.086469495\n",
      "Gradient Descent(37/49): loss=10022158.79174767\n",
      "Gradient Descent(38/49): loss=17107515.97320655\n",
      "Gradient Descent(39/49): loss=29202002.356360182\n",
      "Gradient Descent(40/49): loss=49846917.93728682\n",
      "Gradient Descent(41/49): loss=85087152.68894035\n",
      "Gradient Descent(42/49): loss=145241147.53007537\n",
      "Gradient Descent(43/49): loss=247922163.16025496\n",
      "Gradient Descent(44/49): loss=423195492.8649213\n",
      "Gradient Descent(45/49): loss=722381665.8611182\n",
      "Gradient Descent(46/49): loss=1233083244.1499095\n",
      "Gradient Descent(47/49): loss=2104835101.713075\n",
      "Gradient Descent(48/49): loss=3592888660.7233834\n",
      "Gradient Descent(49/49): loss=6132950233.603683\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47311012172351957\n",
      "Gradient Descent(2/49): loss=0.49622288435447953\n",
      "Gradient Descent(3/49): loss=0.5648768301044255\n",
      "Gradient Descent(4/49): loss=0.7047162985209822\n",
      "Gradient Descent(5/49): loss=0.9717140633032194\n",
      "Gradient Descent(6/49): loss=1.4736143307390324\n",
      "Gradient Descent(7/49): loss=2.4129669553666306\n",
      "Gradient Descent(8/49): loss=4.168594234872462\n",
      "Gradient Descent(9/49): loss=7.448155646271531\n",
      "Gradient Descent(10/49): loss=13.57322008005788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=25.011670802202353\n",
      "Gradient Descent(12/49): loss=46.37192120045448\n",
      "Gradient Descent(13/49): loss=86.25946975260314\n",
      "Gradient Descent(14/49): loss=160.7437278453808\n",
      "Gradient Descent(15/49): loss=299.8317719874658\n",
      "Gradient Descent(16/49): loss=559.5570468125896\n",
      "Gradient Descent(17/49): loss=1044.553093795814\n",
      "Gradient Descent(18/49): loss=1950.2063955378137\n",
      "Gradient Descent(19/49): loss=3641.370100524997\n",
      "Gradient Descent(20/49): loss=6799.349370344105\n",
      "Gradient Descent(21/49): loss=12696.373110606744\n",
      "Gradient Descent(22/49): loss=23708.12622323344\n",
      "Gradient Descent(23/49): loss=44270.821974986124\n",
      "Gradient Descent(24/49): loss=82668.38224709629\n",
      "Gradient Descent(25/49): loss=154369.71207127508\n",
      "Gradient Descent(26/49): loss=288260.5284175137\n",
      "Gradient Descent(27/49): loss=538280.3081350911\n",
      "Gradient Descent(28/49): loss=1005152.4352323996\n",
      "Gradient Descent(29/49): loss=1876961.7907394946\n",
      "Gradient Descent(30/49): loss=3504926.940436604\n",
      "Gradient Descent(31/49): loss=6544892.629913862\n",
      "Gradient Descent(32/49): loss=12221544.488651773\n",
      "Gradient Descent(33/49): loss=22821787.9178756\n",
      "Gradient Descent(34/49): loss=42616054.600834236\n",
      "Gradient Descent(35/49): loss=79578695.70724341\n",
      "Gradient Descent(36/49): loss=148600542.25716022\n",
      "Gradient Descent(37/49): loss=277487850.0510512\n",
      "Gradient Descent(38/49): loss=518164374.089532\n",
      "Gradient Descent(39/49): loss=967589458.7989291\n",
      "Gradient Descent(40/49): loss=1806819240.652239\n",
      "Gradient Descent(41/49): loss=3373947224.1846504\n",
      "Gradient Descent(42/49): loss=6300309193.0687\n",
      "Gradient Descent(43/49): loss=11764824193.067423\n",
      "Gradient Descent(44/49): loss=21968935817.918175\n",
      "Gradient Descent(45/49): loss=41023489433.82687\n",
      "Gradient Descent(46/49): loss=76604834175.03061\n",
      "Gradient Descent(47/49): loss=143047329712.44333\n",
      "Gradient Descent(48/49): loss=267118110211.27267\n",
      "Gradient Descent(49/49): loss=498800536481.986\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.473056886253934\n",
      "Gradient Descent(2/49): loss=0.49975633767953204\n",
      "Gradient Descent(3/49): loss=0.5782341928867039\n",
      "Gradient Descent(4/49): loss=0.7415515390471717\n",
      "Gradient Descent(5/49): loss=1.0619262016199855\n",
      "Gradient Descent(6/49): loss=1.6817139745685648\n",
      "Gradient Descent(7/49): loss=2.8762019650768846\n",
      "Gradient Descent(8/49): loss=5.175577841218802\n",
      "Gradient Descent(9/49): loss=9.600022775932912\n",
      "Gradient Descent(10/49): loss=18.11214184434383\n",
      "Gradient Descent(11/49): loss=34.487364324991546\n",
      "Gradient Descent(12/49): loss=65.98832430991486\n",
      "Gradient Descent(13/49): loss=126.58579640815086\n",
      "Gradient Descent(14/49): loss=243.1546505280504\n",
      "Gradient Descent(15/49): loss=467.39269288882645\n",
      "Gradient Descent(16/49): loss=898.7483252472385\n",
      "Gradient Descent(17/49): loss=1728.525297084346\n",
      "Gradient Descent(18/49): loss=3324.72473502852\n",
      "Gradient Descent(19/49): loss=6395.251363805475\n",
      "Gradient Descent(20/49): loss=12301.86486632049\n",
      "Gradient Descent(21/49): loss=23664.111865496605\n",
      "Gradient Descent(22/49): loss=45521.07861919501\n",
      "Gradient Descent(23/49): loss=87566.19454891232\n",
      "Gradient Descent(24/49): loss=168446.2081179025\n",
      "Gradient Descent(25/49): loss=324030.9042194147\n",
      "Gradient Descent(26/49): loss=623321.1314883037\n",
      "Gradient Descent(27/49): loss=1199050.251064801\n",
      "Gradient Descent(28/49): loss=2306550.5596962655\n",
      "Gradient Descent(29/49): loss=4436991.581020389\n",
      "Gradient Descent(30/49): loss=8535210.705462402\n",
      "Gradient Descent(31/49): loss=16418742.717618624\n",
      "Gradient Descent(32/49): loss=31583885.209743686\n",
      "Gradient Descent(33/49): loss=60756284.896939985\n",
      "Gradient Descent(34/49): loss=116873720.25566645\n",
      "Gradient Descent(35/49): loss=224823926.10108072\n",
      "Gradient Descent(36/49): loss=432482149.74528116\n",
      "Gradient Descent(37/49): loss=831943526.4915738\n",
      "Gradient Descent(38/49): loss=1600366701.4168596\n",
      "Gradient Descent(39/49): loss=3078542590.589854\n",
      "Gradient Descent(40/49): loss=5922033040.425388\n",
      "Gradient Descent(41/49): loss=11391908443.973848\n",
      "Gradient Descent(42/49): loss=21914024645.234375\n",
      "Gradient Descent(43/49): loss=42154874972.64847\n",
      "Gradient Descent(44/49): loss=81091151111.44444\n",
      "Gradient Descent(45/49): loss=155990850236.40002\n",
      "Gradient Descent(46/49): loss=300071524747.0039\n",
      "Gradient Descent(47/49): loss=577232060903.4747\n",
      "Gradient Descent(48/49): loss=1110391438894.084\n",
      "Gradient Descent(49/49): loss=2136002538804.9795\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47394554006754547\n",
      "Gradient Descent(2/49): loss=0.500728614221771\n",
      "Gradient Descent(3/49): loss=0.5791830415405819\n",
      "Gradient Descent(4/49): loss=0.7428277069827\n",
      "Gradient Descent(5/49): loss=1.0648085204573965\n",
      "Gradient Descent(6/49): loss=1.6896477806089678\n",
      "Gradient Descent(7/49): loss=2.897651827058716\n",
      "Gradient Descent(8/49): loss=5.230352533691693\n",
      "Gradient Descent(9/49): loss=9.733028614417421\n",
      "Gradient Descent(10/49): loss=18.42288886744818\n",
      "Gradient Descent(11/49): loss=35.19260663367343\n",
      "Gradient Descent(12/49): loss=67.55390515309567\n",
      "Gradient Descent(13/49): loss=130.00216811899003\n",
      "Gradient Descent(14/49): loss=250.50913149809503\n",
      "Gradient Descent(15/49): loss=483.0518254418415\n",
      "Gradient Descent(16/49): loss=931.7896577712793\n",
      "Gradient Descent(17/49): loss=1797.7188991911069\n",
      "Gradient Descent(18/49): loss=3468.7017789285696\n",
      "Gradient Descent(19/49): loss=6693.195505540765\n",
      "Gradient Descent(20/49): loss=12915.496916723121\n",
      "Gradient Descent(21/49): loss=24922.663613549128\n",
      "Gradient Descent(22/49): loss=48092.87677288775\n",
      "Gradient Descent(23/49): loss=92804.40512908802\n",
      "Gradient Descent(24/49): loss=179084.17941663298\n",
      "Gradient Descent(25/49): loss=345578.1400021914\n",
      "Gradient Descent(26/49): loss=666861.3042251046\n",
      "Gradient Descent(27/49): loss=1286840.9792518485\n",
      "Gradient Descent(28/49): loss=2483214.8954300703\n",
      "Gradient Descent(29/49): loss=4791855.976507683\n",
      "Gradient Descent(30/49): loss=9246837.45760354\n",
      "Gradient Descent(31/49): loss=17843609.021283407\n",
      "Gradient Descent(32/49): loss=34432787.14268875\n",
      "Gradient Descent(33/49): loss=66444901.07453379\n",
      "Gradient Descent(34/49): loss=128218632.7738223\n",
      "Gradient Descent(35/49): loss=247423316.8561155\n",
      "Gradient Descent(36/49): loss=477452429.8183212\n",
      "Gradient Descent(37/49): loss=921339288.9424016\n",
      "Gradient Descent(38/49): loss=1777907143.1835103\n",
      "Gradient Descent(39/49): loss=3430824939.324201\n",
      "Gradient Descent(40/49): loss=6620458109.972502\n",
      "Gradient Descent(41/49): loss=12775488799.976467\n",
      "Gradient Descent(42/49): loss=24652842955.780575\n",
      "Gradient Descent(43/49): loss=47572556739.09418\n",
      "Gradient Descent(44/49): loss=91800696526.63165\n",
      "Gradient Descent(45/49): loss=177147676317.02298\n",
      "Gradient Descent(46/49): loss=341841624430.8857\n",
      "Gradient Descent(47/49): loss=659651306881.8358\n",
      "Gradient Descent(48/49): loss=1272928208773.467\n",
      "Gradient Descent(49/49): loss=2456367792782.5435\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4726230808662751\n",
      "Gradient Descent(2/49): loss=0.4978091852787653\n",
      "Gradient Descent(3/49): loss=0.5722901025684891\n",
      "Gradient Descent(4/49): loss=0.7259795691190273\n",
      "Gradient Descent(5/49): loss=1.024171129027494\n",
      "Gradient Descent(6/49): loss=1.5943254617210747\n",
      "Gradient Descent(7/49): loss=2.680100920459476\n",
      "Gradient Descent(8/49): loss=4.745193105361037\n",
      "Gradient Descent(9/49): loss=8.671140881145982\n",
      "Gradient Descent(10/49): loss=16.133448602061485\n",
      "Gradient Descent(11/49): loss=30.316490776553294\n",
      "Gradient Descent(12/49): loss=57.272229287621705\n",
      "Gradient Descent(13/49): loss=108.50248103856507\n",
      "Gradient Descent(14/49): loss=205.8665429534608\n",
      "Gradient Descent(15/49): loss=390.9081708855987\n",
      "Gradient Descent(16/49): loss=742.5815695851613\n",
      "Gradient Descent(17/49): loss=1410.9397607890735\n",
      "Gradient Descent(18/49): loss=2681.159561594296\n",
      "Gradient Descent(19/49): loss=5095.221501545989\n",
      "Gradient Descent(20/49): loss=9683.163351781697\n",
      "Gradient Descent(21/49): loss=18402.579066178583\n",
      "Gradient Descent(22/49): loss=34973.88957681685\n",
      "Gradient Descent(23/49): loss=66467.78075206853\n",
      "Gradient Descent(24/49): loss=126322.14028032689\n",
      "Gradient Descent(25/49): loss=240075.76720739924\n",
      "Gradient Descent(26/49): loss=456265.3268036798\n",
      "Gradient Descent(27/49): loss=867135.0891029697\n",
      "Gradient Descent(28/49): loss=1647995.9310800415\n",
      "Gradient Descent(29/49): loss=3132027.394122808\n",
      "Gradient Descent(30/49): loss=5952439.514680923\n",
      "Gradient Descent(31/49): loss=11312652.372480368\n",
      "Gradient Descent(32/49): loss=21499774.201634124\n",
      "Gradient Descent(33/49): loss=40860470.113246486\n",
      "Gradient Descent(34/49): loss=77655607.39216372\n",
      "Gradient Descent(35/49): loss=147585021.78683203\n",
      "Gradient Descent(36/49): loss=280486360.36614364\n",
      "Gradient Descent(37/49): loss=533066278.97494465\n",
      "Gradient Descent(38/49): loss=1013096171.5733905\n",
      "Gradient Descent(39/49): loss=1925396322.1841176\n",
      "Gradient Descent(40/49): loss=3659229105.5948706\n",
      "Gradient Descent(41/49): loss=6954390373.317309\n",
      "Gradient Descent(42/49): loss=13216867288.1546\n",
      "Gradient Descent(43/49): loss=25118748234.93383\n",
      "Gradient Descent(44/49): loss=47738355779.62421\n",
      "Gradient Descent(45/49): loss=90727077290.42589\n",
      "Gradient Descent(46/49): loss=172427441608.51074\n",
      "Gradient Descent(47/49): loss=327699552411.58575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=622795279272.4424\n",
      "Gradient Descent(49/49): loss=1183626761250.4385\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4775749294063277\n",
      "Gradient Descent(2/49): loss=0.5157339620756412\n",
      "Gradient Descent(3/49): loss=0.6241175947934511\n",
      "Gradient Descent(4/49): loss=0.8615365159602862\n",
      "Gradient Descent(5/49): loss=1.3592390355500794\n",
      "Gradient Descent(6/49): loss=2.3924760703449848\n",
      "Gradient Descent(7/49): loss=4.532140910853837\n",
      "Gradient Descent(8/49): loss=8.959769099558647\n",
      "Gradient Descent(9/49): loss=18.119628110391677\n",
      "Gradient Descent(10/49): loss=37.06775815519196\n",
      "Gradient Descent(11/49): loss=76.26251569522856\n",
      "Gradient Descent(12/49): loss=157.33677798739473\n",
      "Gradient Descent(13/49): loss=325.0376154957343\n",
      "Gradient Descent(14/49): loss=671.9232033462364\n",
      "Gradient Descent(15/49): loss=1389.447748449677\n",
      "Gradient Descent(16/49): loss=2873.6293440285453\n",
      "Gradient Descent(17/49): loss=5943.6211995431695\n",
      "Gradient Descent(18/49): loss=12293.820587500095\n",
      "Gradient Descent(19/49): loss=25429.044528353952\n",
      "Gradient Descent(20/49): loss=52598.916552030714\n",
      "Gradient Descent(21/49): loss=108799.09577538425\n",
      "Gradient Descent(22/49): loss=225047.71595174327\n",
      "Gradient Descent(23/49): loss=465504.9859772831\n",
      "Gradient Descent(24/49): loss=962884.6415723857\n",
      "Gradient Descent(25/49): loss=1991701.6188891223\n",
      "Gradient Descent(26/49): loss=4119782.976378651\n",
      "Gradient Descent(27/49): loss=8521664.325210402\n",
      "Gradient Descent(28/49): loss=17626842.254833214\n",
      "Gradient Descent(29/49): loss=36460667.7397178\n",
      "Gradient Descent(30/49): loss=75417949.53419062\n",
      "Gradient Descent(31/49): loss=156000081.1902505\n",
      "Gradient Descent(32/49): loss=322682140.18207353\n",
      "Gradient Descent(33/49): loss=667459676.0795125\n",
      "Gradient Descent(34/49): loss=1380623108.1757934\n",
      "Gradient Descent(35/49): loss=2855783256.169313\n",
      "Gradient Descent(36/49): loss=5907113938.999812\n",
      "Gradient Descent(37/49): loss=12218712682.124104\n",
      "Gradient Descent(38/49): loss=25274091739.648724\n",
      "Gradient Descent(39/49): loss=52278806277.51872\n",
      "Gradient Descent(40/49): loss=108137361135.14557\n",
      "Gradient Descent(41/49): loss=223679339792.4963\n",
      "Gradient Descent(42/49): loss=462674939769.6625\n",
      "Gradient Descent(43/49): loss=957031168321.3265\n",
      "Gradient Descent(44/49): loss=1979594264592.4407\n",
      "Gradient Descent(45/49): loss=4094739630352.3525\n",
      "Gradient Descent(46/49): loss=8469863214031.317\n",
      "Gradient Descent(47/49): loss=17519693397020.543\n",
      "Gradient Descent(48/49): loss=36239033496683.836\n",
      "Gradient Descent(49/49): loss=74959505227249.97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4775928210016103\n",
      "Gradient Descent(2/49): loss=0.520304402029296\n",
      "Gradient Descent(3/49): loss=0.6424958904691115\n",
      "Gradient Descent(4/49): loss=0.9166883225708395\n",
      "Gradient Descent(5/49): loss=1.5075392709230155\n",
      "Gradient Descent(6/49): loss=2.7696956740502863\n",
      "Gradient Descent(7/49): loss=5.460026117529686\n",
      "Gradient Descent(8/49): loss=11.19099728833078\n",
      "Gradient Descent(9/49): loss=23.39670282494521\n",
      "Gradient Descent(10/49): loss=49.3902673173538\n",
      "Gradient Descent(11/49): loss=104.74522817001933\n",
      "Gradient Descent(12/49): loss=222.62582815252026\n",
      "Gradient Descent(13/49): loss=473.6561040860472\n",
      "Gradient Descent(14/49): loss=1008.2315733578887\n",
      "Gradient Descent(15/49): loss=2146.622941620991\n",
      "Gradient Descent(16/49): loss=4570.854010816306\n",
      "Gradient Descent(17/49): loss=9733.310073655444\n",
      "Gradient Descent(18/49): loss=20726.87883555326\n",
      "Gradient Descent(19/49): loss=44137.93540946486\n",
      "Gradient Descent(20/49): loss=93992.31624287907\n",
      "Gradient Descent(21/49): loss=200158.3608453341\n",
      "Gradient Descent(22/49): loss=426241.3813364133\n",
      "Gradient Descent(23/49): loss=907690.3446195048\n",
      "Gradient Descent(24/49): loss=1932946.9236233206\n",
      "Gradient Descent(25/49): loss=4116254.2579115233\n",
      "Gradient Descent(26/49): loss=8765657.16177619\n",
      "Gradient Descent(27/49): loss=18666666.98404556\n",
      "Gradient Descent(28/49): loss=39751093.85054158\n",
      "Gradient Descent(29/49): loss=84650863.09289272\n",
      "Gradient Descent(30/49): loss=180265948.61437535\n",
      "Gradient Descent(31/49): loss=383880460.0815662\n",
      "Gradient Descent(32/49): loss=817482219.1961253\n",
      "Gradient Descent(33/49): loss=1740847082.3016617\n",
      "Gradient Descent(34/49): loss=3707173676.9144297\n",
      "Gradient Descent(35/49): loss=7894511132.746754\n",
      "Gradient Descent(36/49): loss=16811542015.13122\n",
      "Gradient Descent(37/49): loss=35800563223.978096\n",
      "Gradient Descent(38/49): loss=76238118193.35258\n",
      "Gradient Descent(39/49): loss=162350816363.3529\n",
      "Gradient Descent(40/49): loss=345729776632.6791\n",
      "Gradient Descent(41/49): loss=736239466656.2432\n",
      "Gradient Descent(42/49): loss=1567838783057.243\n",
      "Gradient Descent(43/49): loss=3338748547157.7773\n",
      "Gradient Descent(44/49): loss=7109941392961.464\n",
      "Gradient Descent(45/49): loss=15140782810487.955\n",
      "Gradient Descent(46/49): loss=32242643285542.555\n",
      "Gradient Descent(47/49): loss=68661446310334.66\n",
      "Gradient Descent(48/49): loss=146216120299940.38\n",
      "Gradient Descent(49/49): loss=311370572343286.44\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4784789036793321\n",
      "Gradient Descent(2/49): loss=0.5211294536260235\n",
      "Gradient Descent(3/49): loss=0.6431065256273232\n",
      "Gradient Descent(4/49): loss=0.9175570875744388\n",
      "Gradient Descent(5/49): loss=1.5107590348251512\n",
      "Gradient Descent(6/49): loss=2.781834814493042\n",
      "Gradient Descent(7/49): loss=5.499518481611814\n",
      "Gradient Descent(8/49): loss=11.30658137878134\n",
      "Gradient Descent(9/49): loss=23.712426001703477\n",
      "Gradient Descent(10/49): loss=50.213567152358166\n",
      "Gradient Descent(11/49): loss=106.82324443551539\n",
      "Gradient Descent(12/49): loss=227.74708342932018\n",
      "Gradient Descent(13/49): loss=486.0511145381467\n",
      "Gradient Descent(14/49): loss=1037.810355028009\n",
      "Gradient Descent(15/49): loss=2216.413785227813\n",
      "Gradient Descent(16/49): loss=4734.00747222922\n",
      "Gradient Descent(17/49): loss=10111.793515431453\n",
      "Gradient Descent(18/49): loss=21599.183695748576\n",
      "Gradient Descent(19/49): loss=46137.18664930976\n",
      "Gradient Descent(20/49): loss=98552.36303132314\n",
      "Gradient Descent(21/49): loss=210515.45586246773\n",
      "Gradient Descent(22/49): loss=449677.7557371116\n",
      "Gradient Descent(23/49): loss=960547.9379314057\n",
      "Gradient Descent(24/49): loss=2051808.3209393718\n",
      "Gradient Descent(25/49): loss=4382829.517412497\n",
      "Gradient Descent(26/49): loss=9362080.943248728\n",
      "Gradient Descent(27/49): loss=19998168.164809376\n",
      "Gradient Descent(28/49): loss=42717718.094801985\n",
      "Gradient Descent(29/49): loss=91248530.06223106\n",
      "Gradient Descent(30/49): loss=194914303.26064196\n",
      "Gradient Descent(31/49): loss=416352851.2088254\n",
      "Gradient Descent(32/49): loss=889363653.1784402\n",
      "Gradient Descent(33/49): loss=1899753311.4065526\n",
      "Gradient Descent(34/49): loss=4058028042.5612297\n",
      "Gradient Descent(35/49): loss=8668278926.670135\n",
      "Gradient Descent(36/49): loss=18516150890.150623\n",
      "Gradient Descent(37/49): loss=39552008731.07346\n",
      "Gradient Descent(38/49): loss=84486317051.13498\n",
      "Gradient Descent(39/49): loss=180469665078.63605\n",
      "Gradient Descent(40/49): loss=385497926178.07666\n",
      "Gradient Descent(41/49): loss=823455016791.5095\n",
      "Gradient Descent(42/49): loss=1758967088103.2444\n",
      "Gradient Descent(43/49): loss=3757297185565.8013\n",
      "Gradient Descent(44/49): loss=8025893284839.566\n",
      "Gradient Descent(45/49): loss=17143962757881.912\n",
      "Gradient Descent(46/49): loss=36620902946570.9\n",
      "Gradient Descent(47/49): loss=78225235994847.1\n",
      "Gradient Descent(48/49): loss=167095485203553.66\n",
      "Gradient Descent(49/49): loss=356929586984569.25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4771472797405941\n",
      "Gradient Descent(2/49): loss=0.5179726869435449\n",
      "Gradient Descent(3/49): loss=0.6345926425958613\n",
      "Gradient Descent(4/49): loss=0.8937758725470432\n",
      "Gradient Descent(5/49): loss=1.4460580621251\n",
      "Gradient Descent(6/49): loss=2.6121657164588363\n",
      "Gradient Descent(7/49): loss=5.068660585452661\n",
      "Gradient Descent(8/49): loss=10.24001066604329\n",
      "Gradient Descent(9/49): loss=21.124221777360592\n",
      "Gradient Descent(10/49): loss=44.03054550576882\n",
      "Gradient Descent(11/49): loss=92.2364646711742\n",
      "Gradient Descent(12/49): loss=193.68362181100213\n",
      "Gradient Descent(13/49): loss=407.1734200709404\n",
      "Gradient Descent(14/49): loss=856.4496256006422\n",
      "Gradient Descent(15/49): loss=1801.9230038005855\n",
      "Gradient Descent(16/49): loss=3791.6111797873023\n",
      "Gradient Descent(17/49): loss=7978.7818148098995\n",
      "Gradient Descent(18/49): loss=16790.412130557685\n",
      "Gradient Descent(19/49): loss=35333.91875131353\n",
      "Gradient Descent(20/49): loss=74357.52968848291\n",
      "Gradient Descent(21/49): loss=156480.19574493033\n",
      "Gradient Descent(22/49): loss=329302.03619825136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=692994.4239332317\n",
      "Gradient Descent(24/49): loss=1458361.535445891\n",
      "Gradient Descent(25/49): loss=3069027.1281835632\n",
      "Gradient Descent(26/49): loss=6458568.712052507\n",
      "Gradient Descent(27/49): loss=13591639.785361135\n",
      "Gradient Descent(28/49): loss=28602726.587821838\n",
      "Gradient Descent(29/49): loss=60192588.04770762\n",
      "Gradient Descent(30/49): loss=126671408.68099637\n",
      "Gradient Descent(31/49): loss=266571787.7440262\n",
      "Gradient Descent(32/49): loss=560983088.5992432\n",
      "Gradient Descent(33/49): loss=1180552632.6687856\n",
      "Gradient Descent(34/49): loss=2484396672.701966\n",
      "Gradient Descent(35/49): loss=5228252139.779487\n",
      "Gradient Descent(36/49): loss=11002518534.416227\n",
      "Gradient Descent(37/49): loss=23154088759.7037\n",
      "Gradient Descent(38/49): loss=48726282497.98387\n",
      "Gradient Descent(39/49): loss=102541310553.14693\n",
      "Gradient Descent(40/49): loss=215791557059.87448\n",
      "Gradient Descent(41/49): loss=454119377323.8995\n",
      "Gradient Descent(42/49): loss=955664863217.6329\n",
      "Gradient Descent(43/49): loss=2011134905035.299\n",
      "Gradient Descent(44/49): loss=4232303354372.3345\n",
      "Gradient Descent(45/49): loss=8906608720570.984\n",
      "Gradient Descent(46/49): loss=18743382092259.777\n",
      "Gradient Descent(47/49): loss=39444235542203.914\n",
      "Gradient Descent(48/49): loss=83007842973619.94\n",
      "Gradient Descent(49/49): loss=174684637702279.2\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4823503048820219\n",
      "Gradient Descent(2/49): loss=0.5385778193190489\n",
      "Gradient Descent(3/49): loss=0.7003439740699436\n",
      "Gradient Descent(4/49): loss=1.0840087098430773\n",
      "Gradient Descent(5/49): loss=1.966082714889249\n",
      "Gradient Descent(6/49): loss=3.981308284813555\n",
      "Gradient Descent(7/49): loss=8.57856600499637\n",
      "Gradient Descent(8/49): loss=19.061870388667796\n",
      "Gradient Descent(9/49): loss=42.96433416631768\n",
      "Gradient Descent(10/49): loss=97.46079180311294\n",
      "Gradient Descent(11/49): loss=221.70809843738013\n",
      "Gradient Descent(12/49): loss=504.9797302968297\n",
      "Gradient Descent(13/49): loss=1150.8096780676149\n",
      "Gradient Descent(14/49): loss=2623.2336625641683\n",
      "Gradient Descent(15/49): loss=5980.20344990107\n",
      "Gradient Descent(16/49): loss=13633.735788675831\n",
      "Gradient Descent(17/49): loss=31082.97058850616\n",
      "Gradient Descent(18/49): loss=70865.35798643062\n",
      "Gradient Descent(19/49): loss=161564.94175353114\n",
      "Gradient Descent(20/49): loss=368350.28082804044\n",
      "Gradient Descent(21/49): loss=839798.7111483578\n",
      "Gradient Descent(22/49): loss=1914650.6485492175\n",
      "Gradient Descent(23/49): loss=4365197.967792251\n",
      "Gradient Descent(24/49): loss=9952183.443993876\n",
      "Gradient Descent(25/49): loss=22689912.05877495\n",
      "Gradient Descent(26/49): loss=51730569.3063472\n",
      "Gradient Descent(27/49): loss=117940158.06988509\n",
      "Gradient Descent(28/49): loss=268890930.5284971\n",
      "Gradient Descent(29/49): loss=613042527.4702826\n",
      "Gradient Descent(30/49): loss=1397671315.706602\n",
      "Gradient Descent(31/49): loss=3186540932.4680905\n",
      "Gradient Descent(32/49): loss=7264972101.129654\n",
      "Gradient Descent(33/49): loss=16563358434.965866\n",
      "Gradient Descent(34/49): loss=37762683576.85517\n",
      "Gradient Descent(35/49): loss=86094874812.6782\n",
      "Gradient Descent(36/49): loss=196287095273.5218\n",
      "Gradient Descent(37/49): loss=447513558208.87933\n",
      "Gradient Descent(38/49): loss=1020282991614.0127\n",
      "Gradient Descent(39/49): loss=2326135965899.0024\n",
      "Gradient Descent(40/49): loss=5303340912593.129\n",
      "Gradient Descent(41/49): loss=12091049382969.117\n",
      "Gradient Descent(42/49): loss=27566297847128.125\n",
      "Gradient Descent(43/49): loss=62848207209123.27\n",
      "Gradient Descent(44/49): loss=143287182461188.28\n",
      "Gradient Descent(45/49): loss=326679432387779.0\n",
      "Gradient Descent(46/49): loss=744794124025075.9\n",
      "Gradient Descent(47/49): loss=1698050847975679.0\n",
      "Gradient Descent(48/49): loss=3871374100977498.5\n",
      "Gradient Descent(49/49): loss=8826318391811837.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4824436023810503\n",
      "Gradient Descent(2/49): loss=0.5443337666453386\n",
      "Gradient Descent(3/49): loss=0.7250439679584303\n",
      "Gradient Descent(4/49): loss=1.1646075377452765\n",
      "Gradient Descent(5/49): loss=2.203465942400025\n",
      "Gradient Descent(6/49): loss=4.644843616056564\n",
      "Gradient Descent(7/49): loss=10.374831716717164\n",
      "Gradient Descent(8/49): loss=23.81870078856173\n",
      "Gradient Descent(9/49): loss=55.357851891724586\n",
      "Gradient Descent(10/49): loss=129.34576364139465\n",
      "Gradient Descent(11/49): loss=302.9123950052644\n",
      "Gradient Descent(12/49): loss=710.0767493862187\n",
      "Gradient Descent(13/49): loss=1665.2288514503307\n",
      "Gradient Descent(14/49): loss=3905.8841240203506\n",
      "Gradient Descent(15/49): loss=9162.151499935122\n",
      "Gradient Descent(16/49): loss=21492.626652722825\n",
      "Gradient Descent(17/49): loss=50418.21228405001\n",
      "Gradient Descent(18/49): loss=118273.62598866488\n",
      "Gradient Descent(19/49): loss=277453.0183590078\n",
      "Gradient Descent(20/49): loss=650865.8018125147\n",
      "Gradient Descent(21/49): loss=1526840.4154884086\n",
      "Gradient Descent(22/49): loss=3581755.3985107727\n",
      "Gradient Descent(23/49): loss=8402301.018241046\n",
      "Gradient Descent(24/49): loss=19710632.634303376\n",
      "Gradient Descent(25/49): loss=46238410.613033384\n",
      "Gradient Descent(26/49): loss=108468899.45758508\n",
      "Gradient Descent(27/49): loss=254452997.51986593\n",
      "Gradient Descent(28/49): loss=596911449.6825908\n",
      "Gradient Descent(29/49): loss=1400271493.7855315\n",
      "Gradient Descent(30/49): loss=3284842764.7970915\n",
      "Gradient Descent(31/49): loss=7705785655.338526\n",
      "Gradient Descent(32/49): loss=18076704676.584766\n",
      "Gradient Descent(33/49): loss=42405442687.51063\n",
      "Gradient Descent(34/49): loss=99477288682.22972\n",
      "Gradient Descent(35/49): loss=233359925906.6561\n",
      "Gradient Descent(36/49): loss=547430028910.2631\n",
      "Gradient Descent(37/49): loss=1284194942162.9575\n",
      "Gradient Descent(38/49): loss=3012543270160.3\n",
      "Gradient Descent(39/49): loss=7067008797981.536\n",
      "Gradient Descent(40/49): loss=16578222741375.947\n",
      "Gradient Descent(41/49): loss=38890211844816.27\n",
      "Gradient Descent(42/49): loss=91231044541337.03\n",
      "Gradient Descent(43/49): loss=214015380561958.53\n",
      "Gradient Descent(44/49): loss=502050407811857.06\n",
      "Gradient Descent(45/49): loss=1177740643322978.5\n",
      "Gradient Descent(46/49): loss=2762816245843224.0\n",
      "Gradient Descent(47/49): loss=6481183825632741.0\n",
      "Gradient Descent(48/49): loss=1.5203958585679764e+16\n",
      "Gradient Descent(49/49): loss=3.5666378688541304e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4833253786264509\n",
      "Gradient Descent(2/49): loss=0.5449838619464319\n",
      "Gradient Descent(3/49): loss=0.7252072564069575\n",
      "Gradient Descent(4/49): loss=1.1648463275794279\n",
      "Gradient Descent(5/49): loss=2.2070197852156586\n",
      "Gradient Descent(6/49): loss=4.66359312098732\n",
      "Gradient Descent(7/49): loss=10.446664017909095\n",
      "Gradient Descent(8/49): loss=24.056058145420433\n",
      "Gradient Descent(9/49): loss=56.07996066838367\n",
      "Gradient Descent(10/49): loss=131.4319685357459\n",
      "Gradient Descent(11/49): loss=308.73254377771923\n",
      "Gradient Descent(12/49): loss=725.9125979889089\n",
      "Gradient Descent(13/49): loss=1707.5163504716284\n",
      "Gradient Descent(14/49): loss=4017.1793277151\n",
      "Gradient Descent(15/49): loss=9451.695843915362\n",
      "Gradient Descent(16/49): loss=22238.828593930913\n",
      "Gradient Descent(17/49): loss=52326.28123690833\n",
      "Gradient Descent(18/49): loss=123120.47820445533\n",
      "Gradient Descent(19/49): loss=289695.5072788781\n",
      "Gradient Descent(20/49): loss=681637.8054522648\n",
      "Gradient Descent(21/49): loss=1603857.4552700778\n",
      "Gradient Descent(22/49): loss=3773791.872212456\n",
      "Gradient Descent(23/49): loss=8879533.62715068\n",
      "Gradient Descent(24/49): loss=20893075.90909264\n",
      "Gradient Descent(25/49): loss=49160310.149437666\n",
      "Gradient Descent(26/49): loss=115671628.1970974\n",
      "Gradient Descent(27/49): loss=272169267.5066985\n",
      "Gradient Descent(28/49): loss=640399996.1761218\n",
      "Gradient Descent(29/49): loss=1506827567.4451869\n",
      "Gradient Descent(30/49): loss=3545486152.4244843\n",
      "Gradient Descent(31/49): loss=8342342766.788622\n",
      "Gradient Descent(32/49): loss=19629094530.08219\n",
      "Gradient Descent(33/49): loss=46186228838.566154\n",
      "Gradient Descent(34/49): loss=108673771531.98866\n",
      "Gradient Descent(35/49): loss=255703678693.87622\n",
      "Gradient Descent(36/49): loss=601657330705.6914\n",
      "Gradient Descent(37/49): loss=1415668110216.917\n",
      "Gradient Descent(38/49): loss=3330992736238.912\n",
      "Gradient Descent(39/49): loss=7837651020603.446\n",
      "Gradient Descent(40/49): loss=18441581349747.582\n",
      "Gradient Descent(41/49): loss=43392072673985.87\n",
      "Gradient Descent(42/49): loss=102099268779370.5\n",
      "Gradient Descent(43/49): loss=240234218899884.88\n",
      "Gradient Descent(44/49): loss=565258504007022.9\n",
      "Gradient Descent(45/49): loss=1330023582050217.2\n",
      "Gradient Descent(46/49): loss=3129475658074672.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=7363491915974245.0\n",
      "Gradient Descent(48/49): loss=1.7325909871424348e+16\n",
      "Gradient Descent(49/49): loss=4.076695626181039e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48198640817282096\n",
      "Gradient Descent(2/49): loss=0.5415646533570154\n",
      "Gradient Descent(3/49): loss=0.7146795304450113\n",
      "Gradient Descent(4/49): loss=1.1315132099932754\n",
      "Gradient Descent(5/49): loss=2.1056516353473476\n",
      "Gradient Descent(6/49): loss=4.3687404710048074\n",
      "Gradient Descent(7/49): loss=9.619098092955115\n",
      "Gradient Descent(8/49): loss=21.79546077048402\n",
      "Gradient Descent(9/49): loss=50.03111710714546\n",
      "Gradient Descent(10/49): loss=115.50406968029151\n",
      "Gradient Descent(11/49): loss=267.3209780468964\n",
      "Gradient Descent(12/49): loss=619.3481621227879\n",
      "Gradient Descent(13/49): loss=1435.6136680563138\n",
      "Gradient Descent(14/49): loss=3328.3336847637074\n",
      "Gradient Descent(15/49): loss=7717.087155198663\n",
      "Gradient Descent(16/49): loss=17893.52888902938\n",
      "Gradient Descent(17/49): loss=41490.19768724087\n",
      "Gradient Descent(18/49): loss=96205.07583327271\n",
      "Gradient Descent(19/49): loss=223075.43611218093\n",
      "Gradient Descent(20/49): loss=517256.6318569948\n",
      "Gradient Descent(21/49): loss=1199390.5491649932\n",
      "Gradient Descent(22/49): loss=2781091.51339605\n",
      "Gradient Descent(23/49): loss=6448667.448860531\n",
      "Gradient Descent(24/49): loss=14952874.97342426\n",
      "Gradient Descent(25/49): loss=34672042.8630036\n",
      "Gradient Descent(26/49): loss=80395948.48628396\n",
      "Gradient Descent(27/49): loss=186418451.92792282\n",
      "Gradient Descent(28/49): loss=432258589.2212975\n",
      "Gradient Descent(29/49): loss=1002301468.256516\n",
      "Gradient Descent(30/49): loss=2324090853.4898357\n",
      "Gradient Descent(31/49): loss=5388995693.406913\n",
      "Gradient Descent(32/49): loss=12495757014.605438\n",
      "Gradient Descent(33/49): loss=28974590490.5081\n",
      "Gradient Descent(34/49): loss=67184956711.26856\n",
      "Gradient Descent(35/49): loss=155785408245.6373\n",
      "Gradient Descent(36/49): loss=361228087510.7428\n",
      "Gradient Descent(37/49): loss=837599186446.4247\n",
      "Gradient Descent(38/49): loss=1942186727423.5356\n",
      "Gradient Descent(39/49): loss=4503453853846.27\n",
      "Gradient Descent(40/49): loss=10442403053916.816\n",
      "Gradient Descent(41/49): loss=24213367135387.266\n",
      "Gradient Descent(42/49): loss=56144849514609.19\n",
      "Gradient Descent(43/49): loss=130186112050943.39\n",
      "Gradient Descent(44/49): loss=301869609010718.3\n",
      "Gradient Descent(45/49): loss=699961458320728.0\n",
      "Gradient Descent(46/49): loss=1623038651489707.2\n",
      "Gradient Descent(47/49): loss=3763427875799547.5\n",
      "Gradient Descent(48/49): loss=8726464624453222.0\n",
      "Gradient Descent(49/49): loss=2.023452749859188e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48743624815060244\n",
      "Gradient Descent(2/49): loss=0.5650931531734597\n",
      "Gradient Descent(3/49): loss=0.797194427877105\n",
      "Gradient Descent(4/49): loss=1.3943311247261927\n",
      "Gradient Descent(5/49): loss=2.8962621200796344\n",
      "Gradient Descent(6/49): loss=6.658139736885685\n",
      "Gradient Descent(7/49): loss=16.071937050873032\n",
      "Gradient Descent(8/49): loss=39.623790103310746\n",
      "Gradient Descent(9/49): loss=98.5429266076033\n",
      "Gradient Descent(10/49): loss=245.93647076539006\n",
      "Gradient Descent(11/49): loss=614.6571192929661\n",
      "Gradient Descent(12/49): loss=1537.0488177290772\n",
      "Gradient Descent(13/49): loss=3844.502198948459\n",
      "Gradient Descent(14/49): loss=9616.821566970802\n",
      "Gradient Descent(15/49): loss=24056.839073426574\n",
      "Gradient Descent(16/49): loss=60179.94385523325\n",
      "Gradient Descent(17/49): loss=150545.39389768374\n",
      "Gradient Descent(18/49): loss=376603.33019755274\n",
      "Gradient Descent(19/49): loss=942109.1783518413\n",
      "Gradient Descent(20/49): loss=2356776.8928252948\n",
      "Gradient Descent(21/49): loss=5895705.35558667\n",
      "Gradient Descent(22/49): loss=14748678.060988944\n",
      "Gradient Descent(23/49): loss=36895247.720368624\n",
      "Gradient Descent(24/49): loss=92297039.18559775\n",
      "Gradient Descent(25/49): loss=230889992.62043518\n",
      "Gradient Descent(26/49): loss=577593704.4275962\n",
      "Gradient Descent(27/49): loss=1444906657.9487925\n",
      "Gradient Descent(28/49): loss=3614574110.9580965\n",
      "Gradient Descent(29/49): loss=9042207628.40497\n",
      "Gradient Descent(30/49): loss=22619959167.633663\n",
      "Gradient Descent(31/49): loss=56586021221.828545\n",
      "Gradient Descent(32/49): loss=141555418999.96365\n",
      "Gradient Descent(33/49): loss=354114606675.5726\n",
      "Gradient Descent(34/49): loss=885852025637.7317\n",
      "Gradient Descent(35/49): loss=2216044739565.661\n",
      "Gradient Descent(36/49): loss=5543650796781.839\n",
      "Gradient Descent(37/49): loss=13867980013203.715\n",
      "Gradient Descent(38/49): loss=34692096724105.855\n",
      "Gradient Descent(39/49): loss=86785643905517.28\n",
      "Gradient Descent(40/49): loss=217102703477181.56\n",
      "Gradient Descent(41/49): loss=543103464305859.9\n",
      "Gradient Descent(42/49): loss=1358625978473951.0\n",
      "Gradient Descent(43/49): loss=3398734625534964.0\n",
      "Gradient Descent(44/49): loss=8502264227116540.0\n",
      "Gradient Descent(45/49): loss=2.126923839378221e+16\n",
      "Gradient Descent(46/49): loss=5.320706223275768e+16\n",
      "Gradient Descent(47/49): loss=1.331026254455903e+17\n",
      "Gradient Descent(48/49): loss=3.3296912396718925e+17\n",
      "Gradient Descent(49/49): loss=8.329545502526397e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48760923039225373\n",
      "Gradient Descent(2/49): loss=0.5721958607359966\n",
      "Gradient Descent(3/49): loss=0.8297704768990735\n",
      "Gradient Descent(4/49): loss=1.5097553878974133\n",
      "Gradient Descent(5/49): loss=3.267601514473847\n",
      "Gradient Descent(6/49): loss=7.794759419994469\n",
      "Gradient Descent(7/49): loss=19.444799123274986\n",
      "Gradient Descent(8/49): loss=49.418818346411214\n",
      "Gradient Descent(9/49): loss=126.53382487431084\n",
      "Gradient Descent(10/49): loss=324.92644601245223\n",
      "Gradient Descent(11/49): loss=835.3253669322689\n",
      "Gradient Descent(12/49): loss=2148.411387504396\n",
      "Gradient Descent(13/49): loss=5526.541194639232\n",
      "Gradient Descent(14/49): loss=14217.333973541314\n",
      "Gradient Descent(15/49): loss=36575.82157552471\n",
      "Gradient Descent(16/49): loss=94096.69077490717\n",
      "Gradient Descent(17/49): loss=242078.5409744099\n",
      "Gradient Descent(18/49): loss=622786.0689157941\n",
      "Gradient Descent(19/49): loss=1602218.488987107\n",
      "Gradient Descent(20/49): loss=4121968.676994227\n",
      "Gradient Descent(21/49): loss=10604438.400237178\n",
      "Gradient Descent(22/49): loss=27281652.637861934\n",
      "Gradient Descent(23/49): loss=70186515.53688292\n",
      "Gradient Descent(24/49): loss=180566296.84468827\n",
      "Gradient Descent(25/49): loss=464536348.21642435\n",
      "Gradient Descent(26/49): loss=1195095777.856294\n",
      "Gradient Descent(27/49): loss=3074579468.680119\n",
      "Gradient Descent(28/49): loss=7909858846.038591\n",
      "Gradient Descent(29/49): loss=20349406351.161842\n",
      "Gradient Descent(30/49): loss=52352178074.50972\n",
      "Gradient Descent(31/49): loss=134684545675.09099\n",
      "Gradient Descent(32/49): loss=346498035247.6794\n",
      "Gradient Descent(33/49): loss=891422901037.6512\n",
      "Gradient Descent(34/49): loss=2293331296746.5684\n",
      "Gradient Descent(35/49): loss=5899970070901.343\n",
      "Gradient Descent(36/49): loss=15178638553843.54\n",
      "Gradient Descent(37/49): loss=39049531706019.93\n",
      "Gradient Descent(38/49): loss=100461310877802.2\n",
      "Gradient Descent(39/49): loss=258453163005039.38\n",
      "Gradient Descent(40/49): loss=664913058406743.2\n",
      "Gradient Descent(41/49): loss=1710597657615766.5\n",
      "Gradient Descent(42/49): loss=4400792418263150.5\n",
      "Gradient Descent(43/49): loss=1.132175869785555e+16\n",
      "Gradient Descent(44/49): loss=2.912707708741599e+16\n",
      "Gradient Descent(45/49): loss=7.493417253425146e+16\n",
      "Gradient Descent(46/49): loss=1.9278042202933488e+17\n",
      "Gradient Descent(47/49): loss=4.95959184720704e+17\n",
      "Gradient Descent(48/49): loss=1.2759361677888448e+18\n",
      "Gradient Descent(49/49): loss=3.2825546021261553e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48848496490890214\n",
      "Gradient Descent(2/49): loss=0.5726407478193445\n",
      "Gradient Descent(3/49): loss=0.8293523141722835\n",
      "Gradient Descent(4/49): loss=1.5090506311082028\n",
      "Gradient Descent(5/49): loss=3.271398859039515\n",
      "Gradient Descent(6/49): loss=7.823641118681561\n",
      "Gradient Descent(7/49): loss=19.572996635688018\n",
      "Gradient Descent(8/49): loss=49.892225624413065\n",
      "Gradient Descent(9/49): loss=128.1267749272163\n",
      "Gradient Descent(10/49): loss=329.99675668939125\n",
      "Gradient Descent(11/49): loss=850.8825827926769\n",
      "Gradient Descent(12/49): loss=2194.923667477022\n",
      "Gradient Descent(13/49): loss=5662.949143789921\n",
      "Gradient Descent(14/49): loss=14611.48373198869\n",
      "Gradient Descent(15/49): loss=37701.35602486427\n",
      "Gradient Descent(16/49): loss=97280.07068040405\n",
      "Gradient Descent(17/49): loss=251010.8130272634\n",
      "Gradient Descent(18/49): loss=647681.6913002665\n",
      "Gradient Descent(19/49): loss=1671210.1222186661\n",
      "Gradient Descent(20/49): loss=4312216.825455258\n",
      "Gradient Descent(21/49): loss=11126796.855582815\n",
      "Gradient Descent(22/49): loss=28710433.022771966\n",
      "Gradient Descent(23/49): loss=74081425.7307632\n",
      "Gradient Descent(24/49): loss=191152033.864573\n",
      "Gradient Descent(25/49): loss=493228899.9583047\n",
      "Gradient Descent(26/49): loss=1272676743.3029375\n",
      "Gradient Descent(27/49): loss=3283883190.0287604\n",
      "Gradient Descent(28/49): loss=8473391899.160799\n",
      "Gradient Descent(29/49): loss=21863862422.959408\n",
      "Gradient Descent(30/49): loss=56415245010.13844\n",
      "Gradient Descent(31/49): loss=145568052341.58432\n",
      "Gradient Descent(32/49): loss=375608718153.5276\n",
      "Gradient Descent(33/49): loss=969181814853.2657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(34/49): loss=2500776326120.0312\n",
      "Gradient Descent(35/49): loss=6452744095523.615\n",
      "Gradient Descent(36/49): loss=16649992215384.842\n",
      "Gradient Descent(37/49): loss=42961914600750.586\n",
      "Gradient Descent(38/49): loss=110854472619916.86\n",
      "Gradient Descent(39/49): loss=286037394144096.56\n",
      "Gradient Descent(40/49): loss=738061251973747.0\n",
      "Gradient Descent(41/49): loss=1904416774929210.8\n",
      "Gradient Descent(42/49): loss=4913959705827773.0\n",
      "Gradient Descent(43/49): loss=1.267947242871586e+16\n",
      "Gradient Descent(44/49): loss=3.2716796777942732e+16\n",
      "Gradient Descent(45/49): loss=8.441903221344086e+16\n",
      "Gradient Descent(46/49): loss=2.178261230224891e+17\n",
      "Gradient Descent(47/49): loss=5.620559561858355e+17\n",
      "Gradient Descent(48/49): loss=1.4502709477658066e+18\n",
      "Gradient Descent(49/49): loss=3.742128873086973e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4871404661629554\n",
      "Gradient Descent(2/49): loss=0.568932229797019\n",
      "Gradient Descent(3/49): loss=0.8163459140784357\n",
      "Gradient Descent(4/49): loss=1.4627469736753893\n",
      "Gradient Descent(5/49): loss=3.115225801275342\n",
      "Gradient Descent(6/49): loss=7.323001979431093\n",
      "Gradient Descent(7/49): loss=18.028469792866023\n",
      "Gradient Descent(8/49): loss=45.259785639705484\n",
      "Gradient Descent(9/49): loss=114.52355220741636\n",
      "Gradient Descent(10/49): loss=290.69505116123406\n",
      "Gradient Descent(11/49): loss=738.7822983403942\n",
      "Gradient Descent(12/49): loss=1878.4772695216254\n",
      "Gradient Descent(13/49): loss=4777.251382633264\n",
      "Gradient Descent(14/49): loss=12150.17967431302\n",
      "Gradient Descent(15/49): loss=30902.957503652025\n",
      "Gradient Descent(16/49): loss=78599.97173888139\n",
      "Gradient Descent(17/49): loss=199915.606329131\n",
      "Gradient Descent(18/49): loss=508477.5441471139\n",
      "Gradient Descent(19/49): loss=1293293.6947150992\n",
      "Gradient Descent(20/49): loss=3289445.2429453568\n",
      "Gradient Descent(21/49): loss=8366584.64679961\n",
      "Gradient Descent(22/49): loss=21280105.440084577\n",
      "Gradient Descent(23/49): loss=54125179.40997133\n",
      "Gradient Descent(24/49): loss=137665439.37067884\n",
      "Gradient Descent(25/49): loss=350147074.8415542\n",
      "Gradient Descent(26/49): loss=890586444.2374392\n",
      "Gradient Descent(27/49): loss=2265174470.8480906\n",
      "Gradient Descent(28/49): loss=5761389494.955694\n",
      "Gradient Descent(29/49): loss=14653886198.084188\n",
      "Gradient Descent(30/49): loss=37271630550.15962\n",
      "Gradient Descent(31/49): loss=94799046826.3841\n",
      "Gradient Descent(32/49): loss=241117953429.21585\n",
      "Gradient Descent(33/49): loss=613274810372.857\n",
      "Gradient Descent(34/49): loss=1559842341430.983\n",
      "Gradient Descent(35/49): loss=3967402686313.1436\n",
      "Gradient Descent(36/49): loss=10090945512433.117\n",
      "Gradient Descent(37/49): loss=25665955635455.03\n",
      "Gradient Descent(38/49): loss=65280431637403.12\n",
      "Gradient Descent(39/49): loss=166038421296061.16\n",
      "Gradient Descent(40/49): loss=422312730706464.6\n",
      "Gradient Descent(41/49): loss=1074137185385187.6\n",
      "Gradient Descent(42/49): loss=2732029155496064.0\n",
      "Gradient Descent(43/49): loss=6948817532840466.0\n",
      "Gradient Descent(44/49): loss=1.7674066547779362e+16\n",
      "Gradient Descent(45/49): loss=4.495335024398666e+16\n",
      "Gradient Descent(46/49): loss=1.1433722356400538e+17\n",
      "Gradient Descent(47/49): loss=2.908126006486941e+17\n",
      "Gradient Descent(48/49): loss=7.396713516374115e+17\n",
      "Gradient Descent(49/49): loss=1.8813273813194703e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4928327592120691\n",
      "Gradient Descent(2/49): loss=0.5956355476780427\n",
      "Gradient Descent(3/49): loss=0.9188654318908744\n",
      "Gradient Descent(4/49): loss=1.8206403830130515\n",
      "Gradient Descent(5/49): loss=4.294666760793353\n",
      "Gradient Descent(6/49): loss=11.062777332956747\n",
      "Gradient Descent(7/49): loss=29.567451343048347\n",
      "Gradient Descent(8/49): loss=80.15419906849414\n",
      "Gradient Descent(9/49): loss=218.4395883862444\n",
      "Gradient Descent(10/49): loss=596.4564715146113\n",
      "Gradient Descent(11/49): loss=1629.7998563755264\n",
      "Gradient Descent(12/49): loss=4454.53462186523\n",
      "Gradient Descent(13/49): loss=12176.192221463756\n",
      "Gradient Descent(14/49): loss=33284.011006866254\n",
      "Gradient Descent(15/49): loss=90984.05690871771\n",
      "Gradient Descent(16/49): loss=248712.1145506346\n",
      "Gradient Descent(17/49): loss=679875.3776470864\n",
      "Gradient Descent(18/49): loss=1858497.3805311145\n",
      "Gradient Descent(19/49): loss=5080362.376929654\n",
      "Gradient Descent(20/49): loss=13887608.489923473\n",
      "Gradient Descent(21/49): loss=37962976.07299053\n",
      "Gradient Descent(22/49): loss=103775071.79674767\n",
      "Gradient Descent(23/49): loss=283678117.039452\n",
      "Gradient Descent(24/49): loss=775458622.303946\n",
      "Gradient Descent(25/49): loss=2119783089.028736\n",
      "Gradient Descent(26/49): loss=5794610074.825166\n",
      "Gradient Descent(27/49): loss=15840066889.605753\n",
      "Gradient Descent(28/49): loss=43300190320.62724\n",
      "Gradient Descent(29/49): loss=118364808361.43617\n",
      "Gradient Descent(30/49): loss=323560422130.8537\n",
      "Gradient Descent(31/49): loss=884480346979.0421\n",
      "Gradient Descent(32/49): loss=2417803385965.8867\n",
      "Gradient Descent(33/49): loss=6609274285353.455\n",
      "Gradient Descent(34/49): loss=18067021839985.836\n",
      "Gradient Descent(35/49): loss=49387763931948.195\n",
      "Gradient Descent(36/49): loss=135005716371003.86\n",
      "Gradient Descent(37/49): loss=369049780791101.3\n",
      "Gradient Descent(38/49): loss=1008829435989804.0\n",
      "Gradient Descent(39/49): loss=2757722355878038.5\n",
      "Gradient Descent(40/49): loss=7538472134933183.0\n",
      "Gradient Descent(41/49): loss=2.060706437978976e+16\n",
      "Gradient Descent(42/49): loss=5.63311894972685e+16\n",
      "Gradient Descent(43/49): loss=1.5398616958217805e+17\n",
      "Gradient Descent(44/49): loss=4.20934488233047e+17\n",
      "Gradient Descent(45/49): loss=1.1506607630073972e+18\n",
      "Gradient Descent(46/49): loss=3.145430532628957e+18\n",
      "Gradient Descent(47/49): loss=8.598305907065082e+18\n",
      "Gradient Descent(48/49): loss=2.350421149173434e+19\n",
      "Gradient Descent(49/49): loss=6.425079123949759e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4930897050352209\n",
      "Gradient Descent(2/49): loss=0.6042595216159642\n",
      "Gradient Descent(3/49): loss=0.9611606109196434\n",
      "Gradient Descent(4/49): loss=1.983076185121145\n",
      "Gradient Descent(5/49): loss=4.863861729719844\n",
      "Gradient Descent(6/49): loss=12.963958214612644\n",
      "Gradient Descent(7/49): loss=35.72819997436655\n",
      "Gradient Descent(8/49): loss=99.69679890432643\n",
      "Gradient Descent(9/49): loss=279.4462601764689\n",
      "Gradient Descent(10/49): loss=784.531495897557\n",
      "Gradient Descent(11/49): loss=2203.7871826761975\n",
      "Gradient Descent(12/49): loss=6191.797475506802\n",
      "Gradient Descent(13/49): loss=17397.827736070343\n",
      "Gradient Descent(14/49): loss=48885.98729469862\n",
      "Gradient Descent(15/49): loss=137365.5063428554\n",
      "Gradient Descent(16/49): loss=385986.744902441\n",
      "Gradient Descent(17/49): loss=1084594.973933745\n",
      "Gradient Descent(18/49): loss=3047635.058948569\n",
      "Gradient Descent(19/49): loss=8563639.90150099\n",
      "Gradient Descent(20/49): loss=24063226.309751898\n",
      "Gradient Descent(21/49): loss=67615976.11286627\n",
      "Gradient Descent(22/49): loss=189996145.8435028\n",
      "Gradient Descent(23/49): loss=533875832.2229537\n",
      "Gradient Descent(24/49): loss=1500153612.0675645\n",
      "Gradient Descent(25/49): loss=4215326344.867118\n",
      "Gradient Descent(26/49): loss=11844771130.530275\n",
      "Gradient Descent(27/49): loss=33282975423.876602\n",
      "Gradient Descent(28/49): loss=93522824618.00067\n",
      "Gradient Descent(29/49): loss=262792572273.9217\n",
      "Gradient Descent(30/49): loss=738428681175.304\n",
      "Gradient Descent(31/49): loss=2074932759569.2332\n",
      "Gradient Descent(32/49): loss=5830415402990.489\n",
      "Gradient Descent(33/49): loss=16383058012197.336\n",
      "Gradient Descent(34/49): loss=46035242993726.164\n",
      "Gradient Descent(35/49): loss=129355801335357.95\n",
      "Gradient Descent(36/49): loss=363480721528787.7\n",
      "Gradient Descent(37/49): loss=1021355312705054.5\n",
      "Gradient Descent(38/49): loss=2869936733929988.0\n",
      "Gradient Descent(39/49): loss=8064320765068744.0\n",
      "Gradient Descent(40/49): loss=2.266017526904342e+16\n",
      "Gradient Descent(41/49): loss=6.367350185870057e+16\n",
      "Gradient Descent(42/49): loss=1.789180706156616e+17\n",
      "Gradient Descent(43/49): loss=5.027472191472769e+17\n",
      "Gradient Descent(44/49): loss=1.4126843951009805e+18\n",
      "Gradient Descent(45/49): loss=3.9695439858359813e+18\n",
      "Gradient Descent(46/49): loss=1.1154139955202255e+19\n",
      "Gradient Descent(47/49): loss=3.1342350301236834e+19\n",
      "Gradient Descent(48/49): loss=8.806980424763957e+19\n",
      "Gradient Descent(49/49): loss=2.4746996781257612e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4939576625266855\n",
      "Gradient Descent(2/49): loss=0.6044663130472138\n",
      "Gradient Descent(3/49): loss=0.9599978773783584\n",
      "Gradient Descent(4/49): loss=1.980995052279347\n",
      "Gradient Descent(5/49): loss=4.86765907389633\n",
      "Gradient Descent(6/49): loss=13.00801299152062\n",
      "Gradient Descent(7/49): loss=35.95216148419031\n",
      "Gradient Descent(8/49): loss=100.61442106656868\n",
      "Gradient Descent(9/49): loss=282.8431664213176\n",
      "Gradient Descent(10/49): loss=796.3890147581826\n",
      "Gradient Descent(11/49): loss=2243.628697951726\n",
      "Gradient Descent(12/49): loss=6322.137129807197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=17815.898105598626\n",
      "Gradient Descent(14/49): loss=50206.79136716894\n",
      "Gradient Descent(15/49): loss=141488.4826391744\n",
      "Gradient Descent(16/49): loss=398731.9933483837\n",
      "Gradient Descent(17/49): loss=1123677.1901173436\n",
      "Gradient Descent(18/49): loss=3166665.7050096514\n",
      "Gradient Descent(19/49): loss=8924069.284682008\n",
      "Gradient Descent(20/49): loss=25149170.764672834\n",
      "Gradient Descent(21/49): loss=70873587.0551578\n",
      "Gradient Descent(22/49): loss=199730854.7688143\n",
      "Gradient Descent(23/49): loss=562867156.7633319\n",
      "Gradient Descent(24/49): loss=1586231815.7065651\n",
      "Gradient Descent(25/49): loss=4470204636.439599\n",
      "Gradient Descent(26/49): loss=12597609817.027918\n",
      "Gradient Descent(27/49): loss=35501679680.16261\n",
      "Gradient Descent(28/49): loss=100048285225.10493\n",
      "Gradient Descent(29/49): loss=281948895565.11206\n",
      "Gradient Descent(30/49): loss=794568138091.0173\n",
      "Gradient Descent(31/49): loss=2239194889571.685\n",
      "Gradient Descent(32/49): loss=6310338299659.555\n",
      "Gradient Descent(33/49): loss=17783342415438.48\n",
      "Gradient Descent(34/49): loss=50115739037604.67\n",
      "Gradient Descent(35/49): loss=141232578252830.9\n",
      "Gradient Descent(36/49): loss=398011513807571.9\n",
      "Gradient Descent(37/49): loss=1121647477395809.4\n",
      "Gradient Descent(38/49): loss=3160946404572222.0\n",
      "Gradient Descent(39/49): loss=8907952252320815.0\n",
      "Gradient Descent(40/49): loss=2.5103751589982212e+16\n",
      "Gradient Descent(41/49): loss=7.0745590685821464e+16\n",
      "Gradient Descent(42/49): loss=1.9937014527673312e+17\n",
      "Gradient Descent(43/49): loss=5.618506318533279e+17\n",
      "Gradient Descent(44/49): loss=1.5833671188623168e+18\n",
      "Gradient Descent(45/49): loss=4.4621315541188787e+18\n",
      "Gradient Descent(46/49): loss=1.257485883663518e+19\n",
      "Gradient Descent(47/49): loss=3.543756450105978e+19\n",
      "Gradient Descent(48/49): loss=9.986760043048075e+19\n",
      "Gradient Descent(49/49): loss=2.814397026478455e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4926094537109978\n",
      "Gradient Descent(2/49): loss=0.6004398144654121\n",
      "Gradient Descent(3/49): loss=0.9439668337063951\n",
      "Gradient Descent(4/49): loss=1.9173101951038471\n",
      "Gradient Descent(5/49): loss=4.630998655373235\n",
      "Gradient Descent(6/49): loss=12.176398479133963\n",
      "Gradient Descent(7/49): loss=33.14527238297776\n",
      "Gradient Descent(8/49): loss=91.41126803826111\n",
      "Gradient Descent(9/49): loss=253.30921789065533\n",
      "Gradient Descent(10/49): loss=703.1548735922191\n",
      "Gradient Descent(11/49): loss=1953.0814483682439\n",
      "Gradient Descent(12/49): loss=5426.0839466823145\n",
      "Gradient Descent(13/49): loss=15076.045222281135\n",
      "Gradient Descent(14/49): loss=41889.082219552416\n",
      "Gradient Descent(15/49): loss=116390.82500940497\n",
      "Gradient Descent(16/49): loss=323398.6932096786\n",
      "Gradient Descent(17/49): loss=898583.3233657448\n",
      "Gradient Descent(18/49): loss=2496770.683750605\n",
      "Gradient Descent(19/49): loss=6937436.696377485\n",
      "Gradient Descent(20/49): loss=19276111.824561935\n",
      "Gradient Descent(21/49): loss=53559911.480088584\n",
      "Gradient Descent(22/49): loss=148819646.14343068\n",
      "Gradient Descent(23/49): loss=413504924.3003575\n",
      "Gradient Descent(24/49): loss=1148949933.889409\n",
      "Gradient Descent(25/49): loss=3192431029.232274\n",
      "Gradient Descent(26/49): loss=8870374223.593557\n",
      "Gradient Descent(27/49): loss=24646903300.307625\n",
      "Gradient Descent(28/49): loss=68483000490.49828\n",
      "Gradient Descent(29/49): loss=190284406080.02628\n",
      "Gradient Descent(30/49): loss=528717418015.77954\n",
      "Gradient Descent(31/49): loss=1469075232554.4546\n",
      "Gradient Descent(32/49): loss=4081919689737.046\n",
      "Gradient Descent(33/49): loss=11341875476650.885\n",
      "Gradient Descent(34/49): loss=31514127936247.44\n",
      "Gradient Descent(35/49): loss=87564024276824.73\n",
      "Gradient Descent(36/49): loss=243302253613475.66\n",
      "Gradient Descent(37/49): loss=676030905412185.9\n",
      "Gradient Descent(38/49): loss=1878395198913629.8\n",
      "Gradient Descent(39/49): loss=5219241450434100.0\n",
      "Gradient Descent(40/49): loss=1.450199688206397e+16\n",
      "Gradient Descent(41/49): loss=4.029472779993764e+16\n",
      "Gradient Descent(42/49): loss=1.1196148376498464e+17\n",
      "Gradient Descent(43/49): loss=3.110921584852177e+17\n",
      "Gradient Descent(44/49): loss=8.643895008942268e+17\n",
      "Gradient Descent(45/49): loss=2.401761628754375e+18\n",
      "Gradient Descent(46/49): loss=6.673448619388915e+18\n",
      "Gradient Descent(47/49): loss=1.8542604704164962e+19\n",
      "Gradient Descent(48/49): loss=5.1521815604599e+19\n",
      "Gradient Descent(49/49): loss=1.4315666679762992e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49853983806642194\n",
      "Gradient Descent(2/49): loss=0.63057747382239\n",
      "Gradient Descent(3/49): loss=1.0701658500887938\n",
      "Gradient Descent(4/49): loss=2.3982201463785833\n",
      "Gradient Descent(5/49): loss=6.360126471244746\n",
      "Gradient Descent(6/49): loss=18.155898023044802\n",
      "Gradient Descent(7/49): loss=53.26233836492285\n",
      "Gradient Descent(8/49): loss=157.73717628518077\n",
      "Gradient Descent(9/49): loss=468.6421955953071\n",
      "Gradient Descent(10/49): loss=1393.8543898001005\n",
      "Gradient Descent(11/49): loss=4147.159089705072\n",
      "Gradient Descent(12/49): loss=12340.612787687123\n",
      "Gradient Descent(13/49): loss=36723.193600760074\n",
      "Gradient Descent(14/49): loss=109282.36643376252\n",
      "Gradient Descent(15/49): loss=325208.3809372242\n",
      "Gradient Descent(16/49): loss=967774.1896289657\n",
      "Gradient Descent(17/49): loss=2879960.727209572\n",
      "Gradient Descent(18/49): loss=8570362.089740897\n",
      "Gradient Descent(19/49): loss=25504205.63851532\n",
      "Gradient Descent(20/49): loss=75896970.41146035\n",
      "Gradient Descent(21/49): loss=225858834.30395344\n",
      "Gradient Descent(22/49): loss=672124498.1026022\n",
      "Gradient Descent(23/49): loss=2000149087.2624984\n",
      "Gradient Descent(24/49): loss=5952165682.939021\n",
      "Gradient Descent(25/49): loss=17712817782.296097\n",
      "Gradient Descent(26/49): loss=52710883821.14389\n",
      "Gradient Descent(27/49): loss=156860263984.04465\n",
      "Gradient Descent(28/49): loss=466794343663.64856\n",
      "Gradient Descent(29/49): loss=1389115087162.9932\n",
      "Gradient Descent(30/49): loss=4133813426785.983\n",
      "Gradient Descent(31/49): loss=12301654200861.146\n",
      "Gradient Descent(32/49): loss=36608013099234.05\n",
      "Gradient Descent(33/49): loss=108940358848641.22\n",
      "Gradient Descent(34/49): loss=324191366351948.4\n",
      "Gradient Descent(35/49): loss=964748447021061.6\n",
      "Gradient Descent(36/49): loss=2870957288292248.5\n",
      "Gradient Descent(37/49): loss=8543569856628571.0\n",
      "Gradient Descent(38/49): loss=2.5424476425600616e+16\n",
      "Gradient Descent(39/49): loss=7.56597081036801e+16\n",
      "Gradient Descent(40/49): loss=2.251527754007198e+17\n",
      "Gradient Descent(41/49): loss=6.700233656886205e+17\n",
      "Gradient Descent(42/49): loss=1.993896409980805e+18\n",
      "Gradient Descent(43/49): loss=5.933558585152241e+18\n",
      "Gradient Descent(44/49): loss=1.7657445646222426e+19\n",
      "Gradient Descent(45/49): loss=5.2546104041087705e+19\n",
      "Gradient Descent(46/49): loss=1.5636990226202614e+20\n",
      "Gradient Descent(47/49): loss=4.653350953348666e+20\n",
      "Gradient Descent(48/49): loss=1.3847725669576978e+21\n",
      "Gradient Descent(49/49): loss=4.1208906902211244e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49888502630995163\n",
      "Gradient Descent(2/49): loss=0.6409109947062951\n",
      "Gradient Descent(3/49): loss=1.1243501476866729\n",
      "Gradient Descent(4/49): loss=2.623326887214197\n",
      "Gradient Descent(5/49): loss=7.216804140561115\n",
      "Gradient Descent(6/49): loss=21.267910165626663\n",
      "Gradient Descent(7/49): loss=64.23534081302884\n",
      "Gradient Descent(8/49): loss=195.61812109766623\n",
      "Gradient Descent(9/49): loss=597.3444244261791\n",
      "Gradient Descent(10/49): loss=1825.6888910206146\n",
      "Gradient Descent(11/49): loss=5581.550213918043\n",
      "Gradient Descent(12/49): loss=17065.69799645789\n",
      "Gradient Descent(13/49): loss=52180.31639930769\n",
      "Gradient Descent(14/49): loss=159548.87463003438\n",
      "Gradient Descent(15/49): loss=487845.3996632477\n",
      "Gradient Descent(16/49): loss=1491664.4865550746\n",
      "Gradient Descent(17/49): loss=4561001.752253127\n",
      "Gradient Descent(18/49): loss=13945990.911789162\n",
      "Gradient Descent(19/49): loss=42642094.954664245\n",
      "Gradient Descent(20/49): loss=130385018.59861447\n",
      "Gradient Descent(21/49): loss=398673028.63801396\n",
      "Gradient Descent(22/49): loss=1219006490.7741218\n",
      "Gradient Descent(23/49): loss=3727307137.5345554\n",
      "Gradient Descent(24/49): loss=11396837182.249489\n",
      "Gradient Descent(25/49): loss=34847650856.73976\n",
      "Gradient Descent(26/49): loss=106552261020.51637\n",
      "Gradient Descent(27/49): loss=325800564730.00116\n",
      "Gradient Descent(28/49): loss=996187288396.6166\n",
      "Gradient Descent(29/49): loss=3046001821347.158\n",
      "Gradient Descent(30/49): loss=9313637308688.371\n",
      "Gradient Descent(31/49): loss=28477934356399.383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=87075834964161.14\n",
      "Gradient Descent(33/49): loss=266248279802008.22\n",
      "Gradient Descent(34/49): loss=814096660993319.9\n",
      "Gradient Descent(35/49): loss=2489230630647921.0\n",
      "Gradient Descent(36/49): loss=7611220423131894.0\n",
      "Gradient Descent(37/49): loss=2.3272522688836024e+16\n",
      "Gradient Descent(38/49): loss=7.115945698489226e+16\n",
      "Gradient Descent(39/49): loss=2.1758141075156614e+17\n",
      "Gradient Descent(40/49): loss=6.652899320844003e+17\n",
      "Gradient Descent(41/49): loss=2.034230278239322e+18\n",
      "Gradient Descent(42/49): loss=6.21998413825486e+18\n",
      "Gradient Descent(43/49): loss=1.9018595433368433e+19\n",
      "Gradient Descent(44/49): loss=5.815239463932588e+19\n",
      "Gradient Descent(45/49): loss=1.7781023914913468e+20\n",
      "Gradient Descent(46/49): loss=5.436832196225959e+20\n",
      "Gradient Descent(47/49): loss=1.662398322580694e+21\n",
      "Gradient Descent(48/49): loss=5.083048516445735e+21\n",
      "Gradient Descent(49/49): loss=1.554223309154434e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4997434714798013\n",
      "Gradient Descent(2/49): loss=0.6408440525985811\n",
      "Gradient Descent(3/49): loss=1.122246262040595\n",
      "Gradient Descent(4/49): loss=2.6192825730376845\n",
      "Gradient Descent(5/49): loss=7.220108969691688\n",
      "Gradient Descent(6/49): loss=21.334200349972576\n",
      "Gradient Descent(7/49): loss=64.61825241992781\n",
      "Gradient Descent(8/49): loss=197.34936636385277\n",
      "Gradient Descent(9/49): loss=604.3642452200103\n",
      "Gradient Descent(10/49): loss=1852.4542910495877\n",
      "Gradient Descent(11/49): loss=5679.653132127892\n",
      "Gradient Descent(12/49): loss=17415.54189720491\n",
      "Gradient Descent(13/49): loss=53402.97743254653\n",
      "Gradient Descent(14/49): loss=163756.398940986\n",
      "Gradient Descent(15/49): loss=502148.87835012464\n",
      "Gradient Descent(16/49): loss=1539810.13326203\n",
      "Gradient Descent(17/49): loss=4721739.238401388\n",
      "Gradient Descent(18/49): loss=14478943.509192342\n",
      "Gradient Descent(19/49): loss=44398854.365301214\n",
      "Gradient Descent(20/49): loss=136146555.93813735\n",
      "Gradient Descent(21/49): loss=417485654.31769526\n",
      "Gradient Descent(22/49): loss=1280195966.6197958\n",
      "Gradient Descent(23/49): loss=3925647975.3837376\n",
      "Gradient Descent(24/49): loss=12037775801.92772\n",
      "Gradient Descent(25/49): loss=36913153490.19406\n",
      "Gradient Descent(26/49): loss=113192081579.70914\n",
      "Gradient Descent(27/49): loss=347097067602.4894\n",
      "Gradient Descent(28/49): loss=1064353377524.8281\n",
      "Gradient Descent(29/49): loss=3263778977085.5923\n",
      "Gradient Descent(30/49): loss=10008192237844.562\n",
      "Gradient Descent(31/49): loss=30689551153092.965\n",
      "Gradient Descent(32/49): loss=94107759682798.58\n",
      "Gradient Descent(33/49): loss=288576082078764.94\n",
      "Gradient Descent(34/49): loss=884902110395810.9\n",
      "Gradient Descent(35/49): loss=2713501892957424.0\n",
      "Gradient Descent(36/49): loss=8320798918413736.0\n",
      "Gradient Descent(37/49): loss=2.551525569979083e+16\n",
      "Gradient Descent(38/49): loss=7.82410775466519e+16\n",
      "Gradient Descent(39/49): loss=2.399218055146266e+17\n",
      "Gradient Descent(40/49): loss=7.357065440091446e+17\n",
      "Gradient Descent(41/49): loss=2.2560021909508434e+18\n",
      "Gradient Descent(42/49): loss=6.917902154084613e+18\n",
      "Gradient Descent(43/49): loss=2.121335271989154e+19\n",
      "Gradient Descent(44/49): loss=6.504953721451899e+19\n",
      "Gradient Descent(45/49): loss=1.9947069884220998e+20\n",
      "Gradient Descent(46/49): loss=6.116655306153226e+20\n",
      "Gradient Descent(47/49): loss=1.8756374921956845e+21\n",
      "Gradient Descent(48/49): loss=5.751535481476431e+21\n",
      "Gradient Descent(49/49): loss=1.7636755786939182e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.498393370816948\n",
      "Gradient Descent(2/49): loss=0.6364690584880659\n",
      "Gradient Descent(3/49): loss=1.102553526315277\n",
      "Gradient Descent(4/49): loss=2.532586651242215\n",
      "Gradient Descent(5/49): loss=6.867129936623804\n",
      "Gradient Descent(6/49): loss=19.98080268316264\n",
      "Gradient Descent(7/49): loss=59.64115993338814\n",
      "Gradient Descent(8/49): loss=179.57921104150753\n",
      "Gradient Descent(9/49): loss=542.2808751330485\n",
      "Gradient Descent(10/49): loss=1639.1126706088587\n",
      "Gradient Descent(11/49): loss=4955.994010771891\n",
      "Gradient Descent(12/49): loss=14986.426814194967\n",
      "Gradient Descent(13/49): loss=45319.007559184116\n",
      "Gradient Descent(14/49): loss=137046.3978794027\n",
      "Gradient Descent(15/49): loss=414435.0620892307\n",
      "Gradient Descent(16/49): loss=1253273.6090893976\n",
      "Gradient Descent(17/49): loss=3789967.418674701\n",
      "Gradient Descent(18/49): loss=11461068.734917961\n",
      "Gradient Descent(19/49): loss=34658900.17102655\n",
      "Gradient Descent(20/49): loss=104810415.72811343\n",
      "Gradient Descent(21/49): loss=316952449.2832059\n",
      "Gradient Descent(22/49): loss=958481602.907551\n",
      "Gradient Descent(23/49): loss=2898500976.0265117\n",
      "Gradient Descent(24/49): loss=8765226045.048183\n",
      "Gradient Descent(25/49): loss=26506524669.37644\n",
      "Gradient Descent(26/49): loss=80157185502.21451\n",
      "Gradient Descent(27/49): loss=242399728663.80154\n",
      "Gradient Descent(28/49): loss=733030084430.6012\n",
      "Gradient Descent(29/49): loss=2216723210222.589\n",
      "Gradient Descent(30/49): loss=6703492660274.171\n",
      "Gradient Descent(31/49): loss=20271729749173.61\n",
      "Gradient Descent(32/49): loss=61302823445880.14\n",
      "Gradient Descent(33/49): loss=185383102918980.53\n",
      "Gradient Descent(34/49): loss=560608678623251.8\n",
      "Gradient Descent(35/49): loss=1695311415113494.5\n",
      "Gradient Descent(36/49): loss=5126714772365502.0\n",
      "Gradient Descent(37/49): loss=1.550346686920115e+16\n",
      "Gradient Descent(38/49): loss=4.688333477415499e+16\n",
      "Gradient Descent(39/49): loss=1.4177777771190506e+17\n",
      "Gradient Descent(40/49): loss=4.287437817671455e+17\n",
      "Gradient Descent(41/49): loss=1.2965447291572193e+18\n",
      "Gradient Descent(42/49): loss=3.9208224263374945e+18\n",
      "Gradient Descent(43/49): loss=1.1856782225217657e+19\n",
      "Gradient Descent(44/49): loss=3.5855560249781125e+19\n",
      "Gradient Descent(45/49): loss=1.0842918225244671e+20\n",
      "Gradient Descent(46/49): loss=3.2789579864411655e+20\n",
      "Gradient Descent(47/49): loss=9.915748928009368e+20\n",
      "Gradient Descent(48/49): loss=2.9985769018661305e+21\n",
      "Gradient Descent(49/49): loss=9.067861138563599e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5045574847136609\n",
      "Gradient Descent(2/49): loss=0.6703082895465055\n",
      "Gradient Descent(3/49): loss=1.2565738450194528\n",
      "Gradient Descent(4/49): loss=3.1708857150607397\n",
      "Gradient Descent(5/49): loss=9.361792718495012\n",
      "Gradient Descent(6/49): loss=29.354942359067646\n",
      "Gradient Descent(7/49): loss=93.90557167346691\n",
      "Gradient Descent(8/49): loss=302.30550102074227\n",
      "Gradient Descent(9/49): loss=975.1109393639739\n",
      "Gradient Descent(10/49): loss=3147.2124757374672\n",
      "Gradient Descent(11/49): loss=10159.67272581361\n",
      "Gradient Descent(12/49): loss=32798.84847907703\n",
      "Gradient Descent(13/49): loss=105887.64061168837\n",
      "Gradient Descent(14/49): loss=341849.0338285094\n",
      "Gradient Descent(15/49): loss=1103631.7535212312\n",
      "Gradient Descent(16/49): loss=3562987.0888318564\n",
      "Gradient Descent(17/49): loss=11502821.515344307\n",
      "Gradient Descent(18/49): loss=37135949.76223873\n",
      "Gradient Descent(19/49): loss=119890479.44249262\n",
      "Gradient Descent(20/49): loss=387056941.3971107\n",
      "Gradient Descent(21/49): loss=1249582759.246559\n",
      "Gradient Descent(22/49): loss=4034179227.731563\n",
      "Gradient Descent(23/49): loss=13024028959.570293\n",
      "Gradient Descent(24/49): loss=42047048673.80833\n",
      "Gradient Descent(25/49): loss=135745575174.17796\n",
      "Gradient Descent(26/49): loss=438243866350.73114\n",
      "Gradient Descent(27/49): loss=1414835703836.8213\n",
      "Gradient Descent(28/49): loss=4567685306176.593\n",
      "Gradient Descent(29/49): loss=14746411190843.459\n",
      "Gradient Descent(30/49): loss=47607623650296.04\n",
      "Gradient Descent(31/49): loss=153697452234046.2\n",
      "Gradient Descent(32/49): loss=496200083347158.5\n",
      "Gradient Descent(33/49): loss=1601942772211983.5\n",
      "Gradient Descent(34/49): loss=5171745696073951.0\n",
      "Gradient Descent(35/49): loss=1.6696572442426842e+16\n",
      "Gradient Descent(36/49): loss=5.390356519981958e+16\n",
      "Gradient Descent(37/49): loss=1.7402340218450528e+17\n",
      "Gradient Descent(38/49): loss=5.6182080713228525e+17\n",
      "Gradient Descent(39/49): loss=1.8137940953028616e+18\n",
      "Gradient Descent(40/49): loss=5.855690957670275e+18\n",
      "Gradient Descent(41/49): loss=1.890463569185669e+19\n",
      "Gradient Descent(42/49): loss=6.103212297665544e+19\n",
      "Gradient Descent(43/49): loss=1.9703738785308125e+20\n",
      "Gradient Descent(44/49): loss=6.361196418944057e+20\n",
      "Gradient Descent(45/49): loss=2.0536620141634813e+21\n",
      "Gradient Descent(46/49): loss=6.630085585563643e+21\n",
      "Gradient Descent(47/49): loss=2.1404707575411014e+22\n",
      "Gradient Descent(48/49): loss=6.910340756180553e+22\n",
      "Gradient Descent(49/49): loss=2.2309489255245386e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5049951942164459\n",
      "Gradient Descent(2/49): loss=0.6825539335344041\n",
      "Gradient Descent(3/49): loss=1.325185555766379\n",
      "Gradient Descent(4/49): loss=3.4785818637163164\n",
      "Gradient Descent(5/49): loss=10.629934618482535\n",
      "Gradient Descent(6/49): loss=34.349172315428575\n",
      "Gradient Descent(7/49): loss=113.00306173704753\n",
      "Gradient Descent(8/49): loss=373.8111964743212\n",
      "Gradient Descent(9/49): loss=1238.6155498745916\n",
      "Gradient Descent(10/49): loss=4106.182552859625\n",
      "Gradient Descent(11/49): loss=13614.617050573876\n",
      "Gradient Descent(12/49): loss=45143.195883757246\n",
      "Gradient Descent(13/49): loss=149687.3500146197\n",
      "Gradient Descent(14/49): loss=496340.46428749483\n",
      "Gradient Descent(15/49): loss=1645791.4524897577\n",
      "Gradient Descent(16/49): loss=5457202.684122392\n",
      "Gradient Descent(17/49): loss=18095284.44887788\n",
      "Gradient Descent(18/49): loss=60001313.73213618\n",
      "Gradient Descent(19/49): loss=198955573.00580165\n",
      "Gradient Descent(20/49): loss=659707557.8740915\n",
      "Gradient Descent(21/49): loss=2187493698.935981\n",
      "Gradient Descent(22/49): loss=7253408919.002295\n",
      "Gradient Descent(23/49): loss=24051242285.30045\n",
      "Gradient Descent(24/49): loss=79750399011.06534\n",
      "Gradient Descent(25/49): loss=264440649968.41373\n",
      "Gradient Descent(26/49): loss=876846488832.925\n",
      "Gradient Descent(27/49): loss=2907494612008.368\n",
      "Gradient Descent(28/49): loss=9640826560315.06\n",
      "Gradient Descent(29/49): loss=31967569735883.297\n",
      "Gradient Descent(30/49): loss=105999782116730.75\n",
      "Gradient Descent(31/49): loss=351479762197317.5\n",
      "Gradient Descent(32/49): loss=1165455444976655.5\n",
      "Gradient Descent(33/49): loss=3864479666579503.5\n",
      "Gradient Descent(34/49): loss=1.2814048926345416e+16\n",
      "Gradient Descent(35/49): loss=4.248951063368073e+16\n",
      "Gradient Descent(36/49): loss=1.4088899802605814e+17\n",
      "Gradient Descent(37/49): loss=4.6716729538071174e+17\n",
      "Gradient Descent(38/49): loss=1.5490583717045153e+18\n",
      "Gradient Descent(39/49): loss=5.136450823237436e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=1.7031719102041162e+19\n",
      "Gradient Descent(41/49): loss=5.647468759138199e+19\n",
      "Gradient Descent(42/49): loss=1.8726179779244848e+20\n",
      "Gradient Descent(43/49): loss=6.2093271176964e+20\n",
      "Gradient Descent(44/49): loss=2.0589219856413457e+21\n",
      "Gradient Descent(45/49): loss=6.827083937768135e+21\n",
      "Gradient Descent(46/49): loss=2.263761105004335e+22\n",
      "Gradient Descent(47/49): loss=7.506300475054271e+22\n",
      "Gradient Descent(48/49): loss=2.4889793670031233e+23\n",
      "Gradient Descent(49/49): loss=8.253091266403774e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5058423917682494\n",
      "Gradient Descent(2/49): loss=0.6821747546080951\n",
      "Gradient Descent(3/49): loss=1.3219056073541147\n",
      "Gradient Descent(4/49): loss=3.471791379948327\n",
      "Gradient Descent(5/49): loss=10.631869031444593\n",
      "Gradient Descent(6/49): loss=34.447393642788164\n",
      "Gradient Descent(7/49): loss=113.64420968471912\n",
      "Gradient Descent(8/49): loss=376.99609830092686\n",
      "Gradient Descent(9/49): loss=1252.7073221285013\n",
      "Gradient Descent(10/49): loss=4164.660353733569\n",
      "Gradient Descent(11/49): loss=13847.607341926188\n",
      "Gradient Descent(12/49): loss=46045.73921116623\n",
      "Gradient Descent(13/49): loss=153112.28149387962\n",
      "Gradient Descent(14/49): loss=509134.3139970607\n",
      "Gradient Descent(15/49): loss=1692993.2913676612\n",
      "Gradient Descent(16/49): loss=5629609.425347909\n",
      "Gradient Descent(17/49): loss=18719805.875499807\n",
      "Gradient Descent(18/49): loss=62247860.74851072\n",
      "Gradient Descent(19/49): loss=206989123.28142995\n",
      "Gradient Descent(20/49): loss=688288669.9911684\n",
      "Gradient Descent(21/49): loss=2288725543.409038\n",
      "Gradient Descent(22/49): loss=7610563478.443593\n",
      "Gradient Descent(23/49): loss=25306955930.515713\n",
      "Gradient Descent(24/49): loss=84151721524.77574\n",
      "Gradient Descent(25/49): loss=279824734949.4967\n",
      "Gradient Descent(26/49): loss=930484616011.971\n",
      "Gradient Descent(27/49): loss=3094085377376.607\n",
      "Gradient Descent(28/49): loss=10288578830597.674\n",
      "Gradient Descent(29/49): loss=34212001752576.496\n",
      "Gradient Descent(30/49): loss=113763142917020.2\n",
      "Gradient Descent(31/49): loss=378289840505568.6\n",
      "Gradient Descent(32/49): loss=1257904799044710.0\n",
      "Gradient Descent(33/49): loss=4182836317636780.5\n",
      "Gradient Descent(34/49): loss=1.390893784126446e+16\n",
      "Gradient Descent(35/49): loss=4.625056712270787e+16\n",
      "Gradient Descent(36/49): loss=1.5379427125095373e+17\n",
      "Gradient Descent(37/49): loss=5.1140298035392845e+17\n",
      "Gradient Descent(38/49): loss=1.7005380381699756e+18\n",
      "Gradient Descent(39/49): loss=5.654698408800136e+18\n",
      "Gradient Descent(40/49): loss=1.8803233668855255e+19\n",
      "Gradient Descent(41/49): loss=6.252527912989034e+19\n",
      "Gradient Descent(42/49): loss=2.0791160707352258e+20\n",
      "Gradient Descent(43/49): loss=6.9135615158302e+20\n",
      "Gradient Descent(44/49): loss=2.2989256591271422e+21\n",
      "Gradient Descent(45/49): loss=7.644481319927209e+21\n",
      "Gradient Descent(46/49): loss=2.5419740920593154e+22\n",
      "Gradient Descent(47/49): loss=8.452675877246229e+22\n",
      "Gradient Descent(48/49): loss=2.810718240951818e+23\n",
      "Gradient Descent(49/49): loss=9.346314876790357e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5044922174808057\n",
      "Gradient Descent(2/49): loss=0.6774188659148681\n",
      "Gradient Descent(3/49): loss=1.2978123492382183\n",
      "Gradient Descent(4/49): loss=3.3549686480075267\n",
      "Gradient Descent(5/49): loss=10.113255083344718\n",
      "Gradient Descent(6/49): loss=32.28637555443635\n",
      "Gradient Descent(7/49): loss=105.01713819486548\n",
      "Gradient Descent(8/49): loss=343.5727194126534\n",
      "Gradient Descent(9/49): loss=1126.0225381525518\n",
      "Gradient Descent(10/49): loss=3692.410397362133\n",
      "Gradient Descent(11/49): loss=12110.00106892424\n",
      "Gradient Descent(12/49): loss=39719.16393918409\n",
      "Gradient Descent(13/49): loss=130275.46072040999\n",
      "Gradient Descent(14/49): loss=427294.3461904082\n",
      "Gradient Descent(15/49): loss=1401497.3686690666\n",
      "Gradient Descent(16/49): loss=4596821.216977253\n",
      "Gradient Descent(17/49): loss=15077279.866197573\n",
      "Gradient Descent(18/49): loss=49452516.526088245\n",
      "Gradient Descent(19/49): loss=162201102.72365415\n",
      "Gradient Descent(20/49): loss=532009282.2355731\n",
      "Gradient Descent(21/49): loss=1744956550.5418327\n",
      "Gradient Descent(22/49): loss=5723346313.6693535\n",
      "Gradient Descent(23/49): loss=18772211273.36186\n",
      "Gradient Descent(24/49): loss=61571656997.45996\n",
      "Gradient Descent(25/49): loss=201951112218.154\n",
      "Gradient Descent(26/49): loss=662386781761.4849\n",
      "Gradient Descent(27/49): loss=2172586443489.2441\n",
      "Gradient Descent(28/49): loss=7125945119077.177\n",
      "Gradient Descent(29/49): loss=23372645996331.895\n",
      "Gradient Descent(30/49): loss=76660789795783.55\n",
      "Gradient Descent(31/49): loss=251442506468273.66\n",
      "Gradient Descent(32/49): loss=824715401804078.5\n",
      "Gradient Descent(33/49): loss=2705013975267105.0\n",
      "Gradient Descent(34/49): loss=8872273502330684.0\n",
      "Gradient Descent(35/49): loss=2.9100491834755384e+16\n",
      "Gradient Descent(36/49): loss=9.544775922453334e+16\n",
      "Gradient Descent(37/49): loss=3.130625692760237e+17\n",
      "Gradient Descent(38/49): loss=1.0268252819969065e+18\n",
      "Gradient Descent(39/49): loss=3.367921505871257e+18\n",
      "Gradient Descent(40/49): loss=1.104656796884785e+19\n",
      "Gradient Descent(41/49): loss=3.6232039160546796e+19\n",
      "Gradient Descent(42/49): loss=1.1883878010197113e+20\n",
      "Gradient Descent(43/49): loss=3.897836275111696e+20\n",
      "Gradient Descent(44/49): loss=1.278465465106593e+21\n",
      "Gradient Descent(45/49): loss=4.193285274465178e+21\n",
      "Gradient Descent(46/49): loss=1.3753708545878094e+22\n",
      "Gradient Descent(47/49): loss=4.511128778117449e+22\n",
      "Gradient Descent(48/49): loss=1.4796214988035657e+23\n",
      "Gradient Descent(49/49): loss=4.853064249332501e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5108856991537861\n",
      "Gradient Descent(2/49): loss=0.7152342397408161\n",
      "Gradient Descent(3/49): loss=1.484296325772515\n",
      "Gradient Descent(4/49): loss=4.192560881891628\n",
      "Gradient Descent(5/49): loss=13.659474479566446\n",
      "Gradient Descent(6/49): loss=46.71796515591781\n",
      "Gradient Descent(7/49): loss=162.1388212646833\n",
      "Gradient Descent(8/49): loss=565.1075241370631\n",
      "Gradient Descent(9/49): loss=1971.9816665336828\n",
      "Gradient Descent(10/49): loss=6883.75659024909\n",
      "Gradient Descent(11/49): loss=24032.073203538912\n",
      "Gradient Descent(12/49): loss=83901.41537091981\n",
      "Gradient Descent(13/49): loss=292921.2348476734\n",
      "Gradient Descent(14/49): loss=1022665.0917204176\n",
      "Gradient Descent(15/49): loss=3570395.2406389704\n",
      "Gradient Descent(16/49): loss=12465199.695613125\n",
      "Gradient Descent(17/49): loss=43519330.88674596\n",
      "Gradient Descent(18/49): loss=151937573.11932346\n",
      "Gradient Descent(19/49): loss=530454530.37618124\n",
      "Gradient Descent(20/49): loss=1851958033.6647685\n",
      "Gradient Descent(21/49): loss=6465678702.506951\n",
      "Gradient Descent(22/49): loss=22573406269.726913\n",
      "Gradient Descent(23/49): loss=78809772968.72774\n",
      "Gradient Descent(24/49): loss=275145905815.94525\n",
      "Gradient Descent(25/49): loss=960607633237.3328\n",
      "Gradient Descent(26/49): loss=3353737073782.776\n",
      "Gradient Descent(27/49): loss=11708789281802.805\n",
      "Gradient Descent(28/49): loss=40878501632518.914\n",
      "Gradient Descent(29/49): loss=142717735839430.2\n",
      "Gradient Descent(30/49): loss=498265623976061.56\n",
      "Gradient Descent(31/49): loss=1739577989911329.5\n",
      "Gradient Descent(32/49): loss=6073330041988470.0\n",
      "Gradient Descent(33/49): loss=2.1203612607676244e+16\n",
      "Gradient Descent(34/49): loss=7.402745849609765e+16\n",
      "Gradient Descent(35/49): loss=2.5844957238124422e+17\n",
      "Gradient Descent(36/49): loss=9.023162867001423e+17\n",
      "Gradient Descent(37/49): loss=3.1502264590453e+18\n",
      "Gradient Descent(38/49): loss=1.0998279527416996e+19\n",
      "Gradient Descent(39/49): loss=3.839792285912596e+19\n",
      "Gradient Descent(40/49): loss=1.3405737472119536e+20\n",
      "Gradient Descent(41/49): loss=4.6802999691082944e+20\n",
      "Gradient Descent(42/49): loss=1.6340173635648375e+21\n",
      "Gradient Descent(43/49): loss=5.704789782822566e+21\n",
      "Gradient Descent(44/49): loss=1.9916940414388317e+22\n",
      "Gradient Descent(45/49): loss=6.953534320663893e+22\n",
      "Gradient Descent(46/49): loss=2.4276640157903515e+23\n",
      "Gradient Descent(47/49): loss=8.475621607344877e+23\n",
      "Gradient Descent(48/49): loss=2.959065223344097e+24\n",
      "Gradient Descent(49/49): loss=1.0330884744095412e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5114202087547038\n",
      "Gradient Descent(2/49): loss=0.7296093997340548\n",
      "Gradient Descent(3/49): loss=1.570286766394374\n",
      "Gradient Descent(4/49): loss=4.60794543350403\n",
      "Gradient Descent(5/49): loss=15.508407913072126\n",
      "Gradient Descent(6/49): loss=54.58829346289986\n",
      "Gradient Descent(7/49): loss=194.67546355818232\n",
      "Gradient Descent(8/49): loss=696.823268200943\n",
      "Gradient Descent(9/49): loss=2496.780837814552\n",
      "Gradient Descent(10/49): loss=8948.751715926554\n",
      "Gradient Descent(11/49): loss=32075.91652541243\n",
      "Gradient Descent(12/49): loss=114975.50216378272\n",
      "Gradient Descent(13/49): loss=412130.0055795142\n",
      "Gradient Descent(14/49): loss=1477283.6830907331\n",
      "Gradient Descent(15/49): loss=5295339.079951572\n",
      "Gradient Descent(16/49): loss=18981201.803147763\n",
      "Gradient Descent(17/49): loss=68038331.41407074\n",
      "Gradient Descent(18/49): loss=243884166.95232707\n",
      "Gradient Descent(19/49): loss=874205551.8613142\n",
      "Gradient Descent(20/49): loss=3133599680.00221\n",
      "Gradient Descent(21/49): loss=11232423468.145325\n",
      "Gradient Descent(22/49): loss=40262748870.185295\n",
      "Gradient Descent(23/49): loss=144322278376.44846\n",
      "Gradient Descent(24/49): loss=517324837979.30786\n",
      "Gradient Descent(25/49): loss=1854356728575.2688\n",
      "Gradient Descent(26/49): loss=6646962651639.037\n",
      "Gradient Descent(27/49): loss=23826112749209.074\n",
      "Gradient Descent(28/49): loss=85404970433835.2\n",
      "Gradient Descent(29/49): loss=306135081772685.8\n",
      "Gradient Descent(30/49): loss=1097344660573058.8\n",
      "Gradient Descent(31/49): loss=3933444338085820.0\n",
      "Gradient Descent(32/49): loss=1.4099475685915778e+16\n",
      "Gradient Descent(33/49): loss=5.053972994936869e+16\n",
      "Gradient Descent(34/49): loss=1.8116023320687018e+17\n",
      "Gradient Descent(35/49): loss=6.493709034149169e+17\n",
      "Gradient Descent(36/49): loss=2.3276773425235005e+18\n",
      "Gradient Descent(37/49): loss=8.343585741838239e+18\n",
      "Gradient Descent(38/49): loss=2.9907677391375196e+19\n",
      "Gradient Descent(39/49): loss=1.072044076279249e+20\n",
      "Gradient Descent(40/49): loss=3.842754107735792e+20\n",
      "Gradient Descent(41/49): loss=1.3774395530239082e+21\n",
      "Gradient Descent(42/49): loss=4.937447645726763e+21\n",
      "Gradient Descent(43/49): loss=1.769833688946621e+22\n",
      "Gradient Descent(44/49): loss=6.343988860806175e+22\n",
      "Gradient Descent(45/49): loss=2.2740099771739907e+23\n",
      "Gradient Descent(46/49): loss=8.151214464190835e+23\n",
      "Gradient Descent(47/49): loss=2.9218120372455167e+24\n",
      "Gradient Descent(48/49): loss=1.0473268270018751e+25\n",
      "Gradient Descent(49/49): loss=3.754154848345019e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5122544233920298\n",
      "Gradient Descent(2/49): loss=0.7288765003765011\n",
      "Gradient Descent(3/49): loss=1.5655522075341268\n",
      "Gradient Descent(4/49): loss=4.597378780043458\n",
      "Gradient Descent(5/49): loss=15.507519734622921\n",
      "Gradient Descent(6/49): loss=54.73150100053915\n",
      "Gradient Descent(7/49): loss=195.727954178107\n",
      "Gradient Descent(8/49): loss=702.5465790510839\n",
      "Gradient Descent(9/49): loss=2524.320154683372\n",
      "Gradient Descent(10/49): loss=9072.727061439427\n",
      "Gradient Descent(11/49): loss=32611.11863127788\n",
      "Gradient Descent(12/49): loss=117220.37468673386\n",
      "Gradient Descent(13/49): loss=421350.16454997985\n",
      "Gradient Descent(14/49): loss=1514551.2277514273\n",
      "Gradient Descent(15/49): loss=5444085.9301325455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=19568882.815971985\n",
      "Gradient Descent(17/49): loss=70340768.58966328\n",
      "Gradient Descent(18/49): loss=252841402.46134767\n",
      "Gradient Descent(19/49): loss=908843850.585674\n",
      "Gradient Descent(20/49): loss=3266858739.71337\n",
      "Gradient Descent(21/49): loss=11742793903.133266\n",
      "Gradient Descent(22/49): loss=42209724890.60864\n",
      "Gradient Descent(23/49): loss=151723762680.98773\n",
      "Gradient Descent(24/49): loss=545374323614.83875\n",
      "Gradient Descent(25/49): loss=1960359719552.355\n",
      "Gradient Descent(26/49): loss=7046555115709.531\n",
      "Gradient Descent(27/49): loss=25328993706356.816\n",
      "Gradient Descent(28/49): loss=91045611882945.9\n",
      "Gradient Descent(29/49): loss=327265407352513.5\n",
      "Gradient Descent(30/49): loss=1176362535597047.2\n",
      "Gradient Descent(31/49): loss=4228460399622123.5\n",
      "Gradient Descent(32/49): loss=1.519929172353144e+16\n",
      "Gradient Descent(33/49): loss=5.463418054421325e+16\n",
      "Gradient Descent(34/49): loss=1.9638373537606816e+17\n",
      "Gradient Descent(35/49): loss=7.059055546563478e+17\n",
      "Gradient Descent(36/49): loss=2.537392677354106e+18\n",
      "Gradient Descent(37/49): loss=9.120712475799498e+18\n",
      "Gradient Descent(38/49): loss=3.2784596885078454e+19\n",
      "Gradient Descent(39/49): loss=1.1784493763716404e+20\n",
      "Gradient Descent(40/49): loss=4.235961593606707e+20\n",
      "Gradient Descent(41/49): loss=1.5226254926416398e+21\n",
      "Gradient Descent(42/49): loss=5.473110035608739e+21\n",
      "Gradient Descent(43/49): loss=1.9673211572145275e+22\n",
      "Gradient Descent(44/49): loss=7.0715781529020045e+22\n",
      "Gradient Descent(45/49): loss=2.54189395509805e+23\n",
      "Gradient Descent(46/49): loss=9.136892415326638e+23\n",
      "Gradient Descent(47/49): loss=3.284275602521485e+24\n",
      "Gradient Descent(48/49): loss=1.1805399191550032e+25\n",
      "Gradient Descent(49/49): loss=4.243476094541276e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5109059937025714\n",
      "Gradient Descent(2/49): loss=0.7237053937197198\n",
      "Gradient Descent(3/49): loss=1.5362063227679748\n",
      "Gradient Descent(4/49): loss=4.441516551483549\n",
      "Gradient Descent(5/49): loss=14.7562377658951\n",
      "Gradient Descent(6/49): loss=51.341427930212504\n",
      "Gradient Descent(7/49): loss=181.08499528179692\n",
      "Gradient Descent(8/49): loss=641.1864435315671\n",
      "Gradient Descent(9/49): loss=2272.8050687057957\n",
      "Gradient Descent(10/49): loss=8058.86715797307\n",
      "Gradient Descent(11/49): loss=28577.45077354323\n",
      "Gradient Descent(12/49): loss=101340.62417128985\n",
      "Gradient Descent(13/49): loss=359373.9950684044\n",
      "Gradient Descent(14/49): loss=1274414.0772384708\n",
      "Gradient Descent(15/49): loss=4519336.809629796\n",
      "Gradient Descent(16/49): loss=16026508.726029119\n",
      "Gradient Descent(17/49): loss=56833337.2466267\n",
      "Gradient Descent(18/49): loss=201542851.10239497\n",
      "Gradient Descent(19/49): loss=714712929.7253714\n",
      "Gradient Descent(20/49): loss=2534520920.0697947\n",
      "Gradient Descent(21/49): loss=8987939113.521158\n",
      "Gradient Descent(22/49): loss=31873104252.115555\n",
      "Gradient Descent(23/49): loss=113028666734.05515\n",
      "Gradient Descent(24/49): loss=400823195717.0512\n",
      "Gradient Descent(25/49): loss=1421402542092.444\n",
      "Gradient Descent(26/49): loss=5040589487475.865\n",
      "Gradient Descent(27/49): loss=17874980259886.89\n",
      "Gradient Descent(28/49): loss=63388403297916.99\n",
      "Gradient Descent(29/49): loss=224788481678852.1\n",
      "Gradient Descent(30/49): loss=797146778693886.8\n",
      "Gradient Descent(31/49): loss=2826848520156295.5\n",
      "Gradient Descent(32/49): loss=1.002459367521136e+16\n",
      "Gradient Descent(33/49): loss=3.5549297260375096e+16\n",
      "Gradient Descent(34/49): loss=1.2606521288054798e+17\n",
      "Gradient Descent(35/49): loss=4.470535038207992e+17\n",
      "Gradient Descent(36/49): loss=1.5853448442420426e+18\n",
      "Gradient Descent(37/49): loss=5.621963039511921e+18\n",
      "Gradient Descent(38/49): loss=1.993665197350135e+19\n",
      "Gradient Descent(39/49): loss=7.0699520633458065e+19\n",
      "Gradient Descent(40/49): loss=2.507152266310505e+20\n",
      "Gradient Descent(41/49): loss=8.89088416745385e+20\n",
      "Gradient Descent(42/49): loss=3.152892719811062e+21\n",
      "Gradient Descent(43/49): loss=1.1180814321062452e+22\n",
      "Gradient Descent(44/49): loss=3.964949650731106e+22\n",
      "Gradient Descent(45/49): loss=1.4060537346745656e+23\n",
      "Gradient Descent(46/49): loss=4.986159419269684e+23\n",
      "Gradient Descent(47/49): loss=1.768195990043457e+24\n",
      "Gradient Descent(48/49): loss=6.270391289782853e+24\n",
      "Gradient Descent(49/49): loss=2.2236113614316216e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5175244813867974\n",
      "Gradient Descent(2/49): loss=0.7657784562461711\n",
      "Gradient Descent(3/49): loss=1.7603309336494828\n",
      "Gradient Descent(4/49): loss=5.5290640827257995\n",
      "Gradient Descent(5/49): loss=19.728658701206044\n",
      "Gradient Descent(6/49): loss=73.1892878159634\n",
      "Gradient Descent(7/49): loss=274.441799370404\n",
      "Gradient Descent(8/49): loss=1032.0404119875445\n",
      "Gradient Descent(9/49): loss=3883.945823745871\n",
      "Gradient Descent(10/49): loss=14619.652807322824\n",
      "Gradient Descent(11/49): loss=55033.11791662459\n",
      "Gradient Descent(12/49): loss=207165.44467497343\n",
      "Gradient Descent(13/49): loss=779851.9124977161\n",
      "Gradient Descent(14/49): loss=2935671.10335326\n",
      "Gradient Descent(15/49): loss=11051030.277999094\n",
      "Gradient Descent(16/49): loss=41600463.55305699\n",
      "Gradient Descent(17/49): loss=156600656.80533412\n",
      "Gradient Descent(18/49): loss=589507032.8052363\n",
      "Gradient Descent(19/49): loss=2219138471.509828\n",
      "Gradient Descent(20/49): loss=8353718078.657554\n",
      "Gradient Descent(21/49): loss=31446710802.425655\n",
      "Gradient Descent(22/49): loss=118377902031.67155\n",
      "Gradient Descent(23/49): loss=445621412603.50385\n",
      "Gradient Descent(24/49): loss=1677495883631.676\n",
      "Gradient Descent(25/49): loss=6314760377339.168\n",
      "Gradient Descent(26/49): loss=23771264664380.793\n",
      "Gradient Descent(27/49): loss=89484476049455.19\n",
      "Gradient Descent(28/49): loss=336855088145313.44\n",
      "Gradient Descent(29/49): loss=1268056264269513.5\n",
      "Gradient Descent(30/49): loss=4773467125601256.0\n",
      "Gradient Descent(31/49): loss=1.796922505826038e+16\n",
      "Gradient Descent(32/49): loss=6.764329588920016e+16\n",
      "Gradient Descent(33/49): loss=2.5463621630418813e+17\n",
      "Gradient Descent(34/49): loss=9.585517943998556e+17\n",
      "Gradient Descent(35/49): loss=3.608369445175723e+18\n",
      "Gradient Descent(36/49): loss=1.3583334911004744e+19\n",
      "Gradient Descent(37/49): loss=5.11330644236557e+19\n",
      "Gradient Descent(38/49): loss=1.924851514362249e+20\n",
      "Gradient Descent(39/49): loss=7.245905157658714e+20\n",
      "Gradient Descent(40/49): loss=2.727646322951895e+21\n",
      "Gradient Descent(41/49): loss=1.026794348149721e+22\n",
      "Gradient Descent(42/49): loss=3.865261505939049e+22\n",
      "Gradient Descent(43/49): loss=1.4550378599391973e+23\n",
      "Gradient Descent(44/49): loss=5.477340072860273e+23\n",
      "Gradient Descent(45/49): loss=2.0618882229645226e+24\n",
      "Gradient Descent(46/49): loss=7.761765724689961e+24\n",
      "Gradient Descent(47/49): loss=2.9218367171404406e+25\n",
      "Gradient Descent(48/49): loss=1.0998953207868235e+26\n",
      "Gradient Descent(49/49): loss=4.140442583912521e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5181600699247255\n",
      "Gradient Descent(2/49): loss=0.7825158630453859\n",
      "Gradient Descent(3/49): loss=1.8671126101534792\n",
      "Gradient Descent(4/49): loss=6.083490491338235\n",
      "Gradient Descent(5/49): loss=22.386928949361\n",
      "Gradient Descent(6/49): loss=85.38521469337806\n",
      "Gradient Descent(7/49): loss=328.79293255511584\n",
      "Gradient Descent(8/49): loss=1269.235225814282\n",
      "Gradient Descent(9/49): loss=4902.762416781142\n",
      "Gradient Descent(10/49): loss=18941.380258966867\n",
      "Gradient Descent(11/49): loss=73181.44738331328\n",
      "Gradient Descent(12/49): loss=282745.1528663556\n",
      "Gradient Descent(13/49): loss=1092422.3643011625\n",
      "Gradient Descent(14/49): loss=4220717.810274429\n",
      "Gradient Descent(15/49): loss=16307302.75420358\n",
      "Gradient Descent(16/49): loss=63005428.975226514\n",
      "Gradient Descent(17/49): loss=243429841.87786\n",
      "Gradient Descent(18/49): loss=940523524.331414\n",
      "Gradient Descent(19/49): loss=3633837551.509775\n",
      "Gradient Descent(20/49): loss=14039814010.028357\n",
      "Gradient Descent(21/49): loss=54244686135.05101\n",
      "Gradient Descent(22/49): loss=209581549430.19736\n",
      "Gradient Descent(23/49): loss=809746151952.5103\n",
      "Gradient Descent(24/49): loss=3128561805107.397\n",
      "Gradient Descent(25/49): loss=12087614056304.176\n",
      "Gradient Descent(26/49): loss=46702102332019.18\n",
      "Gradient Descent(27/49): loss=180439775134357.6\n",
      "Gradient Descent(28/49): loss=697153036475091.4\n",
      "Gradient Descent(29/49): loss=2693543349322755.0\n",
      "Gradient Descent(30/49): loss=1.0406862475080224e+16\n",
      "Gradient Descent(31/49): loss=4.0208295367684664e+16\n",
      "Gradient Descent(32/49): loss=1.553500894478305e+17\n",
      "Gradient Descent(33/49): loss=6.002157035198541e+17\n",
      "Gradient Descent(34/49): loss=2.3190130886459167e+18\n",
      "Gradient Descent(35/49): loss=8.959815069438877e+18\n",
      "Gradient Descent(36/49): loss=3.4617435525307326e+19\n",
      "Gradient Descent(37/49): loss=1.3374905989257939e+20\n",
      "Gradient Descent(38/49): loss=5.167572568762645e+20\n",
      "Gradient Descent(39/49): loss=1.9965602954424877e+21\n",
      "Gradient Descent(40/49): loss=7.713975876088911e+21\n",
      "Gradient Descent(41/49): loss=2.980397033473717e+22\n",
      "Gradient Descent(42/49): loss=1.1515159782483752e+23\n",
      "Gradient Descent(43/49): loss=4.449034921417283e+23\n",
      "Gradient Descent(44/49): loss=1.7189437320791618e+24\n",
      "Gradient Descent(45/49): loss=6.641367411683454e+24\n",
      "Gradient Descent(46/49): loss=2.5659805073212166e+25\n",
      "Gradient Descent(47/49): loss=9.914006492653063e+25\n",
      "Gradient Descent(48/49): loss=3.830408082054199e+26\n",
      "Gradient Descent(49/49): loss=1.4799290363526596e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5189795663511424\n",
      "Gradient Descent(2/49): loss=0.7813846643706436\n",
      "Gradient Descent(3/49): loss=1.8605954898773795\n",
      "Gradient Descent(4/49): loss=6.067809965965851\n",
      "Gradient Descent(5/49): loss=22.380935415422535\n",
      "Gradient Descent(6/49): loss=85.59065906404966\n",
      "Gradient Descent(7/49): loss=330.4888040337185\n",
      "Gradient Descent(8/49): loss=1279.2984229873741\n",
      "Gradient Descent(9/49): loss=4955.261010920364\n",
      "Gradient Descent(10/49): loss=19196.991277758367\n",
      "Gradient Descent(11/49): loss=74373.51789988102\n",
      "Gradient Descent(12/49): loss=288143.1176852047\n",
      "Gradient Descent(13/49): loss=1116347.504922185\n",
      "Gradient Descent(14/49): loss=4325047.46258906\n",
      "Gradient Descent(15/49): loss=16756466.050456813\n",
      "Gradient Descent(16/49): loss=64919326.43971851\n",
      "Gradient Descent(17/49): loss=251515981.07164016\n",
      "Gradient Descent(18/49): loss=974444628.5448712\n",
      "Gradient Descent(19/49): loss=3775276349.605462\n",
      "Gradient Descent(20/49): loss=14626497085.034397\n",
      "Gradient Descent(21/49): loss=56667220404.32362\n",
      "Gradient Descent(22/49): loss=219544970318.5253\n",
      "Gradient Descent(23/49): loss=850579817545.8262\n",
      "Gradient Descent(24/49): loss=3295388753235.0137\n",
      "Gradient Descent(25/49): loss=12767275699396.977\n",
      "Gradient Descent(26/49): loss=49464066606529.555\n",
      "Gradient Descent(27/49): loss=191637898551119.75\n",
      "Gradient Descent(28/49): loss=742459863909399.4\n",
      "Gradient Descent(29/49): loss=2876501222796083.5\n",
      "Gradient Descent(30/49): loss=1.1144385962063222e+16\n",
      "Gradient Descent(31/49): loss=4.3176529002378456e+16\n",
      "Gradient Descent(32/49): loss=1.6727818500177827e+17\n",
      "Gradient Descent(33/49): loss=6.480833875263087e+17\n",
      "Gradient Descent(34/49): loss=2.51085984214323e+18\n",
      "Gradient Descent(35/49): loss=9.727786991965889e+18\n",
      "Gradient Descent(36/49): loss=3.768822065364069e+19\n",
      "Gradient Descent(37/49): loss=1.4601491348552545e+20\n",
      "Gradient Descent(38/49): loss=5.6570341051974726e+20\n",
      "Gradient Descent(39/49): loss=2.191696320837795e+21\n",
      "Gradient Descent(40/49): loss=8.491256501990326e+21\n",
      "Gradient Descent(41/49): loss=3.2897548942835246e+22\n",
      "Gradient Descent(42/49): loss=1.2745448523343555e+23\n",
      "Gradient Descent(43/49): loss=4.937950190255125e+23\n",
      "Gradient Descent(44/49): loss=1.9131027077455814e+24\n",
      "Gradient Descent(45/49): loss=7.411905404810096e+24\n",
      "Gradient Descent(46/49): loss=2.871583501891057e+25\n",
      "Gradient Descent(47/49): loss=1.1125333309005145e+26\n",
      "Gradient Descent(48/49): loss=4.310271359162979e+26\n",
      "Gradient Descent(49/49): loss=1.6699220305231474e+27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5176346994822447\n",
      "Gradient Descent(2/49): loss=0.7757620518005364\n",
      "Gradient Descent(3/49): loss=1.8250192918119987\n",
      "Gradient Descent(4/49): loss=5.861837893043521\n",
      "Gradient Descent(5/49): loss=21.306917090747373\n",
      "Gradient Descent(6/49): loss=80.35924052105948\n",
      "Gradient Descent(7/49): loss=306.11412132264496\n",
      "Gradient Descent(8/49): loss=1169.1501225634552\n",
      "Gradient Descent(9/49): loss=4468.428806694827\n",
      "Gradient Descent(10/49): loss=17081.148224241635\n",
      "Gradient Descent(11/49): loss=65297.94886170476\n",
      "Gradient Descent(12/49): loss=249624.5554753321\n",
      "Gradient Descent(13/49): loss=954281.3794001102\n",
      "Gradient Descent(14/49): loss=3648093.45350282\n",
      "Gradient Descent(15/49): loss=13946189.265175944\n",
      "Gradient Descent(16/49): loss=53314480.15461247\n",
      "Gradient Descent(17/49): loss=203814374.10190076\n",
      "Gradient Descent(18/49): loss=779156040.3587099\n",
      "Gradient Descent(19/49): loss=2978612958.5585628\n",
      "Gradient Descent(20/49): loss=11386852824.948195\n",
      "Gradient Descent(21/49): loss=43530468399.19755\n",
      "Gradient Descent(22/49): loss=166411361261.82623\n",
      "Gradient Descent(23/49): loss=636169151757.9698\n",
      "Gradient Descent(24/49): loss=2431992542938.166\n",
      "Gradient Descent(25/49): loss=9297193541316.17\n",
      "Gradient Descent(26/49): loss=35541970716846.91\n",
      "Gradient Descent(27/49): loss=135872365872938.3\n",
      "Gradient Descent(28/49): loss=519422514721702.56\n",
      "Gradient Descent(29/49): loss=1985685220585046.5\n",
      "Gradient Descent(30/49): loss=7591018262584270.0\n",
      "Gradient Descent(31/49): loss=2.901948287952228e+16\n",
      "Gradient Descent(32/49): loss=1.1093773687065178e+17\n",
      "Gradient Descent(33/49): loss=4.241006469025196e+17\n",
      "Gradient Descent(34/49): loss=1.621282025185391e+18\n",
      "Gradient Descent(35/49): loss=6.197951888041844e+18\n",
      "Gradient Descent(36/49): loss=2.369396996311546e+19\n",
      "Gradient Descent(37/49): loss=9.057898847136432e+19\n",
      "Gradient Descent(38/49): loss=3.462717799198558e+20\n",
      "Gradient Descent(39/49): loss=1.3237523137805e+21\n",
      "Gradient Descent(40/49): loss=5.060534209991926e+21\n",
      "Gradient Descent(41/49): loss=1.934576901124482e+22\n",
      "Gradient Descent(42/49): loss=7.395637754952332e+22\n",
      "Gradient Descent(43/49): loss=2.827256842086982e+23\n",
      "Gradient Descent(44/49): loss=1.0808237931576747e+24\n",
      "Gradient Descent(45/49): loss=4.131849835734885e+24\n",
      "Gradient Descent(46/49): loss=1.5795528534013148e+25\n",
      "Gradient Descent(47/49): loss=6.038426651205931e+25\n",
      "Gradient Descent(48/49): loss=2.3084125576094302e+26\n",
      "Gradient Descent(49/49): loss=8.824763210570303e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.524473831412695\n",
      "Gradient Descent(2/49): loss=0.8223809578538206\n",
      "Gradient Descent(3/49): loss=2.0925305655348043\n",
      "Gradient Descent(4/49): loss=7.2601216097056325\n",
      "Gradient Descent(5/49): loss=28.191042149941428\n",
      "Gradient Descent(6/49): loss=112.92398187305143\n",
      "Gradient Descent(7/49): loss=455.91324347043246\n",
      "Gradient Descent(8/49): loss=1844.274478101601\n",
      "Gradient Descent(9/49): loss=7464.105004324105\n",
      "Gradient Descent(10/49): loss=30212.130830181653\n",
      "Gradient Descent(11/49): loss=122291.88581213584\n",
      "Gradient Descent(12/49): loss=495013.50707382674\n",
      "Gradient Descent(13/49): loss=2003720.8906438209\n",
      "Gradient Descent(14/49): loss=8110686.080754531\n",
      "Gradient Descent(15/49): loss=32830538.47241703\n",
      "Gradient Descent(16/49): loss=132891875.52123088\n",
      "Gradient Descent(17/49): loss=537921445.0043048\n",
      "Gradient Descent(18/49): loss=2177405355.5100927\n",
      "Gradient Descent(19/49): loss=8813729454.603323\n",
      "Gradient Descent(20/49): loss=35676327657.603004\n",
      "Gradient Descent(21/49): loss=144411098811.16855\n",
      "Gradient Descent(22/49): loss=584549106626.7258\n",
      "Gradient Descent(23/49): loss=2366145406215.7207\n",
      "Gradient Descent(24/49): loss=9577713865078.055\n",
      "Gradient Descent(25/49): loss=38768793600064.07\n",
      "Gradient Descent(26/49): loss=156928822303271.12\n",
      "Gradient Descent(27/49): loss=635218509080739.8\n",
      "Gradient Descent(28/49): loss=2571245666388632.5\n",
      "Gradient Descent(29/49): loss=1.0407921341098684e+16\n",
      "Gradient Descent(30/49): loss=4.212931811943229e+16\n",
      "Gradient Descent(31/49): loss=1.705315967559923e+17\n",
      "Gradient Descent(32/49): loss=6.902799947938067e+17\n",
      "Gradient Descent(33/49): loss=2.7941242577721574e+18\n",
      "Gradient Descent(34/49): loss=1.1310092175281455e+19\n",
      "Gradient Descent(35/49): loss=4.578113684727693e+19\n",
      "Gradient Descent(36/49): loss=1.8531347565935654e+20\n",
      "Gradient Descent(37/49): loss=7.501142746959396e+20\n",
      "Gradient Descent(38/49): loss=3.036322226975648e+21\n",
      "Gradient Descent(39/49): loss=1.2290464235950512e+22\n",
      "Gradient Descent(40/49): loss=4.9749499507382076e+22\n",
      "Gradient Descent(41/49): loss=2.0137666517066234e+23\n",
      "Gradient Descent(42/49): loss=8.15135060187691e+23\n",
      "Gradient Descent(43/49): loss=3.299514200337397e+24\n",
      "Gradient Descent(44/49): loss=1.3355816097176919e+25\n",
      "Gradient Descent(45/49): loss=5.406184449922074e+25\n",
      "Gradient Descent(46/49): loss=2.1883223079686552e+26\n",
      "Gradient Descent(47/49): loss=8.85791923659265e+26\n",
      "Gradient Descent(48/49): loss=3.5855199627715246e+27\n",
      "Gradient Descent(49/49): loss=1.4513513907785773e+28\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5252147777265105\n",
      "Gradient Descent(2/49): loss=0.8417292013148776\n",
      "Gradient Descent(3/49): loss=2.224028918559439\n",
      "Gradient Descent(4/49): loss=7.992442315700634\n",
      "Gradient Descent(5/49): loss=31.963767946916647\n",
      "Gradient Descent(6/49): loss=131.53048555129288\n",
      "Gradient Descent(7/49): loss=545.058942544093\n",
      "Gradient Descent(8/49): loss=2262.5376718293396\n",
      "Gradient Descent(9/49): loss=9395.605506660699\n",
      "Gradient Descent(10/49): loss=39020.795794980266\n",
      "Gradient Descent(11/49): loss=162060.67725734037\n",
      "Gradient Descent(12/49): loss=673072.151058208\n",
      "Gradient Descent(13/49): loss=2795414.2912719017\n",
      "Gradient Descent(14/49): loss=11609964.232794534\n",
      "Gradient Descent(15/49): loss=48218713.21170648\n",
      "Gradient Descent(16/49): loss=200262834.6522959\n",
      "Gradient Descent(17/49): loss=831735242.4578955\n",
      "Gradient Descent(18/49): loss=3454377920.3924375\n",
      "Gradient Descent(19/49): loss=14346785143.731966\n",
      "Gradient Descent(20/49): loss=59585328738.42708\n",
      "Gradient Descent(21/49): loss=247470869978.93195\n",
      "Gradient Descent(22/49): loss=1027800513733.7893\n",
      "Gradient Descent(23/49): loss=4268679768743.275\n",
      "Gradient Descent(24/49): loss=17728758377329.92\n",
      "Gradient Descent(25/49): loss=73631401423746.83\n",
      "Gradient Descent(26/49): loss=305807274273514.9\n",
      "Gradient Descent(27/49): loss=1270084328021992.5\n",
      "Gradient Descent(28/49): loss=5274937308536050.0\n",
      "Gradient Descent(29/49): loss=2.1907965475268584e+16\n",
      "Gradient Descent(30/49): loss=9.098856027897765e+16\n",
      "Gradient Descent(31/49): loss=3.7789534181013094e+17\n",
      "Gradient Descent(32/49): loss=1.569481799953153e+18\n",
      "Gradient Descent(33/49): loss=6.518400329003835e+18\n",
      "Gradient Descent(34/49): loss=2.7072338685562073e+19\n",
      "Gradient Descent(35/49): loss=1.1243732893247862e+20\n",
      "Gradient Descent(36/49): loss=4.669767575053492e+20\n",
      "Gradient Descent(37/49): loss=1.939456354225245e+21\n",
      "Gradient Descent(38/49): loss=8.054985370233508e+21\n",
      "Gradient Descent(39/49): loss=3.345411159850236e+22\n",
      "Gradient Descent(40/49): loss=1.3894222415113012e+23\n",
      "Gradient Descent(41/49): loss=5.770573699206226e+23\n",
      "Gradient Descent(42/49): loss=2.3966451538698684e+24\n",
      "Gradient Descent(43/49): loss=9.953790200024788e+24\n",
      "Gradient Descent(44/49): loss=4.134026231881988e+25\n",
      "Gradient Descent(45/49): loss=1.7169512861388075e+26\n",
      "Gradient Descent(46/49): loss=7.130873278546356e+26\n",
      "Gradient Descent(47/49): loss=2.961607246821767e+27\n",
      "Gradient Descent(48/49): loss=1.2300201029817236e+28\n",
      "Gradient Descent(49/49): loss=5.108541841132987e+28\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5260178206455873\n",
      "Gradient Descent(2/49): loss=0.840151914223461\n",
      "Gradient Descent(3/49): loss=2.215345639044855\n",
      "Gradient Descent(4/49): loss=7.969930610818921\n",
      "Gradient Descent(5/49): loss=31.949213954831436\n",
      "Gradient Descent(6/49): loss=131.82054385706203\n",
      "Gradient Descent(7/49): loss=547.7442488604959\n",
      "Gradient Descent(8/49): loss=2279.876737232938\n",
      "Gradient Descent(9/49): loss=9493.401628147509\n",
      "Gradient Descent(10/49): loss=39534.358691642476\n",
      "Gradient Descent(11/49): loss=164640.8818420731\n",
      "Gradient Descent(12/49): loss=685650.9751280608\n",
      "Gradient Descent(13/49): loss=2855414.065180818\n",
      "Gradient Descent(14/49): loss=11891461.396765934\n",
      "Gradient Descent(15/49): loss=49522367.618371814\n",
      "Gradient Descent(16/49): loss=206237472.2481507\n",
      "Gradient Descent(17/49): loss=858882504.8792453\n",
      "Gradient Descent(18/49): loss=3576843480.138197\n",
      "Gradient Descent(19/49): loss=14895878320.94091\n",
      "Gradient Descent(20/49): loss=62034358561.66223\n",
      "Gradient Descent(21/49): loss=258344057282.16232\n",
      "Gradient Descent(22/49): loss=1075882035064.5327\n",
      "Gradient Descent(23/49): loss=4480544919643.092\n",
      "Gradient Descent(24/49): loss=18659371680780.47\n",
      "Gradient Descent(25/49): loss=77707546239545.97\n",
      "Gradient Descent(26/49): loss=323615545361100.94\n",
      "Gradient Descent(27/49): loss=1347707221078948.5\n",
      "Gradient Descent(28/49): loss=5612569543659124.0\n",
      "Gradient Descent(29/49): loss=2.33737241959652e+16\n",
      "Gradient Descent(30/49): loss=9.73406170808649e+16\n",
      "Gradient Descent(31/49): loss=4.053780926926158e+17\n",
      "Gradient Descent(32/49): loss=1.6882099473294467e+18\n",
      "Gradient Descent(33/49): loss=7.030603966118055e+18\n",
      "Gradient Descent(34/49): loss=2.927917360431786e+19\n",
      "Gradient Descent(35/49): loss=1.2193404877918693e+20\n",
      "Gradient Descent(36/49): loss=5.0779822042151256e+20\n",
      "Gradient Descent(37/49): loss=2.1147418235099858e+21\n",
      "Gradient Descent(38/49): loss=8.806909516914303e+21\n",
      "Gradient Descent(39/49): loss=3.6676654510185606e+22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=1.527410930560837e+23\n",
      "Gradient Descent(41/49): loss=6.36095135162568e+23\n",
      "Gradient Descent(42/49): loss=2.649038401400709e+24\n",
      "Gradient Descent(43/49): loss=1.103200459205242e+25\n",
      "Gradient Descent(44/49): loss=4.5943133649822434e+25\n",
      "Gradient Descent(45/49): loss=1.913316398622686e+26\n",
      "Gradient Descent(46/49): loss=7.968066934965347e+26\n",
      "Gradient Descent(47/49): loss=3.3183267924631823e+27\n",
      "Gradient Descent(48/49): loss=1.3819277362316692e+28\n",
      "Gradient Descent(49/49): loss=5.755081966320703e+28\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5246783348198258\n",
      "Gradient Descent(2/49): loss=0.8340395029792509\n",
      "Gradient Descent(3/49): loss=2.1724227065898396\n",
      "Gradient Descent(4/49): loss=7.700204630401\n",
      "Gradient Descent(5/49): loss=30.432728264985755\n",
      "Gradient Descent(6/49): loss=123.87023342139304\n",
      "Gradient Descent(7/49): loss=507.8974505251611\n",
      "Gradient Descent(8/49): loss=2086.2250058143777\n",
      "Gradient Descent(9/49): loss=8573.035467891992\n",
      "Gradient Descent(10/49): loss=35233.33761595331\n",
      "Gradient Descent(11/49): loss=144805.15943943386\n",
      "Gradient Descent(12/49): loss=595137.035817059\n",
      "Gradient Descent(13/49): loss=2445966.880892254\n",
      "Gradient Descent(14/49): loss=10052737.114151087\n",
      "Gradient Descent(15/49): loss=41315985.61018966\n",
      "Gradient Descent(16/49): loss=169805564.82892287\n",
      "Gradient Descent(17/49): loss=697887986.2623149\n",
      "Gradient Descent(18/49): loss=2868266670.059683\n",
      "Gradient Descent(19/49): loss=11788358382.81555\n",
      "Gradient Descent(20/49): loss=48449258509.58699\n",
      "Gradient Descent(21/49): loss=199122776381.95535\n",
      "Gradient Descent(22/49): loss=818379502472.4414\n",
      "Gradient Descent(23/49): loss=3363477660551.819\n",
      "Gradient Descent(24/49): loss=13823637980736.3\n",
      "Gradient Descent(25/49): loss=56814103231212.45\n",
      "Gradient Descent(26/49): loss=233501653505758.78\n",
      "Gradient Descent(27/49): loss=959674078952450.1\n",
      "Gradient Descent(28/49): loss=3944187649148802.0\n",
      "Gradient Descent(29/49): loss=1.6210311972455334e+16\n",
      "Gradient Descent(30/49): loss=6.662315224810338e+16\n",
      "Gradient Descent(31/49): loss=2.7381610070282176e+17\n",
      "Gradient Descent(32/49): loss=1.1253633980705486e+18\n",
      "Gradient Descent(33/49): loss=4.625158179034137e+18\n",
      "Gradient Descent(34/49): loss=1.9009049181592003e+19\n",
      "Gradient Descent(35/49): loss=7.812574982325015e+19\n",
      "Gradient Descent(36/49): loss=3.2109090397618176e+20\n",
      "Gradient Descent(37/49): loss=1.3196592525446621e+21\n",
      "Gradient Descent(38/49): loss=5.423699398709623e+21\n",
      "Gradient Descent(39/49): loss=2.229099300508072e+22\n",
      "Gradient Descent(40/49): loss=9.161428992004451e+22\n",
      "Gradient Descent(41/49): loss=3.765277803299753e+23\n",
      "Gradient Descent(42/49): loss=1.5475006080814325e+24\n",
      "Gradient Descent(43/49): loss=6.360110082485083e+24\n",
      "Gradient Descent(44/49): loss=2.6139569865164055e+25\n",
      "Gradient Descent(45/49): loss=1.0743164880391686e+26\n",
      "Gradient Descent(46/49): loss=4.415359251993478e+26\n",
      "Gradient Descent(47/49): loss=1.8146791509964828e+27\n",
      "Gradient Descent(48/49): loss=7.458193621673101e+27\n",
      "Gradient Descent(49/49): loss=3.065260989405225e+28\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5317337492314788\n",
      "Gradient Descent(2/49): loss=0.8854986503054487\n",
      "Gradient Descent(3/49): loss=2.489670434967947\n",
      "Gradient Descent(4/49): loss=9.481626378591926\n",
      "Gradient Descent(5/49): loss=39.85150646250171\n",
      "Gradient Descent(6/49): loss=171.71195988600647\n",
      "Gradient Descent(7/49): loss=744.1928533052061\n",
      "Gradient Descent(8/49): loss=3229.6322888520053\n",
      "Gradient Descent(9/49): loss=14020.207382999522\n",
      "Gradient Descent(10/49): loss=60867.6457125838\n",
      "Gradient Descent(11/49): loss=264256.4651964355\n",
      "Gradient Descent(12/49): loss=1147271.9045592716\n",
      "Gradient Descent(13/49): loss=4980895.926103922\n",
      "Gradient Descent(14/49): loss=21624628.839075197\n",
      "Gradient Descent(15/49): loss=93883630.68488064\n",
      "Gradient Descent(16/49): loss=407597109.202147\n",
      "Gradient Descent(17/49): loss=1769588613.2224994\n",
      "Gradient Descent(18/49): loss=7682693991.383358\n",
      "Gradient Descent(19/49): loss=33354524623.232876\n",
      "Gradient Descent(20/49): loss=144809140403.36343\n",
      "Gradient Descent(21/49): loss=628690931181.6631\n",
      "Gradient Descent(22/49): loss=2729470569673.363\n",
      "Gradient Descent(23/49): loss=11850035082759.863\n",
      "Gradient Descent(24/49): loss=51447094913888.45\n",
      "Gradient Descent(25/49): loss=223358290215479.28\n",
      "Gradient Descent(26/49): loss=969713176059525.4\n",
      "Gradient Descent(27/49): loss=4210023469092117.5\n",
      "Gradient Descent(28/49): loss=1.827787643592717e+16\n",
      "Gradient Descent(29/49): loss=7.935365906144539e+16\n",
      "Gradient Descent(30/49): loss=3.445150331612193e+17\n",
      "Gradient Descent(31/49): loss=1.4957168891502595e+18\n",
      "Gradient Descent(32/49): loss=6.493676029058617e+18\n",
      "Gradient Descent(33/49): loss=2.819238632407679e+19\n",
      "Gradient Descent(34/49): loss=1.2239764396765258e+20\n",
      "Gradient Descent(35/49): loss=5.313911024281747e+20\n",
      "Gradient Descent(36/49): loss=2.307041986972048e+21\n",
      "Gradient Descent(37/49): loss=1.0016055416304859e+22\n",
      "Gradient Descent(38/49): loss=4.3484846252911e+22\n",
      "Gradient Descent(39/49): loss=1.8879007503903353e+23\n",
      "Gradient Descent(40/49): loss=8.196347809521743e+23\n",
      "Gradient Descent(41/49): loss=3.5584559940856355e+24\n",
      "Gradient Descent(42/49): loss=1.544908702768049e+25\n",
      "Gradient Descent(43/49): loss=6.707242983629262e+25\n",
      "Gradient Descent(44/49): loss=2.9119590277949658e+26\n",
      "Gradient Descent(45/49): loss=1.2642311304738768e+27\n",
      "Gradient Descent(46/49): loss=5.488677333724468e+27\n",
      "Gradient Descent(47/49): loss=2.382917027398988e+28\n",
      "Gradient Descent(48/49): loss=1.034546797746424e+29\n",
      "Gradient Descent(49/49): loss=4.491499554626254e+29\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5325843321600594\n",
      "Gradient Descent(2/49): loss=0.9077227004953832\n",
      "Gradient Descent(3/49): loss=2.650379290554714\n",
      "Gradient Descent(4/49): loss=10.439627425197125\n",
      "Gradient Descent(5/49): loss=45.141917815932544\n",
      "Gradient Descent(6/49): loss=199.68984420631813\n",
      "Gradient Descent(7/49): loss=887.9399708200424\n",
      "Gradient Descent(8/49): loss=3952.907292027861\n",
      "Gradient Descent(9/49): loss=17602.03079153167\n",
      "Gradient Descent(10/49): loss=78385.23087933127\n",
      "Gradient Descent(11/49): loss=349069.07544642064\n",
      "Gradient Descent(12/49): loss=1554496.6094541335\n",
      "Gradient Descent(13/49): loss=6922587.709663126\n",
      "Gradient Descent(14/49): loss=30828132.619300365\n",
      "Gradient Descent(15/49): loss=137285915.60352558\n",
      "Gradient Descent(16/49): loss=611370886.2911303\n",
      "Gradient Descent(17/49): loss=2722598014.36823\n",
      "Gradient Descent(18/49): loss=12124456883.383787\n",
      "Gradient Descent(19/49): loss=53993448152.63967\n",
      "Gradient Descent(20/49): loss=240447260566.4995\n",
      "Gradient Descent(21/49): loss=1070775938420.0447\n",
      "Gradient Descent(22/49): loss=4768451541510.898\n",
      "Gradient Descent(23/49): loss=21235189630141.586\n",
      "Gradient Descent(24/49): loss=94565976963921.38\n",
      "Gradient Descent(25/49): loss=421127579027952.44\n",
      "Gradient Descent(26/49): loss=1875393704076114.0\n",
      "Gradient Descent(27/49): loss=8351629578396378.0\n",
      "Gradient Descent(28/49): loss=3.719203944384908e+16\n",
      "Gradient Descent(29/49): loss=1.6562609548332323e+17\n",
      "Gradient Descent(30/49): loss=7.375772857647855e+17\n",
      "Gradient Descent(31/49): loss=3.284628855668008e+18\n",
      "Gradient Descent(32/49): loss=1.4627330488221518e+19\n",
      "Gradient Descent(33/49): loss=6.513941349642764e+19\n",
      "Gradient Descent(34/49): loss=2.9008322428178553e+20\n",
      "Gradient Descent(35/49): loss=1.291818155751912e+21\n",
      "Gradient Descent(36/49): loss=5.752811634185412e+21\n",
      "Gradient Descent(37/49): loss=2.5618808306000418e+22\n",
      "Gradient Descent(38/49): loss=1.1408740295257878e+23\n",
      "Gradient Descent(39/49): loss=5.080617082963822e+23\n",
      "Gradient Descent(40/49): loss=2.2625346248290958e+24\n",
      "Gradient Descent(41/49): loss=1.007567160634014e+25\n",
      "Gradient Descent(42/49): loss=4.486965954232734e+25\n",
      "Gradient Descent(43/49): loss=1.9981659050672876e+26\n",
      "Gradient Descent(44/49): loss=8.898367014367397e+26\n",
      "Gradient Descent(45/49): loss=3.962680742453941e+27\n",
      "Gradient Descent(46/49): loss=1.7646876827243996e+28\n",
      "Gradient Descent(47/49): loss=7.858626066430857e+28\n",
      "Gradient Descent(48/49): loss=3.4996562993312536e+29\n",
      "Gradient Descent(49/49): loss=1.5584905185610052e+30\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5333691862753644\n",
      "Gradient Descent(2/49): loss=0.9056482107339983\n",
      "Gradient Descent(3/49): loss=2.6390838675655113\n",
      "Gradient Descent(4/49): loss=10.408103043299317\n",
      "Gradient Descent(5/49): loss=45.11372718984933\n",
      "Gradient Descent(6/49): loss=200.09301322485013\n",
      "Gradient Descent(7/49): loss=892.1231549349835\n",
      "Gradient Descent(8/49): loss=3982.2243083269123\n",
      "Gradient Descent(9/49): loss=17780.338319748564\n",
      "Gradient Descent(10/49): loss=79392.5276408778\n",
      "Gradient Descent(11/49): loss=354507.05923465925\n",
      "Gradient Descent(12/49): loss=1582965.3491613378\n",
      "Gradient Descent(13/49): loss=7068352.683738941\n",
      "Gradient Descent(14/49): loss=31562040.785137746\n",
      "Gradient Descent(15/49): loss=140932759.7830422\n",
      "Gradient Descent(16/49): loss=629301605.0311718\n",
      "Gradient Descent(17/49): loss=2809996138.2077456\n",
      "Gradient Descent(18/49): loss=12547367170.971832\n",
      "Gradient Descent(19/49): loss=56027273772.831215\n",
      "Gradient Descent(20/49): loss=250176420574.90518\n",
      "Gradient Descent(21/49): loss=1117103103493.7883\n",
      "Gradient Descent(22/49): loss=4988157321017.181\n",
      "Gradient Descent(23/49): loss=22273426133544.56\n",
      "Gradient Descent(24/49): loss=99456669026093.38\n",
      "Gradient Descent(25/49): loss=444100021005247.8\n",
      "Gradient Descent(26/49): loss=1983022662915831.5\n",
      "Gradient Descent(27/49): loss=8854714468908658.0\n",
      "Gradient Descent(28/49): loss=3.953861435482166e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=1.7655024683048634e+17\n",
      "Gradient Descent(30/49): loss=7.883429949310977e+17\n",
      "Gradient Descent(31/49): loss=3.52015751217636e+18\n",
      "Gradient Descent(32/49): loss=1.5718423313464816e+19\n",
      "Gradient Descent(33/49): loss=7.018686823150837e+19\n",
      "Gradient Descent(34/49): loss=3.134027105585822e+20\n",
      "Gradient Descent(35/49): loss=1.3994250129737662e+21\n",
      "Gradient Descent(36/49): loss=6.248798433957902e+21\n",
      "Gradient Descent(37/49): loss=2.7902518181563126e+22\n",
      "Gradient Descent(38/49): loss=1.2459203622916944e+23\n",
      "Gradient Descent(39/49): loss=5.563360048982175e+23\n",
      "Gradient Descent(40/49): loss=2.484185664778845e+24\n",
      "Gradient Descent(41/49): loss=1.1092538255225364e+25\n",
      "Gradient Descent(42/49): loss=4.953108243404654e+25\n",
      "Gradient Descent(43/49): loss=2.211692284164648e+26\n",
      "Gradient Descent(44/49): loss=9.875784092437035e+26\n",
      "Gradient Descent(45/49): loss=4.409795708866866e+27\n",
      "Gradient Descent(46/49): loss=1.9690890375816123e+28\n",
      "Gradient Descent(47/49): loss=8.79249719012582e+28\n",
      "Gradient Descent(48/49): loss=3.926079794406786e+29\n",
      "Gradient Descent(49/49): loss=1.7530972394690755e+30\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5320368997153149\n",
      "Gradient Descent(2/49): loss=0.8990056630018125\n",
      "Gradient Descent(3/49): loss=2.587545022372679\n",
      "Gradient Descent(4/49): loss=10.05792790243231\n",
      "Gradient Descent(5/49): loss=42.99718677916675\n",
      "Gradient Descent(6/49): loss=188.18190989025547\n",
      "Gradient Descent(7/49): loss=828.0712764790363\n",
      "Gradient Descent(8/49): loss=3648.304653858721\n",
      "Gradient Descent(9/49): loss=16078.115404304153\n",
      "Gradient Descent(10/49): loss=70860.86874738825\n",
      "Gradient Descent(11/49): loss=312308.6231627432\n",
      "Gradient Descent(12/49): loss=1376457.7196229834\n",
      "Gradient Descent(13/49): loss=6066554.340372959\n",
      "Gradient Descent(14/49): loss=26737535.855935834\n",
      "Gradient Descent(15/49): loss=117842157.21724942\n",
      "Gradient Descent(16/49): loss=519373745.20708823\n",
      "Gradient Descent(17/49): loss=2289071195.720626\n",
      "Gradient Descent(18/49): loss=10088779014.551939\n",
      "Gradient Descent(19/49): loss=44464961249.24791\n",
      "Gradient Descent(20/49): loss=195973444961.91876\n",
      "Gradient Descent(21/49): loss=863727079737.1067\n",
      "Gradient Descent(22/49): loss=3806763045967.8535\n",
      "Gradient Descent(23/49): loss=16777805429651.389\n",
      "Gradient Descent(24/49): loss=73945961867371.31\n",
      "Gradient Descent(25/49): loss=325907062125490.9\n",
      "Gradient Descent(26/49): loss=1436392339229772.5\n",
      "Gradient Descent(27/49): loss=6330709554871585.0\n",
      "Gradient Descent(28/49): loss=2.7901766372294384e+16\n",
      "Gradient Descent(29/49): loss=1.229733507668218e+17\n",
      "Gradient Descent(30/49): loss=5.419888044735414e+17\n",
      "Gradient Descent(31/49): loss=2.388744084331403e+18\n",
      "Gradient Descent(32/49): loss=1.0528074110259331e+19\n",
      "Gradient Descent(33/49): loss=4.6401096374514336e+19\n",
      "Gradient Descent(34/49): loss=2.0450670485486744e+20\n",
      "Gradient Descent(35/49): loss=9.013362958718139e+20\n",
      "Gradient Descent(36/49): loss=3.9725206996634624e+21\n",
      "Gradient Descent(37/49): loss=1.7508360399478426e+22\n",
      "Gradient Descent(38/49): loss=7.716578642472375e+22\n",
      "Gradient Descent(39/49): loss=3.4009801367371e+23\n",
      "Gradient Descent(40/49): loss=1.4989370842172077e+24\n",
      "Gradient Descent(41/49): loss=6.606367259166559e+24\n",
      "Gradient Descent(42/49): loss=2.911669130247728e+25\n",
      "Gradient Descent(43/49): loss=1.2832797196181197e+26\n",
      "Gradient Descent(44/49): loss=5.655885902953034e+26\n",
      "Gradient Descent(45/49): loss=2.4927570239123056e+27\n",
      "Gradient Descent(46/49): loss=1.0986497406214889e+28\n",
      "Gradient Descent(47/49): loss=4.842153651515059e+28\n",
      "Gradient Descent(48/49): loss=2.1341152796902625e+29\n",
      "Gradient Descent(49/49): loss=9.405831278365977e+29\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5393042348431486\n",
      "Gradient Descent(2/49): loss=0.9556053262931515\n",
      "Gradient Descent(3/49): loss=2.961517670915769\n",
      "Gradient Descent(4/49): loss=12.308161466167485\n",
      "Gradient Descent(5/49): loss=55.74258175216309\n",
      "Gradient Descent(6/49): loss=257.5268292315258\n",
      "Gradient Descent(7/49): loss=1194.9230658225172\n",
      "Gradient Descent(8/49): loss=5549.604032958225\n",
      "Gradient Descent(9/49): loss=25779.282765225773\n",
      "Gradient Descent(10/49): loss=119756.27429323112\n",
      "Gradient Descent(11/49): loss=556326.4597023728\n",
      "Gradient Descent(12/49): loss=2584413.572395402\n",
      "Gradient Descent(13/49): loss=12005893.711298933\n",
      "Gradient Descent(14/49): loss=55773386.45330686\n",
      "Gradient Descent(15/49): loss=259095305.36483037\n",
      "Gradient Descent(16/49): loss=1203627425.4285786\n",
      "Gradient Descent(17/49): loss=5591452066.3065195\n",
      "Gradient Descent(18/49): loss=25975094581.0679\n",
      "Gradient Descent(19/49): loss=120667320496.09975\n",
      "Gradient Descent(20/49): loss=560560123867.72\n",
      "Gradient Descent(21/49): loss=2604082457282.711\n",
      "Gradient Descent(22/49): loss=12097266922130.473\n",
      "Gradient Descent(23/49): loss=56197862159092.836\n",
      "Gradient Descent(24/49): loss=261067208947410.53\n",
      "Gradient Descent(25/49): loss=1212787906320097.0\n",
      "Gradient Descent(26/49): loss=5634007088238242.0\n",
      "Gradient Descent(27/49): loss=2.6172783967340304e+16\n",
      "Gradient Descent(28/49): loss=1.2158568668313003e+17\n",
      "Gradient Descent(29/49): loss=5.6482639464931104e+17\n",
      "Gradient Descent(30/49): loss=2.6239014212583834e+18\n",
      "Gradient Descent(31/49): loss=1.2189335933488714e+19\n",
      "Gradient Descent(32/49): loss=5.662556881736212e+19\n",
      "Gradient Descent(33/49): loss=2.6305412053501906e+20\n",
      "Gradient Descent(34/49): loss=1.2220181055247255e+21\n",
      "Gradient Descent(35/49): loss=5.676885985260359e+21\n",
      "Gradient Descent(36/49): loss=2.6371977914195785e+22\n",
      "Gradient Descent(37/49): loss=1.2251104230604596e+23\n",
      "Gradient Descent(38/49): loss=5.6912513485894096e+23\n",
      "Gradient Descent(39/49): loss=2.643871221983892e+24\n",
      "Gradient Descent(40/49): loss=1.2282105657075113e+25\n",
      "Gradient Descent(41/49): loss=5.705653063478732e+25\n",
      "Gradient Descent(42/49): loss=2.650561539668176e+26\n",
      "Gradient Descent(43/49): loss=1.231318553267368e+27\n",
      "Gradient Descent(44/49): loss=5.720091221916101e+27\n",
      "Gradient Descent(45/49): loss=2.6572687872053024e+28\n",
      "Gradient Descent(46/49): loss=1.2344344055915761e+29\n",
      "Gradient Descent(47/49): loss=5.734565916121925e+29\n",
      "Gradient Descent(48/49): loss=2.663993007436277e+30\n",
      "Gradient Descent(49/49): loss=1.2375581425819214e+31\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5402687332253718\n",
      "Gradient Descent(2/49): loss=0.9809870546460989\n",
      "Gradient Descent(3/49): loss=3.1565585239101805\n",
      "Gradient Descent(4/49): loss=13.55020812574462\n",
      "Gradient Descent(5/49): loss=63.07854699812436\n",
      "Gradient Descent(6/49): loss=299.03064896970847\n",
      "Gradient Descent(7/49): loss=1423.0621999468178\n",
      "Gradient Descent(8/49): loss=6777.707825860255\n",
      "Gradient Descent(9/49): loss=32286.06889061791\n",
      "Gradient Descent(10/49): loss=153802.30379911992\n",
      "Gradient Descent(11/49): loss=732678.9657611734\n",
      "Gradient Descent(12/49): loss=3490320.2653653705\n",
      "Gradient Descent(13/49): loss=16627117.84165178\n",
      "Gradient Descent(14/49): loss=79207936.65745531\n",
      "Gradient Descent(15/49): loss=377329214.75326985\n",
      "Gradient Descent(16/49): loss=1797513516.185052\n",
      "Gradient Descent(17/49): loss=8562959655.8021755\n",
      "Gradient Descent(18/49): loss=40792059373.28701\n",
      "Gradient Descent(19/49): loss=194324412918.60834\n",
      "Gradient Descent(20/49): loss=925718829510.934\n",
      "Gradient Descent(21/49): loss=4409921215977.427\n",
      "Gradient Descent(22/49): loss=21007896254424.203\n",
      "Gradient Descent(23/49): loss=100077004423050.08\n",
      "Gradient Descent(24/49): loss=476744872165961.7\n",
      "Gradient Descent(25/49): loss=2271107877847211.5\n",
      "Gradient Descent(26/49): loss=1.0819059194881292e+16\n",
      "Gradient Descent(27/49): loss=5.153962213952584e+16\n",
      "Gradient Descent(28/49): loss=2.4552344177411008e+17\n",
      "Gradient Descent(29/49): loss=1.1696197596756375e+18\n",
      "Gradient Descent(30/49): loss=5.571811686650705e+18\n",
      "Gradient Descent(31/49): loss=2.6542887305620607e+19\n",
      "Gradient Descent(32/49): loss=1.2644448630717684e+20\n",
      "Gradient Descent(33/49): loss=6.023537655641642e+20\n",
      "Gradient Descent(34/49): loss=2.8694810622891954e+21\n",
      "Gradient Descent(35/49): loss=1.366957764283986e+22\n",
      "Gradient Descent(36/49): loss=6.511886605188396e+22\n",
      "Gradient Descent(37/49): loss=3.102119777712645e+23\n",
      "Gradient Descent(38/49): loss=1.4777817395666363e+24\n",
      "Gradient Descent(39/49): loss=7.039827686495173e+24\n",
      "Gradient Descent(40/49): loss=3.3536193152635248e+25\n",
      "Gradient Descent(41/49): loss=1.5975905963272085e+26\n",
      "Gradient Descent(42/49): loss=7.610570770083346e+26\n",
      "Gradient Descent(43/49): loss=3.6255087867695316e+27\n",
      "Gradient Descent(44/49): loss=1.7271127698611502e+28\n",
      "Gradient Descent(45/49): loss=8.227585961735768e+28\n",
      "Gradient Descent(46/49): loss=3.9194412744220585e+29\n",
      "Gradient Descent(47/49): loss=1.8671357522227816e+30\n",
      "Gradient Descent(48/49): loss=8.894624700666301e+30\n",
      "Gradient Descent(49/49): loss=4.23720388148098e+31\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.541033663240474\n",
      "Gradient Descent(2/49): loss=0.9783608078673985\n",
      "Gradient Descent(3/49): loss=3.1421353325611014\n",
      "Gradient Descent(4/49): loss=13.506926523852307\n",
      "Gradient Descent(5/49): loss=63.0294411901527\n",
      "Gradient Descent(6/49): loss=299.5825452812893\n",
      "Gradient Descent(7/49): loss=1429.4799758725808\n",
      "Gradient Descent(8/49): loss=6826.411879035262\n",
      "Gradient Descent(9/49): loss=32604.721607725387\n",
      "Gradient Descent(10/49): loss=155734.14982249154\n",
      "Gradient Descent(11/49): loss=743858.6540318328\n",
      "Gradient Descent(12/49): loss=3553020.0019565164\n",
      "Gradient Descent(13/49): loss=16970905.86493768\n",
      "Gradient Descent(14/49): loss=81061087.48863555\n",
      "Gradient Descent(15/49): loss=387186167.2787474\n",
      "Gradient Descent(16/49): loss=1849384621.1097608\n",
      "Gradient Descent(17/49): loss=8833537372.819162\n",
      "Gradient Descent(18/49): loss=42193160706.70987\n",
      "Gradient Descent(19/49): loss=201534530883.16354\n",
      "Gradient Descent(20/49): loss=962624426760.8442\n",
      "Gradient Descent(21/49): loss=4597950450163.444\n",
      "Gradient Descent(22/49): loss=21961990319840.23\n",
      "Gradient Descent(23/49): loss=104900873560225.97\n",
      "Gradient Descent(24/49): loss=501056284673689.75\n",
      "Gradient Descent(25/49): loss=2393282266299365.5\n",
      "Gradient Descent(26/49): loss=1.1431450280906552e+16\n",
      "Gradient Descent(27/49): loss=5.460202390873923e+16\n",
      "Gradient Descent(28/49): loss=2.6080514210083827e+17\n",
      "Gradient Descent(29/49): loss=1.2457289542952712e+18\n",
      "Gradient Descent(30/49): loss=5.950191836975266e+18\n",
      "Gradient Descent(31/49): loss=2.842093601078427e+19\n",
      "Gradient Descent(32/49): loss=1.3575185907614476e+20\n",
      "Gradient Descent(33/49): loss=6.484152117874224e+20\n",
      "Gradient Descent(34/49): loss=3.097138335626756e+21\n",
      "Gradient Descent(35/49): loss=1.4793400425580516e+22\n",
      "Gradient Descent(36/49): loss=7.066029102871134e+22\n",
      "Gradient Descent(37/49): loss=3.3750703588260506e+23\n",
      "Gradient Descent(38/49): loss=1.6120935480435053e+24\n",
      "Gradient Descent(39/49): loss=7.70012275698889e+24\n",
      "Gradient Descent(40/49): loss=3.6779435377467502e+25\n",
      "Gradient Descent(41/49): loss=1.7567601314635245e+26\n",
      "Gradient Descent(42/49): loss=8.39111891693272e+26\n",
      "Gradient Descent(43/49): loss=4.00799605006109e+27\n",
      "Gradient Descent(44/49): loss=1.9144088525415934e+28\n",
      "Gradient Descent(45/49): loss=9.144123918569688e+28\n",
      "Gradient Descent(46/49): loss=4.367666923768648e+29\n",
      "Gradient Descent(47/49): loss=2.0862047066359726e+30\n",
      "Gradient Descent(48/49): loss=9.964702331822409e+30\n",
      "Gradient Descent(49/49): loss=4.75961406116953e+31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5397103941687115\n",
      "Gradient Descent(2/49): loss=0.971145700538184\n",
      "Gradient Descent(3/49): loss=3.080543718265175\n",
      "Gradient Descent(4/49): loss=13.056010371435033\n",
      "Gradient Descent(5/49): loss=60.107346462043914\n",
      "Gradient Descent(6/49): loss=281.973712088217\n",
      "Gradient Descent(7/49): loss=1328.1259651749078\n",
      "Gradient Descent(8/49): loss=6260.9516379990055\n",
      "Gradient Descent(9/49): loss=29520.228105560884\n",
      "Gradient Descent(10/49): loss=139192.42813208353\n",
      "Gradient Descent(11/49): loss=656319.04984087\n",
      "Gradient Descent(12/49): loss=3094675.727319781\n",
      "Gradient Descent(13/49): loss=14592020.923673997\n",
      "Gradient Descent(14/49): loss=68804330.28676406\n",
      "Gradient Descent(15/49): loss=324426340.07905275\n",
      "Gradient Descent(16/49): loss=1529735847.4116502\n",
      "Gradient Descent(17/49): loss=7213014097.375073\n",
      "Gradient Descent(18/49): loss=34010821191.7367\n",
      "Gradient Descent(19/49): loss=160367904811.77875\n",
      "Gradient Descent(20/49): loss=756167125424.927\n",
      "Gradient Descent(21/49): loss=3565481024680.881\n",
      "Gradient Descent(22/49): loss=16811964590790.162\n",
      "Gradient Descent(23/49): loss=79271815344268.97\n",
      "Gradient Descent(24/49): loss=373782651875110.8\n",
      "Gradient Descent(25/49): loss=1762460847351983.2\n",
      "Gradient Descent(26/49): loss=8310359570905244.0\n",
      "Gradient Descent(27/49): loss=3.918502717464554e+16\n",
      "Gradient Descent(28/49): loss=1.847653331455613e+17\n",
      "Gradient Descent(29/49): loss=8.712059374168109e+17\n",
      "Gradient Descent(30/49): loss=4.1079123040486917e+18\n",
      "Gradient Descent(31/49): loss=1.9369637846810354e+19\n",
      "Gradient Descent(32/49): loss=9.133176235208696e+19\n",
      "Gradient Descent(33/49): loss=4.306477426325057e+20\n",
      "Gradient Descent(34/49): loss=2.0305912582692788e+21\n",
      "Gradient Descent(35/49): loss=9.57464872091111e+21\n",
      "Gradient Descent(36/49): loss=4.514640637573738e+22\n",
      "Gradient Descent(37/49): loss=2.1287444250479813e+23\n",
      "Gradient Descent(38/49): loss=1.0037460765887683e+24\n",
      "Gradient Descent(39/49): loss=4.7328658828766216e+24\n",
      "Gradient Descent(40/49): loss=2.2316420445123065e+25\n",
      "Gradient Descent(41/49): loss=1.0522643865429187e+26\n",
      "Gradient Descent(42/49): loss=4.961639533138119e+26\n",
      "Gradient Descent(43/49): loss=2.3395134503865435e+27\n",
      "Gradient Descent(44/49): loss=1.1031279374456677e+28\n",
      "Gradient Descent(45/49): loss=5.201471469087211e+28\n",
      "Gradient Descent(46/49): loss=2.452599061753047e+29\n",
      "Gradient Descent(47/49): loss=1.156450091759808e+30\n",
      "Gradient Descent(48/49): loss=5.452896217677531e+30\n",
      "Gradient Descent(49/49): loss=2.5711509188878907e+31\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5471852882477047\n",
      "Gradient Descent(2/49): loss=1.033191665459448\n",
      "Gradient Descent(3/49): loss=3.5189034542456508\n",
      "Gradient Descent(4/49): loss=15.875808358509735\n",
      "Gradient Descent(5/49): loss=77.17756654968761\n",
      "Gradient Descent(6/49): loss=381.22975367565533\n",
      "Gradient Descent(7/49): loss=1889.2667179791288\n",
      "Gradient Descent(8/49): loss=9368.793185550054\n",
      "Gradient Descent(9/49): loss=46465.54886867676\n",
      "Gradient Descent(10/49): loss=230457.02601958366\n",
      "Gradient Descent(11/49): loss=1143012.9180926005\n",
      "Gradient Descent(12/49): loss=5669082.636343089\n",
      "Gradient Descent(13/49): loss=28117359.23967182\n",
      "Gradient Descent(14/49): loss=139455706.58534142\n",
      "Gradient Descent(15/49): loss=691668591.7191949\n",
      "Gradient Descent(16/49): loss=3430518931.9346395\n",
      "Gradient Descent(17/49): loss=17014593820.507269\n",
      "Gradient Descent(18/49): loss=84388516326.9169\n",
      "Gradient Descent(19/49): loss=418547851514.1925\n",
      "Gradient Descent(20/49): loss=2075902168122.0479\n",
      "Gradient Descent(21/49): loss=10296002705607.4\n",
      "Gradient Descent(22/49): loss=51065832167701.51\n",
      "Gradient Descent(23/49): loss=253274915473719.62\n",
      "Gradient Descent(24/49): loss=1256185987482899.2\n",
      "Gradient Descent(25/49): loss=6230396848408556.0\n",
      "Gradient Descent(26/49): loss=3.0901351611508484e+16\n",
      "Gradient Descent(27/49): loss=1.5326367720251824e+17\n",
      "Gradient Descent(28/49): loss=7.601529876411418e+17\n",
      "Gradient Descent(29/49): loss=3.7701859642596623e+18\n",
      "Gradient Descent(30/49): loss=1.8699265064009654e+19\n",
      "Gradient Descent(31/49): loss=9.274410261159451e+19\n",
      "Gradient Descent(32/49): loss=4.599896594751813e+20\n",
      "Gradient Descent(33/49): loss=2.2814441119800227e+21\n",
      "Gradient Descent(34/49): loss=1.1315444008082458e+22\n",
      "Gradient Descent(35/49): loss=5.612202921285922e+22\n",
      "Gradient Descent(36/49): loss=2.783525030674237e+23\n",
      "Gradient Descent(37/49): loss=1.3805651194477361e+24\n",
      "Gradient Descent(38/49): loss=6.847289059851042e+24\n",
      "Gradient Descent(39/49): loss=3.3960996702503638e+25\n",
      "Gradient Descent(40/49): loss=1.6843882110806562e+26\n",
      "Gradient Descent(41/49): loss=8.354182506717535e+26\n",
      "Gradient Descent(42/49): loss=4.143484554001291e+27\n",
      "Gradient Descent(43/49): loss=2.055074118316452e+28\n",
      "Gradient Descent(44/49): loss=1.0192700314752533e+29\n",
      "Gradient Descent(45/49): loss=5.055347579943536e+29\n",
      "Gradient Descent(46/49): loss=2.5073374439402918e+30\n",
      "Gradient Descent(47/49): loss=1.2435823567754042e+31\n",
      "Gradient Descent(48/49): loss=6.167885706092862e+31\n",
      "Gradient Descent(49/49): loss=3.0591310560298665e+32\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5482679809224477\n",
      "Gradient Descent(2/49): loss=1.062030365932591\n",
      "Gradient Descent(3/49): loss=3.7540887115350663\n",
      "Gradient Descent(4/49): loss=17.47272416570154\n",
      "Gradient Descent(5/49): loss=87.24403533763459\n",
      "Gradient Descent(6/49): loss=442.02438429009607\n",
      "Gradient Descent(7/49): loss=2246.0042009518743\n",
      "Gradient Descent(8/49): loss=11418.810230432859\n",
      "Gradient Descent(9/49): loss=58060.30803159771\n",
      "Gradient Descent(10/49): loss=295221.00820732396\n",
      "Gradient Descent(11/49): loss=1501125.608250222\n",
      "Gradient Descent(12/49): loss=7632857.801111702\n",
      "Gradient Descent(13/49): loss=38811227.68594313\n",
      "Gradient Descent(14/49): loss=197345670.81146112\n",
      "Gradient Descent(15/49): loss=1003454833.9772558\n",
      "Gradient Descent(16/49): loss=5102324266.5562935\n",
      "Gradient Descent(17/49): loss=25944080436.82769\n",
      "Gradient Descent(18/49): loss=131919351766.14636\n",
      "Gradient Descent(19/49): loss=670777883723.351\n",
      "Gradient Descent(20/49): loss=3410742724773.426\n",
      "Gradient Descent(21/49): loss=17342798886012.281\n",
      "Gradient Descent(22/49): loss=88183922820115.95\n",
      "Gradient Descent(23/49): loss=448393843176964.7\n",
      "Gradient Descent(24/49): loss=2279973856562738.0\n",
      "Gradient Descent(25/49): loss=1.1593113656018596e+16\n",
      "Gradient Descent(26/49): loss=5.8948169012773736e+16\n",
      "Gradient Descent(27/49): loss=2.997371312886692e+17\n",
      "Gradient Descent(28/49): loss=1.5240905591773893e+18\n",
      "Gradient Descent(29/49): loss=7.749630559907368e+18\n",
      "Gradient Descent(30/49): loss=3.940499037502411e+19\n",
      "Gradient Descent(31/49): loss=2.0036481151616274e+20\n",
      "Gradient Descent(32/49): loss=1.0188064331910994e+21\n",
      "Gradient Descent(33/49): loss=5.180383423901893e+21\n",
      "Gradient Descent(34/49): loss=2.6340992306635717e+22\n",
      "Gradient Descent(35/49): loss=1.3393755228558524e+23\n",
      "Gradient Descent(36/49): loss=6.810399435003355e+23\n",
      "Gradient Descent(37/49): loss=3.4629228078916714e+24\n",
      "Gradient Descent(38/49): loss=1.7608121943306293e+25\n",
      "Gradient Descent(39/49): loss=8.953302616615646e+25\n",
      "Gradient Descent(40/49): loss=4.552537062316878e+26\n",
      "Gradient Descent(41/49): loss=2.3148545951419054e+27\n",
      "Gradient Descent(42/49): loss=1.1770473745297747e+28\n",
      "Gradient Descent(43/49): loss=5.9850002017189925e+28\n",
      "Gradient Descent(44/49): loss=3.0432273321952287e+29\n",
      "Gradient Descent(45/49): loss=1.547407231959672e+30\n",
      "Gradient Descent(46/49): loss=7.86819018148687e+30\n",
      "Gradient Descent(47/49): loss=4.0007837273478206e+31\n",
      "Gradient Descent(48/49): loss=2.0343014166933097e+32\n",
      "Gradient Descent(49/49): loss=1.0343928929904517e+33\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5490112515409157\n",
      "Gradient Descent(2/49): loss=1.0587942527548961\n",
      "Gradient Descent(3/49): loss=3.735944698692078\n",
      "Gradient Descent(4/49): loss=17.41426291275914\n",
      "Gradient Descent(5/49): loss=87.16378942606572\n",
      "Gradient Descent(6/49): loss=442.7686656187462\n",
      "Gradient Descent(7/49): loss=2255.7102601356323\n",
      "Gradient Descent(8/49): loss=11498.40002946854\n",
      "Gradient Descent(9/49): loss=58619.20370351591\n",
      "Gradient Descent(10/49): loss=298849.0479263653\n",
      "Gradient Descent(11/49): loss=1523581.492983532\n",
      "Gradient Descent(12/49): loss=7767474.971437671\n",
      "Gradient Descent(13/49): loss=39599901.64406224\n",
      "Gradient Descent(14/49): loss=201887005.24002585\n",
      "Gradient Descent(15/49): loss=1029254150.8248353\n",
      "Gradient Descent(16/49): loss=5247312015.087323\n",
      "Gradient Descent(17/49): loss=26751685546.480385\n",
      "Gradient Descent(18/49): loss=136384624652.32971\n",
      "Gradient Descent(19/49): loss=695311920043.6702\n",
      "Gradient Descent(20/49): loss=3544817954282.7197\n",
      "Gradient Descent(21/49): loss=18072082423411.348\n",
      "Gradient Descent(22/49): loss=92134537606933.31\n",
      "Gradient Descent(23/49): loss=469717480319079.25\n",
      "Gradient Descent(24/49): loss=2394699284849983.0\n",
      "Gradient Descent(25/49): loss=1.2208582616438944e+16\n",
      "Gradient Descent(26/49): loss=6.224142231359582e+16\n",
      "Gradient Descent(27/49): loss=3.173173146572354e+17\n",
      "Gradient Descent(28/49): loss=1.6177374237041536e+18\n",
      "Gradient Descent(29/49): loss=8.247499430908505e+18\n",
      "Gradient Descent(30/49): loss=4.204714922591516e+19\n",
      "Gradient Descent(31/49): loss=2.1436348954456667e+20\n",
      "Gradient Descent(32/49): loss=1.0928613828925694e+21\n",
      "Gradient Descent(33/49): loss=5.571592460802624e+21\n",
      "Gradient Descent(34/49): loss=2.8404922193434536e+22\n",
      "Gradient Descent(35/49): loss=1.4481310513849731e+23\n",
      "Gradient Descent(36/49): loss=7.382817413490631e+23\n",
      "Gradient Descent(37/49): loss=3.763885382390738e+24\n",
      "Gradient Descent(38/49): loss=1.9188925281949322e+25\n",
      "Gradient Descent(39/49): loss=9.782839169304151e+25\n",
      "Gradient Descent(40/49): loss=4.98745712989458e+26\n",
      "Gradient Descent(41/49): loss=2.542690132388783e+27\n",
      "Gradient Descent(42/49): loss=1.2963065026854545e+28\n",
      "Gradient Descent(43/49): loss=6.608790145128335e+28\n",
      "Gradient Descent(44/49): loss=3.3692731689507885e+29\n",
      "Gradient Descent(45/49): loss=1.7177125370488273e+30\n",
      "Gradient Descent(46/49): loss=8.757189494532803e+30\n",
      "Gradient Descent(47/49): loss=4.464563551181418e+31\n",
      "Gradient Descent(48/49): loss=2.2761101281389117e+32\n",
      "Gradient Descent(49/49): loss=1.1603995006512185e+33\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5476988181800158\n",
      "Gradient Descent(2/49): loss=1.0509620371823372\n",
      "Gradient Descent(3/49): loss=3.662679935029519\n",
      "Gradient Descent(4/49): loss=16.838097002313155\n",
      "Gradient Descent(5/49): loss=83.17047143537681\n",
      "Gradient Descent(6/49): loss=417.0594011725831\n",
      "Gradient Descent(7/49): loss=2097.672582657039\n",
      "Gradient Descent(8/49): loss=10556.923252132061\n",
      "Gradient Descent(9/49): loss=53135.9527484359\n",
      "Gradient Descent(10/49): loss=267454.4113820733\n",
      "Gradient Descent(11/49): loss=1346210.8187012728\n",
      "Gradient Descent(12/49): loss=6776052.886937495\n",
      "Gradient Descent(13/49): loss=34106768.76473959\n",
      "Gradient Descent(14/49): loss=171673943.08286363\n",
      "Gradient Descent(15/49): loss=864108328.4156078\n",
      "Gradient Descent(16/49): loss=4349426540.163735\n",
      "Gradient Descent(17/49): loss=21892522744.577766\n",
      "Gradient Descent(18/49): loss=110194423959.7708\n",
      "Gradient Descent(19/49): loss=554655633507.0159\n",
      "Gradient Descent(20/49): loss=2791818866388.937\n",
      "Gradient Descent(21/49): loss=14052417593681.65\n",
      "Gradient Descent(22/49): loss=70731823831625.06\n",
      "Gradient Descent(23/49): loss=356023500525462.3\n",
      "Gradient Descent(24/49): loss=1792018444599074.2\n",
      "Gradient Descent(25/49): loss=9019994750469064.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=4.540148877579847e+16\n",
      "Gradient Descent(27/49): loss=2.2852509786125718e+17\n",
      "Gradient Descent(28/49): loss=1.1502644904529015e+18\n",
      "Gradient Descent(29/49): loss=5.789772809987712e+18\n",
      "Gradient Descent(30/49): loss=2.9142401134259323e+19\n",
      "Gradient Descent(31/49): loss=1.4668616053552506e+20\n",
      "Gradient Descent(32/49): loss=7.383341404685798e+20\n",
      "Gradient Descent(33/49): loss=3.716351297159025e+21\n",
      "Gradient Descent(34/49): loss=1.870598446813032e+22\n",
      "Gradient Descent(35/49): loss=9.415521487148569e+22\n",
      "Gradient Descent(36/49): loss=4.739234389186739e+23\n",
      "Gradient Descent(37/49): loss=2.3854592256314973e+24\n",
      "Gradient Descent(38/49): loss=1.2007035841345864e+25\n",
      "Gradient Descent(39/49): loss=6.04365432644107e+25\n",
      "Gradient Descent(40/49): loss=3.0420295316962787e+26\n",
      "Gradient Descent(41/49): loss=1.53118348136261e+27\n",
      "Gradient Descent(42/49): loss=7.707100898163804e+27\n",
      "Gradient Descent(43/49): loss=3.8793132878900474e+28\n",
      "Gradient Descent(44/49): loss=1.9526241818354405e+29\n",
      "Gradient Descent(45/49): loss=9.828392069778617e+29\n",
      "Gradient Descent(46/49): loss=4.9470497997462513e+30\n",
      "Gradient Descent(47/49): loss=2.490061603914096e+31\n",
      "Gradient Descent(48/49): loss=1.2533544318889334e+32\n",
      "Gradient Descent(49/49): loss=6.308668546458303e+32\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.555376909445147\n",
      "Gradient Descent(2/49): loss=1.1187652343972625\n",
      "Gradient Descent(3/49): loss=4.173797691899207\n",
      "Gradient Descent(4/49): loss=20.34526057571004\n",
      "Gradient Descent(5/49): loss=105.8136034926962\n",
      "Gradient Descent(6/49): loss=557.4644499269497\n",
      "Gradient Descent(7/49): loss=2944.14218167779\n",
      "Gradient Descent(8/49): loss=15556.138874502285\n",
      "Gradient Descent(9/49): loss=82202.09245299238\n",
      "Gradient Descent(10/49): loss=434381.29442979407\n",
      "Gradient Descent(11/49): loss=2295412.3520809608\n",
      "Gradient Descent(12/49): loss=12129714.249177756\n",
      "Gradient Descent(13/49): loss=64097408.92674416\n",
      "Gradient Descent(14/49): loss=338711847.0746386\n",
      "Gradient Descent(15/49): loss=1789865108.8654077\n",
      "Gradient Descent(16/49): loss=9458237549.17865\n",
      "Gradient Descent(17/49): loss=49980446630.51947\n",
      "Gradient Descent(18/49): loss=264113164050.3889\n",
      "Gradient Descent(19/49): loss=1395661066031.171\n",
      "Gradient Descent(20/49): loss=7375133376030.915\n",
      "Gradient Descent(21/49): loss=38972637152464.73\n",
      "Gradient Descent(22/49): loss=205944268283204.28\n",
      "Gradient Descent(23/49): loss=1088277436109363.1\n",
      "Gradient Descent(24/49): loss=5750816897298234.0\n",
      "Gradient Descent(25/49): loss=3.0389213162853236e+16\n",
      "Gradient Descent(26/49): loss=1.6058662502212525e+17\n",
      "Gradient Descent(27/49): loss=8.485926897086998e+17\n",
      "Gradient Descent(28/49): loss=4.4842436468655345e+18\n",
      "Gradient Descent(29/49): loss=2.3696222378909037e+19\n",
      "Gradient Descent(30/49): loss=1.252186543037659e+20\n",
      "Gradient Descent(31/49): loss=6.616966677187221e+20\n",
      "Gradient Descent(32/49): loss=3.4966234264736975e+21\n",
      "Gradient Descent(33/49): loss=1.8477311407229277e+22\n",
      "Gradient Descent(34/49): loss=9.764020747983005e+22\n",
      "Gradient Descent(35/49): loss=5.159630590505826e+23\n",
      "Gradient Descent(36/49): loss=2.7265189738543636e+24\n",
      "Gradient Descent(37/49): loss=1.4407825491357753e+25\n",
      "Gradient Descent(38/49): loss=7.613570174278428e+25\n",
      "Gradient Descent(39/49): loss=4.0232615833272115e+26\n",
      "Gradient Descent(40/49): loss=2.1260241118629466e+27\n",
      "Gradient Descent(41/49): loss=1.1234612591321075e+28\n",
      "Gradient Descent(42/49): loss=5.936739822130761e+28\n",
      "Gradient Descent(43/49): loss=3.1371691216927697e+29\n",
      "Gradient Descent(44/49): loss=1.657783630910455e+30\n",
      "Gradient Descent(45/49): loss=8.760275459525547e+30\n",
      "Gradient Descent(46/49): loss=4.629218475550937e+31\n",
      "Gradient Descent(47/49): loss=2.4462317187846324e+32\n",
      "Gradient Descent(48/49): loss=1.2926695194000045e+33\n",
      "Gradient Descent(49/49): loss=6.830892075980618e+33\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5565820752512873\n",
      "Gradient Descent(2/49): loss=1.1513781446267852\n",
      "Gradient Descent(3/49): loss=4.45569800269449\n",
      "Gradient Descent(4/49): loss=22.382463691038243\n",
      "Gradient Descent(5/49): loss=119.4920250532483\n",
      "Gradient Descent(6/49): loss=645.4660897147004\n",
      "Gradient Descent(7/49): loss=3494.2529739153288\n",
      "Gradient Descent(8/49): loss=18923.852722885145\n",
      "Gradient Descent(9/49): loss=102493.62497389421\n",
      "Gradient Descent(10/49): loss=555124.0620420193\n",
      "Gradient Descent(11/49): loss=3006660.130367005\n",
      "Gradient Descent(12/49): loss=16284664.865021657\n",
      "Gradient Descent(13/49): loss=88200967.45349732\n",
      "Gradient Descent(14/49): loss=477713901.1491067\n",
      "Gradient Descent(15/49): loss=2587393070.6065936\n",
      "Gradient Descent(16/49): loss=14013833152.611423\n",
      "Gradient Descent(17/49): loss=75901694983.84888\n",
      "Gradient Descent(18/49): loss=411098607981.6437\n",
      "Gradient Descent(19/49): loss=2226591455183.863\n",
      "Gradient Descent(20/49): loss=12059660169226.934\n",
      "Gradient Descent(21/49): loss=65317507196328.24\n",
      "Gradient Descent(22/49): loss=353772551338480.2\n",
      "Gradient Descent(23/49): loss=1916102182288625.0\n",
      "Gradient Descent(24/49): loss=1.0377988792744114e+16\n",
      "Gradient Descent(25/49): loss=5.620924206332285e+16\n",
      "Gradient Descent(26/49): loss=3.044403840118031e+17\n",
      "Gradient Descent(27/49): loss=1.6489093966583237e+18\n",
      "Gradient Descent(28/49): loss=8.930819763657528e+18\n",
      "Gradient Descent(29/49): loss=4.837108807347231e+19\n",
      "Gradient Descent(30/49): loss=2.619873901086759e+20\n",
      "Gradient Descent(31/49): loss=1.4189755763132746e+21\n",
      "Gradient Descent(32/49): loss=7.685452667543892e+21\n",
      "Gradient Descent(33/49): loss=4.162593330783092e+22\n",
      "Gradient Descent(34/49): loss=2.2545429640928808e+23\n",
      "Gradient Descent(35/49): loss=1.2211051075663152e+24\n",
      "Gradient Descent(36/49): loss=6.613747031982114e+24\n",
      "Gradient Descent(37/49): loss=3.582136339616993e+25\n",
      "Gradient Descent(38/49): loss=1.9401559650761326e+26\n",
      "Gradient Descent(39/49): loss=1.0508268842784933e+27\n",
      "Gradient Descent(40/49): loss=5.691486460879003e+27\n",
      "Gradient Descent(41/49): loss=3.0826217542588126e+28\n",
      "Gradient Descent(42/49): loss=1.669608975642212e+29\n",
      "Gradient Descent(43/49): loss=9.042932781791371e+29\n",
      "Gradient Descent(44/49): loss=4.8978314377199117e+30\n",
      "Gradient Descent(45/49): loss=2.652762479957883e+31\n",
      "Gradient Descent(46/49): loss=1.43678868179839e+32\n",
      "Gradient Descent(47/49): loss=7.78193197370891e+32\n",
      "Gradient Descent(48/49): loss=4.2148484332180275e+33\n",
      "Gradient Descent(49/49): loss=2.282845362182401e+34\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5573019511766897\n",
      "Gradient Descent(2/49): loss=1.1474703856938333\n",
      "Gradient Descent(3/49): loss=4.433154347324864\n",
      "Gradient Descent(4/49): loss=22.30458979123052\n",
      "Gradient Descent(5/49): loss=119.36652576618089\n",
      "Gradient Descent(6/49): loss=646.45517235188\n",
      "Gradient Descent(7/49): loss=3508.7361506450807\n",
      "Gradient Descent(8/49): loss=19051.922502036745\n",
      "Gradient Descent(9/49): loss=103456.83527287729\n",
      "Gradient Descent(10/49): loss=561804.8758694084\n",
      "Gradient Descent(11/49): loss=3050794.17893585\n",
      "Gradient Descent(12/49): loss=16566871.910717877\n",
      "Gradient Descent(13/49): loss=89963875.66652335\n",
      "Gradient Descent(14/49): loss=488535137.7676466\n",
      "Gradient Descent(15/49): loss=2652915737.0207343\n",
      "Gradient Descent(16/49): loss=14406255287.20745\n",
      "Gradient Descent(17/49): loss=78230977533.21405\n",
      "Gradient Descent(18/49): loss=424821421243.7558\n",
      "Gradient Descent(19/49): loss=2306928094712.0474\n",
      "Gradient Descent(20/49): loss=12527422036756.023\n",
      "Gradient Descent(21/49): loss=68028259418551.64\n",
      "Gradient Descent(22/49): loss=369417112789811.75\n",
      "Gradient Descent(23/49): loss=2006063426999063.2\n",
      "Gradient Descent(24/49): loss=1.0893622233014862e+16\n",
      "Gradient Descent(25/49): loss=5.9156158154559816e+16\n",
      "Gradient Descent(26/49): loss=3.212385166984828e+17\n",
      "Gradient Descent(27/49): loss=1.7444368909323261e+18\n",
      "Gradient Descent(28/49): loss=9.472899133393398e+18\n",
      "Gradient Descent(29/49): loss=5.14411375142874e+19\n",
      "Gradient Descent(30/49): loss=2.7934327089323577e+20\n",
      "Gradient Descent(31/49): loss=1.5169311326301604e+21\n",
      "Gradient Descent(32/49): loss=8.237463726205456e+21\n",
      "Gradient Descent(33/49): loss=4.473229349766035e+22\n",
      "Gradient Descent(34/49): loss=2.4291191416057147e+23\n",
      "Gradient Descent(35/49): loss=1.3190961926474692e+24\n",
      "Gradient Descent(36/49): loss=7.163151183711989e+24\n",
      "Gradient Descent(37/49): loss=3.889840268413805e+25\n",
      "Gradient Descent(38/49): loss=2.1123185767988455e+26\n",
      "Gradient Descent(39/49): loss=1.1470624658089067e+27\n",
      "Gradient Descent(40/49): loss=6.228948203739227e+27\n",
      "Gradient Descent(41/49): loss=3.3825355533278815e+28\n",
      "Gradient Descent(42/49): loss=1.8368344695271165e+29\n",
      "Gradient Descent(43/49): loss=9.974650126363017e+29\n",
      "Gradient Descent(44/49): loss=5.416581994400866e+30\n",
      "Gradient Descent(45/49): loss=2.941392442881146e+31\n",
      "Gradient Descent(46/49): loss=1.5972784150561503e+32\n",
      "Gradient Descent(47/49): loss=8.673777419191474e+32\n",
      "Gradient Descent(48/49): loss=4.7101628625608744e+33\n",
      "Gradient Descent(49/49): loss=2.5577822809656153e+34\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.5560021717492281\n",
      "Gradient Descent(2/49): loss=1.138974347452272\n",
      "Gradient Descent(3/49): loss=4.3463957319518505\n",
      "Gradient Descent(4/49): loss=21.573745883998892\n",
      "Gradient Descent(5/49): loss=113.96130140337118\n",
      "Gradient Descent(6/49): loss=609.355899570045\n",
      "Gradient Descent(7/49): loss=3265.689348689859\n",
      "Gradient Descent(8/49): loss=17509.068651729507\n",
      "Gradient Descent(9/49): loss=93882.68567372816\n",
      "Gradient Descent(10/49): loss=503401.3109077438\n",
      "Gradient Descent(11/49): loss=2699257.8090748354\n",
      "Gradient Descent(12/49): loss=14473534.8702793\n",
      "Gradient Descent(13/49): loss=77607715.27140003\n",
      "Gradient Descent(14/49): loss=416135908.0509757\n",
      "Gradient Descent(15/49): loss=2231338648.9121423\n",
      "Gradient Descent(16/49): loss=11964533876.68231\n",
      "Gradient Descent(17/49): loss=64154345631.2283\n",
      "Gradient Descent(18/49): loss=343998362650.7968\n",
      "Gradient Descent(19/49): loss=1844534027158.6692\n",
      "Gradient Descent(20/49): loss=9890470847393.13\n",
      "Gradient Descent(21/49): loss=53033130396532.8\n",
      "Gradient Descent(22/49): loss=284365927876630.2\n",
      "Gradient Descent(23/49): loss=1524782345158788.5\n",
      "Gradient Descent(24/49): loss=8175948565527908.0\n",
      "Gradient Descent(25/49): loss=4.383978812346254e+16\n",
      "Gradient Descent(26/49): loss=2.350708309019309e+17\n",
      "Gradient Descent(27/49): loss=1.2604599133851784e+18\n",
      "Gradient Descent(28/49): loss=6.758640309200172e+18\n",
      "Gradient Descent(29/49): loss=3.6240120248224625e+19\n",
      "Gradient Descent(30/49): loss=1.9432108464449385e+20\n",
      "Gradient Descent(31/49): loss=1.041958019972694e+21\n",
      "Gradient Descent(32/49): loss=5.587023751805657e+21\n",
      "Gradient Descent(33/49): loss=2.9957861837906347e+22\n",
      "Gradient Descent(34/49): loss=1.6063534464284947e+23\n",
      "Gradient Descent(35/49): loss=8.613336321578562e+23\n",
      "Gradient Descent(36/49): loss=4.618508009776717e+24\n",
      "Gradient Descent(37/49): loss=2.476463874159098e+25\n",
      "Gradient Descent(38/49): loss=1.3278905886993485e+26\n",
      "Gradient Descent(39/49): loss=7.120206492634771e+26\n",
      "Gradient Descent(40/49): loss=3.8178853686594405e+27\n",
      "Gradient Descent(41/49): loss=2.0471665678940143e+28\n",
      "Gradient Descent(42/49): loss=1.0976995252674371e+29\n",
      "Gradient Descent(43/49): loss=5.8859121024623165e+29\n",
      "Gradient Descent(44/49): loss=3.1560514039096375e+30\n",
      "Gradient Descent(45/49): loss=1.6922883472814755e+31\n",
      "Gradient Descent(46/49): loss=9.074122958824425e+31\n",
      "Gradient Descent(47/49): loss=4.865583788019153e+32\n",
      "Gradient Descent(48/49): loss=2.608946969934133e+33\n",
      "Gradient Descent(49/49): loss=1.398928594897267e+34\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5638790984354755\n",
      "Gradient Descent(2/49): loss=1.2128504866499465\n",
      "Gradient Descent(3/49): loss=4.939386228766504\n",
      "Gradient Descent(4/49): loss=25.90526406340369\n",
      "Gradient Descent(5/49): loss=143.72615135906548\n",
      "Gradient Descent(6/49): loss=805.786211027673\n",
      "Gradient Descent(7/49): loss=4526.016970001722\n",
      "Gradient Descent(8/49): loss=25430.63375769882\n",
      "Gradient Descent(9/49): loss=142897.27515044223\n",
      "Gradient Descent(10/49): loss=802962.57152939\n",
      "Gradient Descent(11/49): loss=4511983.214498225\n",
      "Gradient Descent(12/49): loss=25353609.37465199\n",
      "Gradient Descent(13/49): loss=142466298.39641973\n",
      "Gradient Descent(14/49): loss=800542680.0313561\n",
      "Gradient Descent(15/49): loss=4498387274.750997\n",
      "Gradient Descent(16/49): loss=25277213302.16214\n",
      "Gradient Descent(17/49): loss=142037017566.04126\n",
      "Gradient Descent(18/49): loss=798130478946.2448\n",
      "Gradient Descent(19/49): loss=4484832703052.646\n",
      "Gradient Descent(20/49): loss=25201047829840.414\n",
      "Gradient Descent(21/49): loss=141609030653397.97\n",
      "Gradient Descent(22/49): loss=795725546731056.2\n",
      "Gradient Descent(23/49): loss=4471318974495414.0\n",
      "Gradient Descent(24/49): loss=2.5125111860257868e+16\n",
      "Gradient Descent(25/49): loss=1.4118233335426698e+17\n",
      "Gradient Descent(26/49): loss=7.933278610744759e+17\n",
      "Gradient Descent(27/49): loss=4.457845965597839e+18\n",
      "Gradient Descent(28/49): loss=2.5049404701458133e+19\n",
      "Gradient Descent(29/49): loss=1.4075692178235433e+20\n",
      "Gradient Descent(30/49): loss=7.909374001407045e+20\n",
      "Gradient Descent(31/49): loss=4.4444135536627265e+21\n",
      "Gradient Descent(32/49): loss=2.4973925663986965e+22\n",
      "Gradient Descent(33/49): loss=1.403327920634977e+23\n",
      "Gradient Descent(34/49): loss=7.885541421601641e+23\n",
      "Gradient Descent(35/49): loss=4.43102161636312e+24\n",
      "Gradient Descent(36/49): loss=2.489867406046742e+25\n",
      "Gradient Descent(37/49): loss=1.399099403352097e+26\n",
      "Gradient Descent(38/49): loss=7.861780654288526e+26\n",
      "Gradient Descent(39/49): loss=4.4176700317405094e+27\n",
      "Gradient Descent(40/49): loss=2.4823649205593655e+28\n",
      "Gradient Descent(41/49): loss=1.3948836274663767e+29\n",
      "Gradient Descent(42/49): loss=7.83809148308187e+29\n",
      "Gradient Descent(43/49): loss=4.404358678203924e+30\n",
      "Gradient Descent(44/49): loss=2.4748850416125643e+31\n",
      "Gradient Descent(45/49): loss=1.390680554585395e+32\n",
      "Gradient Descent(46/49): loss=7.81447369224801e+32\n",
      "Gradient Descent(47/49): loss=4.3910874345288523e+33\n",
      "Gradient Descent(48/49): loss=2.4674277010881333e+34\n",
      "Gradient Descent(49/49): loss=1.3864901464323374e+35\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5652110162118905\n",
      "Gradient Descent(2/49): loss=1.2495733091069512\n",
      "Gradient Descent(3/49): loss=5.275402029135454\n",
      "Gradient Descent(4/49): loss=28.485186467591937\n",
      "Gradient Descent(5/49): loss=162.14207380220705\n",
      "Gradient Descent(6/49): loss=931.7591750109124\n",
      "Gradient Descent(7/49): loss=5363.297875498038\n",
      "Gradient Descent(8/49): loss=30880.555107511416\n",
      "Gradient Descent(9/49): loss=177811.54077673753\n",
      "Gradient Descent(10/49): loss=1023855.2246621451\n",
      "Gradient Descent(11/49): loss=5895461.51744624\n",
      "Gradient Descent(12/49): loss=33946670.12980137\n",
      "Gradient Descent(13/49): loss=195468405.94792345\n",
      "Gradient Descent(14/49): loss=1125527124.7163825\n",
      "Gradient Descent(15/49): loss=6480900604.190669\n",
      "Gradient Descent(16/49): loss=37317690288.44546\n",
      "Gradient Descent(17/49): loss=214879087579.52844\n",
      "Gradient Descent(18/49): loss=1237295821966.786\n",
      "Gradient Descent(19/49): loss=7124476226621.125\n",
      "Gradient Descent(20/49): loss=41023464722458.914\n",
      "Gradient Descent(21/49): loss=236217316796784.06\n",
      "Gradient Descent(22/49): loss=1360163534020671.0\n",
      "Gradient Descent(23/49): loss=7831961112618998.0\n",
      "Gradient Descent(24/49): loss=4.509723524806939e+16\n",
      "Gradient Descent(25/49): loss=2.59674505245293e+17\n",
      "Gradient Descent(26/49): loss=1.4952324306238853e+18\n",
      "Gradient Descent(27/49): loss=8.60970167047208e+18\n",
      "Gradient Descent(28/49): loss=4.957554513688576e+19\n",
      "Gradient Descent(29/49): loss=2.854610728323454e+20\n",
      "Gradient Descent(30/49): loss=1.6437141311828771e+21\n",
      "Gradient Descent(31/49): loss=9.46467452897539e+21\n",
      "Gradient Descent(32/49): loss=5.449856653295867e+22\n",
      "Gradient Descent(33/49): loss=3.13808334883004e+23\n",
      "Gradient Descent(34/49): loss=1.806940573060566e+24\n",
      "Gradient Descent(35/49): loss=1.040454912005356e+25\n",
      "Gradient Descent(36/49): loss=5.991046081180639e+25\n",
      "Gradient Descent(37/49): loss=3.4497057712622066e+26\n",
      "Gradient Descent(38/49): loss=1.9863759595610678e+27\n",
      "Gradient Descent(39/49): loss=1.1437756476484869e+28\n",
      "Gradient Descent(40/49): loss=6.585977472476015e+28\n",
      "Gradient Descent(41/49): loss=3.792273367345877e+29\n",
      "Gradient Descent(42/49): loss=2.1836298943904886e+30\n",
      "Gradient Descent(43/49): loss=1.2573564861472499e+31\n",
      "Gradient Descent(44/49): loss=7.239987588179823e+31\n",
      "Gradient Descent(45/49): loss=4.16885909879175e+32\n",
      "Gradient Descent(46/49): loss=2.4004718204148356e+33\n",
      "Gradient Descent(47/49): loss=1.3822162908493953e+34\n",
      "Gradient Descent(48/49): loss=7.958943147932105e+34\n",
      "Gradient Descent(49/49): loss=4.582841082931328e+35\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5659057621477962\n",
      "Gradient Descent(2/49): loss=1.2449283401476487\n",
      "Gradient Descent(3/49): loss=5.247685231919848\n",
      "Gradient Descent(4/49): loss=28.3827038667427\n",
      "Gradient Descent(5/49): loss=161.9521385426134\n",
      "Gradient Descent(6/49): loss=933.0545719324881\n",
      "Gradient Descent(7/49): loss=5384.637981803367\n",
      "Gradient Descent(8/49): loss=31083.670194037146\n",
      "Gradient Descent(9/49): loss=179444.4196883158\n",
      "Gradient Descent(10/49): loss=1035932.3989119077\n",
      "Gradient Descent(11/49): loss=5980445.440153791\n",
      "Gradient Descent(12/49): loss=34525165.070261106\n",
      "Gradient Descent(13/49): loss=199314096.17585766\n",
      "Gradient Descent(14/49): loss=1150642123.4904795\n",
      "Gradient Descent(15/49): loss=6642667646.622457\n",
      "Gradient Descent(16/49): loss=38348181917.99733\n",
      "Gradient Descent(17/49): loss=221384409804.90298\n",
      "Gradient Descent(18/49): loss=1278054250650.7522\n",
      "Gradient Descent(19/49): loss=7378219040121.298\n",
      "Gradient Descent(20/49): loss=42594526935223.69\n",
      "Gradient Descent(21/49): loss=245898598966742.44\n",
      "Gradient Descent(22/49): loss=1419574891998728.5\n",
      "Gradient Descent(23/49): loss=8195219014914987.0\n",
      "Gradient Descent(24/49): loss=4.731107536557102e+16\n",
      "Gradient Descent(25/49): loss=2.7312727679065504e+17\n",
      "Gradient Descent(26/49): loss=1.5767663015617254e+18\n",
      "Gradient Descent(27/49): loss=9.102686479923611e+18\n",
      "Gradient Descent(28/49): loss=5.254989345581242e+19\n",
      "Gradient Descent(29/49): loss=3.0337102220402885e+20\n",
      "Gradient Descent(30/49): loss=1.7513637242767477e+21\n",
      "Gradient Descent(31/49): loss=1.0110639020261018e+22\n",
      "Gradient Descent(32/49): loss=5.836881281770325e+22\n",
      "Gradient Descent(33/49): loss=3.369636976377895e+23\n",
      "Gradient Descent(34/49): loss=1.9452945510533663e+24\n",
      "Gradient Descent(35/49): loss=1.123020348152045e+25\n",
      "Gradient Descent(36/49): loss=6.483206883402914e+25\n",
      "Gradient Descent(37/49): loss=3.742761345523902e+26\n",
      "Gradient Descent(38/49): loss=2.160699595351371e+27\n",
      "Gradient Descent(39/49): loss=1.247373879965638e+28\n",
      "Gradient Descent(40/49): loss=7.201100975665777e+28\n",
      "Gradient Descent(41/49): loss=4.157202270674737e+29\n",
      "Gradient Descent(42/49): loss=2.3999567257429152e+30\n",
      "Gradient Descent(43/49): loss=1.3854972431985747e+31\n",
      "Gradient Descent(44/49): loss=7.998488432397162e+31\n",
      "Gradient Descent(45/49): loss=4.617534788845634e+32\n",
      "Gradient Descent(46/49): loss=2.6657071153392216e+33\n",
      "Gradient Descent(47/49): loss=1.5389151895370217e+34\n",
      "Gradient Descent(48/49): loss=8.884171659219915e+34\n",
      "Gradient Descent(49/49): loss=5.128840536965027e+35\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.564620454876348\n",
      "Gradient Descent(2/49): loss=1.23571955878999\n",
      "Gradient Descent(3/49): loss=5.1453939627506715\n",
      "Gradient Descent(4/49): loss=27.462041454248016\n",
      "Gradient Descent(5/49): loss=154.70143920159802\n",
      "Gradient Descent(6/49): loss=880.1048049442553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=5015.66761236582\n",
      "Gradient Descent(8/49): loss=28592.71558078405\n",
      "Gradient Descent(9/49): loss=163006.6270714607\n",
      "Gradient Descent(10/49): loss=929306.9586672441\n",
      "Gradient Descent(11/49): loss=5298022.91236984\n",
      "Gradient Descent(12/49): loss=30204287.870999977\n",
      "Gradient Descent(13/49): loss=172196131.89614326\n",
      "Gradient Descent(14/49): loss=981698632.7276524\n",
      "Gradient Descent(15/49): loss=5596712286.198393\n",
      "Gradient Descent(16/49): loss=31907132575.052727\n",
      "Gradient Descent(17/49): loss=181904135347.90048\n",
      "Gradient Descent(18/49): loss=1037044440741.2942\n",
      "Gradient Descent(19/49): loss=5912241467282.756\n",
      "Gradient Descent(20/49): loss=33705979989132.367\n",
      "Gradient Descent(21/49): loss=192159453113471.6\n",
      "Gradient Descent(22/49): loss=1095510512756920.9\n",
      "Gradient Descent(23/49): loss=6245559425339515.0\n",
      "Gradient Descent(24/49): loss=3.560624209555398e+16\n",
      "Gradient Descent(25/49): loss=2.0299294103638832e+17\n",
      "Gradient Descent(26/49): loss=1.1572727613327122e+18\n",
      "Gradient Descent(27/49): loss=6.597669048415623e+18\n",
      "Gradient Descent(28/49): loss=3.761363641039402e+19\n",
      "Gradient Descent(29/49): loss=2.1443719495949255e+20\n",
      "Gradient Descent(30/49): loss=1.222517016977081e+21\n",
      "Gradient Descent(31/49): loss=6.969629765399965e+21\n",
      "Gradient Descent(32/49): loss=3.973420278996372e+22\n",
      "Gradient Descent(33/49): loss=2.2652664840129555e+23\n",
      "Gradient Descent(34/49): loss=1.29143958687616e+24\n",
      "Gradient Descent(35/49): loss=7.362560733235661e+24\n",
      "Gradient Descent(36/49): loss=4.1974321603153524e+25\n",
      "Gradient Descent(37/49): loss=2.3929767615929536e+26\n",
      "Gradient Descent(38/49): loss=1.3642478455431865e+27\n",
      "Gradient Descent(39/49): loss=7.777644204243244e+27\n",
      "Gradient Descent(40/49): loss=4.434073292870994e+28\n",
      "Gradient Descent(41/49): loss=2.5278870375460786e+29\n",
      "Gradient Descent(42/49): loss=1.4411608587678366e+30\n",
      "Gradient Descent(43/49): loss=8.216129083286164e+30\n",
      "Gradient Descent(44/49): loss=4.684055683481199e+31\n",
      "Gradient Descent(45/49): loss=2.6704032304683744e+32\n",
      "Gradient Descent(46/49): loss=1.5224100427422945e+33\n",
      "Gradient Descent(47/49): loss=8.679334685481497e+33\n",
      "Gradient Descent(48/49): loss=4.948131480196306e+34\n",
      "Gradient Descent(49/49): loss=2.8209541436702308e+35\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5726918552186901\n",
      "Gradient Descent(2/49): loss=1.3159887627112772\n",
      "Gradient Descent(3/49): loss=5.830150597261318\n",
      "Gradient Descent(4/49): loss=32.77640646625919\n",
      "Gradient Descent(5/49): loss=193.49644510277696\n",
      "Gradient Descent(6/49): loss=1152.0755912325926\n",
      "Gradient Descent(7/49): loss=6869.320447691824\n",
      "Gradient Descent(8/49): loss=40968.67073328342\n",
      "Gradient Descent(9/49): loss=244347.41137157698\n",
      "Gradient Descent(10/49): loss=1457359.2412626627\n",
      "Gradient Descent(11/49): loss=8692125.864636717\n",
      "Gradient Descent(12/49): loss=51842445.59006935\n",
      "Gradient Descent(13/49): loss=309203904.61538285\n",
      "Gradient Descent(14/49): loss=1844184897.2088065\n",
      "Gradient Descent(15/49): loss=10999272285.31382\n",
      "Gradient Descent(16/49): loss=65602961509.407875\n",
      "Gradient Descent(17/49): loss=391275754186.3981\n",
      "Gradient Descent(18/49): loss=2333686045454.0874\n",
      "Gradient Descent(19/49): loss=13918804067164.365\n",
      "Gradient Descent(20/49): loss=83015925401588.64\n",
      "Gradient Descent(21/49): loss=495131897613267.3\n",
      "Gradient Descent(22/49): loss=2953115258887739.5\n",
      "Gradient Descent(23/49): loss=1.7613265827375778e+16\n",
      "Gradient Descent(24/49): loss=1.0505080428951778e+17\n",
      "Gradient Descent(25/49): loss=6.265545294116984e+17\n",
      "Gradient Descent(26/49): loss=3.7369592834758093e+18\n",
      "Gradient Descent(27/49): loss=2.2288346872966173e+19\n",
      "Gradient Descent(28/49): loss=1.3293439094353961e+20\n",
      "Gradient Descent(29/49): loss=7.928606099075043e+20\n",
      "Gradient Descent(30/49): loss=4.728858666903505e+21\n",
      "Gradient Descent(31/49): loss=2.8204332529720603e+22\n",
      "Gradient Descent(32/49): loss=1.682191051753184e+23\n",
      "Gradient Descent(33/49): loss=1.003309236840326e+24\n",
      "Gradient Descent(34/49): loss=5.984037447351843e+24\n",
      "Gradient Descent(35/49): loss=3.569059553770246e+25\n",
      "Gradient Descent(36/49): loss=2.128694248729316e+26\n",
      "Gradient Descent(37/49): loss=1.2696171460032127e+27\n",
      "Gradient Descent(38/49): loss=7.572377754050593e+27\n",
      "Gradient Descent(39/49): loss=4.516393389184388e+28\n",
      "Gradient Descent(40/49): loss=2.6937125838653643e+29\n",
      "Gradient Descent(41/49): loss=1.60661104098044e+30\n",
      "Gradient Descent(42/49): loss=9.582310497641626e+30\n",
      "Gradient Descent(43/49): loss=5.715177608712207e+31\n",
      "Gradient Descent(44/49): loss=3.4087034757603106e+32\n",
      "Gradient Descent(45/49): loss=2.0330530704676804e+33\n",
      "Gradient Descent(46/49): loss=1.2125738764695965e+34\n",
      "Gradient Descent(47/49): loss=7.232154572129601e+34\n",
      "Gradient Descent(48/49): loss=4.313474071159948e+35\n",
      "Gradient Descent(49/49): loss=2.5726854116574018e+36\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5741548038042574\n",
      "Gradient Descent(2/49): loss=1.357176185857745\n",
      "Gradient Descent(3/49): loss=6.228587996120656\n",
      "Gradient Descent(4/49): loss=36.02122311237918\n",
      "Gradient Descent(5/49): loss=218.07666254146716\n",
      "Gradient Descent(6/49): loss=1330.5227956714525\n",
      "Gradient Descent(7/49): loss=8128.090348430283\n",
      "Gradient Descent(8/49): loss=49664.42401642551\n",
      "Gradient Descent(9/49): loss=303470.975707132\n",
      "Gradient Descent(10/49): loss=1854348.5242892036\n",
      "Gradient Descent(11/49): loss=11330940.711742107\n",
      "Gradient Descent(12/49): loss=69237381.92813204\n",
      "Gradient Descent(13/49): loss=423073008.5853108\n",
      "Gradient Descent(14/49): loss=2585175325.1584973\n",
      "Gradient Descent(15/49): loss=15796638714.370071\n",
      "Gradient Descent(16/49): loss=96524901917.6941\n",
      "Gradient Descent(17/49): loss=589812608800.2997\n",
      "Gradient Descent(18/49): loss=3604032810077.9556\n",
      "Gradient Descent(19/49): loss=22022337776985.848\n",
      "Gradient Descent(20/49): loss=134566855165009.97\n",
      "Gradient Descent(21/49): loss=822266858876569.8\n",
      "Gradient Descent(22/49): loss=5024437751611704.0\n",
      "Gradient Descent(23/49): loss=3.070168090480043e+16\n",
      "Gradient Descent(24/49): loss=1.876017291840918e+17\n",
      "Gradient Descent(25/49): loss=1.1463349157328512e+18\n",
      "Gradient Descent(26/49): loss=7.004646197790366e+18\n",
      "Gradient Descent(27/49): loss=4.280168708361451e+19\n",
      "Gradient Descent(28/49): loss=2.615384653948053e+20\n",
      "Gradient Descent(29/49): loss=1.598123194243343e+21\n",
      "Gradient Descent(30/49): loss=9.765285347694995e+21\n",
      "Gradient Descent(31/49): loss=5.967049240346995e+22\n",
      "Gradient Descent(32/49): loss=3.646148102075674e+23\n",
      "Gradient Descent(33/49): loss=2.227968204515274e+24\n",
      "Gradient Descent(34/49): loss=1.3613934983894967e+25\n",
      "Gradient Descent(35/49): loss=8.318755418955437e+25\n",
      "Gradient Descent(36/49): loss=5.083151330035372e+26\n",
      "Gradient Descent(37/49): loss=3.106044851993594e+27\n",
      "Gradient Descent(38/49): loss=1.8979396827299974e+28\n",
      "Gradient Descent(39/49): loss=1.1597305289938734e+29\n",
      "Gradient Descent(40/49): loss=7.086499703435243e+29\n",
      "Gradient Descent(41/49): loss=4.330185055174419e+30\n",
      "Gradient Descent(42/49): loss=2.64594699735418e+31\n",
      "Gradient Descent(43/49): loss=1.616798225388003e+32\n",
      "Gradient Descent(44/49): loss=9.879398582933417e+32\n",
      "Gradient Descent(45/49): loss=6.036777801202998e+33\n",
      "Gradient Descent(46/49): loss=3.6887555366023656e+34\n",
      "Gradient Descent(47/49): loss=2.2540033535942193e+35\n",
      "Gradient Descent(48/49): loss=1.3773022005935205e+36\n",
      "Gradient Descent(49/49): loss=8.415965081573139e+36\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5748226844542347\n",
      "Gradient Descent(2/49): loss=1.3517245427458688\n",
      "Gradient Descent(3/49): loss=6.194820379640764\n",
      "Gradient Descent(4/49): loss=35.88779826402603\n",
      "Gradient Descent(5/49): loss=217.7965719710334\n",
      "Gradient Descent(6/49): loss=1332.1948368159242\n",
      "Gradient Descent(7/49): loss=8159.161757009015\n",
      "Gradient Descent(8/49): loss=49982.19715429107\n",
      "Gradient Descent(9/49): loss=306196.5199129646\n",
      "Gradient Descent(10/49): loss=1875804.8035589547\n",
      "Gradient Descent(11/49): loss=11491466.282162704\n",
      "Gradient Descent(12/49): loss=70398485.86614534\n",
      "Gradient Descent(13/49): loss=431271938.5961538\n",
      "Gradient Descent(14/49): loss=2642038156.5904894\n",
      "Gradient Descent(15/49): loss=16185531682.520096\n",
      "Gradient Descent(16/49): loss=99155053920.8725\n",
      "Gradient Descent(17/49): loss=607439095057.4321\n",
      "Gradient Descent(18/49): loss=3721265226681.5957\n",
      "Gradient Descent(19/49): loss=22797042534788.742\n",
      "Gradient Descent(20/49): loss=139658185234075.33\n",
      "Gradient Descent(21/49): loss=855567500613798.9\n",
      "Gradient Descent(22/49): loss=5241337962967800.0\n",
      "Gradient Descent(23/49): loss=3.210924166981438e+16\n",
      "Gradient Descent(24/49): loss=1.967061479139487e+17\n",
      "Gradient Descent(25/49): loss=1.2050520851608855e+18\n",
      "Gradient Descent(26/49): loss=7.382334224682422e+18\n",
      "Gradient Descent(27/49): loss=4.522531372379737e+19\n",
      "Gradient Descent(28/49): loss=2.7705722054380362e+20\n",
      "Gradient Descent(29/49): loss=1.697295101682539e+21\n",
      "Gradient Descent(30/49): loss=1.039789057488255e+22\n",
      "Gradient Descent(31/49): loss=6.369907525217902e+22\n",
      "Gradient Descent(32/49): loss=3.902303220793973e+23\n",
      "Gradient Descent(33/49): loss=2.3906109102420783e+24\n",
      "Gradient Descent(34/49): loss=1.4645249743062336e+25\n",
      "Gradient Descent(35/49): loss=8.97190501046233e+25\n",
      "Gradient Descent(36/49): loss=5.496326858808993e+26\n",
      "Gradient Descent(37/49): loss=3.367134282366692e+27\n",
      "Gradient Descent(38/49): loss=2.0627581959246467e+28\n",
      "Gradient Descent(39/49): loss=1.263677364201701e+29\n",
      "Gradient Descent(40/49): loss=7.74148169160435e+29\n",
      "Gradient Descent(41/49): loss=4.742550628760041e+30\n",
      "Gradient Descent(42/49): loss=2.905359382396329e+31\n",
      "Gradient Descent(43/49): loss=1.7798677972332608e+32\n",
      "Gradient Descent(44/49): loss=1.0903743594759994e+33\n",
      "Gradient Descent(45/49): loss=6.67980085740536e+33\n",
      "Gradient Descent(46/49): loss=4.092148637467638e+34\n",
      "Gradient Descent(47/49): loss=2.50691312938823e+35\n",
      "Gradient Descent(48/49): loss=1.5357735006879517e+36\n",
      "Gradient Descent(49/49): loss=9.408384430101489e+36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5735536675613757\n",
      "Gradient Descent(2/49): loss=1.3417518515615192\n",
      "Gradient Descent(3/49): loss=6.074720770527666\n",
      "Gradient Descent(4/49): loss=34.735573244765234\n",
      "Gradient Descent(5/49): loss=208.1525494570712\n",
      "Gradient Descent(6/49): loss=1257.401105847921\n",
      "Gradient Descent(7/49): loss=7605.8166034710475\n",
      "Gradient Descent(8/49): loss=46016.55186893943\n",
      "Gradient Descent(9/49): loss=278418.62119967944\n",
      "Gradient Descent(10/49): loss=1684554.8023079399\n",
      "Gradient Descent(11/49): loss=10192305.994340701\n",
      "Gradient Descent(12/49): loss=61667996.2543316\n",
      "Gradient Descent(13/49): loss=373118887.2458562\n",
      "Gradient Descent(14/49): loss=2257535725.9450545\n",
      "Gradient Descent(15/49): loss=13659098298.921272\n",
      "Gradient Descent(16/49): loss=82643638468.80779\n",
      "Gradient Descent(17/49): loss=500030882716.7377\n",
      "Gradient Descent(18/49): loss=3025409920279.11\n",
      "Gradient Descent(19/49): loss=18305079750273.66\n",
      "Gradient Descent(20/49): loss=110753898973468.42\n",
      "Gradient Descent(21/49): loss=670110499662918.0\n",
      "Gradient Descent(22/49): loss=4054467480788737.5\n",
      "Gradient Descent(23/49): loss=2.4531337087006464e+16\n",
      "Gradient Descent(24/49): loss=1.4842553359418493e+17\n",
      "Gradient Descent(25/49): loss=8.980406956450493e+17\n",
      "Gradient Descent(26/49): loss=5.433546853465676e+18\n",
      "Gradient Descent(27/49): loss=3.2875382543327154e+19\n",
      "Gradient Descent(28/49): loss=1.9891073115174165e+20\n",
      "Gradient Descent(29/49): loss=1.203498663937247e+21\n",
      "Gradient Descent(30/49): loss=7.281703836249052e+21\n",
      "Gradient Descent(31/49): loss=4.405755681138761e+22\n",
      "Gradient Descent(32/49): loss=2.665678742007918e+23\n",
      "Gradient Descent(33/49): loss=1.6128545634096953e+24\n",
      "Gradient Descent(34/49): loss=9.758489654878888e+24\n",
      "Gradient Descent(35/49): loss=5.904321598784382e+25\n",
      "Gradient Descent(36/49): loss=3.572377978024766e+26\n",
      "Gradient Descent(37/49): loss=2.1614480519665073e+27\n",
      "Gradient Descent(38/49): loss=1.3077725005832067e+28\n",
      "Gradient Descent(39/49): loss=7.912607067866469e+28\n",
      "Gradient Descent(40/49): loss=4.7874802828878784e+29\n",
      "Gradient Descent(41/49): loss=2.8966391560272923e+30\n",
      "Gradient Descent(42/49): loss=1.7525959177776856e+31\n",
      "Gradient Descent(43/49): loss=1.0603987191914121e+32\n",
      "Gradient Descent(44/49): loss=6.4158853290528784e+32\n",
      "Gradient Descent(45/49): loss=3.881896857339144e+33\n",
      "Gradient Descent(46/49): loss=2.348720782583561e+34\n",
      "Gradient Descent(47/49): loss=1.4210808574448408e+35\n",
      "Gradient Descent(48/49): loss=8.598173177378625e+35\n",
      "Gradient Descent(49/49): loss=5.202278364449945e+36\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5818151797947909\n",
      "Gradient Descent(2/49): loss=1.428738290025417\n",
      "Gradient Descent(3/49): loss=6.861950304596164\n",
      "Gradient Descent(4/49): loss=41.215278123352576\n",
      "Gradient Descent(5/49): loss=258.31369682899026\n",
      "Gradient Descent(6/49): loss=1630.296210471102\n",
      "Gradient Descent(7/49): loss=10300.803385780573\n",
      "Gradient Descent(8/49): loss=65095.88025192827\n",
      "Gradient Descent(9/49): loss=411384.9969434331\n",
      "Gradient Descent(10/49): loss=2599832.8561517172\n",
      "Gradient Descent(11/49): loss=16430195.821925607\n",
      "Gradient Descent(12/49): loss=103834124.67758185\n",
      "Gradient Descent(13/49): loss=656201897.5813864\n",
      "Gradient Descent(14/49): loss=4147007867.3592186\n",
      "Gradient Descent(15/49): loss=26207900839.672993\n",
      "Gradient Descent(16/49): loss=165626419931.17728\n",
      "Gradient Descent(17/49): loss=1046711491605.9863\n",
      "Gradient Descent(18/49): loss=6614916552076.734\n",
      "Gradient Descent(19/49): loss=41804376221981.69\n",
      "Gradient Descent(20/49): loss=264191673099874.78\n",
      "Gradient Descent(21/49): loss=1669615634609356.5\n",
      "Gradient Descent(22/49): loss=1.0551492159551224e+16\n",
      "Gradient Descent(23/49): loss=6.668240551012816e+16\n",
      "Gradient Descent(24/49): loss=4.214136860815602e+17\n",
      "Gradient Descent(25/49): loss=2.663213683703615e+18\n",
      "Gradient Descent(26/49): loss=1.6830746981704434e+19\n",
      "Gradient Descent(27/49): loss=1.0636549582766439e+20\n",
      "Gradient Descent(28/49): loss=6.72199440402922e+20\n",
      "Gradient Descent(29/49): loss=4.248107754887954e+21\n",
      "Gradient Descent(30/49): loss=2.6846823148680324e+22\n",
      "Gradient Descent(31/49): loss=1.6966422575962266e+23\n",
      "Gradient Descent(32/49): loss=1.072229266874253e+24\n",
      "Gradient Descent(33/49): loss=6.776181576253678e+24\n",
      "Gradient Descent(34/49): loss=4.282352494277206e+25\n",
      "Gradient Descent(35/49): loss=2.7063240084220167e+26\n",
      "Gradient Descent(36/49): loss=1.7103191874908016e+27\n",
      "Gradient Descent(37/49): loss=1.0808726944726716e+28\n",
      "Gradient Descent(38/49): loss=6.8308055607479695e+28\n",
      "Gradient Descent(39/49): loss=4.316873286498324e+29\n",
      "Gradient Descent(40/49): loss=2.7281401594517703e+30\n",
      "Gradient Descent(41/49): loss=1.7241063695086528e+31\n",
      "Gradient Descent(42/49): loss=1.0895857982522614e+32\n",
      "Gradient Descent(43/49): loss=6.885869878732295e+32\n",
      "Gradient Descent(44/49): loss=4.3516723568615243e+33\n",
      "Gradient Descent(45/49): loss=2.750132174289504e+34\n",
      "Gradient Descent(46/49): loss=1.7380046924114037e+35\n",
      "Gradient Descent(47/49): loss=1.0983691398848649e+36\n",
      "Gradient Descent(48/49): loss=6.941378079811634e+36\n",
      "Gradient Descent(49/49): loss=4.386751948615384e+37\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5834134380283876\n",
      "Gradient Descent(2/49): loss=1.4747645094701485\n",
      "Gradient Descent(3/49): loss=7.332101438370093\n",
      "Gradient Descent(4/49): loss=45.269974850893966\n",
      "Gradient Descent(5/49): loss=290.8544877688009\n",
      "Gradient Descent(6/49): loss=1880.588884796132\n",
      "Gradient Descent(7/49): loss=12171.405288619926\n",
      "Gradient Descent(8/49): loss=78786.95544661241\n",
      "Gradient Descent(9/49): loss=510009.57452152635\n",
      "Gradient Descent(10/49): loss=3301444.282498884\n",
      "Gradient Descent(11/49): loss=21371247.40803907\n",
      "Gradient Descent(12/49): loss=138342561.4509276\n",
      "Gradient Descent(13/49): loss=895533341.068428\n",
      "Gradient Descent(14/49): loss=5797058826.094101\n",
      "Gradient Descent(15/49): loss=37526119358.22756\n",
      "Gradient Descent(16/49): loss=242917947949.45648\n",
      "Gradient Descent(17/49): loss=1572481526088.1646\n",
      "Gradient Descent(18/49): loss=10179149670766.049\n",
      "Gradient Descent(19/49): loss=65892721981700.63\n",
      "Gradient Descent(20/49): loss=426543567055254.44\n",
      "Gradient Descent(21/49): loss=2761145831048682.5\n",
      "Gradient Descent(22/49): loss=1.7873734101656132e+16\n",
      "Gradient Descent(23/49): loss=1.1570209988342699e+17\n",
      "Gradient Descent(24/49): loss=7.489747716563741e+17\n",
      "Gradient Descent(25/49): loss=4.848340774652305e+18\n",
      "Gradient Descent(26/49): loss=3.138477977725655e+19\n",
      "Gradient Descent(27/49): loss=2.0316319488444557e+20\n",
      "Gradient Descent(28/49): loss=1.3151369564672143e+21\n",
      "Gradient Descent(29/49): loss=8.51328024866717e+21\n",
      "Gradient Descent(30/49): loss=5.510904414627223e+22\n",
      "Gradient Descent(31/49): loss=3.567375509799814e+23\n",
      "Gradient Descent(32/49): loss=2.309270324874688e+24\n",
      "Gradient Descent(33/49): loss=1.4948606948434412e+25\n",
      "Gradient Descent(34/49): loss=9.676686496671183e+25\n",
      "Gradient Descent(35/49): loss=6.26401255166219e+26\n",
      "Gradient Descent(36/49): loss=4.054885239991954e+27\n",
      "Gradient Descent(37/49): loss=2.624850153779106e+28\n",
      "Gradient Descent(38/49): loss=1.6991450859920843e+29\n",
      "Gradient Descent(39/49): loss=1.0999081296486107e+30\n",
      "Gradient Descent(40/49): loss=7.120038798574606e+30\n",
      "Gradient Descent(41/49): loss=4.609016983027785e+31\n",
      "Gradient Descent(42/49): loss=2.9835564314749677e+32\n",
      "Gradient Descent(43/49): loss=1.931346534971516e+33\n",
      "Gradient Descent(44/49): loss=1.2502191675665718e+34\n",
      "Gradient Descent(45/49): loss=8.093047719030342e+34\n",
      "Gradient Descent(46/49): loss=5.238875157384323e+35\n",
      "Gradient Descent(47/49): loss=3.391282724074581e+36\n",
      "Gradient Descent(48/49): loss=2.195280125810199e+37\n",
      "Gradient Descent(49/49): loss=1.4210713829801314e+38\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5840527180960056\n",
      "Gradient Descent(2/49): loss=1.4684327132841433\n",
      "Gradient Descent(3/49): loss=7.291291039185173\n",
      "Gradient Descent(4/49): loss=45.09793807327539\n",
      "Gradient Descent(5/49): loss=290.45017737343716\n",
      "Gradient Descent(6/49): loss=1882.71550263764\n",
      "Gradient Descent(7/49): loss=12216.140541179278\n",
      "Gradient Descent(8/49): loss=79277.76474152222\n",
      "Gradient Descent(9/49): loss=514492.9607322678\n",
      "Gradient Descent(10/49): loss=3338944.1829232844\n",
      "Gradient Descent(11/49): loss=21669013.889312524\n",
      "Gradient Descent(12/49): loss=140627151.4239557\n",
      "Gradient Descent(13/49): loss=912639409.1729041\n",
      "Gradient Descent(14/49): loss=5922829872.626892\n",
      "Gradient Descent(15/49): loss=38437868628.91958\n",
      "Gradient Descent(16/49): loss=249453348587.84506\n",
      "Gradient Descent(17/49): loss=1618897596097.6982\n",
      "Gradient Descent(18/49): loss=10506290821483.209\n",
      "Gradient Descent(19/49): loss=68183526303145.055\n",
      "Gradient Descent(20/49): loss=442496151888902.3\n",
      "Gradient Descent(21/49): loss=2871703108547738.0\n",
      "Gradient Descent(22/49): loss=1.863672420300122e+16\n",
      "Gradient Descent(23/49): loss=1.2094825819037408e+17\n",
      "Gradient Descent(24/49): loss=7.849277051022523e+17\n",
      "Gradient Descent(25/49): loss=5.094008888224913e+18\n",
      "Gradient Descent(26/49): loss=3.3059001975136104e+19\n",
      "Gradient Descent(27/49): loss=2.1454568210871176e+20\n",
      "Gradient Descent(28/49): loss=1.392354486264039e+21\n",
      "Gradient Descent(29/49): loss=9.036075657012198e+21\n",
      "Gradient Descent(30/49): loss=5.864215189792128e+22\n",
      "Gradient Descent(31/49): loss=3.80574721787574e+23\n",
      "Gradient Descent(32/49): loss=2.469846589460203e+24\n",
      "Gradient Descent(33/49): loss=1.6028763410285421e+25\n",
      "Gradient Descent(34/49): loss=1.0402316385126415e+26\n",
      "Gradient Descent(35/49): loss=6.750875498407727e+26\n",
      "Gradient Descent(36/49): loss=4.381170338191706e+27\n",
      "Gradient Descent(37/49): loss=2.843283591406487e+28\n",
      "Gradient Descent(38/49): loss=1.8452287761306644e+29\n",
      "Gradient Descent(39/49): loss=1.1975130607975593e+30\n",
      "Gradient Descent(40/49): loss=7.771597480653948e+30\n",
      "Gradient Descent(41/49): loss=5.04359654842361e+31\n",
      "Gradient Descent(42/49): loss=3.273183693133035e+32\n",
      "Gradient Descent(43/49): loss=2.1242245263136108e+33\n",
      "Gradient Descent(44/49): loss=1.378575191993925e+34\n",
      "Gradient Descent(45/49): loss=8.946651055193119e+34\n",
      "Gradient Descent(46/49): loss=5.806180581823604e+35\n",
      "Gradient Descent(47/49): loss=3.7680840284004755e+36\n",
      "Gradient Descent(48/49): loss=2.445404004404464e+37\n",
      "Gradient Descent(49/49): loss=1.5870136386782816e+38\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5828018098043112\n",
      "Gradient Descent(2/49): loss=1.4576426590569014\n",
      "Gradient Descent(3/49): loss=7.150850701761071\n",
      "Gradient Descent(4/49): loss=43.66480401944531\n",
      "Gradient Descent(5/49): loss=277.7252279268755\n",
      "Gradient Descent(6/49): loss=1778.093107938859\n",
      "Gradient Descent(7/49): loss=11395.777420025714\n",
      "Gradient Descent(8/49): loss=73047.34468905385\n",
      "Gradient Descent(9/49): loss=468248.24194565153\n",
      "Gradient Descent(10/49): loss=3001578.238549651\n",
      "Gradient Descent(11/49): loss=19240815.1728754\n",
      "Gradient Descent(12/49): loss=123338117.03548945\n",
      "Gradient Descent(13/49): loss=790626137.3295679\n",
      "Gradient Descent(14/49): loss=5068098215.323997\n",
      "Gradient Descent(15/49): loss=32487693386.035553\n",
      "Gradient Descent(16/49): loss=208253703221.48972\n",
      "Gradient Descent(17/49): loss=1334954882476.1729\n",
      "Gradient Descent(18/49): loss=8557372621393.568\n",
      "Gradient Descent(19/49): loss=54854757372458.08\n",
      "Gradient Descent(20/49): loss=351631807976768.44\n",
      "Gradient Descent(21/49): loss=2254042024859861.5\n",
      "Gradient Descent(22/49): loss=1.4448935888558868e+16\n",
      "Gradient Descent(23/49): loss=9.262105409266496e+16\n",
      "Gradient Descent(24/49): loss=5.937225915736212e+17\n",
      "Gradient Descent(25/49): loss=3.8059004963625897e+18\n",
      "Gradient Descent(26/49): loss=2.439671118092672e+19\n",
      "Gradient Descent(27/49): loss=1.5638861736254174e+20\n",
      "Gradient Descent(28/49): loss=1.0024875672458746e+21\n",
      "Gradient Descent(29/49): loss=6.426179471571051e+21\n",
      "Gradient Descent(30/49): loss=4.119331146848337e+22\n",
      "Gradient Descent(31/49): loss=2.6405874863072297e+23\n",
      "Gradient Descent(32/49): loss=1.6926782587452448e+24\n",
      "Gradient Descent(33/49): loss=1.0850463021907538e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(34/49): loss=6.955400247005937e+25\n",
      "Gradient Descent(35/49): loss=4.458574025677436e+26\n",
      "Gradient Descent(36/49): loss=2.858050095823395e+27\n",
      "Gradient Descent(37/49): loss=1.8320768710339055e+28\n",
      "Gradient Descent(38/49): loss=1.1744040688028564e+29\n",
      "Gradient Descent(39/49): loss=7.528204403575883e+29\n",
      "Gradient Descent(40/49): loss=4.825754869854147e+30\n",
      "Gradient Descent(41/49): loss=3.093421593714892e+31\n",
      "Gradient Descent(42/49): loss=1.9829554990949627e+32\n",
      "Gradient Descent(43/49): loss=1.271120793680403e+33\n",
      "Gradient Descent(44/49): loss=8.148181201565677e+33\n",
      "Gradient Descent(45/49): loss=5.2231744790606725e+34\n",
      "Gradient Descent(46/49): loss=3.3481768463210536e+35\n",
      "Gradient Descent(47/49): loss=2.1462595667025314e+36\n",
      "Gradient Descent(48/49): loss=1.3758025155462249e+37\n",
      "Gradient Descent(49/49): loss=8.819215490749938e+37\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.591249072163778\n",
      "Gradient Descent(2/49): loss=1.5516741829869896\n",
      "Gradient Descent(3/49): loss=8.052107657758846\n",
      "Gradient Descent(4/49): loss=51.51902835015365\n",
      "Gradient Descent(5/49): loss=342.09396247361065\n",
      "Gradient Descent(6/49): loss=2284.666150030682\n",
      "Gradient Descent(7/49): loss=15271.494946592838\n",
      "Gradient Descent(8/49): loss=102093.6928975495\n",
      "Gradient Descent(9/49): loss=682535.6900152527\n",
      "Gradient Descent(10/49): loss=4563029.241109052\n",
      "Gradient Descent(11/49): loss=30505726.239516117\n",
      "Gradient Descent(12/49): loss=203943337.98825908\n",
      "Gradient Descent(13/49): loss=1363445190.5617385\n",
      "Gradient Descent(14/49): loss=9115192537.374447\n",
      "Gradient Descent(15/49): loss=60938815588.180084\n",
      "Gradient Descent(16/49): loss=407401075774.01794\n",
      "Gradient Descent(17/49): loss=2723643952423.572\n",
      "Gradient Descent(18/49): loss=18208681372536.85\n",
      "Gradient Descent(19/49): loss=121732533002988.39\n",
      "Gradient Descent(20/49): loss=813832110527019.6\n",
      "Gradient Descent(21/49): loss=5440802781197574.0\n",
      "Gradient Descent(22/49): loss=3.637400702304245e+16\n",
      "Gradient Descent(23/49): loss=2.4317521515108435e+17\n",
      "Gradient Descent(24/49): loss=1.6257264487334418e+18\n",
      "Gradient Descent(25/49): loss=1.0868650756489998e+19\n",
      "Gradient Descent(26/49): loss=7.266140583403818e+19\n",
      "Gradient Descent(27/49): loss=4.8577141874083994e+20\n",
      "Gradient Descent(28/49): loss=3.247581966752226e+21\n",
      "Gradient Descent(29/49): loss=2.1711422747168746e+22\n",
      "Gradient Descent(30/49): loss=1.451498014621908e+23\n",
      "Gradient Descent(31/49): loss=9.703861930126558e+23\n",
      "Gradient Descent(32/49): loss=6.487431288942354e+24\n",
      "Gradient Descent(33/49): loss=4.337114958126718e+25\n",
      "Gradient Descent(34/49): loss=2.8995399445800138e+26\n",
      "Gradient Descent(35/49): loss=1.9384618511118207e+27\n",
      "Gradient Descent(36/49): loss=1.2959415700548775e+28\n",
      "Gradient Descent(37/49): loss=8.66390304267803e+28\n",
      "Gradient Descent(38/49): loss=5.792175948931616e+29\n",
      "Gradient Descent(39/49): loss=3.8723081338883363e+30\n",
      "Gradient Descent(40/49): loss=2.5887974426163043e+31\n",
      "Gradient Descent(41/49): loss=1.730717692697417e+32\n",
      "Gradient Descent(42/49): loss=1.1570560456011025e+33\n",
      "Gradient Descent(43/49): loss=7.735396120990145e+33\n",
      "Gradient Descent(44/49): loss=5.171430837435709e+34\n",
      "Gradient Descent(45/49): loss=3.457314465617551e+35\n",
      "Gradient Descent(46/49): loss=2.311357086638599e+36\n",
      "Gradient Descent(47/49): loss=1.5452373902008327e+37\n",
      "Gradient Descent(48/49): loss=1.0330548256164057e+38\n",
      "Gradient Descent(49/49): loss=6.90639690378341e+38\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5929869188842817\n",
      "Gradient Descent(2/49): loss=1.6029334226415095\n",
      "Gradient Descent(3/49): loss=8.604335640910872\n",
      "Gradient Descent(4/49): loss=56.55483909224802\n",
      "Gradient Descent(5/49): loss=384.8421561271708\n",
      "Gradient Descent(6/49): loss=2632.467191976025\n",
      "Gradient Descent(7/49): loss=18021.011797922518\n",
      "Gradient Descent(8/49): loss=123380.14588949688\n",
      "Gradient Descent(9/49): loss=844731.8242983399\n",
      "Gradient Descent(10/49): loss=5783537.566077384\n",
      "Gradient Descent(11/49): loss=39597561.141421035\n",
      "Gradient Descent(12/49): loss=271108629.7962818\n",
      "Gradient Descent(13/49): loss=1856172141.5583212\n",
      "Gradient Descent(14/49): loss=12708466813.018532\n",
      "Gradient Descent(15/49): loss=87009779515.18338\n",
      "Gradient Descent(16/49): loss=595721092323.626\n",
      "Gradient Descent(17/49): loss=4078663591832.144\n",
      "Gradient Descent(18/49): loss=27924975143102.72\n",
      "Gradient Descent(19/49): loss=191191114242595.7\n",
      "Gradient Descent(20/49): loss=1309008941923985.8\n",
      "Gradient Descent(21/49): loss=8962259657437678.0\n",
      "Gradient Descent(22/49): loss=6.1361000368169624e+16\n",
      "Gradient Descent(23/49): loss=4.201141799164281e+17\n",
      "Gradient Descent(24/49): loss=2.8763534347202033e+18\n",
      "Gradient Descent(25/49): loss=1.969323930716288e+19\n",
      "Gradient Descent(26/49): loss=1.3483171773253004e+20\n",
      "Gradient Descent(27/49): loss=9.231387392978186e+20\n",
      "Gradient Descent(28/49): loss=6.320361012405617e+21\n",
      "Gradient Descent(29/49): loss=4.32729790513643e+22\n",
      "Gradient Descent(30/49): loss=2.9627274649412583e+23\n",
      "Gradient Descent(31/49): loss=2.0284607678843235e+24\n",
      "Gradient Descent(32/49): loss=1.388805799904184e+25\n",
      "Gradient Descent(33/49): loss=9.508596766499115e+25\n",
      "Gradient Descent(34/49): loss=6.510155161658681e+26\n",
      "Gradient Descent(35/49): loss=4.457242353381978e+27\n",
      "Gradient Descent(36/49): loss=3.0516952213041037e+28\n",
      "Gradient Descent(37/49): loss=2.0893734254015114e+29\n",
      "Gradient Descent(38/49): loss=1.4305102555125094e+30\n",
      "Gradient Descent(39/49): loss=9.794130461543586e+30\n",
      "Gradient Descent(40/49): loss=6.705648640272747e+31\n",
      "Gradient Descent(41/49): loss=4.59108890404805e+32\n",
      "Gradient Descent(42/49): loss=3.143334590822773e+33\n",
      "Gradient Descent(43/49): loss=2.152115229385131e+34\n",
      "Gradient Descent(44/49): loss=1.4734670544057805e+35\n",
      "Gradient Descent(45/49): loss=1.0088238449200288e+36\n",
      "Gradient Descent(46/49): loss=6.907012593434967e+36\n",
      "Gradient Descent(47/49): loss=4.7289547333857e+37\n",
      "Gradient Descent(48/49): loss=3.2377257993805823e+38\n",
      "Gradient Descent(49/49): loss=2.2167411072825808e+39\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5935958630731087\n",
      "Gradient Descent(2/49): loss=1.5956438647241997\n",
      "Gradient Descent(3/49): loss=8.555365474835977\n",
      "Gradient Descent(4/49): loss=56.33495929731836\n",
      "Gradient Descent(5/49): loss=384.26900428636765\n",
      "Gradient Descent(6/49): loss=2635.131359413141\n",
      "Gradient Descent(7/49): loss=18084.74008709397\n",
      "Gradient Descent(8/49): loss=124129.076382926\n",
      "Gradient Descent(9/49): loss=852005.727262627\n",
      "Gradient Descent(10/49): loss=5848071.707926105\n",
      "Gradient Descent(11/49): loss=40140525.0566406\n",
      "Gradient Descent(12/49): loss=275520195.61496556\n",
      "Gradient Descent(13/49): loss=1891140659.575629\n",
      "Gradient Descent(14/49): loss=12980583847.80954\n",
      "Gradient Descent(15/49): loss=89097316072.72137\n",
      "Gradient Descent(16/49): loss=611554289459.56\n",
      "Gradient Descent(17/49): loss=4197642145079.347\n",
      "Gradient Descent(18/49): loss=28812159250411.68\n",
      "Gradient Descent(19/49): loss=197763528185584.22\n",
      "Gradient Descent(20/49): loss=1357427353517572.2\n",
      "Gradient Descent(21/49): loss=9317233753782058.0\n",
      "Gradient Descent(22/49): loss=6.395247937037527e+16\n",
      "Gradient Descent(23/49): loss=4.3896286448304634e+17\n",
      "Gradient Descent(24/49): loss=3.012993370893742e+18\n",
      "Gradient Descent(25/49): loss=2.0680858877984334e+19\n",
      "Gradient Descent(26/49): loss=1.4195116659159323e+20\n",
      "Gradient Descent(27/49): loss=9.743373723305553e+20\n",
      "Gradient Descent(28/49): loss=6.6877457784573e+21\n",
      "Gradient Descent(29/49): loss=4.590395982686309e+22\n",
      "Gradient Descent(30/49): loss=3.150797888541055e+23\n",
      "Gradient Descent(31/49): loss=2.162673410285006e+24\n",
      "Gradient Descent(32/49): loss=1.484435512847031e+25\n",
      "Gradient Descent(33/49): loss=1.0189003949103178e+26\n",
      "Gradient Descent(34/49): loss=6.993621519854955e+26\n",
      "Gradient Descent(35/49): loss=4.8003457656215297e+27\n",
      "Gradient Descent(36/49): loss=3.2949051366449212e+28\n",
      "Gradient Descent(37/49): loss=2.261587058423801e+29\n",
      "Gradient Descent(38/49): loss=1.5523287653853943e+30\n",
      "Gradient Descent(39/49): loss=1.0655015852108668e+31\n",
      "Gradient Descent(40/49): loss=7.313487022866624e+31\n",
      "Gradient Descent(41/49): loss=5.019897968810042e+32\n",
      "Gradient Descent(42/49): loss=3.4456033815980937e+33\n",
      "Gradient Descent(43/49): loss=2.365024695132296e+34\n",
      "Gradient Descent(44/49): loss=1.623327234486104e+35\n",
      "Gradient Descent(45/49): loss=1.1142341623950423e+36\n",
      "Gradient Descent(46/49): loss=7.647982133689804e+36\n",
      "Gradient Descent(47/49): loss=5.2494917757242825e+37\n",
      "Gradient Descent(48/49): loss=3.603194074160538e+38\n",
      "Gradient Descent(49/49): loss=2.473193232934324e+39\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5923648816051544\n",
      "Gradient Descent(2/49): loss=1.5839806674901737\n",
      "Gradient Descent(3/49): loss=8.391774439340173\n",
      "Gradient Descent(4/49): loss=54.562851934325295\n",
      "Gradient Descent(5/49): loss=367.6055829396667\n",
      "Gradient Descent(6/49): loss=2490.1283339849924\n",
      "Gradient Descent(7/49): loss=16881.64853888484\n",
      "Gradient Descent(8/49): loss=114461.9931832945\n",
      "Gradient Descent(9/49): loss=776096.7721916025\n",
      "Gradient Descent(10/49): loss=5262252.74997345\n",
      "Gradient Descent(11/49): loss=35680236.71220458\n",
      "Gradient Descent(12/49): loss=241926688.9285668\n",
      "Gradient Descent(13/49): loss=1640362534.119945\n",
      "Gradient Descent(14/49): loss=11122333218.56055\n",
      "Gradient Descent(15/49): loss=75413997634.33687\n",
      "Gradient Descent(16/49): loss=511337947513.82684\n",
      "Gradient Descent(17/49): loss=3467081772251.43\n",
      "Gradient Descent(18/49): loss=23508241611930.85\n",
      "Gradient Descent(19/49): loss=159395555105778.44\n",
      "Gradient Descent(20/49): loss=1080767477504008.4\n",
      "Gradient Descent(21/49): loss=7328048386639458.0\n",
      "Gradient Descent(22/49): loss=4.968718459307063e+16\n",
      "Gradient Descent(23/49): loss=3.368995648673715e+17\n",
      "Gradient Descent(24/49): loss=2.2843177317729108e+18\n",
      "Gradient Descent(25/49): loss=1.5488614542278674e+19\n",
      "Gradient Descent(26/49): loss=1.0501918235914502e+20\n",
      "Gradient Descent(27/49): loss=7.120732866892497e+20\n",
      "Gradient Descent(28/49): loss=4.828150002943478e+21\n",
      "Gradient Descent(29/49): loss=3.2736844488727744e+22\n",
      "Gradient Descent(30/49): loss=2.2196928148996622e+23\n",
      "Gradient Descent(31/49): loss=1.505043100355541e+24\n",
      "Gradient Descent(32/49): loss=1.020481175919022e+25\n",
      "Gradient Descent(33/49): loss=6.919282445526387e+25\n",
      "Gradient Descent(34/49): loss=4.691558324714142e+26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=3.181069668347681e+27\n",
      "Gradient Descent(36/49): loss=2.1568961812913635e+28\n",
      "Gradient Descent(37/49): loss=1.4624643977966461e+29\n",
      "Gradient Descent(38/49): loss=9.916110628663549e+29\n",
      "Gradient Descent(39/49): loss=6.723531194881561e+30\n",
      "Gradient Descent(40/49): loss=4.558830918835577e+31\n",
      "Gradient Descent(41/49): loss=3.09107502354608e+32\n",
      "Gradient Descent(42/49): loss=2.0958761075593468e+33\n",
      "Gradient Descent(43/49): loss=1.4210902759645088e+34\n",
      "Gradient Descent(44/49): loss=9.635577051320112e+34\n",
      "Gradient Descent(45/49): loss=6.533317881505612e+35\n",
      "Gradient Descent(46/49): loss=4.4298584623899895e+36\n",
      "Gradient Descent(47/49): loss=3.003626389029451e+37\n",
      "Gradient Descent(48/49): loss=2.0365823336049928e+38\n",
      "Gradient Descent(49/49): loss=1.380886656443376e+39\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6009935323256511\n",
      "Gradient Descent(2/49): loss=1.685388442941017\n",
      "Gradient Descent(3/49): loss=9.419495126189023\n",
      "Gradient Descent(4/49): loss=64.03034129661387\n",
      "Gradient Descent(5/49): loss=449.61778166556314\n",
      "Gradient Descent(6/49): loss=3172.3237313380864\n",
      "Gradient Descent(7/49): loss=22398.295651383596\n",
      "Gradient Descent(8/49): loss=158160.27867593148\n",
      "Gradient Descent(9/49): loss=1116829.0148784753\n",
      "Gradient Descent(10/49): loss=7886367.666485983\n",
      "Gradient Descent(11/49): loss=55688757.08917881\n",
      "Gradient Descent(12/49): loss=393240336.545704\n",
      "Gradient Descent(13/49): loss=2776825560.8769073\n",
      "Gradient Descent(14/49): loss=19608263690.439148\n",
      "Gradient Descent(15/49): loss=138461706250.67374\n",
      "Gradient Descent(16/49): loss=977732878452.1688\n",
      "Gradient Descent(17/49): loss=6904158611859.233\n",
      "Gradient Descent(18/49): loss=48752995003449.67\n",
      "Gradient Descent(19/49): loss=344264182709469.3\n",
      "Gradient Descent(20/49): loss=2430985573055619.0\n",
      "Gradient Descent(21/49): loss=1.7166150744737914e+16\n",
      "Gradient Descent(22/49): loss=1.2121698074113936e+17\n",
      "Gradient Descent(23/49): loss=8.559610502372979e+17\n",
      "Gradient Descent(24/49): loss=6.044279564164066e+18\n",
      "Gradient Descent(25/49): loss=4.268104890946081e+19\n",
      "Gradient Descent(26/49): loss=3.0138776948906997e+20\n",
      "Gradient Descent(27/49): loss=2.1282182588877887e+21\n",
      "Gradient Descent(28/49): loss=1.5028190975173744e+22\n",
      "Gradient Descent(29/49): loss=1.0612000110567615e+23\n",
      "Gradient Descent(30/49): loss=7.493553051909174e+23\n",
      "Gradient Descent(31/49): loss=5.291494228864458e+24\n",
      "Gradient Descent(32/49): loss=3.736533388119818e+25\n",
      "Gradient Descent(33/49): loss=2.6385140296241524e+26\n",
      "Gradient Descent(34/49): loss=1.8631591267612355e+27\n",
      "Gradient Descent(35/49): loss=1.315650359504884e+28\n",
      "Gradient Descent(36/49): loss=9.290327614014594e+28\n",
      "Gradient Descent(37/49): loss=6.560267821323257e+29\n",
      "Gradient Descent(38/49): loss=4.632464610028082e+30\n",
      "Gradient Descent(39/49): loss=3.271166505338501e+31\n",
      "Gradient Descent(40/49): loss=2.3099000653959903e+32\n",
      "Gradient Descent(41/49): loss=1.6311118077935542e+33\n",
      "Gradient Descent(42/49): loss=1.1517925685964565e+34\n",
      "Gradient Descent(43/49): loss=8.133262935963742e+34\n",
      "Gradient Descent(44/49): loss=5.7432186826947794e+35\n",
      "Gradient Descent(45/49): loss=4.055513893618618e+36\n",
      "Gradient Descent(46/49): loss=2.863758782316923e+37\n",
      "Gradient Descent(47/49): loss=2.022213356537076e+38\n",
      "Gradient Descent(48/49): loss=1.4279648427820567e+39\n",
      "Gradient Descent(49/49): loss=1.0083424608140289e+40\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6028752463719391\n",
      "Gradient Descent(2/49): loss=1.7422954761755576\n",
      "Gradient Descent(3/49): loss=10.065323724835297\n",
      "Gradient Descent(4/49): loss=70.24858688895763\n",
      "Gradient Descent(5/49): loss=505.36659964579843\n",
      "Gradient Descent(6/49): loss=3651.3693973820855\n",
      "Gradient Descent(7/49): loss=26398.007701017417\n",
      "Gradient Descent(8/49): loss=190864.23153260828\n",
      "Gradient Descent(9/49): loss=1380013.8905571124\n",
      "Gradient Descent(10/49): loss=9977992.730237592\n",
      "Gradient Descent(11/49): loss=72144467.51489726\n",
      "Gradient Descent(12/49): loss=521630408.6201503\n",
      "Gradient Descent(13/49): loss=3771575209.2369514\n",
      "Gradient Descent(14/49): loss=27269843439.219547\n",
      "Gradient Descent(15/49): loss=197170762904.53677\n",
      "Gradient Descent(16/49): loss=1425615436058.2566\n",
      "Gradient Descent(17/49): loss=10307711658680.43\n",
      "Gradient Descent(18/49): loss=74528457641011.02\n",
      "Gradient Descent(19/49): loss=538867518055941.3\n",
      "Gradient Descent(20/49): loss=3896205707281392.0\n",
      "Gradient Descent(21/49): loss=2.817096671222331e+16\n",
      "Gradient Descent(22/49): loss=2.0368620784525792e+17\n",
      "Gradient Descent(23/49): loss=1.4727244432254436e+18\n",
      "Gradient Descent(24/49): loss=1.0648326701243455e+19\n",
      "Gradient Descent(25/49): loss=7.699122674170003e+19\n",
      "Gradient Descent(26/49): loss=5.566742232373151e+20\n",
      "Gradient Descent(27/49): loss=4.0249545815981876e+21\n",
      "Gradient Descent(28/49): loss=2.9101867317865486e+22\n",
      "Gradient Descent(29/49): loss=2.1041695358717926e+23\n",
      "Gradient Descent(30/49): loss=1.521390152505037e+24\n",
      "Gradient Descent(31/49): loss=1.1000197259201907e+25\n",
      "Gradient Descent(32/49): loss=7.953537726145686e+25\n",
      "Gradient Descent(33/49): loss=5.7506934530929036e+26\n",
      "Gradient Descent(34/49): loss=4.1579579213829514e+27\n",
      "Gradient Descent(35/49): loss=3.0063529236970265e+28\n",
      "Gradient Descent(36/49): loss=2.173701146743575e+29\n",
      "Gradient Descent(37/49): loss=1.571664004618539e+30\n",
      "Gradient Descent(38/49): loss=1.1363695267466207e+31\n",
      "Gradient Descent(39/49): loss=8.21635984233005e+31\n",
      "Gradient Descent(40/49): loss=5.940723283202407e+32\n",
      "Gradient Descent(41/49): loss=4.295356314089428e+33\n",
      "Gradient Descent(42/49): loss=3.10569689673247e+34\n",
      "Gradient Descent(43/49): loss=2.245530407509013e+35\n",
      "Gradient Descent(44/49): loss=1.623599140132697e+36\n",
      "Gradient Descent(45/49): loss=1.1739204951421043e+37\n",
      "Gradient Descent(46/49): loss=8.487866831476934e+37\n",
      "Gradient Descent(47/49): loss=6.137032588409238e+38\n",
      "Gradient Descent(48/49): loss=4.437294992839037e+39\n",
      "Gradient Descent(49/49): loss=3.208323659656194e+40\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6034521193855442\n",
      "Gradient Descent(2/49): loss=1.7339663031938681\n",
      "Gradient Descent(3/49): loss=10.00694040673391\n",
      "Gradient Descent(4/49): loss=69.96981710965099\n",
      "Gradient Descent(5/49): loss=504.56674100855037\n",
      "Gradient Descent(6/49): loss=3654.654688182102\n",
      "Gradient Descent(7/49): loss=26487.88606021084\n",
      "Gradient Descent(8/49): loss=191994.00254401425\n",
      "Gradient Descent(9/49): loss=1391662.204179635\n",
      "Gradient Descent(10/49): loss=10087438.091146354\n",
      "Gradient Descent(11/49): loss=73118634.16490208\n",
      "Gradient Descent(12/49): loss=529999281.42809755\n",
      "Gradient Descent(13/49): loss=3841691574.3965635\n",
      "Gradient Descent(14/49): loss=27846441859.889973\n",
      "Gradient Descent(15/49): loss=201844502471.42014\n",
      "Gradient Descent(16/49): loss=1463066749612.105\n",
      "Gradient Descent(17/49): loss=10605016671916.115\n",
      "Gradient Descent(18/49): loss=76870299076704.97\n",
      "Gradient Descent(19/49): loss=557193172151525.5\n",
      "Gradient Descent(20/49): loss=4038806077526917.0\n",
      "Gradient Descent(21/49): loss=2.92752232926369e+16\n",
      "Gradient Descent(22/49): loss=2.1220100256919168e+17\n",
      "Gradient Descent(23/49): loss=1.5381356801707325e+18\n",
      "Gradient Descent(24/49): loss=1.1149152652296544e+19\n",
      "Gradient Descent(25/49): loss=8.081446030197614e+19\n",
      "Gradient Descent(26/49): loss=5.857823636986778e+20\n",
      "Gradient Descent(27/49): loss=4.246034389615538e+21\n",
      "Gradient Descent(28/49): loss=3.077731450288542e+22\n",
      "Gradient Descent(29/49): loss=2.2308888743957894e+23\n",
      "Gradient Descent(30/49): loss=1.6170563450025274e+24\n",
      "Gradient Descent(31/49): loss=1.1721207868864107e+25\n",
      "Gradient Descent(32/49): loss=8.496099367824265e+25\n",
      "Gradient Descent(33/49): loss=6.15838446647553e+26\n",
      "Gradient Descent(34/49): loss=4.4638954413075933e+27\n",
      "Gradient Descent(35/49): loss=3.235647696145018e+28\n",
      "Gradient Descent(36/49): loss=2.345354220595691e+29\n",
      "Gradient Descent(37/49): loss=1.7000263738909422e+30\n",
      "Gradient Descent(38/49): loss=1.2322614838072185e+31\n",
      "Gradient Descent(39/49): loss=8.93202827788706e+31\n",
      "Gradient Descent(40/49): loss=6.474366861689216e+32\n",
      "Gradient Descent(41/49): loss=4.692934791027731e+33\n",
      "Gradient Descent(42/49): loss=3.4016665140122075e+34\n",
      "Gradient Descent(43/49): loss=2.465692703566803e+35\n",
      "Gradient Descent(44/49): loss=1.7872535368706048e+36\n",
      "Gradient Descent(45/49): loss=1.2954879577798597e+37\n",
      "Gradient Descent(46/49): loss=9.39032439511207e+37\n",
      "Gradient Descent(47/49): loss=6.806562092368023e+38\n",
      "Gradient Descent(48/49): loss=4.933725989421324e+39\n",
      "Gradient Descent(49/49): loss=3.5762036411869354e+40\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6022428829639054\n",
      "Gradient Descent(2/49): loss=1.7213718159994353\n",
      "Gradient Descent(3/49): loss=9.81708915464396\n",
      "Gradient Descent(4/49): loss=67.79071210369061\n",
      "Gradient Descent(5/49): loss=482.90176371546187\n",
      "Gradient Descent(6/49): loss=3455.432315359868\n",
      "Gradient Descent(7/49): loss=24741.547760232388\n",
      "Gradient Descent(8/49): loss=177170.81722556177\n",
      "Gradient Descent(9/49): loss=1268713.5301253118\n",
      "Gradient Descent(10/49): loss=9085228.965334076\n",
      "Gradient Descent(11/49): loss=65059141.386757284\n",
      "Gradient Descent(12/49): loss=465887224.79605526\n",
      "Gradient Descent(13/49): loss=3336209234.0201836\n",
      "Gradient Descent(14/49): loss=23890528607.031116\n",
      "Gradient Descent(15/49): loss=171079604803.79068\n",
      "Gradient Descent(16/49): loss=1225097680464.6035\n",
      "Gradient Descent(17/49): loss=8772900360732.725\n",
      "Gradient Descent(18/49): loss=62822566695528.4\n",
      "Gradient Descent(19/49): loss=449871162777728.7\n",
      "Gradient Descent(20/49): loss=3221518536163393.0\n",
      "Gradient Descent(21/49): loss=2.3069230787686796e+16\n",
      "Gradient Descent(22/49): loss=1.6519830730800736e+17\n",
      "Gradient Descent(23/49): loss=1.1829818249508803e+18\n",
      "Gradient Descent(24/49): loss=8.471309548922299e+18\n",
      "Gradient Descent(25/49): loss=6.066288083220659e+19\n",
      "Gradient Descent(26/49): loss=4.344056948468684e+20\n",
      "Gradient Descent(27/49): loss=3.110770624912432e+21\n",
      "Gradient Descent(28/49): loss=2.22761671764669e+22\n",
      "Gradient Descent(29/49): loss=1.5951919440793476e+23\n",
      "Gradient Descent(30/49): loss=1.1423138093270666e+24\n",
      "Gradient Descent(31/49): loss=8.18008669002159e+24\n",
      "Gradient Descent(32/49): loss=5.857743967543137e+25\n",
      "Gradient Descent(33/49): loss=4.19471891797242e+26\n",
      "Gradient Descent(34/49): loss=3.0038299554044417e+27\n",
      "Gradient Descent(35/49): loss=2.1510367148382005e+28\n",
      "Gradient Descent(36/49): loss=1.54035315489719e+29\n",
      "Gradient Descent(37/49): loss=1.1030438604020831e+30\n",
      "Gradient Descent(38/49): loss=7.898875359215502e+30\n",
      "Gradient Descent(39/49): loss=5.6563690874158325e+31\n",
      "Gradient Descent(40/49): loss=4.05051476293341e+32\n",
      "Gradient Descent(41/49): loss=2.9005656439999124e+33\n",
      "Gradient Descent(42/49): loss=2.077089344826801e+34\n",
      "Gradient Descent(43/49): loss=1.4873995888758997e+35\n",
      "Gradient Descent(44/49): loss=1.0651239160695131e+36\n",
      "Gradient Descent(45/49): loss=7.62733138470649e+36\n",
      "Gradient Descent(46/49): loss=5.461916982092413e+37\n",
      "Gradient Descent(47/49): loss=3.911267993296638e+38\n",
      "Gradient Descent(48/49): loss=2.800851306518044e+39\n",
      "Gradient Descent(49/49): loss=2.005684104149532e+40\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6110485602804104\n",
      "Gradient Descent(2/49): loss=1.8304899581829281\n",
      "Gradient Descent(3/49): loss=10.984625242155607\n",
      "Gradient Descent(4/49): loss=79.14285639564454\n",
      "Gradient Descent(5/49): loss=586.6888921820313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=4366.581220629891\n",
      "Gradient Descent(7/49): loss=32517.70857253796\n",
      "Gradient Descent(8/49): loss=242177.4550210716\n",
      "Gradient Descent(9/49): loss=1803652.2562122636\n",
      "Gradient Descent(10/49): loss=13432991.820597231\n",
      "Gradient Descent(11/49): loss=100044411.26931645\n",
      "Gradient Descent(12/49): loss=745097216.1972154\n",
      "Gradient Descent(13/49): loss=5549234192.039532\n",
      "Gradient Descent(14/49): loss=41328835374.32396\n",
      "Gradient Descent(15/49): loss=307803306738.62573\n",
      "Gradient Descent(16/49): loss=2292415810662.9194\n",
      "Gradient Descent(17/49): loss=17073144225670.055\n",
      "Gradient Descent(18/49): loss=127155052934117.86\n",
      "Gradient Descent(19/49): loss=947008194447555.1\n",
      "Gradient Descent(20/49): loss=7052999465272297.0\n",
      "Gradient Descent(21/49): loss=5.252837488502757e+16\n",
      "Gradient Descent(22/49): loss=3.912137214312818e+17\n",
      "Gradient Descent(23/49): loss=2.913628608749064e+18\n",
      "Gradient Descent(24/49): loss=2.1699728830222602e+19\n",
      "Gradient Descent(25/49): loss=1.616123036035666e+20\n",
      "Gradient Descent(26/49): loss=1.203634242639693e+21\n",
      "Gradient Descent(27/49): loss=8.964264215975516e+21\n",
      "Gradient Descent(28/49): loss=6.67628338303053e+22\n",
      "Gradient Descent(29/49): loss=4.972271983136652e+23\n",
      "Gradient Descent(30/49): loss=3.7031814343181666e+24\n",
      "Gradient Descent(31/49): loss=2.7580053508713703e+25\n",
      "Gradient Descent(32/49): loss=2.0540698991800034e+26\n",
      "Gradient Descent(33/49): loss=1.5298023803268935e+27\n",
      "Gradient Descent(34/49): loss=1.1393455129195468e+28\n",
      "Gradient Descent(35/49): loss=8.485463315415424e+28\n",
      "Gradient Descent(36/49): loss=6.3196885282634134e+29\n",
      "Gradient Descent(37/49): loss=4.7066920932542013e+30\n",
      "Gradient Descent(38/49): loss=3.5053864382124773e+31\n",
      "Gradient Descent(39/49): loss=2.6106942705717256e+32\n",
      "Gradient Descent(40/49): loss=1.9443575464597343e+33\n",
      "Gradient Descent(41/49): loss=1.4480922990829544e+34\n",
      "Gradient Descent(42/49): loss=1.078490584451145e+35\n",
      "Gradient Descent(43/49): loss=8.032236215097408e+35\n",
      "Gradient Descent(44/49): loss=5.982140182332332e+36\n",
      "Gradient Descent(45/49): loss=4.455297404452742e+37\n",
      "Gradient Descent(46/49): loss=3.3181561041895074e+38\n",
      "Gradient Descent(47/49): loss=2.4712513963189533e+39\n",
      "Gradient Descent(48/49): loss=1.8405051697530356e+40\n",
      "Gradient Descent(49/49): loss=1.3707465314671938e+41\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6130784204913606\n",
      "Gradient Descent(2/49): loss=1.8934806289823611\n",
      "Gradient Descent(3/49): loss=11.736833397966492\n",
      "Gradient Descent(4/49): loss=86.77921912312277\n",
      "Gradient Descent(5/49): loss=658.8907424339639\n",
      "Gradient Descent(6/49): loss=5020.897688061065\n",
      "Gradient Descent(7/49): loss=38279.20567303903\n",
      "Gradient Descent(8/49): loss=291859.64854187926\n",
      "Gradient Descent(9/49): loss=2225304.3122513974\n",
      "Gradient Descent(10/49): loss=16967012.791242726\n",
      "Gradient Descent(11/49): loss=129366390.6520109\n",
      "Gradient Descent(12/49): loss=986364767.9788135\n",
      "Gradient Descent(13/49): loss=7520619974.268478\n",
      "Gradient Descent(14/49): loss=57341590754.76061\n",
      "Gradient Descent(15/49): loss=437205714717.89484\n",
      "Gradient Descent(16/49): loss=3333511234579.8745\n",
      "Gradient Descent(17/49): loss=25416632896321.184\n",
      "Gradient Descent(18/49): loss=193791225626035.34\n",
      "Gradient Descent(19/49): loss=1477577273230746.2\n",
      "Gradient Descent(20/49): loss=1.1265910473063412e+16\n",
      "Gradient Descent(21/49): loss=8.589786881979227e+16\n",
      "Gradient Descent(22/49): loss=6.549354253634438e+17\n",
      "Gradient Descent(23/49): loss=4.99360947238262e+18\n",
      "Gradient Descent(24/49): loss=3.807418960248152e+19\n",
      "Gradient Descent(25/49): loss=2.9029981657616908e+20\n",
      "Gradient Descent(26/49): loss=2.2134150295523418e+21\n",
      "Gradient Descent(27/49): loss=1.6876366478043364e+22\n",
      "Gradient Descent(28/49): loss=1.2867525597258982e+23\n",
      "Gradient Descent(29/49): loss=9.810951617549364e+23\n",
      "Gradient Descent(30/49): loss=7.480441434862884e+24\n",
      "Gradient Descent(31/49): loss=5.703524616339958e+25\n",
      "Gradient Descent(32/49): loss=4.348699649941445e+26\n",
      "Gradient Descent(33/49): loss=3.3157021171123835e+27\n",
      "Gradient Descent(34/49): loss=2.5280845803116335e+28\n",
      "Gradient Descent(35/49): loss=1.9275590567151105e+29\n",
      "Gradient Descent(36/49): loss=1.4696833903660114e+30\n",
      "Gradient Descent(37/49): loss=1.1205722908426426e+31\n",
      "Gradient Descent(38/49): loss=8.54389637411369e+31\n",
      "Gradient Descent(39/49): loss=6.514364655286979e+32\n",
      "Gradient Descent(40/49): loss=4.966931362911652e+33\n",
      "Gradient Descent(41/49): loss=3.7870780144082564e+34\n",
      "Gradient Descent(42/49): loss=2.8874890428940794e+35\n",
      "Gradient Descent(43/49): loss=2.2015899701860637e+36\n",
      "Gradient Descent(44/49): loss=1.6786205332110268e+37\n",
      "Gradient Descent(45/49): loss=1.2798781483727112e+38\n",
      "Gradient Descent(46/49): loss=9.758537098009074e+38\n",
      "Gradient Descent(47/49): loss=7.44047754970252e+39\n",
      "Gradient Descent(48/49): loss=5.673053820630718e+40\n",
      "Gradient Descent(49/49): loss=4.3254669390218794e+41\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6136214870333121\n",
      "Gradient Descent(2/49): loss=1.8840256279870886\n",
      "Gradient Descent(3/49): loss=11.667635097371651\n",
      "Gradient Descent(4/49): loss=86.42841010540043\n",
      "Gradient Descent(5/49): loss=657.7898258112259\n",
      "Gradient Descent(6/49): loss=5024.881289430483\n",
      "Gradient Descent(7/49): loss=38404.76415590438\n",
      "Gradient Descent(8/49): loss=293545.517207707\n",
      "Gradient Descent(9/49): loss=2243728.68295018\n",
      "Gradient Descent(10/49): loss=17150071.016533807\n",
      "Gradient Descent(11/49): loss=131087601.87420581\n",
      "Gradient Descent(12/49): loss=1001976033.642925\n",
      "Gradient Descent(13/49): loss=7658664632.00303\n",
      "Gradient Descent(14/49): loss=58539468078.19637\n",
      "Gradient Descent(15/49): loss=447449978369.61755\n",
      "Gradient Descent(16/49): loss=3420111075966.597\n",
      "Gradient Descent(17/49): loss=26141826656738.03\n",
      "Gradient Descent(18/49): loss=199816639218252.53\n",
      "Gradient Descent(19/49): loss=1527310613476539.0\n",
      "Gradient Descent(20/49): loss=1.1674091402820846e+16\n",
      "Gradient Descent(21/49): loss=8.923162641501062e+16\n",
      "Gradient Descent(22/49): loss=6.820473540873693e+17\n",
      "Gradient Descent(23/49): loss=5.21327036060083e+18\n",
      "Gradient Descent(24/49): loss=3.984794851830415e+19\n",
      "Gradient Descent(25/49): loss=3.045802138169609e+20\n",
      "Gradient Descent(26/49): loss=2.3280773565086255e+21\n",
      "Gradient Descent(27/49): loss=1.7794800620717065e+22\n",
      "Gradient Descent(28/49): loss=1.3601563893304317e+23\n",
      "Gradient Descent(29/49): loss=1.039643794200525e+24\n",
      "Gradient Descent(30/49): loss=7.946580461616898e+24\n",
      "Gradient Descent(31/49): loss=6.074017022485291e+25\n",
      "Gradient Descent(32/49): loss=4.6427117887552494e+26\n",
      "Gradient Descent(33/49): loss=3.5486849433667626e+27\n",
      "Gradient Descent(34/49): loss=2.712458881849793e+28\n",
      "Gradient Descent(35/49): loss=2.0732844146895832e+29\n",
      "Gradient Descent(36/49): loss=1.584727530049509e+30\n",
      "Gradient Descent(37/49): loss=1.2112961090641443e+31\n",
      "Gradient Descent(38/49): loss=9.258615351928056e+31\n",
      "Gradient Descent(39/49): loss=7.07687885674683e+32\n",
      "Gradient Descent(40/49): loss=5.409255320520581e+33\n",
      "Gradient Descent(41/49): loss=4.1345971458427306e+34\n",
      "Gradient Descent(42/49): loss=3.160304431103421e+35\n",
      "Gradient Descent(43/49): loss=2.4155978792986312e+36\n",
      "Gradient Descent(44/49): loss=1.846376905035921e+37\n",
      "Gradient Descent(45/49): loss=1.4112893974057713e+38\n",
      "Gradient Descent(46/49): loss=1.0787276193704298e+39\n",
      "Gradient Descent(47/49): loss=8.245320052227578e+39\n",
      "Gradient Descent(48/49): loss=6.302360442327818e+40\n",
      "Gradient Descent(49/49): loss=4.817247468069759e+41\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6124358138805642\n",
      "Gradient Descent(2/49): loss=1.8704392966467336\n",
      "Gradient Descent(3/49): loss=11.448091478659654\n",
      "Gradient Descent(4/49): loss=83.76294371256795\n",
      "Gradient Descent(5/49): loss=629.8128755357645\n",
      "Gradient Descent(6/49): loss=4753.420879884527\n",
      "Gradient Descent(7/49): loss=35894.440420167644\n",
      "Gradient Descent(8/49): loss=271069.1749322643\n",
      "Gradient Descent(9/49): loss=2047093.9630631786\n",
      "Gradient Descent(10/49): loss=15459524.869572084\n",
      "Gradient Descent(11/49): loss=116749391.0079085\n",
      "Gradient Descent(12/49): loss=881684336.7697681\n",
      "Gradient Descent(13/49): loss=6658426819.486067\n",
      "Gradient Descent(14/49): loss=50284036965.0564\n",
      "Gradient Descent(15/49): loss=379742008563.9834\n",
      "Gradient Descent(16/49): loss=2867788701566.8643\n",
      "Gradient Descent(17/49): loss=21657366979349.086\n",
      "Gradient Descent(18/49): loss=163555126716015.56\n",
      "Gradient Descent(19/49): loss=1235158433645935.5\n",
      "Gradient Descent(20/49): loss=9327841852709578.0\n",
      "Gradient Descent(21/49): loss=7.044329800860536e+16\n",
      "Gradient Descent(22/49): loss=5.31983529811646e+17\n",
      "Gradient Descent(23/49): loss=4.0175074704237394e+18\n",
      "Gradient Descent(24/49): loss=3.03399736465999e+19\n",
      "Gradient Descent(25/49): loss=2.29125647594449e+20\n",
      "Gradient Descent(26/49): loss=1.7303430450230227e+21\n",
      "Gradient Descent(27/49): loss=1.3067446114801846e+22\n",
      "Gradient Descent(28/49): loss=9.86845634190282e+22\n",
      "Gradient Descent(29/49): loss=7.45259859627278e+23\n",
      "Gradient Descent(30/49): loss=5.628157425324119e+24\n",
      "Gradient Descent(31/49): loss=4.250350477761233e+25\n",
      "Gradient Descent(32/49): loss=3.2098389967769626e+26\n",
      "Gradient Descent(33/49): loss=2.4240510139429763e+27\n",
      "Gradient Descent(34/49): loss=1.830628677668301e+28\n",
      "Gradient Descent(35/49): loss=1.3824797152476315e+29\n",
      "Gradient Descent(36/49): loss=1.044040326903189e+30\n",
      "Gradient Descent(37/49): loss=7.884529459478365e+30\n",
      "Gradient Descent(38/49): loss=5.954349003144033e+31\n",
      "Gradient Descent(39/49): loss=4.4966883862195986e+32\n",
      "Gradient Descent(40/49): loss=3.395871896673399e+33\n",
      "Gradient Descent(41/49): loss=2.564541935784716e+34\n",
      "Gradient Descent(42/49): loss=1.9367265728843088e+35\n",
      "Gradient Descent(43/49): loss=1.4626042045861243e+36\n",
      "Gradient Descent(44/49): loss=1.1045498570751664e+37\n",
      "Gradient Descent(45/49): loss=8.341493774865729e+37\n",
      "Gradient Descent(46/49): loss=6.2994456927795425e+38\n",
      "Gradient Descent(47/49): loss=4.7573033208811815e+39\n",
      "Gradient Descent(48/49): loss=3.5926867204852684e+40\n",
      "Gradient Descent(49/49): loss=2.7131753014143303e+41\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.621414156028056\n",
      "Gradient Descent(2/49): loss=1.987604503958576\n",
      "Gradient Descent(3/49): loss=12.76974303883473\n",
      "Gradient Descent(4/49): loss=97.30705914106107\n",
      "Gradient Descent(5/49): loss=760.3165258925341\n",
      "Gradient Descent(6/49): loss=5960.874315491576\n",
      "Gradient Descent(7/49): loss=46754.76234882568\n",
      "Gradient Descent(8/49): loss=366750.1934145552\n",
      "Gradient Descent(9/49): loss=2876863.101222683\n",
      "Gradient Descent(10/49): loss=22566736.252881773\n",
      "Gradient Descent(11/49): loss=177018410.58619037\n",
      "Gradient Descent(12/49): loss=1388571173.2217803\n",
      "Gradient Descent(13/49): loss=10892256452.355515\n",
      "Gradient Descent(14/49): loss=85441245789.00502\n",
      "Gradient Descent(15/49): loss=670219850008.0912\n",
      "Gradient Descent(16/49): loss=5257351332460.744\n",
      "Gradient Descent(17/49): loss=41239815611685.34\n",
      "Gradient Descent(18/49): loss=323494148316575.1\n",
      "Gradient Descent(19/49): loss=2537558969239123.5\n",
      "Gradient Descent(20/49): loss=1.99051684732989e+16\n",
      "Gradient Descent(21/49): loss=1.5614050225175283e+17\n",
      "Gradient Descent(22/49): loss=1.2248003063191078e+18\n",
      "Gradient Descent(23/49): loss=9.60760192727338e+18\n",
      "Gradient Descent(24/49): loss=7.536413431374383e+19\n",
      "Gradient Descent(25/49): loss=5.911727800395993e+20\n",
      "Gradient Descent(26/49): loss=4.637288798473125e+21\n",
      "Gradient Descent(27/49): loss=3.6375909254488656e+22\n",
      "Gradient Descent(28/49): loss=2.8534060128549163e+23\n",
      "Gradient Descent(29/49): loss=2.238274187796949e+24\n",
      "Gradient Descent(30/49): loss=1.7557513081517461e+25\n",
      "Gradient Descent(31/49): loss=1.3772497904337363e+26\n",
      "Gradient Descent(32/49): loss=1.0803449078710947e+27\n",
      "Gradient Descent(33/49): loss=8.474462135118141e+27\n",
      "Gradient Descent(34/49): loss=6.6475537540202315e+28\n",
      "Gradient Descent(35/49): loss=5.214486796685937e+29\n",
      "Gradient Descent(36/49): loss=4.0903576802771565e+30\n",
      "Gradient Descent(37/49): loss=3.2085661743808903e+31\n",
      "Gradient Descent(38/49): loss=2.5168695992091525e+32\n",
      "Gradient Descent(39/49): loss=1.9742876522238254e+33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=1.5486744863334372e+34\n",
      "Gradient Descent(41/49): loss=1.2148141948406483e+35\n",
      "Gradient Descent(42/49): loss=9.529268681117803e+35\n",
      "Gradient Descent(43/49): loss=7.47496711699553e+36\n",
      "Gradient Descent(44/49): loss=5.863527965255159e+37\n",
      "Gradient Descent(45/49): loss=4.599479791845342e+38\n",
      "Gradient Descent(46/49): loss=3.607932712344975e+39\n",
      "Gradient Descent(47/49): loss=2.8301414607555684e+40\n",
      "Gradient Descent(48/49): loss=2.2200249634594305e+41\n",
      "Gradient Descent(49/49): loss=1.7414362167844568e+42\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6235964412425455\n",
      "Gradient Descent(2/49): loss=2.057136248078351\n",
      "Gradient Descent(3/49): loss=13.642464370432052\n",
      "Gradient Descent(4/49): loss=106.63632903567084\n",
      "Gradient Descent(5/49): loss=853.2151748026916\n",
      "Gradient Descent(6/49): loss=6847.519250207818\n",
      "Gradient Descent(7/49): loss=54977.09277557953\n",
      "Gradient Descent(8/49): loss=441421.91757856257\n",
      "Gradient Descent(9/49): loss=3544291.043001494\n",
      "Gradient Descent(10/49): loss=28458065.14479843\n",
      "Gradient Descent(11/49): loss=228497494.99278277\n",
      "Gradient Descent(12/49): loss=1834668191.895024\n",
      "Gradient Descent(13/49): loss=14731047246.836031\n",
      "Gradient Descent(14/49): loss=118279563681.8127\n",
      "Gradient Descent(15/49): loss=949698616366.8542\n",
      "Gradient Descent(16/49): loss=7625387124331.971\n",
      "Gradient Descent(17/49): loss=61226296210608.375\n",
      "Gradient Descent(18/49): loss=491602496575538.06\n",
      "Gradient Descent(19/49): loss=3947209444257221.0\n",
      "Gradient Descent(20/49): loss=3.1693212514921652e+16\n",
      "Gradient Descent(21/49): loss=2.5447337763578714e+17\n",
      "Gradient Descent(22/49): loss=2.0432355948415503e+18\n",
      "Gradient Descent(23/49): loss=1.6405691372567368e+19\n",
      "Gradient Descent(24/49): loss=1.3172573446323764e+20\n",
      "Gradient Descent(25/49): loss=1.0576615593838198e+21\n",
      "Gradient Descent(26/49): loss=8.492250802446002e+21\n",
      "Gradient Descent(27/49): loss=6.818657920560107e+22\n",
      "Gradient Descent(28/49): loss=5.4748849179331295e+23\n",
      "Gradient Descent(29/49): loss=4.3959332193848434e+24\n",
      "Gradient Descent(30/49): loss=3.5296137104168595e+25\n",
      "Gradient Descent(31/49): loss=2.8340223390622874e+26\n",
      "Gradient Descent(32/49): loss=2.2755132083152056e+27\n",
      "Gradient Descent(33/49): loss=1.8270711172058916e+28\n",
      "Gradient Descent(34/49): loss=1.4670048300003505e+29\n",
      "Gradient Descent(35/49): loss=1.1778978666881475e+30\n",
      "Gradient Descent(36/49): loss=9.457660642795336e+30\n",
      "Gradient Descent(37/49): loss=7.593811599793012e+31\n",
      "Gradient Descent(38/49): loss=6.0972767781724155e+32\n",
      "Gradient Descent(39/49): loss=4.89566848230126e+33\n",
      "Gradient Descent(40/49): loss=3.9308646729634247e+34\n",
      "Gradient Descent(41/49): loss=3.156197592425343e+35\n",
      "Gradient Descent(42/49): loss=2.5341964354426868e+36\n",
      "Gradient Descent(43/49): loss=2.0347748787411504e+37\n",
      "Gradient Descent(44/49): loss=1.633775799401597e+38\n",
      "Gradient Descent(45/49): loss=1.3118027898799756e+39\n",
      "Gradient Descent(46/49): loss=1.0532819498043628e+40\n",
      "Gradient Descent(47/49): loss=8.457085732263036e+40\n",
      "Gradient Descent(48/49): loss=6.790422934346434e+41\n",
      "Gradient Descent(49/49): loss=5.4522142836265025e+42\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6241039660164122\n",
      "Gradient Descent(2/49): loss=2.046464731563886\n",
      "Gradient Descent(3/49): loss=13.560888084308324\n",
      "Gradient Descent(4/49): loss=106.19790799746404\n",
      "Gradient Descent(5/49): loss=851.7184742651866\n",
      "Gradient Descent(6/49): loss=6852.261588941232\n",
      "Gradient Descent(7/49): loss=55150.91886270799\n",
      "Gradient Descent(8/49): loss=443911.8364368443\n",
      "Gradient Descent(9/49): loss=3573093.7259452385\n",
      "Gradient Descent(10/49): loss=28760251.528569594\n",
      "Gradient Descent(11/49): loss=231494699.91869065\n",
      "Gradient Descent(12/49): loss=1863328570.6957736\n",
      "Gradient Descent(13/49): loss=14998155000.190912\n",
      "Gradient Descent(14/49): loss=120721947445.97588\n",
      "Gradient Descent(15/49): loss=971705426435.8356\n",
      "Gradient Descent(16/49): loss=7821373460336.381\n",
      "Gradient Descent(17/49): loss=62955172568493.84\n",
      "Gradient Descent(18/49): loss=506733730749597.3\n",
      "Gradient Descent(19/49): loss=4078760543468006.0\n",
      "Gradient Descent(20/49): loss=3.2830432555465292e+16\n",
      "Gradient Descent(21/49): loss=2.642560871844027e+17\n",
      "Gradient Descent(22/49): loss=2.1270289234244974e+18\n",
      "Gradient Descent(23/49): loss=1.7120710782065134e+19\n",
      "Gradient Descent(24/49): loss=1.3780665342867245e+20\n",
      "Gradient Descent(25/49): loss=1.1092222730089016e+21\n",
      "Gradient Descent(26/49): loss=8.928263043379597e+21\n",
      "Gradient Descent(27/49): loss=7.186465951097751e+22\n",
      "Gradient Descent(28/49): loss=5.784472591741476e+23\n",
      "Gradient Descent(29/49): loss=4.655991330411469e+24\n",
      "Gradient Descent(30/49): loss=3.747663235506878e+25\n",
      "Gradient Descent(31/49): loss=3.016539063342435e+26\n",
      "Gradient Descent(32/49): loss=2.4280484528221306e+27\n",
      "Gradient Descent(33/49): loss=1.954365305887873e+28\n",
      "Gradient Descent(34/49): loss=1.573092062647556e+29\n",
      "Gradient Descent(35/49): loss=1.2662006586534895e+30\n",
      "Gradient Descent(36/49): loss=1.0191800887204226e+31\n",
      "Gradient Descent(37/49): loss=8.203502708242029e+31\n",
      "Gradient Descent(38/49): loss=6.603097669286833e+32\n",
      "Gradient Descent(39/49): loss=5.314912468589236e+33\n",
      "Gradient Descent(40/49): loss=4.278036758438011e+34\n",
      "Gradient Descent(41/49): loss=3.443443069798036e+35\n",
      "Gradient Descent(42/49): loss=2.7716686051265917e+36\n",
      "Gradient Descent(43/49): loss=2.23094928562153e+37\n",
      "Gradient Descent(44/49): loss=1.7957178234834155e+38\n",
      "Gradient Descent(45/49): loss=1.44539480227479e+39\n",
      "Gradient Descent(46/49): loss=1.163415603009569e+40\n",
      "Gradient Descent(47/49): loss=9.36447165297597e+40\n",
      "Gradient Descent(48/49): loss=7.537575490009105e+41\n",
      "Gradient Descent(49/49): loss=6.067084868534006e+42\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6229436743551308\n",
      "Gradient Descent(2/49): loss=2.031823554418201\n",
      "Gradient Descent(3/49): loss=13.30787309214603\n",
      "Gradient Descent(4/49): loss=102.95384957170124\n",
      "Gradient Descent(5/49): loss=815.8229391323\n",
      "Gradient Descent(6/49): loss=6485.261901377997\n",
      "Gradient Descent(7/49): loss=51575.56387998886\n",
      "Gradient Descent(8/49): loss=410191.1049699071\n",
      "Gradient Descent(9/49): loss=3262363.1116430247\n",
      "Gradient Descent(10/49): loss=25946510.553237386\n",
      "Gradient Descent(11/49): loss=206360097.35752273\n",
      "Gradient Descent(12/49): loss=1641241566.700218\n",
      "Gradient Descent(13/49): loss=13053269200.27808\n",
      "Gradient Descent(14/49): loss=103816428101.57959\n",
      "Gradient Descent(15/49): loss=825682101721.4512\n",
      "Gradient Descent(16/49): loss=6566888744119.1\n",
      "Gradient Descent(17/49): loss=52228366932542.71\n",
      "Gradient Descent(18/49): loss=415387319435372.94\n",
      "Gradient Descent(19/49): loss=3303695583105138.5\n",
      "Gradient Descent(20/49): loss=2.627524720943797e+16\n",
      "Gradient Descent(21/49): loss=2.0897464628632947e+17\n",
      "Gradient Descent(22/49): loss=1.6620358485080458e+18\n",
      "Gradient Descent(23/49): loss=1.3218652170565143e+19\n",
      "Gradient Descent(24/49): loss=1.05131766780625e+20\n",
      "Gradient Descent(25/49): loss=8.36143371033505e+20\n",
      "Gradient Descent(26/49): loss=6.650090247052933e+21\n",
      "Gradient Descent(27/49): loss=5.289009256784066e+22\n",
      "Gradient Descent(28/49): loss=4.206502149462471e+23\n",
      "Gradient Descent(29/49): loss=3.3455529144207703e+24\n",
      "Gradient Descent(30/49): loss=2.6608150680773155e+25\n",
      "Gradient Descent(31/49): loss=2.1162232395098838e+26\n",
      "Gradient Descent(32/49): loss=1.6830935953312148e+27\n",
      "Gradient Descent(33/49): loss=1.3386130526101838e+28\n",
      "Gradient Descent(34/49): loss=1.0646377061792262e+29\n",
      "Gradient Descent(35/49): loss=8.467371830929139e+29\n",
      "Gradient Descent(36/49): loss=6.734345900683637e+30\n",
      "Gradient Descent(37/49): loss=5.3560202168513305e+31\n",
      "Gradient Descent(38/49): loss=4.259797905600313e+32\n",
      "Gradient Descent(39/49): loss=3.38794057189434e+33\n",
      "Gradient Descent(40/49): loss=2.694527198954089e+34\n",
      "Gradient Descent(41/49): loss=2.143035472975767e+35\n",
      "Gradient Descent(42/49): loss=1.7044181406723798e+36\n",
      "Gradient Descent(43/49): loss=1.3555730807475744e+37\n",
      "Gradient Descent(44/49): loss=1.0781265074558276e+38\n",
      "Gradient Descent(45/49): loss=8.574652171743292e+38\n",
      "Gradient Descent(46/49): loss=6.819669060905143e+39\n",
      "Gradient Descent(47/49): loss=5.423880195808734e+40\n",
      "Gradient Descent(48/49): loss=4.31376891103591e+41\n",
      "Gradient Descent(49/49): loss=3.4308652746791394e+42\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6320903195685876\n",
      "Gradient Descent(2/49): loss=2.157374742464255\n",
      "Gradient Descent(3/49): loss=14.798921026088385\n",
      "Gradient Descent(4/49): loss=119.03666865883518\n",
      "Gradient Descent(5/49): loss=978.9240104612351\n",
      "Gradient Descent(6/49): loss=8073.531284131165\n",
      "Gradient Descent(7/49): loss=66610.83205157863\n",
      "Gradient Descent(8/49): loss=549604.1715799654\n",
      "Gradient Descent(9/49): loss=4534808.023793488\n",
      "Gradient Descent(10/49): loss=37416954.45959301\n",
      "Gradient Descent(11/49): loss=308729469.093651\n",
      "Gradient Descent(12/49): loss=2547344964.42372\n",
      "Gradient Descent(13/49): loss=21018293025.097317\n",
      "Gradient Descent(14/49): loss=173423171593.90106\n",
      "Gradient Descent(15/49): loss=1430924786700.5994\n",
      "Gradient Descent(16/49): loss=11806644559970.41\n",
      "Gradient Descent(17/49): loss=97417318551866.38\n",
      "Gradient Descent(18/49): loss=803796023984498.4\n",
      "Gradient Descent(19/49): loss=6632168261029843.0\n",
      "Gradient Descent(20/49): loss=5.472241032567736e+16\n",
      "Gradient Descent(21/49): loss=4.515178255424804e+17\n",
      "Gradient Descent(22/49): loss=3.725500130007087e+18\n",
      "Gradient Descent(23/49): loss=3.0739320650314383e+19\n",
      "Gradient Descent(24/49): loss=2.53631942308117e+20\n",
      "Gradient Descent(25/49): loss=2.0927320707827683e+21\n",
      "Gradient Descent(26/49): loss=1.726725537890818e+22\n",
      "Gradient Descent(27/49): loss=1.4247313953042681e+23\n",
      "Gradient Descent(28/49): loss=1.1755542523828721e+24\n",
      "Gradient Descent(29/49): loss=9.699567264749735e+24\n",
      "Gradient Descent(30/49): loss=8.003169988343774e+25\n",
      "Gradient Descent(31/49): loss=6.603462619935626e+26\n",
      "Gradient Descent(32/49): loss=5.448555839298297e+27\n",
      "Gradient Descent(33/49): loss=4.495635463177781e+28\n",
      "Gradient Descent(34/49): loss=3.709375257202939e+29\n",
      "Gradient Descent(35/49): loss=3.060627337658599e+30\n",
      "Gradient Descent(36/49): loss=2.5253416142875458e+31\n",
      "Gradient Descent(37/49): loss=2.0836742161922755e+32\n",
      "Gradient Descent(38/49): loss=1.719251848803586e+33\n",
      "Gradient Descent(39/49): loss=1.4185648104894474e+34\n",
      "Gradient Descent(40/49): loss=1.1704661669896283e+35\n",
      "Gradient Descent(41/49): loss=9.657585172965797e+35\n",
      "Gradient Descent(42/49): loss=7.968530317537577e+36\n",
      "Gradient Descent(43/49): loss=6.574881223855216e+37\n",
      "Gradient Descent(44/49): loss=5.424973161319765e+38\n",
      "Gradient Descent(45/49): loss=4.4761772568999926e+39\n",
      "Gradient Descent(46/49): loss=3.6933201767793684e+40\n",
      "Gradient Descent(47/49): loss=3.047380196389382e+41\n",
      "Gradient Descent(48/49): loss=2.5144113201266394e+42\n",
      "Gradient Descent(49/49): loss=2.0746555662046395e+43\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.634429308625494\n",
      "Gradient Descent(2/49): loss=2.233927108586332\n",
      "Gradient Descent(3/49): loss=15.807748435146266\n",
      "Gradient Descent(4/49): loss=130.37799949030185\n",
      "Gradient Descent(5/49): loss=1097.7088289298883\n",
      "Gradient Descent(6/49): loss=9265.966101366492\n",
      "Gradient Descent(7/49): loss=78241.63494238592\n",
      "Gradient Descent(8/49): loss=660700.3000820095\n",
      "Gradient Descent(9/49): loss=5579225.264197202\n",
      "Gradient Descent(10/49): loss=47113322.35757716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=397844759.0551815\n",
      "Gradient Descent(12/49): loss=3359569002.699404\n",
      "Gradient Descent(13/49): loss=28369618294.327694\n",
      "Gradient Descent(14/49): loss=239565028294.30383\n",
      "Gradient Descent(15/49): loss=2022988191145.7231\n",
      "Gradient Descent(16/49): loss=17082965952413.83\n",
      "Gradient Descent(17/49): loss=144255773223002.38\n",
      "Gradient Descent(18/49): loss=1218156622578604.5\n",
      "Gradient Descent(19/49): loss=1.0286628562442008e+16\n",
      "Gradient Descent(20/49): loss=8.686463236366968e+16\n",
      "Gradient Descent(21/49): loss=7.335216110772503e+17\n",
      "Gradient Descent(22/49): loss=6.194166017588553e+18\n",
      "Gradient Descent(23/49): loss=5.23061516852953e+19\n",
      "Gradient Descent(24/49): loss=4.416952171375973e+20\n",
      "Gradient Descent(25/49): loss=3.7298608013840283e+21\n",
      "Gradient Descent(26/49): loss=3.1496518544748645e+22\n",
      "Gradient Descent(27/49): loss=2.6596989358734007e+23\n",
      "Gradient Descent(28/49): loss=2.245962016225911e+24\n",
      "Gradient Descent(29/49): loss=1.8965851022808098e+25\n",
      "Gradient Descent(30/49): loss=1.601556493033617e+26\n",
      "Gradient Descent(31/49): loss=1.3524218856794334e+27\n",
      "Gradient Descent(32/49): loss=1.1420421101725865e+28\n",
      "Gradient Descent(33/49): loss=9.64388550065658e+28\n",
      "Gradient Descent(34/49): loss=8.143703872330883e+29\n",
      "Gradient Descent(35/49): loss=6.876887200257795e+30\n",
      "Gradient Descent(36/49): loss=5.807133744848905e+31\n",
      "Gradient Descent(37/49): loss=4.9037887853241116e+32\n",
      "Gradient Descent(38/49): loss=4.140966181879464e+33\n",
      "Gradient Descent(39/49): loss=3.4968065857134996e+34\n",
      "Gradient Descent(40/49): loss=2.9528510402708968e+35\n",
      "Gradient Descent(41/49): loss=2.4935120236996945e+36\n",
      "Gradient Descent(42/49): loss=2.1056267747812218e+37\n",
      "Gradient Descent(43/49): loss=1.7780801024962324e+38\n",
      "Gradient Descent(44/49): loss=1.5014858705059525e+39\n",
      "Gradient Descent(45/49): loss=1.2679180292068957e+40\n",
      "Gradient Descent(46/49): loss=1.0706834878480567e+41\n",
      "Gradient Descent(47/49): loss=9.041303181622574e+41\n",
      "Gradient Descent(48/49): loss=7.634857934188909e+42\n",
      "Gradient Descent(49/49): loss=6.447196217657015e+43\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6348995563348444\n",
      "Gradient Descent(2/49): loss=2.2219437995503935\n",
      "Gradient Descent(3/49): loss=15.712056559105795\n",
      "Gradient Descent(4/49): loss=129.83361098021274\n",
      "Gradient Descent(5/49): loss=1095.696605291621\n",
      "Gradient Descent(6/49): loss=9271.4960455133\n",
      "Gradient Descent(7/49): loss=78480.22692639266\n",
      "Gradient Descent(8/49): loss=664341.9661677334\n",
      "Gradient Descent(9/49): loss=5623753.923290314\n",
      "Gradient Descent(10/49): loss=47605974.74925164\n",
      "Gradient Descent(11/49): loss=402992266.00432754\n",
      "Gradient Descent(12/49): loss=3411394775.3187985\n",
      "Gradient Descent(13/49): loss=28878009328.46527\n",
      "Gradient Descent(14/49): loss=244457026897.10086\n",
      "Gradient Descent(15/49): loss=2069368332970.8499\n",
      "Gradient Descent(16/49): loss=17517538162979.838\n",
      "Gradient Descent(17/49): loss=148288798281157.34\n",
      "Gradient Descent(18/49): loss=1255288699312379.0\n",
      "Gradient Descent(19/49): loss=1.062622218864547e+16\n",
      "Gradient Descent(20/49): loss=8.995269220884195e+16\n",
      "Gradient Descent(21/49): loss=7.6146411132501e+17\n",
      "Gradient Descent(22/49): loss=6.445917054820473e+18\n",
      "Gradient Descent(23/49): loss=5.45657320675633e+19\n",
      "Gradient Descent(24/49): loss=4.6190776126144984e+20\n",
      "Gradient Descent(25/49): loss=3.910124025265221e+21\n",
      "Gradient Descent(26/49): loss=3.309983328966477e+22\n",
      "Gradient Descent(27/49): loss=2.8019545076432444e+23\n",
      "Gradient Descent(28/49): loss=2.371899880641906e+24\n",
      "Gradient Descent(29/49): loss=2.007851672267548e+25\n",
      "Gradient Descent(30/49): loss=1.699678966523892e+26\n",
      "Gradient Descent(31/49): loss=1.438805778905559e+27\n",
      "Gradient Descent(32/49): loss=1.217972399603106e+28\n",
      "Gradient Descent(33/49): loss=1.031033366660057e+29\n",
      "Gradient Descent(34/49): loss=8.72786446977599e+29\n",
      "Gradient Descent(35/49): loss=7.388278659646286e+30\n",
      "Gradient Descent(36/49): loss=6.254297570913794e+31\n",
      "Gradient Descent(37/49): loss=5.2943642095127236e+32\n",
      "Gradient Descent(38/49): loss=4.4817650687628194e+33\n",
      "Gradient Descent(39/49): loss=3.7938867325168752e+34\n",
      "Gradient Descent(40/49): loss=3.211586577683083e+35\n",
      "Gradient Descent(41/49): loss=2.7186600637156164e+36\n",
      "Gradient Descent(42/49): loss=2.3013897845389362e+37\n",
      "Gradient Descent(43/49): loss=1.948163733696635e+38\n",
      "Gradient Descent(44/49): loss=1.6491521596160933e+39\n",
      "Gradient Descent(45/49): loss=1.3960340183552222e+40\n",
      "Gradient Descent(46/49): loss=1.181765411421307e+41\n",
      "Gradient Descent(47/49): loss=1.0003835646334533e+42\n",
      "Gradient Descent(48/49): loss=8.468408930543268e+42\n",
      "Gradient Descent(49/49): loss=7.168645342667296e+43\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.633766464387605\n",
      "Gradient Descent(2/49): loss=2.206182287223911\n",
      "Gradient Descent(3/49): loss=15.421418934836053\n",
      "Gradient Descent(4/49): loss=125.90417576687021\n",
      "Gradient Descent(5/49): loss=1049.9227816088035\n",
      "Gradient Descent(6/49): loss=8779.019659157158\n",
      "Gradient Descent(7/49): loss=73432.48826045281\n",
      "Gradient Descent(8/49): loss=614259.4565707268\n",
      "Gradient Descent(9/49): loss=5138290.939017116\n",
      "Gradient Descent(10/49): loss=42981945.40875837\n",
      "Gradient Descent(11/49): loss=359545239.5397646\n",
      "Gradient Descent(12/49): loss=3007606652.674005\n",
      "Gradient Descent(13/49): loss=25158719582.842976\n",
      "Gradient Descent(14/49): loss=210453442008.13144\n",
      "Gradient Descent(15/49): loss=1760449339466.4758\n",
      "Gradient Descent(16/49): loss=14726211401138.203\n",
      "Gradient Descent(17/49): loss=123185199013484.73\n",
      "Gradient Descent(18/49): loss=1030447875743913.1\n",
      "Gradient Descent(19/49): loss=8619727314073975.0\n",
      "Gradient Descent(20/49): loss=7.21042769051953e+16\n",
      "Gradient Descent(21/49): loss=6.031544338453155e+17\n",
      "Gradient Descent(22/49): loss=5.045404886947203e+18\n",
      "Gradient Descent(23/49): loss=4.22049628499612e+19\n",
      "Gradient Descent(24/49): loss=3.530457771139125e+20\n",
      "Gradient Descent(25/49): loss=2.9532384895365483e+21\n",
      "Gradient Descent(26/49): loss=2.4703928332970986e+22\n",
      "Gradient Descent(27/49): loss=2.066490997062472e+23\n",
      "Gradient Descent(28/49): loss=1.728625902480772e+24\n",
      "Gradient Descent(29/49): loss=1.446000739889574e+25\n",
      "Gradient Descent(30/49): loss=1.2095839456995784e+26\n",
      "Gradient Descent(31/49): loss=1.0118205899437576e+27\n",
      "Gradient Descent(32/49): loss=8.463909510984827e+27\n",
      "Gradient Descent(33/49): loss=7.080085631991548e+28\n",
      "Gradient Descent(34/49): loss=5.922512816479788e+29\n",
      "Gradient Descent(35/49): loss=4.954199692567981e+30\n",
      "Gradient Descent(36/49): loss=4.144202866990029e+31\n",
      "Gradient Descent(37/49): loss=3.4666380986887864e+32\n",
      "Gradient Descent(38/49): loss=2.899853142568035e+33\n",
      "Gradient Descent(39/49): loss=2.4257358308161506e+34\n",
      "Gradient Descent(40/49): loss=2.02913528086267e+35\n",
      "Gradient Descent(41/49): loss=1.6973777341023596e+36\n",
      "Gradient Descent(42/49): loss=1.4198615535389944e+37\n",
      "Gradient Descent(43/49): loss=1.1877184381025914e+38\n",
      "Gradient Descent(44/49): loss=9.935300274120131e+38\n",
      "Gradient Descent(45/49): loss=8.31090840810917e+39\n",
      "Gradient Descent(46/49): loss=6.952099751619929e+40\n",
      "Gradient Descent(47/49): loss=5.815452244584439e+41\n",
      "Gradient Descent(48/49): loss=4.864643203826513e+42\n",
      "Gradient Descent(49/49): loss=4.069288596183193e+43\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6430770509020057\n",
      "Gradient Descent(2/49): loss=2.340460222846637\n",
      "Gradient Descent(3/49): loss=17.098156703943328\n",
      "Gradient Descent(4/49): loss=144.9155492602926\n",
      "Gradient Descent(5/49): loss=1252.586630673057\n",
      "Gradient Descent(6/49): loss=10853.503492335345\n",
      "Gradient Descent(7/49): loss=94074.83243088801\n",
      "Gradient Descent(8/49): loss=815450.221236472\n",
      "Gradient Descent(9/49): loss=7068459.905160894\n",
      "Gradient Descent(10/49): loss=61270687.79036415\n",
      "Gradient Descent(11/49): loss=531105544.7252426\n",
      "Gradient Descent(12/49): loss=4603720424.430982\n",
      "Gradient Descent(13/49): loss=39905894870.48617\n",
      "Gradient Descent(14/49): loss=345911633088.44794\n",
      "Gradient Descent(15/49): loss=2998425630557.169\n",
      "Gradient Descent(16/49): loss=25990904622564.133\n",
      "Gradient Descent(17/49): loss=225293939672916.7\n",
      "Gradient Descent(18/49): loss=1952889289190701.8\n",
      "Gradient Descent(19/49): loss=1.6928003395831482e+16\n",
      "Gradient Descent(20/49): loss=1.467350456349558e+17\n",
      "Gradient Descent(21/49): loss=1.2719263526847416e+18\n",
      "Gradient Descent(22/49): loss=1.102529146771563e+19\n",
      "Gradient Descent(23/49): loss=9.556925343319246e+19\n",
      "Gradient Descent(24/49): loss=8.284118590898531e+20\n",
      "Gradient Descent(25/49): loss=7.180826297449734e+21\n",
      "Gradient Descent(26/49): loss=6.2244722535475855e+22\n",
      "Gradient Descent(27/49): loss=5.395486985800464e+23\n",
      "Gradient Descent(28/49): loss=4.6769073148892905e+24\n",
      "Gradient Descent(29/49): loss=4.054029245113595e+25\n",
      "Gradient Descent(30/49): loss=3.514107082668482e+26\n",
      "Gradient Descent(31/49): loss=3.0460926258351735e+27\n",
      "Gradient Descent(32/49): loss=2.64040909024366e+28\n",
      "Gradient Descent(33/49): loss=2.2887551431335257e+29\n",
      "Gradient Descent(34/49): loss=1.9839350366487252e+30\n",
      "Gradient Descent(35/49): loss=1.7197113642543664e+31\n",
      "Gradient Descent(36/49): loss=1.4906774272918169e+32\n",
      "Gradient Descent(37/49): loss=1.2921465999620236e+33\n",
      "Gradient Descent(38/49): loss=1.1200564288591657e+34\n",
      "Gradient Descent(39/49): loss=9.70885504683139e+34\n",
      "Gradient Descent(40/49): loss=8.415814051118328e+35\n",
      "Gradient Descent(41/49): loss=7.294982343578785e+36\n",
      "Gradient Descent(42/49): loss=6.323424813082053e+37\n",
      "Gradient Descent(43/49): loss=5.481260883639831e+38\n",
      "Gradient Descent(44/49): loss=4.751257706482745e+39\n",
      "Gradient Descent(45/49): loss=4.1184775314728246e+40\n",
      "Gradient Descent(46/49): loss=3.569972042161256e+41\n",
      "Gradient Descent(47/49): loss=3.094517399796346e+42\n",
      "Gradient Descent(48/49): loss=2.68238457459879e+43\n",
      "Gradient Descent(49/49): loss=2.3251402646884767e+44\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6455770226402059\n",
      "Gradient Descent(2/49): loss=2.424535393735418\n",
      "Gradient Descent(3/49): loss=18.26025221319946\n",
      "Gradient Descent(4/49): loss=158.6382641386993\n",
      "Gradient Descent(5/49): loss=1403.5719016689472\n",
      "Gradient Descent(6/49): loss=12445.719724514278\n",
      "Gradient Descent(7/49): loss=110389.08061527298\n",
      "Gradient Descent(8/49): loss=979148.595655429\n",
      "Gradient Descent(9/49): loss=8685074.503341807\n",
      "Gradient Descent(10/49): loss=77036917.98883924\n",
      "Gradient Descent(11/49): loss=683320304.3495443\n",
      "Gradient Descent(12/49): loss=6061076510.18749\n",
      "Gradient Descent(13/49): loss=53761974408.298035\n",
      "Gradient Descent(14/49): loss=476870716216.1804\n",
      "Gradient Descent(15/49): loss=4229861022727.2466\n",
      "Gradient Descent(16/49): loss=37519024893653.62\n",
      "Gradient Descent(17/49): loss=332795148925020.25\n",
      "Gradient Descent(18/49): loss=2951905372335564.5\n",
      "Gradient Descent(19/49): loss=2.618351065325347e+16\n",
      "Gradient Descent(20/49): loss=2.322487152041402e+17\n",
      "Gradient Descent(21/49): loss=2.0600547584429192e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=1.827276247385541e+19\n",
      "Gradient Descent(23/49): loss=1.6208008406451837e+20\n",
      "Gradient Descent(24/49): loss=1.437656385450656e+21\n",
      "Gradient Descent(25/49): loss=1.2752065712183956e+22\n",
      "Gradient Descent(26/49): loss=1.1311129806367718e+23\n",
      "Gradient Descent(27/49): loss=1.003301428836412e+24\n",
      "Gradient Descent(28/49): loss=8.899321061088932e+24\n",
      "Gradient Descent(29/49): loss=7.893730943869119e+25\n",
      "Gradient Descent(30/49): loss=7.001768762635555e+26\n",
      "Gradient Descent(31/49): loss=6.210594984048116e+27\n",
      "Gradient Descent(32/49): loss=5.508820894188558e+28\n",
      "Gradient Descent(33/49): loss=4.88634466137218e+29\n",
      "Gradient Descent(34/49): loss=4.334205923250923e+30\n",
      "Gradient Descent(35/49): loss=3.8444568050318615e+31\n",
      "Gradient Descent(36/49): loss=3.4100475121564844e+32\n",
      "Gradient Descent(37/49): loss=3.0247248505808923e+33\n",
      "Gradient Descent(38/49): loss=2.682942213886021e+34\n",
      "Gradient Descent(39/49): loss=2.379779741509117e+35\n",
      "Gradient Descent(40/49): loss=2.110873498797532e+36\n",
      "Gradient Descent(41/49): loss=1.8723526594524783e+37\n",
      "Gradient Descent(42/49): loss=1.6607837861225779e+38\n",
      "Gradient Descent(43/49): loss=1.4731214070826778e+39\n",
      "Gradient Descent(44/49): loss=1.3066641775638747e+40\n",
      "Gradient Descent(45/49): loss=1.159015994689729e+41\n",
      "Gradient Descent(46/49): loss=1.028051506279973e+42\n",
      "Gradient Descent(47/49): loss=9.118855170307277e+42\n",
      "Gradient Descent(48/49): loss=8.088458516823942e+43\n",
      "Gradient Descent(49/49): loss=7.174492845484813e+44\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6460082579886092\n",
      "Gradient Descent(2/49): loss=2.411140310738841\n",
      "Gradient Descent(3/49): loss=18.148518392485485\n",
      "Gradient Descent(4/49): loss=157.96636975333644\n",
      "Gradient Descent(5/49): loss=1400.89389898282\n",
      "Gradient Descent(6/49): loss=12452.014087264279\n",
      "Gradient Descent(7/49): loss=110713.90483640687\n",
      "Gradient Descent(8/49): loss=984425.4758704859\n",
      "Gradient Descent(9/49): loss=8753190.569747662\n",
      "Gradient Descent(10/49): loss=77830609.22808352\n",
      "Gradient Descent(11/49): loss=692045380.7976605\n",
      "Gradient Descent(12/49): loss=6153450886.421908\n",
      "Gradient Descent(13/49): loss=54714559533.062035\n",
      "Gradient Descent(14/49): loss=486504741261.42755\n",
      "Gradient Descent(15/49): loss=4325847916939.5835\n",
      "Gradient Descent(16/49): loss=38464085990075.71\n",
      "Gradient Descent(17/49): loss=342010616067419.75\n",
      "Gradient Descent(18/49): loss=3041051372794968.0\n",
      "Gradient Descent(19/49): loss=2.7040077171656292e+16\n",
      "Gradient Descent(20/49): loss=2.4043190456770173e+17\n",
      "Gradient Descent(21/49): loss=2.1378452571374438e+18\n",
      "Gradient Descent(22/49): loss=1.9009051031237698e+19\n",
      "Gradient Descent(23/49): loss=1.6902253327354282e+20\n",
      "Gradient Descent(24/49): loss=1.502895473701395e+21\n",
      "Gradient Descent(25/49): loss=1.3363276251555856e+22\n",
      "Gradient Descent(26/49): loss=1.1882207066309805e+23\n",
      "Gradient Descent(27/49): loss=1.0565286693839281e+24\n",
      "Gradient Descent(28/49): loss=9.394322308985308e+24\n",
      "Gradient Descent(29/49): loss=8.35313742092396e+25\n",
      "Gradient Descent(30/49): loss=7.427348400225038e+26\n",
      "Gradient Descent(31/49): loss=6.604165773705609e+27\n",
      "Gradient Descent(32/49): loss=5.872217542031975e+28\n",
      "Gradient Descent(33/49): loss=5.221392079260244e+29\n",
      "Gradient Descent(34/49): loss=4.6426984440238766e+30\n",
      "Gradient Descent(35/49): loss=4.128142172612995e+31\n",
      "Gradient Descent(36/49): loss=3.670614838067309e+32\n",
      "Gradient Descent(37/49): loss=3.263795849577436e+33\n",
      "Gradient Descent(38/49): loss=2.9020651355857563e+34\n",
      "Gradient Descent(39/49): loss=2.580425504331958e+35\n",
      "Gradient Descent(40/49): loss=2.2944336092796866e+36\n",
      "Gradient Descent(41/49): loss=2.0401385657343934e+37\n",
      "Gradient Descent(42/49): loss=1.8140273706605352e+38\n",
      "Gradient Descent(43/49): loss=1.6129763716912074e+39\n",
      "Gradient Descent(44/49): loss=1.4342081148901244e+40\n",
      "Gradient Descent(45/49): loss=1.275252975131916e+41\n",
      "Gradient Descent(46/49): loss=1.1339150390369836e+42\n",
      "Gradient Descent(47/49): loss=1.0082417691448587e+43\n",
      "Gradient Descent(48/49): loss=8.964970302463746e+43\n",
      "Gradient Descent(49/49): loss=7.971371052423566e+44\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.644904183977987\n",
      "Gradient Descent(2/49): loss=2.3941904458980376\n",
      "Gradient Descent(3/49): loss=17.815708033685404\n",
      "Gradient Descent(4/49): loss=153.22835981034166\n",
      "Gradient Descent(5/49): loss=1342.8629890615045\n",
      "Gradient Descent(6/49): loss=11795.834118621224\n",
      "Gradient Descent(7/49): loss=103646.6893922008\n",
      "Gradient Descent(8/49): loss=910752.7873556021\n",
      "Gradient Descent(9/49): loss=8002919.752528348\n",
      "Gradient Descent(10/49): loss=70322924.3233347\n",
      "Gradient Descent(11/49): loss=617938821.6936699\n",
      "Gradient Descent(12/49): loss=5429927848.0716505\n",
      "Gradient Descent(13/49): loss=47713650113.504654\n",
      "Gradient Descent(14/49): loss=419267525433.06085\n",
      "Gradient Descent(15/49): loss=3684171249736.923\n",
      "Gradient Descent(16/49): loss=32373405941326.895\n",
      "Gradient Descent(17/49): loss=284470330292659.7\n",
      "Gradient Descent(18/49): loss=2499686593493433.0\n",
      "Gradient Descent(19/49): loss=2.1965148559676304e+16\n",
      "Gradient Descent(20/49): loss=1.9301129689802877e+17\n",
      "Gradient Descent(21/49): loss=1.6960213416744684e+18\n",
      "Gradient Descent(22/49): loss=1.4903212597628436e+19\n",
      "Gradient Descent(23/49): loss=1.3095692859078415e+20\n",
      "Gradient Descent(24/49): loss=1.1507396162798432e+21\n",
      "Gradient Descent(25/49): loss=1.011173428336706e+22\n",
      "Gradient Descent(26/49): loss=8.88534371902215e+22\n",
      "Gradient Descent(27/49): loss=7.807694584600729e+23\n",
      "Gradient Descent(28/49): loss=6.860746939468147e+24\n",
      "Gradient Descent(29/49): loss=6.028648797336172e+25\n",
      "Gradient Descent(30/49): loss=5.297470762628114e+26\n",
      "Gradient Descent(31/49): loss=4.654972851180165e+27\n",
      "Gradient Descent(32/49): loss=4.090399591837311e+28\n",
      "Gradient Descent(33/49): loss=3.594299978927066e+29\n",
      "Gradient Descent(34/49): loss=3.158369261598758e+30\n",
      "Gradient Descent(35/49): loss=2.775309921569111e+31\n",
      "Gradient Descent(36/49): loss=2.438709512028699e+32\n",
      "Gradient Descent(37/49): loss=2.142933312722346e+33\n",
      "Gradient Descent(38/49): loss=1.8830300042398548e+34\n",
      "Gradient Descent(39/49): loss=1.6546487824966443e+35\n",
      "Gradient Descent(40/49): loss=1.4539665258933884e+36\n",
      "Gradient Descent(41/49): loss=1.2776237959264799e+37\n",
      "Gradient Descent(42/49): loss=1.122668599894079e+38\n",
      "Gradient Descent(43/49): loss=9.865069742804465e+38\n",
      "Gradient Descent(44/49): loss=8.668595615801291e+39\n",
      "Gradient Descent(45/49): loss=7.61723453654238e+40\n",
      "Gradient Descent(46/49): loss=6.693386628732537e+41\n",
      "Gradient Descent(47/49): loss=5.881586597703971e+42\n",
      "Gradient Descent(48/49): loss=5.168244840032721e+43\n",
      "Gradient Descent(49/49): loss=4.541419952390403e+44\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6543743500283098\n",
      "Gradient Descent(2/49): loss=2.537537381202829\n",
      "Gradient Descent(3/49): loss=19.695472613771095\n",
      "Gradient Descent(4/49): loss=175.605173890703\n",
      "Gradient Descent(5/49): loss=1593.3019454023795\n",
      "Gradient Descent(6/49): loss=14487.218922115137\n",
      "Gradient Descent(7/49): loss=131763.15937409908\n",
      "Gradient Descent(8/49): loss=1198453.4605794938\n",
      "Gradient Descent(9/49): loss=10900626.902245857\n",
      "Gradient Descent(10/49): loss=99147640.93091029\n",
      "Gradient Descent(11/49): loss=901806631.1222786\n",
      "Gradient Descent(12/49): loss=8202467009.707528\n",
      "Gradient Descent(13/49): loss=74606311091.28513\n",
      "Gradient Descent(14/49): loss=678588730394.537\n",
      "Gradient Descent(15/49): loss=6172167724963.106\n",
      "Gradient Descent(16/49): loss=56139533013226.945\n",
      "Gradient Descent(17/49): loss=510622411367434.06\n",
      "Gradient Descent(18/49): loss=4644414247835105.0\n",
      "Gradient Descent(19/49): loss=4.224370733705519e+16\n",
      "Gradient Descent(20/49): loss=3.8423161982409805e+17\n",
      "Gradient Descent(21/49): loss=3.494814896209864e+18\n",
      "Gradient Descent(22/49): loss=3.178741813170426e+19\n",
      "Gradient Descent(23/49): loss=2.8912545627971273e+20\n",
      "Gradient Descent(24/49): loss=2.6297678258297236e+21\n",
      "Gradient Descent(25/49): loss=2.391930100779052e+22\n",
      "Gradient Descent(26/49): loss=2.1756025573123452e+23\n",
      "Gradient Descent(27/49): loss=1.9788398021507391e+24\n",
      "Gradient Descent(28/49): loss=1.7998723845100583e+25\n",
      "Gradient Descent(29/49): loss=1.6370908837597583e+26\n",
      "Gradient Descent(30/49): loss=1.489031436203096e+27\n",
      "Gradient Descent(31/49): loss=1.3543625708237901e+28\n",
      "Gradient Descent(32/49): loss=1.2318732356153081e+29\n",
      "Gradient Descent(33/49): loss=1.1204619068159136e+30\n",
      "Gradient Descent(34/49): loss=1.0191266831107637e+31\n",
      "Gradient Descent(35/49): loss=9.26956275720126e+31\n",
      "Gradient Descent(36/49): loss=8.431218133492267e+32\n",
      "Gradient Descent(37/49): loss=7.668693883031814e+33\n",
      "Gradient Descent(38/49): loss=6.975132767356187e+34\n",
      "Gradient Descent(39/49): loss=6.344297720619339e+35\n",
      "Gradient Descent(40/49): loss=5.770515760822113e+36\n",
      "Gradient Descent(41/49): loss=5.24862697374259e+37\n",
      "Gradient Descent(42/49): loss=4.7739381107891595e+38\n",
      "Gradient Descent(43/49): loss=4.3421803834906875e+39\n",
      "Gradient Descent(44/49): loss=3.949471075077395e+40\n",
      "Gradient Descent(45/49): loss=3.5922786239325765e+41\n",
      "Gradient Descent(46/49): loss=3.26739086491729e+42\n",
      "Gradient Descent(47/49): loss=2.971886142967873e+43\n",
      "Gradient Descent(48/49): loss=2.703107039196548e+44\n",
      "Gradient Descent(49/49): loss=2.4586364732186818e+45\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6570395832866814\n",
      "Gradient Descent(2/49): loss=2.6296606948611396\n",
      "Gradient Descent(3/49): loss=21.029682564156317\n",
      "Gradient Descent(4/49): loss=192.1351624284909\n",
      "Gradient Descent(5/49): loss=1784.1345352941505\n",
      "Gradient Descent(6/49): loss=16598.762245419894\n",
      "Gradient Descent(7/49): loss=154463.95239787354\n",
      "Gradient Descent(8/49): loss=1437450.4594226612\n",
      "Gradient Descent(9/49): loss=13377066.077537362\n",
      "Gradient Descent(10/49): loss=124488506.0746328\n",
      "Gradient Descent(11/49): loss=1158504472.866719\n",
      "Gradient Descent(12/49): loss=10781177352.521328\n",
      "Gradient Descent(13/49): loss=100330891008.96782\n",
      "Gradient Descent(14/49): loss=933690948419.8425\n",
      "Gradient Descent(15/49): loss=8689036633772.958\n",
      "Gradient Descent(16/49): loss=80861186195263.77\n",
      "Gradient Descent(17/49): loss=752503609848295.8\n",
      "Gradient Descent(18/49): loss=7002886174207903.0\n",
      "Gradient Descent(19/49): loss=6.516967377583654e+16\n",
      "Gradient Descent(20/49): loss=6.064765690026563e+17\n",
      "Gradient Descent(21/49): loss=5.643941536586699e+18\n",
      "Gradient Descent(22/49): loss=5.252317681587037e+19\n",
      "Gradient Descent(23/49): loss=4.887867964166723e+20\n",
      "Gradient Descent(24/49): loss=4.548706815446897e+21\n",
      "Gradient Descent(25/49): loss=4.233079503083567e+22\n",
      "Gradient Descent(26/49): loss=3.9393530527348046e+23\n",
      "Gradient Descent(27/49): loss=3.6660077994723845e+24\n",
      "Gradient Descent(28/49): loss=3.4116295254273368e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=3.174902142991249e+26\n",
      "Gradient Descent(30/49): loss=2.954600885718329e+27\n",
      "Gradient Descent(31/49): loss=2.749585971699545e+28\n",
      "Gradient Descent(32/49): loss=2.5587967066248635e+29\n",
      "Gradient Descent(33/49): loss=2.3812459960243347e+30\n",
      "Gradient Descent(34/49): loss=2.2160152382958558e+31\n",
      "Gradient Descent(35/49): loss=2.0622495720972182e+32\n",
      "Gradient Descent(36/49): loss=1.9191534535140218e+33\n",
      "Gradient Descent(37/49): loss=1.7859865401204365e+34\n",
      "Gradient Descent(38/49): loss=1.662059860638481e+35\n",
      "Gradient Descent(39/49): loss=1.5467322503781806e+36\n",
      "Gradient Descent(40/49): loss=1.4394070340167515e+37\n",
      "Gradient Descent(41/49): loss=1.3395289385543776e+38\n",
      "Gradient Descent(42/49): loss=1.2465812204748056e+39\n",
      "Gradient Descent(43/49): loss=1.1600829922476285e+40\n",
      "Gradient Descent(44/49): loss=1.0795867343401943e+41\n",
      "Gradient Descent(45/49): loss=1.0046759798669128e+42\n",
      "Gradient Descent(46/49): loss=9.349631598969511e+42\n",
      "Gradient Descent(47/49): loss=8.700875982725256e+43\n",
      "Gradient Descent(48/49): loss=8.097136455634165e+44\n",
      "Gradient Descent(49/49): loss=7.535289425033773e+45\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6574300709777061\n",
      "Gradient Descent(2/49): loss=2.6147490370875515\n",
      "Gradient Descent(3/49): loss=20.899776805707127\n",
      "Gradient Descent(4/49): loss=191.310595968554\n",
      "Gradient Descent(5/49): loss=1780.6034833251474\n",
      "Gradient Descent(6/49): loss=16605.71804218763\n",
      "Gradient Descent(7/49): loss=154902.74498099714\n",
      "Gradient Descent(8/49): loss=1445029.4301975304\n",
      "Gradient Descent(9/49): loss=13480219.255851975\n",
      "Gradient Descent(10/49): loss=125752817.41907997\n",
      "Gradient Descent(11/49): loss=1173109615.8815672\n",
      "Gradient Descent(12/49): loss=10943581773.604874\n",
      "Gradient Descent(13/49): loss=102089337390.3036\n",
      "Gradient Descent(14/49): loss=952360301252.6151\n",
      "Gradient Descent(15/49): loss=8884278878604.11\n",
      "Gradient Descent(16/49): loss=82878728879066.56\n",
      "Gradient Descent(17/49): loss=773150392357217.5\n",
      "Gradient Descent(18/49): loss=7212484280232577.0\n",
      "Gradient Descent(19/49): loss=6.728306679645582e+16\n",
      "Gradient Descent(20/49): loss=6.276632158415974e+17\n",
      "Gradient Descent(21/49): loss=5.855278768913093e+18\n",
      "Gradient Descent(22/49): loss=5.4622110387200205e+19\n",
      "Gradient Descent(23/49): loss=5.095530137680067e+20\n",
      "Gradient Descent(24/49): loss=4.753464705034557e+21\n",
      "Gradient Descent(25/49): loss=4.4343622923397764e+22\n",
      "Gradient Descent(26/49): loss=4.136681380824938e+23\n",
      "Gradient Descent(27/49): loss=3.858983934628949e+24\n",
      "Gradient Descent(28/49): loss=3.5999284539421365e+25\n",
      "Gradient Descent(29/49): loss=3.358263494493743e+26\n",
      "Gradient Descent(30/49): loss=3.132821622079561e+27\n",
      "Gradient Descent(31/49): loss=2.9225137729250103e+28\n",
      "Gradient Descent(32/49): loss=2.7263239926398225e+29\n",
      "Gradient Descent(33/49): loss=2.543304528349343e+30\n",
      "Gradient Descent(34/49): loss=2.3725712502933943e+31\n",
      "Gradient Descent(35/49): loss=2.2132993807753508e+32\n",
      "Gradient Descent(36/49): loss=2.064719509829159e+33\n",
      "Gradient Descent(37/49): loss=1.9261138783564662e+34\n",
      "Gradient Descent(38/49): loss=1.7968129107785427e+35\n",
      "Gradient Descent(39/49): loss=1.6761919804530697e+36\n",
      "Gradient Descent(40/49): loss=1.5636683922299915e+37\n",
      "Gradient Descent(41/49): loss=1.4586985675699557e+38\n",
      "Gradient Descent(42/49): loss=1.3607754186270329e+39\n",
      "Gradient Descent(43/49): loss=1.269425898610673e+40\n",
      "Gradient Descent(44/49): loss=1.1842087165929147e+41\n",
      "Gradient Descent(45/49): loss=1.1047122057218545e+42\n",
      "Gradient Descent(46/49): loss=1.0305523345428678e+43\n",
      "Gradient Descent(47/49): loss=9.61370851820893e+43\n",
      "Gradient Descent(48/49): loss=8.968335559016517e+44\n",
      "Gradient Descent(49/49): loss=8.366286802515278e+45\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6563568331262769\n",
      "Gradient Descent(2/49): loss=2.596540234198689\n",
      "Gradient Descent(3/49): loss=20.519816950159296\n",
      "Gradient Descent(4/49): loss=185.62235645789664\n",
      "Gradient Descent(5/49): loss=1707.4413071690863\n",
      "Gradient Descent(6/49): loss=15737.308753412475\n",
      "Gradient Descent(7/49): loss=145086.49012988206\n",
      "Gradient Descent(8/49): loss=1337641.2208216647\n",
      "Gradient Descent(9/49): loss=12332609.700451948\n",
      "Gradient Descent(10/49): loss=113702714.06999446\n",
      "Gradient Descent(11/49): loss=1048302876.8583266\n",
      "Gradient Descent(12/49): loss=9665019751.253685\n",
      "Gradient Descent(13/49): loss=89108415040.05858\n",
      "Gradient Descent(14/49): loss=821551311356.3989\n",
      "Gradient Descent(15/49): loss=7574442405601.715\n",
      "Gradient Descent(16/49): loss=69833955554716.81\n",
      "Gradient Descent(17/49): loss=643846911418136.9\n",
      "Gradient Descent(18/49): loss=5936064226243623.0\n",
      "Gradient Descent(19/49): loss=5.472862861224231e+16\n",
      "Gradient Descent(20/49): loss=5.045805900373807e+17\n",
      "Gradient Descent(21/49): loss=4.652072933278976e+18\n",
      "Gradient Descent(22/49): loss=4.289063631033453e+19\n",
      "Gradient Descent(23/49): loss=3.954380572896057e+20\n",
      "Gradient Descent(24/49): loss=3.6458134130152686e+21\n",
      "Gradient Descent(25/49): loss=3.3613242826518256e+22\n",
      "Gradient Descent(26/49): loss=3.099034331491014e+23\n",
      "Gradient Descent(27/49): loss=2.857211319159933e+24\n",
      "Gradient Descent(28/49): loss=2.634258174999943e+25\n",
      "Gradient Descent(29/49): loss=2.4287024505398762e+26\n",
      "Gradient Descent(30/49): loss=2.2391865950111324e+27\n",
      "Gradient Descent(31/49): loss=2.0644589896811216e+28\n",
      "Gradient Descent(32/49): loss=1.9033656817930285e+29\n",
      "Gradient Descent(33/49): loss=1.754842763520824e+30\n",
      "Gradient Descent(34/49): loss=1.6179093456074299e+31\n",
      "Gradient Descent(35/49): loss=1.491661079282103e+32\n",
      "Gradient Descent(36/49): loss=1.3752641836738237e+33\n",
      "Gradient Descent(37/49): loss=1.267949939275943e+34\n",
      "Gradient Descent(38/49): loss=1.1690096110953263e+35\n",
      "Gradient Descent(39/49): loss=1.0777897679569537e+36\n",
      "Gradient Descent(40/49): loss=9.936879670512594e+36\n",
      "Gradient Descent(41/49): loss=9.161487752237633e+37\n",
      "Gradient Descent(42/49): loss=8.446601007302971e+38\n",
      "Gradient Descent(43/49): loss=7.787498112317596e+39\n",
      "Gradient Descent(44/49): loss=7.179826156925874e+40\n",
      "Gradient Descent(45/49): loss=6.61957189590066e+41\n",
      "Gradient Descent(46/49): loss=6.103035244485545e+42\n",
      "Gradient Descent(47/49): loss=5.626804841941326e+43\n",
      "Gradient Descent(48/49): loss=5.187735521911328e+44\n",
      "Gradient Descent(49/49): loss=4.7829275408129156e+45\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6659822169475001\n",
      "Gradient Descent(2/49): loss=2.749299540580374\n",
      "Gradient Descent(3/49): loss=22.62101892716858\n",
      "Gradient Descent(4/49): loss=211.85266811143268\n",
      "Gradient Descent(5/49): loss=2015.2960112876017\n",
      "Gradient Descent(6/49): loss=19206.74232438593\n",
      "Gradient Descent(7/49): loss=183094.808121192\n",
      "Gradient Descent(8/49): loss=1745480.4903058405\n",
      "Gradient Descent(9/49): loss=16640140.793558339\n",
      "Gradient Descent(10/49): loss=158635220.50473642\n",
      "Gradient Descent(11/49): loss=1512315399.0078568\n",
      "Gradient Descent(12/49): loss=14417340773.682\n",
      "Gradient Descent(13/49): loss=137444688234.17816\n",
      "Gradient Descent(14/49): loss=1310300054380.4275\n",
      "Gradient Descent(15/49): loss=12491470250341.076\n",
      "Gradient Descent(16/49): loss=119084806971321.95\n",
      "Gradient Descent(17/49): loss=1135269985674432.0\n",
      "Gradient Descent(18/49): loss=1.0822857870525902e+16\n",
      "Gradient Descent(19/49): loss=1.0317744145800914e+17\n",
      "Gradient Descent(20/49): loss=9.836204589565883e+17\n",
      "Gradient Descent(21/49): loss=9.377138971524893e+18\n",
      "Gradient Descent(22/49): loss=8.939498410247632e+19\n",
      "Gradient Descent(23/49): loss=8.522282976662061e+20\n",
      "Gradient Descent(24/49): loss=8.124539409397619e+21\n",
      "Gradient Descent(25/49): loss=7.745358936756233e+22\n",
      "Gradient Descent(26/49): loss=7.383875200334277e+23\n",
      "Gradient Descent(27/49): loss=7.039262275551171e+24\n",
      "Gradient Descent(28/49): loss=6.710732784562087e+25\n",
      "Gradient Descent(29/49): loss=6.397536097242579e+26\n",
      "Gradient Descent(30/49): loss=6.098956616135361e+27\n",
      "Gradient Descent(31/49): loss=5.814312141440484e+28\n",
      "Gradient Descent(32/49): loss=5.542952312312688e+29\n",
      "Gradient Descent(33/49): loss=5.284257120905252e+30\n",
      "Gradient Descent(34/49): loss=5.037635495764748e+31\n",
      "Gradient Descent(35/49): loss=4.80252395134048e+32\n",
      "Gradient Descent(36/49): loss=4.5783853005223606e+33\n",
      "Gradient Descent(37/49): loss=4.364707427266153e+34\n",
      "Gradient Descent(38/49): loss=4.161002116501333e+35\n",
      "Gradient Descent(39/49): loss=3.966803938648734e+36\n",
      "Gradient Descent(40/49): loss=3.781669186198305e+37\n",
      "Gradient Descent(41/49): loss=3.605174859918406e+38\n",
      "Gradient Descent(42/49): loss=3.436917702379442e+39\n",
      "Gradient Descent(43/49): loss=3.2765132765838756e+40\n",
      "Gradient Descent(44/49): loss=3.123595087597812e+41\n",
      "Gradient Descent(45/49): loss=2.977813745176634e+42\n",
      "Gradient Descent(46/49): loss=2.838836165471855e+43\n",
      "Gradient Descent(47/49): loss=2.7063448099951374e+44\n",
      "Gradient Descent(48/49): loss=2.5800369601005685e+45\n",
      "Gradient Descent(49/49): loss=2.4596240253277225e+46\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6688169905649209\n",
      "Gradient Descent(2/49): loss=2.8500200114053484\n",
      "Gradient Descent(3/49): loss=24.14799466126148\n",
      "Gradient Descent(4/49): loss=231.67941917044286\n",
      "Gradient Descent(5/49): loss=2255.195046242706\n",
      "Gradient Descent(6/49): loss=21988.80059863307\n",
      "Gradient Descent(7/49): loss=214441.6234969425\n",
      "Gradient Descent(8/49): loss=2091363.4168642417\n",
      "Gradient Descent(9/49): loss=20396331.36870118\n",
      "Gradient Descent(10/49): loss=198918424.48565298\n",
      "Gradient Descent(11/49): loss=1939983533.415443\n",
      "Gradient Descent(12/49): loss=18919998332.801094\n",
      "Gradient Descent(13/49): loss=184520298161.40604\n",
      "Gradient Descent(14/49): loss=1799563609870.7002\n",
      "Gradient Descent(15/49): loss=17550530860868.578\n",
      "Gradient Descent(16/49): loss=171164348866388.62\n",
      "Gradient Descent(17/49): loss=1669307587087691.0\n",
      "Gradient Descent(18/49): loss=1.628018824467148e+16\n",
      "Gradient Descent(19/49): loss=1.587751300793022e+17\n",
      "Gradient Descent(20/49): loss=1.548479756673335e+18\n",
      "Gradient Descent(21/49): loss=1.5101795574846208e+19\n",
      "Gradient Descent(22/49): loss=1.4728266779180134e+20\n",
      "Gradient Descent(23/49): loss=1.4363976869082327e+21\n",
      "Gradient Descent(24/49): loss=1.4008697329355252e+22\n",
      "Gradient Descent(25/49): loss=1.3662205296911173e+23\n",
      "Gradient Descent(26/49): loss=1.3324283420972208e+24\n",
      "Gradient Descent(27/49): loss=1.2994719726729204e+25\n",
      "Gradient Descent(28/49): loss=1.2673307482371467e+26\n",
      "Gradient Descent(29/49): loss=1.2359845069406423e+27\n",
      "Gradient Descent(30/49): loss=1.2054135856186417e+28\n",
      "Gradient Descent(31/49): loss=1.1755988074563934e+29\n",
      "Gradient Descent(32/49): loss=1.1465214699597172e+30\n",
      "Gradient Descent(33/49): loss=1.118163333223137e+31\n",
      "Gradient Descent(34/49): loss=1.0905066084882014e+32\n",
      "Gradient Descent(35/49): loss=1.0635339469847572e+33\n",
      "Gradient Descent(36/49): loss=1.0372284290482791e+34\n",
      "Gradient Descent(37/49): loss=1.0115735535063023e+35\n",
      "Gradient Descent(38/49): loss=9.865532273274614e+35\n",
      "Gradient Descent(39/49): loss=9.62151755526454e+36\n",
      "Gradient Descent(40/49): loss=9.383538313187899e+37\n",
      "Gradient Descent(41/49): loss=9.15144526518972e+38\n",
      "Gradient Descent(42/49): loss=8.925092821762139e+39\n",
      "Gradient Descent(43/49): loss=8.70433899441766e+40\n",
      "Gradient Descent(44/49): loss=8.489045306621393e+41\n",
      "Gradient Descent(45/49): loss=8.279076706925984e+42\n",
      "Gradient Descent(46/49): loss=8.074301484255418e+43\n",
      "Gradient Descent(47/49): loss=7.874591185283911e+44\n",
      "Gradient Descent(48/49): loss=7.679820533858835e+45\n",
      "Gradient Descent(49/49): loss=7.489867352415824e+46\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6691649953021352\n",
      "Gradient Descent(2/49): loss=2.833482043720963\n",
      "Gradient Descent(3/49): loss=23.99756768816739\n",
      "Gradient Descent(4/49): loss=230.6728936321072\n",
      "Gradient Descent(5/49): loss=2250.579025052847\n",
      "Gradient Descent(6/49): loss=21996.198186900278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=215029.9837269577\n",
      "Gradient Descent(8/49): loss=2102157.3368067527\n",
      "Gradient Descent(9/49): loss=20551050.201390605\n",
      "Gradient Descent(10/49): loss=200910825.88444883\n",
      "Gradient Descent(11/49): loss=1964141480.8218386\n",
      "Gradient Descent(12/49): loss=19201812323.67899\n",
      "Gradient Descent(13/49): loss=187720489970.85342\n",
      "Gradient Descent(14/49): loss=1835190442518.2341\n",
      "Gradient Descent(15/49): loss=17941163284951.523\n",
      "Gradient Descent(16/49): loss=175396151051928.5\n",
      "Gradient Descent(17/49): loss=1714705413259075.5\n",
      "Gradient Descent(18/49): loss=1.6763279220489784e+16\n",
      "Gradient Descent(19/49): loss=1.6388093724536528e+17\n",
      "Gradient Descent(20/49): loss=1.6021305401632415e+18\n",
      "Gradient Descent(21/49): loss=1.5662726311363928e+19\n",
      "Gradient Descent(22/49): loss=1.5312172719690062e+20\n",
      "Gradient Descent(23/49): loss=1.4969465004793468e+21\n",
      "Gradient Descent(24/49): loss=1.4634427565043308e+22\n",
      "Gradient Descent(25/49): loss=1.4306888729017447e+23\n",
      "Gradient Descent(26/49): loss=1.3986680667538553e+24\n",
      "Gradient Descent(27/49): loss=1.367363930768009e+25\n",
      "Gradient Descent(28/49): loss=1.3367604248695359e+26\n",
      "Gradient Descent(29/49): loss=1.3068418679829543e+27\n",
      "Gradient Descent(30/49): loss=1.2775929299970615e+28\n",
      "Gradient Descent(31/49): loss=1.2489986239098383e+29\n",
      "Gradient Descent(32/49): loss=1.2210442981492127e+30\n",
      "Gradient Descent(33/49): loss=1.1937156290656857e+31\n",
      "Gradient Descent(34/49): loss=1.1669986135929317e+32\n",
      "Gradient Descent(35/49): loss=1.1408795620727242e+33\n",
      "Gradient Descent(36/49): loss=1.115345091240416e+34\n",
      "Gradient Descent(37/49): loss=1.0903821173674379e+35\n",
      "Gradient Descent(38/49): loss=1.0659778495572544e+36\n",
      "Gradient Descent(39/49): loss=1.0421197831913838e+37\n",
      "Gradient Descent(40/49): loss=1.0187956935220719e+38\n",
      "Gradient Descent(41/49): loss=9.959936294084415e+38\n",
      "Gradient Descent(42/49): loss=9.737019071927455e+39\n",
      "Gradient Descent(43/49): loss=9.519091047137488e+40\n",
      "Gradient Descent(44/49): loss=9.306040554540713e+41\n",
      "Gradient Descent(45/49): loss=9.097758428185128e+42\n",
      "Gradient Descent(46/49): loss=8.894137945404478e+43\n",
      "Gradient Descent(47/49): loss=8.695074772134206e+44\n",
      "Gradient Descent(48/49): loss=8.500466909451092e+45\n",
      "Gradient Descent(49/49): loss=8.310214641309838e+46\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6681244118324743\n",
      "Gradient Descent(2/49): loss=2.8139411088080553\n",
      "Gradient Descent(3/49): loss=23.565025846564755\n",
      "Gradient Descent(4/49): loss=223.8720711109032\n",
      "Gradient Descent(5/49): loss=2158.8281457389958\n",
      "Gradient Descent(6/49): loss=20854.304161062708\n",
      "Gradient Descent(7/49): loss=201498.37068816906\n",
      "Gradient Descent(8/49): loss=1946982.6859031683\n",
      "Gradient Descent(9/49): loss=18812876.997623123\n",
      "Gradient Descent(10/49): loss=181781152.33291852\n",
      "Gradient Descent(11/49): loss=1756477524.6125758\n",
      "Gradient Descent(12/49): loss=16972130644.992939\n",
      "Gradient Descent(13/49): loss=163994823860.86993\n",
      "Gradient Descent(14/49): loss=1584615561434.4338\n",
      "Gradient Descent(15/49): loss=15311498375914.715\n",
      "Gradient Descent(16/49): loss=147948807431353.25\n",
      "Gradient Descent(17/49): loss=1429569404883321.8\n",
      "Gradient Descent(18/49): loss=1.381335016399661e+16\n",
      "Gradient Descent(19/49): loss=1.3347280803698643e+17\n",
      "Gradient Descent(20/49): loss=1.2896936857301565e+18\n",
      "Gradient Descent(21/49): loss=1.2461787741451155e+19\n",
      "Gradient Descent(22/49): loss=1.2041320774945536e+20\n",
      "Gradient Descent(23/49): loss=1.16350405747042e+21\n",
      "Gradient Descent(24/49): loss=1.1242468472120419e+22\n",
      "Gradient Descent(25/49): loss=1.0863141949106227e+23\n",
      "Gradient Descent(26/49): loss=1.0496614093165785e+24\n",
      "Gradient Descent(27/49): loss=1.0142453070855046e+25\n",
      "Gradient Descent(28/49): loss=9.80024161900681e+25\n",
      "Gradient Descent(29/49): loss=9.469576553122387e+26\n",
      "Gradient Descent(30/49): loss=9.150068292350234e+27\n",
      "Gradient Descent(31/49): loss=8.841340400492e+28\n",
      "Gradient Descent(32/49): loss=8.543029142495536e+29\n",
      "Gradient Descent(33/49): loss=8.254783055911585e+30\n",
      "Gradient Descent(34/49): loss=7.976262536810214e+31\n",
      "Gradient Descent(35/49): loss=7.707139439668355e+32\n",
      "Gradient Descent(36/49): loss=7.447096690757381e+33\n",
      "Gradient Descent(37/49): loss=7.195827914575216e+34\n",
      "Gradient Descent(38/49): loss=6.953037072882952e+35\n",
      "Gradient Descent(39/49): loss=6.71843811592023e+36\n",
      "Gradient Descent(40/49): loss=6.491754645388971e+37\n",
      "Gradient Descent(41/49): loss=6.27271958880835e+38\n",
      "Gradient Descent(42/49): loss=6.061074884856884e+39\n",
      "Gradient Descent(43/49): loss=5.85657117933142e+40\n",
      "Gradient Descent(44/49): loss=5.658967531364777e+41\n",
      "Gradient Descent(45/49): loss=5.468031129555296e+42\n",
      "Gradient Descent(46/49): loss=5.283537017674844e+43\n",
      "Gradient Descent(47/49): loss=5.105267829631151e+44\n",
      "Gradient Descent(48/49): loss=4.933013533372933e+45\n",
      "Gradient Descent(49/49): loss=4.766571183435582e+46\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6779006516595764\n",
      "Gradient Descent(2/49): loss=2.9764569109772134\n",
      "Gradient Descent(3/49): loss=25.907178572538925\n",
      "Gradient Descent(4/49): loss=254.49946397867063\n",
      "Gradient Descent(5/49): loss=2535.369232445686\n",
      "Gradient Descent(6/49): loss=25299.45002869397\n",
      "Gradient Descent(7/49): loss=252509.32154076817\n",
      "Gradient Descent(8/49): loss=2520341.5734944656\n",
      "Gradient Descent(9/49): loss=25156163.129393566\n",
      "Gradient Descent(10/49): loss=251090366.11593476\n",
      "Gradient Descent(11/49): loss=2506200693.9663825\n",
      "Gradient Descent(12/49): loss=25015067365.229435\n",
      "Gradient Descent(13/49): loss=249682161834.55057\n",
      "Gradient Descent(14/49): loss=2492145286427.437\n",
      "Gradient Descent(15/49): loss=24874777156096.734\n",
      "Gradient Descent(16/49): loss=248281888740013.3\n",
      "Gradient Descent(17/49): loss=2478168784925438.5\n",
      "Gradient Descent(18/49): loss=2.473527391723349e+16\n",
      "Gradient Descent(19/49): loss=2.468894691453461e+17\n",
      "Gradient Descent(20/49): loss=2.464270667827786e+18\n",
      "Gradient Descent(21/49): loss=2.4596553045940814e+19\n",
      "Gradient Descent(22/49): loss=2.4550485855318114e+20\n",
      "Gradient Descent(23/49): loss=2.450450494451104e+21\n",
      "Gradient Descent(24/49): loss=2.44586101519246e+22\n",
      "Gradient Descent(25/49): loss=2.4412801316267043e+23\n",
      "Gradient Descent(26/49): loss=2.4367078276548354e+24\n",
      "Gradient Descent(27/49): loss=2.4321440872080362e+25\n",
      "Gradient Descent(28/49): loss=2.427588894247594e+26\n",
      "Gradient Descent(29/49): loss=2.423042232764805e+27\n",
      "Gradient Descent(30/49): loss=2.4185040867809356e+28\n",
      "Gradient Descent(31/49): loss=2.41397444034722e+29\n",
      "Gradient Descent(32/49): loss=2.4094532775447297e+30\n",
      "Gradient Descent(33/49): loss=2.4049405824843785e+31\n",
      "Gradient Descent(34/49): loss=2.4004363393068083e+32\n",
      "Gradient Descent(35/49): loss=2.3959405321823957e+33\n",
      "Gradient Descent(36/49): loss=2.391453145311159e+34\n",
      "Gradient Descent(37/49): loss=2.3869741629227146e+35\n",
      "Gradient Descent(38/49): loss=2.3825035692761802e+36\n",
      "Gradient Descent(39/49): loss=2.378041348660181e+37\n",
      "Gradient Descent(40/49): loss=2.373587485392762e+38\n",
      "Gradient Descent(41/49): loss=2.369141963821349e+39\n",
      "Gradient Descent(42/49): loss=2.3647047683226696e+40\n",
      "Gradient Descent(43/49): loss=2.3602758833027105e+41\n",
      "Gradient Descent(44/49): loss=2.3558552931966977e+42\n",
      "Gradient Descent(45/49): loss=2.351442982468948e+43\n",
      "Gradient Descent(46/49): loss=2.347038935612933e+44\n",
      "Gradient Descent(47/49): loss=2.3426431371511397e+45\n",
      "Gradient Descent(48/49): loss=2.338255571635043e+46\n",
      "Gradient Descent(49/49): loss=2.3338762236450552e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6809092444749236\n",
      "Gradient Descent(2/49): loss=3.086347750916268\n",
      "Gradient Descent(3/49): loss=27.64950273155295\n",
      "Gradient Descent(4/49): loss=278.1837801562294\n",
      "Gradient Descent(5/49): loss=2835.401785615611\n",
      "Gradient Descent(6/49): loss=28942.197448722967\n",
      "Gradient Descent(7/49): loss=295480.28039432\n",
      "Gradient Descent(8/49): loss=3016736.860536491\n",
      "Gradient Descent(9/49): loss=30799839.0444019\n",
      "Gradient Descent(10/49): loss=314455993.27827895\n",
      "Gradient Descent(11/49): loss=3210490529.3047833\n",
      "Gradient Descent(12/49): loss=32778036074.038685\n",
      "Gradient Descent(13/49): loss=334652804470.75946\n",
      "Gradient Descent(14/49): loss=3416693408108.052\n",
      "Gradient Descent(15/49): loss=34883299033457.586\n",
      "Gradient Descent(16/49): loss=356146837379166.75\n",
      "Gradient Descent(17/49): loss=3636140310487173.5\n",
      "Gradient Descent(18/49): loss=3.7123778649612504e+16\n",
      "Gradient Descent(19/49): loss=3.7902138629037325e+17\n",
      "Gradient Descent(20/49): loss=3.869681818260086e+18\n",
      "Gradient Descent(21/49): loss=3.950815947652468e+19\n",
      "Gradient Descent(22/49): loss=4.0336511851104993e+20\n",
      "Gradient Descent(23/49): loss=4.118223197112239e+21\n",
      "Gradient Descent(24/49): loss=4.204568397941101e+22\n",
      "Gradient Descent(25/49): loss=4.29272396536483e+23\n",
      "Gradient Descent(26/49): loss=4.382727856643046e+24\n",
      "Gradient Descent(27/49): loss=4.47461882487071e+25\n",
      "Gradient Descent(28/49): loss=4.568436435663899e+26\n",
      "Gradient Descent(29/49): loss=4.6642210841958273e+27\n",
      "Gradient Descent(30/49): loss=4.762014012589698e+28\n",
      "Gradient Descent(31/49): loss=4.8618573276765075e+29\n",
      "Gradient Descent(32/49): loss=4.9637940191248613e+30\n",
      "Gradient Descent(33/49): loss=5.06786797795134e+31\n",
      "Gradient Descent(34/49): loss=5.174124015418538e+32\n",
      "Gradient Descent(35/49): loss=5.282607882329463e+33\n",
      "Gradient Descent(36/49): loss=5.393366288726643e+34\n",
      "Gradient Descent(37/49): loss=5.506446924004107e+35\n",
      "Gradient Descent(38/49): loss=5.621898477441097e+36\n",
      "Gradient Descent(39/49): loss=5.739770659166129e+37\n",
      "Gradient Descent(40/49): loss=5.8601142215609886e+38\n",
      "Gradient Descent(41/49): loss=5.982980981112931e+39\n",
      "Gradient Descent(42/49): loss=6.108423840725744e+40\n",
      "Gradient Descent(43/49): loss=6.236496812497924e+41\n",
      "Gradient Descent(44/49): loss=6.367255040978965e+42\n",
      "Gradient Descent(45/49): loss=6.500754826912769e+43\n",
      "Gradient Descent(46/49): loss=6.637053651479324e+44\n",
      "Gradient Descent(47/49): loss=6.776210201044394e+45\n",
      "Gradient Descent(48/49): loss=6.918284392428248e+46\n",
      "Gradient Descent(49/49): loss=7.06333739870394e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6812130309618968\n",
      "Gradient Descent(2/49): loss=3.068068688929563\n",
      "Gradient Descent(3/49): loss=27.47596956122013\n",
      "Gradient Descent(4/49): loss=276.9613427660747\n",
      "Gradient Descent(5/49): loss=2829.415292329478\n",
      "Gradient Descent(6/49): loss=28949.652307407356\n",
      "Gradient Descent(7/49): loss=296263.6101095593\n",
      "Gradient Descent(8/49): loss=3031986.2279925263\n",
      "Gradient Descent(9/49): loss=31029782.80798069\n",
      "Gradient Descent(10/49): loss=317563651.16491354\n",
      "Gradient Descent(11/49): loss=3249996970.0085964\n",
      "Gradient Descent(12/49): loss=33260988527.832047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=340398278437.1804\n",
      "Gradient Descent(14/49): loss=3483690457481.989\n",
      "Gradient Descent(15/49): loss=35652645684346.95\n",
      "Gradient Descent(16/49): loss=364874881975949.2\n",
      "Gradient Descent(17/49): loss=3734187938842386.5\n",
      "Gradient Descent(18/49): loss=3.8216277007646824e+16\n",
      "Gradient Descent(19/49): loss=3.911114952562027e+17\n",
      "Gradient Descent(20/49): loss=4.0026976382605716e+18\n",
      "Gradient Descent(21/49): loss=4.096424824548635e+19\n",
      "Gradient Descent(22/49): loss=4.1923467270616134e+20\n",
      "Gradient Descent(23/49): loss=4.290514737284574e+21\n",
      "Gradient Descent(24/49): loss=4.3909814500859664e+22\n",
      "Gradient Descent(25/49): loss=4.493800691895894e+23\n",
      "Gradient Descent(26/49): loss=4.599027549544498e+24\n",
      "Gradient Descent(27/49): loss=4.70671839977524e+25\n",
      "Gradient Descent(28/49): loss=4.816930939449816e+26\n",
      "Gradient Descent(29/49): loss=4.9297242164597927e+27\n",
      "Gradient Descent(30/49): loss=5.045158661362423e+28\n",
      "Gradient Descent(31/49): loss=5.163296119757289e+29\n",
      "Gradient Descent(32/49): loss=5.284199885420727e+30\n",
      "Gradient Descent(33/49): loss=5.407934734216433e+31\n",
      "Gradient Descent(34/49): loss=5.53456695880006e+32\n",
      "Gradient Descent(35/49): loss=5.664164404136324e+33\n",
      "Gradient Descent(36/49): loss=5.796796503848048e+34\n",
      "Gradient Descent(37/49): loss=5.932534317416032e+35\n",
      "Gradient Descent(38/49): loss=6.071450568250196e+36\n",
      "Gradient Descent(39/49): loss=6.213619682652137e+37\n",
      "Gradient Descent(40/49): loss=6.3591178296901e+38\n",
      "Gradient Descent(41/49): loss=6.508022962007642e+39\n",
      "Gradient Descent(42/49): loss=6.660414857587644e+40\n",
      "Gradient Descent(43/49): loss=6.8163751624947806e+41\n",
      "Gradient Descent(44/49): loss=6.975987434618198e+42\n",
      "Gradient Descent(45/49): loss=7.139337188438977e+43\n",
      "Gradient Descent(46/49): loss=7.306511940845766e+44\n",
      "Gradient Descent(47/49): loss=7.477601258022997e+45\n",
      "Gradient Descent(48/49): loss=7.652696803437286e+46\n",
      "Gradient Descent(49/49): loss=7.83189238694753e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6802069200965795\n",
      "Gradient Descent(2/49): loss=3.0471197793322813\n",
      "Gradient Descent(3/49): loss=26.984927171423127\n",
      "Gradient Descent(4/49): loss=268.8624314785573\n",
      "Gradient Descent(5/49): loss=2714.9341268713556\n",
      "Gradient Descent(6/49): loss=27457.36086233959\n",
      "Gradient Descent(7/49): loss=277744.9798587268\n",
      "Gradient Descent(8/49): loss=2809618.93969591\n",
      "Gradient Descent(9/49): loss=28421773.677188955\n",
      "Gradient Descent(10/49): loss=287511659.67713004\n",
      "Gradient Descent(11/49): loss=2908438357.3576274\n",
      "Gradient Descent(12/49): loss=29421464881.97837\n",
      "Gradient Descent(13/49): loss=297624533073.8518\n",
      "Gradient Descent(14/49): loss=3010739381421.2935\n",
      "Gradient Descent(15/49): loss=30456332131546.4\n",
      "Gradient Descent(16/49): loss=308093145755768.7\n",
      "Gradient Descent(17/49): loss=3116638801273204.0\n",
      "Gradient Descent(18/49): loss=3.152759985577887e+16\n",
      "Gradient Descent(19/49): loss=3.1892998067735725e+17\n",
      "Gradient Descent(20/49): loss=3.226263116767791e+18\n",
      "Gradient Descent(21/49): loss=3.263654823704868e+19\n",
      "Gradient Descent(22/49): loss=3.301479892614403e+20\n",
      "Gradient Descent(23/49): loss=3.3397433460699043e+21\n",
      "Gradient Descent(24/49): loss=3.3784502648554966e+22\n",
      "Gradient Descent(25/49): loss=3.417605788640512e+23\n",
      "Gradient Descent(26/49): loss=3.457215116661986e+24\n",
      "Gradient Descent(27/49): loss=3.497283508415023e+25\n",
      "Gradient Descent(28/49): loss=3.53781628435115e+26\n",
      "Gradient Descent(29/49): loss=3.578818826584798e+27\n",
      "Gradient Descent(30/49): loss=3.6202965796079445e+28\n",
      "Gradient Descent(31/49): loss=3.662255051013079e+29\n",
      "Gradient Descent(32/49): loss=3.704699812224605e+30\n",
      "Gradient Descent(33/49): loss=3.7476364992384485e+31\n",
      "Gradient Descent(34/49): loss=3.791070813370588e+32\n",
      "Gradient Descent(35/49): loss=3.835008522014054e+33\n",
      "Gradient Descent(36/49): loss=3.8794554594047125e+34\n",
      "Gradient Descent(37/49): loss=3.924417527396016e+35\n",
      "Gradient Descent(38/49): loss=3.9699006962426084e+36\n",
      "Gradient Descent(39/49): loss=4.01591100539321e+37\n",
      "Gradient Descent(40/49): loss=4.062454564292387e+38\n",
      "Gradient Descent(41/49): loss=4.109537553191903e+39\n",
      "Gradient Descent(42/49): loss=4.157166223971327e+40\n",
      "Gradient Descent(43/49): loss=4.205346900968204e+41\n",
      "Gradient Descent(44/49): loss=4.2540859818178045e+42\n",
      "Gradient Descent(45/49): loss=4.3033899383026223e+43\n",
      "Gradient Descent(46/49): loss=4.353265317211799e+44\n",
      "Gradient Descent(47/49): loss=4.403718741210313e+45\n",
      "Gradient Descent(48/49): loss=4.4547569097184145e+46\n",
      "Gradient Descent(49/49): loss=4.506386599801295e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6901296541645391\n",
      "Gradient Descent(2/49): loss=3.2197365893416983\n",
      "Gradient Descent(3/49): loss=29.588674899372435\n",
      "Gradient Descent(4/49): loss=304.4905939065022\n",
      "Gradient Descent(5/49): loss=3173.2858373917734\n",
      "Gradient Descent(6/49): loss=33119.45433800004\n",
      "Gradient Descent(7/49): loss=345736.64140682184\n",
      "Gradient Descent(8/49): loss=3609297.7269359613\n",
      "Gradient Descent(9/49): loss=37679320.20493886\n",
      "Gradient Descent(10/49): loss=393354472.96055096\n",
      "Gradient Descent(11/49): loss=4106438166.472836\n",
      "Gradient Descent(12/49): loss=42869312713.10696\n",
      "Gradient Descent(13/49): loss=447535780965.91284\n",
      "Gradient Descent(14/49): loss=4672066417863.449\n",
      "Gradient Descent(15/49): loss=48774211065990.836\n",
      "Gradient Descent(16/49): loss=509180189892343.25\n",
      "Gradient Descent(17/49): loss=5315605524552338.0\n",
      "Gradient Descent(18/49): loss=5.549246151767124e+16\n",
      "Gradient Descent(19/49): loss=5.793156153292737e+17\n",
      "Gradient Descent(20/49): loss=6.047786906295776e+18\n",
      "Gradient Descent(21/49): loss=6.313609627660895e+19\n",
      "Gradient Descent(22/49): loss=6.591116246010394e+20\n",
      "Gradient Descent(23/49): loss=6.880820312059308e+21\n",
      "Gradient Descent(24/49): loss=7.1832579489865274e+22\n",
      "Gradient Descent(25/49): loss=7.498988844577994e+23\n",
      "Gradient Descent(26/49): loss=7.828597286978798e+24\n",
      "Gradient Descent(27/49): loss=8.17269324597066e+25\n",
      "Gradient Descent(28/49): loss=8.531913501775097e+26\n",
      "Gradient Descent(29/49): loss=8.906922823472119e+27\n",
      "Gradient Descent(30/49): loss=9.298415199214459e+28\n",
      "Gradient Descent(31/49): loss=9.707115120514528e+29\n",
      "Gradient Descent(32/49): loss=1.0133778922980597e+31\n",
      "Gradient Descent(33/49): loss=1.0579196185983038e+32\n",
      "Gradient Descent(34/49): loss=1.1044191193841458e+33\n",
      "Gradient Descent(35/49): loss=1.1529624461236076e+34\n",
      "Gradient Descent(36/49): loss=1.2036394325666818e+35\n",
      "Gradient Descent(37/49): loss=1.256543860990707e+36\n",
      "Gradient Descent(38/49): loss=1.3117736357527931e+37\n",
      "Gradient Descent(39/49): loss=1.3694309644705956e+38\n",
      "Gradient Descent(40/49): loss=1.4296225471665717e+39\n",
      "Gradient Descent(41/49): loss=1.4924597737259033e+40\n",
      "Gradient Descent(42/49): loss=1.5580589300333975e+41\n",
      "Gradient Descent(43/49): loss=1.6265414131708895e+42\n",
      "Gradient Descent(44/49): loss=1.6980339560732958e+43\n",
      "Gradient Descent(45/49): loss=1.7726688620592713e+44\n",
      "Gradient Descent(46/49): loss=1.8505842496702664e+45\n",
      "Gradient Descent(47/49): loss=1.9319243082711426e+46\n",
      "Gradient Descent(48/49): loss=2.0168395648854926e+47\n",
      "Gradient Descent(49/49): loss=2.1054871627592873e+48\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.69331634501669\n",
      "Gradient Descent(2/49): loss=3.3393957290484964\n",
      "Gradient Descent(3/49): loss=31.570993460885223\n",
      "Gradient Descent(4/49): loss=332.6730360931522\n",
      "Gradient Descent(5/49): loss=3546.68302370306\n",
      "Gradient Descent(6/49): loss=37860.87277471482\n",
      "Gradient Descent(7/49): loss=404232.70066607225\n",
      "Gradient Descent(8/49): loss=4316021.494701364\n",
      "Gradient Descent(9/49): loss=46082693.12163848\n",
      "Gradient Descent(10/49): loss=492031080.639587\n",
      "Gradient Descent(11/49): loss=5253482825.693659\n",
      "Gradient Descent(12/49): loss=56092153982.93136\n",
      "Gradient Descent(13/49): loss=598903599261.5505\n",
      "Gradient Descent(14/49): loss=6394575651545.472\n",
      "Gradient Descent(15/49): loss=68275758965096.22\n",
      "Gradient Descent(16/49): loss=728989618214092.1\n",
      "Gradient Descent(17/49): loss=7783521875622238.0\n",
      "Gradient Descent(18/49): loss=8.310572781148082e+16\n",
      "Gradient Descent(19/49): loss=8.873312242774815e+17\n",
      "Gradient Descent(20/49): loss=9.47415686394208e+18\n",
      "Gradient Descent(21/49): loss=1.0115686885207479e+20\n",
      "Gradient Descent(22/49): loss=1.0800657264712358e+21\n",
      "Gradient Descent(23/49): loss=1.153200950895106e+22\n",
      "Gradient Descent(24/49): loss=1.2312884304646025e+23\n",
      "Gradient Descent(25/49): loss=1.3146635005973748e+24\n",
      "Gradient Descent(26/49): loss=1.4036842035060786e+25\n",
      "Gradient Descent(27/49): loss=1.4987328257589817e+26\n",
      "Gradient Descent(28/49): loss=1.6002175399544952e+27\n",
      "Gradient Descent(29/49): loss=1.708574157559576e+28\n",
      "Gradient Descent(30/49): loss=1.824268000439128e+29\n",
      "Gradient Descent(31/49): loss=1.9477958991136954e+30\n",
      "Gradient Descent(32/49): loss=2.079688326326437e+31\n",
      "Gradient Descent(33/49): loss=2.220511675081832e+32\n",
      "Gradient Descent(34/49): loss=2.3708706909387e+33\n",
      "Gradient Descent(35/49): loss=2.5314110690028245e+34\n",
      "Gradient Descent(36/49): loss=2.7028222267714146e+35\n",
      "Gradient Descent(37/49): loss=2.8858402647371063e+36\n",
      "Gradient Descent(38/49): loss=3.081251127465374e+37\n",
      "Gradient Descent(39/49): loss=3.289893978720123e+38\n",
      "Gradient Descent(40/49): loss=3.5126648051313854e+39\n",
      "Gradient Descent(41/49): loss=3.7505202638806023e+40\n",
      "Gradient Descent(42/49): loss=4.004481790927068e+41\n",
      "Gradient Descent(43/49): loss=4.2756399874172086e+42\n",
      "Gradient Descent(44/49): loss=4.56515930311396e+43\n",
      "Gradient Descent(45/49): loss=4.874283036958222e+44\n",
      "Gradient Descent(46/49): loss=5.204338676236865e+45\n",
      "Gradient Descent(47/49): loss=5.5567435972855e+46\n",
      "Gradient Descent(48/49): loss=5.933011152206571e+47\n",
      "Gradient Descent(49/49): loss=6.334757167741802e+48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6935741779569904\n",
      "Gradient Descent(2/49): loss=3.319255624170057\n",
      "Gradient Descent(3/49): loss=31.37151618822029\n",
      "Gradient Descent(4/49): loss=331.1954674017836\n",
      "Gradient Descent(5/49): loss=3538.9765641569384\n",
      "Gradient Descent(6/49): loss=37867.773249479294\n",
      "Gradient Descent(7/49): loss=405268.55209447653\n",
      "Gradient Descent(8/49): loss=4337400.357228522\n",
      "Gradient Descent(9/49): loss=46421455.82150786\n",
      "Gradient Descent(10/49): loss=496830868.54742604\n",
      "Gradient Descent(11/49): loss=5317390002.958591\n",
      "Gradient Descent(12/49): loss=56909987212.828804\n",
      "Gradient Descent(13/49): loss=609085791002.8773\n",
      "Gradient Descent(14/49): loss=6518811905352.706\n",
      "Gradient Descent(15/49): loss=69768346794268.266\n",
      "Gradient Descent(16/49): loss=746703891148173.1\n",
      "Gradient Descent(17/49): loss=7991685724277274.0\n",
      "Gradient Descent(18/49): loss=8.553195111668365e+16\n",
      "Gradient Descent(19/49): loss=9.154157100575036e+17\n",
      "Gradient Descent(20/49): loss=9.797343697648124e+18\n",
      "Gradient Descent(21/49): loss=1.0485721675437392e+20\n",
      "Gradient Descent(22/49): loss=1.1222466256964656e+21\n",
      "Gradient Descent(23/49): loss=1.2010975761804848e+22\n",
      "Gradient Descent(24/49): loss=1.2854887281228e+23\n",
      "Gradient Descent(25/49): loss=1.3758093454702336e+24\n",
      "Gradient Descent(26/49): loss=1.4724760425144934e+25\n",
      "Gradient Descent(27/49): loss=1.5759347055736683e+26\n",
      "Gradient Descent(28/49): loss=1.6866625496945002e+27\n",
      "Gradient Descent(29/49): loss=1.8051703198619444e+28\n",
      "Gradient Descent(30/49): loss=1.9320046468694707e+29\n",
      "Gradient Descent(31/49): loss=2.067750568716797e+30\n",
      "Gradient Descent(32/49): loss=2.2130342291653363e+31\n",
      "Gradient Descent(33/49): loss=2.3685257658986948e+32\n",
      "Gradient Descent(34/49): loss=2.5349424016102205e+33\n",
      "Gradient Descent(35/49): loss=2.713051752275608e+34\n",
      "Gradient Descent(36/49): loss=2.9036753678703794e+35\n",
      "Gradient Descent(37/49): loss=3.107692521864075e+36\n",
      "Gradient Descent(38/49): loss=3.3260442669709218e+37\n",
      "Gradient Descent(39/49): loss=3.5597377758641644e+38\n",
      "Gradient Descent(40/49): loss=3.8098509868765686e+39\n",
      "Gradient Descent(41/49): loss=4.077537576115627e+40\n",
      "Gradient Descent(42/49): loss=4.3640322789279794e+41\n",
      "Gradient Descent(43/49): loss=4.6706565852589417e+42\n",
      "Gradient Descent(44/49): loss=4.998824835177795e+43\n",
      "Gradient Descent(45/49): loss=5.3500507426848075e+44\n",
      "Gradient Descent(46/49): loss=5.725954377892169e+45\n",
      "Gradient Descent(47/49): loss=6.128269639784624e+46\n",
      "Gradient Descent(48/49): loss=6.558852254029112e+47\n",
      "Gradient Descent(49/49): loss=7.019688332724028e+48\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6926043579185928\n",
      "Gradient Descent(2/49): loss=3.29682020830159\n",
      "Gradient Descent(3/49): loss=30.815536963885805\n",
      "Gradient Descent(4/49): loss=321.58712893131803\n",
      "Gradient Descent(5/49): loss=3396.8239143914575\n",
      "Gradient Descent(6/49): loss=35929.00351420421\n",
      "Gradient Descent(7/49): loss=380099.72158603347\n",
      "Gradient Descent(8/49): loss=4021269.1636149473\n",
      "Gradient Descent(9/49): loss=42543313.77562895\n",
      "Gradient Descent(10/49): loss=450090704.7002029\n",
      "Gradient Descent(11/49): loss=4761775320.945989\n",
      "Gradient Descent(12/49): loss=50377636126.64891\n",
      "Gradient Descent(13/49): loss=532974802676.7493\n",
      "Gradient Descent(14/49): loss=5638655625627.591\n",
      "Gradient Descent(15/49): loss=59654672477533.586\n",
      "Gradient Descent(16/49): loss=631122059124803.5\n",
      "Gradient Descent(17/49): loss=6677013501054288.0\n",
      "Gradient Descent(18/49): loss=7.064007452948359e+16\n",
      "Gradient Descent(19/49): loss=7.473431241013188e+17\n",
      "Gradient Descent(20/49): loss=7.906584879221252e+18\n",
      "Gradient Descent(21/49): loss=8.364843729245944e+19\n",
      "Gradient Descent(22/49): loss=8.849662867541236e+20\n",
      "Gradient Descent(23/49): loss=9.362581705539976e+21\n",
      "Gradient Descent(24/49): loss=9.905228877635967e+22\n",
      "Gradient Descent(25/49): loss=1.0479327412469787e+24\n",
      "Gradient Descent(26/49): loss=1.10867002039382e+25\n",
      "Gradient Descent(27/49): loss=1.1729275799298098e+26\n",
      "Gradient Descent(28/49): loss=1.240909452274454e+27\n",
      "Gradient Descent(29/49): loss=1.3128314953905552e+28\n",
      "Gradient Descent(30/49): loss=1.3889220781825537e+29\n",
      "Gradient Descent(31/49): loss=1.4694228056198805e+30\n",
      "Gradient Descent(32/49): loss=1.554589285887942e+31\n",
      "Gradient Descent(33/49): loss=1.6446919420024026e+32\n",
      "Gradient Descent(34/49): loss=1.7400168704640105e+33\n",
      "Gradient Descent(35/49): loss=1.8408667496803235e+34\n",
      "Gradient Descent(36/49): loss=1.9475618010386957e+35\n",
      "Gradient Descent(37/49): loss=2.0604408056822977e+36\n",
      "Gradient Descent(38/49): loss=2.1798621802175994e+37\n",
      "Gradient Descent(39/49): loss=2.3062051147688866e+38\n",
      "Gradient Descent(40/49): loss=2.4398707769934403e+39\n",
      "Gradient Descent(41/49): loss=2.581283585880511e+40\n",
      "Gradient Descent(42/49): loss=2.73089255937839e+41\n",
      "Gradient Descent(43/49): loss=2.889172740128956e+42\n",
      "Gradient Descent(44/49): loss=3.0566267038364445e+43\n",
      "Gradient Descent(45/49): loss=3.233786155060074e+44\n",
      "Gradient Descent(46/49): loss=3.4212136154974176e+45\n",
      "Gradient Descent(47/49): loss=3.6195042101191333e+46\n",
      "Gradient Descent(48/49): loss=3.829287556826658e+47\n",
      "Gradient Descent(49/49): loss=4.0512297656326725e+48\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7026692244623879\n",
      "Gradient Descent(2/49): loss=3.47988255957263\n",
      "Gradient Descent(3/49): loss=33.702681880230344\n",
      "Gradient Descent(4/49): loss=362.88465532690157\n",
      "Gradient Descent(5/49): loss=3952.21128005549\n",
      "Gradient Descent(6/49): loss=43101.042057104416\n",
      "Gradient Descent(7/49): loss=470129.57532364735\n",
      "Gradient Descent(8/49): loss=5128166.879275284\n",
      "Gradient Descent(9/49): loss=55938376.806245774\n",
      "Gradient Descent(10/49): loss=610180458.9294825\n",
      "Gradient Descent(11/49): loss=6655901774.229782\n",
      "Gradient Descent(12/49): loss=72603165331.70833\n",
      "Gradient Descent(13/49): loss=791961768755.4221\n",
      "Gradient Descent(14/49): loss=8638789286517.085\n",
      "Gradient Descent(15/49): loss=94232680652156.86\n",
      "Gradient Descent(16/49): loss=1027898448672181.0\n",
      "Gradient Descent(17/49): loss=1.121240755926419e+16\n",
      "Gradient Descent(18/49): loss=1.2230593736191771e+17\n",
      "Gradient Descent(19/49): loss=1.3341240259943654e+18\n",
      "Gradient Descent(20/49): loss=1.45527433510509e+19\n",
      "Gradient Descent(21/49): loss=1.5874261681463457e+20\n",
      "Gradient Descent(22/49): loss=1.731578561188625e+21\n",
      "Gradient Descent(23/49): loss=1.888821271649651e+22\n",
      "Gradient Descent(24/49): loss=2.0603430165982335e+23\n",
      "Gradient Descent(25/49): loss=2.2474404591693594e+24\n",
      "Gradient Descent(26/49): loss=2.4515280110255113e+25\n",
      "Gradient Descent(27/49): loss=2.674148524968693e+26\n",
      "Gradient Descent(28/49): loss=2.916984958536491e+27\n",
      "Gradient Descent(29/49): loss=3.1818730967561664e+28\n",
      "Gradient Descent(30/49): loss=3.470815430238022e+29\n",
      "Gradient Descent(31/49): loss=3.7859962935226835e+30\n",
      "Gradient Descent(32/49): loss=4.129798378124786e+31\n",
      "Gradient Descent(33/49): loss=4.504820745107785e+32\n",
      "Gradient Descent(34/49): loss=4.9138984733603366e+33\n",
      "Gradient Descent(35/49): loss=5.360124092110895e+34\n",
      "Gradient Descent(36/49): loss=5.8468709597046096e+35\n",
      "Gradient Descent(37/49): loss=6.377818765381259e+36\n",
      "Gradient Descent(38/49): loss=6.956981346840722e+37\n",
      "Gradient Descent(39/49): loss=7.58873703389039e+38\n",
      "Gradient Descent(40/49): loss=8.27786174756551e+39\n",
      "Gradient Descent(41/49): loss=9.02956510494339e+40\n",
      "Gradient Descent(42/49): loss=9.849529802595376e+41\n",
      "Gradient Descent(43/49): loss=1.0743954576406252e+43\n",
      "Gradient Descent(44/49): loss=1.171960106252625e+44\n",
      "Gradient Descent(45/49): loss=1.278384491371419e+45\n",
      "Gradient Descent(46/49): loss=1.3944731557497774e+46\n",
      "Gradient Descent(47/49): loss=1.521103701767124e+47\n",
      "Gradient Descent(48/49): loss=1.6592334258923778e+48\n",
      "Gradient Descent(49/49): loss=1.8099065556149987e+49\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7060382921902202\n",
      "Gradient Descent(2/49): loss=3.6099331695629506\n",
      "Gradient Descent(3/49): loss=35.95184206385769\n",
      "Gradient Descent(4/49): loss=396.29476789702665\n",
      "Gradient Descent(5/49): loss=4414.729574468298\n",
      "Gradient Descent(6/49): loss=49237.47320003786\n",
      "Gradient Descent(7/49): loss=549230.1538537422\n",
      "Gradient Descent(8/49): loss=6126662.996762256\n",
      "Gradient Descent(9/49): loss=68343269.55352636\n",
      "Gradient Descent(10/49): loss=762373810.7469728\n",
      "Gradient Descent(11/49): loss=8504333608.59181\n",
      "Gradient Descent(12/49): loss=94866446069.47118\n",
      "Gradient Descent(13/49): loss=1058241963992.7474\n",
      "Gradient Descent(14/49): loss=11804764528626.629\n",
      "Gradient Descent(15/49): loss=131682989721097.8\n",
      "Gradient Descent(16/49): loss=1468933136483232.5\n",
      "Gradient Descent(17/49): loss=1.638605384114498e+16\n",
      "Gradient Descent(18/49): loss=1.827875985763743e+17\n",
      "Gradient Descent(19/49): loss=2.039008691005498e+18\n",
      "Gradient Descent(20/49): loss=2.2745287286328553e+19\n",
      "Gradient Descent(21/49): loss=2.537253009365792e+20\n",
      "Gradient Descent(22/49): loss=2.830323817191502e+21\n",
      "Gradient Descent(23/49): loss=3.1572463922956955e+22\n",
      "Gradient Descent(24/49): loss=3.521930855090457e+23\n",
      "Gradient Descent(25/49): loss=3.9287389727664215e+24\n",
      "Gradient Descent(26/49): loss=4.382536327714813e+25\n",
      "Gradient Descent(27/49): loss=4.888750511774497e+26\n",
      "Gradient Descent(28/49): loss=5.453436042328816e+27\n",
      "Gradient Descent(29/49): loss=6.083346776674854e+28\n",
      "Gradient Descent(30/49): loss=6.786016690768262e+29\n",
      "Gradient Descent(31/49): loss=7.569849988488731e+30\n",
      "Gradient Descent(32/49): loss=8.444221619168359e+31\n",
      "Gradient Descent(33/49): loss=9.419589405610671e+32\n",
      "Gradient Descent(34/49): loss=1.0507619123695006e+34\n",
      "Gradient Descent(35/49): loss=1.1721324029567262e+35\n",
      "Gradient Descent(36/49): loss=1.307522050321495e+36\n",
      "Gradient Descent(37/49): loss=1.458550166998541e+37\n",
      "Gradient Descent(38/49): loss=1.6270231076626132e+38\n",
      "Gradient Descent(39/49): loss=1.8149558738288832e+39\n",
      "Gradient Descent(40/49): loss=2.024596214050236e+40\n",
      "Gradient Descent(41/49): loss=2.2584515078590877e+41\n",
      "Gradient Descent(42/49): loss=2.5193187549961782e+42\n",
      "Gradient Descent(43/49): loss=2.8103180286089315e+43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=3.1349297925329217e+44\n",
      "Gradient Descent(45/49): loss=3.4970365289849674e+45\n",
      "Gradient Descent(46/49): loss=3.9009691745518946e+46\n",
      "Gradient Descent(47/49): loss=4.351558919866627e+47\n",
      "Gradient Descent(48/49): loss=4.8541949925164985e+48\n",
      "Gradient Descent(49/49): loss=5.41488911428877e+49\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7062484362874165\n",
      "Gradient Descent(2/49): loss=3.587806794065097\n",
      "Gradient Descent(3/49): loss=35.72331183078322\n",
      "Gradient Descent(4/49): loss=394.51692074864195\n",
      "Gradient Descent(5/49): loss=4404.877583710301\n",
      "Gradient Descent(6/49): loss=49242.901046308725\n",
      "Gradient Descent(7/49): loss=550591.0572078193\n",
      "Gradient Descent(8/49): loss=6156415.510468191\n",
      "Gradient Descent(9/49): loss=68838184.97210623\n",
      "Gradient Descent(10/49): loss=769717753.0097011\n",
      "Gradient Descent(11/49): loss=8606641940.713629\n",
      "Gradient Descent(12/49): loss=96235653637.23715\n",
      "Gradient Descent(13/49): loss=1076064423166.0344\n",
      "Gradient Descent(14/49): loss=12032075471921.51\n",
      "Gradient Descent(15/49): loss=134537335487096.66\n",
      "Gradient Descent(16/49): loss=1504336860788248.0\n",
      "Gradient Descent(17/49): loss=1.6820828082092084e+16\n",
      "Gradient Descent(18/49): loss=1.880830449256791e+17\n",
      "Gradient Descent(19/49): loss=2.1030612533449142e+18\n",
      "Gradient Descent(20/49): loss=2.3515498896094556e+19\n",
      "Gradient Descent(21/49): loss=2.6293988701128173e+20\n",
      "Gradient Descent(22/49): loss=2.9400772863462925e+21\n",
      "Gradient Descent(23/49): loss=3.287464122671723e+22\n",
      "Gradient Descent(24/49): loss=3.675896687493012e+23\n",
      "Gradient Descent(25/49): loss=4.110224766845735e+24\n",
      "Gradient Descent(26/49): loss=4.595871176541111e+25\n",
      "Gradient Descent(27/49): loss=5.1388994688898565e+26\n",
      "Gradient Descent(28/49): loss=5.746089639360046e+27\n",
      "Gradient Descent(29/49): loss=6.4250227784069626e+28\n",
      "Gradient Descent(30/49): loss=7.184175725397448e+29\n",
      "Gradient Descent(31/49): loss=8.033026906433341e+30\n",
      "Gradient Descent(32/49): loss=8.982174677514846e+31\n",
      "Gradient Descent(33/49): loss=1.0043469650621459e+33\n",
      "Gradient Descent(34/49): loss=1.1230162654870868e+34\n",
      "Gradient Descent(35/49): loss=1.255707018012979e+35\n",
      "Gradient Descent(36/49): loss=1.4040759368726843e+36\n",
      "Gradient Descent(37/49): loss=1.5699754864988142e+37\n",
      "Gradient Descent(38/49): loss=1.755477010522036e+38\n",
      "Gradient Descent(39/49): loss=1.9628965935919444e+39\n",
      "Gradient Descent(40/49): loss=2.1948239789190485e+40\n",
      "Gradient Descent(41/49): loss=2.4541549025885528e+41\n",
      "Gradient Descent(42/49): loss=2.744127248357146e+42\n",
      "Gradient Descent(43/49): loss=3.068361474344395e+43\n",
      "Gradient Descent(44/49): loss=3.43090581636744e+44\n",
      "Gradient Descent(45/49): loss=3.8362868323065266e+45\n",
      "Gradient Descent(46/49): loss=4.2895659185743764e+46\n",
      "Gradient Descent(47/49): loss=4.796402504327779e+47\n",
      "Gradient Descent(48/49): loss=5.3631247124342135e+48\n",
      "Gradient Descent(49/49): loss=5.996808369433126e+49\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7053167252985136\n",
      "Gradient Descent(2/49): loss=3.5638036111701688\n",
      "Gradient Descent(3/49): loss=35.09540877719189\n",
      "Gradient Descent(4/49): loss=383.1590617323967\n",
      "Gradient Descent(5/49): loss=4229.180874855658\n",
      "Gradient Descent(6/49): loss=46738.210511594625\n",
      "Gradient Descent(7/49): loss=516609.57269822666\n",
      "Gradient Descent(8/49): loss=5710389.649155688\n",
      "Gradient Descent(9/49): loss=63120682.013865575\n",
      "Gradient Descent(10/49): loss=697715246.806075\n",
      "Gradient Descent(11/49): loss=7712317170.540165\n",
      "Gradient Descent(12/49): loss=85249449336.84512\n",
      "Gradient Descent(13/49): loss=942319743930.0083\n",
      "Gradient Descent(14/49): loss=10416096648846.625\n",
      "Gradient Descent(15/49): loss=115136152252218.14\n",
      "Gradient Descent(16/49): loss=1272677664727387.0\n",
      "Gradient Descent(17/49): loss=1.4067765916308646e+16\n",
      "Gradient Descent(18/49): loss=1.5550051938811117e+17\n",
      "Gradient Descent(19/49): loss=1.718852280731996e+18\n",
      "Gradient Descent(20/49): loss=1.8999635336303247e+19\n",
      "Gradient Descent(21/49): loss=2.100158035447007e+20\n",
      "Gradient Descent(22/49): loss=2.3214465413582173e+21\n",
      "Gradient Descent(23/49): loss=2.5660516748859823e+22\n",
      "Gradient Descent(24/49): loss=2.83643025194658e+23\n",
      "Gradient Descent(25/49): loss=3.1352979571290935e+24\n",
      "Gradient Descent(26/49): loss=3.4656566200532088e+25\n",
      "Gradient Descent(27/49): loss=3.8308243657698564e+26\n",
      "Gradient Descent(28/49): loss=4.234468942035791e+27\n",
      "Gradient Descent(29/49): loss=4.680644558201346e+28\n",
      "Gradient Descent(30/49): loss=5.173832605721483e+29\n",
      "Gradient Descent(31/49): loss=5.71898666928754e+30\n",
      "Gradient Descent(32/49): loss=6.321582280671358e+31\n",
      "Gradient Descent(33/49): loss=6.987671915010142e+32\n",
      "Gradient Descent(34/49): loss=7.723945781915276e+33\n",
      "Gradient Descent(35/49): loss=8.53779902199087e+34\n",
      "Gradient Descent(36/49): loss=9.437405983685341e+35\n",
      "Gradient Descent(37/49): loss=1.0431802326512407e+37\n",
      "Gradient Descent(38/49): loss=1.1530975775287587e+38\n",
      "Gradient Descent(39/49): loss=1.2745966436916006e+39\n",
      "Gradient Descent(40/49): loss=1.4088977687314295e+40\n",
      "Gradient Descent(41/49): loss=1.55734987422161e+41\n",
      "Gradient Descent(42/49): loss=1.7214440142962563e+42\n",
      "Gradient Descent(43/49): loss=1.9028283518098696e+43\n",
      "Gradient Descent(44/49): loss=2.103324712498222e+44\n",
      "Gradient Descent(45/49): loss=2.324946883410619e+45\n",
      "Gradient Descent(46/49): loss=2.569920839403115e+46\n",
      "Gradient Descent(47/49): loss=2.840707100847757e+47\n",
      "Gradient Descent(48/49): loss=3.1400254471188794e+48\n",
      "Gradient Descent(49/49): loss=3.470882234078865e+49\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7155193625531229\n",
      "Gradient Descent(2/49): loss=3.7576556925192213\n",
      "Gradient Descent(3/49): loss=38.288936850426175\n",
      "Gradient Descent(4/49): loss=430.86447768394066\n",
      "Gradient Descent(5/49): loss=4899.202171740518\n",
      "Gradient Descent(6/49): loss=55774.424240041175\n",
      "Gradient Descent(7/49): loss=635071.4794594073\n",
      "Gradient Descent(8/49): loss=7231440.726162795\n",
      "Gradient Descent(9/49): loss=82343685.18675157\n",
      "Gradient Descent(10/49): loss=937640889.7119979\n",
      "Gradient Descent(11/49): loss=10676845669.138908\n",
      "Gradient Descent(12/49): loss=121576444519.75784\n",
      "Gradient Descent(13/49): loss=1384381933622.5261\n",
      "Gradient Descent(14/49): loss=15763854242111.994\n",
      "Gradient Descent(15/49): loss=179501837559761.2\n",
      "Gradient Descent(16/49): loss=2043974094496495.0\n",
      "Gradient Descent(17/49): loss=2.3274581230950884e+16\n",
      "Gradient Descent(18/49): loss=2.6502592814072902e+17\n",
      "Gradient Descent(19/49): loss=3.017830563322072e+18\n",
      "Gradient Descent(20/49): loss=3.4363812525160505e+19\n",
      "Gradient Descent(21/49): loss=3.9129818142105934e+20\n",
      "Gradient Descent(22/49): loss=4.4556833346514227e+21\n",
      "Gradient Descent(23/49): loss=5.073653525961005e+22\n",
      "Gradient Descent(24/49): loss=5.7773315938554024e+23\n",
      "Gradient Descent(25/49): loss=6.578604584363651e+24\n",
      "Gradient Descent(26/49): loss=7.491008188527698e+25\n",
      "Gradient Descent(27/49): loss=8.529955397222996e+26\n",
      "Gradient Descent(28/49): loss=9.712996868705112e+27\n",
      "Gradient Descent(29/49): loss=1.1060117407201134e+29\n",
      "Gradient Descent(30/49): loss=1.2594073560880467e+30\n",
      "Gradient Descent(31/49): loss=1.434077804215692e+31\n",
      "Gradient Descent(32/49): loss=1.632973746423265e+32\n",
      "Gradient Descent(33/49): loss=1.8594550788449027e+33\n",
      "Gradient Descent(34/49): loss=2.1173476902585345e+34\n",
      "Gradient Descent(35/49): loss=2.4110080918051023e+35\n",
      "Gradient Descent(36/49): loss=2.745397010370023e+36\n",
      "Gradient Descent(37/49): loss=3.1261631888201757e+37\n",
      "Gradient Descent(38/49): loss=3.5597388087113385e+38\n",
      "Gradient Descent(39/49): loss=4.0534481474167814e+39\n",
      "Gradient Descent(40/49): loss=4.615631305192443e+40\n",
      "Gradient Descent(41/49): loss=5.2557850922675124e+41\n",
      "Gradient Descent(42/49): loss=5.984723455928134e+42\n",
      "Gradient Descent(43/49): loss=6.814760157646397e+43\n",
      "Gradient Descent(44/49): loss=7.759916786170425e+44\n",
      "Gradient Descent(45/49): loss=8.836159620485622e+45\n",
      "Gradient Descent(46/49): loss=1.006166934390967e+47\n",
      "Gradient Descent(47/49): loss=1.1457148165529366e+48\n",
      "Gradient Descent(48/49): loss=1.3046169537100626e+49\n",
      "Gradient Descent(49/49): loss=1.4855576373084877e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7190750859955138\n",
      "Gradient Descent(2/49): loss=3.8987467043269697\n",
      "Gradient Descent(3/49): loss=40.834131018653515\n",
      "Gradient Descent(4/49): loss=470.3308471594939\n",
      "Gradient Descent(5/49): loss=5469.535215120452\n",
      "Gradient Descent(6/49): loss=63673.14258113266\n",
      "Gradient Descent(7/49): loss=741352.8953989301\n",
      "Gradient Descent(8/49): loss=8631863.451128636\n",
      "Gradient Descent(9/49): loss=100504693.6724505\n",
      "Gradient Descent(10/49): loss=1170222956.913187\n",
      "Gradient Descent(11/49): loss=13625454381.061047\n",
      "Gradient Descent(12/49): loss=158647561087.69095\n",
      "Gradient Descent(13/49): loss=1847208047510.4329\n",
      "Gradient Descent(14/49): loss=21507910731534.188\n",
      "Gradient Descent(15/49): loss=250426704778447.97\n",
      "Gradient Descent(16/49): loss=2915835724808595.5\n",
      "Gradient Descent(17/49): loss=3.395044462982871e+16\n",
      "Gradient Descent(18/49): loss=3.953009700673362e+17\n",
      "Gradient Descent(19/49): loss=4.6026748291604096e+18\n",
      "Gradient Descent(20/49): loss=5.359110446753714e+19\n",
      "Gradient Descent(21/49): loss=6.239863958790069e+20\n",
      "Gradient Descent(22/49): loss=7.265366633336275e+21\n",
      "Gradient Descent(23/49): loss=8.459407555262097e+22\n",
      "Gradient Descent(24/49): loss=9.84968547322495e+23\n",
      "Gradient Descent(25/49): loss=1.1468451341029284e+25\n",
      "Gradient Descent(26/49): loss=1.3353256458705285e+26\n",
      "Gradient Descent(27/49): loss=1.5547823568299776e+27\n",
      "Gradient Descent(28/49): loss=1.8103061111612644e+28\n",
      "Gradient Descent(29/49): loss=2.107824417810913e+29\n",
      "Gradient Descent(30/49): loss=2.454238953803163e+30\n",
      "Gradient Descent(31/49): loss=2.857585665802447e+31\n",
      "Gradient Descent(32/49): loss=3.327221183880906e+32\n",
      "Gradient Descent(33/49): loss=3.8740398718220795e+33\n",
      "Gradient Descent(34/49): loss=4.510726548982095e+34\n",
      "Gradient Descent(35/49): loss=5.252050746220712e+35\n",
      "Gradient Descent(36/49): loss=6.115209321900028e+36\n",
      "Gradient Descent(37/49): loss=7.120225385781455e+37\n",
      "Gradient Descent(38/49): loss=8.290412784852061e+38\n",
      "Gradient Descent(39/49): loss=9.652916925985128e+39\n",
      "Gradient Descent(40/49): loss=1.1239344481160516e+41\n",
      "Gradient Descent(41/49): loss=1.308649658282461e+42\n",
      "Gradient Descent(42/49): loss=1.5237222517678126e+43\n",
      "Gradient Descent(43/49): loss=1.7741413722443703e+44\n",
      "Gradient Descent(44/49): loss=2.0657161139816294e+45\n",
      "Gradient Descent(45/49): loss=2.4052102782345703e+46\n",
      "Gradient Descent(46/49): loss=2.8004992764347964e+47\n",
      "Gradient Descent(47/49): loss=3.260752819944056e+48\n",
      "Gradient Descent(48/49): loss=3.7966476343135765e+49\n",
      "Gradient Descent(49/49): loss=4.4206151324853466e+50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7192358059531749\n",
      "Gradient Descent(2/49): loss=3.8745034364035034\n",
      "Gradient Descent(3/49): loss=40.573149151269895\n",
      "Gradient Descent(4/49): loss=468.20092115204267\n",
      "Gradient Descent(5/49): loss=5457.022090556896\n",
      "Gradient Descent(6/49): loss=63675.772570413406\n",
      "Gradient Descent(7/49): loss=743129.7473221049\n",
      "Gradient Descent(8/49): loss=8672978.974050108\n",
      "Gradient Descent(9/49): loss=101221964.43857859\n",
      "Gradient Descent(10/49): loss=1181358945.641748\n",
      "Gradient Descent(11/49): loss=13787614704.006796\n",
      "Gradient Descent(12/49): loss=160914966333.7186\n",
      "Gradient Descent(13/49): loss=1878035284338.0732\n",
      "Gradient Descent(14/49): loss=21918511535485.93\n",
      "Gradient Descent(15/49): loss=255810501792689.0\n",
      "Gradient Descent(16/49): loss=2985559158021768.5\n",
      "Gradient Descent(17/49): loss=3.4844400148364264e+16\n",
      "Gradient Descent(18/49): loss=4.0666828471835584e+17\n",
      "Gradient Descent(19/49): loss=4.746217271418181e+18\n",
      "Gradient Descent(20/49): loss=5.539300514454669e+19\n",
      "Gradient Descent(21/49): loss=6.464906352733332e+20\n",
      "Gradient Descent(22/49): loss=7.545179042110267e+21\n",
      "Gradient Descent(23/49): loss=8.805963098511236e+22\n",
      "Gradient Descent(24/49): loss=1.0277421603855565e+24\n",
      "Gradient Descent(25/49): loss=1.199475782964101e+25\n",
      "Gradient Descent(26/49): loss=1.3999057442360834e+26\n",
      "Gradient Descent(27/49): loss=1.6338271439731372e+27\n",
      "Gradient Descent(28/49): loss=1.906836333356193e+28\n",
      "Gradient Descent(29/49): loss=2.2254648024546902e+29\n",
      "Gradient Descent(30/49): loss=2.59733544003095e+30\n",
      "Gradient Descent(31/49): loss=3.0313449040397174e+31\n",
      "Gradient Descent(32/49): loss=3.537876465866908e+32\n",
      "Gradient Descent(33/49): loss=4.129048420407287e+33\n",
      "Gradient Descent(34/49): loss=4.819004004960442e+34\n",
      "Gradient Descent(35/49): loss=5.6242497629839076e+35\n",
      "Gradient Descent(36/49): loss=6.56405044774066e+36\n",
      "Gradient Descent(37/49): loss=7.660889913541978e+37\n",
      "Gradient Descent(38/49): loss=8.941009020978746e+38\n",
      "Gradient Descent(39/49): loss=1.0435033425021948e+40\n",
      "Gradient Descent(40/49): loss=1.2178706265235938e+41\n",
      "Gradient Descent(41/49): loss=1.421374328703558e+42\n",
      "Gradient Descent(42/49): loss=1.6588830851963595e+43\n",
      "Gradient Descent(43/49): loss=1.9360790713454154e+44\n",
      "Gradient Descent(44/49): loss=2.259593942425446e+45\n",
      "Gradient Descent(45/49): loss=2.637167489805917e+46\n",
      "Gradient Descent(46/49): loss=3.077832808236375e+47\n",
      "Gradient Descent(47/49): loss=3.5921324042081934e+48\n",
      "Gradient Descent(48/49): loss=4.192370415583535e+49\n",
      "Gradient Descent(49/49): loss=4.892906976610851e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7183440222363421\n",
      "Gradient Descent(2/49): loss=3.8488484563162175\n",
      "Gradient Descent(3/49): loss=39.86575022116862\n",
      "Gradient Descent(4/49): loss=454.8215130899506\n",
      "Gradient Descent(5/49): loss=5240.827448109531\n",
      "Gradient Descent(6/49): loss=60457.367818500825\n",
      "Gradient Descent(7/49): loss=697539.8376366894\n",
      "Gradient Descent(8/49): loss=8048253.376183286\n",
      "Gradient Descent(9/49): loss=92861779.96975408\n",
      "Gradient Descent(10/49): loss=1071452682.809445\n",
      "Gradient Descent(11/49): loss=12362580660.37785\n",
      "Gradient Descent(12/49): loss=142641308925.66525\n",
      "Gradient Descent(13/49): loss=1645816838111.8352\n",
      "Gradient Descent(14/49): loss=18989681866170.285\n",
      "Gradient Descent(15/49): loss=219105801715068.62\n",
      "Gradient Descent(16/49): loss=2528075651648881.5\n",
      "Gradient Descent(17/49): loss=2.916931660803365e+16\n",
      "Gradient Descent(18/49): loss=3.3655995651805754e+17\n",
      "Gradient Descent(19/49): loss=3.88327933266274e+18\n",
      "Gradient Descent(20/49): loss=4.480586024407324e+19\n",
      "Gradient Descent(21/49): loss=5.169767457432196e+20\n",
      "Gradient Descent(22/49): loss=5.964955346987609e+21\n",
      "Gradient Descent(23/49): loss=6.882455078400976e+22\n",
      "Gradient Descent(24/49): loss=7.941080050185004e+23\n",
      "Gradient Descent(25/49): loss=9.162537444137922e+24\n",
      "Gradient Descent(26/49): loss=1.0571873332680197e+26\n",
      "Gradient Descent(27/49): loss=1.2197986250385271e+27\n",
      "Gradient Descent(28/49): loss=1.4074219760526255e+28\n",
      "Gradient Descent(29/49): loss=1.6239046167258178e+29\n",
      "Gradient Descent(30/49): loss=1.8736855393004192e+30\n",
      "Gradient Descent(31/49): loss=2.161886519703304e+31\n",
      "Gradient Descent(32/49): loss=2.4944171399326163e+32\n",
      "Gradient Descent(33/49): loss=2.8780959644651344e+33\n",
      "Gradient Descent(34/49): loss=3.3207903554151744e+34\n",
      "Gradient Descent(35/49): loss=3.831577793365116e+35\n",
      "Gradient Descent(36/49): loss=4.4209320117629704e+36\n",
      "Gradient Descent(37/49): loss=5.10093776158603e+37\n",
      "Gradient Descent(38/49): loss=5.885538610035816e+38\n",
      "Gradient Descent(39/49): loss=6.790822854394515e+39\n",
      "Gradient Descent(40/49): loss=7.835353413727116e+40\n",
      "Gradient Descent(41/49): loss=9.040548462882734e+41\n",
      "Gradient Descent(42/49): loss=1.0431120613722703e+43\n",
      "Gradient Descent(43/49): loss=1.2035583649018456e+44\n",
      "Gradient Descent(44/49): loss=1.388683719963468e+45\n",
      "Gradient Descent(45/49): loss=1.6022841353845333e+46\n",
      "Gradient Descent(46/49): loss=1.8487395031695843e+47\n",
      "Gradient Descent(47/49): loss=2.1331034085034307e+48\n",
      "Gradient Descent(48/49): loss=2.4612067538817515e+49\n",
      "Gradient Descent(49/49): loss=2.8397773221894906e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.728680068436744\n",
      "Gradient Descent(2/49): loss=4.0538337459810405\n",
      "Gradient Descent(3/49): loss=43.38985578540902\n",
      "Gradient Descent(4/49): loss=509.74852402439933\n",
      "Gradient Descent(5/49): loss=6045.753675394343\n",
      "Gradient Descent(6/49): loss=71784.12990502489\n",
      "Gradient Descent(7/49): loss=852473.9230332244\n",
      "Gradient Descent(8/49): loss=10123916.967187898\n",
      "Gradient Descent(9/49): loss=120231822.03927374\n",
      "Gradient Descent(10/49): loss=1427877999.9767778\n",
      "Gradient Descent(11/49): loss=16957545022.92135\n",
      "Gradient Descent(12/49): loss=201388610736.53506\n",
      "Gradient Descent(13/49): loss=2391700783426.348\n",
      "Gradient Descent(14/49): loss=28403953223775.277\n",
      "Gradient Descent(15/49): loss=337326711519344.56\n",
      "Gradient Descent(16/49): loss=4006108215289690.5\n",
      "Gradient Descent(17/49): loss=4.757673343496277e+16\n",
      "Gradient Descent(18/49): loss=5.650235696997206e+17\n",
      "Gradient Descent(19/49): loss=6.710247031880041e+18\n",
      "Gradient Descent(20/49): loss=7.969121580691844e+19\n",
      "Gradient Descent(21/49): loss=9.464167036795984e+20\n",
      "Gradient Descent(22/49): loss=1.1239690195893486e+22\n",
      "Gradient Descent(23/49): loss=1.3348310021209939e+23\n",
      "Gradient Descent(24/49): loss=1.585251704601541e+24\n",
      "Gradient Descent(25/49): loss=1.8826525327543376e+25\n",
      "Gradient Descent(26/49): loss=2.2358471836352344e+26\n",
      "Gradient Descent(27/49): loss=2.655302846168886e+27\n",
      "Gradient Descent(28/49): loss=3.153450404159102e+28\n",
      "Gradient Descent(29/49): loss=3.745052834873024e+29\n",
      "Gradient Descent(30/49): loss=4.447642720967579e+30\n",
      "Gradient Descent(31/49): loss=5.282041841753254e+31\n",
      "Gradient Descent(32/49): loss=6.272978242272654e+32\n",
      "Gradient Descent(33/49): loss=7.449819067500028e+33\n",
      "Gradient Descent(34/49): loss=8.847440879753442e+34\n",
      "Gradient Descent(35/49): loss=1.0507263251830871e+36\n",
      "Gradient Descent(36/49): loss=1.247847626717934e+37\n",
      "Gradient Descent(37/49): loss=1.4819498304986892e+38\n",
      "Gradient Descent(38/49): loss=1.7599707312754466e+39\n",
      "Gradient Descent(39/49): loss=2.0901496873911568e+40\n",
      "Gradient Descent(40/49): loss=2.4822718003584973e+41\n",
      "Gradient Descent(41/49): loss=2.947957903697234e+42\n",
      "Gradient Descent(42/49): loss=3.5010089550692456e+43\n",
      "Gradient Descent(43/49): loss=4.157815038031108e+44\n",
      "Gradient Descent(44/49): loss=4.937841094478344e+45\n",
      "Gradient Descent(45/49): loss=5.864203782827507e+46\n",
      "Gradient Descent(46/49): loss=6.964356557561797e+47\n",
      "Gradient Descent(47/49): loss=8.270903272987536e+48\n",
      "Gradient Descent(48/49): loss=9.822564422960217e+49\n",
      "Gradient Descent(49/49): loss=1.1665324651820474e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7324267264325713\n",
      "Gradient Descent(2/49): loss=4.206640373314152\n",
      "Gradient Descent(3/49): loss=46.26277146678411\n",
      "Gradient Descent(4/49): loss=556.2097263808582\n",
      "Gradient Descent(5/49): loss=6746.000310996185\n",
      "Gradient Descent(6/49): loss=81898.267289311\n",
      "Gradient Descent(7/49): loss=994404.2035811319\n",
      "Gradient Descent(8/49): loss=12074303.10015481\n",
      "Gradient Descent(9/49): loss=146609966.05592522\n",
      "Gradient Descent(10/49): loss=1780186205.2202387\n",
      "Gradient Descent(11/49): loss=21615609685.08261\n",
      "Gradient Descent(12/49): loss=262463898962.07462\n",
      "Gradient Descent(13/49): loss=3186923723569.7812\n",
      "Gradient Descent(14/49): loss=38696685129904.24\n",
      "Gradient Descent(15/49): loss=469867989065618.9\n",
      "Gradient Descent(16/49): loss=5705293010266173.0\n",
      "Gradient Descent(17/49): loss=6.927556056591806e+16\n",
      "Gradient Descent(18/49): loss=8.411668398343342e+17\n",
      "Gradient Descent(19/49): loss=1.021372684188418e+19\n",
      "Gradient Descent(20/49): loss=1.2401845990652029e+20\n",
      "Gradient Descent(21/49): loss=1.5058732855978822e+21\n",
      "Gradient Descent(22/49): loss=1.8284813034984426e+22\n",
      "Gradient Descent(23/49): loss=2.2202026619497227e+23\n",
      "Gradient Descent(24/49): loss=2.6958437314603185e+24\n",
      "Gradient Descent(25/49): loss=3.2733828983303995e+25\n",
      "Gradient Descent(26/49): loss=3.9746501156718175e+26\n",
      "Gradient Descent(27/49): loss=4.826152036801995e+27\n",
      "Gradient Descent(28/49): loss=5.860073919586046e+28\n",
      "Gradient Descent(29/49): loss=7.115496171929095e+29\n",
      "Gradient Descent(30/49): loss=8.639871521674237e+30\n",
      "Gradient Descent(31/49): loss=1.0490818645300451e+32\n",
      "Gradient Descent(32/49): loss=1.2738300051394343e+33\n",
      "Gradient Descent(33/49): loss=1.5467266539017186e+34\n",
      "Gradient Descent(34/49): loss=1.878086818678872e+35\n",
      "Gradient Descent(35/49): loss=2.280435324236328e+36\n",
      "Gradient Descent(36/49): loss=2.7689802283384325e+37\n",
      "Gradient Descent(37/49): loss=3.3621876592780637e+38\n",
      "Gradient Descent(38/49): loss=4.082479802676318e+39\n",
      "Gradient Descent(39/49): loss=4.957082420211743e+40\n",
      "Gradient Descent(40/49): loss=6.019053934979262e+41\n",
      "Gradient Descent(41/49): loss=7.308534981074974e+42\n",
      "Gradient Descent(42/49): loss=8.874265648157955e+43\n",
      "Gradient Descent(43/49): loss=1.0775427770134715e+45\n",
      "Gradient Descent(44/49): loss=1.3083881893201139e+46\n",
      "Gradient Descent(45/49): loss=1.5886883476654363e+47\n",
      "Gradient Descent(46/49): loss=1.9290380994034098e+48\n",
      "Gradient Descent(47/49): loss=2.3423020596948347e+49\n",
      "Gradient Descent(48/49): loss=2.8441008710752842e+50\n",
      "Gradient Descent(49/49): loss=3.453401635955124e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7325362869542658\n",
      "Gradient Descent(2/49): loss=4.180144082140278\n",
      "Gradient Descent(3/49): loss=45.96562976149393\n",
      "Gradient Descent(4/49): loss=553.6684732240212\n",
      "Gradient Descent(5/49): loss=6730.20432029539\n",
      "Gradient Descent(6/49): loss=81896.24149873323\n",
      "Gradient Descent(7/49): loss=996710.3082320843\n",
      "Gradient Descent(8/49): loss=12130739.490800196\n",
      "Gradient Descent(9/49): loss=147641533.34989837\n",
      "Gradient Descent(10/49): loss=1796927292.9642587\n",
      "Gradient Descent(11/49): loss=21870193580.599022\n",
      "Gradient Descent(12/49): loss=266179612545.146\n",
      "Gradient Descent(13/49): loss=3239641543188.2153\n",
      "Gradient Descent(14/49): loss=39429305960773.8\n",
      "Gradient Descent(15/49): loss=479889564877578.2\n",
      "Gradient Descent(16/49): loss=5840680908203389.0\n",
      "Gradient Descent(17/49): loss=7.108625811163152e+16\n",
      "Gradient Descent(18/49): loss=8.651827024664065e+17\n",
      "Gradient Descent(19/49): loss=1.053003953977702e+19\n",
      "Gradient Descent(20/49): loss=1.2815990471525676e+20\n",
      "Gradient Descent(21/49): loss=1.5598195158320756e+21\n",
      "Gradient Descent(22/49): loss=1.898438460434617e+22\n",
      "Gradient Descent(23/49): loss=2.3105676980421842e+23\n",
      "Gradient Descent(24/49): loss=2.8121654709912197e+24\n",
      "Gradient Descent(25/49): loss=3.422654373181194e+25\n",
      "Gradient Descent(26/49): loss=4.16567342110466e+26\n",
      "Gradient Descent(27/49): loss=5.069993390880747e+27\n",
      "Gradient Descent(28/49): loss=6.170630864470866e+28\n",
      "Gradient Descent(29/49): loss=7.51020412256319e+29\n",
      "Gradient Descent(30/49): loss=9.140583386266366e+30\n",
      "Gradient Descent(31/49): loss=1.1124899307367191e+32\n",
      "Gradient Descent(32/49): loss=1.3539987478809211e+33\n",
      "Gradient Descent(33/49): loss=1.6479363620388332e+34\n",
      "Gradient Descent(34/49): loss=2.005684464317255e+35\n",
      "Gradient Descent(35/49): loss=2.4410955805517968e+36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=2.971029461215871e+37\n",
      "Gradient Descent(37/49): loss=3.6160059154329986e+38\n",
      "Gradient Descent(38/49): loss=4.400999367773131e+39\n",
      "Gradient Descent(39/49): loss=5.356405904225466e+40\n",
      "Gradient Descent(40/49): loss=6.519220252771573e+41\n",
      "Gradient Descent(41/49): loss=7.934468272955278e+42\n",
      "Gradient Descent(42/49): loss=9.656950422524646e+43\n",
      "Gradient Descent(43/49): loss=1.1753363710705771e+45\n",
      "Gradient Descent(44/49): loss=1.4304884303218844e+46\n",
      "Gradient Descent(45/49): loss=1.7410310781251981e+47\n",
      "Gradient Descent(46/49): loss=2.1189889765943293e+48\n",
      "Gradient Descent(47/49): loss=2.578997204210376e+49\n",
      "Gradient Descent(48/49): loss=3.1388679473051854e+50\n",
      "Gradient Descent(49/49): loss=3.8202802137726354e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7316862487320784\n",
      "Gradient Descent(2/49): loss=4.152750465042015\n",
      "Gradient Descent(3/49): loss=45.170542123772684\n",
      "Gradient Descent(4/49): loss=537.9600977284192\n",
      "Gradient Descent(5/49): loss=6465.306448418536\n",
      "Gradient Descent(6/49): loss=77782.06407854993\n",
      "Gradient Descent(7/49): loss=935916.8977845851\n",
      "Gradient Descent(8/49): loss=11261805.693775877\n",
      "Gradient Descent(9/49): loss=135513207.37006512\n",
      "Gradient Descent(10/49): loss=1630631730.412021\n",
      "Gradient Descent(11/49): loss=19621414603.77916\n",
      "Gradient Descent(12/49): loss=236104779793.03973\n",
      "Gradient Descent(13/49): loss=2841052461456.5825\n",
      "Gradient Descent(14/49): loss=34186428324917.43\n",
      "Gradient Descent(15/49): loss=411365822693526.0\n",
      "Gradient Descent(16/49): loss=4949971331462205.0\n",
      "Gradient Descent(17/49): loss=5.956308189079341e+16\n",
      "Gradient Descent(18/49): loss=7.167234892544817e+17\n",
      "Gradient Descent(19/49): loss=8.624344875123707e+18\n",
      "Gradient Descent(20/49): loss=1.037768757969997e+20\n",
      "Gradient Descent(21/49): loss=1.2487487578625304e+21\n",
      "Gradient Descent(22/49): loss=1.5026213193328783e+22\n",
      "Gradient Descent(23/49): loss=1.8081065667512498e+23\n",
      "Gradient Descent(24/49): loss=2.1756974393125708e+24\n",
      "Gradient Descent(25/49): loss=2.6180201070429976e+25\n",
      "Gradient Descent(26/49): loss=3.1502676599403565e+26\n",
      "Gradient Descent(27/49): loss=3.7907219667901264e+27\n",
      "Gradient Descent(28/49): loss=4.561381628689059e+28\n",
      "Gradient Descent(29/49): loss=5.488717596495245e+29\n",
      "Gradient Descent(30/49): loss=6.604582406479016e+30\n",
      "Gradient Descent(31/49): loss=7.947304264993634e+31\n",
      "Gradient Descent(32/49): loss=9.563003562258104e+32\n",
      "Gradient Descent(33/49): loss=1.150717703543649e+34\n",
      "Gradient Descent(34/49): loss=1.3846603994531039e+35\n",
      "Gradient Descent(35/49): loss=1.6661640086958934e+36\n",
      "Gradient Descent(36/49): loss=2.004897738802985e+37\n",
      "Gradient Descent(37/49): loss=2.4124965622102773e+38\n",
      "Gradient Descent(38/49): loss=2.9029608593160865e+39\n",
      "Gradient Descent(39/49): loss=3.493137309592868e+40\n",
      "Gradient Descent(40/49): loss=4.203297548608465e+41\n",
      "Gradient Descent(41/49): loss=5.05783446691855e+42\n",
      "Gradient Descent(42/49): loss=6.086100067604843e+43\n",
      "Gradient Descent(43/49): loss=7.323413661551976e+44\n",
      "Gradient Descent(44/49): loss=8.812275030389464e+45\n",
      "Gradient Descent(45/49): loss=1.0603824227343815e+47\n",
      "Gradient Descent(46/49): loss=1.2759598157870239e+48\n",
      "Gradient Descent(47/49): loss=1.5353644275854643e+49\n",
      "Gradient Descent(48/49): loss=1.8475063997535142e+50\n",
      "Gradient Descent(49/49): loss=2.2231073195423515e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7421513421132514\n",
      "Gradient Descent(2/49): loss=4.369211364708129\n",
      "Gradient Descent(3/49): loss=49.05065111584947\n",
      "Gradient Descent(4/49): loss=601.003060171679\n",
      "Gradient Descent(5/49): loss=7428.409632973148\n",
      "Gradient Descent(6/49): loss=91910.41547543717\n",
      "Gradient Descent(7/49): loss=1137381.5779476536\n",
      "Gradient Descent(8/49): loss=14075462.92012978\n",
      "Gradient Descent(9/49): loss=174189799.12029245\n",
      "Gradient Descent(10/49): loss=2155676668.2848024\n",
      "Gradient Descent(11/49): loss=26677476291.560043\n",
      "Gradient Descent(12/49): loss=330145908246.92725\n",
      "Gradient Descent(13/49): loss=4085705972733.1606\n",
      "Gradient Descent(14/49): loss=50562472572429.86\n",
      "Gradient Descent(15/49): loss=625733632063479.1\n",
      "Gradient Descent(16/49): loss=7743738756775086.0\n",
      "Gradient Descent(17/49): loss=9.583229487482686e+16\n",
      "Gradient Descent(18/49): loss=1.185968306748914e+18\n",
      "Gradient Descent(19/49): loss=1.4676898079731358e+19\n",
      "Gradient Descent(20/49): loss=1.816332999941457e+20\n",
      "Gradient Descent(21/49): loss=2.2477948329109776e+21\n",
      "Gradient Descent(22/49): loss=2.7817485070330285e+22\n",
      "Gradient Descent(23/49): loss=3.442540503734296e+23\n",
      "Gradient Descent(24/49): loss=4.260300702917063e+24\n",
      "Gradient Descent(25/49): loss=5.272316203567456e+25\n",
      "Gradient Descent(26/49): loss=6.524731489344651e+26\n",
      "Gradient Descent(27/49): loss=8.074652460950534e+27\n",
      "Gradient Descent(28/49): loss=9.992750272039702e+28\n",
      "Gradient Descent(29/49): loss=1.236648369477872e+30\n",
      "Gradient Descent(30/49): loss=1.5304086944025345e+31\n",
      "Gradient Descent(31/49): loss=1.893950479142067e+32\n",
      "Gradient Descent(32/49): loss=2.3438499993904079e+33\n",
      "Gradient Descent(33/49): loss=2.9006211514732647e+34\n",
      "Gradient Descent(34/49): loss=3.589650816631752e+35\n",
      "Gradient Descent(35/49): loss=4.442356416935118e+36\n",
      "Gradient Descent(36/49): loss=5.497618443456829e+37\n",
      "Gradient Descent(37/49): loss=6.803553275153176e+38\n",
      "Gradient Descent(38/49): loss=8.419707122988707e+39\n",
      "Gradient Descent(39/49): loss=1.041977113573948e+41\n",
      "Gradient Descent(40/49): loss=1.289494146711482e+42\n",
      "Gradient Descent(41/49): loss=1.5958077511893174e+43\n",
      "Gradient Descent(42/49): loss=1.974884791257377e+44\n",
      "Gradient Descent(43/49): loss=2.4440098977041497e+45\n",
      "Gradient Descent(44/49): loss=3.0245735885548973e+46\n",
      "Gradient Descent(45/49): loss=3.74304760433964e+47\n",
      "Gradient Descent(46/49): loss=4.6321919299198406e+48\n",
      "Gradient Descent(47/49): loss=5.732548538986653e+49\n",
      "Gradient Descent(48/49): loss=7.094289970926648e+50\n",
      "Gradient Descent(49/49): loss=8.779507028908131e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7460932135013921\n",
      "Gradient Descent(2/49): loss=4.534435624604529\n",
      "Gradient Descent(3/49): loss=52.28562727774511\n",
      "Gradient Descent(4/49): loss=655.519554334588\n",
      "Gradient Descent(5/49): loss=8284.604427315497\n",
      "Gradient Descent(6/49): loss=104796.61327766518\n",
      "Gradient Descent(7/49): loss=1325807.7451782564\n",
      "Gradient Descent(8/49): loss=16773545.265959946\n",
      "Gradient Descent(9/49): loss=212212801.09571722\n",
      "Gradient Descent(10/49): loss=2684842682.6843925\n",
      "Gradient Descent(11/49): loss=33967707647.83949\n",
      "Gradient Descent(12/49): loss=429747803076.90173\n",
      "Gradient Descent(13/49): loss=5437022105309.355\n",
      "Gradient Descent(14/49): loss=68787342901254.81\n",
      "Gradient Descent(15/49): loss=870273921319060.6\n",
      "Gradient Descent(16/49): loss=1.1010407821467386e+16\n",
      "Gradient Descent(17/49): loss=1.3929991170795941e+17\n",
      "Gradient Descent(18/49): loss=1.7623748108818296e+18\n",
      "Gradient Descent(19/49): loss=2.2296962976916414e+19\n",
      "Gradient Descent(20/49): loss=2.82093544985233e+20\n",
      "Gradient Descent(21/49): loss=3.568950991430226e+21\n",
      "Gradient Descent(22/49): loss=4.515314655604071e+22\n",
      "Gradient Descent(23/49): loss=5.7126215764993866e+23\n",
      "Gradient Descent(24/49): loss=7.227413317869979e+24\n",
      "Gradient Descent(25/49): loss=9.143875988952514e+25\n",
      "Gradient Descent(26/49): loss=1.1568518974086193e+27\n",
      "Gradient Descent(27/49): loss=1.463609430130986e+28\n",
      "Gradient Descent(28/49): loss=1.8517085624934635e+29\n",
      "Gradient Descent(29/49): loss=2.342718303000257e+30\n",
      "Gradient Descent(30/49): loss=2.9639270230634517e+31\n",
      "Gradient Descent(31/49): loss=3.749859036314895e+32\n",
      "Gradient Descent(32/49): loss=4.74419332285005e+33\n",
      "Gradient Descent(33/49): loss=6.002191033477795e+34\n",
      "Gradient Descent(34/49): loss=7.593766685021716e+35\n",
      "Gradient Descent(35/49): loss=9.60737373151109e+36\n",
      "Gradient Descent(36/49): loss=1.2154920455877271e+38\n",
      "Gradient Descent(37/49): loss=1.53779893878933e+39\n",
      "Gradient Descent(38/49): loss=1.9455705898907215e+40\n",
      "Gradient Descent(39/49): loss=2.4614693278613978e+41\n",
      "Gradient Descent(40/49): loss=3.114166755749894e+42\n",
      "Gradient Descent(41/49): loss=3.9399372045166857e+43\n",
      "Gradient Descent(42/49): loss=4.984673716291362e+44\n",
      "Gradient Descent(43/49): loss=6.306438597397413e+45\n",
      "Gradient Descent(44/49): loss=7.978690290752642e+46\n",
      "Gradient Descent(45/49): loss=1.0094365904398355e+48\n",
      "Gradient Descent(46/49): loss=1.2771046286879811e+49\n",
      "Gradient Descent(47/49): loss=1.6157490703854827e+50\n",
      "Gradient Descent(48/49): loss=2.0441904287306302e+51\n",
      "Gradient Descent(49/49): loss=2.586239772935139e+52\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7461498792906885\n",
      "Gradient Descent(2/49): loss=4.505544555396353\n",
      "Gradient Descent(3/49): loss=51.94828741764314\n",
      "Gradient Descent(4/49): loss=652.4994093005241\n",
      "Gradient Descent(5/49): loss=8264.779229915323\n",
      "Gradient Descent(6/49): loss=104787.38494918794\n",
      "Gradient Descent(7/49): loss=1328783.611378618\n",
      "Gradient Descent(8/49): loss=16850512.198102634\n",
      "Gradient Descent(9/49): loss=213685489.4938814\n",
      "Gradient Descent(10/49): loss=2709802818.1149793\n",
      "Gradient Descent(11/49): loss=34363747962.39059\n",
      "Gradient Descent(12/49): loss=435776095183.19257\n",
      "Gradient Descent(13/49): loss=5526196092097.912\n",
      "Gradient Descent(14/49): loss=70079207566803.96\n",
      "Gradient Descent(15/49): loss=888693643572689.2\n",
      "Gradient Descent(16/49): loss=1.1269767735990776e+16\n",
      "Gradient Descent(17/49): loss=1.4291501436118909e+17\n",
      "Gradient Descent(18/49): loss=1.812344478498023e+18\n",
      "Gradient Descent(19/49): loss=2.2982837201817825e+19\n",
      "Gradient Descent(20/49): loss=2.914516595010994e+20\n",
      "Gradient Descent(21/49): loss=3.6959783981436277e+21\n",
      "Gradient Descent(22/49): loss=4.686971535152212e+22\n",
      "Gradient Descent(23/49): loss=5.943677101132747e+23\n",
      "Gradient Descent(24/49): loss=7.537339883030104e+24\n",
      "Gradient Descent(25/49): loss=9.558307348407194e+25\n",
      "Gradient Descent(26/49): loss=1.2121151597834873e+27\n",
      "Gradient Descent(27/49): loss=1.5371164653141126e+28\n",
      "Gradient Descent(28/49): loss=1.949259531051301e+29\n",
      "Gradient Descent(29/49): loss=2.4719094519737014e+30\n",
      "Gradient Descent(30/49): loss=3.1346961455981156e+31\n",
      "Gradient Descent(31/49): loss=3.975194122657615e+32\n",
      "Gradient Descent(32/49): loss=5.041052650350744e+33\n",
      "Gradient Descent(33/49): loss=6.392697070758141e+34\n",
      "Gradient Descent(34/49): loss=8.106754416785624e+35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=1.0280397529657904e+37\n",
      "Gradient Descent(36/49): loss=1.3036853953410005e+38\n",
      "Gradient Descent(37/49): loss=1.6532391915023405e+39\n",
      "Gradient Descent(38/49): loss=2.0965179437362547e+40\n",
      "Gradient Descent(39/49): loss=2.6586518823171103e+41\n",
      "Gradient Descent(40/49): loss=3.3715093412229393e+42\n",
      "Gradient Descent(41/49): loss=4.2755034284694423e+43\n",
      "Gradient Descent(42/49): loss=5.421883114291919e+44\n",
      "Gradient Descent(43/49): loss=6.875638622880763e+45\n",
      "Gradient Descent(44/49): loss=8.719185839295538e+46\n",
      "Gradient Descent(45/49): loss=1.1057038606883494e+48\n",
      "Gradient Descent(46/49): loss=1.4021733795731298e+49\n",
      "Gradient Descent(47/49): loss=1.7781345044409444e+50\n",
      "Gradient Descent(48/49): loss=2.2549011141874653e+51\n",
      "Gradient Descent(49/49): loss=2.859501922978816e+52\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7453434047857227\n",
      "Gradient Descent(2/49): loss=4.476322611573765\n",
      "Gradient Descent(3/49): loss=51.05666031167453\n",
      "Gradient Descent(4/49): loss=634.1155114333834\n",
      "Gradient Descent(5/49): loss=7941.52887596007\n",
      "Gradient Descent(6/49): loss=99554.12584478005\n",
      "Gradient Descent(7/49): loss=1248187.7026030233\n",
      "Gradient Descent(8/49): loss=15649974.77710221\n",
      "Gradient Descent(9/49): loss=196223205.65366116\n",
      "Gradient Descent(10/49): loss=2460298514.2043657\n",
      "Gradient Descent(11/49): loss=30847886618.484818\n",
      "Gradient Descent(12/49): loss=386779163798.07745\n",
      "Gradient Descent(13/49): loss=4849542109400.636\n",
      "Gradient Descent(14/49): loss=60804875264222.09\n",
      "Gradient Descent(15/49): loss=762388031250179.0\n",
      "Gradient Descent(16/49): loss=9559028085732402.0\n",
      "Gradient Descent(17/49): loss=1.1985368894361891e+17\n",
      "Gradient Descent(18/49): loss=1.5027580863730698e+18\n",
      "Gradient Descent(19/49): loss=1.8841988812158026e+19\n",
      "Gradient Descent(20/49): loss=2.362459704041872e+20\n",
      "Gradient Descent(21/49): loss=2.9621161061416467e+21\n",
      "Gradient Descent(22/49): loss=3.713981580829922e+22\n",
      "Gradient Descent(23/49): loss=4.656690922460637e+23\n",
      "Gradient Descent(24/49): loss=5.838685484940279e+24\n",
      "Gradient Descent(25/49): loss=7.320702352742583e+25\n",
      "Gradient Descent(26/49): loss=9.178895331095e+26\n",
      "Gradient Descent(27/49): loss=1.1508748128194875e+28\n",
      "Gradient Descent(28/49): loss=1.4429980809295118e+29\n",
      "Gradient Descent(29/49): loss=1.8092701642023387e+30\n",
      "Gradient Descent(30/49): loss=2.2685120447035895e+31\n",
      "Gradient Descent(31/49): loss=2.844321980644636e+32\n",
      "Gradient Descent(32/49): loss=3.5662881087480755e+33\n",
      "Gradient Descent(33/49): loss=4.471508838009762e+34\n",
      "Gradient Descent(34/49): loss=5.6064991606689035e+35\n",
      "Gradient Descent(35/49): loss=7.029580836649201e+36\n",
      "Gradient Descent(36/49): loss=8.81387927169335e+37\n",
      "Gradient Descent(37/49): loss=1.1051081084518202e+39\n",
      "Gradient Descent(38/49): loss=1.3856145446513766e+40\n",
      "Gradient Descent(39/49): loss=1.7373211287347586e+41\n",
      "Gradient Descent(40/49): loss=2.1783003909702808e+42\n",
      "Gradient Descent(41/49): loss=2.7312121603890863e+43\n",
      "Gradient Descent(42/49): loss=3.4244679457338573e+44\n",
      "Gradient Descent(43/49): loss=4.293690867899462e+45\n",
      "Gradient Descent(44/49): loss=5.383546162857328e+46\n",
      "Gradient Descent(45/49): loss=6.750036315910788e+47\n",
      "Gradient Descent(46/49): loss=8.463378763326426e+48\n",
      "Gradient Descent(47/49): loss=1.0611614032755002e+50\n",
      "Gradient Descent(48/49): loss=1.3305129727634265e+51\n",
      "Gradient Descent(49/49): loss=1.6682332821637505e+52\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7559331835826448\n",
      "Gradient Descent(2/49): loss=4.704600080400979\n",
      "Gradient Descent(3/49): loss=55.31945208042278\n",
      "Gradient Descent(4/49): loss=706.2551251946563\n",
      "Gradient Descent(5/49): loss=9089.441051337097\n",
      "Gradient Descent(6/49): loss=117094.10429215083\n",
      "Gradient Descent(7/49): loss=1508704.4025916758\n",
      "Gradient Descent(8/49): loss=19439659.136382677\n",
      "Gradient Descent(9/49): loss=250482178.2237446\n",
      "Gradient Descent(10/49): loss=3227497710.372674\n",
      "Gradient Descent(11/49): loss=41586779439.28226\n",
      "Gradient Descent(12/49): loss=535851801343.42566\n",
      "Gradient Descent(13/49): loss=6904529923520.606\n",
      "Gradient Descent(14/49): loss=88965892754048.05\n",
      "Gradient Descent(15/49): loss=1146338734020366.0\n",
      "Gradient Descent(16/49): loss=1.477074475620054e+16\n",
      "Gradient Descent(17/49): loss=1.9032323885620403e+17\n",
      "Gradient Descent(18/49): loss=2.4523431856660347e+18\n",
      "Gradient Descent(19/49): loss=3.159880599175893e+19\n",
      "Gradient Descent(20/49): loss=4.071553059716156e+20\n",
      "Gradient Descent(21/49): loss=5.246256558687003e+21\n",
      "Gradient Descent(22/49): loss=6.759879455307769e+22\n",
      "Gradient Descent(23/49): loss=8.710205026978414e+23\n",
      "Gradient Descent(24/49): loss=1.1223228478198743e+25\n",
      "Gradient Descent(25/49): loss=1.4461296500335986e+26\n",
      "Gradient Descent(26/49): loss=1.863359521521507e+27\n",
      "Gradient Descent(27/49): loss=2.400966404612618e+28\n",
      "Gradient Descent(28/49): loss=3.0936808541227575e+29\n",
      "Gradient Descent(29/49): loss=3.986253705499013e+30\n",
      "Gradient Descent(30/49): loss=5.136347074530588e+31\n",
      "Gradient Descent(31/49): loss=6.618259453392273e+32\n",
      "Gradient Descent(32/49): loss=8.527725552194926e+33\n",
      "Gradient Descent(33/49): loss=1.098810096607553e+35\n",
      "Gradient Descent(34/49): loss=1.4158331210552795e+36\n",
      "Gradient Descent(35/49): loss=1.8243219941881162e+37\n",
      "Gradient Descent(36/49): loss=2.350665971140659e+38\n",
      "Gradient Descent(37/49): loss=3.0288679988960853e+39\n",
      "Gradient Descent(38/49): loss=3.9027413794079037e+40\n",
      "Gradient Descent(39/49): loss=5.0287402026413845e+41\n",
      "Gradient Descent(40/49): loss=6.479606401564444e+42\n",
      "Gradient Descent(41/49): loss=8.349069036642968e+43\n",
      "Gradient Descent(42/49): loss=1.0757899393673036e+45\n",
      "Gradient Descent(43/49): loss=1.386171306722428e+46\n",
      "Gradient Descent(44/49): loss=1.786102306097604e+47\n",
      "Gradient Descent(45/49): loss=2.301419335673782e+48\n",
      "Gradient Descent(46/49): loss=2.965412978042331e+49\n",
      "Gradient Descent(47/49): loss=3.820978643063045e+50\n",
      "Gradient Descent(48/49): loss=4.923387703112559e+51\n",
      "Gradient Descent(49/49): loss=6.343858141988572e+52\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7600745472019769\n",
      "Gradient Descent(2/49): loss=4.882971314384507\n",
      "Gradient Descent(3/49): loss=58.9536417785777\n",
      "Gradient Descent(4/49): loss=770.0221527045007\n",
      "Gradient Descent(5/49): loss=10132.154097345927\n",
      "Gradient Descent(6/49): loss=133433.319674115\n",
      "Gradient Descent(7/49): loss=1757451.2887864604\n",
      "Gradient Descent(8/49): loss=23148002.49127713\n",
      "Gradient Descent(9/49): loss=304892159.8414962\n",
      "Gradient Descent(10/49): loss=4015869411.3845034\n",
      "Gradient Descent(11/49): loss=52894808241.804535\n",
      "Gradient Descent(12/49): loss=696701179236.5234\n",
      "Gradient Descent(13/49): loss=9176563035950.268\n",
      "Gradient Descent(14/49): loss=120868619193657.56\n",
      "Gradient Descent(15/49): loss=1592014686110046.5\n",
      "Gradient Descent(16/49): loss=2.0969138047089184e+16\n",
      "Gradient Descent(17/49): loss=2.7619390342370554e+17\n",
      "Gradient Descent(18/49): loss=3.637873531960598e+18\n",
      "Gradient Descent(19/49): loss=4.791606067525434e+19\n",
      "Gradient Descent(20/49): loss=6.311238833525837e+20\n",
      "Gradient Descent(21/49): loss=8.312815171465788e+21\n",
      "Gradient Descent(22/49): loss=1.0949180960776584e+23\n",
      "Gradient Descent(23/49): loss=1.442165635094894e+24\n",
      "Gradient Descent(24/49): loss=1.8995409122374555e+25\n",
      "Gradient Descent(25/49): loss=2.5019703628054335e+26\n",
      "Gradient Descent(26/49): loss=3.295457158110576e+27\n",
      "Gradient Descent(27/49): loss=4.3405941342826135e+28\n",
      "Gradient Descent(28/49): loss=5.7171908280460706e+29\n",
      "Gradient Descent(29/49): loss=7.530367952657326e+30\n",
      "Gradient Descent(30/49): loss=9.918584704962321e+31\n",
      "Gradient Descent(31/49): loss=1.3064211890840758e+33\n",
      "Gradient Descent(32/49): loss=1.720745826200355e+34\n",
      "Gradient Descent(33/49): loss=2.2664713517559e+35\n",
      "Gradient Descent(34/49): loss=2.9852708692446507e+36\n",
      "Gradient Descent(35/49): loss=3.932033888650923e+37\n",
      "Gradient Descent(36/49): loss=5.179057840540733e+38\n",
      "Gradient Descent(37/49): loss=6.821568906891914e+39\n",
      "Gradient Descent(38/49): loss=8.984993754504182e+40\n",
      "Gradient Descent(39/49): loss=1.1834537460570987e+42\n",
      "Gradient Descent(40/49): loss=1.558779902717784e+43\n",
      "Gradient Descent(41/49): loss=2.053138784014314e+44\n",
      "Gradient Descent(42/49): loss=2.704280995074488e+45\n",
      "Gradient Descent(43/49): loss=3.5619295477056705e+46\n",
      "Gradient Descent(44/49): loss=4.691576846461996e+47\n",
      "Gradient Descent(45/49): loss=6.179485868954394e+48\n",
      "Gradient Descent(46/49): loss=8.139277444299777e+49\n",
      "Gradient Descent(47/49): loss=1.0720606652426367e+51\n",
      "Gradient Descent(48/49): loss=1.412059089797195e+52\n",
      "Gradient Descent(49/49): loss=1.8598862337959264e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7600765829624436\n",
      "Gradient Descent(2/49): loss=4.851537973458858\n",
      "Gradient Descent(3/49): loss=58.57171386143025\n",
      "Gradient Descent(4/49): loss=766.4462871494602\n",
      "Gradient Descent(5/49): loss=10107.407593485266\n",
      "Gradient Descent(6/49): loss=133413.4606991477\n",
      "Gradient Descent(7/49): loss=1761270.29925541\n",
      "Gradient Descent(8/49): loss=23252319.149012864\n",
      "Gradient Descent(9/49): loss=306979779.495636\n",
      "Gradient Descent(10/49): loss=4052789469.1814127\n",
      "Gradient Descent(11/49): loss=53505510067.61649\n",
      "Gradient Descent(12/49): loss=706387523222.8\n",
      "Gradient Descent(13/49): loss=9325831031167.621\n",
      "Gradient Descent(14/49): loss=123120981226218.81\n",
      "Gradient Descent(15/49): loss=1625461151027921.5\n",
      "Gradient Descent(16/49): loss=2.1459575193591476e+16\n",
      "Gradient Descent(17/49): loss=2.8331244166761146e+17\n",
      "Gradient Descent(18/49): loss=3.740332177196307e+18\n",
      "Gradient Descent(19/49): loss=4.93804109466694e+19\n",
      "Gradient Descent(20/49): loss=6.51927387661386e+20\n",
      "Gradient Descent(21/49): loss=8.606840458297431e+21\n",
      "Gradient Descent(22/49): loss=1.1362876307486588e+23\n",
      "Gradient Descent(23/49): loss=1.500143503354589e+24\n",
      "Gradient Descent(24/49): loss=1.980511333362179e+25\n",
      "Gradient Descent(25/49): loss=2.614699948908081e+26\n",
      "Gradient Descent(26/49): loss=3.451965009063517e+27\n",
      "Gradient Descent(27/49): loss=4.557334553349083e+28\n",
      "Gradient Descent(28/49): loss=6.016659548001623e+29\n",
      "Gradient Descent(29/49): loss=7.943281690819996e+30\n",
      "Gradient Descent(30/49): loss=1.0486836344375302e+32\n",
      "Gradient Descent(31/49): loss=1.3844874296829458e+33\n",
      "Gradient Descent(32/49): loss=1.8278204980076613e+34\n",
      "Gradient Descent(33/49): loss=2.4131152810120386e+35\n",
      "Gradient Descent(34/49): loss=3.185829990308715e+36\n",
      "Gradient Descent(35/49): loss=4.205979219896115e+37\n",
      "Gradient Descent(36/49): loss=5.5527951121094496e+38\n",
      "Gradient Descent(37/49): loss=7.330881096894263e+39\n",
      "Gradient Descent(38/49): loss=9.678336148150389e+40\n",
      "Gradient Descent(39/49): loss=1.2777480545452945e+42\n",
      "Gradient Descent(40/49): loss=1.686901618111587e+43\n",
      "Gradient Descent(41/49): loss=2.227072120411198e+44\n",
      "Gradient Descent(42/49): loss=2.9402130961645586e+45\n",
      "Gradient Descent(43/49): loss=3.881712213819726e+46\n",
      "Gradient Descent(44/49): loss=5.1246930811146535e+47\n",
      "Gradient Descent(45/49): loss=6.765694551524029e+48\n",
      "Gradient Descent(46/49): loss=8.932168627465515e+49\n",
      "Gradient Descent(47/49): loss=1.1792379301472803e+51\n",
      "Gradient Descent(48/49): loss=1.5568471150691024e+52\n",
      "Gradient Descent(49/49): loss=2.055372268594066e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7593154903972745\n",
      "Gradient Descent(2/49): loss=4.820395123061777\n",
      "Gradient Descent(3/49): loss=57.57400000988759\n",
      "Gradient Descent(4/49): loss=744.9971187800489\n",
      "Gradient Descent(5/49): loss=9714.494193306275\n",
      "Gradient Descent(6/49): loss=126788.33738325431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=1655017.8167710367\n",
      "Gradient Descent(8/49): loss=21604261.411204275\n",
      "Gradient Descent(9/49): loss=282019602.5026445\n",
      "Gradient Descent(10/49): loss=3681458655.113195\n",
      "Gradient Descent(11/49): loss=48057452533.2859\n",
      "Gradient Descent(12/49): loss=627338021712.1191\n",
      "Gradient Descent(13/49): loss=8189219009908.34\n",
      "Gradient Descent(14/49): loss=106901393042981.11\n",
      "Gradient Descent(15/49): loss=1395482015950041.0\n",
      "Gradient Descent(16/49): loss=1.8216507775879844e+16\n",
      "Gradient Descent(17/49): loss=2.377967983741424e+17\n",
      "Gradient Descent(18/49): loss=3.104179901773257e+18\n",
      "Gradient Descent(19/49): loss=4.052170983173138e+19\n",
      "Gradient Descent(20/49): loss=5.289670765381584e+20\n",
      "Gradient Descent(21/49): loss=6.90509283106121e+21\n",
      "Gradient Descent(22/49): loss=9.01385154585093e+22\n",
      "Gradient Descent(23/49): loss=1.1766607876023927e+24\n",
      "Gradient Descent(24/49): loss=1.5360033411226838e+25\n",
      "Gradient Descent(25/49): loss=2.005086163147713e+26\n",
      "Gradient Descent(26/49): loss=2.617423031585257e+27\n",
      "Gradient Descent(27/49): loss=3.4167625572349354e+28\n",
      "Gradient Descent(28/49): loss=4.4602138178069214e+29\n",
      "Gradient Descent(29/49): loss=5.822326534933375e+30\n",
      "Gradient Descent(30/49): loss=7.600417303773495e+31\n",
      "Gradient Descent(31/49): loss=9.921522409453864e+32\n",
      "Gradient Descent(32/49): loss=1.2951473976622692e+34\n",
      "Gradient Descent(33/49): loss=1.6906747900634767e+35\n",
      "Gradient Descent(34/49): loss=2.206993003974315e+36\n",
      "Gradient Descent(35/49): loss=2.880990565553256e+37\n",
      "Gradient Descent(36/49): loss=3.760821454286527e+38\n",
      "Gradient Descent(37/49): loss=4.909345480034798e+39\n",
      "Gradient Descent(38/49): loss=6.408619323011854e+40\n",
      "Gradient Descent(39/49): loss=8.3657591005369e+41\n",
      "Gradient Descent(40/49): loss=1.092059331358828e+43\n",
      "Gradient Descent(41/49): loss=1.4255652940464784e+44\n",
      "Gradient Descent(42/49): loss=1.860921242311217e+45\n",
      "Gradient Descent(43/49): loss=2.4292313263710935e+46\n",
      "Gradient Descent(44/49): loss=3.171098648803415e+47\n",
      "Gradient Descent(45/49): loss=4.139526166684469e+48\n",
      "Gradient Descent(46/49): loss=5.403703505449598e+49\n",
      "Gradient Descent(47/49): loss=7.053950234646239e+50\n",
      "Gradient Descent(48/49): loss=9.208168779557436e+51\n",
      "Gradient Descent(49/49): loss=1.2020267999107623e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7700255928449246\n",
      "Gradient Descent(2/49): loss=5.060828311710432\n",
      "Gradient Descent(3/49): loss=62.247427616294246\n",
      "Gradient Descent(4/49): loss=827.306337608058\n",
      "Gradient Descent(5/49): loss=11077.59894233043\n",
      "Gradient Descent(6/49): loss=148465.3169468916\n",
      "Gradient Descent(7/49): loss=1990100.4025321521\n",
      "Gradient Descent(8/49): loss=26677229.078791603\n",
      "Gradient Descent(9/49): loss=357610547.5573039\n",
      "Gradient Descent(10/49): loss=4793810947.481445\n",
      "Gradient Descent(11/49): loss=64261630024.55469\n",
      "Gradient Descent(12/49): loss=861435244785.343\n",
      "Gradient Descent(13/49): loss=11547648402403.918\n",
      "Gradient Descent(14/49): loss=154797688788734.2\n",
      "Gradient Descent(15/49): loss=2075082621131273.8\n",
      "Gradient Descent(16/49): loss=2.7816745333897228e+16\n",
      "Gradient Descent(17/49): loss=3.72886994098561e+17\n",
      "Gradient Descent(18/49): loss=4.998597380994686e+18\n",
      "Gradient Descent(19/49): loss=6.700683095146397e+19\n",
      "Gradient Descent(20/49): loss=8.982350551460744e+20\n",
      "Gradient Descent(21/49): loss=1.2040954673386542e+22\n",
      "Gradient Descent(22/49): loss=1.6141052235263655e+23\n",
      "Gradient Descent(23/49): loss=2.1637284943640348e+24\n",
      "Gradient Descent(24/49): loss=2.9005054497591435e+25\n",
      "Gradient Descent(25/49): loss=3.88816428955667e+26\n",
      "Gradient Descent(26/49): loss=5.212133472750111e+27\n",
      "Gradient Descent(27/49): loss=6.9869309305496505e+28\n",
      "Gradient Descent(28/49): loss=9.366069400082676e+29\n",
      "Gradient Descent(29/49): loss=1.2555334649667816e+31\n",
      "Gradient Descent(30/49): loss=1.683058510795974e+32\n",
      "Gradient Descent(31/49): loss=2.2561612492246052e+33\n",
      "Gradient Descent(32/49): loss=3.0244127282867865e+34\n",
      "Gradient Descent(33/49): loss=4.054263565676762e+35\n",
      "Gradient Descent(34/49): loss=5.434791655993665e+36\n",
      "Gradient Descent(35/49): loss=7.285407045096211e+37\n",
      "Gradient Descent(36/49): loss=9.766180411755523e+38\n",
      "Gradient Descent(37/49): loss=1.3091688528118128e+40\n",
      "Gradient Descent(38/49): loss=1.754957427480595e+41\n",
      "Gradient Descent(39/49): loss=2.352542657621603e+42\n",
      "Gradient Descent(40/49): loss=3.153613226888654e+43\n",
      "Gradient Descent(41/49): loss=4.227458470343582e+44\n",
      "Gradient Descent(42/49): loss=5.6669616191683927e+45\n",
      "Gradient Descent(43/49): loss=7.596633821104734e+46\n",
      "Gradient Descent(44/49): loss=1.018338384660184e+48\n",
      "Gradient Descent(45/49): loss=1.365095501630362e+49\n",
      "Gradient Descent(46/49): loss=1.8299278085184685e+50\n",
      "Gradient Descent(47/49): loss=2.4530414028834318e+51\n",
      "Gradient Descent(48/49): loss=3.288333067702863e+52\n",
      "Gradient Descent(49/49): loss=4.408052123147132e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7743707275343249\n",
      "Gradient Descent(2/49): loss=5.253103706946794\n",
      "Gradient Descent(3/49): loss=66.32096714833638\n",
      "Gradient Descent(4/49): loss=901.6678909104819\n",
      "Gradient Descent(5/49): loss=12342.612321454555\n",
      "Gradient Descent(6/49): loss=169087.26555080473\n",
      "Gradient Descent(7/49): loss=2316703.460776911\n",
      "Gradient Descent(8/49): loss=31742526.091429796\n",
      "Gradient Descent(9/49): loss=434925785.55851877\n",
      "Gradient Descent(10/49): loss=5959220481.450553\n",
      "Gradient Descent(11/49): loss=81651450053.27716\n",
      "Gradient Descent(12/49): loss=1118763750098.382\n",
      "Gradient Descent(13/49): loss=15328966638191.152\n",
      "Gradient Descent(14/49): loss=210032921893965.44\n",
      "Gradient Descent(15/49): loss=2877808359336512.5\n",
      "Gradient Descent(16/49): loss=3.943087055471371e+16\n",
      "Gradient Descent(17/49): loss=5.402700106021719e+17\n",
      "Gradient Descent(18/49): loss=7.402618310339116e+18\n",
      "Gradient Descent(19/49): loss=1.0142846497756057e+20\n",
      "Gradient Descent(20/49): loss=1.3897425311454307e+21\n",
      "Gradient Descent(21/49): loss=1.9041837055330157e+22\n",
      "Gradient Descent(22/49): loss=2.609055636679092e+23\n",
      "Gradient Descent(23/49): loss=3.574850102701408e+24\n",
      "Gradient Descent(24/49): loss=4.898152832436584e+25\n",
      "Gradient Descent(25/49): loss=6.711302706588068e+26\n",
      "Gradient Descent(26/49): loss=9.195626506625371e+27\n",
      "Gradient Descent(27/49): loss=1.259957277241338e+29\n",
      "Gradient Descent(28/49): loss=1.726355827228992e+30\n",
      "Gradient Descent(29/49): loss=2.3654011894219467e+31\n",
      "Gradient Descent(30/49): loss=3.241002056858444e+32\n",
      "Gradient Descent(31/49): loss=4.440724211831367e+33\n",
      "Gradient Descent(32/49): loss=6.084547673709389e+34\n",
      "Gradient Descent(33/49): loss=8.336865481311732e+35\n",
      "Gradient Descent(34/49): loss=1.1422924066122815e+37\n",
      "Gradient Descent(35/49): loss=1.5651349360608518e+38\n",
      "Gradient Descent(36/49): loss=2.1445011398991863e+39\n",
      "Gradient Descent(37/49): loss=2.9383314071331355e+40\n",
      "Gradient Descent(38/49): loss=4.026013928139401e+41\n",
      "Gradient Descent(39/49): loss=5.5163240301021905e+42\n",
      "Gradient Descent(40/49): loss=7.558302417285981e+43\n",
      "Gradient Descent(41/49): loss=1.0356160210931687e+45\n",
      "Gradient Descent(42/49): loss=1.4189701389719827e+46\n",
      "Gradient Descent(43/49): loss=1.944230500768795e+47\n",
      "Gradient Descent(44/49): loss=2.663926559341296e+48\n",
      "Gradient Descent(45/49): loss=3.650032601976866e+49\n",
      "Gradient Descent(46/49): loss=5.001165647294844e+50\n",
      "Gradient Descent(47/49): loss=6.852447788585712e+51\n",
      "Gradient Descent(48/49): loss=9.389019282073229e+52\n",
      "Gradient Descent(49/49): loss=1.2864553776824459e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7743163979695312\n",
      "Gradient Descent(2/49): loss=5.218974746781104\n",
      "Gradient Descent(3/49): loss=65.88968730746124\n",
      "Gradient Descent(4/49): loss=897.4491806236693\n",
      "Gradient Descent(5/49): loss=12311.882515995181\n",
      "Gradient Descent(6/49): loss=169052.23069855777\n",
      "Gradient Descent(7/49): loss=2321578.543041582\n",
      "Gradient Descent(8/49): loss=31883070.642981566\n",
      "Gradient Descent(9/49): loss=437865058.07542264\n",
      "Gradient Descent(10/49): loss=6013416237.652023\n",
      "Gradient Descent(11/49): loss=82585242175.17706\n",
      "Gradient Descent(12/49): loss=1134184421878.2917\n",
      "Gradient Descent(13/49): loss=15576322185694.916\n",
      "Gradient Descent(14/49): loss=213917428139603.72\n",
      "Gradient Descent(15/49): loss=2937835106208670.5\n",
      "Gradient Descent(16/49): loss=4.034675991625183e+16\n",
      "Gradient Descent(17/49): loss=5.541022477660291e+17\n",
      "Gradient Descent(18/49): loss=7.609763500834183e+18\n",
      "Gradient Descent(19/49): loss=1.045086909004301e+20\n",
      "Gradient Descent(20/49): loss=1.435270159518397e+21\n",
      "Gradient Descent(21/49): loss=1.9711283464142305e+22\n",
      "Gradient Descent(22/49): loss=2.7070492145825885e+23\n",
      "Gradient Descent(23/49): loss=3.717726176229622e+24\n",
      "Gradient Descent(24/49): loss=5.105739432799426e+25\n",
      "Gradient Descent(25/49): loss=7.011967509151201e+26\n",
      "Gradient Descent(26/49): loss=9.629885934549849e+27\n",
      "Gradient Descent(27/49): loss=1.322520433693036e+29\n",
      "Gradient Descent(28/49): loss=1.8162835047301933e+30\n",
      "Gradient Descent(29/49): loss=2.494393043397532e+31\n",
      "Gradient Descent(30/49): loss=3.425674812740351e+32\n",
      "Gradient Descent(31/49): loss=4.7046506779298285e+33\n",
      "Gradient Descent(32/49): loss=6.461132247295826e+34\n",
      "Gradient Descent(33/49): loss=8.873396299725927e+35\n",
      "Gradient Descent(34/49): loss=1.2186279258553164e+37\n",
      "Gradient Descent(35/49): loss=1.6736027238187173e+38\n",
      "Gradient Descent(36/49): loss=2.298442385691708e+39\n",
      "Gradient Descent(37/49): loss=3.15656596703558e+40\n",
      "Gradient Descent(38/49): loss=4.335070031023916e+41\n",
      "Gradient Descent(39/49): loss=5.953568647111349e+42\n",
      "Gradient Descent(40/49): loss=8.176333803653734e+43\n",
      "Gradient Descent(41/49): loss=1.1228968444196494e+45\n",
      "Gradient Descent(42/49): loss=1.5421304382706969e+46\n",
      "Gradient Descent(43/49): loss=2.1178849156621415e+47\n",
      "Gradient Descent(44/49): loss=2.908597356407191e+48\n",
      "Gradient Descent(45/49): loss=3.994522327033045e+49\n",
      "Gradient Descent(46/49): loss=5.485877440552698e+50\n",
      "Gradient Descent(47/49): loss=7.534030061391134e+51\n",
      "Gradient Descent(48/49): loss=1.0346860567163235e+53\n",
      "Gradient Descent(49/49): loss=1.4209861485016436e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7736025055667342\n",
      "Gradient Descent(2/49): loss=5.185815479580299\n",
      "Gradient Descent(3/49): loss=64.77560286043237\n",
      "Gradient Descent(4/49): loss=872.4974150112228\n",
      "Gradient Descent(5/49): loss=11836.089412796708\n",
      "Gradient Descent(6/49): loss=160703.33920268732\n",
      "Gradient Descent(7/49): loss=2182253.3250379995\n",
      "Gradient Descent(8/49): loss=29634605.7723964\n",
      "Gradient Descent(9/49): loss=402435622.0203964\n",
      "Gradient Descent(10/49): loss=5465054321.275809\n",
      "Gradient Descent(11/49): loss=74215180979.38345\n",
      "Gradient Descent(12/49): loss=1007838787772.8035\n",
      "Gradient Descent(13/49): loss=13686405368280.951\n",
      "Gradient Descent(14/49): loss=185860770119169.34\n",
      "Gradient Descent(15/49): loss=2523980914038485.0\n",
      "Gradient Descent(16/49): loss=3.427554751443706e+16\n",
      "Gradient Descent(17/49): loss=4.654603967137085e+17\n",
      "Gradient Descent(18/49): loss=6.320931294349018e+18\n",
      "Gradient Descent(19/49): loss=8.58379632515442e+19\n",
      "Gradient Descent(20/49): loss=1.1656756879755257e+21\n",
      "Gradient Descent(21/49): loss=1.5829823519413036e+22\n",
      "Gradient Descent(22/49): loss=2.1496829284566204e+23\n",
      "Gradient Descent(23/49): loss=2.919259767634676e+24\n",
      "Gradient Descent(24/49): loss=3.9643416608648235e+25\n",
      "Gradient Descent(25/49): loss=5.383558180847474e+26\n",
      "Gradient Descent(26/49): loss=7.310847844594446e+27\n",
      "Gradient Descent(27/49): loss=9.9280985570026e+28\n",
      "Gradient Descent(28/49): loss=1.3482313276487733e+30\n",
      "Gradient Descent(29/49): loss=1.8308920911864487e+31\n",
      "Gradient Descent(30/49): loss=2.4863432415674996e+32\n",
      "Gradient Descent(31/49): loss=3.3764429616834606e+33\n",
      "Gradient Descent(32/49): loss=4.585194386240285e+34\n",
      "Gradient Descent(33/49): loss=6.2266733951066426e+35\n",
      "Gradient Descent(34/49): loss=8.455794521095559e+36\n",
      "Gradient Descent(35/49): loss=1.14829310044075e+38\n",
      "Gradient Descent(36/49): loss=1.559376876093957e+39\n",
      "Gradient Descent(37/49): loss=2.1176267982130923e+40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/49): loss=2.875727686653263e+41\n",
      "Gradient Descent(39/49): loss=3.9052252902930895e+42\n",
      "Gradient Descent(40/49): loss=5.303278414964692e+43\n",
      "Gradient Descent(41/49): loss=7.201828282874756e+44\n",
      "Gradient Descent(42/49): loss=9.78005048153969e+45\n",
      "Gradient Descent(43/49): loss=1.3281264654547484e+47\n",
      "Gradient Descent(44/49): loss=1.8035897785709922e+48\n",
      "Gradient Descent(45/49): loss=2.4492668235866845e+49\n",
      "Gradient Descent(46/49): loss=3.3260933524891835e+50\n",
      "Gradient Descent(47/49): loss=4.516819842957055e+51\n",
      "Gradient Descent(48/49): loss=6.133821072238518e+52\n",
      "Gradient Descent(49/49): loss=8.329701483423794e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7844285699000906\n",
      "Gradient Descent(2/49): loss=5.438741364237755\n",
      "Gradient Descent(3/49): loss=69.88891178730415\n",
      "Gradient Descent(4/49): loss=966.1475724655072\n",
      "Gradient Descent(5/49): loss=13448.947898821933\n",
      "Gradient Descent(6/49): loss=187376.603659367\n",
      "Gradient Descent(7/49): loss=2611035.8960072338\n",
      "Gradient Descent(8/49): loss=36385347.14030216\n",
      "Gradient Descent(9/49): loss=507042372.5145615\n",
      "Gradient Descent(10/49): loss=7065827396.02503\n",
      "Gradient Descent(11/49): loss=98465040318.8197\n",
      "Gradient Descent(12/49): loss=1372148677460.1482\n",
      "Gradient Descent(13/49): loss=19121426904534.65\n",
      "Gradient Descent(14/49): loss=266464543382567.1\n",
      "Gradient Descent(15/49): loss=3713287372963826.0\n",
      "Gradient Descent(16/49): loss=5.174610831564721e+16\n",
      "Gradient Descent(17/49): loss=7.211022087357636e+17\n",
      "Gradient Descent(18/49): loss=1.004884062578919e+19\n",
      "Gradient Descent(19/49): loss=1.4003451480313024e+20\n",
      "Gradient Descent(20/49): loss=1.9514356000319029e+21\n",
      "Gradient Descent(21/49): loss=2.719401646393202e+22\n",
      "Gradient Descent(22/49): loss=3.789592295173244e+23\n",
      "Gradient Descent(23/49): loss=5.2809447191019e+24\n",
      "Gradient Descent(24/49): loss=7.359202508864019e+25\n",
      "Gradient Descent(25/49): loss=1.0255335824776654e+27\n",
      "Gradient Descent(26/49): loss=1.429121059683703e+28\n",
      "Gradient Descent(27/49): loss=1.9915359556506315e+29\n",
      "Gradient Descent(28/49): loss=2.775283056515241e+30\n",
      "Gradient Descent(29/49): loss=3.867465220463086e+31\n",
      "Gradient Descent(30/49): loss=5.389463678804953e+32\n",
      "Gradient Descent(31/49): loss=7.510427913215905e+33\n",
      "Gradient Descent(32/49): loss=1.0466074325993147e+35\n",
      "Gradient Descent(33/49): loss=1.458488292051373e+36\n",
      "Gradient Descent(34/49): loss=2.032460339755021e+37\n",
      "Gradient Descent(35/49): loss=2.832312782481752e+38\n",
      "Gradient Descent(36/49): loss=3.946938368684966e+39\n",
      "Gradient Descent(37/49): loss=5.500212611598434e+40\n",
      "Gradient Descent(38/49): loss=7.66476087207449e+41\n",
      "Gradient Descent(39/49): loss=1.0681143325659765e+43\n",
      "Gradient Descent(40/49): loss=1.488458996274065e+44\n",
      "Gradient Descent(41/49): loss=2.0742256854346013e+45\n",
      "Gradient Descent(42/49): loss=2.890514421214497e+46\n",
      "Gradient Descent(43/49): loss=4.028044623070237e+47\n",
      "Gradient Descent(44/49): loss=5.6132373415482335e+48\n",
      "Gradient Descent(45/49): loss=7.822265243063634e+49\n",
      "Gradient Descent(46/49): loss=1.0900631811867166e+51\n",
      "Gradient Descent(47/49): loss=1.5190455731893825e+52\n",
      "Gradient Descent(48/49): loss=2.1168492737404287e+53\n",
      "Gradient Descent(49/49): loss=2.9499120545323283e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7889817544984367\n",
      "Gradient Descent(2/49): loss=5.645706474690493\n",
      "Gradient Descent(3/49): loss=74.4450964774687\n",
      "Gradient Descent(4/49): loss=1052.6114968139368\n",
      "Gradient Descent(5/49): loss=14978.016793837312\n",
      "Gradient Descent(6/49): loss=213288.38296875943\n",
      "Gradient Descent(7/49): loss=3037634.4071791056\n",
      "Gradient Descent(8/49): loss=43262899.02404371\n",
      "Gradient Descent(9/49): loss=616167082.3077362\n",
      "Gradient Descent(10/49): loss=8775705346.977808\n",
      "Gradient Descent(11/49): loss=124987256164.91417\n",
      "Gradient Descent(12/49): loss=1780120799348.6426\n",
      "Gradient Descent(13/49): loss=25353225819701.11\n",
      "Gradient Descent(14/49): loss=361091260297332.6\n",
      "Gradient Descent(15/49): loss=5142812964357866.0\n",
      "Gradient Descent(16/49): loss=7.324609621686726e+16\n",
      "Gradient Descent(17/49): loss=1.043201580265477e+18\n",
      "Gradient Descent(18/49): loss=1.485771383456213e+19\n",
      "Gradient Descent(19/49): loss=2.116097833503863e+20\n",
      "Gradient Descent(20/49): loss=3.013835163891994e+21\n",
      "Gradient Descent(21/49): loss=4.292430270142232e+22\n",
      "Gradient Descent(22/49): loss=6.113458972401482e+23\n",
      "Gradient Descent(23/49): loss=8.70704432107133e+24\n",
      "Gradient Descent(24/49): loss=1.2400937202874598e+26\n",
      "Gradient Descent(25/49): loss=1.766193415801031e+27\n",
      "Gradient Descent(26/49): loss=2.515486637006603e+28\n",
      "Gradient Descent(27/49): loss=3.582661425611179e+29\n",
      "Gradient Descent(28/49): loss=5.102576456473016e+30\n",
      "Gradient Descent(29/49): loss=7.2673031026678455e+31\n",
      "Gradient Descent(30/49): loss=1.0350397458336511e+33\n",
      "Gradient Descent(31/49): loss=1.4741469570219327e+34\n",
      "Gradient Descent(32/49): loss=2.0995418385085824e+35\n",
      "Gradient Descent(33/49): loss=2.9902554223991154e+36\n",
      "Gradient Descent(34/49): loss=4.258847014708206e+37\n",
      "Gradient Descent(35/49): loss=6.065628293430809e+38\n",
      "Gradient Descent(36/49): loss=8.638921864768893e+39\n",
      "Gradient Descent(37/49): loss=1.2303914347407165e+41\n",
      "Gradient Descent(38/49): loss=1.7523750143604506e+42\n",
      "Gradient Descent(39/49): loss=2.4958058909130063e+43\n",
      "Gradient Descent(40/49): loss=3.5546312827277094e+44\n",
      "Gradient Descent(41/49): loss=5.0626547529800606e+45\n",
      "Gradient Descent(42/49): loss=7.210444940495699e+46\n",
      "Gradient Descent(43/49): loss=1.0269417682356589e+48\n",
      "Gradient Descent(44/49): loss=1.4626134781558685e+49\n",
      "Gradient Descent(45/49): loss=2.083115374845968e+50\n",
      "Gradient Descent(46/49): loss=2.9668601648543085e+51\n",
      "Gradient Descent(47/49): loss=4.2255265090394723e+52\n",
      "Gradient Descent(48/49): loss=6.0181718336806566e+53\n",
      "Gradient Descent(49/49): loss=8.571332387153848e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7888693243119509\n",
      "Gradient Descent(2/49): loss=5.608722578982258\n",
      "Gradient Descent(3/49): loss=73.95930357682116\n",
      "Gradient Descent(4/49): loss=1047.6514007231808\n",
      "Gradient Descent(5/49): loss=14940.04433366345\n",
      "Gradient Descent(6/49): loss=213232.22292159827\n",
      "Gradient Descent(7/49): loss=3043825.850272599\n",
      "Gradient Descent(8/49): loss=43451170.8669635\n",
      "Gradient Descent(9/49): loss=620278485.3324121\n",
      "Gradient Descent(10/49): loss=8854679164.234278\n",
      "Gradient Descent(11/49): loss=126403517832.74413\n",
      "Gradient Descent(12/49): loss=1804452880976.7034\n",
      "Gradient Descent(13/49): loss=25759174725746.992\n",
      "Gradient Descent(14/49): loss=367720927902359.25\n",
      "Gradient Descent(15/49): loss=5249340575600575.0\n",
      "Gradient Descent(16/49): loss=7.49361116031503e+16\n",
      "Gradient Descent(17/49): loss=1.0697383303696653e+18\n",
      "Gradient Descent(18/49): loss=1.5270876364084376e+19\n",
      "Gradient Descent(19/49): loss=2.1799692346102766e+20\n",
      "Gradient Descent(20/49): loss=3.111979791175034e+21\n",
      "Gradient Descent(21/49): loss=4.442456373664284e+22\n",
      "Gradient Descent(22/49): loss=6.341756681029382e+23\n",
      "Gradient Descent(23/49): loss=9.053072088631204e+24\n",
      "Gradient Descent(24/49): loss=1.2923566507545476e+26\n",
      "Gradient Descent(25/49): loss=1.844882815908343e+27\n",
      "Gradient Descent(26/49): loss=2.6336325985916496e+28\n",
      "Gradient Descent(27/49): loss=3.759599582453479e+29\n",
      "Gradient Descent(28/49): loss=5.366955522931707e+30\n",
      "Gradient Descent(29/49): loss=7.661510475626138e+31\n",
      "Gradient Descent(30/49): loss=1.0937065253722339e+33\n",
      "Gradient Descent(31/49): loss=1.5613030451988593e+34\n",
      "Gradient Descent(32/49): loss=2.2288128875500315e+35\n",
      "Gradient Descent(33/49): loss=3.181705757242308e+36\n",
      "Gradient Descent(34/49): loss=4.541992547789244e+37\n",
      "Gradient Descent(35/49): loss=6.483847935094212e+38\n",
      "Gradient Descent(36/49): loss=9.255912158175562e+39\n",
      "Gradient Descent(37/49): loss=1.3213127565216094e+41\n",
      "Gradient Descent(38/49): loss=1.8862186359500472e+42\n",
      "Gradient Descent(39/49): loss=2.6926408793413126e+43\n",
      "Gradient Descent(40/49): loss=3.843835898402155e+44\n",
      "Gradient Descent(41/49): loss=5.48720571213324e+45\n",
      "Gradient Descent(42/49): loss=7.833171686591503e+46\n",
      "Gradient Descent(43/49): loss=1.1182117436556725e+48\n",
      "Gradient Descent(44/49): loss=1.5962850728649713e+49\n",
      "Gradient Descent(45/49): loss=2.2787509148501482e+50\n",
      "Gradient Descent(46/49): loss=3.252993979709808e+51\n",
      "Gradient Descent(47/49): loss=4.6437588957475647e+52\n",
      "Gradient Descent(48/49): loss=6.6291228377121865e+53\n",
      "Gradient Descent(49/49): loss=9.463296993674547e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7882044502941014\n",
      "Gradient Descent(2/49): loss=5.573448414127645\n",
      "Gradient Descent(3/49): loss=72.71778656005311\n",
      "Gradient Descent(4/49): loss=1018.7073987867428\n",
      "Gradient Descent(5/49): loss=14365.973748751854\n",
      "Gradient Descent(6/49): loss=202757.25351038633\n",
      "Gradient Descent(7/49): loss=2862075.8515388104\n",
      "Gradient Descent(8/49): loss=40401733.59347291\n",
      "Gradient Descent(9/49): loss=570324841.4477428\n",
      "Gradient Descent(10/49): loss=8050918498.421962\n",
      "Gradient Descent(11/49): loss=113649828952.11816\n",
      "Gradient Descent(12/49): loss=1604324429427.8167\n",
      "Gradient Descent(13/49): loss=22647257626060.55\n",
      "Gradient Descent(14/49): loss=319697358350908.8\n",
      "Gradient Descent(15/49): loss=4512970304271294.0\n",
      "Gradient Descent(16/49): loss=6.370681660457137e+16\n",
      "Gradient Descent(17/49): loss=8.993098134412618e+17\n",
      "Gradient Descent(18/49): loss=1.269500162906303e+19\n",
      "Gradient Descent(19/49): loss=1.7920750330356285e+20\n",
      "Gradient Descent(20/49): loss=2.529761726602609e+21\n",
      "Gradient Descent(21/49): loss=3.571108505733751e+22\n",
      "Gradient Descent(22/49): loss=5.041113487337118e+23\n",
      "Gradient Descent(23/49): loss=7.116228798819804e+24\n",
      "Gradient Descent(24/49): loss=1.0045541018736919e+26\n",
      "Gradient Descent(25/49): loss=1.4180670297709084e+27\n",
      "Gradient Descent(26/49): loss=2.0017977102204236e+28\n",
      "Gradient Descent(27/49): loss=2.8258142869953706e+29\n",
      "Gradient Descent(28/49): loss=3.989027634419597e+30\n",
      "Gradient Descent(29/49): loss=5.631064129512388e+31\n",
      "Gradient Descent(30/49): loss=7.949025711699454e+32\n",
      "Gradient Descent(31/49): loss=1.1221149024763557e+34\n",
      "Gradient Descent(32/49): loss=1.5840203567417217e+35\n",
      "Gradient Descent(33/49): loss=2.2360637801305965e+36\n",
      "Gradient Descent(34/49): loss=3.156513240206532e+37\n",
      "Gradient Descent(35/49): loss=4.455854937651745e+38\n",
      "Gradient Descent(36/49): loss=6.290055423336783e+39\n",
      "Gradient Descent(37/49): loss=8.879283051682888e+40\n",
      "Gradient Descent(38/49): loss=1.2534335900982925e+42\n",
      "Gradient Descent(39/49): loss=1.7693948437525374e+43\n",
      "Gradient Descent(40/49): loss=2.4977455030964723e+44\n",
      "Gradient Descent(41/49): loss=3.5259131788852124e+45\n",
      "Gradient Descent(42/49): loss=4.977314033645257e+46\n",
      "Gradient Descent(43/49): loss=7.026167047412869e+47\n",
      "Gradient Descent(44/49): loss=9.91840640241768e+48\n",
      "Gradient Descent(45/49): loss=1.4001202205937217e+50\n",
      "Gradient Descent(46/49): loss=1.9764633072885227e+51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=2.79005127388372e+52\n",
      "Gradient Descent(48/49): loss=3.938543195916681e+53\n",
      "Gradient Descent(49/49): loss=5.559798363314274e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7991421147481426\n",
      "Gradient Descent(2/49): loss=5.839201430534703\n",
      "Gradient Descent(3/49): loss=78.30153174985833\n",
      "Gradient Descent(4/49): loss=1124.9745452314412\n",
      "Gradient Descent(5/49): loss=16267.787190836454\n",
      "Gradient Descent(6/49): loss=235441.0447246752\n",
      "Gradient Descent(7/49): loss=3408054.336090112\n",
      "Gradient Descent(8/49): loss=49334136.78253001\n",
      "Gradient Descent(9/49): loss=714155512.9904234\n",
      "Gradient Descent(10/49): loss=10338062425.716686\n",
      "Gradient Descent(11/49): loss=149653126184.27368\n",
      "Gradient Descent(12/49): loss=2166369393329.2449\n",
      "Gradient Descent(13/49): loss=31360230678451.832\n",
      "Gradient Descent(14/49): loss=453968784249058.06\n",
      "Gradient Descent(15/49): loss=6571624418705290.0\n",
      "Gradient Descent(16/49): loss=9.513043414744726e+16\n",
      "Gradient Descent(17/49): loss=1.377102361055356e+18\n",
      "Gradient Descent(18/49): loss=1.9934849766249447e+19\n",
      "Gradient Descent(19/49): loss=2.8857566906212568e+20\n",
      "Gradient Descent(20/49): loss=4.177403780386236e+21\n",
      "Gradient Descent(21/49): loss=6.04718422766452e+22\n",
      "Gradient Descent(22/49): loss=8.753866996295639e+23\n",
      "Gradient Descent(23/49): loss=1.2672044459679833e+25\n",
      "Gradient Descent(24/49): loss=1.8343974252299752e+26\n",
      "Gradient Descent(25/49): loss=2.6554625217716327e+27\n",
      "Gradient Descent(26/49): loss=3.844031346505943e+28\n",
      "Gradient Descent(27/49): loss=5.564596326165383e+29\n",
      "Gradient Descent(28/49): loss=8.055275694179445e+30\n",
      "Gradient Descent(29/49): loss=1.1660767952588055e+32\n",
      "Gradient Descent(30/49): loss=1.6880056550063952e+33\n",
      "Gradient Descent(31/49): loss=2.4435466882789285e+34\n",
      "Gradient Descent(32/49): loss=3.537263278763286e+35\n",
      "Gradient Descent(33/49): loss=5.120520742781464e+36\n",
      "Gradient Descent(34/49): loss=7.412434588816434e+37\n",
      "Gradient Descent(35/49): loss=1.0730195090204225e+39\n",
      "Gradient Descent(36/49): loss=1.5532964951563425e+40\n",
      "Gradient Descent(37/49): loss=2.2485425302915582e+41\n",
      "Gradient Descent(38/49): loss=3.2549764493102065e+42\n",
      "Gradient Descent(39/49): loss=4.711884050594423e+43\n",
      "Gradient Descent(40/49): loss=6.820894606150279e+44\n",
      "Gradient Descent(41/49): loss=9.873885420066862e+45\n",
      "Gradient Descent(42/49): loss=1.4293376297106293e+47\n",
      "Gradient Descent(43/49): loss=2.0691004329002806e+48\n",
      "Gradient Descent(44/49): loss=2.9952171638375246e+49\n",
      "Gradient Descent(45/49): loss=4.3358580936410984e+50\n",
      "Gradient Descent(46/49): loss=6.276561724862192e+51\n",
      "Gradient Descent(47/49): loss=9.08591246189108e+52\n",
      "Gradient Descent(48/49): loss=1.3152711449987355e+54\n",
      "Gradient Descent(49/49): loss=1.9039784855094664e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.803907628094312\n",
      "Gradient Descent(2/49): loss=6.06167069812106\n",
      "Gradient Descent(3/49): loss=83.3869984921019\n",
      "Gradient Descent(4/49): loss=1225.2288417684845\n",
      "Gradient Descent(5/49): loss=18109.49429371496\n",
      "Gradient Descent(6/49): loss=267860.55067614815\n",
      "Gradient Descent(7/49): loss=3962475.939127088\n",
      "Gradient Descent(8/49): loss=58618768.50756324\n",
      "Gradient Descent(9/49): loss=867180762.9754673\n",
      "Gradient Descent(10/49): loss=12828718754.08583\n",
      "Gradient Descent(11/49): loss=189782911808.53748\n",
      "Gradient Descent(12/49): loss=2807572433272.476\n",
      "Gradient Descent(13/49): loss=41534103791818.234\n",
      "Gradient Descent(14/49): loss=614438925066513.5\n",
      "Gradient Descent(15/49): loss=9089763801702696.0\n",
      "Gradient Descent(16/49): loss=1.3447033159750848e+17\n",
      "Gradient Descent(17/49): loss=1.9893003246284582e+18\n",
      "Gradient Descent(18/49): loss=2.942891368407325e+19\n",
      "Gradient Descent(19/49): loss=4.353595834208462e+20\n",
      "Gradient Descent(20/49): loss=6.440535621238225e+21\n",
      "Gradient Descent(21/49): loss=9.527870906738247e+22\n",
      "Gradient Descent(22/49): loss=1.409515129706285e+24\n",
      "Gradient Descent(23/49): loss=2.0851803307556605e+25\n",
      "Gradient Descent(24/49): loss=3.0847324162290746e+26\n",
      "Gradient Descent(25/49): loss=4.5634298095868194e+27\n",
      "Gradient Descent(26/49): loss=6.7509556152954595e+28\n",
      "Gradient Descent(27/49): loss=9.987093835418476e+29\n",
      "Gradient Descent(28/49): loss=1.4774507338112437e+31\n",
      "Gradient Descent(29/49): loss=2.18568154741675e+32\n",
      "Gradient Descent(30/49): loss=3.233409898139056e+33\n",
      "Gradient Descent(31/49): loss=4.783377332228597e+34\n",
      "Gradient Descent(32/49): loss=7.076337186833948e+35\n",
      "Gradient Descent(33/49): loss=1.046845032366264e+37\n",
      "Gradient Descent(34/49): loss=1.5486606882285129e+38\n",
      "Gradient Descent(35/49): loss=2.2910267070219636e+39\n",
      "Gradient Descent(36/49): loss=3.38925331558068e+40\n",
      "Gradient Descent(37/49): loss=5.013925853403185e+41\n",
      "Gradient Descent(38/49): loss=7.417401451776032e+42\n",
      "Gradient Descent(39/49): loss=1.097300716153716e+44\n",
      "Gradient Descent(40/49): loss=1.6233028096155585e+45\n",
      "Gradient Descent(41/49): loss=2.4014492772249594e+46\n",
      "Gradient Descent(42/49): loss=3.5526080512667957e+47\n",
      "Gradient Descent(43/49): loss=5.25558631848768e+48\n",
      "Gradient Descent(44/49): loss=7.774904282285228e+49\n",
      "Gradient Descent(45/49): loss=1.1501882556101074e+51\n",
      "Gradient Descent(46/49): loss=1.7015425210541333e+52\n",
      "Gradient Descent(47/49): loss=2.5171939783192244e+53\n",
      "Gradient Descent(48/49): loss=3.723836134615727e+54\n",
      "Gradient Descent(49/49): loss=5.508894299329742e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8037353619897031\n",
      "Gradient Descent(2/49): loss=6.02166646684787\n",
      "Gradient Descent(3/49): loss=82.84110987689024\n",
      "Gradient Descent(4/49): loss=1219.416185301494\n",
      "Gradient Descent(5/49): loss=18062.79130714427\n",
      "Gradient Descent(6/49): loss=267775.56475162576\n",
      "Gradient Descent(7/49): loss=3970300.515646212\n",
      "Gradient Descent(8/49): loss=58869588.520801894\n",
      "Gradient Descent(9/49): loss=872895671.1139911\n",
      "Gradient Descent(10/49): loss=12942989786.159868\n",
      "Gradient Descent(11/49): loss=191914200266.8884\n",
      "Gradient Descent(12/49): loss=2845638129364.9\n",
      "Gradient Descent(13/49): loss=42194150446045.234\n",
      "Gradient Descent(14/49): loss=625640459598811.2\n",
      "Gradient Descent(15/49): loss=9276783188965466.0\n",
      "Gradient Descent(16/49): loss=1.375529748138963e+17\n",
      "Gradient Descent(17/49): loss=2.039588561824007e+18\n",
      "Gradient Descent(18/49): loss=3.0242323056222257e+19\n",
      "Gradient Descent(19/49): loss=4.4842284417740394e+20\n",
      "Gradient Descent(20/49): loss=6.64906088089748e+21\n",
      "Gradient Descent(21/49): loss=9.859000533080498e+22\n",
      "Gradient Descent(22/49): loss=1.4618589489915343e+24\n",
      "Gradient Descent(23/49): loss=2.1675945544137004e+25\n",
      "Gradient Descent(24/49): loss=3.2140352224578246e+26\n",
      "Gradient Descent(25/49): loss=4.765661728650016e+27\n",
      "Gradient Descent(26/49): loss=7.066360552997187e+28\n",
      "Gradient Descent(27/49): loss=1.0477758243890155e+30\n",
      "Gradient Descent(28/49): loss=1.553606230449754e+31\n",
      "Gradient Descent(29/49): loss=2.3036342919057207e+32\n",
      "Gradient Descent(30/49): loss=3.415750301997475e+33\n",
      "Gradient Descent(31/49): loss=5.064757963792911e+34\n",
      "Gradient Descent(32/49): loss=7.509850241922828e+35\n",
      "Gradient Descent(33/49): loss=1.1135349617748143e+37\n",
      "Gradient Descent(34/49): loss=1.6511116349203577e+38\n",
      "Gradient Descent(35/49): loss=2.448211977668197e+39\n",
      "Gradient Descent(36/49): loss=3.6301251598212545e+40\n",
      "Gradient Descent(37/49): loss=5.382625686080745e+41\n",
      "Gradient Descent(38/49): loss=7.981173651291665e+42\n",
      "Gradient Descent(39/49): loss=1.1834211882278031e+44\n",
      "Gradient Descent(40/49): loss=1.7547365461968806e+45\n",
      "Gradient Descent(41/49): loss=2.6018634592557554e+46\n",
      "Gradient Descent(42/49): loss=3.857954332394016e+47\n",
      "Gradient Descent(43/49): loss=5.720443007065098e+48\n",
      "Gradient Descent(44/49): loss=8.482077644701794e+49\n",
      "Gradient Descent(45/49): loss=1.2576935227200579e+51\n",
      "Gradient Descent(46/49): loss=1.8648650287704324e+52\n",
      "Gradient Descent(47/49): loss=2.7651582143871908e+53\n",
      "Gradient Descent(48/49): loss=4.100082221840142e+54\n",
      "Gradient Descent(49/49): loss=6.079461977395406e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8031213245793766\n",
      "Gradient Descent(2/49): loss=5.984175912626091\n",
      "Gradient Descent(3/49): loss=81.46027711696651\n",
      "Gradient Descent(4/49): loss=1185.9328932817582\n",
      "Gradient Descent(5/49): loss=17372.55601365983\n",
      "Gradient Descent(6/49): loss=254688.64347336796\n",
      "Gradient Descent(7/49): loss=3734384.4240142996\n",
      "Gradient Descent(8/49): loss=54757425.34661968\n",
      "Gradient Descent(9/49): loss=802916907.8510069\n",
      "Gradient Descent(10/49): loss=11773323849.942444\n",
      "Gradient Descent(11/49): loss=172634585420.61505\n",
      "Gradient Descent(12/49): loss=2531375544244.455\n",
      "Gradient Descent(13/49): loss=37118068479091.914\n",
      "Gradient Descent(14/49): loss=544269704518688.7\n",
      "Gradient Descent(15/49): loss=7980736176288042.0\n",
      "Gradient Descent(16/49): loss=1.1702314022277082e+17\n",
      "Gradient Descent(17/49): loss=1.715933849588723e+18\n",
      "Gradient Descent(18/49): loss=2.516108327519378e+19\n",
      "Gradient Descent(19/49): loss=3.6894202637166386e+20\n",
      "Gradient Descent(20/49): loss=5.409871162333531e+21\n",
      "Gradient Descent(21/49): loss=7.932602929757459e+22\n",
      "Gradient Descent(22/49): loss=1.1631735276679496e+24\n",
      "Gradient Descent(23/49): loss=1.7055847462024957e+25\n",
      "Gradient Descent(24/49): loss=2.5009332290350065e+26\n",
      "Gradient Descent(25/49): loss=3.6671687115035283e+27\n",
      "Gradient Descent(26/49): loss=5.377243263635423e+28\n",
      "Gradient Descent(27/49): loss=7.884759985437869e+29\n",
      "Gradient Descent(28/49): loss=1.1561582204843476e+31\n",
      "Gradient Descent(29/49): loss=1.6952980601340386e+32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=2.485849654288862e+33\n",
      "Gradient Descent(31/49): loss=3.645051362377806e+34\n",
      "Gradient Descent(32/49): loss=5.344812149620229e+35\n",
      "Gradient Descent(33/49): loss=7.837205590456385e+36\n",
      "Gradient Descent(34/49): loss=1.1491852238706847e+38\n",
      "Gradient Descent(35/49): loss=1.6850734149055458e+39\n",
      "Gradient Descent(36/49): loss=2.4708570512745795e+40\n",
      "Gradient Descent(37/49): loss=3.623067406932834e+41\n",
      "Gradient Descent(38/49): loss=5.312576633442966e+42\n",
      "Gradient Descent(39/49): loss=7.789938004520059e+43\n",
      "Gradient Descent(40/49): loss=1.1422542826443639e+45\n",
      "Gradient Descent(41/49): loss=1.6749104363376408e+46\n",
      "Gradient Descent(42/49): loss=2.4559548713254136e+47\n",
      "Gradient Descent(43/49): loss=3.601216040647519e+48\n",
      "Gradient Descent(44/49): loss=5.280535535418066e+49\n",
      "Gradient Descent(45/49): loss=7.742955497832161e+50\n",
      "Gradient Descent(46/49): loss=1.1353651431618066e+52\n",
      "Gradient Descent(47/49): loss=1.664808752507652e+53\n",
      "Gradient Descent(48/49): loss=2.441142569083712e+54\n",
      "Gradient Descent(49/49): loss=3.5794964638529536e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8141662273890808\n",
      "Gradient Descent(2/49): loss=6.26308759010338\n",
      "Gradient Descent(3/49): loss=87.54633825650882\n",
      "Gradient Descent(4/49): loss=1306.204339042518\n",
      "Gradient Descent(5/49): loss=19607.666584836625\n",
      "Gradient Descent(6/49): loss=294575.9444647495\n",
      "Gradient Descent(7/49): loss=4426289.395407626\n",
      "Gradient Descent(8/49): loss=66511928.45254212\n",
      "Gradient Descent(9/49): loss=999456047.8359016\n",
      "Gradient Descent(10/49): loss=15018585034.94805\n",
      "Gradient Descent(11/49): loss=225680813865.16452\n",
      "Gradient Descent(12/49): loss=3391254169364.879\n",
      "Gradient Descent(13/49): loss=50959606183386.94\n",
      "Gradient Descent(14/49): loss=765758437947054.4\n",
      "Gradient Descent(15/49): loss=1.1506878313376708e+16\n",
      "Gradient Descent(16/49): loss=1.7291124991113805e+17\n",
      "Gradient Descent(17/49): loss=2.5982981255652925e+18\n",
      "Gradient Descent(18/49): loss=3.904403648228093e+19\n",
      "Gradient Descent(19/49): loss=5.8670587868835925e+20\n",
      "Gradient Descent(20/49): loss=8.816296138980049e+21\n",
      "Gradient Descent(21/49): loss=1.3248048201604468e+23\n",
      "Gradient Descent(22/49): loss=1.9907541487415391e+24\n",
      "Gradient Descent(23/49): loss=2.9914610970784316e+25\n",
      "Gradient Descent(24/49): loss=4.495200726313142e+26\n",
      "Gradient Descent(25/49): loss=6.754836153336949e+27\n",
      "Gradient Descent(26/49): loss=1.015033904745133e+29\n",
      "Gradient Descent(27/49): loss=1.5252684216080993e+30\n",
      "Gradient Descent(28/49): loss=2.2919862549212092e+31\n",
      "Gradient Descent(29/49): loss=3.444115749285146e+32\n",
      "Gradient Descent(30/49): loss=5.1753946032637725e+33\n",
      "Gradient Descent(31/49): loss=7.776948061356842e+34\n",
      "Gradient Descent(32/49): loss=1.1686243424008896e+36\n",
      "Gradient Descent(33/49): loss=1.7560652879217528e+37\n",
      "Gradient Descent(34/49): loss=2.638799470074564e+38\n",
      "Gradient Descent(35/49): loss=3.965264099893808e+39\n",
      "Gradient Descent(36/49): loss=5.958512406955401e+40\n",
      "Gradient Descent(37/49): loss=8.953721419158995e+41\n",
      "Gradient Descent(38/49): loss=1.3454554052505594e+43\n",
      "Gradient Descent(39/49): loss=2.02178531447763e+44\n",
      "Gradient Descent(40/49): loss=3.0380909258573335e+45\n",
      "Gradient Descent(41/49): loss=4.5652703121752464e+46\n",
      "Gradient Descent(42/49): loss=6.860128130413763e+47\n",
      "Gradient Descent(43/49): loss=1.0308558912751538e+49\n",
      "Gradient Descent(44/49): loss=1.5490437618292738e+50\n",
      "Gradient Descent(45/49): loss=2.32771291930436e+51\n",
      "Gradient Descent(46/49): loss=3.497801397358864e+52\n",
      "Gradient Descent(47/49): loss=5.256066808711888e+53\n",
      "Gradient Descent(48/49): loss=7.898172354354646e+54\n",
      "Gradient Descent(49/49): loss=1.1868404419003173e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8191483483219509\n",
      "Gradient Descent(2/49): loss=6.5019048658502845\n",
      "Gradient Descent(3/49): loss=93.21125494323529\n",
      "Gradient Descent(4/49): loss=1422.1347392569407\n",
      "Gradient Descent(5/49): loss=21818.379135885094\n",
      "Gradient Descent(6/49): loss=334970.7692527602\n",
      "Gradient Descent(7/49): loss=5143362.058256628\n",
      "Gradient Descent(8/49): loss=78976856.07963839\n",
      "Gradient Descent(9/49): loss=1212706245.0234091\n",
      "Gradient Descent(10/49): loss=18621391518.18786\n",
      "Gradient Descent(11/49): loss=285935996795.71375\n",
      "Gradient Descent(12/49): loss=4390617250960.173\n",
      "Gradient Descent(13/49): loss=67419004815435.914\n",
      "Gradient Descent(14/49): loss=1035235362072577.9\n",
      "Gradient Descent(15/49): loss=1.5896293033768412e+16\n",
      "Gradient Descent(16/49): loss=2.440914806166555e+17\n",
      "Gradient Descent(17/49): loss=3.748084587323463e+18\n",
      "Gradient Descent(18/49): loss=5.75527586566352e+19\n",
      "Gradient Descent(19/49): loss=8.837367332133959e+20\n",
      "Gradient Descent(20/49): loss=1.3569994416623935e+22\n",
      "Gradient Descent(21/49): loss=2.0837059448427962e+23\n",
      "Gradient Descent(22/49): loss=3.1995816146060444e+24\n",
      "Gradient Descent(23/49): loss=4.913036090271147e+25\n",
      "Gradient Descent(24/49): loss=7.544087487600738e+26\n",
      "Gradient Descent(25/49): loss=1.1584131476924086e+28\n",
      "Gradient Descent(26/49): loss=1.778771816939e+29\n",
      "Gradient Descent(27/49): loss=2.7313477778108027e+30\n",
      "Gradient Descent(28/49): loss=4.1940515429292225e+31\n",
      "Gradient Descent(29/49): loss=6.440069070532497e+32\n",
      "Gradient Descent(30/49): loss=9.888884103761522e+33\n",
      "Gradient Descent(31/49): loss=1.51846242247743e+35\n",
      "Gradient Descent(32/49): loss=2.3316363143531751e+36\n",
      "Gradient Descent(33/49): loss=3.580284781457123e+37\n",
      "Gradient Descent(34/49): loss=5.497615145820654e+38\n",
      "Gradient Descent(35/49): loss=8.441722973571959e+39\n",
      "Gradient Descent(36/49): loss=1.2962472794536576e+41\n",
      "Gradient Descent(37/49): loss=1.9904195088506307e+42\n",
      "Gradient Descent(38/49): loss=3.056338002794475e+43\n",
      "Gradient Descent(39/49): loss=4.6930820089880504e+44\n",
      "Gradient Descent(40/49): loss=7.2063425978898e+45\n",
      "Gradient Descent(41/49): loss=1.106551591016388e+47\n",
      "Gradient Descent(42/49): loss=1.6991371239267092e+48\n",
      "Gradient Descent(43/49): loss=2.609066752372667e+49\n",
      "Gradient Descent(44/49): loss=4.006286027466059e+50\n",
      "Gradient Descent(45/49): loss=6.15175051357877e+51\n",
      "Gradient Descent(46/49): loss=9.44616388392335e+52\n",
      "Gradient Descent(47/49): loss=1.4504816462400387e+54\n",
      "Gradient Descent(48/49): loss=2.227250164122072e+55\n",
      "Gradient Descent(49/49): loss=3.4199972860331217e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8189145110027877\n",
      "Gradient Descent(2/49): loss=6.458708700329401\n",
      "Gradient Descent(3/49): loss=92.59924122736875\n",
      "Gradient Descent(4/49): loss=1415.3443964202775\n",
      "Gradient Descent(5/49): loss=21761.193970174718\n",
      "Gradient Descent(6/49): loss=334847.08736734523\n",
      "Gradient Descent(7/49): loss=5153203.627704534\n",
      "Gradient Descent(8/49): loss=79309235.1102563\n",
      "Gradient Descent(9/49): loss=1220602092.6865473\n",
      "Gradient Descent(10/49): loss=18785615986.480724\n",
      "Gradient Descent(11/49): loss=289119256813.23114\n",
      "Gradient Descent(12/49): loss=4449678777852.698\n",
      "Gradient Descent(13/49): loss=68482612235792.055\n",
      "Gradient Descent(14/49): loss=1053979052537158.4\n",
      "Gradient Descent(15/49): loss=1.6221224768551506e+16\n",
      "Gradient Descent(16/49): loss=2.4965214680146666e+17\n",
      "Gradient Descent(17/49): loss=3.842261931595536e+18\n",
      "Gradient Descent(18/49): loss=5.913418707069122e+19\n",
      "Gradient Descent(19/49): loss=9.101024716171865e+20\n",
      "Gradient Descent(20/49): loss=1.4006897699559324e+22\n",
      "Gradient Descent(21/49): loss=2.155726297692844e+23\n",
      "Gradient Descent(22/49): loss=3.317762412665757e+24\n",
      "Gradient Descent(23/49): loss=5.10618970445315e+25\n",
      "Gradient Descent(24/49): loss=7.85866196998545e+26\n",
      "Gradient Descent(25/49): loss=1.2094844009543195e+28\n",
      "Gradient Descent(26/49): loss=1.8614523970351332e+29\n",
      "Gradient Descent(27/49): loss=2.8648612778253716e+30\n",
      "Gradient Descent(28/49): loss=4.409153924245265e+31\n",
      "Gradient Descent(29/49): loss=6.785891686331198e+32\n",
      "Gradient Descent(30/49): loss=1.0443800958139889e+34\n",
      "Gradient Descent(31/49): loss=1.6073492400851165e+35\n",
      "Gradient Descent(32/49): loss=2.473784774295774e+36\n",
      "Gradient Descent(33/49): loss=3.807269109240699e+37\n",
      "Gradient Descent(34/49): loss=5.85956313612811e+38\n",
      "Gradient Descent(35/49): loss=9.01813850324832e+39\n",
      "Gradient Descent(36/49): loss=1.3879331986771423e+41\n",
      "Gradient Descent(37/49): loss=2.1360933448696814e+42\n",
      "Gradient Descent(38/49): loss=3.287546390810079e+43\n",
      "Gradient Descent(39/49): loss=5.0596858501928736e+44\n",
      "Gradient Descent(40/49): loss=7.787090388809396e+45\n",
      "Gradient Descent(41/49): loss=1.1984692037980214e+47\n",
      "Gradient Descent(42/49): loss=1.8444994994746197e+48\n",
      "Gradient Descent(43/49): loss=2.838769984894417e+49\n",
      "Gradient Descent(44/49): loss=4.36899821845051e+50\n",
      "Gradient Descent(45/49): loss=6.724090199063315e+51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=1.0348685612688251e+53\n",
      "Gradient Descent(47/49): loss=1.592710548784418e+54\n",
      "Gradient Descent(48/49): loss=2.451255151764354e+55\n",
      "Gradient Descent(49/49): loss=3.7725949788159366e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8183531284225597\n",
      "Gradient Descent(2/49): loss=6.418897213922048\n",
      "Gradient Descent(3/49): loss=91.06634372666163\n",
      "Gradient Descent(4/49): loss=1376.711853867423\n",
      "Gradient Descent(5/49): loss=20934.07238013592\n",
      "Gradient Descent(6/49): loss=318563.47676369146\n",
      "Gradient Descent(7/49): loss=4848442.961216293\n",
      "Gradient Descent(8/49): loss=73794435.44152401\n",
      "Gradient Descent(9/49): loss=1123178261.350474\n",
      "Gradient Descent(10/49): loss=17095220922.68233\n",
      "Gradient Descent(11/49): loss=260196223824.37277\n",
      "Gradient Descent(12/49): loss=3960293049568.2417\n",
      "Gradient Descent(13/49): loss=60277283748487.34\n",
      "Gradient Descent(14/49): loss=917444978114438.2\n",
      "Gradient Descent(15/49): loss=1.3963888840775848e+16\n",
      "Gradient Descent(16/49): loss=2.1253611532980304e+17\n",
      "Gradient Descent(17/49): loss=3.2348868457527076e+18\n",
      "Gradient Descent(18/49): loss=4.923630456407906e+19\n",
      "Gradient Descent(19/49): loss=7.493967494827822e+20\n",
      "Gradient Descent(20/49): loss=1.1406125888386688e+22\n",
      "Gradient Descent(21/49): loss=1.736059141857831e+23\n",
      "Gradient Descent(22/49): loss=2.64235321749107e+24\n",
      "Gradient Descent(23/49): loss=4.021769971795966e+25\n",
      "Gradient Descent(24/49): loss=6.121298847925322e+26\n",
      "Gradient Descent(25/49): loss=9.316867908504304e+27\n",
      "Gradient Descent(26/49): loss=1.4180655083346908e+29\n",
      "Gradient Descent(27/49): loss=2.158353864921696e+30\n",
      "Gradient Descent(28/49): loss=3.285103106197929e+31\n",
      "Gradient Descent(29/49): loss=5.00006166446795e+32\n",
      "Gradient Descent(30/49): loss=7.610298928308845e+33\n",
      "Gradient Descent(31/49): loss=1.1583187101429772e+35\n",
      "Gradient Descent(32/49): loss=1.763008584690962e+36\n",
      "Gradient Descent(33/49): loss=2.6833713748009485e+37\n",
      "Gradient Descent(34/49): loss=4.084201289560546e+38\n",
      "Gradient Descent(35/49): loss=6.216321874151823e+39\n",
      "Gradient Descent(36/49): loss=9.461496851742216e+40\n",
      "Gradient Descent(37/49): loss=1.4400786266837602e+42\n",
      "Gradient Descent(38/49): loss=2.191858733905841e+43\n",
      "Gradient Descent(39/49): loss=3.3360988909769694e+44\n",
      "Gradient Descent(40/49): loss=5.077679340467887e+45\n",
      "Gradient Descent(41/49): loss=7.72843621463031e+46\n",
      "Gradient Descent(42/49): loss=1.176299689655184e+48\n",
      "Gradient Descent(43/49): loss=1.7903763729892836e+49\n",
      "Gradient Descent(44/49): loss=2.7250262710669186e+50\n",
      "Gradient Descent(45/49): loss=4.14760175013176e+51\n",
      "Gradient Descent(46/49): loss=6.312819975478899e+52\n",
      "Gradient Descent(47/49): loss=9.608370919782563e+53\n",
      "Gradient Descent(48/49): loss=1.4624334622360184e+55\n",
      "Gradient Descent(49/49): loss=2.2258837104886047e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8295009078229053\n",
      "Gradient Descent(2/49): loss=6.711295809396332\n",
      "Gradient Descent(3/49): loss=97.68793869724799\n",
      "Gradient Descent(4/49): loss=1512.4929126944396\n",
      "Gradient Descent(5/49): loss=23552.504525052085\n",
      "Gradient Descent(6/49): loss=367052.8084129444\n",
      "Gradient Descent(7/49): loss=5721263.258999259\n",
      "Gradient Descent(8/49): loss=89181148.94352785\n",
      "Gradient Descent(9/49): loss=1390140871.3545418\n",
      "Gradient Descent(10/49): loss=21669344738.088028\n",
      "Gradient Descent(11/49): loss=337779328269.4186\n",
      "Gradient Descent(12/49): loss=5265267519579.689\n",
      "Gradient Descent(13/49): loss=82074423077512.6\n",
      "Gradient Descent(14/49): loss=1279367285953622.5\n",
      "Gradient Descent(15/49): loss=1.994263982577913e+16\n",
      "Gradient Descent(16/49): loss=3.108637276971402e+17\n",
      "Gradient Descent(17/49): loss=4.84571040076621e+18\n",
      "Gradient Descent(18/49): loss=7.553441330589463e+19\n",
      "Gradient Descent(19/49): loss=1.1774223223629037e+21\n",
      "Gradient Descent(20/49): loss=1.8353532708208087e+22\n",
      "Gradient Descent(21/49): loss=2.8609289672348124e+23\n",
      "Gradient Descent(22/49): loss=4.4595853483321194e+24\n",
      "Gradient Descent(23/49): loss=6.951553745943859e+25\n",
      "Gradient Descent(24/49): loss=1.083600732090935e+27\n",
      "Gradient Descent(25/49): loss=1.68910518353274e+28\n",
      "Gradient Descent(26/49): loss=2.6329590194460487e+29\n",
      "Gradient Descent(27/49): loss=4.1042282420702475e+30\n",
      "Gradient Descent(28/49): loss=6.3976269051658045e+31\n",
      "Gradient Descent(29/49): loss=9.972552110565791e+32\n",
      "Gradient Descent(30/49): loss=1.5545107126776542e+34\n",
      "Gradient Descent(31/49): loss=2.4231546037942766e+35\n",
      "Gradient Descent(32/49): loss=3.7771873722088445e+36\n",
      "Gradient Descent(33/49): loss=5.887839109578062e+37\n",
      "Gradient Descent(34/49): loss=9.177900369820416e+38\n",
      "Gradient Descent(35/49): loss=1.4306412527700134e+40\n",
      "Gradient Descent(36/49): loss=2.230068220022966e+41\n",
      "Gradient Descent(37/49): loss=3.47620639089445e+42\n",
      "Gradient Descent(38/49): loss=5.418673188379427e+43\n",
      "Gradient Descent(39/49): loss=8.446569570602182e+44\n",
      "Gradient Descent(40/49): loss=1.3166421932222187e+46\n",
      "Gradient Descent(41/49): loss=2.0523677103264945e+47\n",
      "Gradient Descent(42/49): loss=3.1992087448467916e+48\n",
      "Gradient Descent(43/49): loss=4.986892232618501e+49\n",
      "Gradient Descent(44/49): loss=7.773514054001399e+50\n",
      "Gradient Descent(45/49): loss=1.2117270221423598e+52\n",
      "Gradient Descent(46/49): loss=1.8888270684147139e+53\n",
      "Gradient Descent(47/49): loss=2.944283348627818e+54\n",
      "Gradient Descent(48/49): loss=4.589517262839066e+55\n",
      "Gradient Descent(49/49): loss=7.1540902188352745e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8347039151813535\n",
      "Gradient Descent(2/49): loss=6.967334874596312\n",
      "Gradient Descent(3/49): loss=103.98620066084153\n",
      "Gradient Descent(4/49): loss=1646.2017971301643\n",
      "Gradient Descent(5/49): loss=26197.44405204108\n",
      "Gradient Descent(6/49): loss=417185.3903583408\n",
      "Gradient Descent(7/49): loss=6644396.744853869\n",
      "Gradient Descent(8/49): loss=105826635.46694759\n",
      "Gradient Descent(9/49): loss=1685534272.993361\n",
      "Gradient Descent(10/49): loss=26846085700.807816\n",
      "Gradient Descent(11/49): loss=427587058777.1847\n",
      "Gradient Descent(12/49): loss=6810330392971.692\n",
      "Gradient Descent(13/49): loss=108470545142766.16\n",
      "Gradient Descent(14/49): loss=1727648816859618.8\n",
      "Gradient Descent(15/49): loss=2.751687507123935e+16\n",
      "Gradient Descent(16/49): loss=4.3827102280658566e+17\n",
      "Gradient Descent(17/49): loss=6.980497929197729e+18\n",
      "Gradient Descent(18/49): loss=1.1118086481924976e+20\n",
      "Gradient Descent(19/49): loss=1.770817043055904e+21\n",
      "Gradient Descent(20/49): loss=2.8204430727180404e+22\n",
      "Gradient Descent(21/49): loss=4.4922196551252515e+23\n",
      "Gradient Descent(22/49): loss=7.154917475589509e+24\n",
      "Gradient Descent(23/49): loss=1.1395890676024913e+26\n",
      "Gradient Descent(24/49): loss=1.8150639017567686e+27\n",
      "Gradient Descent(25/49): loss=2.8909166129432344e+28\n",
      "Gradient Descent(26/49): loss=4.604465360642214e+29\n",
      "Gradient Descent(27/49): loss=7.333695189419267e+30\n",
      "Gradient Descent(28/49): loss=1.1680636277782719e+32\n",
      "Gradient Descent(29/49): loss=1.8604163430558122e+33\n",
      "Gradient Descent(30/49): loss=2.9631510537593546e+34\n",
      "Gradient Descent(31/49): loss=4.71951571494649e+35\n",
      "Gradient Descent(32/49): loss=7.516939966785705e+36\n",
      "Gradient Descent(33/49): loss=1.1972496729974485e+38\n",
      "Gradient Descent(34/49): loss=1.9069019918027125e+39\n",
      "Gradient Descent(35/49): loss=3.0371903942452574e+40\n",
      "Gradient Descent(36/49): loss=4.837440796931127e+41\n",
      "Gradient Descent(37/49): loss=7.70476342482602e+42\n",
      "Gradient Descent(38/49): loss=1.2271649809171101e+44\n",
      "Gradient Descent(39/49): loss=1.9545491631020402e+45\n",
      "Gradient Descent(40/49): loss=3.11307973287168e+46\n",
      "Gradient Descent(41/49): loss=4.958312436529067e+47\n",
      "Gradient Descent(42/49): loss=7.897279969620447e+48\n",
      "Gradient Descent(43/49): loss=1.257827773399186e+50\n",
      "Gradient Descent(44/49): loss=2.003386879559229e+51\n",
      "Gradient Descent(45/49): loss=3.190865294970961e+52\n",
      "Gradient Descent(46/49): loss=5.082204258465664e+53\n",
      "Gradient Descent(47/49): loss=8.094606865878866e+54\n",
      "Gradient Descent(48/49): loss=1.2892567276096631e+56\n",
      "Gradient Descent(49/49): loss=2.0534448889585424e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8344067713512041\n",
      "Gradient Descent(2/49): loss=6.92076886254445\n",
      "Gradient Descent(3/49): loss=103.30155953253546\n",
      "Gradient Descent(4/49): loss=1638.2932651278477\n",
      "Gradient Descent(5/49): loss=26127.72147068518\n",
      "Gradient Descent(6/49): loss=417010.4723097118\n",
      "Gradient Descent(7/49): loss=6656718.531382756\n",
      "Gradient Descent(8/49): loss=106264847.13522968\n",
      "Gradient Descent(9/49): loss=1696379891.4051661\n",
      "Gradient Descent(10/49): loss=27080560184.29612\n",
      "Gradient Descent(11/49): loss=432307167273.587\n",
      "Gradient Descent(12/49): loss=6901242610994.293\n",
      "Gradient Descent(13/49): loss=110169701241922.9\n",
      "Gradient Descent(14/49): loss=1758721418554731.2\n",
      "Gradient Descent(15/49): loss=2.8075786729137284e+16\n",
      "Gradient Descent(16/49): loss=4.481948037162041e+17\n",
      "Gradient Descent(17/49): loss=7.154869213464415e+18\n",
      "Gradient Descent(18/49): loss=1.1421853408278731e+20\n",
      "Gradient Descent(19/49): loss=1.8233559746436798e+21\n",
      "Gradient Descent(20/49): loss=2.9107596564567842e+22\n",
      "Gradient Descent(21/49): loss=4.646663567336722e+23\n",
      "Gradient Descent(22/49): loss=7.417816946900144e+24\n",
      "Gradient Descent(23/49): loss=1.1841616562151815e+26\n",
      "Gradient Descent(24/49): loss=1.890365909657943e+27\n",
      "Gradient Descent(25/49): loss=3.0177326327373846e+28\n",
      "Gradient Descent(26/49): loss=4.817432538410471e+29\n",
      "Gradient Descent(27/49): loss=7.690428240849296e+30\n",
      "Gradient Descent(28/49): loss=1.227680638101188e+32\n",
      "Gradient Descent(29/49): loss=1.9598385186961886e+33\n",
      "Gradient Descent(30/49): loss=3.1286369599393335e+34\n",
      "Gradient Descent(31/49): loss=4.994477419297957e+35\n",
      "Gradient Descent(32/49): loss=7.973058239509169e+36\n",
      "Gradient Descent(33/49): loss=1.2727989808299088e+38\n",
      "Gradient Descent(34/49): loss=2.0318643071913587e+39\n",
      "Gradient Descent(35/49): loss=3.243617118664176e+40\n",
      "Gradient Descent(36/49): loss=5.178028855201739e+41\n",
      "Gradient Descent(37/49): loss=8.266075139085384e+42\n",
      "Gradient Descent(38/49): loss=1.3195754623183457e+44\n",
      "Gradient Descent(39/49): loss=2.106537106733033e+45\n",
      "Gradient Descent(40/49): loss=3.362822899303525e+46\n",
      "Gradient Descent(41/49): loss=5.368325967738734e+47\n",
      "Gradient Descent(42/49): loss=8.569860667318105e+48\n",
      "Gradient Descent(43/49): loss=1.368071020623615e+50\n",
      "Gradient Descent(44/49): loss=2.1839541973041883e+51\n",
      "Gradient Descent(45/49): loss=3.4864095971774866e+52\n",
      "Gradient Descent(46/49): loss=5.565616666455338e+53\n",
      "Gradient Descent(47/49): loss=8.884810580777147e+54\n",
      "Gradient Descent(48/49): loss=1.4183488333300042e+56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=2.2642164340127333e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8338998618236504\n",
      "Gradient Descent(2/49): loss=6.87852880978577\n",
      "Gradient Descent(3/49): loss=101.60293626673317\n",
      "Gradient Descent(4/49): loss=1593.832701363159\n",
      "Gradient Descent(5/49): loss=25139.772590965084\n",
      "Gradient Descent(6/49): loss=396828.83269263303\n",
      "Gradient Descent(7/49): loss=6264837.843915991\n",
      "Gradient Descent(8/49): loss=98908120.21569952\n",
      "Gradient Descent(9/49): loss=1561557496.4960172\n",
      "Gradient Descent(10/49): loss=24653865128.74638\n",
      "Gradient Descent(11/49): loss=389235381356.5188\n",
      "Gradient Descent(12/49): loss=6145251648923.42\n",
      "Gradient Descent(13/49): loss=97021291305000.33\n",
      "Gradient Descent(14/49): loss=1531773082714586.8\n",
      "Gradient Descent(15/49): loss=2.4183648264465664e+16\n",
      "Gradient Descent(16/49): loss=3.8181167326578003e+17\n",
      "Gradient Descent(17/49): loss=6.028046400327825e+18\n",
      "Gradient Descent(18/49): loss=9.517085503261093e+19\n",
      "Gradient Descent(19/49): loss=1.5025583823061448e+21\n",
      "Gradient Descent(20/49): loss=2.3722406313062688e+22\n",
      "Gradient Descent(21/49): loss=3.7452958095295645e+23\n",
      "Gradient Descent(22/49): loss=5.91307665662836e+24\n",
      "Gradient Descent(23/49): loss=9.335571160547865e+25\n",
      "Gradient Descent(24/49): loss=1.4739008802796223e+27\n",
      "Gradient Descent(25/49): loss=2.326996139314488e+28\n",
      "Gradient Descent(26/49): loss=3.6738637616915096e+29\n",
      "Gradient Descent(27/49): loss=5.800299670220464e+30\n",
      "Gradient Descent(28/49): loss=9.157518745025465e+31\n",
      "Gradient Descent(29/49): loss=1.4457899476477363e+33\n",
      "Gradient Descent(30/49): loss=2.2826145716106156e+34\n",
      "Gradient Descent(31/49): loss=3.603794099555196e+35\n",
      "Gradient Descent(32/49): loss=5.68967361967943e+36\n",
      "Gradient Descent(33/49): loss=8.982862229135626e+37\n",
      "Gradient Descent(34/49): loss=1.4182151599792045e+39\n",
      "Gradient Descent(35/49): loss=2.2390794700948995e+40\n",
      "Gradient Descent(36/49): loss=3.5350608390577175e+41\n",
      "Gradient Descent(37/49): loss=5.581157481341976e+42\n",
      "Gradient Descent(38/49): loss=8.811536844679176e+43\n",
      "Gradient Descent(39/49): loss=1.3911662916644647e+45\n",
      "Gradient Descent(40/49): loss=2.196374690565026e+46\n",
      "Gradient Descent(41/49): loss=3.4676384917167817e+47\n",
      "Gradient Descent(42/49): loss=5.4747110139675234e+48\n",
      "Gradient Descent(43/49): loss=8.643479058746517e+49\n",
      "Gradient Descent(44/49): loss=1.3646333121215628e+51\n",
      "Gradient Descent(45/49): loss=2.1544843967284703e+52\n",
      "Gradient Descent(46/49): loss=3.401502055178436e+53\n",
      "Gradient Descent(47/49): loss=5.3702947438153836e+54\n",
      "Gradient Descent(48/49): loss=8.478626550157502e+55\n",
      "Gradient Descent(49/49): loss=1.3386063820765752e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8451461560496158\n",
      "Gradient Descent(2/49): loss=7.184738941816494\n",
      "Gradient Descent(3/49): loss=108.79463267849177\n",
      "Gradient Descent(4/49): loss=1746.7536274141758\n",
      "Gradient Descent(5/49): loss=28197.816768813747\n",
      "Gradient Descent(6/49): loss=455554.3629392199\n",
      "Gradient Descent(7/49): loss=7361016.956465471\n",
      "Gradient Descent(8/49): loss=118947066.47338778\n",
      "Gradient Descent(9/49): loss=1922093303.0019407\n",
      "Gradient Descent(10/49): loss=31059644042.36576\n",
      "Gradient Descent(11/49): loss=501901881395.7125\n",
      "Gradient Descent(12/49): loss=8110381139802.545\n",
      "Gradient Descent(13/49): loss=131058058040653.53\n",
      "Gradient Descent(14/49): loss=2117806122408364.8\n",
      "Gradient Descent(15/49): loss=3.422225887487881e+16\n",
      "Gradient Descent(16/49): loss=5.53007657946814e+17\n",
      "Gradient Descent(17/49): loss=8.936215197022087e+18\n",
      "Gradient Descent(18/49): loss=1.4440295881251322e+20\n",
      "Gradient Descent(19/49): loss=2.3334503539234e+21\n",
      "Gradient Descent(20/49): loss=3.7706918189425263e+22\n",
      "Gradient Descent(21/49): loss=6.093173042895229e+23\n",
      "Gradient Descent(22/49): loss=9.846139518527765e+24\n",
      "Gradient Descent(23/49): loss=1.5910669652056864e+26\n",
      "Gradient Descent(24/49): loss=2.571052424156006e+27\n",
      "Gradient Descent(25/49): loss=4.154640070038748e+28\n",
      "Gradient Descent(26/49): loss=6.713606439681145e+29\n",
      "Gradient Descent(27/49): loss=1.0848716294816832e+31\n",
      "Gradient Descent(28/49): loss=1.7530763279447414e+32\n",
      "Gradient Descent(29/49): loss=2.8328481712334606e+33\n",
      "Gradient Descent(30/49): loss=4.577683603011757e+34\n",
      "Gradient Descent(31/49): loss=7.397215065062427e+35\n",
      "Gradient Descent(32/49): loss=1.195337980169398e+37\n",
      "Gradient Descent(33/49): loss=1.9315821890645582e+38\n",
      "Gradient Descent(34/49): loss=3.1213011006164717e+39\n",
      "Gradient Descent(35/49): loss=5.043803269602402e+40\n",
      "Gradient Descent(36/49): loss=8.15043169575263e+41\n",
      "Gradient Descent(37/49): loss=1.3170524954349746e+43\n",
      "Gradient Descent(38/49): loss=2.12826429382315e+44\n",
      "Gradient Descent(39/49): loss=3.4391255626197544e+45\n",
      "Gradient Descent(40/49): loss=5.557385269203561e+46\n",
      "Gradient Descent(41/49): loss=8.980344121787254e+47\n",
      "Gradient Descent(42/49): loss=1.4511605123478607e+49\n",
      "Gradient Descent(43/49): loss=2.3449734264510497e+50\n",
      "Gradient Descent(44/49): loss=3.789312294519938e+51\n",
      "Gradient Descent(45/49): loss=6.123262423118825e+52\n",
      "Gradient Descent(46/49): loss=9.894761842829091e+53\n",
      "Gradient Descent(47/49): loss=1.59892399118245e+55\n",
      "Gradient Descent(48/49): loss=2.5837488260837765e+56\n",
      "Gradient Descent(49/49): loss=4.175156563478873e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8505743286725198\n",
      "Gradient Descent(2/49): loss=7.458904029183711\n",
      "Gradient Descent(3/49): loss=115.78406627287927\n",
      "Gradient Descent(4/49): loss=1900.5803642385918\n",
      "Gradient Descent(5/49): loss=31352.252369608093\n",
      "Gradient Descent(6/49): loss=517534.25038736\n",
      "Gradient Descent(7/49): loss=8544102.614076784\n",
      "Gradient Descent(8/49): loss=141061080.26278397\n",
      "Gradient Descent(9/49): loss=2328902262.113061\n",
      "Gradient Descent(10/49): loss=38449982594.18758\n",
      "Gradient Descent(11/49): loss=634806317352.7211\n",
      "Gradient Descent(12/49): loss=10480605757576.39\n",
      "Gradient Descent(13/49): loss=173034037875465.22\n",
      "Gradient Descent(14/49): loss=2856779386901012.5\n",
      "Gradient Descent(15/49): loss=4.716522009872472e+16\n",
      "Gradient Descent(16/49): loss=7.786943570882099e+17\n",
      "Gradient Descent(17/49): loss=1.2856187261814809e+19\n",
      "Gradient Descent(18/49): loss=2.122547176711089e+20\n",
      "Gradient Descent(19/49): loss=3.5043099681502716e+21\n",
      "Gradient Descent(20/49): loss=5.785590298128931e+22\n",
      "Gradient Descent(21/49): loss=9.55196754911739e+23\n",
      "Gradient Descent(22/49): loss=1.5770229027262735e+25\n",
      "Gradient Descent(23/49): loss=2.6036533551174773e+26\n",
      "Gradient Descent(24/49): loss=4.298612773406987e+27\n",
      "Gradient Descent(25/49): loss=7.096978458894742e+28\n",
      "Gradient Descent(26/49): loss=1.1717059875131794e+30\n",
      "Gradient Descent(27/49): loss=1.9344780727826021e+31\n",
      "Gradient Descent(28/49): loss=3.193809243920579e+32\n",
      "Gradient Descent(29/49): loss=5.27295585825898e+33\n",
      "Gradient Descent(30/49): loss=8.70561181325181e+34\n",
      "Gradient Descent(31/49): loss=1.4372901856237563e+36\n",
      "Gradient Descent(32/49): loss=2.3729556543582253e+37\n",
      "Gradient Descent(33/49): loss=3.917732545503295e+38\n",
      "Gradient Descent(34/49): loss=6.468147969771852e+39\n",
      "Gradient Descent(35/49): loss=1.0678865306128051e+41\n",
      "Gradient Descent(36/49): loss=1.7630729037024052e+42\n",
      "Gradient Descent(37/49): loss=2.9108205550507993e+43\n",
      "Gradient Descent(38/49): loss=4.805743588885871e+44\n",
      "Gradient Descent(39/49): loss=7.934247750876778e+45\n",
      "Gradient Descent(40/49): loss=1.309938539332003e+47\n",
      "Gradient Descent(41/49): loss=2.1626990115574087e+48\n",
      "Gradient Descent(42/49): loss=3.5706003557819627e+49\n",
      "Gradient Descent(43/49): loss=5.895035246503982e+50\n",
      "Gradient Descent(44/49): loss=9.732660363753863e+51\n",
      "Gradient Descent(45/49): loss=1.6068551551470614e+53\n",
      "Gradient Descent(46/49): loss=2.6529061871288858e+54\n",
      "Gradient Descent(47/49): loss=4.379928841229379e+55\n",
      "Gradient Descent(48/49): loss=7.231230696097347e+56\n",
      "Gradient Descent(49/49): loss=1.1938709343392697e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8502121430349533\n",
      "Gradient Descent(2/49): loss=7.408783829776767\n",
      "Gradient Descent(3/49): loss=115.0197952997185\n",
      "Gradient Descent(4/49): loss=1891.396224205969\n",
      "Gradient Descent(5/49): loss=31267.58873457826\n",
      "Gradient Descent(6/49): loss=517292.2888862433\n",
      "Gradient Descent(7/49): loss=8559461.369680308\n",
      "Gradient Descent(8/49): loss=141635981.01803324\n",
      "Gradient Descent(9/49): loss=2343716063.9321604\n",
      "Gradient Descent(10/49): loss=38782650471.79096\n",
      "Gradient Descent(11/49): loss=641756469123.1425\n",
      "Gradient Descent(12/49): loss=10619476204696.264\n",
      "Gradient Descent(13/49): loss=175725972529637.5\n",
      "Gradient Descent(14/49): loss=2907828706845550.0\n",
      "Gradient Descent(15/49): loss=4.8117348223800296e+16\n",
      "Gradient Descent(16/49): loss=7.962226923193672e+17\n",
      "Gradient Descent(17/49): loss=1.3175509442743538e+19\n",
      "Gradient Descent(18/49): loss=2.1802198148670374e+20\n",
      "Gradient Descent(19/49): loss=3.607722693269656e+21\n",
      "Gradient Descent(20/49): loss=5.969885670618957e+22\n",
      "Gradient Descent(21/49): loss=9.878679142046418e+23\n",
      "Gradient Descent(22/49): loss=1.6346762228933267e+25\n",
      "Gradient Descent(23/49): loss=2.7049834449217564e+26\n",
      "Gradient Descent(24/49): loss=4.4760762619707394e+27\n",
      "Gradient Descent(25/49): loss=7.406795313513479e+28\n",
      "Gradient Descent(26/49): loss=1.2256407979995255e+30\n",
      "Gradient Descent(27/49): loss=2.0281313336419574e+31\n",
      "Gradient Descent(28/49): loss=3.356054003109247e+32\n",
      "Gradient Descent(29/49): loss=5.553436449088453e+33\n",
      "Gradient Descent(30/49): loss=9.189559037337177e+34\n",
      "Gradient Descent(31/49): loss=1.5206439485692954e+36\n",
      "Gradient Descent(32/49): loss=2.5162883321444617e+37\n",
      "Gradient Descent(33/49): loss=4.163832681834253e+38\n",
      "Gradient Descent(34/49): loss=6.890109682913587e+39\n",
      "Gradient Descent(35/49): loss=1.1401421495559859e+41\n",
      "Gradient Descent(36/49): loss=1.8866522900466467e+42\n",
      "Gradient Descent(37/49): loss=3.121941299095398e+43\n",
      "Gradient Descent(38/49): loss=5.166038027471651e+44\n",
      "Gradient Descent(39/49): loss=8.548510796476616e+45\n",
      "Gradient Descent(40/49): loss=1.4145663746351167e+47\n",
      "Gradient Descent(41/49): loss=2.3407562742660226e+48\n",
      "Gradient Descent(42/49): loss=3.8733706906676866e+49\n",
      "Gradient Descent(43/49): loss=6.409467176170634e+50\n",
      "Gradient Descent(44/49): loss=1.0606077435704273e+52\n",
      "Gradient Descent(45/49): loss=1.7550425874770316e+53\n",
      "Gradient Descent(46/49): loss=2.9041599050455397e+54\n",
      "Gradient Descent(47/49): loss=4.8056638706406904e+55\n",
      "Gradient Descent(48/49): loss=7.952181006788927e+56\n",
      "Gradient Descent(49/49): loss=1.3158885945201263e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8497615247826485\n",
      "Gradient Descent(2/49): loss=7.364004444911693\n",
      "Gradient Descent(3/49): loss=113.14082541076594\n",
      "Gradient Descent(4/49): loss=1840.3537206056342\n",
      "Gradient Descent(5/49): loss=30091.223178226468\n",
      "Gradient Descent(6/49): loss=492374.16660438885\n",
      "Gradient Descent(7/49): loss=8057796.411459169\n",
      "Gradient Descent(8/49): loss=131872220.7808497\n",
      "Gradient Descent(9/49): loss=2158213658.721711\n",
      "Gradient Descent(10/49): loss=35321294356.37363\n",
      "Gradient Descent(11/49): loss=578068170731.5625\n",
      "Gradient Descent(12/49): loss=9460663057939.748\n",
      "Gradient Descent(13/49): loss=154833207961049.44\n",
      "Gradient Descent(14/49): loss=2534000249737447.5\n",
      "Gradient Descent(15/49): loss=4.1471447686023304e+16\n",
      "Gradient Descent(16/49): loss=6.787217067669074e+17\n",
      "Gradient Descent(17/49): loss=1.1107959355864162e+19\n",
      "Gradient Descent(18/49): loss=1.8179286123828576e+20\n",
      "Gradient Descent(19/49): loss=2.9752219412095115e+21\n",
      "Gradient Descent(20/49): loss=4.869248186748915e+22\n",
      "Gradient Descent(21/49): loss=7.9690115133192e+23\n",
      "Gradient Descent(22/49): loss=1.3042084129588949e+25\n",
      "Gradient Descent(23/49): loss=2.1344674701372062e+26\n",
      "Gradient Descent(24/49): loss=3.493269431330953e+27\n",
      "Gradient Descent(25/49): loss=5.717084701734541e+28\n",
      "Gradient Descent(26/49): loss=9.356580741713281e+29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=1.5312979909085251e+31\n",
      "Gradient Descent(28/49): loss=2.506122270186421e+32\n",
      "Gradient Descent(29/49): loss=4.10151967181645e+33\n",
      "Gradient Descent(30/49): loss=6.712547036679845e+34\n",
      "Gradient Descent(31/49): loss=1.0985754384955678e+36\n",
      "Gradient Descent(32/49): loss=1.7979285470492105e+37\n",
      "Gradient Descent(33/49): loss=2.942489834581843e+38\n",
      "Gradient Descent(34/49): loss=4.815678821512398e+39\n",
      "Gradient Descent(35/49): loss=7.881339890935818e+40\n",
      "Gradient Descent(36/49): loss=1.2898600753641764e+42\n",
      "Gradient Descent(37/49): loss=2.1109849810333767e+43\n",
      "Gradient Descent(38/49): loss=3.4548379899969458e+44\n",
      "Gradient Descent(39/49): loss=5.654187805392732e+45\n",
      "Gradient Descent(40/49): loss=9.253643682053011e+46\n",
      "Gradient Descent(41/49): loss=1.5144513118706342e+48\n",
      "Gradient Descent(42/49): loss=2.478550995512115e+49\n",
      "Gradient Descent(43/49): loss=4.05639652407581e+50\n",
      "Gradient Descent(44/49): loss=6.638698493727976e+51\n",
      "Gradient Descent(45/49): loss=1.0864893860608767e+53\n",
      "Gradient Descent(46/49): loss=1.7781485138061473e+54\n",
      "Gradient Descent(47/49): loss=2.9101178324569947e+55\n",
      "Gradient Descent(48/49): loss=4.762698803294374e+56\n",
      "Gradient Descent(49/49): loss=7.794632793872196e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8611019720692127\n",
      "Gradient Descent(2/49): loss=7.684346727717268\n",
      "Gradient Descent(3/49): loss=120.93855013977134\n",
      "Gradient Descent(4/49): loss=2012.1768312032345\n",
      "Gradient Descent(5/49): loss=33652.064038515724\n",
      "Gradient Descent(6/49): loss=563239.4514932517\n",
      "Gradient Descent(7/49): loss=9428626.161040395\n",
      "Gradient Descent(8/49): loss=157842026.23539847\n",
      "Gradient Descent(9/49): loss=2642420235.494976\n",
      "Gradient Descent(10/49): loss=44236673986.83755\n",
      "Gradient Descent(11/49): loss=740565380054.4531\n",
      "Gradient Descent(12/49): loss=12397794739131.26\n",
      "Gradient Descent(13/49): loss=207551321872792.0\n",
      "Gradient Descent(14/49): loss=3474614058984668.0\n",
      "Gradient Descent(15/49): loss=5.8168470341956136e+16\n",
      "Gradient Descent(16/49): loss=9.737976328847295e+17\n",
      "Gradient Descent(17/49): loss=1.630233397115207e+19\n",
      "Gradient Descent(18/49): loss=2.729171687774447e+20\n",
      "Gradient Descent(19/49): loss=4.5689029035328e+21\n",
      "Gradient Descent(20/49): loss=7.648794627156034e+22\n",
      "Gradient Descent(21/49): loss=1.280483750339015e+24\n",
      "Gradient Descent(22/49): loss=2.1436562423334728e+25\n",
      "Gradient Descent(23/49): loss=3.5886922298538916e+26\n",
      "Gradient Descent(24/49): loss=6.007825166312575e+27\n",
      "Gradient Descent(25/49): loss=1.0057692584701951e+29\n",
      "Gradient Descent(26/49): loss=1.6837570556410966e+30\n",
      "Gradient Descent(27/49): loss=2.8187755775448416e+31\n",
      "Gradient Descent(28/49): loss=4.718908663184766e+32\n",
      "Gradient Descent(29/49): loss=7.899919081488465e+33\n",
      "Gradient Descent(30/49): loss=1.3225244637802824e+35\n",
      "Gradient Descent(31/49): loss=2.2140365480398955e+36\n",
      "Gradient Descent(32/49): loss=3.706515811469209e+37\n",
      "Gradient Descent(33/49): loss=6.205073476692965e+38\n",
      "Gradient Descent(34/49): loss=1.038790573400967e+40\n",
      "Gradient Descent(35/49): loss=1.7390379975996999e+41\n",
      "Gradient Descent(36/49): loss=2.911321333225284e+42\n",
      "Gradient Descent(37/49): loss=4.873839396833975e+43\n",
      "Gradient Descent(38/49): loss=8.159288428603229e+44\n",
      "Gradient Descent(39/49): loss=1.365945453688631e+46\n",
      "Gradient Descent(40/49): loss=2.2867275728504126e+47\n",
      "Gradient Descent(41/49): loss=3.828207765041806e+48\n",
      "Gradient Descent(42/49): loss=6.4087978237209e+49\n",
      "Gradient Descent(43/49): loss=1.0728960408156222e+51\n",
      "Gradient Descent(44/49): loss=1.796133917873408e+52\n",
      "Gradient Descent(45/49): loss=3.006905541829345e+53\n",
      "Gradient Descent(46/49): loss=5.033856800716157e+54\n",
      "Gradient Descent(47/49): loss=8.42717336398273e+55\n",
      "Gradient Descent(48/49): loss=1.4107920371615804e+57\n",
      "Gradient Descent(49/49): loss=2.3618051820615262e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8667595887954492\n",
      "Gradient Descent(2/49): loss=7.977573042543328\n",
      "Gradient Descent(3/49): loss=128.68112358920965\n",
      "Gradient Descent(4/49): loss=2188.719613021919\n",
      "Gradient Descent(5/49): loss=37402.64086934655\n",
      "Gradient Descent(6/49): loss=639583.6425993039\n",
      "Gradient Descent(7/49): loss=10938311.578151207\n",
      "Gradient Descent(8/49): loss=187075562.42649865\n",
      "Gradient Descent(9/49): loss=3199538047.3295064\n",
      "Gradient Descent(10/49): loss=54721547206.687065\n",
      "Gradient Descent(11/49): loss=935900495776.6676\n",
      "Gradient Descent(12/49): loss=16006671866367.709\n",
      "Gradient Descent(13/49): loss=273761530945900.62\n",
      "Gradient Descent(14/49): loss=4682133616884320.0\n",
      "Gradient Descent(15/49): loss=8.007836300467131e+16\n",
      "Gradient Descent(16/49): loss=1.369577365700576e+18\n",
      "Gradient Descent(17/49): loss=2.3423832487418913e+19\n",
      "Gradient Descent(18/49): loss=4.006169656114385e+20\n",
      "Gradient Descent(19/49): loss=6.851737572119377e+21\n",
      "Gradient Descent(20/49): loss=1.1718502157201868e+23\n",
      "Gradient Descent(21/49): loss=2.004211214498453e+24\n",
      "Gradient Descent(22/49): loss=3.4277952407537306e+25\n",
      "Gradient Descent(23/49): loss=5.862545887148279e+26\n",
      "Gradient Descent(24/49): loss=1.002669117171716e+28\n",
      "Gradient Descent(25/49): loss=1.7148613893731793e+29\n",
      "Gradient Descent(26/49): loss=2.9329212742265795e+30\n",
      "Gradient Descent(27/49): loss=5.0161647198524355e+31\n",
      "Gradient Descent(28/49): loss=8.579128501608868e+32\n",
      "Gradient Descent(29/49): loss=1.4672852658890142e+34\n",
      "Gradient Descent(30/49): loss=2.509492719559147e+35\n",
      "Gradient Descent(31/49): loss=4.291976383818412e+36\n",
      "Gradient Descent(32/49): loss=7.34055179187412e+37\n",
      "Gradient Descent(33/49): loss=1.2554519361368942e+39\n",
      "Gradient Descent(34/49): loss=2.1471949366186148e+40\n",
      "Gradient Descent(35/49): loss=3.6723397870787775e+41\n",
      "Gradient Descent(36/49): loss=6.280789546290361e+42\n",
      "Gradient Descent(37/49): loss=1.074201179955905e+44\n",
      "Gradient Descent(38/49): loss=1.8372024193999474e+45\n",
      "Gradient Descent(39/49): loss=3.142160698415519e+46\n",
      "Gradient Descent(40/49): loss=5.3740261554259196e+47\n",
      "Gradient Descent(41/49): loss=9.191177629382604e+48\n",
      "Gradient Descent(42/49): loss=1.571963808355667e+50\n",
      "Gradient Descent(43/49): loss=2.6885240547201283e+51\n",
      "Gradient Descent(44/49): loss=4.5981730332390526e+52\n",
      "Gradient Descent(45/49): loss=7.86423882147772e+53\n",
      "Gradient Descent(46/49): loss=1.3450179406944085e+55\n",
      "Gradient Descent(47/49): loss=2.3003793524799095e+56\n",
      "Gradient Descent(48/49): loss=3.934330543266816e+57\n",
      "Gradient Descent(49/49): loss=6.728871395491867e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8663306260540341\n",
      "Gradient Descent(2/49): loss=7.923707771476163\n",
      "Gradient Descent(3/49): loss=127.82969200399218\n",
      "Gradient Descent(4/49): loss=2178.0838702006326\n",
      "Gradient Descent(5/49): loss=37300.23379465534\n",
      "Gradient Descent(6/49): loss=639254.8546539207\n",
      "Gradient Descent(7/49): loss=10957373.860930571\n",
      "Gradient Descent(8/49): loss=187826209.77204698\n",
      "Gradient Descent(9/49): loss=3219662366.6985025\n",
      "Gradient Descent(10/49): loss=55190662823.30354\n",
      "Gradient Descent(11/49): loss=946065453037.372\n",
      "Gradient Descent(12/49): loss=16217235911503.963\n",
      "Gradient Descent(13/49): loss=277992132323250.44\n",
      "Gradient Descent(14/49): loss=4765277323967796.0\n",
      "Gradient Descent(15/49): loss=8.168529034979472e+16\n",
      "Gradient Descent(16/49): loss=1.4002305031136558e+18\n",
      "Gradient Descent(17/49): loss=2.400242998287887e+19\n",
      "Gradient Descent(18/49): loss=4.114441471118539e+20\n",
      "Gradient Descent(19/49): loss=7.052881158936123e+21\n",
      "Gradient Descent(20/49): loss=1.2089887045769526e+23\n",
      "Gradient Descent(21/49): loss=2.0724206956813296e+24\n",
      "Gradient Descent(22/49): loss=3.5524960023445164e+25\n",
      "Gradient Descent(23/49): loss=6.089607130913902e+26\n",
      "Gradient Descent(24/49): loss=1.043866481043305e+28\n",
      "Gradient Descent(25/49): loss=1.789371969029163e+29\n",
      "Gradient Descent(26/49): loss=3.0673003700120275e+30\n",
      "Gradient Descent(27/49): loss=5.257895911368573e+31\n",
      "Gradient Descent(28/49): loss=9.012964522505513e+32\n",
      "Gradient Descent(29/49): loss=1.544981697113866e+34\n",
      "Gradient Descent(30/49): loss=2.648372173724367e+35\n",
      "Gradient Descent(31/49): loss=4.539778810104957e+36\n",
      "Gradient Descent(32/49): loss=7.781984665582323e+37\n",
      "Gradient Descent(33/49): loss=1.3339699546718224e+39\n",
      "Gradient Descent(34/49): loss=2.2866606867491084e+40\n",
      "Gradient Descent(35/49): loss=3.919741279038235e+41\n",
      "Gradient Descent(36/49): loss=6.719130557336612e+42\n",
      "Gradient Descent(37/49): loss=1.1517779422832742e+44\n",
      "Gradient Descent(38/49): loss=1.9743513197281022e+45\n",
      "Gradient Descent(39/49): loss=3.384387728405894e+46\n",
      "Gradient Descent(40/49): loss=5.801439785175517e+47\n",
      "Gradient Descent(41/49): loss=9.944694958715691e+48\n",
      "Gradient Descent(42/49): loss=1.7046967905211656e+50\n",
      "Gradient Descent(43/49): loss=2.9221521219877275e+51\n",
      "Gradient Descent(44/49): loss=5.009086115206902e+52\n",
      "Gradient Descent(45/49): loss=8.586460479166008e+53\n",
      "Gradient Descent(46/49): loss=1.4718713526695752e+55\n",
      "Gradient Descent(47/49): loss=2.523048098882966e+56\n",
      "Gradient Descent(48/49): loss=4.3249511567238705e+57\n",
      "Gradient Descent(49/49): loss=7.413732031636041e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8659381172995549\n",
      "Gradient Descent(2/49): loss=7.876275116918067\n",
      "Gradient Descent(3/49): loss=125.7547453612538\n",
      "Gradient Descent(4/49): loss=2119.623564835275\n",
      "Gradient Descent(5/49): loss=35903.73674974684\n",
      "Gradient Descent(6/49): loss=608601.025435263\n",
      "Gradient Descent(7/49): loss=10317924.377064528\n",
      "Gradient Descent(8/49): loss=174931697.54718304\n",
      "Gradient Descent(9/49): loss=2965848335.1672244\n",
      "Gradient Descent(10/49): loss=50284076184.87225\n",
      "Gradient Descent(11/49): loss=852535166038.8657\n",
      "Gradient Descent(12/49): loss=14454204809503.904\n",
      "Gradient Descent(13/49): loss=245062085993886.62\n",
      "Gradient Descent(14/49): loss=4154875864584476.5\n",
      "Gradient Descent(15/49): loss=7.044334676918594e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=1.194323313088935e+18\n",
      "Gradient Descent(17/49): loss=2.0249012034639868e+19\n",
      "Gradient Descent(18/49): loss=3.4330945725340516e+20\n",
      "Gradient Descent(19/49): loss=5.82059920946784e+21\n",
      "Gradient Descent(20/49): loss=9.86846544468424e+22\n",
      "Gradient Descent(21/49): loss=1.673137193067909e+24\n",
      "Gradient Descent(22/49): loss=2.836700480453933e+25\n",
      "Gradient Descent(23/49): loss=4.809449965697779e+26\n",
      "Gradient Descent(24/49): loss=8.154124530218037e+27\n",
      "Gradient Descent(25/49): loss=1.3824813092666697e+29\n",
      "Gradient Descent(26/49): loss=2.3439114320474794e+30\n",
      "Gradient Descent(27/49): loss=3.973956656381203e+31\n",
      "Gradient Descent(28/49): loss=6.737597372867136e+32\n",
      "Gradient Descent(29/49): loss=1.1423179033916477e+34\n",
      "Gradient Descent(30/49): loss=1.9367292525730157e+35\n",
      "Gradient Descent(31/49): loss=3.2836044910398193e+36\n",
      "Gradient Descent(32/49): loss=5.567148035406877e+37\n",
      "Gradient Descent(33/49): loss=9.438754677278449e+38\n",
      "Gradient Descent(34/49): loss=1.600282393987667e+40\n",
      "Gradient Descent(35/49): loss=2.713179681077694e+41\n",
      "Gradient Descent(36/49): loss=4.600028100958751e+42\n",
      "Gradient Descent(37/49): loss=7.799062729677099e+43\n",
      "Gradient Descent(38/49): loss=1.3222827801586904e+45\n",
      "Gradient Descent(39/49): loss=2.2418485545077603e+46\n",
      "Gradient Descent(40/49): loss=3.800915369059973e+47\n",
      "Gradient Descent(41/49): loss=6.444216588006111e+48\n",
      "Gradient Descent(42/49): loss=1.092577008453711e+50\n",
      "Gradient Descent(43/49): loss=1.8523966460460176e+51\n",
      "Gradient Descent(44/49): loss=3.140623780047182e+52\n",
      "Gradient Descent(45/49): loss=5.3247330958257745e+53\n",
      "Gradient Descent(46/49): loss=9.027755161860332e+54\n",
      "Gradient Descent(47/49): loss=1.5305999717880038e+56\n",
      "Gradient Descent(48/49): loss=2.5950374502122333e+57\n",
      "Gradient Descent(49/49): loss=4.3997252659931043e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8773683558816956\n",
      "Gradient Descent(2/49): loss=8.211065794402417\n",
      "Gradient Descent(3/49): loss=134.19579200812146\n",
      "Gradient Descent(4/49): loss=2312.2505402615807\n",
      "Gradient Descent(5/49): loss=40038.12774137096\n",
      "Gradient Descent(6/49): loss=693816.7224221256\n",
      "Gradient Descent(7/49): loss=12025163.260792283\n",
      "Gradient Descent(8/49): loss=208428287.67513806\n",
      "Gradient Descent(9/49): loss=3612663785.377549\n",
      "Gradient Descent(10/49): loss=62618092466.95419\n",
      "Gradient Descent(11/49): loss=1085356722114.8187\n",
      "Gradient Descent(12/49): loss=18812446137217.93\n",
      "Gradient Descent(13/49): loss=326075422922964.7\n",
      "Gradient Descent(14/49): loss=5651853166059428.0\n",
      "Gradient Descent(15/49): loss=9.796336096926595e+16\n",
      "Gradient Descent(16/49): loss=1.6979952986149053e+18\n",
      "Gradient Descent(17/49): loss=2.943128947974367e+19\n",
      "Gradient Descent(18/49): loss=5.10131448082523e+20\n",
      "Gradient Descent(19/49): loss=8.842089454093183e+21\n",
      "Gradient Descent(20/49): loss=1.532596083001347e+23\n",
      "Gradient Descent(21/49): loss=2.656443101859813e+24\n",
      "Gradient Descent(22/49): loss=4.6044029680687715e+25\n",
      "Gradient Descent(23/49): loss=7.980794573586188e+26\n",
      "Gradient Descent(24/49): loss=1.3833081610687138e+28\n",
      "Gradient Descent(25/49): loss=2.3976829009137966e+29\n",
      "Gradient Descent(26/49): loss=4.155894872255311e+30\n",
      "Gradient Descent(27/49): loss=7.203397155918972e+31\n",
      "Gradient Descent(28/49): loss=1.2485621552246473e+33\n",
      "Gradient Descent(29/49): loss=2.164128149144581e+34\n",
      "Gradient Descent(30/49): loss=3.751075287939703e+35\n",
      "Gradient Descent(31/49): loss=6.50172487306431e+36\n",
      "Gradient Descent(32/49): loss=1.1269415588894155e+38\n",
      "Gradient Descent(33/49): loss=1.953323620957137e+39\n",
      "Gradient Descent(34/49): loss=3.3856885817124347e+40\n",
      "Gradient Descent(35/49): loss=5.868401451430273e+41\n",
      "Gradient Descent(36/49): loss=1.0171678452993049e+43\n",
      "Gradient Descent(37/49): loss=1.7630532506576746e+44\n",
      "Gradient Descent(38/49): loss=3.055893655132136e+45\n",
      "Gradient Descent(39/49): loss=5.29676912934607e+46\n",
      "Gradient Descent(40/49): loss=9.180870270952122e+47\n",
      "Gradient Descent(41/49): loss=1.5913168362401068e+49\n",
      "Gradient Descent(42/49): loss=2.7582235654862213e+50\n",
      "Gradient Descent(43/49): loss=4.780818667876928e+51\n",
      "Gradient Descent(44/49): loss=8.286575251231084e+52\n",
      "Gradient Descent(45/49): loss=1.4363090124229547e+54\n",
      "Gradient Descent(46/49): loss=2.489549079833593e+55\n",
      "Gradient Descent(47/49): loss=4.315126179181273e+56\n",
      "Gradient Descent(48/49): loss=7.479392189167093e+57\n",
      "Gradient Descent(49/49): loss=1.296400271891655e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8832596955501427\n",
      "Gradient Descent(2/49): loss=8.524320035712426\n",
      "Gradient Descent(3/49): loss=142.75783365041804\n",
      "Gradient Descent(4/49): loss=2514.3898003975237\n",
      "Gradient Descent(5/49): loss=44484.3432370078\n",
      "Gradient Descent(6/49): loss=787519.1520370286\n",
      "Gradient Descent(7/49): loss=13943567.074861981\n",
      "Gradient Descent(8/49): loss=246888538.49594983\n",
      "Gradient Descent(9/49): loss=4371510854.891144\n",
      "Gradient Descent(10/49): loss=77403946873.42368\n",
      "Gradient Descent(11/49): loss=1370549998613.0188\n",
      "Gradient Descent(12/49): loss=24267593971292.992\n",
      "Gradient Descent(13/49): loss=429693289681622.2\n",
      "Gradient Descent(14/49): loss=7608349020764382.0\n",
      "Gradient Descent(15/49): loss=1.3471696287093818e+17\n",
      "Gradient Descent(16/49): loss=2.3853611402462003e+18\n",
      "Gradient Descent(17/49): loss=4.2236312703080915e+19\n",
      "Gradient Descent(18/49): loss=7.47855777777338e+20\n",
      "Gradient Descent(19/49): loss=1.324188189182151e+22\n",
      "Gradient Descent(20/49): loss=2.344669136053381e+23\n",
      "Gradient Descent(21/49): loss=4.151580117143379e+24\n",
      "Gradient Descent(22/49): loss=7.350980658223857e+25\n",
      "Gradient Descent(23/49): loss=1.3015987916129943e+27\n",
      "Gradient Descent(24/49): loss=2.3046712991049663e+28\n",
      "Gradient Descent(25/49): loss=4.080758088547355e+29\n",
      "Gradient Descent(26/49): loss=7.225579883652753e+30\n",
      "Gradient Descent(27/49): loss=1.2793947477938413e+32\n",
      "Gradient Descent(28/49): loss=2.2653557874098074e+33\n",
      "Gradient Descent(29/49): loss=4.011144216748068e+34\n",
      "Gradient Descent(30/49): loss=7.102318327642387e+35\n",
      "Gradient Descent(31/49): loss=1.257569483952888e+37\n",
      "Gradient Descent(32/49): loss=2.226710961144015e+38\n",
      "Gradient Descent(33/49): loss=3.9427178917333424e+39\n",
      "Gradient Descent(34/49): loss=6.981159497148076e+40\n",
      "Gradient Descent(35/49): loss=1.2361165384621122e+42\n",
      "Gradient Descent(36/49): loss=2.1887253790488066e+43\n",
      "Gradient Descent(37/49): loss=3.8754588550787976e+44\n",
      "Gradient Descent(38/49): loss=6.86206752166225e+45\n",
      "Gradient Descent(39/49): loss=1.215029559922767e+47\n",
      "Gradient Descent(40/49): loss=2.1513877950424153e+48\n",
      "Gradient Descent(41/49): loss=3.809347193949465e+49\n",
      "Gradient Descent(42/49): loss=6.745007142594021e+50\n",
      "Gradient Descent(43/49): loss=1.194302305284903e+52\n",
      "Gradient Descent(44/49): loss=2.11468715489053e+53\n",
      "Gradient Descent(45/49): loss=3.744363335204485e+54\n",
      "Gradient Descent(46/49): loss=6.629943702830756e+55\n",
      "Gradient Descent(47/49): loss=1.1739286379991495e+57\n",
      "Gradient Descent(48/49): loss=2.0786125929336763e+58\n",
      "Gradient Descent(49/49): loss=3.680488039602289e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8827622204084475\n",
      "Gradient Descent(2/49): loss=8.466512150258474\n",
      "Gradient Descent(3/49): loss=141.8111530990879\n",
      "Gradient Descent(4/49): loss=2502.106096821855\n",
      "Gradient Descent(5/49): loss=44360.935158887034\n",
      "Gradient Descent(6/49): loss=787078.9412674262\n",
      "Gradient Descent(7/49): loss=13967127.89105278\n",
      "Gradient Descent(8/49): loss=247864167.58908904\n",
      "Gradient Descent(9/49): loss=4398706520.1106825\n",
      "Gradient Descent(10/49): loss=78061595407.18147\n",
      "Gradient Descent(11/49): loss=1385320212588.049\n",
      "Gradient Descent(12/49): loss=24584591698041.26\n",
      "Gradient Descent(13/49): loss=436290593995052.2\n",
      "Gradient Descent(14/49): loss=7742633563529910.0\n",
      "Gradient Descent(15/49): loss=1.3740469201652976e+17\n",
      "Gradient Descent(16/49): loss=2.4384531761106294e+18\n",
      "Gradient Descent(17/49): loss=4.327402366074929e+19\n",
      "Gradient Descent(18/49): loss=7.67962715973795e+20\n",
      "Gradient Descent(19/49): loss=1.362865486598589e+22\n",
      "Gradient Descent(20/49): loss=2.4186100391785038e+23\n",
      "Gradient Descent(21/49): loss=4.292187731762575e+24\n",
      "Gradient Descent(22/49): loss=7.617133488354989e+25\n",
      "Gradient Descent(23/49): loss=1.3517750435300064e+27\n",
      "Gradient Descent(24/49): loss=2.398928377852561e+28\n",
      "Gradient Descent(25/49): loss=4.257259659889983e+29\n",
      "Gradient Descent(26/49): loss=7.55514836501741e+30\n",
      "Gradient Descent(27/49): loss=1.3407748499630055e+32\n",
      "Gradient Descent(28/49): loss=2.379406877854441e+33\n",
      "Gradient Descent(29/49): loss=4.222615818410692e+34\n",
      "Gradient Descent(30/49): loss=7.493667651314135e+35\n",
      "Gradient Descent(31/49): loss=1.329864171481449e+37\n",
      "Gradient Descent(32/49): loss=2.3600442358554525e+38\n",
      "Gradient Descent(33/49): loss=4.188253894373192e+39\n",
      "Gradient Descent(34/49): loss=7.432687242565138e+40\n",
      "Gradient Descent(35/49): loss=1.319042279648097e+42\n",
      "Gradient Descent(36/49): loss=2.340839159134047e+43\n",
      "Gradient Descent(37/49): loss=4.1541715936484356e+44\n",
      "Gradient Descent(38/49): loss=7.372203067492895e+45\n",
      "Gradient Descent(39/49): loss=1.3083084519534539e+47\n",
      "Gradient Descent(40/49): loss=2.3217903654883034e+48\n",
      "Gradient Descent(41/49): loss=4.1203666407760325e+49\n",
      "Gradient Descent(42/49): loss=7.312211087950404e+50\n",
      "Gradient Descent(43/49): loss=1.2976619717674975e+52\n",
      "Gradient Descent(44/49): loss=2.3028965831503604e+53\n",
      "Gradient Descent(45/49): loss=4.0868367788124024e+54\n",
      "Gradient Descent(46/49): loss=7.25270729865123e+55\n",
      "Gradient Descent(47/49): loss=1.2871021282918578e+57\n",
      "Gradient Descent(48/49): loss=2.2841565507014376e+58\n",
      "Gradient Descent(49/49): loss=4.053579769179915e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8824296393743689\n",
      "Gradient Descent(2/49): loss=8.416309076347403\n",
      "Gradient Descent(3/49): loss=139.52353920156534\n",
      "Gradient Descent(4/49): loss=2435.302907157031\n",
      "Gradient Descent(5/49): loss=42707.936917251754\n",
      "Gradient Descent(6/49): loss=749502.1947889203\n",
      "Gradient Descent(7/49): loss=13155428.151908923\n",
      "Gradient Descent(8/49): loss=230916036.40104482\n",
      "Gradient Descent(9/49): loss=4053289576.3105016\n",
      "Gradient Descent(10/49): loss=71147940160.47018\n",
      "Gradient Descent(11/49): loss=1248870294520.6362\n",
      "Gradient Descent(12/49): loss=21921608653161.895\n",
      "Gradient Descent(13/49): loss=384793321923678.06\n",
      "Gradient Descent(14/49): loss=6754335634306749.0\n",
      "Gradient Descent(15/49): loss=1.1855987984902312e+17\n",
      "Gradient Descent(16/49): loss=2.0810995904350423e+18\n",
      "Gradient Descent(17/49): loss=3.6529857417437966e+19\n",
      "Gradient Descent(18/49): loss=6.412141394640153e+20\n",
      "Gradient Descent(19/49): loss=1.1255329249078821e+22\n",
      "Gradient Descent(20/49): loss=1.975665050237285e+23\n",
      "Gradient Descent(21/49): loss=3.4679148911195847e+24\n",
      "Gradient Descent(22/49): loss=6.087283717757736e+25\n",
      "Gradient Descent(23/49): loss=1.0685101631348897e+27\n",
      "Gradient Descent(24/49): loss=1.8755721298023056e+28\n",
      "Gradient Descent(25/49): loss=3.2922202665535885e+29\n",
      "Gradient Descent(26/49): loss=5.778884272847828e+30\n",
      "Gradient Descent(27/49): loss=1.014376339828789e+32\n",
      "Gradient Descent(28/49): loss=1.7805502069647446e+33\n",
      "Gradient Descent(29/49): loss=3.125426841143902e+34\n",
      "Gradient Descent(30/49): loss=5.4861092381070574e+35\n",
      "Gradient Descent(31/49): loss=9.629850929874322e+36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=1.6903423702805868e+38\n",
      "Gradient Descent(33/49): loss=2.967083654329298e+39\n",
      "Gradient Descent(34/49): loss=5.208167035608763e+40\n",
      "Gradient Descent(35/49): loss=9.141974757342385e+41\n",
      "Gradient Descent(36/49): loss=1.604704724185495e+43\n",
      "Gradient Descent(37/49): loss=2.8167625925187272e+44\n",
      "Gradient Descent(38/49): loss=4.944306191059536e+45\n",
      "Gradient Descent(39/49): loss=8.678815806443229e+46\n",
      "Gradient Descent(40/49): loss=1.5234057295716938e+48\n",
      "Gradient Descent(41/49): loss=2.674057231597095e+49\n",
      "Gradient Descent(42/49): loss=4.693813302032875e+50\n",
      "Gradient Descent(43/49): loss=8.23912183105447e+51\n",
      "Gradient Descent(44/49): loss=1.4462255777740161e+53\n",
      "Gradient Descent(45/49): loss=2.5385817380735266e+54\n",
      "Gradient Descent(46/49): loss=4.456011109137906e+55\n",
      "Gradient Descent(47/49): loss=7.821704027473506e+56\n",
      "Gradient Descent(48/49): loss=1.372955596271652e+58\n",
      "Gradient Descent(49/49): loss=2.40996982590062e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8939453074870649\n",
      "Gradient Descent(2/49): loss=8.765859656126196\n",
      "Gradient Descent(3/49): loss=148.6465733901731\n",
      "Gradient Descent(4/49): loss=2650.7822577272855\n",
      "Gradient Descent(5/49): loss=47494.92331493318\n",
      "Gradient Descent(6/49): loss=851628.1102406979\n",
      "Gradient Descent(7/49): loss=15273174.741442166\n",
      "Gradient Descent(8/49): loss=273923126.3810851\n",
      "Gradient Descent(9/49): loss=4912849754.758869\n",
      "Gradient Descent(10/49): loss=88112947956.15129\n",
      "Gradient Descent(11/49): loss=1580324883458.4412\n",
      "Gradient Descent(12/49): loss=28343477518814.45\n",
      "Gradient Descent(13/49): loss=508346594210810.06\n",
      "Gradient Descent(14/49): loss=9117309773619360.0\n",
      "Gradient Descent(15/49): loss=1.6352098916458605e+17\n",
      "Gradient Descent(16/49): loss=2.932785503358674e+18\n",
      "Gradient Descent(17/49): loss=5.2600163782216425e+19\n",
      "Gradient Descent(18/49): loss=9.433956990803186e+20\n",
      "Gradient Descent(19/49): loss=1.6920012811222819e+22\n",
      "Gradient Descent(20/49): loss=3.034642131749787e+23\n",
      "Gradient Descent(21/49): loss=5.442698519531362e+24\n",
      "Gradient Descent(22/49): loss=9.761601496465168e+25\n",
      "Gradient Descent(23/49): loss=1.7507650558607507e+27\n",
      "Gradient Descent(24/49): loss=3.1400362757413942e+28\n",
      "Gradient Descent(25/49): loss=5.631725273454545e+29\n",
      "Gradient Descent(26/49): loss=1.0100625206368999e+31\n",
      "Gradient Descent(27/49): loss=1.8115697163074894e+32\n",
      "Gradient Descent(28/49): loss=3.2490907938778324e+33\n",
      "Gradient Descent(29/49): loss=5.827316990250358e+34\n",
      "Gradient Descent(30/49): loss=1.0451423323979103e+36\n",
      "Gradient Descent(31/49): loss=1.8744861431044386e+37\n",
      "Gradient Descent(32/49): loss=3.361932812183527e+38\n",
      "Gradient Descent(33/49): loss=6.029701673290274e+39\n",
      "Gradient Descent(34/49): loss=1.081440477844232e+41\n",
      "Gradient Descent(35/49): loss=1.9395876786086227e+42\n",
      "Gradient Descent(36/49): loss=3.478693871815914e+43\n",
      "Gradient Descent(37/49): loss=6.239115244581529e+44\n",
      "Gradient Descent(38/49): loss=1.118999270115415e+46\n",
      "Gradient Descent(39/49): loss=2.006950212381939e+47\n",
      "Gradient Descent(40/49): loss=3.5995100824010966e+48\n",
      "Gradient Descent(41/49): loss=6.455801819782001e+49\n",
      "Gradient Descent(42/49): loss=1.15786249190054e+51\n",
      "Gradient Descent(43/49): loss=2.0766522696562584e+52\n",
      "Gradient Descent(44/49): loss=3.72452228069835e+53\n",
      "Gradient Descent(45/49): loss=6.680013992768602e+54\n",
      "Gradient Descent(46/49): loss=1.1980754464762554e+56\n",
      "Gradient Descent(47/49): loss=2.1487751028712534e+57\n",
      "Gradient Descent(48/49): loss=3.8538761947750855e+58\n",
      "Gradient Descent(49/49): loss=6.912013130088803e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9000746489365998\n",
      "Gradient Descent(2/49): loss=9.100140537834603\n",
      "Gradient Descent(3/49): loss=158.09899744155922\n",
      "Gradient Descent(4/49): loss=2881.7057500633714\n",
      "Gradient Descent(5/49): loss=52750.76457826275\n",
      "Gradient Descent(6/49): loss=966239.474634707\n",
      "Gradient Descent(7/49): loss=17701116.820871003\n",
      "Gradient Descent(8/49): loss=324288303.7249722\n",
      "Gradient Descent(9/49): loss=5941083280.557369\n",
      "Gradient Descent(10/49): loss=108843112261.97597\n",
      "Gradient Descent(11/49): loss=1994052129487.6494\n",
      "Gradient Descent(12/49): loss=36531888761796.0\n",
      "Gradient Descent(13/49): loss=669279867955893.0\n",
      "Gradient Descent(14/49): loss=1.2261494302205314e+16\n",
      "Gradient Descent(15/49): loss=2.2463583634720368e+17\n",
      "Gradient Descent(16/49): loss=4.115424903290894e+18\n",
      "Gradient Descent(17/49): loss=7.539634998911694e+19\n",
      "Gradient Descent(18/49): loss=1.3812934813531416e+21\n",
      "Gradient Descent(19/49): loss=2.5305889236336966e+22\n",
      "Gradient Descent(20/49): loss=4.6361474855882066e+23\n",
      "Gradient Descent(21/49): loss=8.493621112226123e+24\n",
      "Gradient Descent(22/49): loss=1.556067830506323e+26\n",
      "Gradient Descent(23/49): loss=2.8507830301629144e+27\n",
      "Gradient Descent(24/49): loss=5.222756827008337e+28\n",
      "Gradient Descent(25/49): loss=9.568314594781069e+29\n",
      "Gradient Descent(26/49): loss=1.7529562875923414e+31\n",
      "Gradient Descent(27/49): loss=3.2114911312443588e+32\n",
      "Gradient Descent(28/49): loss=5.88358954473807e+33\n",
      "Gradient Descent(29/49): loss=1.077898848736288e+35\n",
      "Gradient Descent(30/49): loss=1.9747569392330085e+36\n",
      "Gradient Descent(31/49): loss=3.6178394416329647e+37\n",
      "Gradient Descent(32/49): loss=6.628037084158177e+38\n",
      "Gradient Descent(33/49): loss=1.2142848320860623e+40\n",
      "Gradient Descent(34/49): loss=2.2246219125093342e+41\n",
      "Gradient Descent(35/49): loss=4.075602793386413e+42\n",
      "Gradient Descent(36/49): loss=7.466679185373439e+43\n",
      "Gradient Descent(37/49): loss=1.3679276633080588e+45\n",
      "Gradient Descent(38/49): loss=2.506102171510218e+46\n",
      "Gradient Descent(39/49): loss=4.591286705073261e+47\n",
      "Gradient Descent(40/49): loss=8.411434237527247e+48\n",
      "Gradient Descent(41/49): loss=1.5410108424304294e+50\n",
      "Gradient Descent(42/49): loss=2.823197981972499e+51\n",
      "Gradient Descent(43/49): loss=5.172219835158939e+52\n",
      "Gradient Descent(44/49): loss=9.475728657371917e+53\n",
      "Gradient Descent(45/49): loss=1.7359941466095918e+55\n",
      "Gradient Descent(46/49): loss=3.1804157611860144e+56\n",
      "Gradient Descent(47/49): loss=5.826658133470784e+57\n",
      "Gradient Descent(48/49): loss=1.0674687699221052e+59\n",
      "Gradient Descent(49/49): loss=1.9556485873322576e+60\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8995069260981934\n",
      "Gradient Descent(2/49): loss=9.038185721905831\n",
      "Gradient Descent(3/49): loss=157.04839167454122\n",
      "Gradient Descent(4/49): loss=2867.5554425684836\n",
      "Gradient Descent(5/49): loss=52602.57964375334\n",
      "Gradient Descent(6/49): loss=965657.4448542598\n",
      "Gradient Descent(7/49): loss=17730120.898702994\n",
      "Gradient Descent(8/49): loss=325550733.32867754\n",
      "Gradient Descent(9/49): loss=5977649426.544214\n",
      "Gradient Descent(10/49): loss=109759837765.74678\n",
      "Gradient Descent(11/49): loss=2015379329012.7812\n",
      "Gradient Descent(12/49): loss=37005836810227.04\n",
      "Gradient Descent(13/49): loss=679490957098073.9\n",
      "Gradient Descent(14/49): loss=1.2476625496609338e+16\n",
      "Gradient Descent(15/49): loss=2.2909235614223024e+17\n",
      "Gradient Descent(16/49): loss=4.2065306605738173e+18\n",
      "Gradient Descent(17/49): loss=7.723915586127464e+19\n",
      "Gradient Descent(18/49): loss=1.4182440780624094e+21\n",
      "Gradient Descent(19/49): loss=2.6041406622901236e+22\n",
      "Gradient Descent(20/49): loss=4.781651264349186e+23\n",
      "Gradient Descent(21/49): loss=8.779936178167774e+24\n",
      "Gradient Descent(22/49): loss=1.6121476668000966e+26\n",
      "Gradient Descent(23/49): loss=2.96018108426794e+27\n",
      "Gradient Descent(24/49): loss=5.435402867933756e+28\n",
      "Gradient Descent(25/49): loss=9.980336842821486e+29\n",
      "Gradient Descent(26/49): loss=1.8325619262522952e+31\n",
      "Gradient Descent(27/49): loss=3.3648996686570166e+32\n",
      "Gradient Descent(28/49): loss=6.178535970832615e+33\n",
      "Gradient Descent(29/49): loss=1.1344857351455134e+35\n",
      "Gradient Descent(30/49): loss=2.0831114188288975e+36\n",
      "Gradient Descent(31/49): loss=3.8249517370077685e+37\n",
      "Gradient Descent(32/49): loss=7.02327089093663e+38\n",
      "Gradient Descent(33/49): loss=1.2895936314758683e+40\n",
      "Gradient Descent(34/49): loss=2.3679162603413704e+41\n",
      "Gradient Descent(35/49): loss=4.3479025323443816e+42\n",
      "Gradient Descent(36/49): loss=7.983498718844631e+43\n",
      "Gradient Descent(37/49): loss=1.4659080170186606e+45\n",
      "Gradient Descent(38/49): loss=2.6916598724908044e+46\n",
      "Gradient Descent(39/49): loss=4.942351624430059e+47\n",
      "Gradient Descent(40/49): loss=9.075009747387679e+48\n",
      "Gradient Descent(41/49): loss=1.6663282617953898e+50\n",
      "Gradient Descent(42/49): loss=3.05966599854873e+51\n",
      "Gradient Descent(43/49): loss=5.618074323836163e+52\n",
      "Gradient Descent(44/49): loss=1.0315753132243257e+54\n",
      "Gradient Descent(45/49): loss=1.8941501402694811e+55\n",
      "Gradient Descent(46/49): loss=3.4779862486905797e+56\n",
      "Gradient Descent(47/49): loss=6.386182430269148e+57\n",
      "Gradient Descent(48/49): loss=1.1726132053579214e+59\n",
      "Gradient Descent(49/49): loss=2.1531200281132984e+60\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8992360910070905\n",
      "Gradient Descent(2/49): loss=8.98509182666601\n",
      "Gradient Descent(3/49): loss=154.5303068669543\n",
      "Gradient Descent(4/49): loss=2791.3872810881758\n",
      "Gradient Descent(5/49): loss=50651.468975978896\n",
      "Gradient Descent(6/49): loss=919751.3508866078\n",
      "Gradient Descent(7/49): loss=16703896.992256448\n",
      "Gradient Descent(8/49): loss=303377052.0394333\n",
      "Gradient Descent(9/49): loss=5510008309.820276\n",
      "Gradient Descent(10/49): loss=100074400222.09775\n",
      "Gradient Descent(11/49): loss=1817582193258.3818\n",
      "Gradient Descent(12/49): loss=33011496129917.14\n",
      "Gradient Descent(13/49): loss=599565146036662.6\n",
      "Gradient Descent(14/49): loss=1.0889490371727636e+16\n",
      "Gradient Descent(15/49): loss=1.977783428026169e+17\n",
      "Gradient Descent(16/49): loss=3.592112356403181e+18\n",
      "Gradient Descent(17/49): loss=6.524107241191294e+19\n",
      "Gradient Descent(18/49): loss=1.184928840589964e+21\n",
      "Gradient Descent(19/49): loss=2.15210496299321e+22\n",
      "Gradient Descent(20/49): loss=3.908720602543296e+23\n",
      "Gradient Descent(21/49): loss=7.099141078842273e+24\n",
      "Gradient Descent(22/49): loss=1.2893682916228352e+26\n",
      "Gradient Descent(23/49): loss=2.3417911730156045e+27\n",
      "Gradient Descent(24/49): loss=4.253234652693081e+28\n",
      "Gradient Descent(25/49): loss=7.724858313294522e+29\n",
      "Gradient Descent(26/49): loss=1.4030130202830753e+31\n",
      "Gradient Descent(27/49): loss=2.5481962972655977e+32\n",
      "Gradient Descent(28/49): loss=4.628114119773446e+33\n",
      "Gradient Descent(29/49): loss=8.405726171343636e+34\n",
      "Gradient Descent(30/49): loss=1.5266743783550012e+36\n",
      "Gradient Descent(31/49): loss=2.772793938341031e+37\n",
      "Gradient Descent(32/49): loss=5.03603540709516e+38\n",
      "Gradient Descent(33/49): loss=9.146605620715547e+39\n",
      "Gradient Descent(34/49): loss=1.6612352300589092e+41\n",
      "Gradient Descent(35/49): loss=3.017187581957837e+42\n",
      "Gradient Descent(36/49): loss=5.479910815759542e+43\n",
      "Gradient Descent(37/49): loss=9.952786074106947e+44\n",
      "Gradient Descent(38/49): loss=1.8076562551357282e+46\n",
      "Gradient Descent(39/49): loss=3.2831220448235616e+47\n",
      "Gradient Descent(40/49): loss=5.962909336652098e+48\n",
      "Gradient Descent(41/49): loss=1.0830023152259498e+50\n",
      "Gradient Descent(42/49): loss=1.9669828074952715e+51\n",
      "Gradient Descent(43/49): loss=3.572495931529731e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=6.488479275041721e+53\n",
      "Gradient Descent(45/49): loss=1.1784579775467719e+55\n",
      "Gradient Descent(46/49): loss=2.1403523783848473e+56\n",
      "Gradient Descent(47/49): loss=3.887375189393063e+57\n",
      "Gradient Descent(48/49): loss=7.060372869308709e+58\n",
      "Gradient Descent(49/49): loss=1.2823270876885256e+60\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9108328268853202\n",
      "Gradient Descent(2/49): loss=9.349708714093188\n",
      "Gradient Descent(3/49): loss=164.37536930194025\n",
      "Gradient Descent(4/49): loss=3031.921970690988\n",
      "Gradient Descent(5/49): loss=56179.161281727625\n",
      "Gradient Descent(6/49): loss=1041743.2056390553\n",
      "Gradient Descent(7/49): loss=19320752.092652116\n",
      "Gradient Descent(8/49): loss=358350500.41351086\n",
      "Gradient Descent(9/49): loss=6646570332.839723\n",
      "Gradient Descent(10/49): loss=123278892247.5776\n",
      "Gradient Descent(11/49): loss=2286547668056.0103\n",
      "Gradient Descent(12/49): loss=42410355234522.18\n",
      "Gradient Descent(13/49): loss=786617478188311.9\n",
      "Gradient Descent(14/49): loss=1.4589999676431488e+16\n",
      "Gradient Descent(15/49): loss=2.7061195257423104e+17\n",
      "Gradient Descent(16/49): loss=5.019248156793774e+18\n",
      "Gradient Descent(17/49): loss=9.309585858990201e+19\n",
      "Gradient Descent(18/49): loss=1.7267205399698837e+21\n",
      "Gradient Descent(19/49): loss=3.202681481651536e+22\n",
      "Gradient Descent(20/49): loss=5.9402598368214385e+23\n",
      "Gradient Descent(21/49): loss=1.1017857108549163e+25\n",
      "Gradient Descent(22/49): loss=2.0435667563228708e+26\n",
      "Gradient Descent(23/49): loss=3.7903605450720546e+27\n",
      "Gradient Descent(24/49): loss=7.0302734261986135e+28\n",
      "Gradient Descent(25/49): loss=1.303958920514128e+30\n",
      "Gradient Descent(26/49): loss=2.4185529684408953e+31\n",
      "Gradient Descent(27/49): loss=4.485876333318848e+32\n",
      "Gradient Descent(28/49): loss=8.320300088694023e+33\n",
      "Gradient Descent(29/49): loss=1.5432300942345548e+35\n",
      "Gradient Descent(30/49): loss=2.8623476297295305e+36\n",
      "Gradient Descent(31/49): loss=5.309016448050838e+37\n",
      "Gradient Descent(32/49): loss=9.847041412065564e+38\n",
      "Gradient Descent(33/49): loss=1.826406557970529e+40\n",
      "Gradient Descent(34/49): loss=3.387576811559309e+41\n",
      "Gradient Descent(35/49): loss=6.283199435598755e+42\n",
      "Gradient Descent(36/49): loss=1.1653933576589911e+44\n",
      "Gradient Descent(37/49): loss=2.161544754382417e+45\n",
      "Gradient Descent(38/49): loss=4.0091834181925214e+46\n",
      "Gradient Descent(39/49): loss=7.436141050571128e+47\n",
      "Gradient Descent(40/49): loss=1.3792383125469155e+49\n",
      "Gradient Descent(41/49): loss=2.558179450685866e+50\n",
      "Gradient Descent(42/49): loss=4.744852316222751e+51\n",
      "Gradient Descent(43/49): loss=8.800642776146298e+52\n",
      "Gradient Descent(44/49): loss=1.6323229494104298e+54\n",
      "Gradient Descent(45/49): loss=3.0275950052124496e+55\n",
      "Gradient Descent(46/49): loss=5.615513473542818e+56\n",
      "Gradient Descent(47/49): loss=1.0415525034639895e+58\n",
      "Gradient Descent(48/49): loss=1.9318475907558977e+59\n",
      "Gradient Descent(49/49): loss=3.583146410284053e+60\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9172044489548203\n",
      "Gradient Descent(2/49): loss=9.706047486159852\n",
      "Gradient Descent(3/49): loss=174.793909270786\n",
      "Gradient Descent(4/49): loss=3295.1516001052587\n",
      "Gradient Descent(5/49): loss=62374.91804030563\n",
      "Gradient Descent(6/49): loss=1181462.4462333606\n",
      "Gradient Descent(7/49): loss=22381585.5148494\n",
      "Gradient Descent(8/49): loss=424010836.93237126\n",
      "Gradient Descent(9/49): loss=8032799891.128966\n",
      "Gradient Descent(10/49): loss=152180126091.60623\n",
      "Gradient Descent(11/49): loss=2883030180244.1196\n",
      "Gradient Descent(12/49): loss=54618592487497.33\n",
      "Gradient Descent(13/49): loss=1034741427066322.5\n",
      "Gradient Descent(14/49): loss=1.9603028621096028e+16\n",
      "Gradient Descent(15/49): loss=3.713765797671062e+17\n",
      "Gradient Descent(16/49): loss=7.035676310972982e+18\n",
      "Gradient Descent(17/49): loss=1.3328988379520216e+20\n",
      "Gradient Descent(18/49): loss=2.5251578295647983e+21\n",
      "Gradient Descent(19/49): loss=4.7838754770644155e+22\n",
      "Gradient Descent(20/49): loss=9.062983831022962e+23\n",
      "Gradient Descent(21/49): loss=1.7169693549768146e+25\n",
      "Gradient Descent(22/49): loss=3.2527739438737846e+26\n",
      "Gradient Descent(23/49): loss=6.162333823417386e+27\n",
      "Gradient Descent(24/49): loss=1.1674453499221682e+29\n",
      "Gradient Descent(25/49): loss=2.2117085573579986e+30\n",
      "Gradient Descent(26/49): loss=4.190050303439682e+31\n",
      "Gradient Descent(27/49): loss=7.937990512785873e+32\n",
      "Gradient Descent(28/49): loss=1.5038409760701607e+34\n",
      "Gradient Descent(29/49): loss=2.8490052711261978e+35\n",
      "Gradient Descent(30/49): loss=5.39739983420037e+36\n",
      "Gradient Descent(31/49): loss=1.0225296971356824e+38\n",
      "Gradient Descent(32/49): loss=1.937167920929632e+39\n",
      "Gradient Descent(33/49): loss=3.669936985097532e+40\n",
      "Gradient Descent(34/49): loss=6.952643252590802e+41\n",
      "Gradient Descent(35/49): loss=1.3171683436006454e+43\n",
      "Gradient Descent(36/49): loss=2.4953566325111905e+44\n",
      "Gradient Descent(37/49): loss=4.727417534493632e+45\n",
      "Gradient Descent(38/49): loss=8.956025064420341e+46\n",
      "Gradient Descent(39/49): loss=1.6967061692619725e+48\n",
      "Gradient Descent(40/49): loss=3.214385627669012e+49\n",
      "Gradient Descent(41/49): loss=6.089607706123588e+50\n",
      "Gradient Descent(42/49): loss=1.153667490772468e+52\n",
      "Gradient Descent(43/49): loss=2.185606599792714e+53\n",
      "Gradient Descent(44/49): loss=4.140600517276458e+54\n",
      "Gradient Descent(45/49): loss=7.84430859848977e+55\n",
      "Gradient Descent(46/49): loss=1.4860930710798272e+57\n",
      "Gradient Descent(47/49): loss=2.8153821183637033e+58\n",
      "Gradient Descent(48/49): loss=5.3337012510546105e+59\n",
      "Gradient Descent(49/49): loss=1.0104620914490818e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9165647431232716\n",
      "Gradient Descent(2/49): loss=9.63973453536626\n",
      "Gradient Descent(3/49): loss=173.63008275904062\n",
      "Gradient Descent(4/49): loss=3278.891696203107\n",
      "Gradient Descent(5/49): loss=62197.59167004703\n",
      "Gradient Descent(6/49): loss=1180701.2441626582\n",
      "Gradient Descent(7/49): loss=22417151.497790337\n",
      "Gradient Descent(8/49): loss=425637388.15578234\n",
      "Gradient Descent(9/49): loss=8081725139.192337\n",
      "Gradient Descent(10/49): loss=153450988329.46716\n",
      "Gradient Descent(11/49): loss=2913638376954.532\n",
      "Gradient Descent(12/49): loss=55322487410254.63\n",
      "Gradient Descent(13/49): loss=1050431586362350.6\n",
      "Gradient Descent(14/49): loss=1.9944991365475676e+16\n",
      "Gradient Descent(15/49): loss=3.787040367631227e+17\n",
      "Gradient Descent(16/49): loss=7.190614675043503e+18\n",
      "Gradient Descent(17/49): loss=1.3653126029689854e+20\n",
      "Gradient Descent(18/49): loss=2.59237713074412e+21\n",
      "Gradient Descent(19/49): loss=4.9222567590134555e+22\n",
      "Gradient Descent(20/49): loss=9.346098341338923e+23\n",
      "Gradient Descent(21/49): loss=1.77458345800715e+25\n",
      "Gradient Descent(22/49): loss=3.3694771169961197e+26\n",
      "Gradient Descent(23/49): loss=6.397769567124974e+27\n",
      "Gradient Descent(24/49): loss=1.2147717290486191e+29\n",
      "Gradient Descent(25/49): loss=2.306538768258452e+30\n",
      "Gradient Descent(26/49): loss=4.3795232982955795e+31\n",
      "Gradient Descent(27/49): loss=8.315587227174935e+32\n",
      "Gradient Descent(28/49): loss=1.578915928125471e+34\n",
      "Gradient Descent(29/49): loss=2.997954852714912e+35\n",
      "Gradient Descent(30/49): loss=5.692344436342024e+36\n",
      "Gradient Descent(31/49): loss=1.0808296580120537e+38\n",
      "Gradient Descent(32/49): loss=2.0522172589913417e+39\n",
      "Gradient Descent(33/49): loss=3.8966322277353475e+40\n",
      "Gradient Descent(34/49): loss=7.398701405370905e+41\n",
      "Gradient Descent(35/49): loss=1.4048229159581725e+43\n",
      "Gradient Descent(36/49): loss=2.6673970431738177e+44\n",
      "Gradient Descent(37/49): loss=5.06470025873655e+45\n",
      "Gradient Descent(38/49): loss=9.616561874989841e+46\n",
      "Gradient Descent(39/49): loss=1.8259375199151174e+48\n",
      "Gradient Descent(40/49): loss=3.4669852593625565e+49\n",
      "Gradient Descent(41/49): loss=6.582912425829341e+50\n",
      "Gradient Descent(42/49): loss=1.2499255913798142e+52\n",
      "Gradient Descent(43/49): loss=2.3732869023992015e+53\n",
      "Gradient Descent(44/49): loss=4.506260820599561e+54\n",
      "Gradient Descent(45/49): loss=8.556229153223134e+55\n",
      "Gradient Descent(46/49): loss=1.6246076345115946e+57\n",
      "Gradient Descent(47/49): loss=3.0847116397287184e+58\n",
      "Gradient Descent(48/49): loss=5.857073239187669e+59\n",
      "Gradient Descent(49/49): loss=1.1121074166992572e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9163574721977199\n",
      "Gradient Descent(2/49): loss=9.583626124264384\n",
      "Gradient Descent(3/49): loss=170.8625557346037\n",
      "Gradient Descent(4/49): loss=3192.231152960864\n",
      "Gradient Descent(5/49): loss=59900.867000482154\n",
      "Gradient Descent(6/49): loss=1124804.3906896508\n",
      "Gradient Descent(7/49): loss=21124729.82339497\n",
      "Gradient Descent(8/49): loss=396755913.00351113\n",
      "Gradient Descent(9/49): loss=7451786944.910703\n",
      "Gradient Descent(10/49): loss=139958319715.44757\n",
      "Gradient Descent(11/49): loss=2628677742155.018\n",
      "Gradient Descent(12/49): loss=49371470842390.55\n",
      "Gradient Descent(13/49): loss=927288358011142.1\n",
      "Gradient Descent(14/49): loss=1.741620609678897e+16\n",
      "Gradient Descent(15/49): loss=3.2710885817456806e+17\n",
      "Gradient Descent(16/49): loss=6.143714917735601e+18\n",
      "Gradient Descent(17/49): loss=1.1539043366620198e+20\n",
      "Gradient Descent(18/49): loss=2.16724772567395e+21\n",
      "Gradient Descent(19/49): loss=4.070495755408863e+22\n",
      "Gradient Descent(20/49): loss=7.645150805146357e+23\n",
      "Gradient Descent(21/49): loss=1.4359020214145886e+25\n",
      "Gradient Descent(22/49): loss=2.6968920138440447e+26\n",
      "Gradient Descent(23/49): loss=5.065266589130683e+27\n",
      "Gradient Descent(24/49): loss=9.513516109380166e+28\n",
      "Gradient Descent(25/49): loss=1.7868158994365997e+30\n",
      "Gradient Descent(26/49): loss=3.355973776437366e+31\n",
      "Gradient Descent(27/49): loss=6.303145159882711e+32\n",
      "Gradient Descent(28/49): loss=1.1838483120904803e+34\n",
      "Gradient Descent(29/49): loss=2.223488100765175e+35\n",
      "Gradient Descent(30/49): loss=4.1761256773802476e+36\n",
      "Gradient Descent(31/49): loss=7.843543514927272e+37\n",
      "Gradient Descent(32/49): loss=1.4731638754021124e+39\n",
      "Gradient Descent(33/49): loss=2.7668767307270995e+40\n",
      "Gradient Descent(34/49): loss=5.19671095040219e+41\n",
      "Gradient Descent(35/49): loss=9.760393154534642e+42\n",
      "Gradient Descent(36/49): loss=1.8331840165886857e+44\n",
      "Gradient Descent(37/49): loss=3.443061755268429e+45\n",
      "Gradient Descent(38/49): loss=6.466712639493869e+46\n",
      "Gradient Descent(39/49): loss=1.2145693378226853e+48\n",
      "Gradient Descent(40/49): loss=2.2811879213091002e+49\n",
      "Gradient Descent(41/49): loss=4.284496710295049e+50\n",
      "Gradient Descent(42/49): loss=8.047084542686278e+51\n",
      "Gradient Descent(43/49): loss=1.5113926796008949e+53\n",
      "Gradient Descent(44/49): loss=2.8386775605921955e+54\n",
      "Gradient Descent(45/49): loss=5.33156631083953e+55\n",
      "Gradient Descent(46/49): loss=1.0013676692800881e+57\n",
      "Gradient Descent(47/49): loss=1.880755392727261e+58\n",
      "Gradient Descent(48/49): loss=3.532409679069933e+59\n",
      "Gradient Descent(49/49): loss=6.634524717588467e+60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9280309140764617\n",
      "Gradient Descent(2/49): loss=9.963610256458484\n",
      "Gradient Descent(3/49): loss=181.47106293631285\n",
      "Gradient Descent(4/49): loss=3460.186367169938\n",
      "Gradient Descent(5/49): loss=66267.26664063512\n",
      "Gradient Descent(6/49): loss=1270065.7097600002\n",
      "Gradient Descent(7/49): loss=24346284.647514865\n",
      "Gradient Descent(8/49): loss=466724314.46678364\n",
      "Gradient Descent(9/49): loss=8947340364.75795\n",
      "Gradient Descent(10/49): loss=171525653769.2416\n",
      "Gradient Descent(11/49): loss=3288248613595.2085\n",
      "Gradient Descent(12/49): loss=63037695541700.43\n",
      "Gradient Descent(13/49): loss=1208470474329627.2\n",
      "Gradient Descent(14/49): loss=2.3167105100289692e+16\n",
      "Gradient Descent(15/49): loss=4.441273272429724e+17\n",
      "Gradient Descent(16/49): loss=8.514187778839456e+18\n",
      "Gradient Descent(17/49): loss=1.6322209672362698e+20\n",
      "Gradient Descent(18/49): loss=3.1290656905939937e+21\n",
      "Gradient Descent(19/49): loss=5.998606985764185e+22\n",
      "Gradient Descent(20/49): loss=1.1499690108184686e+24\n",
      "Gradient Descent(21/49): loss=2.20455970691937e+25\n",
      "Gradient Descent(22/49): loss=4.2262734522878155e+26\n",
      "Gradient Descent(23/49): loss=8.102020207234683e+27\n",
      "Gradient Descent(24/49): loss=1.5532059668999346e+29\n",
      "Gradient Descent(25/49): loss=2.9775891862864134e+30\n",
      "Gradient Descent(26/49): loss=5.708217423337409e+31\n",
      "Gradient Descent(27/49): loss=1.0942995864627857e+33\n",
      "Gradient Descent(28/49): loss=2.0978380746970417e+34\n",
      "Gradient Descent(29/49): loss=4.021681669344426e+35\n",
      "Gradient Descent(30/49): loss=7.70980546335863e+36\n",
      "Gradient Descent(31/49): loss=1.4780160432867843e+38\n",
      "Gradient Descent(32/49): loss=2.8334455838026565e+39\n",
      "Gradient Descent(33/49): loss=5.431885474339945e+40\n",
      "Gradient Descent(34/49): loss=1.0413250910838767e+42\n",
      "Gradient Descent(35/49): loss=1.9962827832864236e+43\n",
      "Gradient Descent(36/49): loss=3.826994072233295e+44\n",
      "Gradient Descent(37/49): loss=7.336577638964418e+45\n",
      "Gradient Descent(38/49): loss=1.4064660262497305e+47\n",
      "Gradient Descent(39/49): loss=2.6962799009838108e+48\n",
      "Gradient Descent(40/49): loss=5.168930616713315e+49\n",
      "Gradient Descent(41/49): loss=9.909150645171295e+50\n",
      "Gradient Descent(42/49): loss=1.8996437327134014e+52\n",
      "Gradient Descent(43/49): loss=3.6417312042741247e+53\n",
      "Gradient Descent(44/49): loss=6.981417586781078e+54\n",
      "Gradient Descent(45/49): loss=1.338379709732883e+56\n",
      "Gradient Descent(46/49): loss=2.5657543402307457e+57\n",
      "Gradient Descent(47/49): loss=4.9187052721583735e+58\n",
      "Gradient Descent(48/49): loss=9.429453621106569e+59\n",
      "Gradient Descent(49/49): loss=1.8076829302192148e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9346490956048046\n",
      "Gradient Descent(2/49): loss=10.343071226044396\n",
      "Gradient Descent(3/49): loss=192.93651281290502\n",
      "Gradient Descent(4/49): loss=3759.6068605744344\n",
      "Gradient Descent(5/49): loss=73551.53517837088\n",
      "Gradient Descent(6/49): loss=1439844.6191632967\n",
      "Gradient Descent(7/49): loss=28190428.54948261\n",
      "Gradient Descent(8/49): loss=551954613.8299568\n",
      "Gradient Descent(9/49): loss=10807097954.855913\n",
      "Gradient Descent(10/49): loss=211600091702.02963\n",
      "Gradient Descent(11/49): loss=4143075865262.125\n",
      "Gradient Descent(12/49): loss=81120382611798.56\n",
      "Gradient Descent(13/49): loss=1588316739771517.5\n",
      "Gradient Descent(14/49): loss=3.109884362036138e+16\n",
      "Gradient Descent(15/49): loss=6.089075642370509e+17\n",
      "Gradient Descent(16/49): loss=1.1922257514860057e+19\n",
      "Gradient Descent(17/49): loss=2.3343481445448152e+20\n",
      "Gradient Descent(18/49): loss=4.5705951690911394e+21\n",
      "Gradient Descent(19/49): loss=8.949110803718319e+22\n",
      "Gradient Descent(20/49): loss=1.7522134692453667e+24\n",
      "Gradient Descent(21/49): loss=3.430790062999165e+25\n",
      "Gradient Descent(22/49): loss=6.717400969098591e+26\n",
      "Gradient Descent(23/49): loss=1.3152502762061323e+28\n",
      "Gradient Descent(24/49): loss=2.575227081155575e+29\n",
      "Gradient Descent(25/49): loss=5.042230090722089e+30\n",
      "Gradient Descent(26/49): loss=9.872560161325639e+31\n",
      "Gradient Descent(27/49): loss=1.9330225393390378e+33\n",
      "Gradient Descent(28/49): loss=3.784809691239228e+34\n",
      "Gradient Descent(29/49): loss=7.410562529600131e+35\n",
      "Gradient Descent(30/49): loss=1.4509695727166945e+37\n",
      "Gradient Descent(31/49): loss=2.8409620626509505e+38\n",
      "Gradient Descent(32/49): loss=5.562532525275639e+39\n",
      "Gradient Descent(33/49): loss=1.0891299289606532e+41\n",
      "Gradient Descent(34/49): loss=2.1324891077361506e+42\n",
      "Gradient Descent(35/49): loss=4.1753602336068003e+43\n",
      "Gradient Descent(36/49): loss=8.175250704512447e+44\n",
      "Gradient Descent(37/49): loss=1.6006936010859537e+46\n",
      "Gradient Descent(38/49): loss=3.1341179581725316e+47\n",
      "Gradient Descent(39/49): loss=6.136524422335141e+48\n",
      "Gradient Descent(40/49): loss=1.2015161040037188e+50\n",
      "Gradient Descent(41/49): loss=2.3525384221170105e+51\n",
      "Gradient Descent(42/49): loss=4.606211276815041e+52\n",
      "Gradient Descent(43/49): loss=9.01884625015603e+53\n",
      "Gradient Descent(44/49): loss=1.7658674949056126e+55\n",
      "Gradient Descent(45/49): loss=3.457524303078417e+56\n",
      "Gradient Descent(46/49): loss=6.769745941224737e+57\n",
      "Gradient Descent(47/49): loss=1.325499290574019e+59\n",
      "Gradient Descent(48/49): loss=2.595294394451634e+60\n",
      "Gradient Descent(49/49): loss=5.081521387276797e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.933935671483682\n",
      "Gradient Descent(2/49): loss=10.272181932753993\n",
      "Gradient Descent(3/49): loss=191.6495182700101\n",
      "Gradient Descent(4/49): loss=3740.9678044735506\n",
      "Gradient Descent(5/49): loss=73340.03561078965\n",
      "Gradient Descent(6/49): loss=1438858.581419079\n",
      "Gradient Descent(7/49): loss=28233876.4157417\n",
      "Gradient Descent(8/49): loss=554041639.4724559\n",
      "Gradient Descent(9/49): loss=10872250432.828962\n",
      "Gradient Descent(10/49): loss=213352557296.3294\n",
      "Gradient Descent(11/49): loss=4186746055938.861\n",
      "Gradient Descent(12/49): loss=82159064005722.05\n",
      "Gradient Descent(13/49): loss=1612257372174820.5\n",
      "Gradient Descent(14/49): loss=3.1638309248449496e+16\n",
      "Gradient Descent(15/49): loss=6.208578317014298e+17\n",
      "Gradient Descent(16/49): loss=1.2183471770893048e+19\n",
      "Gradient Descent(17/49): loss=2.3908369493948465e+20\n",
      "Gradient Descent(18/49): loss=4.6916851177745815e+21\n",
      "Gradient Descent(19/49): loss=9.20677976403598e+22\n",
      "Gradient Descent(20/49): loss=1.8067025278995897e+24\n",
      "Gradient Descent(21/49): loss=3.545402527248642e+25\n",
      "Gradient Descent(22/49): loss=6.957359546532068e+26\n",
      "Gradient Descent(23/49): loss=1.3652850836457683e+28\n",
      "Gradient Descent(24/49): loss=2.6791821626565324e+29\n",
      "Gradient Descent(25/49): loss=5.257522510631457e+30\n",
      "Gradient Descent(26/49): loss=1.031715697987063e+32\n",
      "Gradient Descent(27/49): loss=2.024598619065321e+33\n",
      "Gradient Descent(28/49): loss=3.972993312322962e+34\n",
      "Gradient Descent(29/49): loss=7.796447014791533e+35\n",
      "Gradient Descent(30/49): loss=1.5299443335561952e+37\n",
      "Gradient Descent(31/49): loss=3.0023030482216612e+38\n",
      "Gradient Descent(32/49): loss=5.89160232543189e+39\n",
      "Gradient Descent(33/49): loss=1.1561450461036817e+41\n",
      "Gradient Descent(34/49): loss=2.268773915476541e+42\n",
      "Gradient Descent(35/49): loss=4.4521533841223217e+43\n",
      "Gradient Descent(36/49): loss=8.736732038629928e+44\n",
      "Gradient Descent(37/49): loss=1.7144621968110842e+46\n",
      "Gradient Descent(38/49): loss=3.364393701555291e+47\n",
      "Gradient Descent(39/49): loss=6.602154891556531e+48\n",
      "Gradient Descent(40/49): loss=1.2955811084759059e+50\n",
      "Gradient Descent(41/49): loss=2.5423978022483703e+51\n",
      "Gradient Descent(42/49): loss=4.989102220301086e+52\n",
      "Gradient Descent(43/49): loss=9.790419478258246e+53\n",
      "Gradient Descent(44/49): loss=1.9212337075441468e+55\n",
      "Gradient Descent(45/49): loss=3.770154044166141e+56\n",
      "Gradient Descent(46/49): loss=7.398403151541484e+57\n",
      "Gradient Descent(47/49): loss=1.4518337593509437e+59\n",
      "Gradient Descent(48/49): loss=2.849021905966172e+60\n",
      "Gradient Descent(49/49): loss=5.590809394254556e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9337937829462574\n",
      "Gradient Descent(2/49): loss=10.212931978456963\n",
      "Gradient Descent(3/49): loss=188.61235383271688\n",
      "Gradient Descent(4/49): loss=3642.5732697044336\n",
      "Gradient Descent(5/49): loss=70643.58859629923\n",
      "Gradient Descent(6/49): loss=1371013.7207407686\n",
      "Gradient Descent(7/49): loss=26612302.643495154\n",
      "Gradient Descent(8/49): loss=516584917.90520316\n",
      "Gradient Descent(9/49): loss=10027806427.210299\n",
      "Gradient Descent(10/49): loss=194657653750.21658\n",
      "Gradient Descent(11/49): loss=3778656211752.6978\n",
      "Gradient Descent(12/49): loss=73350549571170.58\n",
      "Gradient Descent(13/49): loss=1423866881488623.8\n",
      "Gradient Descent(14/49): loss=2.7639832826398612e+16\n",
      "Gradient Descent(15/49): loss=5.3653917497299206e+17\n",
      "Gradient Descent(16/49): loss=1.0415196372962986e+19\n",
      "Gradient Descent(17/49): loss=2.021778102695402e+20\n",
      "Gradient Descent(18/49): loss=3.924637184448365e+21\n",
      "Gradient Descent(19/49): loss=7.618431028286642e+22\n",
      "Gradient Descent(20/49): loss=1.4788753356127554e+24\n",
      "Gradient Descent(21/49): loss=2.870764662916393e+25\n",
      "Gradient Descent(22/49): loss=5.5726737415889855e+26\n",
      "Gradient Descent(23/49): loss=1.0817568235861796e+28\n",
      "Gradient Descent(24/49): loss=2.099885763349252e+29\n",
      "Gradient Descent(25/49): loss=4.076258289269415e+30\n",
      "Gradient Descent(26/49): loss=7.912755032128926e+31\n",
      "Gradient Descent(27/49): loss=1.5360089512312013e+33\n",
      "Gradient Descent(28/49): loss=2.9816713504747446e+34\n",
      "Gradient Descent(29/49): loss=5.787963693255643e+35\n",
      "Gradient Descent(30/49): loss=1.1235484993713814e+37\n",
      "Gradient Descent(31/49): loss=2.1810109692129565e+38\n",
      "Gradient Descent(32/49): loss=4.2337369953221054e+39\n",
      "Gradient Descent(33/49): loss=8.218449699969745e+40\n",
      "Gradient Descent(34/49): loss=1.5953498185069535e+42\n",
      "Gradient Descent(35/49): loss=3.0968627129512534e+43\n",
      "Gradient Descent(36/49): loss=6.011570974348032e+44\n",
      "Gradient Descent(37/49): loss=1.1669547193192712e+46\n",
      "Gradient Descent(38/49): loss=2.2652702974852733e+47\n",
      "Gradient Descent(39/49): loss=4.3972996001613704e+48\n",
      "Gradient Descent(40/49): loss=8.535954316376678e+49\n",
      "Gradient Descent(41/49): loss=1.656983210527552e+51\n",
      "Gradient Descent(42/49): loss=3.216504280842563e+52\n",
      "Gradient Descent(43/49): loss=6.243816909517496e+53\n",
      "Gradient Descent(44/49): loss=1.2120378583598331e+55\n",
      "Gradient Descent(45/49): loss=2.3527848291935497e+56\n",
      "Gradient Descent(46/49): loss=4.567181143973728e+57\n",
      "Gradient Descent(47/49): loss=8.865725136887615e+58\n",
      "Gradient Descent(48/49): loss=1.72099769474992e+60\n",
      "Gradient Descent(49/49): loss=3.340767979610903e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9455395690604895\n",
      "Gradient Descent(2/49): loss=10.608578458327537\n",
      "Gradient Descent(3/49): loss=200.0270964682506\n",
      "Gradient Descent(4/49): loss=3940.4843154507525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=77957.4677862094\n",
      "Gradient Descent(6/49): loss=1543453.2757390044\n",
      "Gradient Descent(7/49): loss=30563994.082206555\n",
      "Gradient Descent(8/49): loss=605269158.0597218\n",
      "Gradient Descent(9/49): loss=11986516349.24289\n",
      "Gradient Descent(10/49): loss=237377239941.58536\n",
      "Gradient Descent(11/49): loss=4700949949273.941\n",
      "Gradient Descent(12/49): loss=93096275225530.77\n",
      "Gradient Descent(13/49): loss=1843652294120969.0\n",
      "Gradient Descent(14/49): loss=3.651116920635718e+16\n",
      "Gradient Descent(15/49): loss=7.23056885109748e+17\n",
      "Gradient Descent(16/49): loss=1.4319214403633529e+19\n",
      "Gradient Descent(17/49): loss=2.835736792720281e+20\n",
      "Gradient Descent(18/49): loss=5.615813083007595e+21\n",
      "Gradient Descent(19/49): loss=1.1121397678857696e+23\n",
      "Gradient Descent(20/49): loss=2.2024501974037877e+24\n",
      "Gradient Descent(21/49): loss=4.361670189421293e+25\n",
      "Gradient Descent(22/49): loss=8.637728500615457e+26\n",
      "Gradient Descent(23/49): loss=1.7105913654663371e+28\n",
      "Gradient Descent(24/49): loss=3.38760684524836e+29\n",
      "Gradient Descent(25/49): loss=6.708720954431541e+30\n",
      "Gradient Descent(26/49): loss=1.3285761571641146e+32\n",
      "Gradient Descent(27/49): loss=2.6310747121163127e+33\n",
      "Gradient Descent(28/49): loss=5.210506077057938e+34\n",
      "Gradient Descent(29/49): loss=1.0318739127411597e+36\n",
      "Gradient Descent(30/49): loss=2.0434939640200095e+37\n",
      "Gradient Descent(31/49): loss=4.046877752624942e+38\n",
      "Gradient Descent(32/49): loss=8.014322446283622e+39\n",
      "Gradient Descent(33/49): loss=1.5871337905214423e+41\n",
      "Gradient Descent(34/49): loss=3.143114949390456e+42\n",
      "Gradient Descent(35/49): loss=6.224536106584987e+43\n",
      "Gradient Descent(36/49): loss=1.2326895568898658e+45\n",
      "Gradient Descent(37/49): loss=2.441183596088079e+46\n",
      "Gradient Descent(38/49): loss=4.834451072048771e+47\n",
      "Gradient Descent(39/49): loss=9.574010412607367e+48\n",
      "Gradient Descent(40/49): loss=1.8960099919238517e+50\n",
      "Gradient Descent(41/49): loss=3.754804658183001e+51\n",
      "Gradient Descent(42/49): loss=7.435909136115505e+52\n",
      "Gradient Descent(43/49): loss=1.4725864516031137e+54\n",
      "Gradient Descent(44/49): loss=2.916268633397933e+55\n",
      "Gradient Descent(45/49): loss=5.77529606691826e+56\n",
      "Gradient Descent(46/49): loss=1.1437233277682818e+58\n",
      "Gradient Descent(47/49): loss=2.2649973877085864e+59\n",
      "Gradient Descent(48/49): loss=4.4855368792181785e+60\n",
      "Gradient Descent(49/49): loss=8.883030595978273e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9524085888865524\n",
      "Gradient Descent(2/49): loss=11.012259510950946\n",
      "Gradient Descent(3/49): loss=212.62555981783854\n",
      "Gradient Descent(4/49): loss=4280.373826475654\n",
      "Gradient Descent(5/49): loss=86499.3623211879\n",
      "Gradient Descent(6/49): loss=1749115.8440278247\n",
      "Gradient Descent(7/49): loss=35374280.69967436\n",
      "Gradient Descent(8/49): loss=715439248.6029768\n",
      "Gradient Descent(9/49): loss=14469785122.11481\n",
      "Gradient Descent(10/49): loss=292652669606.39056\n",
      "Gradient Descent(11/49): loss=5918929705096.063\n",
      "Gradient Descent(12/49): loss=119710969565410.31\n",
      "Gradient Descent(13/49): loss=2421166931420371.0\n",
      "Gradient Descent(14/49): loss=4.896835602506408e+16\n",
      "Gradient Descent(15/49): loss=9.903901577099035e+17\n",
      "Gradient Descent(16/49): loss=2.00307452584407e+19\n",
      "Gradient Descent(17/49): loss=4.051239327932275e+20\n",
      "Gradient Descent(18/49): loss=8.193674214967348e+21\n",
      "Gradient Descent(19/49): loss=1.657179240907172e+23\n",
      "Gradient Descent(20/49): loss=3.3516624708939153e+24\n",
      "Gradient Descent(21/49): loss=6.778772652655145e+25\n",
      "Gradient Descent(22/49): loss=1.371013909528347e+27\n",
      "Gradient Descent(23/49): loss=2.7728900738160696e+28\n",
      "Gradient Descent(24/49): loss=5.60819938297557e+29\n",
      "Gradient Descent(25/49): loss=1.1342642326936273e+31\n",
      "Gradient Descent(26/49): loss=2.294061358577182e+32\n",
      "Gradient Descent(27/49): loss=4.6397632625858564e+33\n",
      "Gradient Descent(28/49): loss=9.383970072270802e+34\n",
      "Gradient Descent(29/49): loss=1.8979178318722257e+36\n",
      "Gradient Descent(30/49): loss=3.8385588069836273e+37\n",
      "Gradient Descent(31/49): loss=7.763525621199578e+38\n",
      "Gradient Descent(32/49): loss=1.5701812347219153e+40\n",
      "Gradient Descent(33/49): loss=3.1757080869811055e+41\n",
      "Gradient Descent(34/49): loss=6.422903057750001e+42\n",
      "Gradient Descent(35/49): loss=1.2990389090979484e+44\n",
      "Gradient Descent(36/49): loss=2.6273198772854186e+45\n",
      "Gradient Descent(37/49): loss=5.313782127105304e+46\n",
      "Gradient Descent(38/49): loss=1.0747180325647277e+48\n",
      "Gradient Descent(39/49): loss=2.173628541577031e+49\n",
      "Gradient Descent(40/49): loss=4.396186621604612e+50\n",
      "Gradient Descent(41/49): loss=8.891333750132547e+51\n",
      "Gradient Descent(42/49): loss=1.7982816167933984e+53\n",
      "Gradient Descent(43/49): loss=3.637043512452664e+54\n",
      "Gradient Descent(44/49): loss=7.355958815316987e+55\n",
      "Gradient Descent(45/49): loss=1.4877504189151243e+57\n",
      "Gradient Descent(46/49): loss=3.0089908937136474e+58\n",
      "Gradient Descent(47/49): loss=6.085715778224294e+59\n",
      "Gradient Descent(48/49): loss=1.2308424266322403e+61\n",
      "Gradient Descent(49/49): loss=2.4893917731399275e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9516197111794247\n",
      "Gradient Descent(2/49): loss=10.936568549349332\n",
      "Gradient Descent(3/49): loss=211.2047646094122\n",
      "Gradient Descent(4/49): loss=4259.05712724687\n",
      "Gradient Descent(5/49): loss=86247.90339399337\n",
      "Gradient Descent(6/49): loss=1747849.4205369297\n",
      "Gradient Descent(7/49): loss=35427162.733698726\n",
      "Gradient Descent(8/49): loss=718106387.4677856\n",
      "Gradient Descent(9/49): loss=14556150408.802502\n",
      "Gradient Descent(10/49): loss=295056848379.4784\n",
      "Gradient Descent(11/49): loss=5980882119372.284\n",
      "Gradient Descent(12/49): loss=121234126394288.38\n",
      "Gradient Descent(13/49): loss=2457449256117182.0\n",
      "Gradient Descent(14/49): loss=4.981317658693302e+16\n",
      "Gradient Descent(15/49): loss=1.0097268810223444e+18\n",
      "Gradient Descent(16/49): loss=2.0467443462120526e+19\n",
      "Gradient Descent(17/49): loss=4.148807463487482e+20\n",
      "Gradient Descent(18/49): loss=8.409747608386193e+21\n",
      "Gradient Descent(19/49): loss=1.704679126731316e+23\n",
      "Gradient Descent(20/49): loss=3.455431792308542e+24\n",
      "Gradient Descent(21/49): loss=7.004255923641366e+25\n",
      "Gradient Descent(22/49): loss=1.419782070451752e+27\n",
      "Gradient Descent(23/49): loss=2.877937570460059e+28\n",
      "Gradient Descent(24/49): loss=5.8336591451886706e+29\n",
      "Gradient Descent(25/49): loss=1.1824988620862835e+31\n",
      "Gradient Descent(26/49): loss=2.396957936749877e+32\n",
      "Gradient Descent(27/49): loss=4.8587001093697635e+33\n",
      "Gradient Descent(28/49): loss=9.848719658718475e+34\n",
      "Gradient Descent(29/49): loss=1.9963627458499194e+36\n",
      "Gradient Descent(30/49): loss=4.0466825649660445e+37\n",
      "Gradient Descent(31/49): loss=8.202737611509784e+38\n",
      "Gradient Descent(32/49): loss=1.662717627169321e+40\n",
      "Gradient Descent(33/49): loss=3.3703746707932582e+41\n",
      "Gradient Descent(34/49): loss=6.831842783108899e+42\n",
      "Gradient Descent(35/49): loss=1.3848334494553926e+44\n",
      "Gradient Descent(36/49): loss=2.8070957479759695e+45\n",
      "Gradient Descent(37/49): loss=5.690060809408991e+46\n",
      "Gradient Descent(38/49): loss=1.1533910818010777e+48\n",
      "Gradient Descent(39/49): loss=2.337955660119611e+49\n",
      "Gradient Descent(40/49): loss=4.739100860871784e+50\n",
      "Gradient Descent(41/49): loss=9.6062886703192e+51\n",
      "Gradient Descent(42/49): loss=1.94722131321188e+53\n",
      "Gradient Descent(43/49): loss=3.947071520286318e+54\n",
      "Gradient Descent(44/49): loss=8.000823265721959e+55\n",
      "Gradient Descent(45/49): loss=1.6217890301788755e+57\n",
      "Gradient Descent(46/49): loss=3.2874112713839616e+58\n",
      "Gradient Descent(47/49): loss=6.66367367525621e+59\n",
      "Gradient Descent(48/49): loss=1.3507451056347118e+61\n",
      "Gradient Descent(49/49): loss=2.7379977311478874e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9515450232527023\n",
      "Gradient Descent(2/49): loss=10.874046651482086\n",
      "Gradient Descent(3/49): loss=207.87648566865647\n",
      "Gradient Descent(4/49): loss=4147.5633262869405\n",
      "Gradient Descent(5/49): loss=83090.22914240544\n",
      "Gradient Descent(6/49): loss=1665756.9002250896\n",
      "Gradient Descent(7/49): loss=33399984.67108621\n",
      "Gradient Descent(8/49): loss=669730476.9930636\n",
      "Gradient Descent(9/49): loss=13429473372.224216\n",
      "Gradient Descent(10/49): loss=269289413072.2635\n",
      "Gradient Descent(11/49): loss=5399828262809.15\n",
      "Gradient Descent(12/49): loss=108278122152347.88\n",
      "Gradient Descent(13/49): loss=2171208396024541.0\n",
      "Gradient Descent(14/49): loss=4.35373821013282e+16\n",
      "Gradient Descent(15/49): loss=8.730178329222848e+17\n",
      "Gradient Descent(16/49): loss=1.7505878873112373e+19\n",
      "Gradient Descent(17/49): loss=3.5103039555744557e+20\n",
      "Gradient Descent(18/49): loss=7.038911871212882e+21\n",
      "Gradient Descent(19/49): loss=1.4114527106173868e+23\n",
      "Gradient Descent(20/49): loss=2.8302652324278377e+24\n",
      "Gradient Descent(21/49): loss=5.675288463897306e+25\n",
      "Gradient Descent(22/49): loss=1.1380169879284136e+27\n",
      "Gradient Descent(23/49): loss=2.281967996961468e+28\n",
      "Gradient Descent(24/49): loss=4.575834978206909e+29\n",
      "Gradient Descent(25/49): loss=9.175529970473814e+30\n",
      "Gradient Descent(26/49): loss=1.8398904383578578e+32\n",
      "Gradient Descent(27/49): loss=3.689374713018197e+33\n",
      "Gradient Descent(28/49): loss=7.39798712428041e+34\n",
      "Gradient Descent(29/49): loss=1.4834549957178297e+36\n",
      "Gradient Descent(30/49): loss=2.9746452478913105e+37\n",
      "Gradient Descent(31/49): loss=5.964801343043564e+38\n",
      "Gradient Descent(32/49): loss=1.196070525962572e+40\n",
      "Gradient Descent(33/49): loss=2.3983777846093625e+41\n",
      "Gradient Descent(34/49): loss=4.809261555106459e+42\n",
      "Gradient Descent(35/49): loss=9.643600292600318e+43\n",
      "Gradient Descent(36/49): loss=1.9337485711230413e+45\n",
      "Gradient Descent(37/49): loss=3.877580387886546e+46\n",
      "Gradient Descent(38/49): loss=7.775379844645624e+47\n",
      "Gradient Descent(39/49): loss=1.5591303256377532e+49\n",
      "Gradient Descent(40/49): loss=3.1263905055355978e+50\n",
      "Gradient Descent(41/49): loss=6.269083111512852e+51\n",
      "Gradient Descent(42/49): loss=1.2570855428798348e+53\n",
      "Gradient Descent(43/49): loss=2.5207259722165914e+54\n",
      "Gradient Descent(44/49): loss=5.05459589683201e+55\n",
      "Gradient Descent(45/49): loss=1.0135548235655623e+57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=2.03239467870592e+58\n",
      "Gradient Descent(47/49): loss=4.075386978576126e+59\n",
      "Gradient Descent(48/49): loss=8.172024459207462e+60\n",
      "Gradient Descent(49/49): loss=1.6386660730122335e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9633587918374032\n",
      "Gradient Descent(2/49): loss=11.285644381756233\n",
      "Gradient Descent(3/49): loss=220.1416243976744\n",
      "Gradient Descent(4/49): loss=4478.143648934178\n",
      "Gradient Descent(5/49): loss=91472.06673020858\n",
      "Gradient Descent(6/49): loss=1869852.1555510822\n",
      "Gradient Descent(7/49): loss=38230362.83993449\n",
      "Gradient Descent(8/49): loss=781685359.9545833\n",
      "Gradient Descent(9/49): loss=15983126423.651138\n",
      "Gradient Descent(10/49): loss=326808400549.6348\n",
      "Gradient Descent(11/49): loss=6682287642468.102\n",
      "Gradient Descent(12/49): loss=136633518942103.78\n",
      "Gradient Descent(13/49): loss=2793761819749157.0\n",
      "Gradient Descent(14/49): loss=5.712438170447326e+16\n",
      "Gradient Descent(15/49): loss=1.168029065128641e+18\n",
      "Gradient Descent(16/49): loss=2.38828300414457e+19\n",
      "Gradient Descent(17/49): loss=4.883350835233409e+20\n",
      "Gradient Descent(18/49): loss=9.985045885242514e+21\n",
      "Gradient Descent(19/49): loss=2.0416542799863966e+23\n",
      "Gradient Descent(20/49): loss=4.1745949362062387e+24\n",
      "Gradient Descent(21/49): loss=8.535844218232374e+25\n",
      "Gradient Descent(22/49): loss=1.7453342810837087e+27\n",
      "Gradient Descent(23/49): loss=3.5687058887744086e+28\n",
      "Gradient Descent(24/49): loss=7.29697563303813e+29\n",
      "Gradient Descent(25/49): loss=1.4920213390697598e+31\n",
      "Gradient Descent(26/49): loss=3.0507538851581547e+32\n",
      "Gradient Descent(27/49): loss=6.237912973557275e+33\n",
      "Gradient Descent(28/49): loss=1.2754735298372661e+35\n",
      "Gradient Descent(29/49): loss=2.6079759884623718e+36\n",
      "Gradient Descent(30/49): loss=5.332559710011464e+37\n",
      "Gradient Descent(31/49): loss=1.090354864716497e+39\n",
      "Gradient Descent(32/49): loss=2.2294616388052983e+40\n",
      "Gradient Descent(33/49): loss=4.558606890057551e+41\n",
      "Gradient Descent(34/49): loss=9.321038055275112e+42\n",
      "Gradient Descent(35/49): loss=1.905883804488136e+44\n",
      "Gradient Descent(36/49): loss=3.896983420376068e+45\n",
      "Gradient Descent(37/49): loss=7.968208629992849e+46\n",
      "Gradient Descent(38/49): loss=1.6292691531380858e+48\n",
      "Gradient Descent(39/49): loss=3.3313861328573994e+49\n",
      "Gradient Descent(40/49): loss=6.811725088404724e+50\n",
      "Gradient Descent(41/49): loss=1.3928015795696475e+52\n",
      "Gradient Descent(42/49): loss=2.847878055668905e+53\n",
      "Gradient Descent(43/49): loss=5.8230903374380836e+54\n",
      "Gradient Descent(44/49): loss=1.1906542490633545e+56\n",
      "Gradient Descent(45/49): loss=2.434544990137219e+57\n",
      "Gradient Descent(46/49): loss=4.977943272503119e+58\n",
      "Gradient Descent(47/49): loss=1.017846017413811e+60\n",
      "Gradient Descent(48/49): loss=2.0812019311023765e+61\n",
      "Gradient Descent(49/49): loss=4.2554584916780125e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9704829288000638\n",
      "Gradient Descent(2/49): loss=11.714677502448595\n",
      "Gradient Descent(3/49): loss=233.96477148398782\n",
      "Gradient Descent(4/49): loss=4863.206392380607\n",
      "Gradient Descent(5/49): loss=101463.65582655686\n",
      "Gradient Descent(6/49): loss=2118230.442758274\n",
      "Gradient Descent(7/49): loss=44228328.05101673\n",
      "Gradient Descent(8/49): loss=923515948.767111\n",
      "Gradient Descent(9/49): loss=19283799194.864567\n",
      "Gradient Descent(10/49): loss=402663193530.61194\n",
      "Gradient Descent(11/49): loss=8407977944161.975\n",
      "Gradient Descent(12/49): loss=175566346579744.38\n",
      "Gradient Descent(13/49): loss=3665987673409074.0\n",
      "Gradient Descent(14/49): loss=7.65492137261098e+16\n",
      "Gradient Descent(15/49): loss=1.5984183971914854e+18\n",
      "Gradient Descent(16/49): loss=3.337645483585333e+19\n",
      "Gradient Descent(17/49): loss=6.969312538038867e+20\n",
      "Gradient Descent(18/49): loss=1.4552569317076607e+22\n",
      "Gradient Descent(19/49): loss=3.03871110063832e+23\n",
      "Gradient Descent(20/49): loss=6.345109892274707e+24\n",
      "Gradient Descent(21/49): loss=1.324917644741791e+26\n",
      "Gradient Descent(22/49): loss=2.7665506116543063e+27\n",
      "Gradient Descent(23/49): loss=5.776813613450641e+28\n",
      "Gradient Descent(24/49): loss=1.2062521243590886e+30\n",
      "Gradient Descent(25/49): loss=2.51876602723155e+31\n",
      "Gradient Descent(26/49): loss=5.259416478380595e+32\n",
      "Gradient Descent(27/49): loss=1.098214816064705e+34\n",
      "Gradient Descent(28/49): loss=2.2931741328752736e+35\n",
      "Gradient Descent(29/49): loss=4.78835973323678e+36\n",
      "Gradient Descent(30/49): loss=9.998538098864288e+37\n",
      "Gradient Descent(31/49): loss=2.0877872525016687e+39\n",
      "Gradient Descent(32/49): loss=4.3594929264745095e+40\n",
      "Gradient Descent(33/49): loss=9.103024531454785e+41\n",
      "Gradient Descent(34/49): loss=1.9007957351425206e+43\n",
      "Gradient Descent(35/49): loss=3.969037339459533e+44\n",
      "Gradient Descent(36/49): loss=8.28771714433731e+45\n",
      "Gradient Descent(37/49): loss=1.7305520102236182e+47\n",
      "Gradient Descent(38/49): loss=3.613552692414509e+48\n",
      "Gradient Descent(39/49): loss=7.545432315073172e+49\n",
      "Gradient Descent(40/49): loss=1.5755560709233405e+51\n",
      "Gradient Descent(41/49): loss=3.2899068323288044e+52\n",
      "Gradient Descent(42/49): loss=6.86962981841755e+53\n",
      "Gradient Descent(43/49): loss=1.4344422577063245e+55\n",
      "Gradient Descent(44/49): loss=2.995248135754104e+56\n",
      "Gradient Descent(45/49): loss=6.2543552008039464e+57\n",
      "Gradient Descent(46/49): loss=1.3059672255826324e+59\n",
      "Gradient Descent(47/49): loss=2.7269803833283423e+60\n",
      "Gradient Descent(48/49): loss=5.694187316025467e+61\n",
      "Gradient Descent(49/49): loss=1.1889989890727153e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9696168622104996\n",
      "Gradient Descent(2/49): loss=11.633952313598703\n",
      "Gradient Descent(3/49): loss=232.3988229057665\n",
      "Gradient Descent(4/49): loss=4838.882085991552\n",
      "Gradient Descent(5/49): loss=101165.60019738215\n",
      "Gradient Descent(6/49): loss=2116616.3654624363\n",
      "Gradient Descent(7/49): loss=44292463.71074744\n",
      "Gradient Descent(8/49): loss=926911238.3188165\n",
      "Gradient Descent(9/49): loss=19397777195.778507\n",
      "Gradient Descent(10/49): loss=405945061238.8597\n",
      "Gradient Descent(11/49): loss=8495382813407.332\n",
      "Gradient Descent(12/49): loss=177786488100302.47\n",
      "Gradient Descent(13/49): loss=3720613669713483.5\n",
      "Gradient Descent(14/49): loss=7.786287063117099e+16\n",
      "Gradient Descent(15/49): loss=1.6294695417869066e+18\n",
      "Gradient Descent(16/49): loss=3.4100604941113946e+19\n",
      "Gradient Descent(17/49): loss=7.136379219889458e+20\n",
      "Gradient Descent(18/49): loss=1.4934605548100006e+22\n",
      "Gradient Descent(19/49): loss=3.1254286804317e+23\n",
      "Gradient Descent(20/49): loss=6.540718069230722e+24\n",
      "Gradient Descent(21/49): loss=1.368804002122603e+26\n",
      "Gradient Descent(22/49): loss=2.864554589261016e+27\n",
      "Gradient Descent(23/49): loss=5.994775718169235e+28\n",
      "Gradient Descent(24/49): loss=1.2545523149001471e+30\n",
      "Gradient Descent(25/49): loss=2.6254552043558286e+31\n",
      "Gradient Descent(26/49): loss=5.4944022247711045e+32\n",
      "Gradient Descent(27/49): loss=1.1498370171193455e+34\n",
      "Gradient Descent(28/49): loss=2.406313028880942e+35\n",
      "Gradient Descent(29/49): loss=5.035794035809144e+36\n",
      "Gradient Descent(30/49): loss=1.0538621229543136e+38\n",
      "Gradient Descent(31/49): loss=2.2054622693068972e+39\n",
      "Gradient Descent(32/49): loss=4.615465074027691e+40\n",
      "Gradient Descent(33/49): loss=9.658980861306804e+41\n",
      "Gradient Descent(34/49): loss=2.0213761729904172e+43\n",
      "Gradient Descent(35/49): loss=4.230220238971042e+44\n",
      "Gradient Descent(36/49): loss=8.852762543315625e+45\n",
      "Gradient Descent(37/49): loss=1.8526554226735667e+47\n",
      "Gradient Descent(38/49): loss=3.877131119656451e+48\n",
      "Gradient Descent(39/49): loss=8.113837864849932e+49\n",
      "Gradient Descent(40/49): loss=1.6980174996739906e+51\n",
      "Gradient Descent(41/49): loss=3.5535137344680544e+52\n",
      "Gradient Descent(42/49): loss=7.436589942964352e+53\n",
      "Gradient Descent(43/49): loss=1.5562869349111153e+55\n",
      "Gradient Descent(44/49): loss=3.256908129063208e+56\n",
      "Gradient Descent(45/49): loss=6.815870726154916e+57\n",
      "Gradient Descent(46/49): loss=1.4263863736622469e+59\n",
      "Gradient Descent(47/49): loss=2.985059677205924e+60\n",
      "Gradient Descent(48/49): loss=6.246961861815078e+61\n",
      "Gradient Descent(49/49): loss=1.3073283861279327e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9696111931170551\n",
      "Gradient Descent(2/49): loss=11.56802465850232\n",
      "Gradient Descent(3/49): loss=228.75661067611458\n",
      "Gradient Descent(4/49): loss=4712.789997851841\n",
      "Gradient Descent(5/49): loss=97476.92797522554\n",
      "Gradient Descent(6/49): loss=2017581.156817479\n",
      "Gradient Descent(7/49): loss=41767124.980373204\n",
      "Gradient Descent(8/49): loss=864684812.8536674\n",
      "Gradient Descent(9/49): loss=17901374551.86442\n",
      "Gradient Descent(10/49): loss=370609339689.6555\n",
      "Gradient Descent(11/49): loss=7672673669292.008\n",
      "Gradient Descent(12/49): loss=158846335307704.88\n",
      "Gradient Descent(13/49): loss=3288574618903373.5\n",
      "Gradient Descent(14/49): loss=6.808292551524007e+16\n",
      "Gradient Descent(15/49): loss=1.4095118116590733e+18\n",
      "Gradient Descent(16/49): loss=2.9180936846357074e+19\n",
      "Gradient Descent(17/49): loss=6.041290810658039e+20\n",
      "Gradient Descent(18/49): loss=1.2507204568023478e+22\n",
      "Gradient Descent(19/49): loss=2.5893500414583624e+23\n",
      "Gradient Descent(20/49): loss=5.360697189193264e+24\n",
      "Gradient Descent(21/49): loss=1.1098180583608335e+26\n",
      "Gradient Descent(22/49): loss=2.2976416671089567e+27\n",
      "Gradient Descent(23/49): loss=4.756777194843189e+28\n",
      "Gradient Descent(24/49): loss=9.847892996235465e+29\n",
      "Gradient Descent(25/49): loss=2.0387962793473054e+31\n",
      "Gradient Descent(26/49): loss=4.220893007539201e+32\n",
      "Gradient Descent(27/49): loss=8.738459041526645e+33\n",
      "Gradient Descent(28/49): loss=1.8091116331081177e+35\n",
      "Gradient Descent(29/49): loss=3.74537991823708e+36\n",
      "Gradient Descent(30/49): loss=7.754010573594745e+37\n",
      "Gradient Descent(31/49): loss=1.6053025671083055e+39\n",
      "Gradient Descent(32/49): loss=3.323436700924998e+40\n",
      "Gradient Descent(33/49): loss=6.8804671040621575e+41\n",
      "Gradient Descent(34/49): loss=1.4244540164374157e+43\n",
      "Gradient Descent(35/49): loss=2.949028335222693e+44\n",
      "Gradient Descent(36/49): loss=6.105334409949631e+45\n",
      "Gradient Descent(37/49): loss=1.2639793186151407e+47\n",
      "Gradient Descent(38/49): loss=2.616799687963323e+48\n",
      "Gradient Descent(39/49): loss=5.417525829795579e+49\n",
      "Gradient Descent(40/49): loss=1.1215832167629658e+51\n",
      "Gradient Descent(41/49): loss=2.3219989191483496e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=4.807203692015976e+53\n",
      "Gradient Descent(43/49): loss=9.952290307270227e+54\n",
      "Gradient Descent(44/49): loss=2.060409516756875e+56\n",
      "Gradient Descent(45/49): loss=4.2656386074681714e+57\n",
      "Gradient Descent(46/49): loss=8.83109526603395e+58\n",
      "Gradient Descent(47/49): loss=1.8282899883086125e+60\n",
      "Gradient Descent(48/49): loss=3.785084613689921e+61\n",
      "Gradient Descent(49/49): loss=7.836210680148338e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9814885824072036\n",
      "Gradient Descent(2/49): loss=11.995855975750851\n",
      "Gradient Descent(3/49): loss=241.91766943005487\n",
      "Gradient Descent(4/49): loss=5078.939300341315\n",
      "Gradient Descent(5/49): loss=107059.90300268018\n",
      "Gradient Descent(6/49): loss=2258448.1932511353\n",
      "Gradient Descent(7/49): loss=47651582.608126186\n",
      "Gradient Descent(8/49): loss=1005466305.7023726\n",
      "Gradient Descent(9/49): loss=21216029477.33314\n",
      "Gradient Descent(10/49): loss=447674629927.4683\n",
      "Gradient Descent(11/49): loss=9446291748462.965\n",
      "Gradient Descent(12/49): loss=199324353929092.75\n",
      "Gradient Descent(13/49): loss=4205904568808685.5\n",
      "Gradient Descent(14/49): loss=8.874797953739214e+16\n",
      "Gradient Descent(15/49): loss=1.8726539791989455e+18\n",
      "Gradient Descent(16/49): loss=3.951451008710886e+19\n",
      "Gradient Descent(17/49): loss=8.337880492697851e+20\n",
      "Gradient Descent(18/49): loss=1.759360067170677e+22\n",
      "Gradient Descent(19/49): loss=3.7123917148931964e+23\n",
      "Gradient Descent(20/49): loss=7.833446093348194e+24\n",
      "Gradient Descent(21/49): loss=1.6529203384282547e+26\n",
      "Gradient Descent(22/49): loss=3.487795298051916e+27\n",
      "Gradient Descent(23/49): loss=7.359529529828084e+28\n",
      "Gradient Descent(24/49): loss=1.5529201192130584e+30\n",
      "Gradient Descent(25/49): loss=3.2767867658968283e+31\n",
      "Gradient Descent(26/49): loss=6.914284499448578e+32\n",
      "Gradient Descent(27/49): loss=1.4589698248561565e+34\n",
      "Gradient Descent(28/49): loss=3.078544063395965e+35\n",
      "Gradient Descent(29/49): loss=6.495976399789455e+36\n",
      "Gradient Descent(30/49): loss=1.3707034402513228e+38\n",
      "Gradient Descent(31/49): loss=2.8922948691404e+39\n",
      "Gradient Descent(32/49): loss=6.102975570355384e+40\n",
      "Gradient Descent(33/49): loss=1.2877770938833863e+42\n",
      "Gradient Descent(34/49): loss=2.7173135864841306e+43\n",
      "Gradient Descent(35/49): loss=5.733750943670599e+44\n",
      "Gradient Descent(36/49): loss=1.2098677181598583e+46\n",
      "Gradient Descent(37/49): loss=2.552918516736754e+47\n",
      "Gradient Descent(38/49): loss=5.3868640804880566e+48\n",
      "Gradient Descent(39/49): loss=1.1366717907920058e+50\n",
      "Gradient Descent(40/49): loss=2.3984692033760204e+51\n",
      "Gradient Descent(41/49): loss=5.060963565863516e+52\n",
      "Gradient Descent(42/49): loss=1.0679041523212085e+54\n",
      "Gradient Descent(43/49): loss=2.2533639369330256e+55\n",
      "Gradient Descent(44/49): loss=4.754779744262145e+56\n",
      "Gradient Descent(45/49): loss=1.003296895184021e+58\n",
      "Gradient Descent(46/49): loss=2.117037410829848e+59\n",
      "Gradient Descent(47/49): loss=4.467119773186495e+60\n",
      "Gradient Descent(48/49): loss=9.425983199877314e+61\n",
      "Gradient Descent(49/49): loss=1.988958519036773e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9888721153453389\n",
      "Gradient Descent(2/49): loss=12.451407770212764\n",
      "Gradient Descent(3/49): loss=257.0630024964983\n",
      "Gradient Descent(4/49): loss=5514.340315657317\n",
      "Gradient Descent(5/49): loss=118718.88977712553\n",
      "Gradient Descent(6/49): loss=2557536.6963452776\n",
      "Gradient Descent(7/49): loss=55104847.240125194\n",
      "Gradient Descent(8/49): loss=1187339054.7862687\n",
      "Gradient Descent(9/49): loss=25583748245.66455\n",
      "Gradient Descent(10/49): loss=551257828275.7104\n",
      "Gradient Descent(11/49): loss=11878064446049.91\n",
      "Gradient Descent(12/49): loss=255939116576439.7\n",
      "Gradient Descent(13/49): loss=5514773467249529.0\n",
      "Gradient Descent(14/49): loss=1.188279744044598e+17\n",
      "Gradient Descent(15/49): loss=2.5604111635262326e+18\n",
      "Gradient Descent(16/49): loss=5.516971374072376e+19\n",
      "Gradient Descent(17/49): loss=1.1887533370902076e+21\n",
      "Gradient Descent(18/49): loss=2.5614316274575124e+22\n",
      "Gradient Descent(19/49): loss=5.519170190772249e+23\n",
      "Gradient Descent(20/49): loss=1.189227120814416e+25\n",
      "Gradient Descent(21/49): loss=2.5624524991941037e+26\n",
      "Gradient Descent(22/49): loss=5.521369884443486e+27\n",
      "Gradient Descent(23/49): loss=1.189701093402887e+29\n",
      "Gradient Descent(24/49): loss=2.5634737778244585e+30\n",
      "Gradient Descent(25/49): loss=5.523570454825383e+31\n",
      "Gradient Descent(26/49): loss=1.1901752548962212e+33\n",
      "Gradient Descent(27/49): loss=2.5644954634910288e+34\n",
      "Gradient Descent(28/49): loss=5.52577190225615e+35\n",
      "Gradient Descent(29/49): loss=1.1906496053690613e+37\n",
      "Gradient Descent(30/49): loss=2.565517556355694e+38\n",
      "Gradient Descent(31/49): loss=5.527974227085154e+39\n",
      "Gradient Descent(32/49): loss=1.1911241448967414e+41\n",
      "Gradient Descent(33/49): loss=2.5665400565807405e+42\n",
      "Gradient Descent(34/49): loss=5.530177429662036e+43\n",
      "Gradient Descent(35/49): loss=1.191598873554582e+45\n",
      "Gradient Descent(36/49): loss=2.567562964328498e+46\n",
      "Gradient Descent(37/49): loss=5.532381510336614e+47\n",
      "Gradient Descent(38/49): loss=1.192073791417976e+49\n",
      "Gradient Descent(39/49): loss=2.5685862797614026e+50\n",
      "Gradient Descent(40/49): loss=5.534586469458911e+51\n",
      "Gradient Descent(41/49): loss=1.192548898562327e+53\n",
      "Gradient Descent(42/49): loss=2.5696100030419398e+54\n",
      "Gradient Descent(43/49): loss=5.536792307379e+55\n",
      "Gradient Descent(44/49): loss=1.1930241950630698e+57\n",
      "Gradient Descent(45/49): loss=2.5706341343326565e+58\n",
      "Gradient Descent(46/49): loss=5.5389990244471385e+59\n",
      "Gradient Descent(47/49): loss=1.1934996809956855e+61\n",
      "Gradient Descent(48/49): loss=2.571658673796154e+62\n",
      "Gradient Descent(49/49): loss=5.5412066210137116e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9879271245769068\n",
      "Gradient Descent(2/49): loss=12.365408447114573\n",
      "Gradient Descent(3/49): loss=255.33979190238915\n",
      "Gradient Descent(4/49): loss=5486.644252315234\n",
      "Gradient Descent(5/49): loss=118366.64172818042\n",
      "Gradient Descent(6/49): loss=2555493.8584656897\n",
      "Gradient Descent(7/49): loss=55182362.280569084\n",
      "Gradient Descent(8/49): loss=1191645057.3082547\n",
      "Gradient Descent(9/49): loss=25733522295.99024\n",
      "Gradient Descent(10/49): loss=555716237542.3507\n",
      "Gradient Descent(11/49): loss=12000721357309.033\n",
      "Gradient Descent(12/49): loss=259156274964977.03\n",
      "Gradient Descent(13/49): loss=5596495202437050.0\n",
      "Gradient Descent(14/49): loss=1.2085664967854278e+17\n",
      "Gradient Descent(15/49): loss=2.6099066141734943e+18\n",
      "Gradient Descent(16/49): loss=5.636109028397104e+19\n",
      "Gradient Descent(17/49): loss=1.2171211344917782e+21\n",
      "Gradient Descent(18/49): loss=2.6283804105082025e+22\n",
      "Gradient Descent(19/49): loss=5.676003305465421e+23\n",
      "Gradient Descent(20/49): loss=1.2257363277812635e+25\n",
      "Gradient Descent(21/49): loss=2.6469849723289055e+26\n",
      "Gradient Descent(22/49): loss=5.716179968673513e+27\n",
      "Gradient Descent(23/49): loss=1.2344125023694637e+29\n",
      "Gradient Descent(24/49): loss=2.6657212235389175e+30\n",
      "Gradient Descent(25/49): loss=5.756641015856324e+31\n",
      "Gradient Descent(26/49): loss=1.2431500898449286e+33\n",
      "Gradient Descent(27/49): loss=2.684590096246541e+34\n",
      "Gradient Descent(28/49): loss=5.797388459959854e+35\n",
      "Gradient Descent(29/49): loss=1.2519495249076126e+37\n",
      "Gradient Descent(30/49): loss=2.7035925291907426e+38\n",
      "Gradient Descent(31/49): loss=5.838424328197558e+39\n",
      "Gradient Descent(32/49): loss=1.2608112453355696e+41\n",
      "Gradient Descent(33/49): loss=2.7227294677558623e+42\n",
      "Gradient Descent(34/49): loss=5.879750662132678e+43\n",
      "Gradient Descent(35/49): loss=1.2697356920055812e+45\n",
      "Gradient Descent(36/49): loss=2.7420018640180036e+46\n",
      "Gradient Descent(37/49): loss=5.92136951777924e+47\n",
      "Gradient Descent(38/49): loss=1.2787233089151116e+49\n",
      "Gradient Descent(39/49): loss=2.7614106767923266e+50\n",
      "Gradient Descent(40/49): loss=5.963282965704366e+51\n",
      "Gradient Descent(41/49): loss=1.2877745432044093e+53\n",
      "Gradient Descent(42/49): loss=2.7809568716809076e+54\n",
      "Gradient Descent(43/49): loss=6.005493091131581e+55\n",
      "Gradient Descent(44/49): loss=1.2968898451787113e+57\n",
      "Gradient Descent(45/49): loss=2.800641421120583e+58\n",
      "Gradient Descent(46/49): loss=6.048001994044039e+59\n",
      "Gradient Descent(47/49): loss=1.3060696683306633e+61\n",
      "Gradient Descent(48/49): loss=2.8204653044315007e+62\n",
      "Gradient Descent(49/49): loss=6.090811789289515e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9879922925393156\n",
      "Gradient Descent(2/49): loss=12.295937767604139\n",
      "Gradient Descent(3/49): loss=251.35942428133635\n",
      "Gradient Descent(4/49): loss=5344.310382341732\n",
      "Gradient Descent(5/49): loss=114067.97960195501\n",
      "Gradient Descent(6/49): loss=2436365.4254854852\n",
      "Gradient Descent(7/49): loss=52047146.38549766\n",
      "Gradient Descent(8/49): loss=1111915105.6381009\n",
      "Gradient Descent(9/49): loss=23754823620.070072\n",
      "Gradient Descent(10/49): loss=507496997086.4837\n",
      "Gradient Descent(11/49): loss=10842153404836.078\n",
      "Gradient Descent(12/49): loss=231631558188627.88\n",
      "Gradient Descent(13/49): loss=4948572524227454.0\n",
      "Gradient Descent(14/49): loss=1.057212181170342e+17\n",
      "Gradient Descent(15/49): loss=2.25862629320772e+18\n",
      "Gradient Descent(16/49): loss=4.82532535123372e+19\n",
      "Gradient Descent(17/49): loss=1.0308816834395927e+21\n",
      "Gradient Descent(18/49): loss=2.202373867056222e+22\n",
      "Gradient Descent(19/49): loss=4.705147766563814e+23\n",
      "Gradient Descent(20/49): loss=1.0052069649264032e+25\n",
      "Gradient Descent(21/49): loss=2.1475224423749827e+26\n",
      "Gradient Descent(22/49): loss=4.587963276641717e+27\n",
      "Gradient Descent(23/49): loss=9.801716905243287e+28\n",
      "Gradient Descent(24/49): loss=2.0940371249191732e+30\n",
      "Gradient Descent(25/49): loss=4.4736973358148325e+31\n",
      "Gradient Descent(26/49): loss=9.557599344495565e+32\n",
      "Gradient Descent(27/49): loss=2.041883890950885e+34\n",
      "Gradient Descent(28/49): loss=4.362277255873788e+35\n",
      "Gradient Descent(29/49): loss=9.319561676081362e+36\n",
      "Gradient Descent(30/49): loss=1.991029564141885e+38\n",
      "Gradient Descent(31/49): loss=4.2536321589684886e+39\n",
      "Gradient Descent(32/49): loss=9.087452476683424e+40\n",
      "Gradient Descent(33/49): loss=1.9414417944405887e+42\n",
      "Gradient Descent(34/49): loss=4.1476929325041203e+43\n",
      "Gradient Descent(35/49): loss=8.861124094272183e+44\n",
      "Gradient Descent(36/49): loss=1.8930890374926124e+46\n",
      "Gradient Descent(37/49): loss=4.04439218517578e+47\n",
      "Gradient Descent(38/49): loss=8.640432554179159e+48\n",
      "Gradient Descent(39/49): loss=1.845940534574399e+50\n",
      "Gradient Descent(40/49): loss=3.943664204098999e+51\n",
      "Gradient Descent(41/49): loss=8.425237467510067e+52\n",
      "Gradient Descent(42/49): loss=1.799966293026545e+54\n",
      "Gradient Descent(43/49): loss=3.845444913007567e+55\n",
      "Gradient Descent(44/49): loss=8.21540194183939e+56\n",
      "Gradient Descent(45/49): loss=1.7551370671746668e+58\n",
      "Gradient Descent(46/49): loss=3.7496718314926293e+59\n",
      "Gradient Descent(47/49): loss=8.010792494128349e+60\n",
      "Gradient Descent(48/49): loss=1.711424339725165e+62\n",
      "Gradient Descent(49/49): loss=3.656284035256883e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9999289407698897\n",
      "Gradient Descent(2/49): loss=12.740278076268101\n",
      "Gradient Descent(3/49): loss=265.4632808947155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=5749.122829864116\n",
      "Gradient Descent(5/49): loss=124999.02423320942\n",
      "Gradient Descent(6/49): loss=2719835.8371334937\n",
      "Gradient Descent(7/49): loss=59192164.28542584\n",
      "Gradient Descent(8/49): loss=1288277225.723124\n",
      "Gradient Descent(9/49): loss=28038903942.58265\n",
      "Gradient Descent(10/49): loss=610259550748.826\n",
      "Gradient Descent(11/49): loss=13282158419093.887\n",
      "Gradient Descent(12/49): loss=289083212505105.8\n",
      "Gradient Descent(13/49): loss=6291832168734208.0\n",
      "Gradient Descent(14/49): loss=1.3694033887757213e+17\n",
      "Gradient Descent(15/49): loss=2.980476282246257e+18\n",
      "Gradient Descent(16/49): loss=6.4869409264690225e+19\n",
      "Gradient Descent(17/49): loss=1.4118683936087828e+21\n",
      "Gradient Descent(18/49): loss=3.0729004376531757e+22\n",
      "Gradient Descent(19/49): loss=6.688100068609724e+23\n",
      "Gradient Descent(20/49): loss=1.455650237808409e+25\n",
      "Gradient Descent(21/49): loss=3.1681906566924356e+26\n",
      "Gradient Descent(22/49): loss=6.895497130049257e+27\n",
      "Gradient Descent(23/49): loss=1.5007897510868517e+29\n",
      "Gradient Descent(24/49): loss=3.26643581236819e+30\n",
      "Gradient Descent(25/49): loss=7.109325545830053e+31\n",
      "Gradient Descent(26/49): loss=1.5473290344544804e+33\n",
      "Gradient Descent(27/49): loss=3.367727536784642e+34\n",
      "Gradient Descent(28/49): loss=7.32978475131899e+35\n",
      "Gradient Descent(29/49): loss=1.5953114945861417e+37\n",
      "Gradient Descent(30/49): loss=3.4721603036169716e+38\n",
      "Gradient Descent(31/49): loss=7.557080366390201e+39\n",
      "Gradient Descent(32/49): loss=1.6447818841943695e+41\n",
      "Gradient Descent(33/49): loss=3.579831516157662e+42\n",
      "Gradient Descent(34/49): loss=7.791424387162807e+43\n",
      "Gradient Descent(35/49): loss=1.6957863437671812e+45\n",
      "Gradient Descent(36/49): loss=3.6908415981617753e+46\n",
      "Gradient Descent(37/49): loss=8.033035383726133e+47\n",
      "Gradient Descent(38/49): loss=1.7483724446027534e+49\n",
      "Gradient Descent(39/49): loss=3.805294087511273e+50\n",
      "Gradient Descent(40/49): loss=8.282138703998145e+51\n",
      "Gradient Descent(41/49): loss=1.8025892331787117e+53\n",
      "Gradient Descent(42/49): loss=3.9232957327836246e+54\n",
      "Gradient Descent(43/49): loss=8.538966683904657e+55\n",
      "Gradient Descent(44/49): loss=1.8584872768971713e+57\n",
      "Gradient Descent(45/49): loss=4.044956592814866e+58\n",
      "Gradient Descent(46/49): loss=8.803758864076295e+59\n",
      "Gradient Descent(47/49): loss=1.9161187112484173e+61\n",
      "Gradient Descent(48/49): loss=4.170390139350469e+62\n",
      "Gradient Descent(49/49): loss=9.07676221326603e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0075761485223775\n",
      "Gradient Descent(2/49): loss=13.223550292024996\n",
      "Gradient Descent(3/49): loss=282.03440773045776\n",
      "Gradient Descent(4/49): loss=6240.524976080879\n",
      "Gradient Descent(5/49): loss=138571.69034737255\n",
      "Gradient Descent(6/49): loss=3078966.517340365\n",
      "Gradient Descent(7/49): loss=68423073.52752255\n",
      "Gradient Descent(8/49): loss=1520609403.4655876\n",
      "Gradient Descent(9/49): loss=33793827336.66731\n",
      "Gradient Descent(10/49): loss=751031775639.6733\n",
      "Gradient Descent(11/49): loss=16690892780511.908\n",
      "Gradient Descent(12/49): loss=370937643278158.3\n",
      "Gradient Descent(13/49): loss=8243701773164582.0\n",
      "Gradient Descent(14/49): loss=1.8320766603286995e+17\n",
      "Gradient Descent(15/49): loss=4.071599135965614e+18\n",
      "Gradient Descent(16/49): loss=9.048704073986731e+19\n",
      "Gradient Descent(17/49): loss=2.0109800278838875e+21\n",
      "Gradient Descent(18/49): loss=4.469193201323687e+22\n",
      "Gradient Descent(19/49): loss=9.93231538561756e+23\n",
      "Gradient Descent(20/49): loss=2.2073534187475273e+25\n",
      "Gradient Descent(21/49): loss=4.9056125647380144e+26\n",
      "Gradient Descent(22/49): loss=1.0902211866452401e+28\n",
      "Gradient Descent(23/49): loss=2.4229027876231e+29\n",
      "Gradient Descent(24/49): loss=5.384648537546879e+30\n",
      "Gradient Descent(25/49): loss=1.1966819313188398e+32\n",
      "Gradient Descent(26/49): loss=2.659500680052543e+33\n",
      "Gradient Descent(27/49): loss=5.910462656860654e+34\n",
      "Gradient Descent(28/49): loss=1.3135386307723937e+36\n",
      "Gradient Descent(29/49): loss=2.9192024968276373e+37\n",
      "Gradient Descent(30/49): loss=6.487622836393987e+38\n",
      "Gradient Descent(31/49): loss=1.4418064561482115e+40\n",
      "Gradient Descent(32/49): loss=3.2042643498463938e+41\n",
      "Gradient Descent(33/49): loss=7.121143049342232e+42\n",
      "Gradient Descent(34/49): loss=1.582599710652025e+44\n",
      "Gradient Descent(35/49): loss=3.5171626616702206e+45\n",
      "Gradient Descent(36/49): loss=7.816526886353689e+46\n",
      "Gradient Descent(37/49): loss=1.737141509857722e+48\n",
      "Gradient Descent(38/49): loss=3.8606156789904754e+49\n",
      "Gradient Descent(39/49): loss=8.579815366963287e+50\n",
      "Gradient Descent(40/49): loss=1.9067744072994152e+52\n",
      "Gradient Descent(41/49): loss=4.237607086898043e+53\n",
      "Gradient Descent(42/49): loss=9.417639419841889e+54\n",
      "Gradient Descent(43/49): loss=2.0929720576591475e+56\n",
      "Gradient Descent(44/49): loss=4.651411929100488e+57\n",
      "Gradient Descent(45/49): loss=1.0337277487773175e+59\n",
      "Gradient Descent(46/49): loss=2.2973520188715684e+60\n",
      "Gradient Descent(47/49): loss=5.105625059263251e+61\n",
      "Gradient Descent(48/49): loss=1.1346718757790134e+63\n",
      "Gradient Descent(49/49): loss=2.5216898043618493e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0065504982786464\n",
      "Gradient Descent(2/49): loss=13.132029464675583\n",
      "Gradient Descent(3/49): loss=280.14103349187263\n",
      "Gradient Descent(4/49): loss=6209.055924033782\n",
      "Gradient Descent(5/49): loss=138156.57725904236\n",
      "Gradient Descent(6/49): loss=3076397.5245375014\n",
      "Gradient Descent(7/49): loss=68516443.75626405\n",
      "Gradient Descent(8/49): loss=1526050525.7785153\n",
      "Gradient Descent(9/49): loss=33989821544.474697\n",
      "Gradient Descent(10/49): loss=757060254180.2972\n",
      "Gradient Descent(11/49): loss=16862130222046.898\n",
      "Gradient Descent(12/49): loss=375573160135717.0\n",
      "Gradient Descent(13/49): loss=8365206948082928.0\n",
      "Gradient Descent(14/49): loss=1.8631972700319645e+17\n",
      "Gradient Descent(15/49): loss=4.14993210231414e+18\n",
      "Gradient Descent(16/49): loss=9.243216891630435e+19\n",
      "Gradient Descent(17/49): loss=2.0587579852611956e+21\n",
      "Gradient Descent(18/49): loss=4.585507937356983e+22\n",
      "Gradient Descent(19/49): loss=1.0213382629091906e+24\n",
      "Gradient Descent(20/49): loss=2.2748447097700797e+25\n",
      "Gradient Descent(21/49): loss=5.0668017066545754e+26\n",
      "Gradient Descent(22/49): loss=1.1285376722337827e+28\n",
      "Gradient Descent(23/49): loss=2.513611843104763e+29\n",
      "Gradient Descent(24/49): loss=5.598611950003154e+30\n",
      "Gradient Descent(25/49): loss=1.2469887048273294e+32\n",
      "Gradient Descent(26/49): loss=2.7774399152026746e+33\n",
      "Gradient Descent(27/49): loss=6.186240863849073e+34\n",
      "Gradient Descent(28/49): loss=1.377872328257496e+36\n",
      "Gradient Descent(29/49): loss=3.06895931594307e+37\n",
      "Gradient Descent(30/49): loss=6.835547161923724e+38\n",
      "Gradient Descent(31/49): loss=1.5224934641574208e+40\n",
      "Gradient Descent(32/49): loss=3.3910765202733353e+41\n",
      "Gradient Descent(33/49): loss=7.553004487091939e+42\n",
      "Gradient Descent(34/49): loss=1.6822939984094604e+44\n",
      "Gradient Descent(35/49): loss=3.7470030660264627e+45\n",
      "Gradient Descent(36/49): loss=8.345765954158953e+46\n",
      "Gradient Descent(37/49): loss=1.8588671568785688e+48\n",
      "Gradient Descent(38/49): loss=4.140287573245328e+49\n",
      "Gradient Descent(39/49): loss=9.221735467075904e+50\n",
      "Gradient Descent(40/49): loss=2.0539733900190725e+52\n",
      "Gradient Descent(41/49): loss=4.5748511242473995e+53\n",
      "Gradient Descent(42/49): loss=1.0189646521580987e+55\n",
      "Gradient Descent(43/49): loss=2.26955792472587e+56\n",
      "Gradient Descent(44/49): loss=5.055026357172195e+57\n",
      "Gradient Descent(45/49): loss=1.1259149278946963e+59\n",
      "Gradient Descent(46/49): loss=2.5077701584236065e+60\n",
      "Gradient Descent(47/49): loss=5.585600662777745e+61\n",
      "Gradient Descent(48/49): loss=1.2440906778966854e+63\n",
      "Gradient Descent(49/49): loss=2.7709850887544926e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0066883215194837\n",
      "Gradient Descent(2/49): loss=13.058874999797968\n",
      "Gradient Descent(3/49): loss=275.7968215883723\n",
      "Gradient Descent(4/49): loss=6048.680900156013\n",
      "Gradient Descent(5/49): loss=133158.6636891193\n",
      "Gradient Descent(6/49): loss=2933501.7009886075\n",
      "Gradient Descent(7/49): loss=64636899.91247471\n",
      "Gradient Descent(8/49): loss=1424280190.4744873\n",
      "Gradient Descent(9/49): loss=31384556468.812073\n",
      "Gradient Descent(10/49): loss=691573110518.5062\n",
      "Gradient Descent(11/49): loss=15239145776589.926\n",
      "Gradient Descent(12/49): loss=335801988309305.3\n",
      "Gradient Descent(13/49): loss=7399560656316678.0\n",
      "Gradient Descent(14/49): loss=1.6305293267246954e+17\n",
      "Gradient Descent(15/49): loss=3.592951018563841e+18\n",
      "Gradient Descent(16/49): loss=7.917243087454832e+19\n",
      "Gradient Descent(17/49): loss=1.7446031912859756e+21\n",
      "Gradient Descent(18/49): loss=3.8443183590061856e+22\n",
      "Gradient Descent(19/49): loss=8.471143306200709e+23\n",
      "Gradient Descent(20/49): loss=1.8666578106536902e+25\n",
      "Gradient Descent(21/49): loss=4.113271675538472e+26\n",
      "Gradient Descent(22/49): loss=9.06379507815237e+27\n",
      "Gradient Descent(23/49): loss=1.9972515238249635e+29\n",
      "Gradient Descent(24/49): loss=4.40104130226483e+30\n",
      "Gradient Descent(25/49): loss=9.697909508736798e+31\n",
      "Gradient Descent(26/49): loss=2.1369817363733627e+33\n",
      "Gradient Descent(27/49): loss=4.708943651700615e+34\n",
      "Gradient Descent(28/49): loss=1.0376387377330973e+36\n",
      "Gradient Descent(29/49): loss=2.286487649210877e+37\n",
      "Gradient Descent(30/49): loss=5.038387234284848e+38\n",
      "Gradient Descent(31/49): loss=1.1102332405497803e+40\n",
      "Gradient Descent(32/49): loss=2.4464531825462793e+41\n",
      "Gradient Descent(33/49): loss=5.390879101608415e+42\n",
      "Gradient Descent(34/49): loss=1.1879065455040037e+44\n",
      "Gradient Descent(35/49): loss=2.6176101045008518e+45\n",
      "Gradient Descent(36/49): loss=5.768031740474883e+46\n",
      "Gradient Descent(37/49): loss=1.2710139719402535e+48\n",
      "Gradient Descent(38/49): loss=2.8007413786081653e+49\n",
      "Gradient Descent(39/49): loss=6.171570449279659e+50\n",
      "Gradient Descent(40/49): loss=1.3599356977883433e+52\n",
      "Gradient Descent(41/49): loss=2.9966847455090257e+53\n",
      "Gradient Descent(42/49): loss=6.603341230449967e+54\n",
      "Gradient Descent(43/49): loss=1.4550784987011931e+56\n",
      "Gradient Descent(44/49): loss=3.2063365552263756e+57\n",
      "Gradient Descent(45/49): loss=7.065319235049794e+58\n",
      "Gradient Descent(46/49): loss=1.55687760886474e+60\n",
      "Gradient Descent(47/49): loss=3.43065586755093e+61\n",
      "Gradient Descent(48/49): loss=7.559617798179863e+62\n",
      "Gradient Descent(49/49): loss=1.6657987119924776e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0186798669254622\n",
      "Gradient Descent(2/49): loss=13.519992406215154\n",
      "Gradient Descent(3/49): loss=290.89169570082294\n",
      "Gradient Descent(4/49): loss=6495.453392568983\n",
      "Gradient Descent(5/49): loss=145599.57705682018\n",
      "Gradient Descent(6/49): loss=3266206.983596278\n",
      "Gradient Descent(7/49): loss=73284867.74851668\n",
      "Gradient Descent(8/49): loss=1644406096.5600016\n",
      "Gradient Descent(9/49): loss=36898663732.693726\n",
      "Gradient Descent(10/49): loss=827969067749.2368\n",
      "Gradient Descent(11/49): loss=18578819909879.824\n",
      "Gradient Descent(12/49): loss=416890779537288.25\n",
      "Gradient Descent(13/49): loss=9354627476976884.0\n",
      "Gradient Descent(14/49): loss=2.0990883455296915e+17\n",
      "Gradient Descent(15/49): loss=4.710152197679546e+18\n",
      "Gradient Descent(16/49): loss=1.0569128174924828e+20\n",
      "Gradient Descent(17/49): loss=2.3716106350599796e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=5.3216659995316846e+22\n",
      "Gradient Descent(19/49): loss=1.1941306297633532e+24\n",
      "Gradient Descent(20/49): loss=2.6795141992835515e+25\n",
      "Gradient Descent(21/49): loss=6.012571962594513e+26\n",
      "Gradient Descent(22/49): loss=1.3491632779958297e+28\n",
      "Gradient Descent(23/49): loss=3.027392540192682e+29\n",
      "Gradient Descent(24/49): loss=6.793177476657865e+30\n",
      "Gradient Descent(25/49): loss=1.5243236421014475e+32\n",
      "Gradient Descent(26/49): loss=3.420435538234432e+33\n",
      "Gradient Descent(27/49): loss=7.675128134264342e+34\n",
      "Gradient Descent(28/49): loss=1.722224880980614e+36\n",
      "Gradient Descent(29/49): loss=3.864506870481034e+37\n",
      "Gradient Descent(30/49): loss=8.671581462399702e+38\n",
      "Gradient Descent(31/49): loss=1.9458194170495662e+40\n",
      "Gradient Descent(32/49): loss=4.366231488667047e+41\n",
      "Gradient Descent(33/49): loss=9.797403215111385e+42\n",
      "Gradient Descent(34/49): loss=2.1984429824351635e+44\n",
      "Gradient Descent(35/49): loss=4.933094454624304e+45\n",
      "Gradient Descent(36/49): loss=1.106938915072033e+47\n",
      "Gradient Descent(37/49): loss=2.48386438364714e+48\n",
      "Gradient Descent(38/49): loss=5.573552607416715e+49\n",
      "Gradient Descent(39/49): loss=1.2506515602123464e+51\n",
      "Gradient Descent(40/49): loss=2.806341727142196e+52\n",
      "Gradient Descent(41/49): loss=6.297160728094625e+53\n",
      "Gradient Descent(42/49): loss=1.4130222578359628e+55\n",
      "Gradient Descent(43/49): loss=3.1706859445907256e+56\n",
      "Gradient Descent(44/49): loss=7.114714084278987e+57\n",
      "Gradient Descent(45/49): loss=1.59647336209363e+59\n",
      "Gradient Descent(46/49): loss=3.5823325655577884e+60\n",
      "Gradient Descent(47/49): loss=8.038409481149385e+61\n",
      "Gradient Descent(48/49): loss=1.8037417186746218e+63\n",
      "Gradient Descent(49/49): loss=4.0474228083514826e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0265950283311798\n",
      "Gradient Descent(2/49): loss=14.03222245377369\n",
      "Gradient Descent(3/49): loss=308.9986116189735\n",
      "Gradient Descent(4/49): loss=7049.056680364598\n",
      "Gradient Descent(5/49): loss=161364.0117762122\n",
      "Gradient Descent(6/49): loss=3696247.3337351023\n",
      "Gradient Descent(7/49): loss=84680578.45411307\n",
      "Gradient Descent(8/49): loss=1940101924.6592066\n",
      "Gradient Descent(9/49): loss=44449823298.35333\n",
      "Gradient Descent(10/49): loss=1018396266805.8982\n",
      "Gradient Descent(11/49): loss=23332640837947.746\n",
      "Gradient Descent(12/49): loss=534578001634421.94\n",
      "Gradient Descent(13/49): loss=1.2247805877442174e+16\n",
      "Gradient Descent(14/49): loss=2.8061153013510707e+17\n",
      "Gradient Descent(15/49): loss=6.429137752578087e+18\n",
      "Gradient Descent(16/49): loss=1.472990516990226e+20\n",
      "Gradient Descent(17/49): loss=3.374793241715171e+21\n",
      "Gradient Descent(18/49): loss=7.732045314480752e+22\n",
      "Gradient Descent(19/49): loss=1.7715018510579275e+24\n",
      "Gradient Descent(20/49): loss=4.058717558782002e+25\n",
      "Gradient Descent(21/49): loss=9.298995771385019e+26\n",
      "Gradient Descent(22/49): loss=2.1305084944668732e+28\n",
      "Gradient Descent(23/49): loss=4.8812436918872674e+29\n",
      "Gradient Descent(24/49): loss=1.1183499170019586e+31\n",
      "Gradient Descent(25/49): loss=2.562270224158215e+32\n",
      "Gradient Descent(26/49): loss=5.87046022161627e+33\n",
      "Gradient Descent(27/49): loss=1.3449909727964274e+35\n",
      "Gradient Descent(28/49): loss=3.081531342709344e+36\n",
      "Gradient Descent(29/49): loss=7.060148066538135e+37\n",
      "Gradient Descent(30/49): loss=1.6175623473495798e+39\n",
      "Gradient Descent(31/49): loss=3.706024183776172e+40\n",
      "Gradient Descent(32/49): loss=8.490934073260494e+41\n",
      "Gradient Descent(33/49): loss=1.945372125526599e+43\n",
      "Gradient Descent(34/49): loss=4.457074656478454e+44\n",
      "Gradient Descent(35/49): loss=1.0211678389318504e+46\n",
      "Gradient Descent(36/49): loss=2.3396147375566904e+47\n",
      "Gradient Descent(37/49): loss=5.360330507390558e+48\n",
      "Gradient Descent(38/49): loss=1.2281142996418498e+50\n",
      "Gradient Descent(39/49): loss=2.813753239478927e+51\n",
      "Gradient Descent(40/49): loss=6.44663716967305e+52\n",
      "Gradient Descent(41/49): loss=1.4769998383051775e+54\n",
      "Gradient Descent(42/49): loss=3.3839790652653414e+55\n",
      "Gradient Descent(43/49): loss=7.75309110886179e+56\n",
      "Gradient Descent(44/49): loss=1.776323688267222e+58\n",
      "Gradient Descent(45/49): loss=4.069764950772634e+59\n",
      "Gradient Descent(46/49): loss=9.324306636193261e+60\n",
      "Gradient Descent(47/49): loss=2.136307509092182e+62\n",
      "Gradient Descent(48/49): loss=4.8945299114131946e+63\n",
      "Gradient Descent(49/49): loss=1.1213939450083454e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.025486983315718\n",
      "Gradient Descent(2/49): loss=13.934925174226462\n",
      "Gradient Descent(3/49): loss=306.9213408967435\n",
      "Gradient Descent(4/49): loss=7013.373237018236\n",
      "Gradient Descent(5/49): loss=160876.15329147008\n",
      "Gradient Descent(6/49): loss=3693035.6842045966\n",
      "Gradient Descent(7/49): loss=84792678.50717415\n",
      "Gradient Descent(8/49): loss=1946953139.878135\n",
      "Gradient Descent(9/49): loss=44705267357.60824\n",
      "Gradient Descent(10/49): loss=1026510843599.692\n",
      "Gradient Descent(11/49): loss=23570502148012.836\n",
      "Gradient Descent(12/49): loss=541220515325611.5\n",
      "Gradient Descent(13/49): loss=1.2427383464523786e+16\n",
      "Gradient Descent(14/49): loss=2.8535478353560774e+17\n",
      "Gradient Descent(15/49): loss=6.552252385360588e+18\n",
      "Gradient Descent(16/49): loss=1.504513464114573e+20\n",
      "Gradient Descent(17/49): loss=3.454629998247781e+21\n",
      "Gradient Descent(18/49): loss=7.932443750635705e+22\n",
      "Gradient Descent(19/49): loss=1.8214299039515443e+24\n",
      "Gradient Descent(20/49): loss=4.182326404484484e+25\n",
      "Gradient Descent(21/49): loss=9.603363882266653e+26\n",
      "Gradient Descent(22/49): loss=2.2051028288082547e+28\n",
      "Gradient Descent(23/49): loss=5.06330755059478e+29\n",
      "Gradient Descent(24/49): loss=1.1626252987833165e+31\n",
      "Gradient Descent(25/49): loss=2.6695940783060336e+32\n",
      "Gradient Descent(26/49): loss=6.129861917149767e+33\n",
      "Gradient Descent(27/49): loss=1.407525115097867e+35\n",
      "Gradient Descent(28/49): loss=3.2319275318234885e+36\n",
      "Gradient Descent(29/49): loss=7.421079353338886e+37\n",
      "Gradient Descent(30/49): loss=1.704011560478287e+39\n",
      "Gradient Descent(31/49): loss=3.912713043470192e+40\n",
      "Gradient Descent(32/49): loss=8.984283742913572e+41\n",
      "Gradient Descent(33/49): loss=2.0629510387399201e+43\n",
      "Gradient Descent(34/49): loss=4.7369018054387136e+44\n",
      "Gradient Descent(35/49): loss=1.0876767452548966e+46\n",
      "Gradient Descent(36/49): loss=2.4974988943405502e+47\n",
      "Gradient Descent(37/49): loss=5.7347008239755556e+48\n",
      "Gradient Descent(38/49): loss=1.3167891131014616e+50\n",
      "Gradient Descent(39/49): loss=3.023581563546129e+51\n",
      "Gradient Descent(40/49): loss=6.942680024049994e+52\n",
      "Gradient Descent(41/49): loss=1.594162581802883e+54\n",
      "Gradient Descent(42/49): loss=3.660480287751967e+55\n",
      "Gradient Descent(43/49): loss=8.405112558762553e+56\n",
      "Gradient Descent(44/49): loss=1.929963053259718e+58\n",
      "Gradient Descent(45/49): loss=4.431537782399386e+59\n",
      "Gradient Descent(46/49): loss=1.0175597446626577e+61\n",
      "Gradient Descent(47/49): loss=2.3364978136264823e+62\n",
      "Gradient Descent(48/49): loss=5.365013761320883e+63\n",
      "Gradient Descent(49/49): loss=1.2319024007340232e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0256992800575602\n",
      "Gradient Descent(2/49): loss=13.857942629018353\n",
      "Gradient Descent(3/49): loss=302.1860636833907\n",
      "Gradient Descent(4/49): loss=6832.989698146579\n",
      "Gradient Descent(5/49): loss=155078.3082523676\n",
      "Gradient Descent(6/49): loss=3522097.6448107646\n",
      "Gradient Descent(7/49): loss=80007451.46889861\n",
      "Gradient Descent(8/49): loss=1817526488.361764\n",
      "Gradient Descent(9/49): loss=41289239205.024155\n",
      "Gradient Descent(10/49): loss=937982226623.6432\n",
      "Gradient Descent(11/49): loss=21308494965491.688\n",
      "Gradient Descent(12/49): loss=484073227300740.75\n",
      "Gradient Descent(13/49): loss=1.0996877424427362e+16\n",
      "Gradient Descent(14/49): loss=2.4982029328846122e+17\n",
      "Gradient Descent(15/49): loss=5.675263703371596e+18\n",
      "Gradient Descent(16/49): loss=1.2892714891035466e+20\n",
      "Gradient Descent(17/49): loss=2.928887643314581e+21\n",
      "Gradient Descent(18/49): loss=6.653666741786637e+22\n",
      "Gradient Descent(19/49): loss=1.5115390722910695e+24\n",
      "Gradient Descent(20/49): loss=3.433821463786701e+25\n",
      "Gradient Descent(21/49): loss=7.800744328308532e+26\n",
      "Gradient Descent(22/49): loss=1.7721251007783467e+28\n",
      "Gradient Descent(23/49): loss=4.0258047702099354e+29\n",
      "Gradient Descent(24/49): loss=9.14557558082526e+30\n",
      "Gradient Descent(25/49): loss=2.0776355903673424e+32\n",
      "Gradient Descent(26/49): loss=4.7198447032806535e+33\n",
      "Gradient Descent(27/49): loss=1.0722252798503433e+35\n",
      "Gradient Descent(28/49): loss=2.435815419840055e+36\n",
      "Gradient Descent(29/49): loss=5.5335356021065745e+37\n",
      "Gradient Descent(30/49): loss=1.2570745718405797e+39\n",
      "Gradient Descent(31/49): loss=2.855744668140554e+40\n",
      "Gradient Descent(32/49): loss=6.487505031362126e+41\n",
      "Gradient Descent(33/49): loss=1.4737914772805425e+43\n",
      "Gradient Descent(34/49): loss=3.34806879995394e+44\n",
      "Gradient Descent(35/49): loss=7.605936702734227e+45\n",
      "Gradient Descent(36/49): loss=1.7278699029958894e+47\n",
      "Gradient Descent(37/49): loss=3.925268534782492e+48\n",
      "Gradient Descent(38/49): loss=8.917183546885483e+49\n",
      "Gradient Descent(39/49): loss=2.0257508933271254e+51\n",
      "Gradient Descent(40/49): loss=4.601976240860217e+52\n",
      "Gradient Descent(41/49): loss=1.0454486477682586e+54\n",
      "Gradient Descent(42/49): loss=2.3749859145648026e+55\n",
      "Gradient Descent(43/49): loss=5.395346874686028e+56\n",
      "Gradient Descent(44/49): loss=1.225681706980501e+58\n",
      "Gradient Descent(45/49): loss=2.784428289263697e+59\n",
      "Gradient Descent(46/49): loss=6.325492869720497e+60\n",
      "Gradient Descent(47/49): loss=1.4369865512128271e+62\n",
      "Gradient Descent(48/49): loss=3.2644576334140674e+63\n",
      "Gradient Descent(49/49): loss=7.415993998942482e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.037741360873921\n",
      "Gradient Descent(2/49): loss=14.33609757544949\n",
      "Gradient Descent(3/49): loss=318.32150183107086\n",
      "Gradient Descent(4/49): loss=7325.2301910859205\n",
      "Gradient Descent(5/49): loss=169206.93265378845\n",
      "Gradient Descent(6/49): loss=3911561.6150141037\n",
      "Gradient Descent(7/49): loss=90442128.29936653\n",
      "Gradient Descent(8/49): loss=2091298928.7330675\n",
      "Gradient Descent(9/49): loss=48358012490.153915\n",
      "Gradient Descent(10/49): loss=1118208395542.3374\n",
      "Gradient Descent(11/49): loss=25856968964240.066\n",
      "Gradient Descent(12/49): loss=597905623721438.1\n",
      "Gradient Descent(13/49): loss=1.3825718410856052e+16\n",
      "Gradient Descent(14/49): loss=3.197001125150213e+17\n",
      "Gradient Descent(15/49): loss=7.392611345978253e+18\n",
      "Gradient Descent(16/49): loss=1.709436453157197e+20\n",
      "Gradient Descent(17/49): loss=3.952829186561041e+21\n",
      "Gradient Descent(18/49): loss=9.140356491297451e+22\n",
      "Gradient Descent(19/49): loss=2.1135777148304367e+24\n",
      "Gradient Descent(20/49): loss=4.887348497784171e+25\n",
      "Gradient Descent(21/49): loss=1.1301299768299269e+27\n",
      "Gradient Descent(22/49): loss=2.6132651786755595e+28\n",
      "Gradient Descent(23/49): loss=6.042804840233165e+29\n",
      "Gradient Descent(24/49): loss=1.3973128573065656e+31\n",
      "Gradient Descent(25/49): loss=3.2310876700743688e+32\n",
      "Gradient Descent(26/49): loss=7.47143166765851e+33\n",
      "Gradient Descent(27/49): loss=1.7276625354831392e+35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(28/49): loss=3.994974416258666e+36\n",
      "Gradient Descent(29/49): loss=9.237811354228465e+37\n",
      "Gradient Descent(30/49): loss=2.1361127688079478e+39\n",
      "Gradient Descent(31/49): loss=4.939457611867926e+40\n",
      "Gradient Descent(32/49): loss=1.1421794699095138e+42\n",
      "Gradient Descent(33/49): loss=2.6411279213092e+43\n",
      "Gradient Descent(34/49): loss=6.107233478178125e+44\n",
      "Gradient Descent(35/49): loss=1.4122110654334025e+46\n",
      "Gradient Descent(36/49): loss=3.2655376619520805e+47\n",
      "Gradient Descent(37/49): loss=7.55109238458967e+48\n",
      "Gradient Descent(38/49): loss=1.746082945695477e+50\n",
      "Gradient Descent(39/49): loss=4.037569000573544e+51\n",
      "Gradient Descent(40/49): loss=9.336305285256164e+52\n",
      "Gradient Descent(41/49): loss=2.1588880924913207e+54\n",
      "Gradient Descent(42/49): loss=4.992122315517141e+55\n",
      "Gradient Descent(43/49): loss=1.1543574351890363e+57\n",
      "Gradient Descent(44/49): loss=2.669287737670667e+58\n",
      "Gradient Descent(45/49): loss=6.172349056955811e+59\n",
      "Gradient Descent(46/49): loss=1.4272681188783823e+61\n",
      "Gradient Descent(47/49): loss=3.3003549610840185e+62\n",
      "Gradient Descent(48/49): loss=7.631602447416584e+63\n",
      "Gradient Descent(49/49): loss=1.7646997550919392e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0459287547717457\n",
      "Gradient Descent(2/49): loss=14.87855904945305\n",
      "Gradient Descent(3/49): loss=338.08088018617\n",
      "Gradient Descent(4/49): loss=7947.813560927821\n",
      "Gradient Descent(5/49): loss=187476.56960586028\n",
      "Gradient Descent(6/49): loss=4425138.377649624\n",
      "Gradient Descent(7/49): loss=104466358.99297553\n",
      "Gradient Descent(8/49): loss=2466291778.51333\n",
      "Gradient Descent(9/49): loss=58226054134.51636\n",
      "Gradient Descent(10/49): loss=1374648229540.4421\n",
      "Gradient Descent(11/49): loss=32453844099111.03\n",
      "Gradient Descent(12/49): loss=766197636740248.8\n",
      "Gradient Descent(13/49): loss=1.808903902942985e+16\n",
      "Gradient Descent(14/49): loss=4.2706127295761344e+17\n",
      "Gradient Descent(15/49): loss=1.008242236199702e+19\n",
      "Gradient Descent(16/49): loss=2.3803432255669184e+20\n",
      "Gradient Descent(17/49): loss=5.619714855969871e+21\n",
      "Gradient Descent(18/49): loss=1.3267496353346847e+23\n",
      "Gradient Descent(19/49): loss=3.132302332080566e+24\n",
      "Gradient Descent(20/49): loss=7.395003275910745e+25\n",
      "Gradient Descent(21/49): loss=1.7458746842770496e+27\n",
      "Gradient Descent(22/49): loss=4.121808063466099e+28\n",
      "Gradient Descent(23/49): loss=9.731111783141068e+29\n",
      "Gradient Descent(24/49): loss=2.2974028649058386e+31\n",
      "Gradient Descent(25/49): loss=5.423902264509687e+32\n",
      "Gradient Descent(26/49): loss=1.2805205488484975e+34\n",
      "Gradient Descent(27/49): loss=3.0231608094278542e+35\n",
      "Gradient Descent(28/49): loss=7.137332772893852e+36\n",
      "Gradient Descent(29/49): loss=1.6850416607731041e+38\n",
      "Gradient Descent(30/49): loss=3.978188335738401e+39\n",
      "Gradient Descent(31/49): loss=9.392042228406429e+40\n",
      "Gradient Descent(32/49): loss=2.2173524673963724e+42\n",
      "Gradient Descent(33/49): loss=5.234912540957542e+43\n",
      "Gradient Descent(34/49): loss=1.235902262469484e+45\n",
      "Gradient Descent(35/49): loss=2.917822199371031e+46\n",
      "Gradient Descent(36/49): loss=6.888640506354473e+47\n",
      "Gradient Descent(37/49): loss=1.626328294987167e+49\n",
      "Gradient Descent(38/49): loss=3.839572874554858e+50\n",
      "Gradient Descent(39/49): loss=9.064787167792443e+51\n",
      "Gradient Descent(40/49): loss=2.1400913352087623e+53\n",
      "Gradient Descent(41/49): loss=5.0525079500029926e+54\n",
      "Gradient Descent(42/49): loss=1.1928386496809597e+56\n",
      "Gradient Descent(43/49): loss=2.8161539937247526e+57\n",
      "Gradient Descent(44/49): loss=6.648613639818763e+58\n",
      "Gradient Descent(45/49): loss=1.5696607298494353e+60\n",
      "Gradient Descent(46/49): loss=3.7057873119224304e+61\n",
      "Gradient Descent(47/49): loss=8.748934938649163e+62\n",
      "Gradient Descent(48/49): loss=2.065522279555968e+64\n",
      "Gradient Descent(49/49): loss=4.876459040168414e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0447365796881225\n",
      "Gradient Descent(2/49): loss=14.775222676877895\n",
      "Gradient Descent(3/49): loss=335.8051094963956\n",
      "Gradient Descent(4/49): loss=7907.430861837824\n",
      "Gradient Descent(5/49): loss=186904.73344116093\n",
      "Gradient Descent(6/49): loss=4421145.223514444\n",
      "Gradient Descent(7/49): loss=104600516.52931683\n",
      "Gradient Descent(8/49): loss=2474888990.67843\n",
      "Gradient Descent(9/49): loss=58557678635.439285\n",
      "Gradient Descent(10/49): loss=1385522824441.0989\n",
      "Gradient Descent(11/49): loss=32782644442165.39\n",
      "Gradient Descent(12/49): loss=775665383028897.2\n",
      "Gradient Descent(13/49): loss=1.8352907308896372e+16\n",
      "Gradient Descent(14/49): loss=4.3424551548414163e+17\n",
      "Gradient Descent(15/49): loss=1.0274621109289343e+19\n",
      "Gradient Descent(16/49): loss=2.4310634272314432e+20\n",
      "Gradient Descent(17/49): loss=5.75210445915285e+21\n",
      "Gradient Descent(18/49): loss=1.3609972221401484e+23\n",
      "Gradient Descent(19/49): loss=3.22023609252827e+24\n",
      "Gradient Descent(20/49): loss=7.61935463425187e+25\n",
      "Gradient Descent(21/49): loss=1.8028046197402245e+27\n",
      "Gradient Descent(22/49): loss=4.265590267118566e+28\n",
      "Gradient Descent(23/49): loss=1.009275221935137e+30\n",
      "Gradient Descent(24/49): loss=2.3880316903957433e+31\n",
      "Gradient Descent(25/49): loss=5.650287682085722e+32\n",
      "Gradient Descent(26/49): loss=1.3369064999735732e+34\n",
      "Gradient Descent(27/49): loss=3.163235378860994e+35\n",
      "Gradient Descent(28/49): loss=7.484486059627712e+36\n",
      "Gradient Descent(29/49): loss=1.770893558889441e+38\n",
      "Gradient Descent(30/49): loss=4.190085961723458e+39\n",
      "Gradient Descent(31/49): loss=9.914102560541395e+40\n",
      "Gradient Descent(32/49): loss=2.3457616497324828e+42\n",
      "Gradient Descent(33/49): loss=5.550273142479151e+43\n",
      "Gradient Descent(34/49): loss=1.3132422025758026e+45\n",
      "Gradient Descent(35/49): loss=3.1072436227090083e+46\n",
      "Gradient Descent(36/49): loss=7.352004764946218e+47\n",
      "Gradient Descent(37/49): loss=1.7395473489351814e+49\n",
      "Gradient Descent(38/49): loss=4.115918141967572e+50\n",
      "Gradient Descent(39/49): loss=9.73861514131689e+51\n",
      "Gradient Descent(40/49): loss=2.30423982206189e+53\n",
      "Gradient Descent(41/49): loss=5.4520289389501905e+54\n",
      "Gradient Descent(42/49): loss=1.2899967818693494e+56\n",
      "Gradient Descent(43/49): loss=3.052242964714897e+57\n",
      "Gradient Descent(44/49): loss=7.221868493463519e+58\n",
      "Gradient Descent(45/49): loss=1.7087559915713555e+60\n",
      "Gradient Descent(46/49): loss=4.0430631510027535e+61\n",
      "Gradient Descent(47/49): loss=9.566233987548067e+62\n",
      "Gradient Descent(48/49): loss=2.263452963425081e+64\n",
      "Gradient Descent(49/49): loss=5.355523735156905e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0450251681535438\n",
      "Gradient Descent(2/49): loss=14.694264182123808\n",
      "Gradient Descent(3/49): loss=330.64994655801365\n",
      "Gradient Descent(4/49): loss=7704.89060601032\n",
      "Gradient Descent(5/49): loss=180193.6011754905\n",
      "Gradient Descent(6/49): loss=4217202.547146061\n",
      "Gradient Descent(7/49): loss=98716492.42839454\n",
      "Gradient Descent(8/49): loss=2310876639.63074\n",
      "Gradient Descent(9/49): loss=54096578936.83796\n",
      "Gradient Descent(10/49): loss=1266381309231.5056\n",
      "Gradient Descent(11/49): loss=29645558643247.027\n",
      "Gradient Descent(12/49): loss=693992713992627.6\n",
      "Gradient Descent(13/49): loss=1.6246140993739662e+16\n",
      "Gradient Descent(14/49): loss=3.803168213241341e+17\n",
      "Gradient Descent(15/49): loss=8.903091849187743e+18\n",
      "Gradient Descent(16/49): loss=2.08418455781632e+20\n",
      "Gradient Descent(17/49): loss=4.87900759268123e+21\n",
      "Gradient Descent(18/49): loss=1.1421596519948129e+23\n",
      "Gradient Descent(19/49): loss=2.673758230357749e+24\n",
      "Gradient Descent(20/49): loss=6.259180196034327e+25\n",
      "Gradient Descent(21/49): loss=1.4652535252333503e+27\n",
      "Gradient Descent(22/49): loss=3.430110375430014e+28\n",
      "Gradient Descent(23/49): loss=8.02977572482639e+29\n",
      "Gradient Descent(24/49): loss=1.8797441228966802e+31\n",
      "Gradient Descent(25/49): loss=4.4004192503658915e+32\n",
      "Gradient Descent(26/49): loss=1.0301236930668703e+34\n",
      "Gradient Descent(27/49): loss=2.4114857304323764e+35\n",
      "Gradient Descent(28/49): loss=5.645208888231522e+36\n",
      "Gradient Descent(29/49): loss=1.3215248587041839e+38\n",
      "Gradient Descent(30/49): loss=3.093646287941397e+39\n",
      "Gradient Descent(31/49): loss=7.242124347383126e+40\n",
      "Gradient Descent(32/49): loss=1.6953575225259502e+42\n",
      "Gradient Descent(33/49): loss=3.968776275187669e+43\n",
      "Gradient Descent(34/49): loss=9.290774903351668e+44\n",
      "Gradient Descent(35/49): loss=2.1749398887612353e+46\n",
      "Gradient Descent(36/49): loss=5.0914628423709335e+47\n",
      "Gradient Descent(37/49): loss=1.1918947281806825e+49\n",
      "Gradient Descent(38/49): loss=2.790186410166111e+50\n",
      "Gradient Descent(39/49): loss=6.531734740834831e+51\n",
      "Gradient Descent(40/49): loss=1.5290576489507115e+53\n",
      "Gradient Descent(41/49): loss=3.579473733368227e+54\n",
      "Gradient Descent(42/49): loss=8.379430439830357e+55\n",
      "Gradient Descent(43/49): loss=1.961597143217048e+57\n",
      "Gradient Descent(44/49): loss=4.5920344824238275e+58\n",
      "Gradient Descent(45/49): loss=1.0749801895197951e+60\n",
      "Gradient Descent(46/49): loss=2.516493315289847e+61\n",
      "Gradient Descent(47/49): loss=5.891028195345071e+62\n",
      "Gradient Descent(48/49): loss=1.3790703510910575e+64\n",
      "Gradient Descent(49/49): loss=3.228358395502486e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0571134226152654\n",
      "Gradient Descent(2/49): loss=15.189709080779194\n",
      "Gradient Descent(3/49): loss=347.8768043731006\n",
      "Gradient Descent(4/49): loss=8246.326460341568\n",
      "Gradient Descent(5/49): loss=196205.06191869613\n",
      "Gradient Descent(6/49): loss=4671942.3531483365\n",
      "Gradient Descent(7/49): loss=111269177.08115846\n",
      "Gradient Descent(8/49): loss=2650193563.8972063\n",
      "Gradient Descent(9/49): loss=63122980344.003815\n",
      "Gradient Descent(10/49): loss=1503486173434.1313\n",
      "Gradient Descent(11/49): loss=35810630047877.16\n",
      "Gradient Descent(12/49): loss=852952115540548.8\n",
      "Gradient Descent(13/49): loss=2.031596174602414e+16\n",
      "Gradient Descent(14/49): loss=4.8389388672198246e+17\n",
      "Gradient Descent(15/49): loss=1.1525582622460434e+19\n",
      "Gradient Descent(16/49): loss=2.745210443103608e+20\n",
      "Gradient Descent(17/49): loss=6.538654599190646e+21\n",
      "Gradient Descent(18/49): loss=1.5574035164972403e+23\n",
      "Gradient Descent(19/49): loss=3.7094874435445153e+24\n",
      "Gradient Descent(20/49): loss=8.835409030658563e+25\n",
      "Gradient Descent(21/49): loss=2.1044538882388812e+27\n",
      "Gradient Descent(22/49): loss=5.012474411045491e+28\n",
      "Gradient Descent(23/49): loss=1.1938916724101296e+30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=2.843660054023324e+31\n",
      "Gradient Descent(25/49): loss=6.77314591408783e+32\n",
      "Gradient Descent(26/49): loss=1.6132556178302237e+34\n",
      "Gradient Descent(27/49): loss=3.842518264736495e+35\n",
      "Gradient Descent(28/49): loss=9.152267285882422e+36\n",
      "Gradient Descent(29/49): loss=2.17992448444426e+38\n",
      "Gradient Descent(30/49): loss=5.192233366271694e+39\n",
      "Gradient Descent(31/49): loss=1.2367073961600102e+41\n",
      "Gradient Descent(32/49): loss=2.945640297394986e+42\n",
      "Gradient Descent(33/49): loss=7.016046632031743e+43\n",
      "Gradient Descent(34/49): loss=1.671110705077484e+45\n",
      "Gradient Descent(35/49): loss=3.9803198796811196e+46\n",
      "Gradient Descent(36/49): loss=9.480488812888154e+47\n",
      "Gradient Descent(37/49): loss=2.258101631231162e+49\n",
      "Gradient Descent(38/49): loss=5.378438894455538e+50\n",
      "Gradient Descent(39/49): loss=1.2810585910440222e+52\n",
      "Gradient Descent(40/49): loss=3.0512777887640735e+53\n",
      "Gradient Descent(41/49): loss=7.267658332955255e+54\n",
      "Gradient Descent(42/49): loss=1.7310406099068475e+56\n",
      "Gradient Descent(43/49): loss=4.1230633800697846e+57\n",
      "Gradient Descent(44/49): loss=9.820481124927091e+58\n",
      "Gradient Descent(45/49): loss=2.339082391777744e+60\n",
      "Gradient Descent(46/49): loss=5.571322184650413e+61\n",
      "Gradient Descent(47/49): loss=1.3270003226174239e+63\n",
      "Gradient Descent(48/49): loss=3.160703685524247e+64\n",
      "Gradient Descent(49/49): loss=7.528293412907219e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.065577327844075\n",
      "Gradient Descent(2/49): loss=15.763712281164167\n",
      "Gradient Descent(3/49): loss=369.4122957451137\n",
      "Gradient Descent(4/49): loss=8945.292118989735\n",
      "Gradient Descent(5/49): loss=217332.54759695992\n",
      "Gradient Descent(6/49): loss=5283693.749895192\n",
      "Gradient Descent(7/49): loss=128475863.3580265\n",
      "Gradient Descent(8/49): loss=3124095491.8886223\n",
      "Gradient Descent(9/49): loss=75968247665.76494\n",
      "Gradient Descent(10/49): loss=1847316355352.3608\n",
      "Gradient Descent(11/49): loss=44921143903492.125\n",
      "Gradient Descent(12/49): loss=1092346538209877.0\n",
      "Gradient Descent(13/49): loss=2.656257005914748e+16\n",
      "Gradient Descent(14/49): loss=6.459215230705272e+17\n",
      "Gradient Descent(15/49): loss=1.5706861755879666e+19\n",
      "Gradient Descent(16/49): loss=3.819434678956206e+20\n",
      "Gradient Descent(17/49): loss=9.287712273898638e+21\n",
      "Gradient Descent(18/49): loss=2.258491283157028e+23\n",
      "Gradient Descent(19/49): loss=5.491969093991349e+24\n",
      "Gradient Descent(20/49): loss=1.3354811131872623e+26\n",
      "Gradient Descent(21/49): loss=3.2474869635267123e+27\n",
      "Gradient Descent(22/49): loss=7.896908068666838e+28\n",
      "Gradient Descent(23/49): loss=1.9202896807708804e+30\n",
      "Gradient Descent(24/49): loss=4.669564880344072e+31\n",
      "Gradient Descent(25/49): loss=1.1354972320108314e+33\n",
      "Gradient Descent(26/49): loss=2.761186527960316e+34\n",
      "Gradient Descent(27/49): loss=6.714372195067443e+35\n",
      "Gradient Descent(28/49): loss=1.6327326501624312e+37\n",
      "Gradient Descent(29/49): loss=3.970312978575743e+38\n",
      "Gradient Descent(30/49): loss=9.654602758313646e+39\n",
      "Gradient Descent(31/49): loss=2.3477079747570657e+41\n",
      "Gradient Descent(32/49): loss=5.708917158701042e+42\n",
      "Gradient Descent(33/49): loss=1.3882363341328185e+44\n",
      "Gradient Descent(34/49): loss=3.37577173714852e+45\n",
      "Gradient Descent(35/49): loss=8.208857916436357e+46\n",
      "Gradient Descent(36/49): loss=1.99614646780471e+48\n",
      "Gradient Descent(37/49): loss=4.854025689677198e+49\n",
      "Gradient Descent(38/49): loss=1.180352533046254e+51\n",
      "Gradient Descent(39/49): loss=2.870261080883055e+52\n",
      "Gradient Descent(40/49): loss=6.979608584539024e+53\n",
      "Gradient Descent(41/49): loss=1.6972301341445665e+55\n",
      "Gradient Descent(42/49): loss=4.127151391597159e+56\n",
      "Gradient Descent(43/49): loss=1.0035986438425772e+58\n",
      "Gradient Descent(44/49): loss=2.44044897401469e+59\n",
      "Gradient Descent(45/49): loss=5.9344352758049145e+60\n",
      "Gradient Descent(46/49): loss=1.4430755331377756e+62\n",
      "Gradient Descent(47/49): loss=3.5091241163775655e+63\n",
      "Gradient Descent(48/49): loss=8.533130651427176e+64\n",
      "Gradient Descent(49/49): loss=2.0749998090546815e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0642992873958586\n",
      "Gradient Descent(2/49): loss=15.654066366906836\n",
      "Gradient Descent(3/49): loss=366.92251030020407\n",
      "Gradient Descent(4/49): loss=8899.67833498433\n",
      "Gradient Descent(5/49): loss=216663.99088568546\n",
      "Gradient Descent(6/49): loss=5278754.187157929\n",
      "Gradient Descent(7/49): loss=128635919.07721868\n",
      "Gradient Descent(8/49): loss=3134847821.074755\n",
      "Gradient Descent(9/49): loss=76397137403.27904\n",
      "Gradient Descent(10/49): loss=1861827626414.7986\n",
      "Gradient Descent(11/49): loss=45373506028647.445\n",
      "Gradient Descent(12/49): loss=1105771367280315.8\n",
      "Gradient Descent(13/49): loss=2.6948114146094924e+16\n",
      "Gradient Descent(14/49): loss=6.56736911742596e+17\n",
      "Gradient Descent(15/49): loss=1.6004955732754547e+19\n",
      "Gradient Descent(16/49): loss=3.900475272141187e+20\n",
      "Gradient Descent(17/49): loss=9.505622889517451e+21\n",
      "Gradient Descent(18/49): loss=2.316560424700665e+23\n",
      "Gradient Descent(19/49): loss=5.645555545236515e+24\n",
      "Gradient Descent(20/49): loss=1.3758457182824111e+26\n",
      "Gradient Descent(21/49): loss=3.352994094829912e+27\n",
      "Gradient Descent(22/49): loss=8.17138815099689e+28\n",
      "Gradient Descent(23/49): loss=1.9914017867560173e+30\n",
      "Gradient Descent(24/49): loss=4.853130218531461e+31\n",
      "Gradient Descent(25/49): loss=1.1827283210582566e+33\n",
      "Gradient Descent(26/49): loss=2.8823588456206076e+34\n",
      "Gradient Descent(27/49): loss=7.024430181475453e+35\n",
      "Gradient Descent(28/49): loss=1.7118832878631101e+37\n",
      "Gradient Descent(29/49): loss=4.171931837251822e+38\n",
      "Gradient Descent(30/49): loss=1.0167173999578865e+40\n",
      "Gradient Descent(31/49): loss=2.4777832229830225e+41\n",
      "Gradient Descent(32/49): loss=6.038462310520488e+42\n",
      "Gradient Descent(33/49): loss=1.4715987555876035e+44\n",
      "Gradient Descent(34/49): loss=3.586348288825078e+45\n",
      "Gradient Descent(35/49): loss=8.740082172482383e+46\n",
      "Gradient Descent(36/49): loss=2.129994920453489e+48\n",
      "Gradient Descent(37/49): loss=5.190887535865221e+49\n",
      "Gradient Descent(38/49): loss=1.2650412050872004e+51\n",
      "Gradient Descent(39/49): loss=3.0829588187210196e+52\n",
      "Gradient Descent(40/49): loss=7.513300783965032e+53\n",
      "Gradient Descent(41/49): loss=1.8310231173878675e+55\n",
      "Gradient Descent(42/49): loss=4.4622806311228695e+56\n",
      "Gradient Descent(43/49): loss=1.0874766266905901e+58\n",
      "Gradient Descent(44/49): loss=2.6502264455311704e+59\n",
      "Gradient Descent(45/49): loss=6.458713723316838e+60\n",
      "Gradient Descent(46/49): loss=1.5740157989179108e+62\n",
      "Gradient Descent(47/49): loss=3.835942946811499e+63\n",
      "Gradient Descent(48/49): loss=9.348354890280397e+64\n",
      "Gradient Descent(49/49): loss=2.2782335495180404e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0646659858074354\n",
      "Gradient Descent(2/49): loss=15.568980438896958\n",
      "Gradient Descent(3/49): loss=361.31697265171636\n",
      "Gradient Descent(4/49): loss=8672.638693892693\n",
      "Gradient Descent(5/49): loss=208912.16591111888\n",
      "Gradient Descent(6/49): loss=5036058.914810217\n",
      "Gradient Descent(7/49): loss=121422587.94041462\n",
      "Gradient Descent(8/49): loss=2927726336.0923996\n",
      "Gradient Descent(9/49): loss=70593975199.4442\n",
      "Gradient Descent(10/49): loss=1702183999429.266\n",
      "Gradient Descent(11/49): loss=41043637449599.92\n",
      "Gradient Descent(12/49): loss=989658394338735.5\n",
      "Gradient Descent(13/49): loss=2.3862987793519708e+16\n",
      "Gradient Descent(14/49): loss=5.7539268388272224e+17\n",
      "Gradient Descent(15/49): loss=1.3874069150931393e+19\n",
      "Gradient Descent(16/49): loss=3.345363966901757e+20\n",
      "Gradient Descent(17/49): loss=8.066458336647794e+21\n",
      "Gradient Descent(18/49): loss=1.9450125830718133e+23\n",
      "Gradient Descent(19/49): loss=4.689882214185551e+24\n",
      "Gradient Descent(20/49): loss=1.130840765483804e+26\n",
      "Gradient Descent(21/49): loss=2.7267227160113443e+27\n",
      "Gradient Descent(22/49): loss=6.574768965670758e+28\n",
      "Gradient Descent(23/49): loss=1.585331236583912e+30\n",
      "Gradient Descent(24/49): loss=3.8226060000154283e+31\n",
      "Gradient Descent(25/49): loss=9.217200982452699e+32\n",
      "Gradient Descent(26/49): loss=2.2224836656088624e+34\n",
      "Gradient Descent(27/49): loss=5.358930171211137e+35\n",
      "Gradient Descent(28/49): loss=1.2921639436233725e+37\n",
      "Gradient Descent(29/49): loss=3.115710792743844e+38\n",
      "Gradient Descent(30/49): loss=7.51271059057651e+39\n",
      "Gradient Descent(31/49): loss=1.8114909942606148e+41\n",
      "Gradient Descent(32/49): loss=4.367930299888584e+42\n",
      "Gradient Descent(33/49): loss=1.0532105964165746e+44\n",
      "Gradient Descent(34/49): loss=2.539538143345486e+45\n",
      "Gradient Descent(35/49): loss=6.123422991991846e+46\n",
      "Gradient Descent(36/49): loss=1.4765011203753027e+48\n",
      "Gradient Descent(37/49): loss=3.560191026033273e+49\n",
      "Gradient Descent(38/49): loss=8.584456839847242e+50\n",
      "Gradient Descent(39/49): loss=2.0699141898941274e+52\n",
      "Gradient Descent(40/49): loss=4.991049327241204e+53\n",
      "Gradient Descent(41/49): loss=1.203459230753377e+55\n",
      "Gradient Descent(42/49): loss=2.901822893595912e+56\n",
      "Gradient Descent(43/49): loss=6.99697662423184e+57\n",
      "Gradient Descent(44/49): loss=1.687135420569341e+59\n",
      "Gradient Descent(45/49): loss=4.0680798010414306e+60\n",
      "Gradient Descent(46/49): loss=9.80909597764039e+61\n",
      "Gradient Descent(47/49): loss=2.3652034523494057e+63\n",
      "Gradient Descent(48/49): loss=5.7030611014077e+64\n",
      "Gradient Descent(49/49): loss=1.375141994405678e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0767960521494966\n",
      "Gradient Descent(2/49): loss=16.08195930596261\n",
      "Gradient Descent(3/49): loss=379.68739408857255\n",
      "Gradient Descent(4/49): loss=9267.225031818278\n",
      "Gradient Descent(5/49): loss=227020.17596166386\n",
      "Gradient Descent(6/49): loss=5565695.2190246275\n",
      "Gradient Descent(7/49): loss=136479075.1449205\n",
      "Gradient Descent(8/49): loss=3346868196.978745\n",
      "Gradient Descent(9/49): loss=82076442667.23946\n",
      "Gradient Descent(10/49): loss=2012799642236.4148\n",
      "Gradient Descent(11/49): loss=49360911373486.66\n",
      "Gradient Descent(12/49): loss=1210503263669945.2\n",
      "Gradient Descent(13/49): loss=2.9685803529760764e+16\n",
      "Gradient Descent(14/49): loss=7.280004822253092e+17\n",
      "Gradient Descent(15/49): loss=1.7853136651094725e+19\n",
      "Gradient Descent(16/49): loss=4.3782181043260214e+20\n",
      "Gradient Descent(17/49): loss=1.0736933323324858e+22\n",
      "Gradient Descent(18/49): loss=2.633074334520975e+23\n",
      "Gradient Descent(19/49): loss=6.457225953409379e+24\n",
      "Gradient Descent(20/49): loss=1.5835393048902522e+26\n",
      "Gradient Descent(21/49): loss=3.8833962884958434e+27\n",
      "Gradient Descent(22/49): loss=9.523455898412424e+28\n",
      "Gradient Descent(23/49): loss=2.3354869168959564e+30\n",
      "Gradient Descent(24/49): loss=5.727436759487763e+31\n",
      "Gradient Descent(25/49): loss=1.4045692826030167e+33\n",
      "Gradient Descent(26/49): loss=3.444498739098128e+34\n",
      "Gradient Descent(27/49): loss=8.447124474814539e+35\n",
      "Gradient Descent(28/49): loss=2.071532530497973e+37\n",
      "Gradient Descent(29/49): loss=5.0801276075733266e+38\n",
      "Gradient Descent(30/49): loss=1.2458262725434836e+40\n",
      "Gradient Descent(31/49): loss=3.05520495006027e+41\n",
      "Gradient Descent(32/49): loss=7.492438948021136e+42\n",
      "Gradient Descent(33/49): loss=1.837410003826965e+44\n",
      "Gradient Descent(34/49): loss=4.5059766860764363e+45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=1.105024238094676e+47\n",
      "Gradient Descent(36/49): loss=2.7099087541883633e+48\n",
      "Gradient Descent(37/49): loss=6.645651020912273e+49\n",
      "Gradient Descent(38/49): loss=1.629747770049191e+51\n",
      "Gradient Descent(39/49): loss=3.996715725249891e+52\n",
      "Gradient Descent(40/49): loss=9.801355082067454e+53\n",
      "Gradient Descent(41/49): loss=2.4036375876786383e+55\n",
      "Gradient Descent(42/49): loss=5.894566215106349e+56\n",
      "Gradient Descent(43/49): loss=1.4455553134293394e+58\n",
      "Gradient Descent(44/49): loss=3.5450109268915876e+59\n",
      "Gradient Descent(45/49): loss=8.693615771759958e+60\n",
      "Gradient Descent(46/49): loss=2.131980880895747e+62\n",
      "Gradient Descent(47/49): loss=5.228368259924605e+63\n",
      "Gradient Descent(48/49): loss=1.282180103317909e+65\n",
      "Gradient Descent(49/49): loss=3.1443573513087633e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0855407475481682\n",
      "Gradient Descent(2/49): loss=16.688851759113867\n",
      "Gradient Descent(3/49): loss=403.1299342605943\n",
      "Gradient Descent(4/49): loss=10050.645462853452\n",
      "Gradient Descent(5/49): loss=251401.59550286303\n",
      "Gradient Descent(6/49): loss=6292554.820438795\n",
      "Gradient Descent(7/49): loss=157528204.1392973\n",
      "Gradient Descent(8/49): loss=3943745986.9800425\n",
      "Gradient Descent(9/49): loss=98733548456.7478\n",
      "Gradient Descent(10/49): loss=2471849130244.7686\n",
      "Gradient Descent(11/49): loss=61884167539723.28\n",
      "Gradient Descent(12/49): loss=1549306164334718.0\n",
      "Gradient Descent(13/49): loss=3.878778432540366e+16\n",
      "Gradient Descent(14/49): loss=9.710748420077576e+17\n",
      "Gradient Descent(15/49): loss=2.431142612455414e+19\n",
      "Gradient Descent(16/49): loss=6.086507603569992e+20\n",
      "Gradient Descent(17/49): loss=1.5237927479483421e+22\n",
      "Gradient Descent(18/49): loss=3.8149041948961945e+23\n",
      "Gradient Descent(19/49): loss=9.550835595058234e+24\n",
      "Gradient Descent(20/49): loss=2.3911075063556976e+26\n",
      "Gradient Descent(21/49): loss=5.986277378619264e+27\n",
      "Gradient Descent(22/49): loss=1.4986995255768966e+29\n",
      "Gradient Descent(23/49): loss=3.752081846369049e+30\n",
      "Gradient Descent(24/49): loss=9.393556174266187e+31\n",
      "Gradient Descent(25/49): loss=2.351731684224471e+33\n",
      "Gradient Descent(26/49): loss=5.887697706792431e+34\n",
      "Gradient Descent(27/49): loss=1.4740195286351333e+36\n",
      "Gradient Descent(28/49): loss=3.69029403172507e+37\n",
      "Gradient Descent(29/49): loss=9.238866769421632e+38\n",
      "Gradient Descent(30/49): loss=2.3130042877158647e+40\n",
      "Gradient Descent(31/49): loss=5.790741406401808e+41\n",
      "Gradient Descent(32/49): loss=1.449745952219167e+43\n",
      "Gradient Descent(33/49): loss=3.629523714618476e+44\n",
      "Gradient Descent(34/49): loss=9.086724729124426e+45\n",
      "Gradient Descent(35/49): loss=2.2749146388084863e+47\n",
      "Gradient Descent(36/49): loss=5.695381744400876e+48\n",
      "Gradient Descent(37/49): loss=1.42587210355481e+50\n",
      "Gradient Descent(38/49): loss=3.569754139298146e+51\n",
      "Gradient Descent(39/49): loss=8.937088104372563e+52\n",
      "Gradient Descent(40/49): loss=2.237452235324554e+54\n",
      "Gradient Descent(41/49): loss=5.601592427973823e+55\n",
      "Gradient Descent(42/49): loss=1.4023914000819865e+57\n",
      "Gradient Descent(43/49): loss=3.5109688259402694e+58\n",
      "Gradient Descent(44/49): loss=8.789915636963943e+59\n",
      "Gradient Descent(45/49): loss=2.2006067480320465e+61\n",
      "Gradient Descent(46/49): loss=5.5093475972846145e+62\n",
      "Gradient Descent(47/49): loss=1.3792973676396188e+64\n",
      "Gradient Descent(48/49): loss=3.45315156610421e+65\n",
      "Gradient Descent(49/49): loss=8.645166748120336e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0841751064389271\n",
      "Gradient Descent(2/49): loss=16.57261793175646\n",
      "Gradient Descent(3/49): loss=400.4096660668716\n",
      "Gradient Descent(4/49): loss=9999.218075236106\n",
      "Gradient Descent(5/49): loss=250621.8904845548\n",
      "Gradient Descent(6/49): loss=6286473.649223655\n",
      "Gradient Descent(7/49): loss=157718578.02878365\n",
      "Gradient Descent(8/49): loss=3957150294.199694\n",
      "Gradient Descent(9/49): loss=99286186493.44167\n",
      "Gradient Descent(10/49): loss=2491133108352.5547\n",
      "Gradient Descent(11/49): loss=62503673034191.17\n",
      "Gradient Descent(12/49): loss=1568246343978407.2\n",
      "Gradient Descent(13/49): loss=3.934803656654815e+16\n",
      "Gradient Descent(14/49): loss=9.872607229401289e+17\n",
      "Gradient Descent(15/49): loss=2.477083551443936e+19\n",
      "Gradient Descent(16/49): loss=6.215119055823413e+20\n",
      "Gradient Descent(17/49): loss=1.559402583546945e+22\n",
      "Gradient Descent(18/49): loss=3.9126143780594326e+23\n",
      "Gradient Descent(19/49): loss=9.816933377893078e+24\n",
      "Gradient Descent(20/49): loss=2.4631147267528067e+26\n",
      "Gradient Descent(21/49): loss=6.180070622488722e+27\n",
      "Gradient Descent(22/49): loss=1.5506087671901063e+29\n",
      "Gradient Descent(23/49): loss=3.890550279697478e+30\n",
      "Gradient Descent(24/49): loss=9.761573518176354e+31\n",
      "Gradient Descent(25/49): loss=2.4492246777536226e+33\n",
      "Gradient Descent(26/49): loss=6.145219836686795e+34\n",
      "Gradient Descent(27/49): loss=1.541864541224726e+36\n",
      "Gradient Descent(28/49): loss=3.868610605748321e+37\n",
      "Gradient Descent(29/49): loss=9.706525845013986e+38\n",
      "Gradient Descent(30/49): loss=2.435412957818199e+40\n",
      "Gradient Descent(31/49): loss=6.110565582180529e+41\n",
      "Gradient Descent(32/49): loss=1.5331696258847186e+43\n",
      "Gradient Descent(33/49): loss=3.84679465447563e+44\n",
      "Gradient Descent(34/49): loss=9.651788597861837e+45\n",
      "Gradient Descent(35/49): loss=2.4216791252278317e+47\n",
      "Gradient Descent(36/49): loss=6.076106750683877e+48\n",
      "Gradient Descent(37/49): loss=1.5245237430963527e+50\n",
      "Gradient Descent(38/49): loss=3.8251017281796666e+51\n",
      "Gradient Descent(39/49): loss=9.597360026158916e+52\n",
      "Gradient Descent(40/49): loss=2.4080227407585125e+54\n",
      "Gradient Descent(41/49): loss=6.04184224016329e+55\n",
      "Gradient Descent(42/49): loss=1.5159266163542383e+57\n",
      "Gradient Descent(43/49): loss=3.803531133095456e+58\n",
      "Gradient Descent(44/49): loss=9.543238389216186e+59\n",
      "Gradient Descent(45/49): loss=2.3944433676632043e+61\n",
      "Gradient Descent(46/49): loss=6.00777095479976e+62\n",
      "Gradient Descent(47/49): loss=1.5073779707122462e+64\n",
      "Gradient Descent(48/49): loss=3.78208217937014e+65\n",
      "Gradient Descent(49/49): loss=9.489421956160409e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0846217330192347\n",
      "Gradient Descent(2/49): loss=16.48324943204421\n",
      "Gradient Descent(3/49): loss=394.3215250132325\n",
      "Gradient Descent(4/49): loss=9745.1274807744\n",
      "Gradient Descent(5/49): loss=241686.4179628475\n",
      "Gradient Descent(6/49): loss=5998382.1370707285\n",
      "Gradient Descent(7/49): loss=148901501.01733568\n",
      "Gradient Descent(8/49): loss=3696467141.8340898\n",
      "Gradient Descent(9/49): loss=91765822163.0266\n",
      "Gradient Descent(10/49): loss=2278121234642.5366\n",
      "Gradient Descent(11/49): loss=56555284956131.39\n",
      "Gradient Descent(12/49): loss=1404008361632688.5\n",
      "Gradient Descent(13/49): loss=3.4855091856504856e+16\n",
      "Gradient Descent(14/49): loss=8.65292181398939e+17\n",
      "Gradient Descent(15/49): loss=2.1481239164545528e+19\n",
      "Gradient Descent(16/49): loss=5.3328071931768406e+20\n",
      "Gradient Descent(17/49): loss=1.3238916226722879e+22\n",
      "Gradient Descent(18/49): loss=3.2866161579611254e+23\n",
      "Gradient Descent(19/49): loss=8.159161660384294e+24\n",
      "Gradient Descent(20/49): loss=2.025545905011898e+26\n",
      "Gradient Descent(21/49): loss=5.028502172281017e+27\n",
      "Gradient Descent(22/49): loss=1.2483466325838996e+29\n",
      "Gradient Descent(23/49): loss=3.0990725701069934e+30\n",
      "Gradient Descent(24/49): loss=7.6935768832978175e+31\n",
      "Gradient Descent(25/49): loss=1.9099625426703827e+33\n",
      "Gradient Descent(26/49): loss=4.741561655571895e+34\n",
      "Gradient Descent(27/49): loss=1.1771124528000624e+36\n",
      "Gradient Descent(28/49): loss=2.9222307484048704e+37\n",
      "Gradient Descent(29/49): loss=7.25455968680797e+38\n",
      "Gradient Descent(30/49): loss=1.8009746929871032e+40\n",
      "Gradient Descent(31/49): loss=4.470994774056529e+41\n",
      "Gradient Descent(32/49): loss=1.1099430962289473e+43\n",
      "Gradient Descent(33/49): loss=2.755480019826847e+44\n",
      "Gradient Descent(34/49): loss=6.840594049786162e+45\n",
      "Gradient Descent(35/49): loss=1.6982059974040587e+47\n",
      "Gradient Descent(36/49): loss=4.215867201927067e+48\n",
      "Gradient Descent(37/49): loss=1.0466066125931584e+50\n",
      "Gradient Descent(38/49): loss=2.598244558137472e+51\n",
      "Gradient Descent(39/49): loss=6.450250459591948e+52\n",
      "Gradient Descent(40/49): loss=1.601301573447373e+54\n",
      "Gradient Descent(41/49): loss=3.9752979286439016e+55\n",
      "Gradient Descent(42/49): loss=9.868842873525081e+56\n",
      "Gradient Descent(43/49): loss=2.4499813953705492e+58\n",
      "Gradient Descent(44/49): loss=6.08218097560794e+59\n",
      "Gradient Descent(45/49): loss=1.5099267892380123e+61\n",
      "Gradient Descent(46/49): loss=3.748456216613516e+62\n",
      "Gradient Descent(47/49): loss=9.305698864353116e+63\n",
      "Gradient Descent(48/49): loss=2.3101785468433902e+65\n",
      "Gradient Descent(49/49): loss=5.73511457451019e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0967892494766136\n",
      "Gradient Descent(2/49): loss=17.01399752170861\n",
      "Gradient Descent(3/49): loss=413.8889185199703\n",
      "Gradient Descent(4/49): loss=10397.05552554679\n",
      "Gradient Descent(5/49): loss=262124.64837426585\n",
      "Gradient Descent(6/49): loss=6613759.070260349\n",
      "Gradient Descent(7/49): loss=166909905.3873058\n",
      "Gradient Descent(8/49): loss=4212523203.28311\n",
      "Gradient Descent(9/49): loss=106318801170.05547\n",
      "Gradient Descent(10/49): loss=2683366577543.771\n",
      "Gradient Descent(11/49): loss=67725241749576.72\n",
      "Gradient Descent(12/49): loss=1709311837960458.8\n",
      "Gradient Descent(13/49): loss=4.314118678243846e+16\n",
      "Gradient Descent(14/49): loss=1.0888370504952384e+18\n",
      "Gradient Descent(15/49): loss=2.7481073713375965e+19\n",
      "Gradient Descent(16/49): loss=6.935926860700119e+20\n",
      "Gradient Descent(17/49): loss=1.7505531973465488e+22\n",
      "Gradient Descent(18/49): loss=4.418207629620696e+23\n",
      "Gradient Descent(19/49): loss=1.1151079949648554e+25\n",
      "Gradient Descent(20/49): loss=2.8144124149384955e+26\n",
      "Gradient Descent(21/49): loss=7.103273653465677e+27\n",
      "Gradient Descent(22/49): loss=1.792789725069515e+29\n",
      "Gradient Descent(23/49): loss=4.524808074581425e+30\n",
      "Gradient Descent(24/49): loss=1.1420127985732348e+32\n",
      "Gradient Descent(25/49): loss=2.882317239998669e+33\n",
      "Gradient Descent(26/49): loss=7.274658114491193e+34\n",
      "Gradient Descent(27/49): loss=1.836045316190027e+36\n",
      "Gradient Descent(28/49): loss=4.633980525336505e+37\n",
      "Gradient Descent(29/49): loss=1.1695667486986723e+39\n",
      "Gradient Descent(30/49): loss=2.951860440894388e+40\n",
      "Gradient Descent(31/49): loss=7.450177659558422e+41\n",
      "Gradient Descent(32/49): loss=1.8803445579617467e+43\n",
      "Gradient Descent(33/49): loss=4.745787037870314e+44\n",
      "Gradient Descent(34/49): loss=1.197785507632283e+46\n",
      "Gradient Descent(35/49): loss=3.0230815475819373e+47\n",
      "Gradient Descent(36/49): loss=7.629932058032592e+48\n",
      "Gradient Descent(37/49): loss=1.9257126310985022e+50\n",
      "Gradient Descent(38/49): loss=4.8602911655919914e+51\n",
      "Gradient Descent(39/49): loss=1.2266851155697264e+53\n",
      "Gradient Descent(40/49): loss=3.096021043786669e+54\n",
      "Gradient Descent(41/49): loss=7.814023486473978e+55\n",
      "Gradient Descent(42/49): loss=1.9721753238630124e+57\n",
      "Gradient Descent(43/49): loss=4.977557995297607e+58\n",
      "Gradient Descent(44/49): loss=1.2562819997169751e+60\n",
      "Gradient Descent(45/49): loss=3.1707203900062985e+61\n",
      "Gradient Descent(46/49): loss=8.002556586711074e+62\n",
      "Gradient Descent(47/49): loss=2.0197590467252044e+64\n",
      "Gradient Descent(48/49): loss=5.097654184167268e+65\n",
      "Gradient Descent(49/49): loss=1.2865929836279846e+67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.105819013884025\n",
      "Gradient Descent(2/49): loss=17.65516450161599\n",
      "Gradient Descent(3/49): loss=439.37704537687875\n",
      "Gradient Descent(4/49): loss=11273.7232930223\n",
      "Gradient Descent(5/49): loss=290204.1356828675\n",
      "Gradient Descent(6/49): loss=7475274.725624038\n",
      "Gradient Descent(7/49): loss=192585837.3810801\n",
      "Gradient Descent(8/49): loss=4961823144.530452\n",
      "Gradient Descent(9/49): loss=127839056794.21254\n",
      "Gradient Descent(10/49): loss=3293724600145.05\n",
      "Gradient Descent(11/49): loss=84861636414726.72\n",
      "Gradient Descent(12/49): loss=2186430250559041.5\n",
      "Gradient Descent(13/49): loss=5.633261107041928e+16\n",
      "Gradient Descent(14/49): loss=1.4513900576271089e+18\n",
      "Gradient Descent(15/49): loss=3.739455831497124e+19\n",
      "Gradient Descent(16/49): loss=9.634577459613145e+20\n",
      "Gradient Descent(17/49): loss=2.482315263010568e+22\n",
      "Gradient Descent(18/49): loss=6.395598656410727e+23\n",
      "Gradient Descent(19/49): loss=1.6478036768529359e+25\n",
      "Gradient Descent(20/49): loss=4.2455086745444e+26\n",
      "Gradient Descent(21/49): loss=1.0938404956160183e+28\n",
      "Gradient Descent(22/49): loss=2.81824186822214e+29\n",
      "Gradient Descent(23/49): loss=7.261101833067915e+30\n",
      "Gradient Descent(24/49): loss=1.8707975502274418e+32\n",
      "Gradient Descent(25/49): loss=4.8200446081036387e+33\n",
      "Gradient Descent(26/49): loss=1.2418676740989094e+35\n",
      "Gradient Descent(27/49): loss=3.199628728288093e+36\n",
      "Gradient Descent(28/49): loss=8.2437317698239e+37\n",
      "Gradient Descent(29/49): loss=2.123968724620234e+39\n",
      "Gradient Descent(30/49): loss=5.472331304711162e+40\n",
      "Gradient Descent(31/49): loss=1.4099270653750433e+42\n",
      "Gradient Descent(32/49): loss=3.6326278856064186e+43\n",
      "Gradient Descent(33/49): loss=9.359338989478306e+44\n",
      "Gradient Descent(34/49): loss=2.411401031937679e+46\n",
      "Gradient Descent(35/49): loss=6.2128906147828525e+47\n",
      "Gradient Descent(36/49): loss=1.6007295874896393e+49\n",
      "Gradient Descent(37/49): loss=4.124223925925816e+50\n",
      "Gradient Descent(38/49): loss=1.062591903349136e+52\n",
      "Gradient Descent(39/49): loss=2.7377309606429253e+53\n",
      "Gradient Descent(40/49): loss=7.05366828905732e+54\n",
      "Gradient Descent(41/49): loss=1.817353021436709e+56\n",
      "Gradient Descent(42/49): loss=4.682346644580531e+57\n",
      "Gradient Descent(43/49): loss=1.20639027428375e+59\n",
      "Gradient Descent(44/49): loss=3.108222445621191e+60\n",
      "Gradient Descent(45/49): loss=8.008226672085285e+61\n",
      "Gradient Descent(46/49): loss=2.063291657965003e+63\n",
      "Gradient Descent(47/49): loss=5.315998959754013e+64\n",
      "Gradient Descent(48/49): loss=1.36964858220666e+66\n",
      "Gradient Descent(49/49): loss=3.528851779210115e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.104364036817328\n",
      "Gradient Descent(2/49): loss=17.532056352035564\n",
      "Gradient Descent(3/49): loss=436.4088300700005\n",
      "Gradient Descent(4/49): loss=11215.845136489375\n",
      "Gradient Descent(5/49): loss=289296.97847487737\n",
      "Gradient Descent(6/49): loss=7467821.616423394\n",
      "Gradient Descent(7/49): loss=192811601.5958423\n",
      "Gradient Descent(8/49): loss=4978481162.960167\n",
      "Gradient Descent(9/49): loss=128548598194.43567\n",
      "Gradient Descent(10/49): loss=3319247967168.3647\n",
      "Gradient Descent(11/49): loss=85706265297748.3\n",
      "Gradient Descent(12/49): loss=2213021269474310.5\n",
      "Gradient Descent(13/49): loss=5.714242211989647e+16\n",
      "Gradient Descent(14/49): loss=1.4754745171713388e+18\n",
      "Gradient Descent(15/49): loss=3.809822869706962e+19\n",
      "Gradient Descent(16/49): loss=9.837343958233455e+20\n",
      "Gradient Descent(17/49): loss=2.540100669107616e+22\n",
      "Gradient Descent(18/49): loss=6.558794159854647e+23\n",
      "Gradient Descent(19/49): loss=1.6935462974487463e+25\n",
      "Gradient Descent(20/49): loss=4.3729060429900356e+26\n",
      "Gradient Descent(21/49): loss=1.1291281076689257e+28\n",
      "Gradient Descent(22/49): loss=2.915521785730827e+29\n",
      "Gradient Descent(23/49): loss=7.528169058355934e+30\n",
      "Gradient Descent(24/49): loss=1.9438485985103155e+32\n",
      "Gradient Descent(25/49): loss=5.01921163650885e+33\n",
      "Gradient Descent(26/49): loss=1.2960106806349234e+35\n",
      "Gradient Descent(27/49): loss=3.346429292007463e+36\n",
      "Gradient Descent(28/49): loss=8.640815368064235e+37\n",
      "Gradient Descent(29/49): loss=2.2311450118877455e+39\n",
      "Gradient Descent(30/49): loss=5.761039730660017e+40\n",
      "Gradient Descent(31/49): loss=1.4875581193246643e+42\n",
      "Gradient Descent(32/49): loss=3.8410239502292194e+43\n",
      "Gradient Descent(33/49): loss=9.917908278388747e+44\n",
      "Gradient Descent(34/49): loss=2.560903183450915e+46\n",
      "Gradient Descent(35/49): loss=6.612508334342522e+47\n",
      "Gradient Descent(36/49): loss=1.7074158349410135e+49\n",
      "Gradient Descent(37/49): loss=4.408718576983353e+50\n",
      "Gradient Descent(38/49): loss=1.1383752623864806e+52\n",
      "Gradient Descent(39/49): loss=2.939398864738153e+53\n",
      "Gradient Descent(40/49): loss=7.589822066153242e+54\n",
      "Gradient Descent(41/49): loss=1.9597680221938334e+56\n",
      "Gradient Descent(42/49): loss=5.0603171817967406e+57\n",
      "Gradient Descent(43/49): loss=1.3066245438438335e+59\n",
      "Gradient Descent(44/49): loss=3.37383534912905e+60\n",
      "Gradient Descent(45/49): loss=8.711580550558844e+61\n",
      "Gradient Descent(46/49): loss=2.249417290279626e+63\n",
      "Gradient Descent(47/49): loss=5.808220582296467e+64\n",
      "Gradient Descent(48/49): loss=1.499740687439043e+66\n",
      "Gradient Descent(49/49): loss=3.8724805604246266e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1048924097889417\n",
      "Gradient Descent(2/49): loss=17.43824644719612\n",
      "Gradient Descent(3/49): loss=429.80404408105056\n",
      "Gradient Descent(4/49): loss=10931.927843967098\n",
      "Gradient Descent(5/49): loss=279017.7195188444\n",
      "Gradient Descent(6/49): loss=7126670.873885296\n",
      "Gradient Descent(7/49): loss=182064857.01113254\n",
      "Gradient Descent(8/49): loss=4651455683.511379\n",
      "Gradient Descent(9/49): loss=118838772398.13365\n",
      "Gradient Descent(10/49): loss=3036192084498.263\n",
      "Gradient Descent(11/49): loss=77571258705414.12\n",
      "Gradient Descent(12/49): loss=1981858188727820.5\n",
      "Gradient Descent(13/49): loss=5.063424656214232e+16\n",
      "Gradient Descent(14/49): loss=1.293648054668034e+18\n",
      "Gradient Descent(15/49): loss=3.305125315251619e+19\n",
      "Gradient Descent(16/49): loss=8.444223552100851e+20\n",
      "Gradient Descent(17/49): loss=2.157404171898655e+22\n",
      "Gradient Descent(18/49): loss=5.511925084568995e+23\n",
      "Gradient Descent(19/49): loss=1.408234884077905e+25\n",
      "Gradient Descent(20/49): loss=3.5978817896310515e+26\n",
      "Gradient Descent(21/49): loss=9.192183433716808e+27\n",
      "Gradient Descent(22/49): loss=2.3484995122044722e+29\n",
      "Gradient Descent(23/49): loss=6.000152192998567e+30\n",
      "Gradient Descent(24/49): loss=1.532971420775568e+32\n",
      "Gradient Descent(25/49): loss=3.9165696157788693e+33\n",
      "Gradient Descent(26/49): loss=1.000639499690194e+35\n",
      "Gradient Descent(27/49): loss=2.556521411763867e+36\n",
      "Gradient Descent(28/49): loss=6.531624756798706e+37\n",
      "Gradient Descent(29/49): loss=1.6687566850531787e+39\n",
      "Gradient Descent(30/49): loss=4.263485698579169e+40\n",
      "Gradient Descent(31/49): loss=1.0892726581892182e+42\n",
      "Gradient Descent(32/49): loss=2.782969165990202e+43\n",
      "Gradient Descent(33/49): loss=7.110173307505185e+44\n",
      "Gradient Descent(34/49): loss=1.8165693346721558e+46\n",
      "Gradient Descent(35/49): loss=4.6411303985909476e+47\n",
      "Gradient Descent(36/49): loss=1.185756632879216e+49\n",
      "Gradient Descent(37/49): loss=3.029474872854084e+50\n",
      "Gradient Descent(38/49): loss=7.73996767192378e+51\n",
      "Gradient Descent(39/49): loss=1.9774747134966767e+53\n",
      "Gradient Descent(40/49): loss=5.05222554960213e+54\n",
      "Gradient Descent(41/49): loss=1.2907868216892495e+56\n",
      "Gradient Descent(42/49): loss=3.297815195875107e+57\n",
      "Gradient Descent(43/49): loss=8.425547025582389e+58\n",
      "Gradient Descent(44/49): loss=2.152632529836548e+60\n",
      "Gradient Descent(45/49): loss=5.499734075948851e+61\n",
      "Gradient Descent(46/49): loss=1.4051202184726677e+63\n",
      "Gradient Descent(47/49): loss=3.5899241692336714e+64\n",
      "Gradient Descent(48/49): loss=9.171852608352922e+65\n",
      "Gradient Descent(49/49): loss=2.3433052149206908e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1170930145966167\n",
      "Gradient Descent(2/49): loss=17.986989885676326\n",
      "Gradient Descent(3/49): loss=450.62305563511296\n",
      "Gradient Descent(4/49): loss=11645.633218765292\n",
      "Gradient Descent(5/49): loss=302041.2364444497\n",
      "Gradient Descent(6/49): loss=7839986.378027545\n",
      "Gradient Descent(7/49): loss=203544393.4315802\n",
      "Gradient Descent(8/49): loss=5284817516.530674\n",
      "Gradient Descent(9/49): loss=137217217787.19382\n",
      "Gradient Descent(10/49): loss=3562783704779.054\n",
      "Gradient Descent(11/49): loss=92506222921672.17\n",
      "Gradient Descent(12/49): loss=2401887299774069.0\n",
      "Gradient Descent(13/49): loss=6.236405647228645e+16\n",
      "Gradient Descent(14/49): loss=1.6192581882789606e+18\n",
      "Gradient Descent(15/49): loss=4.204340216770012e+19\n",
      "Gradient Descent(16/49): loss=1.0916404090642207e+21\n",
      "Gradient Descent(17/49): loss=2.8344014096630058e+22\n",
      "Gradient Descent(18/49): loss=7.359411841439118e+23\n",
      "Gradient Descent(19/49): loss=1.9108423553233722e+25\n",
      "Gradient Descent(20/49): loss=4.961427061891032e+26\n",
      "Gradient Descent(21/49): loss=1.2882150336490058e+28\n",
      "Gradient Descent(22/49): loss=3.344799696177695e+29\n",
      "Gradient Descent(23/49): loss=8.684640929756522e+30\n",
      "Gradient Descent(24/49): loss=2.254932878791012e+32\n",
      "Gradient Descent(25/49): loss=5.854844580195547e+33\n",
      "Gradient Descent(26/49): loss=1.5201873803279048e+35\n",
      "Gradient Descent(27/49): loss=3.9471067756866604e+36\n",
      "Gradient Descent(28/49): loss=1.0248507585499771e+38\n",
      "Gradient Descent(29/49): loss=2.660984708521712e+39\n",
      "Gradient Descent(30/49): loss=6.909142194522826e+40\n",
      "Gradient Descent(31/49): loss=1.7939316115294482e+42\n",
      "Gradient Descent(32/49): loss=4.657872911337467e+43\n",
      "Gradient Descent(33/49): loss=1.2093983917075917e+45\n",
      "Gradient Descent(34/49): loss=3.1401553835974683e+46\n",
      "Gradient Descent(35/49): loss=8.153290016545921e+47\n",
      "Gradient Descent(36/49): loss=2.11696970287342e+49\n",
      "Gradient Descent(37/49): loss=5.496628617146342e+50\n",
      "Gradient Descent(38/49): loss=1.4271780136401166e+52\n",
      "Gradient Descent(39/49): loss=3.7056116112046964e+53\n",
      "Gradient Descent(40/49): loss=9.621474883901712e+54\n",
      "Gradient Descent(41/49): loss=2.498178132366532e+56\n",
      "Gradient Descent(42/49): loss=6.486421319330595e+57\n",
      "Gradient Descent(43/49): loss=1.6841737979673165e+59\n",
      "Gradient Descent(44/49): loss=4.372891062914145e+60\n",
      "Gradient Descent(45/49): loss=1.1354039750050476e+62\n",
      "Gradient Descent(46/49): loss=2.9480317892899145e+63\n",
      "Gradient Descent(47/49): loss=7.6544486561492e+64\n",
      "Gradient Descent(48/49): loss=1.9874475045513935e+66\n",
      "Gradient Descent(49/49): loss=5.1603293206159215e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1264121268516452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2/49): loss=18.66385493509046\n",
      "Gradient Descent(3/49): loss=478.3032351103277\n",
      "Gradient Descent(4/49): loss=12625.11368656025\n",
      "Gradient Descent(5/49): loss=334315.9973551567\n",
      "Gradient Descent(6/49): loss=8858677.936234983\n",
      "Gradient Descent(7/49): loss=234777016.7950935\n",
      "Gradient Descent(8/49): loss=6222464643.514979\n",
      "Gradient Descent(9/49): loss=164920552899.7724\n",
      "Gradient Descent(10/49): loss=4371078646164.2324\n",
      "Gradient Descent(11/49): loss=115851821372503.06\n",
      "Gradient Descent(12/49): loss=3070557413535889.0\n",
      "Gradient Descent(13/49): loss=8.138260904346474e+16\n",
      "Gradient Descent(14/49): loss=2.156979430207915e+18\n",
      "Gradient Descent(15/49): loss=5.716897431841386e+19\n",
      "Gradient Descent(16/49): loss=1.515216873859692e+21\n",
      "Gradient Descent(17/49): loss=4.015958311447465e+22\n",
      "Gradient Descent(18/49): loss=1.0643968820045412e+24\n",
      "Gradient Descent(19/49): loss=2.8210968207110246e+25\n",
      "Gradient Descent(20/49): loss=7.477086232063641e+26\n",
      "Gradient Descent(21/49): loss=1.981740509981617e+28\n",
      "Gradient Descent(22/49): loss=5.252441027178778e+29\n",
      "Gradient Descent(23/49): loss=1.3921165059219086e+31\n",
      "Gradient Descent(24/49): loss=3.6896908618910256e+32\n",
      "Gradient Descent(25/49): loss=9.779223648603141e+33\n",
      "Gradient Descent(26/49): loss=2.591903190512442e+35\n",
      "Gradient Descent(27/49): loss=6.86962727347806e+36\n",
      "Gradient Descent(28/49): loss=1.8207384847264907e+38\n",
      "Gradient Descent(29/49): loss=4.8257183363686336e+39\n",
      "Gradient Descent(30/49): loss=1.2790171492125382e+41\n",
      "Gradient Descent(31/49): loss=3.389930273491218e+42\n",
      "Gradient Descent(32/49): loss=8.984732742799728e+43\n",
      "Gradient Descent(33/49): loss=2.3813298783989422e+45\n",
      "Gradient Descent(34/49): loss=6.311519943984993e+46\n",
      "Gradient Descent(35/49): loss=1.672816704844899e+48\n",
      "Gradient Descent(36/49): loss=4.433663765374004e+49\n",
      "Gradient Descent(37/49): loss=1.175106293920763e+51\n",
      "Gradient Descent(38/49): loss=3.1145230560705314e+52\n",
      "Gradient Descent(39/49): loss=8.25478845358737e+53\n",
      "Gradient Descent(40/49): loss=2.1878641187344668e+55\n",
      "Gradient Descent(41/49): loss=5.798754782099255e+56\n",
      "Gradient Descent(42/49): loss=1.5369124953870132e+58\n",
      "Gradient Descent(43/49): loss=4.073460781215522e+59\n",
      "Gradient Descent(44/49): loss=1.0796374410322389e+61\n",
      "Gradient Descent(45/49): loss=2.86149067508838e+62\n",
      "Gradient Descent(46/49): loss=7.584146837098477e+63\n",
      "Gradient Descent(47/49): loss=2.010116047115684e+65\n",
      "Gradient Descent(48/49): loss=5.3276480659726325e+66\n",
      "Gradient Descent(49/49): loss=1.4120495160261854e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1248660785310611\n",
      "Gradient Descent(2/49): loss=18.533577901519518\n",
      "Gradient Descent(3/49): loss=475.0685675098853\n",
      "Gradient Descent(4/49): loss=12560.088749153008\n",
      "Gradient Descent(5/49): loss=333262.9984633755\n",
      "Gradient Descent(6/49): loss=8849581.931366054\n",
      "Gradient Descent(7/49): loss=235043975.5627972\n",
      "Gradient Descent(8/49): loss=6243103135.939464\n",
      "Gradient Descent(9/49): loss=165828374255.60782\n",
      "Gradient Descent(10/49): loss=4404728148266.216\n",
      "Gradient Descent(11/49): loss=116998397814913.72\n",
      "Gradient Descent(12/49): loss=3107712746969968.5\n",
      "Gradient Descent(13/49): loss=8.254710855554222e+16\n",
      "Gradient Descent(14/49): loss=2.1926174868428685e+18\n",
      "Gradient Descent(15/49): loss=5.82403377117387e+19\n",
      "Gradient Descent(16/49): loss=1.5469807041868545e+21\n",
      "Gradient Descent(17/49): loss=4.109092416930291e+22\n",
      "Gradient Descent(18/49): loss=1.0914577310427683e+24\n",
      "Gradient Descent(19/49): loss=2.8991316276996163e+25\n",
      "Gradient Descent(20/49): loss=7.700677686164646e+26\n",
      "Gradient Descent(21/49): loss=2.0454551376631583e+28\n",
      "Gradient Descent(22/49): loss=5.433140939937824e+29\n",
      "Gradient Descent(23/49): loss=1.4431516941975293e+31\n",
      "Gradient Descent(24/49): loss=3.8333016490626835e+32\n",
      "Gradient Descent(25/49): loss=1.0182021468559312e+34\n",
      "Gradient Descent(26/49): loss=2.704550037473668e+35\n",
      "Gradient Descent(27/49): loss=7.183829780545256e+36\n",
      "Gradient Descent(28/49): loss=1.9081699210880836e+38\n",
      "Gradient Descent(29/49): loss=5.068483746101471e+39\n",
      "Gradient Descent(30/49): loss=1.3462913968293784e+41\n",
      "Gradient Descent(31/49): loss=3.576021184976486e+42\n",
      "Gradient Descent(32/49): loss=9.498632722096607e+43\n",
      "Gradient Descent(33/49): loss=2.5230282182983776e+45\n",
      "Gradient Descent(34/49): loss=6.70167125792902e+46\n",
      "Gradient Descent(35/49): loss=1.7800989035169177e+48\n",
      "Gradient Descent(36/49): loss=4.728301321186173e+49\n",
      "Gradient Descent(37/49): loss=1.2559320911754368e+51\n",
      "Gradient Descent(38/49): loss=3.33600866462672e+52\n",
      "Gradient Descent(39/49): loss=8.861111113140574e+53\n",
      "Gradient Descent(40/49): loss=2.353689634922129e+55\n",
      "Gradient Descent(41/49): loss=6.251873864130324e+56\n",
      "Gradient Descent(42/49): loss=1.6606236537337322e+58\n",
      "Gradient Descent(43/49): loss=4.410950987290249e+59\n",
      "Gradient Descent(44/49): loss=1.1716374488904196e+61\n",
      "Gradient Descent(45/49): loss=3.1121051120220223e+62\n",
      "Gradient Descent(46/49): loss=8.266378167961225e+63\n",
      "Gradient Descent(47/49): loss=2.1957165827007817e+65\n",
      "Gradient Descent(48/49): loss=5.832265610873015e+66\n",
      "Gradient Descent(49/49): loss=1.549167247893737e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1254780161165565\n",
      "Gradient Descent(2/49): loss=18.435164022907443\n",
      "Gradient Descent(3/49): loss=467.9112070829259\n",
      "Gradient Descent(4/49): loss=12243.328680800192\n",
      "Gradient Descent(5/49): loss=321460.8504008418\n",
      "Gradient Descent(6/49): loss=8446551.015440768\n",
      "Gradient Descent(7/49): loss=221981442.1532581\n",
      "Gradient Descent(8/49): loss=5834152512.054875\n",
      "Gradient Descent(9/49): loss=153336506050.78253\n",
      "Gradient Descent(10/49): loss=4030094417322.1016\n",
      "Gradient Descent(11/49): loss=105921812670440.98\n",
      "Gradient Descent(12/49): loss=2783913489298315.5\n",
      "Gradient Descent(13/49): loss=7.316882953435077e+16\n",
      "Gradient Descent(14/49): loss=1.923076194503618e+18\n",
      "Gradient Descent(15/49): loss=5.054368234691271e+19\n",
      "Gradient Descent(16/49): loss=1.3284256951865164e+21\n",
      "Gradient Descent(17/49): loss=3.4914647028796507e+22\n",
      "Gradient Descent(18/49): loss=9.17652061207104e+23\n",
      "Gradient Descent(19/49): loss=2.4118396636875147e+25\n",
      "Gradient Descent(20/49): loss=6.338971827471105e+26\n",
      "Gradient Descent(21/49): loss=1.6660545240467812e+28\n",
      "Gradient Descent(22/49): loss=4.378845264890519e+29\n",
      "Gradient Descent(23/49): loss=1.1508798527965402e+31\n",
      "Gradient Descent(24/49): loss=3.024825851219478e+32\n",
      "Gradient Descent(25/49): loss=7.950066558184314e+33\n",
      "Gradient Descent(26/49): loss=2.089494119275666e+35\n",
      "Gradient Descent(27/49): loss=5.49175990230295e+36\n",
      "Gradient Descent(28/49): loss=1.443384144818622e+38\n",
      "Gradient Descent(29/49): loss=3.7936068338314105e+39\n",
      "Gradient Descent(30/49): loss=9.970632462157812e+40\n",
      "Gradient Descent(31/49): loss=2.6205538963306613e+42\n",
      "Gradient Descent(32/49): loss=6.887529702491482e+43\n",
      "Gradient Descent(33/49): loss=1.8102304810111317e+45\n",
      "Gradient Descent(34/49): loss=4.757778965652103e+46\n",
      "Gradient Descent(35/49): loss=1.2504739547506395e+48\n",
      "Gradient Descent(36/49): loss=3.2865862891034685e+49\n",
      "Gradient Descent(37/49): loss=8.638044314866973e+50\n",
      "Gradient Descent(38/49): loss=2.2703134201280715e+52\n",
      "Gradient Descent(39/49): loss=5.9670022955803484e+53\n",
      "Gradient Descent(40/49): loss=1.5682907954379597e+55\n",
      "Gradient Descent(41/49): loss=4.121895546909987e+56\n",
      "Gradient Descent(42/49): loss=1.0833464654041883e+58\n",
      "Gradient Descent(43/49): loss=2.847329707283778e+59\n",
      "Gradient Descent(44/49): loss=7.483558326796162e+60\n",
      "Gradient Descent(45/49): loss=1.9668830443940738e+62\n",
      "Gradient Descent(46/49): loss=5.169504587774262e+63\n",
      "Gradient Descent(47/49): loss=1.358686667170483e+65\n",
      "Gradient Descent(48/49): loss=3.570998783737704e+66\n",
      "Gradient Descent(49/49): loss=9.385557849046021e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1377073475095065\n",
      "Gradient Descent(2/49): loss=19.00211944247537\n",
      "Gradient Descent(3/49): loss=490.0376900093245\n",
      "Gradient Descent(4/49): loss=13023.499640900267\n",
      "Gradient Descent(5/49): loss=347347.619279144\n",
      "Gradient Descent(6/49): loss=9271498.208697755\n",
      "Gradient Descent(7/49): loss=247532257.86764517\n",
      "Gradient Descent(8/49): loss=6609083804.549168\n",
      "Gradient Descent(9/49): loss=176465035417.29736\n",
      "Gradient Descent(10/49): loss=4711708074836.988\n",
      "Gradient Descent(11/49): loss=125805244666837.25\n",
      "Gradient Descent(12/49): loss=3359071983868267.5\n",
      "Gradient Descent(13/49): loss=8.968915458979574e+16\n",
      "Gradient Descent(14/49): loss=2.3947521182877665e+18\n",
      "Gradient Descent(15/49): loss=6.3941262415537e+19\n",
      "Gradient Descent(16/49): loss=1.7072685814674626e+21\n",
      "Gradient Descent(17/49): loss=4.5585055749051544e+22\n",
      "Gradient Descent(18/49): loss=1.2171472788402383e+24\n",
      "Gradient Descent(19/49): loss=3.249853431472523e+25\n",
      "Gradient Descent(20/49): loss=8.67729609217567e+26\n",
      "Gradient Descent(21/49): loss=2.3168881015548604e+28\n",
      "Gradient Descent(22/49): loss=6.1862248540493294e+29\n",
      "Gradient Descent(23/49): loss=1.6517577141164023e+31\n",
      "Gradient Descent(24/49): loss=4.410288359236733e+32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=1.17757242756544e+34\n",
      "Gradient Descent(26/49): loss=3.1441862962501702e+35\n",
      "Gradient Descent(27/49): loss=8.395158747021578e+36\n",
      "Gradient Descent(28/49): loss=2.2415558032215582e+38\n",
      "Gradient Descent(29/49): loss=5.985083272831335e+39\n",
      "Gradient Descent(30/49): loss=1.5980517518789043e+41\n",
      "Gradient Descent(31/49): loss=4.266890342655387e+42\n",
      "Gradient Descent(32/49): loss=1.139284330112577e+44\n",
      "Gradient Descent(33/49): loss=3.041954867844827e+45\n",
      "Gradient Descent(34/49): loss=8.122194937141314e+46\n",
      "Gradient Descent(35/49): loss=2.1686728917074025e+48\n",
      "Gradient Descent(36/49): loss=5.79048169568048e+49\n",
      "Gradient Descent(37/49): loss=1.5460920084454318e+51\n",
      "Gradient Descent(38/49): loss=4.128154830990991e+52\n",
      "Gradient Descent(39/49): loss=1.1022411483627902e+54\n",
      "Gradient Descent(40/49): loss=2.9430474361652516e+55\n",
      "Gradient Descent(41/49): loss=7.858106390225208e+56\n",
      "Gradient Descent(42/49): loss=2.0981597265913416e+58\n",
      "Gradient Descent(43/49): loss=5.602207478083891e+59\n",
      "Gradient Descent(44/49): loss=1.495821706505004e+61\n",
      "Gradient Descent(45/49): loss=3.99393022554892e+62\n",
      "Gradient Descent(46/49): loss=1.066402404590303e+64\n",
      "Gradient Descent(47/49): loss=2.847355923349144e+65\n",
      "Gradient Descent(48/49): loss=7.602604532147725e+66\n",
      "Gradient Descent(49/49): loss=2.0299392569175962e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.147320086451029\n",
      "Gradient Descent(2/49): loss=19.716144894063206\n",
      "Gradient Descent(3/49): loss=520.0646512069171\n",
      "Gradient Descent(4/49): loss=14116.18673388786\n",
      "Gradient Descent(5/49): loss=384373.398136018\n",
      "Gradient Descent(6/49): loss=10473258.096708776\n",
      "Gradient Descent(7/49): loss=285421365.7095061\n",
      "Gradient Descent(8/49): loss=7778785307.183166\n",
      "Gradient Descent(9/49): loss=212003348640.66592\n",
      "Gradient Descent(10/49): loss=5777968842205.623\n",
      "Gradient Descent(11/49): loss=157473721629969.25\n",
      "Gradient Descent(12/49): loss=4291816079096484.0\n",
      "Gradient Descent(13/49): loss=1.1696990712397493e+17\n",
      "Gradient Descent(14/49): loss=3.187918399135673e+18\n",
      "Gradient Descent(15/49): loss=8.688408860569598e+19\n",
      "Gradient Descent(16/49): loss=2.3679542318617524e+21\n",
      "Gradient Descent(17/49): loss=6.4536641132526855e+22\n",
      "Gradient Descent(18/49): loss=1.7588929689140883e+24\n",
      "Gradient Descent(19/49): loss=4.7937178351700145e+25\n",
      "Gradient Descent(20/49): loss=1.3064882906210025e+27\n",
      "Gradient Descent(21/49): loss=3.5607261675074012e+28\n",
      "Gradient Descent(22/49): loss=9.704465727705295e+29\n",
      "Gradient Descent(23/49): loss=2.64487215893236e+31\n",
      "Gradient Descent(24/49): loss=7.208381103479825e+32\n",
      "Gradient Descent(25/49): loss=1.9645848650008202e+34\n",
      "Gradient Descent(26/49): loss=5.354314146802143e+35\n",
      "Gradient Descent(27/49): loss=1.4592741954485986e+37\n",
      "Gradient Descent(28/49): loss=3.977131559929092e+38\n",
      "Gradient Descent(29/49): loss=1.0839344308504965e+40\n",
      "Gradient Descent(30/49): loss=2.954173963518922e+41\n",
      "Gradient Descent(31/49): loss=8.051357682111334e+42\n",
      "Gradient Descent(32/49): loss=2.1943311844802903e+44\n",
      "Gradient Descent(33/49): loss=5.980468806995066e+45\n",
      "Gradient Descent(34/49): loss=1.62992748790158e+47\n",
      "Gradient Descent(35/49): loss=4.442233044857288e+48\n",
      "Gradient Descent(36/49): loss=1.2106940076351132e+50\n",
      "Gradient Descent(37/49): loss=3.2996467437035615e+51\n",
      "Gradient Descent(38/49): loss=8.992915273860857e+52\n",
      "Gradient Descent(39/49): loss=2.4509449466722907e+54\n",
      "Gradient Descent(40/49): loss=6.679848468136915e+55\n",
      "Gradient Descent(41/49): loss=1.8205376509110627e+57\n",
      "Gradient Descent(42/49): loss=4.961725335828164e+58\n",
      "Gradient Descent(43/49): loss=1.3522773503683771e+60\n",
      "Gradient Descent(44/49): loss=3.685520476344774e+61\n",
      "Gradient Descent(45/49): loss=1.0044582332061056e+63\n",
      "Gradient Descent(46/49): loss=2.737568136525938e+64\n",
      "Gradient Descent(47/49): loss=7.461016351273557e+65\n",
      "Gradient Descent(48/49): loss=2.0334385198029782e+67\n",
      "Gradient Descent(49/49): loss=5.541969108689538e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.145681231580127\n",
      "Gradient Descent(2/49): loss=19.57839614714965\n",
      "Gradient Descent(3/49): loss=516.5439395715287\n",
      "Gradient Descent(4/49): loss=14043.255702976765\n",
      "Gradient Descent(5/49): loss=383153.85327749804\n",
      "Gradient Descent(6/49): loss=10462201.36913248\n",
      "Gradient Descent(7/49): loss=285736142.63678366\n",
      "Gradient Descent(8/49): loss=7804279729.746932\n",
      "Gradient Descent(9/49): loss=213160927916.23737\n",
      "Gradient Descent(10/49): loss=5822163094426.905\n",
      "Gradient Descent(11/49): loss=159023639302313.44\n",
      "Gradient Descent(12/49): loss=4343493389060020.0\n",
      "Gradient Descent(13/49): loss=1.1863605179507424e+17\n",
      "Gradient Descent(14/49): loss=3.240367123553906e+18\n",
      "Gradient Descent(15/49): loss=8.850580425303378e+19\n",
      "Gradient Descent(16/49): loss=2.4174042955388546e+21\n",
      "Gradient Descent(17/49): loss=6.602779988252549e+22\n",
      "Gradient Descent(18/49): loss=1.8034510677908317e+24\n",
      "Gradient Descent(19/49): loss=4.925858138326376e+25\n",
      "Gradient Descent(20/49): loss=1.3454248264668918e+27\n",
      "Gradient Descent(21/49): loss=3.6748276398703873e+28\n",
      "Gradient Descent(22/49): loss=1.0037244680724377e+30\n",
      "Gradient Descent(23/49): loss=2.7415239748309213e+31\n",
      "Gradient Descent(24/49): loss=7.48806464687171e+32\n",
      "Gradient Descent(25/49): loss=2.045253394480683e+34\n",
      "Gradient Descent(26/49): loss=5.586305200212651e+35\n",
      "Gradient Descent(27/49): loss=1.5258161103234234e+37\n",
      "Gradient Descent(28/49): loss=4.167539579530812e+38\n",
      "Gradient Descent(29/49): loss=1.1383014001126465e+40\n",
      "Gradient Descent(30/49): loss=3.109100832209214e+41\n",
      "Gradient Descent(31/49): loss=8.49204611703675e+42\n",
      "Gradient Descent(32/49): loss=2.3194759882598203e+44\n",
      "Gradient Descent(33/49): loss=6.335303395633414e+45\n",
      "Gradient Descent(34/49): loss=1.7303938181673568e+47\n",
      "Gradient Descent(35/49): loss=4.726313136030089e+48\n",
      "Gradient Descent(36/49): loss=1.2909220794297846e+50\n",
      "Gradient Descent(37/49): loss=3.5259614993666965e+51\n",
      "Gradient Descent(38/49): loss=9.630638977456957e+52\n",
      "Gradient Descent(39/49): loss=2.6304656795252058e+54\n",
      "Gradient Descent(40/49): loss=7.184725444860471e+55\n",
      "Gradient Descent(41/49): loss=1.9624008068161778e+57\n",
      "Gradient Descent(42/49): loss=5.360005690054008e+58\n",
      "Gradient Descent(43/49): loss=1.4640057677117808e+60\n",
      "Gradient Descent(44/49): loss=3.9987138294843145e+61\n",
      "Gradient Descent(45/49): loss=1.0921891595482438e+63\n",
      "Gradient Descent(46/49): loss=2.9831521111590264e+64\n",
      "Gradient Descent(47/49): loss=8.14803593362305e+65\n",
      "Gradient Descent(48/49): loss=2.2255147274343294e+67\n",
      "Gradient Descent(49/49): loss=6.078662198320437e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.146378552002079\n",
      "Gradient Descent(2/49): loss=19.47521195065662\n",
      "Gradient Descent(3/49): loss=508.796110054412\n",
      "Gradient Descent(4/49): loss=13690.379374337308\n",
      "Gradient Descent(5/49): loss=369628.81431013625\n",
      "Gradient Descent(6/49): loss=9987156.27873983\n",
      "Gradient Descent(7/49): loss=269901461.5869504\n",
      "Gradient Descent(8/49): loss=7294457225.218448\n",
      "Gradient Descent(9/49): loss=197145819449.16528\n",
      "Gradient Descent(10/49): loss=5328243821192.458\n",
      "Gradient Descent(11/49): loss=144006186939068.3\n",
      "Gradient Descent(12/49): loss=3892049591641963.0\n",
      "Gradient Descent(13/49): loss=1.051902828020369e+17\n",
      "Gradient Descent(14/49): loss=2.8429740283255644e+18\n",
      "Gradient Descent(15/49): loss=7.683695845540877e+19\n",
      "Gradient Descent(16/49): loss=2.0766697616654423e+21\n",
      "Gradient Descent(17/49): loss=5.612608031882584e+22\n",
      "Gradient Descent(18/49): loss=1.5169175911659603e+24\n",
      "Gradient Descent(19/49): loss=4.099767818155831e+25\n",
      "Gradient Descent(20/49): loss=1.1080428008025951e+27\n",
      "Gradient Descent(21/49): loss=2.99470336583011e+28\n",
      "Gradient Descent(22/49): loss=8.093774214162337e+29\n",
      "Gradient Descent(23/49): loss=2.187501499391434e+31\n",
      "Gradient Descent(24/49): loss=5.912152579531126e+32\n",
      "Gradient Descent(25/49): loss=1.5978753904114547e+34\n",
      "Gradient Descent(26/49): loss=4.318572176439097e+35\n",
      "Gradient Descent(27/49): loss=1.1671789774740592e+37\n",
      "Gradient Descent(28/49): loss=3.1545305017471907e+38\n",
      "Gradient Descent(29/49): loss=8.525738450146563e+39\n",
      "Gradient Descent(30/49): loss=2.304248320948171e+41\n",
      "Gradient Descent(31/49): loss=6.227683801983374e+42\n",
      "Gradient Descent(32/49): loss=1.683153902506786e+44\n",
      "Gradient Descent(33/49): loss=4.549054110007274e+45\n",
      "Gradient Descent(34/49): loss=1.229471248288935e+47\n",
      "Gradient Descent(35/49): loss=3.3228876021585374e+48\n",
      "Gradient Descent(36/49): loss=8.98075659105128e+49\n",
      "Gradient Descent(37/49): loss=2.427225913248425e+51\n",
      "Gradient Descent(38/49): loss=6.560054906526511e+52\n",
      "Gradient Descent(39/49): loss=1.772983723589549e+54\n",
      "Gradient Descent(40/49): loss=4.7918368503075934e+55\n",
      "Gradient Descent(41/49): loss=1.2950880537965714e+57\n",
      "Gradient Descent(42/49): loss=3.5002299107469146e+58\n",
      "Gradient Descent(43/49): loss=9.460059022374218e+59\n",
      "Gradient Descent(44/49): loss=2.5567668121465476e+61\n",
      "Gradient Descent(45/49): loss=6.910164636640281e+62\n",
      "Gradient Descent(46/49): loss=1.8676077567427692e+64\n",
      "Gradient Descent(47/49): loss=5.047576890644997e+65\n",
      "Gradient Descent(48/49): loss=1.364206824210707e+67\n",
      "Gradient Descent(49/49): loss=3.6870369675245224e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.158632248215282\n",
      "Gradient Descent(2/49): loss=20.060586123665843\n",
      "Gradient Descent(3/49): loss=532.2870915453632\n",
      "Gradient Descent(4/49): loss=14541.964945252603\n",
      "Gradient Descent(5/49): loss=398681.2715906352\n",
      "Gradient Descent(6/49): loss=10939076.490260478\n",
      "Gradient Descent(7/49): loss=300215622.29273504\n",
      "Gradient Descent(8/49): loss=8239750061.164076\n",
      "Gradient Descent(9/49): loss=226153301375.8224\n",
      "Gradient Descent(10/49): loss=6207177832175.025\n",
      "Gradient Descent(11/49): loss=170367256352427.66\n",
      "Gradient Descent(12/49): loss=4676040565500844.0\n",
      "Gradient Descent(13/49): loss=1.2834249226047278e+17\n",
      "Gradient Descent(14/49): loss=3.522594751967255e+18\n",
      "Gradient Descent(15/49): loss=9.668406548156056e+19\n",
      "Gradient Descent(16/49): loss=2.6536712868450256e+21\n",
      "Gradient Descent(17/49): loss=7.283486963658359e+22\n",
      "Gradient Descent(18/49): loss=1.9990864214164248e+24\n",
      "Gradient Descent(19/49): loss=5.486858891284109e+25\n",
      "Gradient Descent(20/49): loss=1.505968935161352e+27\n",
      "Gradient Descent(21/49): loss=4.133407617405459e+28\n",
      "Gradient Descent(22/49): loss=1.1344894395068136e+30\n",
      "Gradient Descent(23/49): loss=3.113814091147178e+31\n",
      "Gradient Descent(24/49): loss=8.546433185348419e+32\n",
      "Gradient Descent(25/49): loss=2.3457251477951264e+34\n",
      "Gradient Descent(26/49): loss=6.438272375932991e+35\n",
      "Gradient Descent(27/49): loss=1.7671017947547785e+37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(28/49): loss=4.850134586878302e+38\n",
      "Gradient Descent(29/49): loss=1.331208285830391e+40\n",
      "Gradient Descent(30/49): loss=3.653745001340397e+41\n",
      "Gradient Descent(31/49): loss=1.0028372477033164e+43\n",
      "Gradient Descent(32/49): loss=2.752470533691384e+44\n",
      "Gradient Descent(33/49): loss=7.554659598244862e+45\n",
      "Gradient Descent(34/49): loss=2.0735147187502044e+47\n",
      "Gradient Descent(35/49): loss=5.6911409878383094e+48\n",
      "Gradient Descent(36/49): loss=1.562037898770034e+50\n",
      "Gradient Descent(37/49): loss=4.287299159180891e+51\n",
      "Gradient Descent(38/49): loss=1.1767277922505293e+53\n",
      "Gradient Descent(39/49): loss=3.229744987796361e+54\n",
      "Gradient Descent(40/49): loss=8.864626768307807e+55\n",
      "Gradient Descent(41/49): loss=2.4330592055509236e+57\n",
      "Gradient Descent(42/49): loss=6.677976695962034e+58\n",
      "Gradient Descent(43/49): loss=1.8328930364731575e+60\n",
      "Gradient Descent(44/49): loss=5.0307107019154407e+61\n",
      "Gradient Descent(45/49): loss=1.3807707085332262e+63\n",
      "Gradient Descent(46/49): loss=3.7897781496706436e+64\n",
      "Gradient Descent(47/49): loss=1.0401740372214326e+66\n",
      "Gradient Descent(48/49): loss=2.854948192161504e+67\n",
      "Gradient Descent(49/49): loss=7.835928304554555e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1685428926821764\n",
      "Gradient Descent(2/49): loss=20.81327362116735\n",
      "Gradient Descent(3/49): loss=564.8241711647289\n",
      "Gradient Descent(4/49): loss=15759.140081979851\n",
      "Gradient Descent(5/49): loss=441078.2933825738\n",
      "Gradient Descent(6/49): loss=12353617.575576099\n",
      "Gradient Descent(7/49): loss=346058945.8110937\n",
      "Gradient Descent(8/49): loss=9694537103.842905\n",
      "Gradient Descent(9/49): loss=271587543333.03305\n",
      "Gradient Descent(10/49): loss=7608415133758.688\n",
      "Gradient Descent(11/49): loss=213146884953380.4\n",
      "Gradient Descent(12/49): loss=5971231363074663.0\n",
      "Gradient Descent(13/49): loss=1.672818568050908e+17\n",
      "Gradient Descent(14/49): loss=4.686339968404092e+18\n",
      "Gradient Descent(15/49): loss=1.3128609906784418e+20\n",
      "Gradient Descent(16/49): loss=3.6779320219348925e+21\n",
      "Gradient Descent(17/49): loss=1.0303591971917792e+23\n",
      "Gradient Descent(18/49): loss=2.8865135870693513e+24\n",
      "Gradient Descent(19/49): loss=8.086462188443835e+25\n",
      "Gradient Descent(20/49): loss=2.2653927914507593e+27\n",
      "Gradient Descent(21/49): loss=6.346415008164717e+28\n",
      "Gradient Descent(22/49): loss=1.7779249412236424e+30\n",
      "Gradient Descent(23/49): loss=4.980791663575412e+31\n",
      "Gradient Descent(24/49): loss=1.3953505584363607e+33\n",
      "Gradient Descent(25/49): loss=3.9090235296673163e+34\n",
      "Gradient Descent(26/49): loss=1.0950986376223712e+36\n",
      "Gradient Descent(27/49): loss=3.06787876056717e+37\n",
      "Gradient Descent(28/49): loss=8.5945500854369e+38\n",
      "Gradient Descent(29/49): loss=2.407731756564841e+40\n",
      "Gradient Descent(30/49): loss=6.745172410355558e+41\n",
      "Gradient Descent(31/49): loss=1.889635368282622e+43\n",
      "Gradient Descent(32/49): loss=5.29374433718354e+44\n",
      "Gradient Descent(33/49): loss=1.4830231047660647e+46\n",
      "Gradient Descent(34/49): loss=4.154634960025534e+47\n",
      "Gradient Descent(35/49): loss=1.163905781076088e+49\n",
      "Gradient Descent(36/49): loss=3.2606394551063207e+50\n",
      "Gradient Descent(37/49): loss=9.13456211753367e+51\n",
      "Gradient Descent(38/49): loss=2.5590141513012003e+53\n",
      "Gradient Descent(39/49): loss=7.168984503362164e+54\n",
      "Gradient Descent(40/49): loss=2.008364775291074e+56\n",
      "Gradient Descent(41/49): loss=5.626360426275535e+57\n",
      "Gradient Descent(42/49): loss=1.576204284989582e+59\n",
      "Gradient Descent(43/49): loss=4.415678626660869e+60\n",
      "Gradient Descent(44/49): loss=1.2370362090519516e+62\n",
      "Gradient Descent(45/49): loss=3.465511673033141e+63\n",
      "Gradient Descent(46/49): loss=9.708504139206325e+64\n",
      "Gradient Descent(47/49): loss=2.7198019084578774e+66\n",
      "Gradient Descent(48/49): loss=7.619425521361432e+67\n",
      "Gradient Descent(49/49): loss=2.1345541781934706e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1668094959645245\n",
      "Gradient Descent(2/49): loss=20.667741949033314\n",
      "Gradient Descent(3/49): loss=560.9966901288694\n",
      "Gradient Descent(4/49): loss=15677.475624947685\n",
      "Gradient Descent(5/49): loss=439668.93310504407\n",
      "Gradient Descent(6/49): loss=12340228.359607926\n",
      "Gradient Descent(7/49): loss=346429078.94059724\n",
      "Gradient Descent(8/49): loss=9725939323.994057\n",
      "Gradient Descent(9/49): loss=273058734340.40744\n",
      "Gradient Descent(10/49): loss=7666243989489.452\n",
      "Gradient Descent(11/49): loss=215233454375848.28\n",
      "Gradient Descent(12/49): loss=6042784072628732.0\n",
      "Gradient Descent(13/49): loss=1.6965412397299453e+17\n",
      "Gradient Descent(14/49): loss=4.763122805060014e+18\n",
      "Gradient Descent(15/49): loss=1.337270118220598e+20\n",
      "Gradient Descent(16/49): loss=3.754451536086868e+21\n",
      "Gradient Descent(17/49): loss=1.0540807091905156e+23\n",
      "Gradient Descent(18/49): loss=2.95938336401351e+24\n",
      "Gradient Descent(19/49): loss=8.3086141500221e+25\n",
      "Gradient Descent(20/49): loss=2.332684231937776e+27\n",
      "Gradient Descent(21/49): loss=6.549125555359254e+28\n",
      "Gradient Descent(22/49): loss=1.8386991669368898e+30\n",
      "Gradient Descent(23/49): loss=5.162238222366803e+31\n",
      "Gradient Descent(24/49): loss=1.4493237362402038e+33\n",
      "Gradient Descent(25/49): loss=4.0690475757746857e+34\n",
      "Gradient Descent(26/49): loss=1.1424050927965976e+36\n",
      "Gradient Descent(27/49): loss=3.207358409416324e+37\n",
      "Gradient Descent(28/49): loss=9.004816269919404e+38\n",
      "Gradient Descent(29/49): loss=2.528146396640507e+40\n",
      "Gradient Descent(30/49): loss=7.097895183266894e+41\n",
      "Gradient Descent(31/49): loss=1.9927689353587403e+43\n",
      "Gradient Descent(32/49): loss=5.594796664640274e+44\n",
      "Gradient Descent(33/49): loss=1.570766643501244e+46\n",
      "Gradient Descent(34/49): loss=4.41000450280853e+47\n",
      "Gradient Descent(35/49): loss=1.2381304247358802e+49\n",
      "Gradient Descent(36/49): loss=3.4761119805668674e+50\n",
      "Gradient Descent(37/49): loss=9.759355121265278e+51\n",
      "Gradient Descent(38/49): loss=2.7399868852163475e+53\n",
      "Gradient Descent(39/49): loss=7.692647759890333e+54\n",
      "Gradient Descent(40/49): loss=2.159748642485687e+56\n",
      "Gradient Descent(41/49): loss=6.0636003939237544e+57\n",
      "Gradient Descent(42/49): loss=1.702385593115892e+59\n",
      "Gradient Descent(43/49): loss=4.779531168565631e+60\n",
      "Gradient Descent(44/49): loss=1.3418768511473947e+62\n",
      "Gradient Descent(45/49): loss=3.767385168419412e+63\n",
      "Gradient Descent(46/49): loss=1.0577118902595586e+65\n",
      "Gradient Descent(47/49): loss=2.9695780834265418e+66\n",
      "Gradient Descent(48/49): loss=8.33723632567203e+67\n",
      "Gradient Descent(49/49): loss=2.340720048347725e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1675940174455095\n",
      "Gradient Descent(2/49): loss=20.559617274846463\n",
      "Gradient Descent(3/49): loss=552.6184524764931\n",
      "Gradient Descent(4/49): loss=15284.934115717417\n",
      "Gradient Descent(5/49): loss=424198.00019401446\n",
      "Gradient Descent(6/49): loss=11781548.736846657\n",
      "Gradient Descent(7/49): loss=327284116.9762306\n",
      "Gradient Descent(8/49): loss=9092270133.95355\n",
      "Gradient Descent(9/49): loss=252596159657.37335\n",
      "Gradient Descent(10/49): loss=7017511601754.883\n",
      "Gradient Descent(11/49): loss=194957566846999.97\n",
      "Gradient Descent(12/49): loss=5416231410564857.0\n",
      "Gradient Descent(13/49): loss=1.5047154209594336e+17\n",
      "Gradient Descent(14/49): loss=4.180339414460234e+18\n",
      "Gradient Descent(15/49): loss=1.1613649678486679e+20\n",
      "Gradient Descent(16/49): loss=3.226457131787791e+21\n",
      "Gradient Descent(17/49): loss=8.963612575170112e+22\n",
      "Gradient Descent(18/49): loss=2.4902345554584524e+24\n",
      "Gradient Descent(19/49): loss=6.918268822446488e+25\n",
      "Gradient Descent(20/49): loss=1.9220054350078305e+27\n",
      "Gradient Descent(21/49): loss=5.3396376854130875e+28\n",
      "Gradient Descent(22/49): loss=1.4834365237578905e+30\n",
      "Gradient Descent(23/49): loss=4.121223292043307e+31\n",
      "Gradient Descent(24/49): loss=1.1449415698526967e+33\n",
      "Gradient Descent(25/49): loss=3.1808303153767135e+34\n",
      "Gradient Descent(26/49): loss=8.836854003406666e+35\n",
      "Gradient Descent(27/49): loss=2.455019002429126e+37\n",
      "Gradient Descent(28/49): loss=6.820434398898769e+38\n",
      "Gradient Descent(29/49): loss=1.8948254715606703e+40\n",
      "Gradient Descent(30/49): loss=5.264127411378424e+41\n",
      "Gradient Descent(31/49): loss=1.4624585651364162e+43\n",
      "Gradient Descent(32/49): loss=4.062943176713179e+44\n",
      "Gradient Descent(33/49): loss=1.1287504241640054e+46\n",
      "Gradient Descent(34/49): loss=3.1358487299374926e+47\n",
      "Gradient Descent(35/49): loss=8.711887983859339e+48\n",
      "Gradient Descent(36/49): loss=2.420301448814645e+50\n",
      "Gradient Descent(37/49): loss=6.723983497018342e+51\n",
      "Gradient Descent(38/49): loss=1.8680298724903847e+53\n",
      "Gradient Descent(39/49): loss=5.189684962885207e+54\n",
      "Gradient Descent(40/49): loss=1.4417772654829738e+56\n",
      "Gradient Descent(41/49): loss=4.005487227316954e+57\n",
      "Gradient Descent(42/49): loss=1.1127882449183187e+59\n",
      "Gradient Descent(43/49): loss=3.0915032498002973e+60\n",
      "Gradient Descent(44/49): loss=8.588689166308812e+61\n",
      "Gradient Descent(45/49): loss=2.3860748521042336e+63\n",
      "Gradient Descent(46/49): loss=6.628896551732032e+64\n",
      "Gradient Descent(47/49): loss=1.8416132023189835e+66\n",
      "Gradient Descent(48/49): loss=5.116295239317664e+67\n",
      "Gradient Descent(49/49): loss=1.4213884296063221e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1798677167139442\n",
      "Gradient Descent(2/49): loss=21.163606747758333\n",
      "Gradient Descent(3/49): loss=577.532096731003\n",
      "Gradient Descent(4/49): loss=16213.152108494529\n",
      "Gradient Descent(5/49): loss=456744.6927233524\n",
      "Gradient Descent(6/49): loss=12877596.871377777\n",
      "Gradient Descent(7/49): loss=363157856.5034903\n",
      "Gradient Descent(8/49): loss=10241999015.224197\n",
      "Gradient Descent(9/49): loss=288856634741.78296\n",
      "Gradient Descent(10/49): loss=8146712511279.76\n",
      "Gradient Descent(11/49): loss=229764612492683.78\n",
      "Gradient Descent(12/49): loss=6480135651532273.0\n",
      "Gradient Descent(13/49): loss=1.8276166816635654e+17\n",
      "Gradient Descent(14/49): loss=5.154495290242312e+18\n",
      "Gradient Descent(15/49): loss=1.4537414916085077e+20\n",
      "Gradient Descent(16/49): loss=4.1000412506956707e+21\n",
      "Gradient Descent(17/49): loss=1.1563498992904167e+23\n",
      "Gradient Descent(18/49): loss=3.261296675369743e+24\n",
      "Gradient Descent(19/49): loss=9.197956442253828e+25\n",
      "Gradient Descent(20/49): loss=2.5941339024636287e+27\n",
      "Gradient Descent(21/49): loss=7.316332433414494e+28\n",
      "Gradient Descent(22/49): loss=2.063452477357036e+30\n",
      "Gradient Descent(23/49): loss=5.819631851154679e+31\n",
      "Gradient Descent(24/49): loss=1.6413324394250054e+33\n",
      "Gradient Descent(25/49): loss=4.629110991229576e+34\n",
      "Gradient Descent(26/49): loss=1.3055654085914366e+36\n",
      "Gradient Descent(27/49): loss=3.682134732435491e+37\n",
      "Gradient Descent(28/49): loss=1.0384861684130864e+39\n",
      "Gradient Descent(29/49): loss=2.9288812071033614e+40\n",
      "Gradient Descent(30/49): loss=8.26043272047796e+41\n",
      "Gradient Descent(31/49): loss=2.3297205965217835e+43\n",
      "Gradient Descent(32/49): loss=6.570597741692859e+44\n",
      "Gradient Descent(33/49): loss=1.853130145631839e+46\n",
      "Gradient Descent(34/49): loss=5.226451947984773e+47\n",
      "Gradient Descent(35/49): loss=1.4740357027261226e+49\n",
      "Gradient Descent(36/49): loss=4.157277775698419e+50\n",
      "Gradient Descent(37/49): loss=1.1724925300216597e+52\n",
      "Gradient Descent(38/49): loss=3.306824338254952e+53\n",
      "Gradient Descent(39/49): loss=9.326359805356952e+54\n",
      "Gradient Descent(40/49): loss=2.630347981074759e+56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(41/49): loss=7.418468347714851e+57\n",
      "Gradient Descent(42/49): loss=2.0922582495552617e+59\n",
      "Gradient Descent(43/49): loss=5.9008738430224565e+60\n",
      "Gradient Descent(44/49): loss=1.664245420882834e+62\n",
      "Gradient Descent(45/49): loss=4.6937333259624414e+63\n",
      "Gradient Descent(46/49): loss=1.3237910862668133e+65\n",
      "Gradient Descent(47/49): loss=3.733537289786552e+66\n",
      "Gradient Descent(48/49): loss=1.0529834230517857e+68\n",
      "Gradient Descent(49/49): loss=2.9697683541423824e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1900805455450874\n",
      "Gradient Descent(2/49): loss=21.956497767141265\n",
      "Gradient Descent(3/49): loss=612.7515929212414\n",
      "Gradient Descent(4/49): loss=17567.046438699253\n",
      "Gradient Descent(5/49): loss=505204.1147522646\n",
      "Gradient Descent(6/49): loss=14538952.421150645\n",
      "Gradient Descent(7/49): loss=418483241.4707845\n",
      "Gradient Descent(8/49): loss=12046046345.560165\n",
      "Gradient Descent(9/49): loss=346750346095.01666\n",
      "Gradient Descent(10/49): loss=9981387277715.045\n",
      "Gradient Descent(11/49): loss=287319670162098.1\n",
      "Gradient Descent(12/49): loss=8270655583035339.0\n",
      "Gradient Descent(13/49): loss=2.3807541306337418e+17\n",
      "Gradient Descent(14/49): loss=6.853133100035905e+18\n",
      "Gradient Descent(15/49): loss=1.972712468016426e+20\n",
      "Gradient Descent(16/49): loss=5.678562536409318e+21\n",
      "Gradient Descent(17/49): loss=1.634605803805204e+23\n",
      "Gradient Descent(18/49): loss=4.70530370500858e+24\n",
      "Gradient Descent(19/49): loss=1.3544478372457921e+26\n",
      "Gradient Descent(20/49): loss=3.898853419141782e+27\n",
      "Gradient Descent(21/49): loss=1.1223066378771834e+29\n",
      "Gradient Descent(22/49): loss=3.23062206760613e+30\n",
      "Gradient Descent(23/49): loss=9.29952527363376e+31\n",
      "Gradient Descent(24/49): loss=2.6769200638513366e+33\n",
      "Gradient Descent(25/49): loss=7.705663264948551e+34\n",
      "Gradient Descent(26/49): loss=2.218118021325968e+36\n",
      "Gradient Descent(27/49): loss=6.384976071964248e+37\n",
      "Gradient Descent(28/49): loss=1.8379508686010082e+39\n",
      "Gradient Descent(29/49): loss=5.290643782087018e+40\n",
      "Gradient Descent(30/49): loss=1.5229412334749568e+42\n",
      "Gradient Descent(31/49): loss=4.383871029970031e+43\n",
      "Gradient Descent(32/49): loss=1.2619216542951679e+45\n",
      "Gradient Descent(33/49): loss=3.632511656233523e+46\n",
      "Gradient Descent(34/49): loss=1.045638680322231e+48\n",
      "Gradient Descent(35/49): loss=3.009929088347903e+49\n",
      "Gradient Descent(36/49): loss=8.664248260298613e+50\n",
      "Gradient Descent(37/49): loss=2.4940520428437004e+52\n",
      "Gradient Descent(38/49): loss=7.17926749734945e+53\n",
      "Gradient Descent(39/49): loss=2.066592072382362e+55\n",
      "Gradient Descent(40/49): loss=5.948800201706353e+56\n",
      "Gradient Descent(41/49): loss=1.7123952188119106e+58\n",
      "Gradient Descent(42/49): loss=4.929224862130661e+59\n",
      "Gradient Descent(43/49): loss=1.4189047875469364e+61\n",
      "Gradient Descent(44/49): loss=4.0843963349916512e+62\n",
      "Gradient Descent(45/49): loss=1.1757161979934102e+64\n",
      "Gradient Descent(46/49): loss=3.3843644564598894e+65\n",
      "Gradient Descent(47/49): loss=9.742081289427957e+66\n",
      "Gradient Descent(48/49): loss=2.8043122740125426e+68\n",
      "Gradient Descent(49/49): loss=8.072368826065465e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1882508716842548\n",
      "Gradient Descent(2/49): loss=21.802863460443906\n",
      "Gradient Descent(3/49): loss=608.5954350952186\n",
      "Gradient Descent(4/49): loss=17475.74820666534\n",
      "Gradient Descent(5/49): loss=503578.8312440725\n",
      "Gradient Descent(6/49): loss=14522797.02136325\n",
      "Gradient Descent(7/49): loss=418917286.40241414\n",
      "Gradient Descent(8/49): loss=12084617038.29456\n",
      "Gradient Descent(9/49): loss=348614123285.5419\n",
      "Gradient Descent(10/49): loss=10056785034914.4\n",
      "Gradient Descent(11/49): loss=290117515305193.8\n",
      "Gradient Descent(12/49): loss=8369295430564876.0\n",
      "Gradient Descent(13/49): loss=2.4143703807319466e+17\n",
      "Gradient Descent(14/49): loss=6.964964448349123e+18\n",
      "Gradient Descent(15/49): loss=2.0092497228576417e+20\n",
      "Gradient Descent(16/49): loss=5.796274322021683e+21\n",
      "Gradient Descent(17/49): loss=1.672106540906011e+23\n",
      "Gradient Descent(18/49): loss=4.8236852318311237e+24\n",
      "Gradient Descent(19/49): loss=1.3915344894493011e+26\n",
      "Gradient Descent(20/49): loss=4.014292273008427e+27\n",
      "Gradient Descent(21/49): loss=1.1580411822621558e+29\n",
      "Gradient Descent(22/49): loss=3.340711858062414e+30\n",
      "Gradient Descent(23/49): loss=9.637270150273622e+31\n",
      "Gradient Descent(24/49): loss=2.7801552452126663e+33\n",
      "Gradient Descent(25/49): loss=8.0201790205749085e+34\n",
      "Gradient Descent(26/49): loss=2.3136575424279882e+36\n",
      "Gradient Descent(27/49): loss=6.674428600535402e+37\n",
      "Gradient Descent(28/49): loss=1.9254360823380908e+39\n",
      "Gradient Descent(29/49): loss=5.554489124165698e+40\n",
      "Gradient Descent(30/49): loss=1.6023564590630548e+42\n",
      "Gradient Descent(31/49): loss=4.622470518000601e+43\n",
      "Gradient Descent(32/49): loss=1.3334881616964825e+45\n",
      "Gradient Descent(33/49): loss=3.8468404946232006e+46\n",
      "Gradient Descent(34/49): loss=1.1097347705169346e+48\n",
      "Gradient Descent(35/49): loss=3.20135774440239e+49\n",
      "Gradient Descent(36/49): loss=9.235262046327626e+50\n",
      "Gradient Descent(37/49): loss=2.664184132918913e+52\n",
      "Gradient Descent(38/49): loss=7.68562609105325e+53\n",
      "Gradient Descent(39/49): loss=2.217145867720547e+55\n",
      "Gradient Descent(40/49): loss=6.396012166754599e+56\n",
      "Gradient Descent(41/49): loss=1.8451186380141657e+58\n",
      "Gradient Descent(42/49): loss=5.322789731456528e+59\n",
      "Gradient Descent(43/49): loss=1.5355159251869025e+61\n",
      "Gradient Descent(44/49): loss=4.429649254353315e+62\n",
      "Gradient Descent(45/49): loss=1.2778631725492852e+64\n",
      "Gradient Descent(46/49): loss=3.686373782648672e+65\n",
      "Gradient Descent(47/49): loss=1.0634434075042037e+67\n",
      "Gradient Descent(48/49): loss=3.067816634024528e+68\n",
      "Gradient Descent(49/49): loss=8.850023267420957e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1891244124468476\n",
      "Gradient Descent(2/49): loss=21.68962429280343\n",
      "Gradient Descent(3/49): loss=599.544724532193\n",
      "Gradient Descent(4/49): loss=17039.698136468458\n",
      "Gradient Descent(5/49): loss=485913.71942241606\n",
      "Gradient Descent(6/49): loss=13867182.819852376\n",
      "Gradient Descent(7/49): loss=395828901.5473896\n",
      "Gradient Descent(8/49): loss=11299314898.43483\n",
      "Gradient Descent(9/49): loss=322555092186.8364\n",
      "Gradient Descent(10/49): loss=9207839283390.584\n",
      "Gradient Descent(11/49): loss=262852515044928.06\n",
      "Gradient Descent(12/49): loss=7503548735374604.0\n",
      "Gradient Descent(13/49): loss=2.1420091668795338e+17\n",
      "Gradient Descent(14/49): loss=6.114711282503443e+18\n",
      "Gradient Descent(15/49): loss=1.7455431550361593e+20\n",
      "Gradient Descent(16/49): loss=4.982935044407266e+21\n",
      "Gradient Descent(17/49): loss=1.42245934181154e+23\n",
      "Gradient Descent(18/49): loss=4.060640087573095e+24\n",
      "Gradient Descent(19/49): loss=1.1591753406981953e+26\n",
      "Gradient Descent(20/49): loss=3.3090533549537286e+27\n",
      "Gradient Descent(21/49): loss=9.446227608141818e+28\n",
      "Gradient Descent(22/49): loss=2.696578339885483e+30\n",
      "Gradient Descent(23/49): loss=7.697818689944278e+31\n",
      "Gradient Descent(24/49): loss=2.197466756548163e+33\n",
      "Gradient Descent(25/49): loss=6.273024009311136e+34\n",
      "Gradient Descent(26/49): loss=1.7907360875487104e+36\n",
      "Gradient Descent(27/49): loss=5.111945579180792e+37\n",
      "Gradient Descent(28/49): loss=1.4592874844152638e+39\n",
      "Gradient Descent(29/49): loss=4.165771973089541e+40\n",
      "Gradient Descent(30/49): loss=1.189186936577606e+42\n",
      "Gradient Descent(31/49): loss=3.39472630586167e+43\n",
      "Gradient Descent(32/49): loss=9.690794892916461e+44\n",
      "Gradient Descent(33/49): loss=2.766394024001842e+46\n",
      "Gradient Descent(34/49): loss=7.897118843808206e+47\n",
      "Gradient Descent(35/49): loss=2.2543602065411772e+49\n",
      "Gradient Descent(36/49): loss=6.4354355573881924e+50\n",
      "Gradient Descent(37/49): loss=1.837099088829217e+52\n",
      "Gradient Descent(38/49): loss=5.244296259485574e+53\n",
      "Gradient Descent(39/49): loss=1.4970691251489254e+55\n",
      "Gradient Descent(40/49): loss=4.273625772801023e+56\n",
      "Gradient Descent(41/49): loss=1.219975546829362e+58\n",
      "Gradient Descent(42/49): loss=3.4826173698548033e+59\n",
      "Gradient Descent(43/49): loss=9.941694139965361e+60\n",
      "Gradient Descent(44/49): loss=2.838017269084656e+62\n",
      "Gradient Descent(45/49): loss=8.101578972586225e+63\n",
      "Gradient Descent(46/49): loss=2.3127266547684164e+65\n",
      "Gradient Descent(47/49): loss=6.6020520169895604e+66\n",
      "Gradient Descent(48/49): loss=1.8846624500637553e+68\n",
      "Gradient Descent(49/49): loss=5.380073561280343e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2014137530054922\n",
      "Gradient Descent(2/49): loss=22.312415020213546\n",
      "Gradient Descent(3/49): loss=625.9402924343269\n",
      "Gradient Descent(4/49): loss=18050.043009808418\n",
      "Gradient Descent(5/49): loss=522311.0113418106\n",
      "Gradient Descent(6/49): loss=15126505.721985111\n",
      "Gradient Descent(7/49): loss=438176252.1956297\n",
      "Gradient Descent(8/49): loss=12693700993.79036\n",
      "Gradient Descent(9/49): loss=367736053570.2882\n",
      "Gradient Descent(10/49): loss=10653361089283.512\n",
      "Gradient Descent(11/49): loss=308629767476984.75\n",
      "Gradient Descent(12/49): loss=8941063694122203.0\n",
      "Gradient Descent(13/49): loss=2.590243711554797e+17\n",
      "Gradient Descent(14/49): loss=7.503987216764389e+18\n",
      "Gradient Descent(15/49): loss=2.1739199515672183e+20\n",
      "Gradient Descent(16/49): loss=6.297889157125572e+21\n",
      "Gradient Descent(17/49): loss=1.8245109645568383e+23\n",
      "Gradient Descent(18/49): loss=5.285644408401098e+24\n",
      "Gradient Descent(19/49): loss=1.5312616562627067e+26\n",
      "Gradient Descent(20/49): loss=4.4360953534352545e+27\n",
      "Gradient Descent(21/49): loss=1.2851456120798131e+29\n",
      "Gradient Descent(22/49): loss=3.72309229776299e+30\n",
      "Gradient Descent(23/49): loss=1.0785872143498654e+32\n",
      "Gradient Descent(24/49): loss=3.124688527486208e+33\n",
      "Gradient Descent(25/49): loss=9.052284566241302e+34\n",
      "Gradient Descent(26/49): loss=2.622464772005137e+36\n",
      "Gradient Descent(27/49): loss=7.59733239723328e+37\n",
      "Gradient Descent(28/49): loss=2.2009622462885638e+39\n",
      "Gradient Descent(29/49): loss=6.376231229992948e+40\n",
      "Gradient Descent(30/49): loss=1.8472068190581407e+42\n",
      "Gradient Descent(31/49): loss=5.351394749181088e+43\n",
      "Gradient Descent(32/49): loss=1.550309660299145e+45\n",
      "Gradient Descent(33/49): loss=4.491277798530266e+46\n",
      "Gradient Descent(34/49): loss=1.3011320757479309e+48\n",
      "Gradient Descent(35/49): loss=3.7694053997152453e+49\n",
      "Gradient Descent(36/49): loss=1.0920042117350042e+51\n",
      "Gradient Descent(37/49): loss=3.163557834710644e+52\n",
      "Gradient Descent(38/49): loss=9.164889719297026e+53\n",
      "Gradient Descent(39/49): loss=2.6550867079235363e+55\n",
      "Gradient Descent(40/49): loss=7.691838791850705e+56\n",
      "Gradient Descent(41/49): loss=2.228340936032552e+58\n",
      "Gradient Descent(42/49): loss=6.455547836570965e+59\n",
      "Gradient Descent(43/49): loss=1.8701849971151496e+61\n",
      "Gradient Descent(44/49): loss=5.417962986224936e+62\n",
      "Gradient Descent(45/49): loss=1.569594610446761e+64\n",
      "Gradient Descent(46/49): loss=4.5471466811553505e+65\n",
      "Gradient Descent(47/49): loss=1.3173174017243141e+67\n",
      "Gradient Descent(48/49): loss=3.8162946097106626e+68\n",
      "Gradient Descent(49/49): loss=1.1055881087612538e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2119330450397625\n",
      "Gradient Descent(2/49): loss=23.14709139083086\n",
      "Gradient Descent(3/49): loss=664.0238282056599\n",
      "Gradient Descent(4/49): loss=19553.903093789588\n",
      "Gradient Descent(5/49): loss=577601.9203142567\n",
      "Gradient Descent(6/49): loss=17073586.685211252\n",
      "Gradient Descent(7/49): loss=504778521.69388294\n",
      "Gradient Descent(8/49): loss=14924469548.948812\n",
      "Gradient Descent(9/49): loss=441268571643.99695\n",
      "Gradient Descent(10/49): loss=13046942987042.527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=385758048346297.5\n",
      "Gradient Descent(12/49): loss=1.1405684532765112e+16\n",
      "Gradient Descent(13/49): loss=3.3723120193399194e+17\n",
      "Gradient Descent(14/49): loss=9.9708952852074e+18\n",
      "Gradient Descent(15/49): loss=2.948088813888552e+20\n",
      "Gradient Descent(16/49): loss=8.716597076839685e+21\n",
      "Gradient Descent(17/49): loss=2.57723119732739e+23\n",
      "Gradient Descent(18/49): loss=7.620084520149347e+24\n",
      "Gradient Descent(19/49): loss=2.253025966648645e+26\n",
      "Gradient Descent(20/49): loss=6.661508796946149e+27\n",
      "Gradient Descent(21/49): loss=1.969604439043573e+29\n",
      "Gradient Descent(22/49): loss=5.8235180115375e+30\n",
      "Gradient Descent(23/49): loss=1.7218361899700683e+32\n",
      "Gradient Descent(24/49): loss=5.0909430677764424e+33\n",
      "Gradient Descent(25/49): loss=1.5052361816017026e+35\n",
      "Gradient Descent(26/49): loss=4.450523080377874e+36\n",
      "Gradient Descent(27/49): loss=1.3158835756857545e+38\n",
      "Gradient Descent(28/49): loss=3.890665329641419e+39\n",
      "Gradient Descent(29/49): loss=1.150350759518012e+41\n",
      "Gradient Descent(30/49): loss=3.401235412981748e+42\n",
      "Gradient Descent(31/49): loss=1.0056413001690165e+44\n",
      "Gradient Descent(32/49): loss=2.973373794550259e+45\n",
      "Gradient Descent(33/49): loss=8.791357038172921e+46\n",
      "Gradient Descent(34/49): loss=2.599335432171005e+48\n",
      "Gradient Descent(35/49): loss=7.685439983386028e+49\n",
      "Gradient Descent(36/49): loss=2.27234957855732e+51\n",
      "Gradient Descent(37/49): loss=6.718642808130604e+52\n",
      "Gradient Descent(38/49): loss=1.9864972189668146e+54\n",
      "Gradient Descent(39/49): loss=5.873464795877221e+55\n",
      "Gradient Descent(40/49): loss=1.7366039267022757e+57\n",
      "Gradient Descent(41/49): loss=5.134606749247289e+58\n",
      "Gradient Descent(42/49): loss=1.518146196955803e+60\n",
      "Gradient Descent(43/49): loss=4.4886940478338424e+61\n",
      "Gradient Descent(44/49): loss=1.3271695634755466e+63\n",
      "Gradient Descent(45/49): loss=3.924034544225364e+64\n",
      "Gradient Descent(46/49): loss=1.1602170158235133e+66\n",
      "Gradient Descent(47/49): loss=3.430406915727468e+67\n",
      "Gradient Descent(48/49): loss=1.0142664214520421e+69\n",
      "Gradient Descent(49/49): loss=2.998875640579724e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2100053587393174\n",
      "Gradient Descent(2/49): loss=22.985026127821186\n",
      "Gradient Descent(3/49): loss=659.5158544199742\n",
      "Gradient Descent(4/49): loss=19451.99243637458\n",
      "Gradient Descent(5/49): loss=575731.469702394\n",
      "Gradient Descent(6/49): loss=17054160.460952416\n",
      "Gradient Descent(7/49): loss=505286163.03574723\n",
      "Gradient Descent(8/49): loss=14971715981.026\n",
      "Gradient Descent(9/49): loss=443622334082.43445\n",
      "Gradient Descent(10/49): loss=13144903014457.055\n",
      "Gradient Descent(11/49): loss=389495084030272.8\n",
      "Gradient Descent(12/49): loss=1.1541087875989322e+16\n",
      "Gradient Descent(13/49): loss=3.419727996537992e+17\n",
      "Gradient Descent(14/49): loss=1.0132961526453135e+19\n",
      "Gradient Descent(15/49): loss=3.002487634205489e+20\n",
      "Gradient Descent(16/49): loss=8.896640920755978e+21\n",
      "Gradient Descent(17/49): loss=2.636154728100785e+23\n",
      "Gradient Descent(18/49): loss=7.811163577128667e+24\n",
      "Gradient Descent(19/49): loss=2.3145180281631194e+26\n",
      "Gradient Descent(20/49): loss=6.858125105028562e+27\n",
      "Gradient Descent(21/49): loss=2.0321241564822435e+29\n",
      "Gradient Descent(22/49): loss=6.021366662349358e+30\n",
      "Gradient Descent(23/49): loss=1.7841851033958948e+32\n",
      "Gradient Descent(24/49): loss=5.286700946290045e+33\n",
      "Gradient Descent(25/49): loss=1.5664970435134855e+35\n",
      "Gradient Descent(26/49): loss=4.6416716441253304e+36\n",
      "Gradient Descent(27/49): loss=1.375369059334699e+38\n",
      "Gradient Descent(28/49): loss=4.0753422353115736e+39\n",
      "Gradient Descent(29/49): loss=1.2075605614501976e+41\n",
      "Gradient Descent(30/49): loss=3.578110561942601e+42\n",
      "Gradient Descent(31/49): loss=1.0602263441023543e+44\n",
      "Gradient Descent(32/49): loss=3.1415460234363506e+45\n",
      "Gradient Descent(33/49): loss=9.308683445066312e+46\n",
      "Gradient Descent(34/49): loss=2.758246635065002e+48\n",
      "Gradient Descent(35/49): loss=8.172932880083762e+49\n",
      "Gradient Descent(36/49): loss=2.4217135267448776e+51\n",
      "Gradient Descent(37/49): loss=7.175755009453812e+52\n",
      "Gradient Descent(38/49): loss=2.1262407542032215e+54\n",
      "Gradient Descent(39/49): loss=6.300242607054645e+55\n",
      "Gradient Descent(40/49): loss=1.8668185542620235e+57\n",
      "Gradient Descent(41/49): loss=5.531551293968663e+58\n",
      "Gradient Descent(42/49): loss=1.6390484039249515e+60\n",
      "Gradient Descent(43/49): loss=4.8566478509168555e+61\n",
      "Gradient Descent(44/49): loss=1.4390684430876204e+63\n",
      "Gradient Descent(45/49): loss=4.2640892390409974e+64\n",
      "Gradient Descent(46/49): loss=1.2634879964078401e+66\n",
      "Gradient Descent(47/49): loss=3.743828582315847e+67\n",
      "Gradient Descent(48/49): loss=1.1093300841491122e+69\n",
      "Gradient Descent(49/49): loss=3.287044822006907e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2109697370060934\n",
      "Gradient Descent(2/49): loss=22.866494554778157\n",
      "Gradient Descent(3/49): loss=649.7483969822795\n",
      "Gradient Descent(4/49): loss=18968.27590490073\n",
      "Gradient Descent(5/49): loss=555596.1403559233\n",
      "Gradient Descent(6/49): loss=16286416.584254706\n",
      "Gradient Descent(7/49): loss=477511051.5373451\n",
      "Gradient Descent(8/49): loss=14001261198.799118\n",
      "Gradient Descent(9/49): loss=410542604565.23956\n",
      "Gradient Descent(10/49): loss=12037918564001.312\n",
      "Gradient Descent(11/49): loss=352975990090127.8\n",
      "Gradient Descent(12/49): loss=1.0349970195147216e+16\n",
      "Gradient Descent(13/49): loss=3.03482100773752e+17\n",
      "Gradient Descent(14/49): loss=8.89871049714029e+18\n",
      "Gradient Descent(15/49): loss=2.609282360588426e+20\n",
      "Gradient Descent(16/49): loss=7.650944995993823e+21\n",
      "Gradient Descent(17/49): loss=2.2434122216537563e+23\n",
      "Gradient Descent(18/49): loss=6.578139563098553e+24\n",
      "Gradient Descent(19/49): loss=1.9288439144857043e+26\n",
      "Gradient Descent(20/49): loss=5.655761497342969e+27\n",
      "Gradient Descent(21/49): loss=1.6583839612295477e+29\n",
      "Gradient Descent(22/49): loss=4.8627180692807887e+30\n",
      "Gradient Descent(23/49): loss=1.4258475464142712e+32\n",
      "Gradient Descent(24/49): loss=4.1808741462087803e+33\n",
      "Gradient Descent(25/49): loss=1.2259170814155833e+35\n",
      "Gradient Descent(26/49): loss=3.5946374799856764e+36\n",
      "Gradient Descent(27/49): loss=1.0540206028940616e+38\n",
      "Gradient Descent(28/49): loss=3.090602146978075e+39\n",
      "Gradient Descent(29/49): loss=9.062272221888927e+40\n",
      "Gradient Descent(30/49): loss=2.657241984508423e+42\n",
      "Gradient Descent(31/49): loss=7.791572346700536e+43\n",
      "Gradient Descent(32/49): loss=2.284647013248951e+45\n",
      "Gradient Descent(33/49): loss=6.699048334393862e+46\n",
      "Gradient Descent(34/49): loss=1.9642968181209672e+48\n",
      "Gradient Descent(35/49): loss=5.759716600147957e+49\n",
      "Gradient Descent(36/49): loss=1.6888657054260406e+51\n",
      "Gradient Descent(37/49): loss=4.952096724500147e+52\n",
      "Gradient Descent(38/49): loss=1.4520551805875317e+54\n",
      "Gradient Descent(39/49): loss=4.257720244113221e+55\n",
      "Gradient Descent(40/49): loss=1.2484499156427746e+57\n",
      "Gradient Descent(41/49): loss=3.6607083192547076e+58\n",
      "Gradient Descent(42/49): loss=1.073393912783528e+60\n",
      "Gradient Descent(43/49): loss=3.147408620185581e+61\n",
      "Gradient Descent(44/49): loss=9.228840320819169e+62\n",
      "Gradient Descent(45/49): loss=2.706083128861587e+64\n",
      "Gradient Descent(46/49): loss=7.934784486182755e+65\n",
      "Gradient Descent(47/49): loss=2.326639716668755e+67\n",
      "Gradient Descent(48/49): loss=6.822179456300093e+68\n",
      "Gradient Descent(49/49): loss=2.000401359975116e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2232703570899264\n",
      "Gradient Descent(2/49): loss=23.50826153344296\n",
      "Gradient Descent(3/49): loss=677.6862022367526\n",
      "Gradient Descent(4/49): loss=20066.52644222467\n",
      "Gradient Descent(5/49): loss=596229.9870687575\n",
      "Gradient Descent(6/49): loss=17730345.07926083\n",
      "Gradient Descent(7/49): loss=527378979.8552076\n",
      "Gradient Descent(8/49): loss=15687660613.224573\n",
      "Gradient Descent(9/49): loss=466661808879.54803\n",
      "Gradient Descent(10/49): loss=13881899235627.217\n",
      "Gradient Descent(11/49): loss=412948852989853.0\n",
      "Gradient Descent(12/49): loss=1.2284114629321152e+16\n",
      "Gradient Descent(13/49): loss=3.6541933352062624e+17\n",
      "Gradient Descent(14/49): loss=1.0870241703238836e+19\n",
      "Gradient Descent(15/49): loss=3.2336043925206204e+20\n",
      "Gradient Descent(16/49): loss=9.619102952853965e+21\n",
      "Gradient Descent(17/49): loss=2.8614242958592952e+23\n",
      "Gradient Descent(18/49): loss=8.511967325552416e+24\n",
      "Gradient Descent(19/49): loss=2.5320812388080565e+26\n",
      "Gradient Descent(20/49): loss=7.5322603516764e+27\n",
      "Gradient Descent(21/49): loss=2.240644776176591e+29\n",
      "Gradient Descent(22/49): loss=6.665315295297928e+30\n",
      "Gradient Descent(23/49): loss=1.9827519497121008e+32\n",
      "Gradient Descent(24/49): loss=5.898153530503355e+33\n",
      "Gradient Descent(25/49): loss=1.7545419675135155e+35\n",
      "Gradient Descent(26/49): loss=5.2192902403195637e+36\n",
      "Gradient Descent(27/49): loss=1.5525984055713484e+38\n",
      "Gradient Descent(28/49): loss=4.618562482616617e+39\n",
      "Gradient Descent(29/49): loss=1.3738980620673888e+41\n",
      "Gradient Descent(30/49): loss=4.086977045470489e+42\n",
      "Gradient Descent(31/49): loss=1.2157656984440327e+44\n",
      "Gradient Descent(32/49): loss=3.61657581402673e+45\n",
      "Gradient Descent(33/49): loss=1.0758339896694553e+47\n",
      "Gradient Descent(34/49): loss=3.2003166333167965e+48\n",
      "Gradient Descent(35/49): loss=9.52008084131174e+49\n",
      "Gradient Descent(36/49): loss=2.8319678834772133e+51\n",
      "Gradient Descent(37/49): loss=8.424342426005421e+52\n",
      "Gradient Descent(38/49): loss=2.5060151891078372e+54\n",
      "Gradient Descent(39/49): loss=7.454720867771119e+55\n",
      "Gradient Descent(40/49): loss=2.2175788661586847e+57\n",
      "Gradient Descent(41/49): loss=6.596700419587924e+58\n",
      "Gradient Descent(42/49): loss=1.9623408704814688e+60\n",
      "Gradient Descent(43/49): loss=5.837436062016155e+61\n",
      "Gradient Descent(44/49): loss=1.736480154427303e+63\n",
      "Gradient Descent(45/49): loss=5.165561206469836e+64\n",
      "Gradient Descent(46/49): loss=1.5366154637446082e+66\n",
      "Gradient Descent(47/49): loss=4.5710175313801006e+67\n",
      "Gradient Descent(48/49): loss=1.3597547184164599e+69\n",
      "Gradient Descent(49/49): loss=4.0449044038069796e+70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2341003911662005\n",
      "Gradient Descent(2/49): loss=24.38634595918731\n",
      "Gradient Descent(3/49): loss=718.825098556031\n",
      "Gradient Descent(4/49): loss=21734.683512806176\n",
      "Gradient Descent(5/49): loss=659206.9794955377\n",
      "Gradient Descent(6/49): loss=20007560.361554224\n",
      "Gradient Descent(7/49): loss=607362088.7610286\n",
      "Gradient Descent(8/49): loss=18438414923.748432\n",
      "Gradient Descent(9/49): loss=559764927328.1028\n",
      "Gradient Descent(10/49): loss=16993760916028.57\n",
      "Gradient Descent(11/49): loss=515909828691308.0\n",
      "Gradient Descent(12/49): loss=1.5662397158527506e+16\n",
      "Gradient Descent(13/49): loss=4.754914365609846e+17\n",
      "Gradient Descent(14/49): loss=1.4435345323453327e+19\n",
      "Gradient Descent(15/49): loss=4.38239639982212e+20\n",
      "Gradient Descent(16/49): loss=1.3304425914178738e+22\n",
      "Gradient Descent(17/49): loss=4.039062942882353e+23\n",
      "Gradient Descent(18/49): loss=1.226210703422577e+25\n",
      "Gradient Descent(19/49): loss=3.722627526506972e+26\n",
      "Gradient Descent(20/49): loss=1.1301447347162866e+28\n",
      "Gradient Descent(21/49): loss=3.430982853686806e+29\n",
      "Gradient Descent(22/49): loss=1.0416049361374923e+31\n",
      "Gradient Descent(23/49): loss=3.162186723901586e+32\n",
      "Gradient Descent(24/49): loss=9.600016791299276e+33\n",
      "Gradient Descent(25/49): loss=2.914449096146971e+35\n",
      "Gradient Descent(26/49): loss=8.847915288783953e+36\n",
      "Gradient Descent(27/49): loss=2.6861201679931023e+38\n",
      "Gradient Descent(28/49): loss=8.154736253008392e+39\n",
      "Gradient Descent(29/49): loss=2.4756793887524954e+41\n",
      "Gradient Descent(30/49): loss=7.515863475821008e+42\n",
      "Gradient Descent(31/49): loss=2.281725333410182e+44\n",
      "Gradient Descent(32/49): loss=6.9270424002175e+45\n",
      "Gradient Descent(33/49): loss=2.102966369869592e+47\n",
      "Gradient Descent(34/49): loss=6.384351787226872e+48\n",
      "Gradient Descent(35/49): loss=1.938212057361358e+50\n",
      "Gradient Descent(36/49): loss=5.8841776025201036e+51\n",
      "Gradient Descent(37/49): loss=1.786365218733344e+53\n",
      "Gradient Descent(38/49): loss=5.4231889488405754e+54\n",
      "Gradient Descent(39/49): loss=1.646414633827277e+56\n",
      "Gradient Descent(40/49): loss=4.998315883978434e+57\n",
      "Gradient Descent(41/49): loss=1.517428305283878e+59\n",
      "Gradient Descent(42/49): loss=4.606728976568697e+60\n",
      "Gradient Descent(43/49): loss=1.39854725193014e+62\n",
      "Gradient Descent(44/49): loss=4.245820463565047e+63\n",
      "Gradient Descent(45/49): loss=1.288979788416062e+65\n",
      "Gradient Descent(46/49): loss=3.9131868839080284e+66\n",
      "Gradient Descent(47/49): loss=1.1879962530061885e+68\n",
      "Gradient Descent(48/49): loss=3.606613072737435e+69\n",
      "Gradient Descent(49/49): loss=1.0949241484158869e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2320729571297118\n",
      "Gradient Descent(2/49): loss=24.21551269077074\n",
      "Gradient Descent(3/49): loss=713.9408867314829\n",
      "Gradient Descent(4/49): loss=21621.097891599908\n",
      "Gradient Descent(5/49): loss=657058.6578294729\n",
      "Gradient Descent(6/49): loss=19984277.574940708\n",
      "Gradient Descent(7/49): loss=607954260.1144439\n",
      "Gradient Descent(8/49): loss=18496134931.447426\n",
      "Gradient Descent(9/49): loss=562728465972.173\n",
      "Gradient Descent(10/49): loss=17120600844795.295\n",
      "Gradient Descent(11/49): loss=520882478141023.8\n",
      "Gradient Descent(12/49): loss=1.5847496826056822e+16\n",
      "Gradient Descent(13/49): loss=4.821494204180154e+17\n",
      "Gradient Descent(14/49): loss=1.4669071958448255e+19\n",
      "Gradient Descent(15/49): loss=4.462966562446892e+20\n",
      "Gradient Descent(16/49): loss=1.3578275876631304e+22\n",
      "Gradient Descent(17/49): loss=4.131099199189157e+23\n",
      "Gradient Descent(18/49): loss=1.256859173599089e+25\n",
      "Gradient Descent(19/49): loss=3.823909584805979e+26\n",
      "Gradient Descent(20/49): loss=1.1633987975877933e+28\n",
      "Gradient Descent(21/49): loss=3.539562670658794e+29\n",
      "Gradient Descent(22/49): loss=1.0768881595473078e+31\n",
      "Gradient Descent(23/49): loss=3.27635986724257e+32\n",
      "Gradient Descent(24/49): loss=9.968104751185593e+33\n",
      "Gradient Descent(25/49): loss=3.0327288929417167e+35\n",
      "Gradient Descent(26/49): loss=9.226873881907776e+36\n",
      "Gradient Descent(27/49): loss=2.8072143814362413e+38\n",
      "Gradient Descent(28/49): loss=8.540761133404639e+39\n",
      "Gradient Descent(29/49): loss=2.598469187827235e+41\n",
      "Gradient Descent(30/49): loss=7.905667907838968e+42\n",
      "Gradient Descent(31/49): loss=2.405246341261992e+44\n",
      "Gradient Descent(32/49): loss=7.317800380177636e+45\n",
      "Gradient Descent(33/49): loss=2.226391595965635e+47\n",
      "Gradient Descent(34/49): loss=6.773646835206645e+48\n",
      "Gradient Descent(35/49): loss=2.0608365361801844e+50\n",
      "Gradient Descent(36/49): loss=6.269956689771203e+51\n",
      "Gradient Descent(37/49): loss=1.9075921938220835e+53\n",
      "Gradient Descent(38/49): loss=5.803721074927772e+54\n",
      "Gradient Descent(39/49): loss=1.7657431407324266e+56\n",
      "Gradient Descent(40/49): loss=5.3721548620122554e+57\n",
      "Gradient Descent(41/49): loss=1.6344420202289994e+59\n",
      "Gradient Descent(42/49): loss=4.9726800252545175e+60\n",
      "Gradient Descent(43/49): loss=1.5129044852934528e+62\n",
      "Gradient Descent(44/49): loss=4.602910241553093e+63\n",
      "Gradient Descent(45/49): loss=1.4004045131563388e+65\n",
      "Gradient Descent(46/49): loss=4.260636635414684e+66\n",
      "Gradient Descent(47/49): loss=1.2962700682907015e+68\n",
      "Gradient Descent(48/49): loss=3.943814583903924e+69\n",
      "Gradient Descent(49/49): loss=1.1998790879066426e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2331299911232472\n",
      "Gradient Descent(2/49): loss=24.091506863945376\n",
      "Gradient Descent(3/49): loss=703.4101136599844\n",
      "Gradient Descent(4/49): loss=21085.221341442357\n",
      "Gradient Descent(5/49): loss=634146.6428027835\n",
      "Gradient Descent(6/49): loss=19087074.31987588\n",
      "Gradient Descent(7/49): loss=574621637.7324035\n",
      "Gradient Descent(8/49): loss=17300191692.597206\n",
      "Gradient Descent(9/49): loss=520867626127.76025\n",
      "Gradient Descent(10/49): loss=15682162938094.53\n",
      "Gradient Descent(11/49): loss=472155629927974.3\n",
      "Gradient Descent(12/49): loss=1.4215579185686052e+16\n",
      "Gradient Descent(13/49): loss=4.280002231562606e+17\n",
      "Gradient Descent(14/49): loss=1.2886157833998907e+19\n",
      "Gradient Descent(15/49): loss=3.8797424933978866e+20\n",
      "Gradient Descent(16/49): loss=1.1681062772783062e+22\n",
      "Gradient Descent(17/49): loss=3.516914533368096e+23\n",
      "Gradient Descent(18/49): loss=1.0588666526628828e+25\n",
      "Gradient Descent(19/49): loss=3.188017728480241e+26\n",
      "Gradient Descent(20/49): loss=9.598429615031862e+27\n",
      "Gradient Descent(21/49): loss=2.8898788815430803e+29\n",
      "Gradient Descent(22/49): loss=8.700798239872643e+30\n",
      "Gradient Descent(23/49): loss=2.6196215521177972e+32\n",
      "Gradient Descent(24/49): loss=7.887112063894239e+33\n",
      "Gradient Descent(25/49): loss=2.3746383006407172e+35\n",
      "Gradient Descent(26/49): loss=7.149520652411919e+36\n",
      "Gradient Descent(27/49): loss=2.1525655315789616e+38\n",
      "Gradient Descent(28/49): loss=6.480907732155e+39\n",
      "Gradient Descent(29/49): loss=1.951260689466518e+41\n",
      "Gradient Descent(30/49): loss=5.874822533527024e+42\n",
      "Gradient Descent(31/49): loss=1.7687815875526578e+44\n",
      "Gradient Descent(32/49): loss=5.325417553655065e+45\n",
      "Gradient Descent(33/49): loss=1.603367669606814e+47\n",
      "Gradient Descent(34/49): loss=4.8273921397505416e+48\n",
      "Gradient Descent(35/49): loss=1.4534230240927738e+50\n",
      "Gradient Descent(36/49): loss=4.375941348473379e+51\n",
      "Gradient Descent(37/49): loss=1.317500986832913e+53\n",
      "Gradient Descent(38/49): loss=3.96670958789534e+54\n",
      "Gradient Descent(39/49): loss=1.1942901836092746e+56\n",
      "Gradient Descent(40/49): loss=3.595748594800101e+57\n",
      "Gradient Descent(41/49): loss=1.0826018780404601e+59\n",
      "Gradient Descent(42/49): loss=3.259479341886227e+60\n",
      "Gradient Descent(43/49): loss=9.813585026670372e+61\n",
      "Gradient Descent(44/49): loss=2.9546575073538363e+63\n",
      "Gradient Descent(45/49): loss=8.895832625933124e+64\n",
      "Gradient Descent(46/49): loss=2.678342173725917e+66\n",
      "Gradient Descent(47/49): loss=8.0639071138171e+67\n",
      "Gradient Descent(48/49): loss=2.4278674539112444e+69\n",
      "Gradient Descent(49/49): loss=7.309782082759217e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.245437528967247\n",
      "Gradient Descent(2/49): loss=24.75241376680831\n",
      "Gradient Descent(3/49): loss=732.9514753037118\n",
      "Gradient Descent(4/49): loss=22277.44810943893\n",
      "Gradient Descent(5/49): loss=679434.4312547749\n",
      "Gradient Descent(6/49): loss=20739329.612129696\n",
      "Gradient Descent(7/49): loss=633206818.4136052\n",
      "Gradient Descent(8/49): loss=19334222960.271873\n",
      "Gradient Descent(9/49): loss=590359766863.1475\n",
      "Gradient Descent(10/49): loss=18026416377401.984\n",
      "Gradient Descent(11/49): loss=550430909772941.25\n",
      "Gradient Descent(12/49): loss=1.6807242119995464e+16\n",
      "Gradient Descent(13/49): loss=5.1320415713183994e+17\n",
      "Gradient Descent(14/49): loss=1.5670537554064572e+19\n",
      "Gradient Descent(15/49): loss=4.7849524812852566e+20\n",
      "Gradient Descent(16/49): loss=1.4610711508179946e+22\n",
      "Gradient Descent(17/49): loss=4.461337742788856e+23\n",
      "Gradient Descent(18/49): loss=1.362256345344045e+25\n",
      "Gradient Descent(19/49): loss=4.159609645370977e+26\n",
      "Gradient Descent(20/49): loss=1.2701245592659952e+28\n",
      "Gradient Descent(21/49): loss=3.878287949110674e+29\n",
      "Gradient Descent(22/49): loss=1.1842238075411948e+31\n",
      "Gradient Descent(23/49): loss=3.615992532658638e+32\n",
      "Gradient Descent(24/49): loss=1.1041326743290507e+34\n",
      "Gradient Descent(25/49): loss=3.371436615287379e+35\n",
      "Gradient Descent(26/49): loss=1.0294582449349048e+37\n",
      "Gradient Descent(27/49): loss=3.143420443555087e+38\n",
      "Gradient Descent(28/49): loss=9.598341781783413e+39\n",
      "Gradient Descent(29/49): loss=2.9308254054534227e+41\n",
      "Gradient Descent(30/49): loss=8.949189091759117e+42\n",
      "Gradient Descent(31/49): loss=2.7326085426665043e+44\n",
      "Gradient Descent(32/49): loss=8.343939736763522e+45\n",
      "Gradient Descent(33/49): loss=2.547797433978764e+47\n",
      "Gradient Descent(34/49): loss=7.779624457243009e+48\n",
      "Gradient Descent(35/49): loss=2.3754854247269948e+50\n",
      "Gradient Descent(36/49): loss=7.253474809875587e+51\n",
      "Gradient Descent(37/49): loss=2.2148271788931595e+53\n",
      "Gradient Descent(38/49): loss=6.762909586016726e+54\n",
      "Gradient Descent(39/49): loss=2.0650345320167955e+56\n",
      "Gradient Descent(40/49): loss=6.305522148690258e+57\n",
      "Gradient Descent(41/49): loss=1.925372624582334e+59\n",
      "Gradient Descent(42/49): loss=5.8790686259996166e+60\n",
      "Gradient Descent(43/49): loss=1.795156296912184e+62\n",
      "Gradient Descent(44/49): loss=5.48145690304053e+63\n",
      "Gradient Descent(45/49): loss=1.6737467278795083e+65\n",
      "Gradient Descent(46/49): loss=5.110736358309082e+66\n",
      "Gradient Descent(47/49): loss=1.5605483009507482e+68\n",
      "Gradient Descent(48/49): loss=4.765088294255165e+69\n",
      "Gradient Descent(49/49): loss=1.4550056821832374e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2565825839244025\n",
      "Gradient Descent(2/49): loss=25.675570347269257\n",
      "Gradient Descent(3/49): loss=777.3471340013584\n",
      "Gradient Descent(4/49): loss=24125.391061059192\n",
      "Gradient Descent(5/49): loss=751045.81711915\n",
      "Gradient Descent(6/49): loss=23397275.486513276\n",
      "Gradient Descent(7/49): loss=729031973.695998\n",
      "Gradient Descent(8/49): loss=22716982596.046917\n",
      "Gradient Descent(9/49): loss=707882295353.9454\n",
      "Gradient Descent(10/49): loss=22058360018782.008\n",
      "Gradient Descent(11/49): loss=687362575869486.9\n",
      "Gradient Descent(12/49): loss=2.141897491616499e+16\n",
      "Gradient Descent(13/49): loss=6.674389077418817e+17\n",
      "Gradient Descent(14/49): loss=2.079813381591835e+19\n",
      "Gradient Descent(15/49): loss=6.480928339192616e+20\n",
      "Gradient Descent(16/49): loss=2.0195288957024486e+22\n",
      "Gradient Descent(17/49): loss=6.293075234428878e+23\n",
      "Gradient Descent(18/49): loss=1.960991793498688e+25\n",
      "Gradient Descent(19/49): loss=6.1106671556857004e+26\n",
      "Gradient Descent(20/49): loss=1.9041514203068849e+28\n",
      "Gradient Descent(21/49): loss=5.933546270957911e+29\n",
      "Gradient Descent(22/49): loss=1.8489585951077452e+31\n",
      "Gradient Descent(23/49): loss=5.761559327777458e+32\n",
      "Gradient Descent(24/49): loss=1.7953655628273915e+34\n",
      "Gradient Descent(25/49): loss=5.5945575161334164e+35\n",
      "Gradient Descent(26/49): loss=1.743325952628513e+37\n",
      "Gradient Descent(27/49): loss=5.43239633937767e+38\n",
      "Gradient Descent(28/49): loss=1.6927947377590974e+40\n",
      "Gradient Descent(29/49): loss=5.2749354891752295e+41\n",
      "Gradient Descent(30/49): loss=1.6437281965913212e+43\n",
      "Gradient Descent(31/49): loss=5.122038724101614e+44\n",
      "Gradient Descent(32/49): loss=1.5960838747915792e+46\n",
      "Gradient Descent(33/49): loss=4.973573751761386e+47\n",
      "Gradient Descent(34/49): loss=1.5498205485873893e+49\n",
      "Gradient Descent(35/49): loss=4.8294121143233775e+50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=1.50489818909884e+52\n",
      "Gradient Descent(37/49): loss=4.689429077373866e+53\n",
      "Gradient Descent(38/49): loss=1.4612779277040672e+55\n",
      "Gradient Descent(39/49): loss=4.5535035219914415e+56\n",
      "Gradient Descent(40/49): loss=1.418922022408554e+58\n",
      "Gradient Descent(41/49): loss=4.421517839949915e+59\n",
      "Gradient Descent(42/49): loss=1.3777938251892367e+61\n",
      "Gradient Descent(43/49): loss=4.293357831959103e+62\n",
      "Gradient Descent(44/49): loss=1.337857750285171e+64\n",
      "Gradient Descent(45/49): loss=4.168912608855076e+65\n",
      "Gradient Descent(46/49): loss=1.2990792434073122e+67\n",
      "Gradient Descent(47/49): loss=4.0480744956541306e+68\n",
      "Gradient Descent(48/49): loss=1.2614247518407586e+70\n",
      "Gradient Descent(49/49): loss=3.9307389383885157e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2544536668554387\n",
      "Gradient Descent(2/49): loss=25.4956231820644\n",
      "Gradient Descent(3/49): loss=772.0609266261663\n",
      "Gradient Descent(4/49): loss=23998.978149106384\n",
      "Gradient Descent(5/49): loss=748583.1081339142\n",
      "Gradient Descent(6/49): loss=23369457.891472902\n",
      "Gradient Descent(7/49): loss=729720986.3023082\n",
      "Gradient Descent(8/49): loss=22787315669.346394\n",
      "Gradient Descent(9/49): loss=711602545379.8613\n",
      "Gradient Descent(10/49): loss=22222048501268.3\n",
      "Gradient Descent(11/49): loss=693955043971837.4\n",
      "Gradient Descent(12/49): loss=2.1670990746451944e+16\n",
      "Gradient Descent(13/49): loss=6.767468595650694e+17\n",
      "Gradient Descent(14/49): loss=2.113361281216032e+19\n",
      "Gradient Descent(15/49): loss=6.599655226444155e+20\n",
      "Gradient Descent(16/49): loss=2.0609561468072106e+22\n",
      "Gradient Descent(17/49): loss=6.436003240620636e+23\n",
      "Gradient Descent(18/49): loss=2.0098505146177704e+25\n",
      "Gradient Descent(19/49): loss=6.276409349632383e+26\n",
      "Gradient Descent(20/49): loss=1.9600121520663191e+28\n",
      "Gradient Descent(21/49): loss=6.1207729169110326e+29\n",
      "Gradient Descent(22/49): loss=1.911409633912292e+31\n",
      "Gradient Descent(23/49): loss=5.96899580855232e+32\n",
      "Gradient Descent(24/49): loss=1.8640123148063212e+34\n",
      "Gradient Descent(25/49): loss=5.820982324650771e+35\n",
      "Gradient Descent(26/49): loss=1.817790309363788e+37\n",
      "Gradient Descent(27/49): loss=5.6766391384209354e+38\n",
      "Gradient Descent(28/49): loss=1.7727144732733465e+40\n",
      "Gradient Descent(29/49): loss=5.535875237309787e+41\n",
      "Gradient Descent(30/49): loss=1.7287563849169435e+43\n",
      "Gradient Descent(31/49): loss=5.398601865607487e+44\n",
      "Gradient Descent(32/49): loss=1.685888327448804e+46\n",
      "Gradient Descent(33/49): loss=5.2647324684838575e+47\n",
      "Gradient Descent(34/49): loss=1.6440832713191602e+49\n",
      "Gradient Descent(35/49): loss=5.134182637413146e+50\n",
      "Gradient Descent(36/49): loss=1.603314857231317e+52\n",
      "Gradient Descent(37/49): loss=5.006870056952012e+53\n",
      "Gradient Descent(38/49): loss=1.5635573795213626e+55\n",
      "Gradient Descent(39/49): loss=4.882714452837115e+56\n",
      "Gradient Descent(40/49): loss=1.5247857699499777e+58\n",
      "Gradient Descent(41/49): loss=4.761637541370058e+59\n",
      "Gradient Descent(42/49): loss=1.4869755818962492e+61\n",
      "Gradient Descent(43/49): loss=4.643562980057232e+62\n",
      "Gradient Descent(44/49): loss=1.450102974943308e+64\n",
      "Gradient Descent(45/49): loss=4.528416319473532e+65\n",
      "Gradient Descent(46/49): loss=1.4141446998462908e+67\n",
      "Gradient Descent(47/49): loss=4.416124956319936e+68\n",
      "Gradient Descent(48/49): loss=1.3790780838730017e+70\n",
      "Gradient Descent(49/49): loss=4.3066180876450786e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2556051747983088\n",
      "Gradient Descent(2/49): loss=25.365957276403858\n",
      "Gradient Descent(3/49): loss=760.717886584766\n",
      "Gradient Descent(4/49): loss=23406.09010853032\n",
      "Gradient Descent(5/49): loss=722554.6158044346\n",
      "Gradient Descent(6/49): loss=22323064.85127391\n",
      "Gradient Descent(7/49): loss=689812829.3376265\n",
      "Gradient Descent(8/49): loss=21317463301.33833\n",
      "Gradient Descent(9/49): loss=658790692489.2916\n",
      "Gradient Descent(10/49): loss=20359241152389.99\n",
      "Gradient Descent(11/49): loss=629182087706354.9\n",
      "Gradient Descent(12/49): loss=1.944425442262673e+16\n",
      "Gradient Descent(13/49): loss=6.009056563678228e+17\n",
      "Gradient Descent(14/49): loss=1.857040195211404e+19\n",
      "Gradient Descent(15/49): loss=5.7390012628102775e+20\n",
      "Gradient Descent(16/49): loss=1.7735822667147907e+22\n",
      "Gradient Descent(17/49): loss=5.481082709901426e+23\n",
      "Gradient Descent(18/49): loss=1.69387505973766e+25\n",
      "Gradient Descent(19/49): loss=5.234755375997019e+26\n",
      "Gradient Descent(20/49): loss=1.617750004023479e+28\n",
      "Gradient Descent(21/49): loss=4.999498329058024e+29\n",
      "Gradient Descent(22/49): loss=1.5450461122008327e+31\n",
      "Gradient Descent(23/49): loss=4.774814054748938e+32\n",
      "Gradient Descent(24/49): loss=1.4756096324500858e+34\n",
      "Gradient Descent(25/49): loss=4.5602273982039695e+35\n",
      "Gradient Descent(26/49): loss=1.409293722812138e+37\n",
      "Gradient Descent(27/49): loss=4.3552845587040906e+38\n",
      "Gradient Descent(28/49): loss=1.3459581406093359e+40\n",
      "Gradient Descent(29/49): loss=4.159552129956774e+41\n",
      "Gradient Descent(30/49): loss=1.285468945861503e+43\n",
      "Gradient Descent(31/49): loss=3.972616183539593e+44\n",
      "Gradient Descent(32/49): loss=1.2276982180338852e+46\n",
      "Gradient Descent(33/49): loss=3.7940813935380605e+47\n",
      "Gradient Descent(34/49): loss=1.1725237855150477e+49\n",
      "Gradient Descent(35/49): loss=3.623570200523548e+50\n",
      "Gradient Descent(36/49): loss=1.1198289672524343e+52\n",
      "Gradient Descent(37/49): loss=3.4607220131031915e+53\n",
      "Gradient Descent(38/49): loss=1.0695023259991411e+55\n",
      "Gradient Descent(39/49): loss=3.3051924453530718e+56\n",
      "Gradient Descent(40/49): loss=1.0214374326501205e+58\n",
      "Gradient Descent(41/49): loss=3.156652588522497e+59\n",
      "Gradient Descent(42/49): loss=9.755326411695097e+60\n",
      "Gradient Descent(43/49): loss=3.014788315468674e+62\n",
      "Gradient Descent(44/49): loss=9.316908736329124e+63\n",
      "Gradient Descent(45/49): loss=2.8792996163510793e+65\n",
      "Gradient Descent(46/49): loss=8.898194149303058e+66\n",
      "Gradient Descent(47/49): loss=2.7498999641806222e+68\n",
      "Gradient Descent(48/49): loss=8.498297166951415e+69\n",
      "Gradient Descent(49/49): loss=2.626315708881939e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2679152686374535\n",
      "Gradient Descent(2/49): loss=26.046156086621586\n",
      "Gradient Descent(3/49): loss=791.9250777930437\n",
      "Gradient Descent(4/49): loss=24698.662662112296\n",
      "Gradient Descent(5/49): loss=772947.069977558\n",
      "Gradient Descent(6/49): loss=24209979.96185919\n",
      "Gradient Descent(7/49): loss=758480197.9394307\n",
      "Gradient Descent(8/49): loss=23764290820.661495\n",
      "Gradient Descent(9/49): loss=744585447255.3191\n",
      "Gradient Descent(10/49): loss=23329579309154.89\n",
      "Gradient Descent(11/49): loss=730970856567716.2\n",
      "Gradient Descent(12/49): loss=2.290305682799651e+16\n",
      "Gradient Descent(13/49): loss=7.176073994660872e+17\n",
      "Gradient Descent(14/49): loss=2.2484352539787493e+19\n",
      "Gradient Descent(15/49): loss=7.044884382831499e+20\n",
      "Gradient Descent(16/49): loss=2.2073304571777538e+22\n",
      "Gradient Descent(17/49): loss=6.91609327892187e+23\n",
      "Gradient Descent(18/49): loss=2.1669771331398065e+25\n",
      "Gradient Descent(19/49): loss=6.789656684300164e+26\n",
      "Gradient Descent(20/49): loss=2.1273615298980032e+28\n",
      "Gradient Descent(21/49): loss=6.665531542112472e+29\n",
      "Gradient Descent(22/49): loss=2.088470159613531e+31\n",
      "Gradient Descent(23/49): loss=6.54367559442581e+32\n",
      "Gradient Descent(24/49): loss=2.0502897821158146e+34\n",
      "Gradient Descent(25/49): loss=6.4240473568550524e+35\n",
      "Gradient Descent(26/49): loss=2.012807399377955e+37\n",
      "Gradient Descent(27/49): loss=6.306606103498684e+38\n",
      "Gradient Descent(28/49): loss=1.9760102510045758e+40\n",
      "Gradient Descent(29/49): loss=6.1913118529933574e+41\n",
      "Gradient Descent(30/49): loss=1.9398858098802077e+43\n",
      "Gradient Descent(31/49): loss=6.07812535489583e+44\n",
      "Gradient Descent(32/49): loss=1.9044217779039714e+46\n",
      "Gradient Descent(33/49): loss=5.967008076320031e+47\n",
      "Gradient Descent(34/49): loss=1.8696060818027488e+49\n",
      "Gradient Descent(35/49): loss=5.857922188819173e+50\n",
      "Gradient Descent(36/49): loss=1.8354268690210805e+52\n",
      "Gradient Descent(37/49): loss=5.750830555507231e+53\n",
      "Gradient Descent(38/49): loss=1.801872503686006e+55\n",
      "Gradient Descent(39/49): loss=5.645696718416542e+56\n",
      "Gradient Descent(40/49): loss=1.76893156264588e+58\n",
      "Gradient Descent(41/49): loss=5.542484886085815e+59\n",
      "Gradient Descent(42/49): loss=1.7365928315814258e+61\n",
      "Gradient Descent(43/49): loss=5.441159921375597e+62\n",
      "Gradient Descent(44/49): loss=1.7048453011880193e+64\n",
      "Gradient Descent(45/49): loss=5.341687329506116e+65\n",
      "Gradient Descent(46/49): loss=1.673678163427656e+67\n",
      "Gradient Descent(47/49): loss=5.244033246314275e+68\n",
      "Gradient Descent(48/49): loss=1.6430808078496028e+70\n",
      "Gradient Descent(49/49): loss=5.148164426724704e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2793796233143684\n",
      "Gradient Descent(2/49): loss=27.01609083824064\n",
      "Gradient Descent(3/49): loss=839.78937440856\n",
      "Gradient Descent(4/49): loss=26743.11491540303\n",
      "Gradient Descent(5/49): loss=854243.7417927858\n",
      "Gradient Descent(6/49): loss=27306205.265495\n",
      "Gradient Descent(7/49): loss=873020694.0962012\n",
      "Gradient Descent(8/49): loss=27913283517.905315\n",
      "Gradient Descent(9/49): loss=892490885458.702\n",
      "Gradient Descent(10/49): loss=28536352582724.25\n",
      "Gradient Descent(11/49): loss=912417573389596.4\n",
      "Gradient Descent(12/49): loss=2.9173528653278504e+16\n",
      "Gradient Descent(13/49): loss=9.327909463439731e+17\n",
      "Gradient Descent(14/49): loss=2.9824947888071307e+19\n",
      "Gradient Descent(15/49): loss=9.536193788831433e+20\n",
      "Gradient Descent(16/49): loss=3.049091401640923e+22\n",
      "Gradient Descent(17/49): loss=9.749129041031218e+23\n",
      "Gradient Descent(18/49): loss=3.1171750713146903e+25\n",
      "Gradient Descent(19/49): loss=9.966818968911022e+26\n",
      "Gradient Descent(20/49): loss=3.1867789934033163e+28\n",
      "Gradient Descent(21/49): loss=1.0189369732217599e+30\n",
      "Gradient Descent(22/49): loss=3.2579371131430105e+31\n",
      "Gradient Descent(23/49): loss=1.0416889868699869e+33\n",
      "Gradient Descent(24/49): loss=3.3306841344137102e+34\n",
      "Gradient Descent(25/49): loss=1.0649490340267943e+36\n",
      "Gradient Descent(26/49): loss=3.4050555360580622e+37\n",
      "Gradient Descent(27/49): loss=1.0887284586567231e+39\n",
      "Gradient Descent(28/49): loss=3.4810875891359753e+40\n",
      "Gradient Descent(29/49): loss=1.1130388580259705e+42\n",
      "Gradient Descent(30/49): loss=3.558817374610366e+43\n",
      "Gradient Descent(31/49): loss=1.1378920883580757e+45\n",
      "Gradient Descent(32/49): loss=3.6382828014310844e+46\n",
      "Gradient Descent(33/49): loss=1.1633002706161462e+48\n",
      "Gradient Descent(34/49): loss=3.7195226250232877e+49\n",
      "Gradient Descent(35/49): loss=1.1892757964143168e+51\n",
      "Gradient Descent(36/49): loss=3.802576466188446e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=1.2158313340611213e+54\n",
      "Gradient Descent(38/49): loss=3.8874848304275796e+55\n",
      "Gradient Descent(39/49): loss=1.242979834737906e+57\n",
      "Gradient Descent(40/49): loss=3.9742891276957984e+58\n",
      "Gradient Descent(41/49): loss=1.2707345388150636e+60\n",
      "Gradient Descent(42/49): loss=4.0630316925979306e+61\n",
      "Gradient Descent(43/49): loss=1.299108982309457e+63\n",
      "Gradient Descent(44/49): loss=4.15375580503533e+64\n",
      "Gradient Descent(45/49): loss=1.3281170034859164e+66\n",
      "Gradient Descent(46/49): loss=4.246505711313501e+67\n",
      "Gradient Descent(47/49): loss=1.3577727496061883e+69\n",
      "Gradient Descent(48/49): loss=4.3413266457209864e+70\n",
      "Gradient Descent(49/49): loss=1.3880906838285994e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2771474879164981\n",
      "Gradient Descent(2/49): loss=26.826674927640067\n",
      "Gradient Descent(3/49): loss=834.0740246038715\n",
      "Gradient Descent(4/49): loss=26602.626369669277\n",
      "Gradient Descent(5/49): loss=851425.934435985\n",
      "Gradient Descent(6/49): loss=27273069.304368544\n",
      "Gradient Descent(7/49): loss=873820374.179415\n",
      "Gradient Descent(8/49): loss=27998770023.8867\n",
      "Gradient Descent(9/49): loss=897147605681.5573\n",
      "Gradient Descent(10/49): loss=28746910821681.504\n",
      "Gradient Descent(11/49): loss=921126181568406.8\n",
      "Gradient Descent(12/49): loss=2.9515304066806164e+16\n",
      "Gradient Descent(13/49): loss=9.457480425274838e+17\n",
      "Gradient Descent(14/49): loss=3.0304258085602243e+19\n",
      "Gradient Descent(15/49): loss=9.71028250868498e+20\n",
      "Gradient Descent(16/49): loss=3.111430294952954e+22\n",
      "Gradient Descent(17/49): loss=9.969842257178243e+23\n",
      "Gradient Descent(18/49): loss=3.1946000788746073e+25\n",
      "Gradient Descent(19/49): loss=1.0236340156026466e+27\n",
      "Gradient Descent(20/49): loss=3.2799930258823007e+28\n",
      "Gradient Descent(21/49): loss=1.050996165222442e+30\n",
      "Gradient Descent(22/49): loss=3.3676685608710944e+31\n",
      "Gradient Descent(23/49): loss=1.0790897161346373e+33\n",
      "Gradient Descent(24/49): loss=3.4576876982409497e+34\n",
      "Gradient Descent(25/49): loss=1.1079342189816183e+36\n",
      "Gradient Descent(26/49): loss=3.550113083419626e+37\n",
      "Gradient Descent(27/49): loss=1.1375497470104156e+39\n",
      "Gradient Descent(28/49): loss=3.64500903637977e+40\n",
      "Gradient Descent(29/49): loss=1.1679569100346818e+42\n",
      "Gradient Descent(30/49): loss=3.7424415963934455e+43\n",
      "Gradient Descent(31/49): loss=1.1991768687767785e+45\n",
      "Gradient Descent(32/49): loss=3.8424785679891074e+46\n",
      "Gradient Descent(33/49): loss=1.2312313495936868e+48\n",
      "Gradient Descent(34/49): loss=3.9451895681375735e+49\n",
      "Gradient Descent(35/49): loss=1.264142659596672e+51\n",
      "Gradient Descent(36/49): loss=4.0506460746993913e+52\n",
      "Gradient Descent(37/49): loss=1.2979337021750792e+54\n",
      "Gradient Descent(38/49): loss=4.158921476167043e+55\n",
      "Gradient Descent(39/49): loss=1.3326279929350683e+57\n",
      "Gradient Descent(40/49): loss=4.270091122737086e+58\n",
      "Gradient Descent(41/49): loss=1.3682496760644276e+60\n",
      "Gradient Descent(42/49): loss=4.384232378746989e+61\n",
      "Gradient Descent(43/49): loss=1.4048235411347917e+63\n",
      "Gradient Descent(44/49): loss=4.5014246765143596e+64\n",
      "Gradient Descent(45/49): loss=1.4423750403530683e+66\n",
      "Gradient Descent(46/49): loss=4.62174957161449e+67\n",
      "Gradient Descent(47/49): loss=1.480930306273879e+69\n",
      "Gradient Descent(48/49): loss=4.7452907996361744e+70\n",
      "Gradient Descent(49/49): loss=1.5205161699856087e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2783952880312779\n",
      "Gradient Descent(2/49): loss=26.691159101176048\n",
      "Gradient Descent(3/49): loss=821.8672936951112\n",
      "Gradient Descent(4/49): loss=25947.494031435905\n",
      "Gradient Descent(5/49): loss=821904.7231576099\n",
      "Gradient Descent(6/49): loss=26055060.194532324\n",
      "Gradient Descent(7/49): loss=826148915.0949273\n",
      "Gradient Descent(8/49): loss=26197019322.943005\n",
      "Gradient Descent(9/49): loss=830717310893.001\n",
      "Gradient Descent(10/49): loss=26342494376397.195\n",
      "Gradient Descent(11/49): loss=835335966451339.6\n",
      "Gradient Descent(12/49): loss=2.6489005463439796e+16\n",
      "Gradient Descent(13/49): loss=8.399823851816197e+17\n",
      "Gradient Descent(14/49): loss=2.663635045183379e+19\n",
      "Gradient Descent(15/49): loss=8.446548227897688e+20\n",
      "Gradient Descent(16/49): loss=2.678451663525359e+22\n",
      "Gradient Descent(17/49): loss=8.493532655483537e+23\n",
      "Gradient Descent(18/49): loss=2.6933507133964337e+25\n",
      "Gradient Descent(19/49): loss=8.540778448864414e+26\n",
      "Gradient Descent(20/49): loss=2.708332641243458e+28\n",
      "Gradient Descent(21/49): loss=8.588287050863884e+29\n",
      "Gradient Descent(22/49): loss=2.723397907071599e+31\n",
      "Gradient Descent(23/49): loss=8.636059922449307e+32\n",
      "Gradient Descent(24/49): loss=2.738546974369291e+34\n",
      "Gradient Descent(25/49): loss=8.684098533559627e+35\n",
      "Gradient Descent(26/49): loss=2.7537803092803033e+37\n",
      "Gradient Descent(27/49): loss=8.732404362380625e+38\n",
      "Gradient Descent(28/49): loss=2.769098380547767e+40\n",
      "Gradient Descent(29/49): loss=8.780978895326655e+41\n",
      "Gradient Descent(30/49): loss=2.784501659522822e+43\n",
      "Gradient Descent(31/49): loss=8.82982362708081e+44\n",
      "Gradient Descent(32/49): loss=2.799990620178543e+46\n",
      "Gradient Descent(33/49): loss=8.878940060640554e+47\n",
      "Gradient Descent(34/49): loss=2.8155657391245354e+49\n",
      "Gradient Descent(35/49): loss=8.92832970736375e+50\n",
      "Gradient Descent(36/49): loss=2.831227495621527e+52\n",
      "Gradient Descent(37/49): loss=8.977994087015201e+53\n",
      "Gradient Descent(38/49): loss=2.846976371596208e+55\n",
      "Gradient Descent(39/49): loss=9.027934727813783e+56\n",
      "Gradient Descent(40/49): loss=2.862812851655991e+58\n",
      "Gradient Descent(41/49): loss=9.078153166478878e+59\n",
      "Gradient Descent(42/49): loss=2.8787374231039386e+61\n",
      "Gradient Descent(43/49): loss=9.128650948278087e+62\n",
      "Gradient Descent(44/49): loss=2.894750575953781e+64\n",
      "Gradient Descent(45/49): loss=9.179429627074795e+65\n",
      "Gradient Descent(46/49): loss=2.9108528029450386e+67\n",
      "Gradient Descent(47/49): loss=9.23049076537568e+68\n",
      "Gradient Descent(48/49): loss=2.9270445995580194e+70\n",
      "Gradient Descent(49/49): loss=9.281835934378983e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2907035761005463\n",
      "Gradient Descent(2/49): loss=27.390789746145423\n",
      "Gradient Descent(3/49): loss=854.8034868011364\n",
      "Gradient Descent(4/49): loss=27347.087828390748\n",
      "Gradient Descent(5/49): loss=877887.8733113145\n",
      "Gradient Descent(6/49): loss=28205817.11639196\n",
      "Gradient Descent(7/49): loss=906452148.4208447\n",
      "Gradient Descent(8/49): loss=29132811075.393677\n",
      "Gradient Descent(9/49): loss=936330473473.7284\n",
      "Gradient Descent(10/49): loss=30093913227032.906\n",
      "Gradient Descent(11/49): loss=967228273096607.2\n",
      "Gradient Descent(12/49): loss=3.1087052179849624e+16\n",
      "Gradient Descent(13/49): loss=9.991488037227578e+17\n",
      "Gradient Descent(14/49): loss=3.211299594493242e+19\n",
      "Gradient Descent(15/49): loss=1.0321230627523677e+21\n",
      "Gradient Descent(16/49): loss=3.3172800913675605e+22\n",
      "Gradient Descent(17/49): loss=1.0661855756999969e+24\n",
      "Gradient Descent(18/49): loss=3.426758220435375e+25\n",
      "Gradient Descent(19/49): loss=1.1013722348400975e+27\n",
      "Gradient Descent(20/49): loss=3.5398493902572906e+28\n",
      "Gradient Descent(21/49): loss=1.1377201376105979e+30\n",
      "Gradient Descent(22/49): loss=3.656672837807799e+31\n",
      "Gradient Descent(23/49): loss=1.1752676076257875e+33\n",
      "Gradient Descent(24/49): loss=3.777351736949099e+34\n",
      "Gradient Descent(25/49): loss=1.2140542334402982e+36\n",
      "Gradient Descent(26/49): loss=3.902013326735682e+37\n",
      "Gradient Descent(27/49): loss=1.2541209101406832e+39\n",
      "Gradient Descent(28/49): loss=4.0307890454281553e+40\n",
      "Gradient Descent(29/49): loss=1.2955098824499347e+42\n",
      "Gradient Descent(30/49): loss=4.163814669063581e+43\n",
      "Gradient Descent(31/49): loss=1.338264789267554e+45\n",
      "Gradient Descent(32/49): loss=4.3012304546112916e+46\n",
      "Gradient Descent(33/49): loss=1.3824307096804879e+48\n",
      "Gradient Descent(34/49): loss=4.443181287854081e+49\n",
      "Gradient Descent(35/49): loss=1.4280542104927273e+51\n",
      "Gradient Descent(36/49): loss=4.5898168361501464e+52\n",
      "Gradient Descent(37/49): loss=1.4751833953235399e+54\n",
      "Gradient Descent(38/49): loss=4.74129170623641e+55\n",
      "Gradient Descent(39/49): loss=1.5238679553260398e+57\n",
      "Gradient Descent(40/49): loss=4.897765607239739e+58\n",
      "Gradient Descent(41/49): loss=1.574159221579542e+60\n",
      "Gradient Descent(42/49): loss=5.059403519068004e+61\n",
      "Gradient Descent(43/49): loss=1.6261102192110319e+63\n",
      "Gradient Descent(44/49): loss=5.2263758663583437e+64\n",
      "Gradient Descent(45/49): loss=1.6797757233028006e+66\n",
      "Gradient Descent(46/49): loss=5.398858698166201e+67\n",
      "Gradient Descent(47/49): loss=1.7352123166451063e+69\n",
      "Gradient Descent(48/49): loss=5.577033873584445e+70\n",
      "Gradient Descent(49/49): loss=1.792478449394825e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3024915093360976\n",
      "Gradient Descent(2/49): loss=28.409251123372343\n",
      "Gradient Descent(3/49): loss=906.359173494379\n",
      "Gradient Descent(4/49): loss=29606.088222484563\n",
      "Gradient Descent(5/49): loss=970032.8849343662\n",
      "Gradient Descent(6/49): loss=31805671.42396286\n",
      "Gradient Descent(7/49): loss=1043055749.8938926\n",
      "Gradient Descent(8/49): loss=34208504633.40145\n",
      "Gradient Descent(9/49): loss=1121933900643.755\n",
      "Gradient Descent(10/49): loss=36796143716347.086\n",
      "Gradient Descent(11/49): loss=1206807116154809.2\n",
      "Gradient Descent(12/49): loss=3.957979718799677e+16\n",
      "Gradient Descent(13/49): loss=1.2981034603096335e+18\n",
      "Gradient Descent(14/49): loss=4.257405944619297e+19\n",
      "Gradient Descent(15/49): loss=1.3963066940665168e+21\n",
      "Gradient Descent(16/49): loss=4.579484346410063e+22\n",
      "Gradient Descent(17/49): loss=1.501939149932988e+24\n",
      "Gradient Descent(18/49): loss=4.925928423014219e+25\n",
      "Gradient Descent(19/49): loss=1.6155628429330232e+27\n",
      "Gradient Descent(20/49): loss=5.298581455843851e+28\n",
      "Gradient Descent(21/49): loss=1.7377823194636733e+30\n",
      "Gradient Descent(22/49): loss=5.699426186065279e+31\n",
      "Gradient Descent(23/49): loss=1.8692478618634488e+33\n",
      "Gradient Descent(24/49): loss=6.13059535295708e+34\n",
      "Gradient Descent(25/49): loss=2.0106589472961667e+36\n",
      "Gradient Descent(26/49): loss=6.594383040453232e+37\n",
      "Gradient Descent(27/49): loss=2.1627679693114933e+39\n",
      "Gradient Descent(28/49): loss=7.093256882994661e+40\n",
      "Gradient Descent(29/49): loss=2.3263842410320104e+42\n",
      "Gradient Descent(30/49): loss=7.629871194848416e+43\n",
      "Gradient Descent(31/49): loss=2.5023783011938332e+45\n",
      "Gradient Descent(32/49): loss=8.207081092684295e+46\n",
      "Gradient Descent(33/49): loss=2.6916865459455638e+48\n",
      "Gradient Descent(34/49): loss=8.827957686542999e+49\n",
      "Gradient Descent(35/49): loss=2.8953162110492552e+51\n",
      "Gradient Descent(36/49): loss=9.495804420022455e+52\n",
      "Gradient Descent(37/49): loss=3.114350730991155e+54\n",
      "Gradient Descent(38/49): loss=1.0214174646619586e+56\n",
      "Gradient Descent(39/49): loss=3.3499555035165424e+57\n",
      "Gradient Descent(40/49): loss=1.0986890535746627e+59\n",
      "Gradient Descent(41/49): loss=3.6033840902592435e+60\n",
      "Gradient Descent(42/49): loss=1.1818063409012698e+62\n",
      "Gradient Descent(43/49): loss=3.875984886456993e+63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=1.2712115614971276e+65\n",
      "Gradient Descent(45/49): loss=4.169208295239538e+66\n",
      "Gradient Descent(46/49): loss=1.3673804058722346e+68\n",
      "Gradient Descent(47/49): loss=4.484614444661357e+69\n",
      "Gradient Descent(48/49): loss=1.4708245511559902e+71\n",
      "Gradient Descent(49/49): loss=4.823881488538049e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3001544203128894\n",
      "Gradient Descent(2/49): loss=28.21000254660162\n",
      "Gradient Descent(3/49): loss=900.1860896494171\n",
      "Gradient Descent(4/49): loss=29450.173115916547\n",
      "Gradient Descent(5/49): loss=966814.6585285965\n",
      "Gradient Descent(6/49): loss=31766313.8863069\n",
      "Gradient Descent(7/49): loss=1043981584.6200049\n",
      "Gradient Descent(8/49): loss=34312154345.485157\n",
      "Gradient Descent(9/49): loss=1127746445448.3599\n",
      "Gradient Descent(10/49): loss=37066137571694.99\n",
      "Gradient Descent(11/49): loss=1218270936203368.0\n",
      "Gradient Descent(12/49): loss=4.004152687928351e+16\n",
      "Gradient Descent(13/49): loss=1.3160652812288617e+18\n",
      "Gradient Descent(14/49): loss=4.325579026787908e+19\n",
      "Gradient Descent(15/49): loss=1.421710182830229e+21\n",
      "Gradient Descent(16/49): loss=4.672807588547885e+22\n",
      "Gradient Descent(17/49): loss=1.5358355762214075e+24\n",
      "Gradient Descent(18/49): loss=5.0479093630114e+25\n",
      "Gradient Descent(19/49): loss=1.659122195990478e+27\n",
      "Gradient Descent(20/49): loss=5.453121804157705e+28\n",
      "Gradient Descent(21/49): loss=1.7923054421801243e+30\n",
      "Gradient Descent(22/49): loss=5.890861993261692e+31\n",
      "Gradient Descent(23/49): loss=1.9361797496670433e+33\n",
      "Gradient Descent(24/49): loss=6.363741040461215e+34\n",
      "Gradient Descent(25/49): loss=2.091603325415216e+36\n",
      "Gradient Descent(26/49): loss=6.874579658525817e+37\n",
      "Gradient Descent(27/49): loss=2.2595032675249274e+39\n",
      "Gradient Descent(28/49): loss=7.426424988216149e+40\n",
      "Gradient Descent(29/49): loss=2.4408810953398225e+42\n",
      "Gradient Descent(30/49): loss=8.022568774398111e+43\n",
      "Gradient Descent(31/49): loss=2.6368187234859017e+45\n",
      "Gradient Descent(32/49): loss=8.666567001225088e+46\n",
      "Gradient Descent(33/49): loss=2.848484915467719e+48\n",
      "Gradient Descent(34/49): loss=9.36226110350283e+49\n",
      "Gradient Descent(35/49): loss=3.077142255316105e+51\n",
      "Gradient Descent(36/49): loss=1.0113800880760759e+53\n",
      "Gradient Descent(37/49): loss=3.324154678873269e+54\n",
      "Gradient Descent(38/49): loss=1.0925669250711808e+56\n",
      "Gradient Descent(39/49): loss=3.5909956096390516e+57\n",
      "Gradient Descent(40/49): loss=1.1802708989755296e+59\n",
      "Gradient Descent(41/49): loss=3.879256747708824e+60\n",
      "Gradient Descent(42/49): loss=1.2750151620027694e+62\n",
      "Gradient Descent(43/49): loss=4.190657564228256e+63\n",
      "Gradient Descent(44/49): loss=1.3773648615313803e+65\n",
      "Gradient Descent(45/49): loss=4.5270555579998215e+66\n",
      "Gradient Descent(46/49): loss=1.4879305111959416e+68\n",
      "Gradient Descent(47/49): loss=4.8904573354208904e+69\n",
      "Gradient Descent(48/49): loss=1.6073716325871069e+71\n",
      "Gradient Descent(49/49): loss=5.283030579845352e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3015003308221547\n",
      "Gradient Descent(2/49): loss=28.068442900208716\n",
      "Gradient Descent(3/49): loss=887.06167920041\n",
      "Gradient Descent(4/49): loss=28727.15770714851\n",
      "Gradient Descent(5/49): loss=933384.6620748325\n",
      "Gradient Descent(6/49): loss=30351239.550933197\n",
      "Gradient Descent(7/49): loss=987163723.6523606\n",
      "Gradient Descent(8/49): loss=32109216051.07279\n",
      "Gradient Descent(9/49): loss=1044427300278.5701\n",
      "Gradient Descent(10/49): loss=33972619698291.695\n",
      "Gradient Descent(11/49): loss=1105046432609207.8\n",
      "Gradient Descent(12/49): loss=3.5944480327635056e+16\n",
      "Gradient Descent(13/49): loss=1.16918691714014e+18\n",
      "Gradient Descent(14/49): loss=3.803082101793567e+19\n",
      "Gradient Descent(15/49): loss=1.2370505875530323e+21\n",
      "Gradient Descent(16/49): loss=4.0238262544658206e+22\n",
      "Gradient Descent(17/49): loss=1.3088533244757832e+24\n",
      "Gradient Descent(18/49): loss=4.257383189777901e+25\n",
      "Gradient Descent(19/49): loss=1.384823745106801e+27\n",
      "Gradient Descent(20/49): loss=4.504496587616702e+28\n",
      "Gradient Descent(21/49): loss=1.4652037546057887e+30\n",
      "Gradient Descent(22/49): loss=4.765953310779726e+31\n",
      "Gradient Descent(23/49): loss=1.5502493007639996e+33\n",
      "Gradient Descent(24/49): loss=5.042585895845593e+34\n",
      "Gradient Descent(25/49): loss=1.64023118761935e+36\n",
      "Gradient Descent(26/49): loss=5.3352752028591125e+37\n",
      "Gradient Descent(27/49): loss=1.7354359376350055e+39\n",
      "Gradient Descent(28/49): loss=5.644953220072189e+40\n",
      "Gradient Descent(29/49): loss=1.8361667040402905e+42\n",
      "Gradient Descent(30/49): loss=5.972606031592686e+43\n",
      "Gradient Descent(31/49): loss=1.9427442361374314e+45\n",
      "Gradient Descent(32/49): loss=6.319276957296215e+46\n",
      "Gradient Descent(33/49): loss=2.0555079006390662e+48\n",
      "Gradient Descent(34/49): loss=6.686069874989904e+49\n",
      "Gradient Descent(35/49): loss=2.1748167622877454e+51\n",
      "Gradient Descent(36/49): loss=7.074152735406364e+52\n",
      "Gradient Descent(37/49): loss=2.301050727198509e+54\n",
      "Gradient Descent(38/49): loss=7.484761281220264e+55\n",
      "Gradient Descent(39/49): loss=2.4346117525648504e+57\n",
      "Gradient Descent(40/49): loss=7.919202981929365e+58\n",
      "Gradient Descent(41/49): loss=2.575925126580464e+60\n",
      "Gradient Descent(42/49): loss=8.378861197130225e+61\n",
      "Gradient Descent(43/49): loss=2.725440822652003e+63\n",
      "Gradient Descent(44/49): loss=8.865199581444517e+64\n",
      "Gradient Descent(45/49): loss=2.8836349322150895e+66\n",
      "Gradient Descent(46/49): loss=9.379766745123007e+67\n",
      "Gradient Descent(47/49): loss=3.0510111807160144e+69\n",
      "Gradient Descent(48/49): loss=9.924201185167203e+70\n",
      "Gradient Descent(49/49): loss=3.2281025315862868e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3138024513565254\n",
      "Gradient Descent(2/49): loss=28.78763288559274\n",
      "Gradient Descent(3/49): loss=921.7908868466867\n",
      "Gradient Descent(4/49): loss=30240.760694091477\n",
      "Gradient Descent(5/49): loss=995481.8758733763\n",
      "Gradient Descent(6/49): loss=32798122.792404946\n",
      "Gradient Descent(7/49): loss=1080867804.8308635\n",
      "Gradient Descent(8/49): loss=35622795679.883095\n",
      "Gradient Descent(9/49): loss=1174066929416.0154\n",
      "Gradient Descent(10/49): loss=38695504526553.62\n",
      "Gradient Descent(11/49): loss=1275348879698316.2\n",
      "Gradient Descent(12/49): loss=4.203371199194517e+16\n",
      "Gradient Descent(13/49): loss=1.38537248244689e+18\n",
      "Gradient Descent(14/49): loss=4.56599458136714e+19\n",
      "Gradient Descent(15/49): loss=1.504888186665997e+21\n",
      "Gradient Descent(16/49): loss=4.959901778475987e+22\n",
      "Gradient Descent(17/49): loss=1.6347145191724933e+24\n",
      "Gradient Descent(18/49): loss=5.387791290229568e+25\n",
      "Gradient Descent(19/49): loss=1.7757409413094587e+27\n",
      "Gradient Descent(20/49): loss=5.852594729255259e+28\n",
      "Gradient Descent(21/49): loss=1.928933678805807e+30\n",
      "Gradient Descent(22/49): loss=6.357496647843616e+31\n",
      "Gradient Descent(23/49): loss=2.0953423164036558e+33\n",
      "Gradient Descent(24/49): loss=6.90595633173044e+34\n",
      "Gradient Descent(25/49): loss=2.276106986548521e+36\n",
      "Gradient Descent(26/49): loss=7.501731498665575e+37\n",
      "Gradient Descent(27/49): loss=2.4724661806608794e+39\n",
      "Gradient Descent(28/49): loss=8.148904043818713e+40\n",
      "Gradient Descent(29/49): loss=2.685765234516387e+42\n",
      "Gradient Descent(30/49): loss=8.851908006461893e+43\n",
      "Gradient Descent(31/49): loss=2.9174655456798904e+45\n",
      "Gradient Descent(32/49): loss=9.615559949353036e+46\n",
      "Gradient Descent(33/49): loss=3.16915458612744e+48\n",
      "Gradient Descent(34/49): loss=1.0445091958943453e+50\n",
      "Gradient Descent(35/49): loss=3.4425567786549685e+51\n",
      "Gradient Descent(36/49): loss=1.1346187492505367e+53\n",
      "Gradient Descent(37/49): loss=3.739545311592028e+54\n",
      "Gradient Descent(38/49): loss=1.2325020317782568e+56\n",
      "Gradient Descent(39/49): loss=4.062154972768134e+57\n",
      "Gradient Descent(40/49): loss=1.338829681195498e+59\n",
      "Gradient Descent(41/49): loss=4.412596090662122e+60\n",
      "Gradient Descent(42/49): loss=1.4543301909724778e+62\n",
      "Gradient Descent(43/49): loss=4.7932696782511674e+63\n",
      "Gradient Descent(44/49): loss=1.5797949015332553e+65\n",
      "Gradient Descent(45/49): loss=5.206783883315846e+66\n",
      "Gradient Descent(46/49): loss=1.716083422047105e+68\n",
      "Gradient Descent(47/49): loss=5.6559718579090646e+69\n",
      "Gradient Descent(48/49): loss=1.864129519956484e+71\n",
      "Gradient Descent(49/49): loss=6.14391116941987e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.32591824198959\n",
      "Gradient Descent(2/49): loss=29.856412302041576\n",
      "Gradient Descent(3/49): loss=977.2720055021917\n",
      "Gradient Descent(4/49): loss=32733.748562849385\n",
      "Gradient Descent(5/49): loss=1099760.7777774544\n",
      "Gradient Descent(6/49): loss=36975695.33386352\n",
      "Gradient Descent(7/49): loss=1243427597.5999575\n",
      "Gradient Descent(8/49): loss=41816596338.171394\n",
      "Gradient Descent(9/49): loss=1406318234246.8945\n",
      "Gradient Descent(10/49): loss=47295566784247.98\n",
      "Gradient Descent(11/49): loss=1590588344351561.8\n",
      "Gradient Descent(12/49): loss=5.349279711806554e+16\n",
      "Gradient Descent(13/49): loss=1.7990070360949274e+18\n",
      "Gradient Descent(14/49): loss=6.0502097092036805e+19\n",
      "Gradient Descent(15/49): loss=2.034735666652539e+21\n",
      "Gradient Descent(16/49): loss=6.842984691732214e+22\n",
      "Gradient Descent(17/49): loss=2.3013524698816704e+24\n",
      "Gradient Descent(18/49): loss=7.739639105062795e+25\n",
      "Gradient Descent(19/49): loss=2.602904781579127e+27\n",
      "Gradient Descent(20/49): loss=8.753784524175943e+28\n",
      "Gradient Descent(21/49): loss=2.9439702918948063e+30\n",
      "Gradient Descent(22/49): loss=9.90081610488861e+31\n",
      "Gradient Descent(23/49): loss=3.3297265197523637e+33\n",
      "Gradient Descent(24/49): loss=1.1198146272880311e+35\n",
      "Gradient Descent(25/49): loss=3.766029408269606e+36\n",
      "Gradient Descent(26/49): loss=1.2665469050265883e+38\n",
      "Gradient Descent(27/49): loss=4.259502220322532e+39\n",
      "Gradient Descent(28/49): loss=1.4325059019074952e+41\n",
      "Gradient Descent(29/49): loss=4.81763608247263e+42\n",
      "Gradient Descent(30/49): loss=1.6202109458841817e+44\n",
      "Gradient Descent(31/49): loss=5.448903703443691e+45\n",
      "Gradient Descent(32/49): loss=1.8325114791271602e+47\n",
      "Gradient Descent(33/49): loss=6.162887993433451e+48\n",
      "Gradient Descent(34/49): loss=2.0726303137646286e+50\n",
      "Gradient Descent(35/49): loss=6.97042753675427e+51\n",
      "Gradient Descent(36/49): loss=2.344212555537283e+53\n",
      "Gradient Descent(37/49): loss=7.883781126139512e+54\n",
      "Gradient Descent(38/49): loss=2.6513809380492745e+56\n",
      "Gradient Descent(39/49): loss=8.916813856415835e+57\n",
      "Gradient Descent(40/49): loss=2.998798407612756e+59\n",
      "Gradient Descent(41/49): loss=1.008520760252305e+61\n",
      "Gradient Descent(42/49): loss=3.3917389087503903e+62\n",
      "Gradient Descent(43/49): loss=1.1406699077025809e+64\n",
      "Gradient Descent(44/49): loss=3.8361674449097746e+65\n",
      "Gradient Descent(45/49): loss=1.2901349080932036e+67\n",
      "Gradient Descent(46/49): loss=4.338830629745409e+68\n",
      "Gradient Descent(47/49): loss=1.4591847035160522e+70\n",
      "Gradient Descent(48/49): loss=4.90735910357525e+71\n",
      "Gradient Descent(49/49): loss=1.650385541557183e+73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3234744640446132\n",
      "Gradient Descent(2/49): loss=29.646957951219303\n",
      "Gradient Descent(3/49): loss=970.6110944603729\n",
      "Gradient Descent(4/49): loss=32560.946462270284\n",
      "Gradient Descent(5/49): loss=1096091.7525712037\n",
      "Gradient Descent(6/49): loss=36929077.319341816\n",
      "Gradient Descent(7/49): loss=1244496889.4378338\n",
      "Gradient Descent(8/49): loss=41941967586.86429\n",
      "Gradient Descent(9/49): loss=1413553610280.7068\n",
      "Gradient Descent(10/49): loss=47640708339147.31\n",
      "Gradient Descent(11/49): loss=1605627649190738.0\n",
      "Gradient Descent(12/49): loss=5.411425304543861e+16\n",
      "Gradient Descent(13/49): loss=1.823805647362962e+18\n",
      "Gradient Descent(14/49): loss=6.146748740014474e+19\n",
      "Gradient Descent(15/49): loss=2.0716308530604307e+21\n",
      "Gradient Descent(16/49): loss=6.9819909459787475e+22\n",
      "Gradient Descent(17/49): loss=2.3531314733169534e+24\n",
      "Gradient Descent(18/49): loss=7.930728892394558e+25\n",
      "Gradient Descent(19/49): loss=2.672883410154768e+27\n",
      "Gradient Descent(20/49): loss=9.00838475425288e+28\n",
      "Gradient Descent(21/49): loss=3.0360843863542835e+30\n",
      "Gradient Descent(22/49): loss=1.0232476356795433e+32\n",
      "Gradient Descent(23/49): loss=3.448638412786643e+33\n",
      "Gradient Descent(24/49): loss=1.1622901912938337e+35\n",
      "Gradient Descent(25/49): loss=3.9172517587493637e+36\n",
      "Gradient Descent(26/49): loss=1.320226347633855e+38\n",
      "Gradient Descent(27/49): loss=4.4495419654699984e+39\n",
      "Gradient Descent(28/49): loss=1.499623434872506e+41\n",
      "Gradient Descent(29/49): loss=5.054161672978545e+42\n",
      "Gradient Descent(30/49): loss=1.7033976412069784e+44\n",
      "Gradient Descent(31/49): loss=5.740939273039786e+45\n",
      "Gradient Descent(32/49): loss=1.934861416937111e+47\n",
      "Gradient Descent(33/49): loss=6.52103867451226e+48\n",
      "Gradient Descent(34/49): loss=2.197777319979851e+50\n",
      "Gradient Descent(35/49): loss=7.407140778196761e+51\n",
      "Gradient Descent(36/49): loss=2.496419178105289e+53\n",
      "Gradient Descent(37/49): loss=8.413649611137922e+54\n",
      "Gradient Descent(38/49): loss=2.835641562116514e+56\n",
      "Gradient Descent(39/49): loss=9.556926471192951e+57\n",
      "Gradient Descent(40/49): loss=3.220958699293998e+59\n",
      "Gradient Descent(41/49): loss=1.0855555888003768e+61\n",
      "Gradient Descent(42/49): loss=3.6586341098817244e+62\n",
      "Gradient Descent(43/49): loss=1.233064772369886e+64\n",
      "Gradient Descent(44/49): loss=4.155782423700066e+65\n",
      "Gradient Descent(45/49): loss=1.400618032412155e+67\n",
      "Gradient Descent(46/49): loss=4.720485031965351e+68\n",
      "Gradient Descent(47/49): loss=1.590939029867629e+70\n",
      "Gradient Descent(48/49): loss=5.361921454292471e+71\n",
      "Gradient Descent(49/49): loss=1.8071215264857755e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3249203031709398\n",
      "Gradient Descent(2/49): loss=29.499156488372886\n",
      "Gradient Descent(3/49): loss=956.5123565518446\n",
      "Gradient Descent(4/49): loss=31763.977359206077\n",
      "Gradient Descent(5/49): loss=1058293.4414065168\n",
      "Gradient Descent(6/49): loss=35288103.95708233\n",
      "Gradient Descent(7/49): loss=1176925146.9830873\n",
      "Gradient Descent(8/49): loss=39255235567.41219\n",
      "Gradient Descent(9/49): loss=1309346198903.9688\n",
      "Gradient Descent(10/49): loss=43673073712614.516\n",
      "Gradient Descent(11/49): loss=1456712042371641.2\n",
      "Gradient Descent(12/49): loss=4.858854127528986e+16\n",
      "Gradient Descent(13/49): loss=1.6206680439353546e+18\n",
      "Gradient Descent(14/49): loss=5.405729294772317e+19\n",
      "Gradient Descent(15/49): loss=1.8030780358456063e+21\n",
      "Gradient Descent(16/49): loss=6.014156900169144e+22\n",
      "Gradient Descent(17/49): loss=2.006018737663843e+24\n",
      "Gradient Descent(18/49): loss=6.6910645062142045e+25\n",
      "Gradient Descent(19/49): loss=2.231800899426684e+27\n",
      "Gradient Descent(20/49): loss=7.444159670213466e+28\n",
      "Gradient Descent(21/49): loss=2.4829953787649853e+30\n",
      "Gradient Descent(22/49): loss=8.282017479621513e+31\n",
      "Gradient Descent(23/49): loss=2.7624623919723604e+33\n",
      "Gradient Descent(24/49): loss=9.21417817076578e+34\n",
      "Gradient Descent(25/49): loss=3.07338407970151e+36\n",
      "Gradient Descent(26/49): loss=1.0251255756407686e+38\n",
      "Gradient Descent(27/49): loss=3.4193007400977926e+39\n",
      "Gradient Descent(28/49): loss=1.1405058881615892e+41\n",
      "Gradient Descent(29/49): loss=3.8041511402534606e+42\n",
      "Gradient Descent(30/49): loss=1.2688725282443608e+44\n",
      "Gradient Descent(31/49): loss=4.232317364829944e+45\n",
      "Gradient Descent(32/49): loss=1.4116871378266158e+47\n",
      "Gradient Descent(33/49): loss=4.708674712500392e+48\n",
      "Gradient Descent(34/49): loss=1.5705758701092477e+50\n",
      "Gradient Descent(35/49): loss=5.238647208355473e+51\n",
      "Gradient Descent(36/49): loss=1.7473479056889924e+53\n",
      "Gradient Descent(37/49): loss=5.8282693643616513e+54\n",
      "Gradient Descent(38/49): loss=1.9440160527254848e+56\n",
      "Gradient Descent(39/49): loss=6.484254891105745e+57\n",
      "Gradient Descent(40/49): loss=2.162819665706036e+59\n",
      "Gradient Descent(41/49): loss=7.214073143208989e+60\n",
      "Gradient Descent(42/49): loss=2.406250143771473e+62\n",
      "Gradient Descent(43/49): loss=8.026034168853316e+63\n",
      "Gradient Descent(44/49): loss=2.677079298939218e+65\n",
      "Gradient Descent(45/49): loss=8.929383331833934e+66\n",
      "Gradient Descent(46/49): loss=2.978390917236864e+68\n",
      "Gradient Descent(47/49): loss=9.934406583547503e+69\n",
      "Gradient Descent(48/49): loss=3.313615872116345e+71\n",
      "Gradient Descent(49/49): loss=1.1052547583592442e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3372118944053903\n",
      "Gradient Descent(2/49): loss=30.238020532127074\n",
      "Gradient Descent(3/49): loss=993.0993688922366\n",
      "Gradient Descent(4/49): loss=33398.89618874095\n",
      "Gradient Descent(5/49): loss=1127067.5146483716\n",
      "Gradient Descent(6/49): loss=38066771.1324548\n",
      "Gradient Descent(7/49): loss=1286031180.4644969\n",
      "Gradient Descent(8/49): loss=43449950739.07163\n",
      "Gradient Descent(9/49): loss=1468035960082.8855\n",
      "Gradient Descent(10/49): loss=49600603917580.46\n",
      "Gradient Descent(11/49): loss=1675861364790321.2\n",
      "Gradient Descent(12/49): loss=5.662255576330955e+16\n",
      "Gradient Descent(13/49): loss=1.9131143239787663e+18\n",
      "Gradient Descent(14/49): loss=6.463866842841592e+19\n",
      "Gradient Descent(15/49): loss=2.1839560076682406e+21\n",
      "Gradient Descent(16/49): loss=7.378963678447507e+22\n",
      "Gradient Descent(17/49): loss=2.493141109620869e+24\n",
      "Gradient Descent(18/49): loss=8.423611859213423e+25\n",
      "Gradient Descent(19/49): loss=2.846097899938269e+27\n",
      "Gradient Descent(20/49): loss=9.616152063643322e+28\n",
      "Gradient Descent(21/49): loss=3.2490231806196795e+30\n",
      "Gradient Descent(22/49): loss=1.0977521526670874e+32\n",
      "Gradient Descent(23/49): loss=3.7089910465229625e+33\n",
      "Gradient Descent(24/49): loss=1.253162159579332e+35\n",
      "Gradient Descent(25/49): loss=4.2340770805415687e+36\n",
      "Gradient Descent(26/49): loss=1.4305737359629377e+38\n",
      "Gradient Descent(27/49): loss=4.833500134969697e+39\n",
      "Gradient Descent(28/49): loss=1.633101668752935e+41\n",
      "Gradient Descent(29/49): loss=5.517784185394165e+42\n",
      "Gradient Descent(30/49): loss=1.8643017087744945e+44\n",
      "Gradient Descent(31/49): loss=6.298943098462725e+45\n",
      "Gradient Descent(32/49): loss=2.1282329984964096e+47\n",
      "Gradient Descent(33/49): loss=7.190691557436679e+48\n",
      "Gradient Descent(34/49): loss=2.429529337752095e+50\n",
      "Gradient Descent(35/49): loss=8.208685848711656e+51\n",
      "Gradient Descent(36/49): loss=2.7734805386291177e+53\n",
      "Gradient Descent(37/49): loss=9.370798736757286e+54\n",
      "Gradient Descent(38/49): loss=3.166125297861859e+56\n",
      "Gradient Descent(39/49): loss=1.0697433253410954e+58\n",
      "Gradient Descent(40/49): loss=3.6143572172731826e+59\n",
      "Gradient Descent(41/49): loss=1.2211880910674787e+61\n",
      "Gradient Descent(42/49): loss=4.126045833649321e+62\n",
      "Gradient Descent(43/49): loss=1.394073062610164e+64\n",
      "Gradient Descent(44/49): loss=4.710174782950438e+65\n",
      "Gradient Descent(45/49): loss=1.5914335540206994e+67\n",
      "Gradient Descent(46/49): loss=5.376999524583499e+68\n",
      "Gradient Descent(47/49): loss=1.816734592174822e+70\n",
      "Gradient Descent(48/49): loss=6.138227394878262e+71\n",
      "Gradient Descent(49/49): loss=2.0739317517001786e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3496598212748465\n",
      "Gradient Descent(2/49): loss=31.358952881731714\n",
      "Gradient Descent(3/49): loss=1052.751674543697\n",
      "Gradient Descent(4/49): loss=36146.8007810535\n",
      "Gradient Descent(5/49): loss=1244899.4947820965\n",
      "Gradient Descent(6/49): loss=42905928.83785363\n",
      "Gradient Descent(7/49): loss=1479065913.84986\n",
      "Gradient Descent(8/49): loss=50989667666.89216\n",
      "Gradient Descent(9/49): loss=1757857714142.0837\n",
      "Gradient Descent(10/49): loss=60602033975405.16\n",
      "Gradient Descent(11/49): loss=2089253945447158.8\n",
      "Gradient Descent(12/49): loss=7.20270149583681e+16\n",
      "Gradient Descent(13/49): loss=2.4831310806942065e+18\n",
      "Gradient Descent(14/49): loss=8.560593748493305e+19\n",
      "Gradient Descent(15/49): loss=2.9512644937299043e+21\n",
      "Gradient Descent(16/49): loss=1.0174483672137204e+23\n",
      "Gradient Descent(17/49): loss=3.5076530172449776e+24\n",
      "Gradient Descent(18/49): loss=1.2092632990623696e+26\n",
      "Gradient Descent(19/49): loss=4.168934952645039e+27\n",
      "Gradient Descent(20/49): loss=1.4372402315619525e+29\n",
      "Gradient Descent(21/49): loss=4.954885376463216e+30\n",
      "Gradient Descent(22/49): loss=1.7081966225810742e+32\n",
      "Gradient Descent(23/49): loss=5.889007473834154e+33\n",
      "Gradient Descent(24/49): loss=2.030235194732778e+35\n",
      "Gradient Descent(25/49): loss=6.999235379214311e+36\n",
      "Gradient Descent(26/49): loss=2.4129862402515274e+38\n",
      "Gradient Descent(27/49): loss=8.318769522931476e+39\n",
      "Gradient Descent(28/49): loss=2.8678956067499168e+41\n",
      "Gradient Descent(29/49): loss=9.887069462067669e+42\n",
      "Gradient Descent(30/49): loss=3.408566975648474e+44\n",
      "Gradient Descent(31/49): loss=1.175103388527391e+46\n",
      "Gradient Descent(32/49): loss=4.0511686688094276e+47\n",
      "Gradient Descent(33/49): loss=1.3966403078549696e+49\n",
      "Gradient Descent(34/49): loss=4.814917148582915e+50\n",
      "Gradient Descent(35/49): loss=1.6599425791543939e+52\n",
      "Gradient Descent(36/49): loss=5.722651669926852e+53\n",
      "Gradient Descent(37/49): loss=1.9728840350609792e+55\n",
      "Gradient Descent(38/49): loss=6.801517269088427e+56\n",
      "Gradient Descent(39/49): loss=2.3448229262130917e+58\n",
      "Gradient Descent(40/49): loss=8.083776513047677e+59\n",
      "Gradient Descent(41/49): loss=2.786881771854649e+61\n",
      "Gradient Descent(42/49): loss=9.607774284407535e+62\n",
      "Gradient Descent(43/49): loss=3.31227996940436e+64\n",
      "Gradient Descent(44/49): loss=1.1419084452808742e+66\n",
      "Gradient Descent(45/49): loss=3.936729109400314e+67\n",
      "Gradient Descent(46/49): loss=1.3571872723112956e+69\n",
      "Gradient Descent(47/49): loss=4.6789028173807223e+70\n",
      "Gradient Descent(48/49): loss=1.6130516415181833e+72\n",
      "Gradient Descent(49/49): loss=5.560995172926193e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3471076191116695\n",
      "Gradient Descent(2/49): loss=31.138910346929205\n",
      "Gradient Descent(3/49): loss=1045.5712833210766\n",
      "Gradient Descent(4/49): loss=35955.534456787645\n",
      "Gradient Descent(5/49): loss=1240723.7455200534\n",
      "Gradient Descent(6/49): loss=42850857.85148385\n",
      "Gradient Descent(7/49): loss=1480297942.9598038\n",
      "Gradient Descent(8/49): loss=51140958593.659\n",
      "Gradient Descent(9/49): loss=1766840144791.775\n",
      "Gradient Descent(10/49): loss=61041915065016.66\n",
      "Gradient Descent(11/49): loss=2108918334235750.0\n",
      "Gradient Descent(12/49): loss=7.286040524152498e+16\n",
      "Gradient Descent(13/49): loss=2.5172332652603223e+18\n",
      "Gradient Descent(14/49): loss=8.696717432501071e+19\n",
      "Gradient Descent(15/49): loss=3.004604142704202e+21\n",
      "Gradient Descent(16/49): loss=1.0380521333869298e+23\n",
      "Gradient Descent(17/49): loss=3.586336773434263e+24\n",
      "Gradient Descent(18/49): loss=1.2390332857434492e+26\n",
      "Gradient Descent(19/49): loss=4.280700838455442e+27\n",
      "Gradient Descent(20/49): loss=1.4789271506760733e+29\n",
      "Gradient Descent(21/49): loss=5.109503325652927e+30\n",
      "Gradient Descent(22/49): loss=1.7652677633900805e+32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=6.098773359870025e+33\n",
      "Gradient Descent(24/49): loss=2.1070478522553254e+35\n",
      "Gradient Descent(25/49): loss=7.279579662538215e+36\n",
      "Gradient Descent(26/49): loss=2.515001261434064e+38\n",
      "Gradient Descent(27/49): loss=8.689006286400759e+39\n",
      "Gradient Descent(28/49): loss=3.0019400547760346e+41\n",
      "Gradient Descent(29/49): loss=1.0371317266248192e+43\n",
      "Gradient Descent(30/49): loss=3.583156887694874e+44\n",
      "Gradient Descent(31/49): loss=1.2379346762072052e+46\n",
      "Gradient Descent(32/49): loss=4.27690528377093e+47\n",
      "Gradient Descent(33/49): loss=1.477615835303242e+49\n",
      "Gradient Descent(34/49): loss=5.104972899502382e+50\n",
      "Gradient Descent(35/49): loss=1.7637025593533577e+52\n",
      "Gradient Descent(36/49): loss=6.093365780987385e+53\n",
      "Gradient Descent(37/49): loss=2.1051796032161448e+55\n",
      "Gradient Descent(38/49): loss=7.273125102099402e+56\n",
      "Gradient Descent(39/49): loss=2.5127712937164127e+58\n",
      "Gradient Descent(40/49): loss=8.68130203439327e+59\n",
      "Gradient Descent(41/49): loss=2.9992783346746796e+61\n",
      "Gradient Descent(42/49): loss=1.0362121365217138e+63\n",
      "Gradient Descent(43/49): loss=3.5799798220172646e+64\n",
      "Gradient Descent(44/49): loss=1.2368370408275179e+66\n",
      "Gradient Descent(45/49): loss=4.273113094534089e+67\n",
      "Gradient Descent(46/49): loss=1.4763056826356027e+69\n",
      "Gradient Descent(47/49): loss=5.10044649033514e+70\n",
      "Gradient Descent(48/49): loss=1.7621387431313894e+72\n",
      "Gradient Descent(49/49): loss=6.087962996825106e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3486552050776321\n",
      "Gradient Descent(2/49): loss=30.984664933463037\n",
      "Gradient Descent(3/49): loss=1030.438814032291\n",
      "Gradient Descent(4/49): loss=35078.08199710986\n",
      "Gradient Descent(5/49): loss=1198050.2068958033\n",
      "Gradient Descent(6/49): loss=40951367.267209426\n",
      "Gradient Descent(7/49): loss=1400107537.4784677\n",
      "Gradient Descent(8/49): loss=47872165236.11081\n",
      "Gradient Descent(9/49): loss=1636865765391.7593\n",
      "Gradient Descent(10/49): loss=55968733169785.125\n",
      "Gradient Descent(11/49): loss=1913720860163507.5\n",
      "Gradient Descent(12/49): loss=6.543527156071046e+16\n",
      "Gradient Descent(13/49): loss=2.2374085162467187e+18\n",
      "Gradient Descent(14/49): loss=7.650303801713731e+19\n",
      "Gradient Descent(15/49): loss=2.615845452956597e+21\n",
      "Gradient Descent(16/49): loss=8.944281997365668e+22\n",
      "Gradient Descent(17/49): loss=3.0582915529804115e+24\n",
      "Gradient Descent(18/49): loss=1.0457124706546781e+26\n",
      "Gradient Descent(19/49): loss=3.5755733305012037e+27\n",
      "Gradient Descent(20/49): loss=1.2225850796343521e+29\n",
      "Gradient Descent(21/49): loss=4.180348544957684e+30\n",
      "Gradient Descent(22/49): loss=1.4293740573532893e+32\n",
      "Gradient Descent(23/49): loss=4.887415902916069e+33\n",
      "Gradient Descent(24/49): loss=1.6711394813133147e+35\n",
      "Gradient Descent(25/49): loss=5.714077175912159e+36\n",
      "Gradient Descent(26/49): loss=1.953797294443753e+38\n",
      "Gradient Descent(27/49): loss=6.680560570423832e+39\n",
      "Gradient Descent(28/49): loss=2.2842640668006355e+41\n",
      "Gradient Descent(29/49): loss=7.810515707285219e+42\n",
      "Gradient Descent(30/49): loss=2.6706262424025335e+44\n",
      "Gradient Descent(31/49): loss=9.131592322330914e+45\n",
      "Gradient Descent(32/49): loss=3.1223380126092617e+47\n",
      "Gradient Descent(33/49): loss=1.067611684891368e+49\n",
      "Gradient Descent(34/49): loss=3.6504526579557705e+50\n",
      "Gradient Descent(35/49): loss=1.2481883438108236e+52\n",
      "Gradient Descent(36/49): loss=4.267893019321236e+53\n",
      "Gradient Descent(37/49): loss=1.4593078772678824e+55\n",
      "Gradient Descent(38/49): loss=4.989767716799015e+56\n",
      "Gradient Descent(39/49): loss=1.7061363304790275e+58\n",
      "Gradient Descent(40/49): loss=5.8337408540688625e+59\n",
      "Gradient Descent(41/49): loss=1.9947135375094588e+61\n",
      "Gradient Descent(42/49): loss=6.820464254849987e+62\n",
      "Gradient Descent(43/49): loss=2.3321009145888984e+64\n",
      "Gradient Descent(44/49): loss=7.974082808159221e+65\n",
      "Gradient Descent(45/49): loss=2.726554251302167e+67\n",
      "Gradient Descent(46/49): loss=9.32282528805347e+68\n",
      "Gradient Descent(47/49): loss=3.187725727814127e+70\n",
      "Gradient Descent(48/49): loss=1.0899695105076682e+72\n",
      "Gradient Descent(49/49): loss=3.726900101443097e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3609319052471416\n",
      "Gradient Descent(2/49): loss=31.74330459986229\n",
      "Gradient Descent(3/49): loss=1068.9491319033677\n",
      "Gradient Descent(4/49): loss=36841.94783437088\n",
      "Gradient Descent(5/49): loss=1274105.5111088809\n",
      "Gradient Descent(6/49): loss=44101137.37553667\n",
      "Gradient Descent(7/49): loss=1526879987.299693\n",
      "Gradient Descent(8/49): loss=52867996168.78292\n",
      "Gradient Descent(9/49): loss=1830587911294.406\n",
      "Gradient Descent(10/49): loss=63385695027321.01\n",
      "Gradient Descent(11/49): loss=2194789119339748.8\n",
      "Gradient Descent(12/49): loss=7.599667684371248e+16\n",
      "Gradient Descent(13/49): loss=2.631458248800508e+18\n",
      "Gradient Descent(14/49): loss=9.111678531469953e+19\n",
      "Gradient Descent(15/49): loss=3.1550068118307435e+21\n",
      "Gradient Descent(16/49): loss=1.0924516263774499e+23\n",
      "Gradient Descent(17/49): loss=3.782719432561245e+24\n",
      "Gradient Descent(18/49): loss=1.3098031955065828e+26\n",
      "Gradient Descent(19/49): loss=4.5353202683056285e+27\n",
      "Gradient Descent(20/49): loss=1.5703985154001107e+29\n",
      "Gradient Descent(21/49): loss=5.437656772408543e+30\n",
      "Gradient Descent(22/49): loss=1.8828412587418597e+32\n",
      "Gradient Descent(23/49): loss=6.519519995474174e+33\n",
      "Gradient Descent(24/49): loss=2.257446865159638e+35\n",
      "Gradient Descent(25/49): loss=7.816628145258012e+36\n",
      "Gradient Descent(26/49): loss=2.7065831096281625e+38\n",
      "Gradient Descent(27/49): loss=9.371805839028722e+39\n",
      "Gradient Descent(28/49): loss=3.245078430143585e+41\n",
      "Gradient Descent(29/49): loss=1.1236397977782403e+43\n",
      "Gradient Descent(30/49): loss=3.890711495362121e+44\n",
      "Gradient Descent(31/49): loss=1.3471964921565047e+46\n",
      "Gradient Descent(32/49): loss=4.664798175455206e+47\n",
      "Gradient Descent(33/49): loss=1.6152314932840833e+49\n",
      "Gradient Descent(34/49): loss=5.5928952952870955e+50\n",
      "Gradient Descent(35/49): loss=1.9365940989947669e+52\n",
      "Gradient Descent(36/49): loss=6.705644404646127e+53\n",
      "Gradient Descent(37/49): loss=2.3218942423144945e+55\n",
      "Gradient Descent(38/49): loss=8.039783422988904e+56\n",
      "Gradient Descent(39/49): loss=2.7838527832402485e+58\n",
      "Gradient Descent(40/49): loss=9.63935955861025e+59\n",
      "Gradient Descent(41/49): loss=3.3377214937357824e+61\n",
      "Gradient Descent(42/49): loss=1.1557183547318557e+63\n",
      "Gradient Descent(43/49): loss=4.0017866019406144e+64\n",
      "Gradient Descent(44/49): loss=1.3856573222969121e+66\n",
      "Gradient Descent(45/49): loss=4.797972520333667e+67\n",
      "Gradient Descent(46/49): loss=1.6613443984633473e+69\n",
      "Gradient Descent(47/49): loss=5.752565690212827e+70\n",
      "Gradient Descent(48/49): loss=1.9918815178130572e+72\n",
      "Gradient Descent(49/49): loss=6.897082440545678e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.373716247191866\n",
      "Gradient Descent(2/49): loss=32.91826877803271\n",
      "Gradient Descent(3/49): loss=1133.0305266055525\n",
      "Gradient Descent(4/49): loss=39867.2822427387\n",
      "Gradient Descent(5/49): loss=1407055.3929902497\n",
      "Gradient Descent(6/49): loss=49696671.08477988\n",
      "Gradient Descent(7/49): loss=1755625034.9828928\n",
      "Gradient Descent(8/49): loss=62024185059.3189\n",
      "Gradient Descent(9/49): loss=2191277544284.6821\n",
      "Gradient Descent(10/49): loss=77416885848729.22\n",
      "Gradient Descent(11/49): loss=2735108568721481.5\n",
      "Gradient Descent(12/49): loss=9.663036107122555e+16\n",
      "Gradient Descent(13/49): loss=3.4139148114315535e+18\n",
      "Gradient Descent(14/49): loss=1.2061234745179159e+20\n",
      "Gradient Descent(15/49): loss=4.2611896556117547e+21\n",
      "Gradient Descent(16/49): loss=1.505462559000149e+23\n",
      "Gradient Descent(17/49): loss=5.318743593331843e+24\n",
      "Gradient Descent(18/49): loss=1.8790924588513365e+26\n",
      "Gradient Descent(19/49): loss=6.638764225313685e+27\n",
      "Gradient Descent(20/49): loss=2.3454508708380015e+29\n",
      "Gradient Descent(21/49): loss=8.286391263245512e+30\n",
      "Gradient Descent(22/49): loss=2.927551415440364e+32\n",
      "Gradient Descent(23/49): loss=1.0342930978971191e+34\n",
      "Gradient Descent(24/49): loss=3.654119298180853e+35\n",
      "Gradient Descent(25/49): loss=1.2909868462320805e+37\n",
      "Gradient Descent(26/49): loss=4.561008826323734e+38\n",
      "Gradient Descent(27/49): loss=1.6113875656067876e+40\n",
      "Gradient Descent(28/49): loss=5.6929727292043914e+41\n",
      "Gradient Descent(29/49): loss=2.0113062299361106e+43\n",
      "Gradient Descent(30/49): loss=7.105870593455607e+44\n",
      "Gradient Descent(31/49): loss=2.510477824778612e+46\n",
      "Gradient Descent(32/49): loss=8.869425393856231e+47\n",
      "Gradient Descent(33/49): loss=3.133535219500265e+49\n",
      "Gradient Descent(34/49): loss=1.107066414770241e+51\n",
      "Gradient Descent(35/49): loss=3.9112247377506786e+52\n",
      "Gradient Descent(36/49): loss=1.3818212480384676e+54\n",
      "Gradient Descent(37/49): loss=4.881923411613207e+55\n",
      "Gradient Descent(38/49): loss=1.7247655028238357e+57\n",
      "Gradient Descent(39/49): loss=6.093532792125757e+58\n",
      "Gradient Descent(40/49): loss=2.1528226201138375e+60\n",
      "Gradient Descent(41/49): loss=7.60584277098315e+61\n",
      "Gradient Descent(42/49): loss=2.6871161477232025e+63\n",
      "Gradient Descent(43/49): loss=9.49348206210868e+64\n",
      "Gradient Descent(44/49): loss=3.3540121345310335e+66\n",
      "Gradient Descent(45/49): loss=1.1849600941978013e+68\n",
      "Gradient Descent(46/49): loss=4.18642022902995e+69\n",
      "Gradient Descent(47/49): loss=1.4790467982718077e+71\n",
      "Gradient Descent(48/49): loss=5.2254176881449546e+72\n",
      "Gradient Descent(49/49): loss=1.8461207615257763e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.371053885514058\n",
      "Gradient Descent(2/49): loss=32.68724623233367\n",
      "Gradient Descent(3/49): loss=1125.2973826228392\n",
      "Gradient Descent(4/49): loss=39655.84999546843\n",
      "Gradient Descent(5/49): loss=1402310.9230050805\n",
      "Gradient Descent(6/49): loss=49631781.07814825\n",
      "Gradient Descent(7/49): loss=1757041229.0698311\n",
      "Gradient Descent(8/49): loss=62206338639.48869\n",
      "Gradient Descent(9/49): loss=2202399800173.134\n",
      "Gradient Descent(10/49): loss=77975870312796.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=2760737033383570.0\n",
      "Gradient Descent(12/49): loss=9.774399037756797e+16\n",
      "Gradient Descent(13/49): loss=3.460629860649078e+18\n",
      "Gradient Descent(14/49): loss=1.2252374268262577e+20\n",
      "Gradient Descent(15/49): loss=4.3379582054190725e+21\n",
      "Gradient Descent(16/49): loss=1.53585591207591e+23\n",
      "Gradient Descent(17/49): loss=5.437704268488561e+24\n",
      "Gradient Descent(18/49): loss=1.925221466909222e+26\n",
      "Gradient Descent(19/49): loss=6.816254643410634e+27\n",
      "Gradient Descent(20/49): loss=2.4132978030590122e+29\n",
      "Gradient Descent(21/49): loss=8.544290950045967e+30\n",
      "Gradient Descent(22/49): loss=3.0251097791017154e+32\n",
      "Gradient Descent(23/49): loss=1.0710413806277255e+34\n",
      "Gradient Descent(24/49): loss=3.792026480962224e+35\n",
      "Gradient Descent(25/49): loss=1.342568559199058e+37\n",
      "Gradient Descent(26/49): loss=4.753369590637671e+38\n",
      "Gradient Descent(27/49): loss=1.6829324886528215e+40\n",
      "Gradient Descent(28/49): loss=5.958429504286107e+41\n",
      "Gradient Descent(29/49): loss=2.109584454333457e+43\n",
      "Gradient Descent(30/49): loss=7.468992570549202e+44\n",
      "Gradient Descent(31/49): loss=2.6443999387806077e+46\n",
      "Gradient Descent(32/49): loss=9.36250902671423e+47\n",
      "Gradient Descent(33/49): loss=3.31480023085033e+49\n",
      "Gradient Descent(34/49): loss=1.1736064060492074e+51\n",
      "Gradient Descent(35/49): loss=4.155158381796099e+52\n",
      "Gradient Descent(36/49): loss=1.4711355603393372e+54\n",
      "Gradient Descent(37/49): loss=5.208561595092412e+55\n",
      "Gradient Descent(38/49): loss=1.8440934079259114e+57\n",
      "Gradient Descent(39/49): loss=6.529020412007751e+58\n",
      "Gradient Descent(40/49): loss=2.31160240350073e+60\n",
      "Gradient Descent(41/49): loss=8.184237963237048e+61\n",
      "Gradient Descent(42/49): loss=2.8976328687602992e+63\n",
      "Gradient Descent(43/49): loss=1.0259081272850867e+65\n",
      "Gradient Descent(44/49): loss=3.632232009018747e+66\n",
      "Gradient Descent(45/49): loss=1.2859932596746238e+68\n",
      "Gradient Descent(46/49): loss=4.553064506403411e+69\n",
      "Gradient Descent(47/49): loss=1.6120143899288913e+71\n",
      "Gradient Descent(48/49): loss=5.707343679588065e+72\n",
      "Gradient Descent(49/49): loss=2.0206874132414379e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3727050365422322\n",
      "Gradient Descent(2/49): loss=32.52635055619779\n",
      "Gradient Descent(3/49): loss=1109.0689229653426\n",
      "Gradient Descent(4/49): loss=38690.89693972324\n",
      "Gradient Descent(5/49): loss=1354203.642012381\n",
      "Gradient Descent(6/49): loss=47436929.51835378\n",
      "Gradient Descent(7/49): loss=1662072821.532898\n",
      "Gradient Descent(8/49): loss=58238834254.09402\n",
      "Gradient Descent(9/49): loss=2040721660255.5364\n",
      "Gradient Descent(10/49): loss=71508446752461.72\n",
      "Gradient Descent(11/49): loss=2505714785759914.5\n",
      "Gradient Descent(12/49): loss=8.780234937946117e+16\n",
      "Gradient Descent(13/49): loss=3.0766684574788936e+18\n",
      "Gradient Descent(14/49): loss=1.0780906495104357e+20\n",
      "Gradient Descent(15/49): loss=3.777720864604095e+21\n",
      "Gradient Descent(16/49): loss=1.323745362913883e+23\n",
      "Gradient Descent(17/49): loss=4.6385157888373515e+24\n",
      "Gradient Descent(18/49): loss=1.6253751916723716e+26\n",
      "Gradient Descent(19/49): loss=5.695452239899049e+27\n",
      "Gradient Descent(20/49): loss=1.995734670000676e+29\n",
      "Gradient Descent(21/49): loss=6.993223198624943e+30\n",
      "Gradient Descent(22/49): loss=2.4504845980290154e+32\n",
      "Gradient Descent(23/49): loss=8.586705435573597e+33\n",
      "Gradient Descent(24/49): loss=3.0088542607705744e+35\n",
      "Gradient Descent(25/49): loss=1.0543279993105995e+37\n",
      "Gradient Descent(26/49): loss=3.694454545783216e+38\n",
      "Gradient Descent(27/49): loss=1.2945681419618024e+40\n",
      "Gradient Descent(28/49): loss=4.536276339074971e+41\n",
      "Gradient Descent(29/49): loss=1.5895496233413917e+43\n",
      "Gradient Descent(30/49): loss=5.569916416467708e+44\n",
      "Gradient Descent(31/49): loss=1.9517458549812922e+46\n",
      "Gradient Descent(32/49): loss=6.839082667693636e+47\n",
      "Gradient Descent(33/49): loss=2.3964724513784645e+49\n",
      "Gradient Descent(34/49): loss=8.397442302232955e+50\n",
      "Gradient Descent(35/49): loss=2.9425348569631686e+52\n",
      "Gradient Descent(36/49): loss=1.0310891189024183e+54\n",
      "Gradient Descent(37/49): loss=3.613023541940931e+55\n",
      "Gradient Descent(38/49): loss=1.2660340289998585e+57\n",
      "Gradient Descent(39/49): loss=4.436290392186472e+58\n",
      "Gradient Descent(40/49): loss=1.554513701290736e+60\n",
      "Gradient Descent(41/49): loss=5.447147580232285e+61\n",
      "Gradient Descent(42/49): loss=1.9087266156736907e+63\n",
      "Gradient Descent(43/49): loss=6.688339612097979e+64\n",
      "Gradient Descent(44/49): loss=2.343650808838863e+66\n",
      "Gradient Descent(45/49): loss=8.212350796056721e+67\n",
      "Gradient Descent(46/49): loss=2.8776772266218005e+69\n",
      "Gradient Descent(47/49): loss=1.0083624562888814e+71\n",
      "Gradient Descent(48/49): loss=3.5333873926041294e+72\n",
      "Gradient Descent(49/49): loss=1.2381288482479102e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3849624838817791\n",
      "Gradient Descent(2/49): loss=33.304853889862564\n",
      "Gradient Descent(3/49): loss=1149.5686869455672\n",
      "Gradient Descent(4/49): loss=40591.670814693396\n",
      "Gradient Descent(5/49): loss=1438188.3256962164\n",
      "Gradient Descent(6/49): loss=51001089.52868464\n",
      "Gradient Descent(7/49): loss=1809069354.1934628\n",
      "Gradient Descent(8/49): loss=64174768372.37735\n",
      "Gradient Descent(9/49): loss=2276582391793.521\n",
      "Gradient Descent(10/49): loss=80761694556909.28\n",
      "Gradient Descent(11/49): loss=2865024691146444.5\n",
      "Gradient Descent(12/49): loss=1.0163694070087656e+17\n",
      "Gradient Descent(13/49): loss=3.605577873279641e+18\n",
      "Gradient Descent(14/49): loss=1.279081447619659e+20\n",
      "Gradient Descent(15/49): loss=4.5375510791374373e+21\n",
      "Gradient Descent(16/49): loss=1.609699674505345e+23\n",
      "Gradient Descent(17/49): loss=5.710421768942194e+24\n",
      "Gradient Descent(18/49): loss=2.0257764420522764e+26\n",
      "Gradient Descent(19/49): loss=7.186457252683209e+27\n",
      "Gradient Descent(20/49): loss=2.54940114687184e+29\n",
      "Gradient Descent(21/49): loss=9.044019854600765e+30\n",
      "Gradient Descent(22/49): loss=3.2083728851802856e+32\n",
      "Gradient Descent(23/49): loss=1.138172708138733e+34\n",
      "Gradient Descent(24/49): loss=4.037676292353662e+35\n",
      "Gradient Descent(25/49): loss=1.4323687191987458e+37\n",
      "Gradient Descent(26/49): loss=5.081338867170878e+38\n",
      "Gradient Descent(27/49): loss=1.8026088071418645e+40\n",
      "Gradient Descent(28/49): loss=6.39476838000097e+41\n",
      "Gradient Descent(29/49): loss=2.268548920422635e+43\n",
      "Gradient Descent(30/49): loss=8.047694456683231e+44\n",
      "Gradient Descent(31/49): loss=2.8549256965577825e+46\n",
      "Gradient Descent(32/49): loss=1.012787050594014e+48\n",
      "Gradient Descent(33/49): loss=3.5928697236767533e+49\n",
      "Gradient Descent(34/49): loss=1.2745732524662378e+51\n",
      "Gradient Descent(35/49): loss=4.5215582552208123e+52\n",
      "Gradient Descent(36/49): loss=1.604026211580742e+54\n",
      "Gradient Descent(37/49): loss=5.69029511997834e+55\n",
      "Gradient Descent(38/49): loss=2.0186364984983604e+57\n",
      "Gradient Descent(39/49): loss=7.161128249329166e+58\n",
      "Gradient Descent(40/49): loss=2.5404156638150715e+60\n",
      "Gradient Descent(41/49): loss=9.012143785529244e+61\n",
      "Gradient Descent(42/49): loss=3.1970648255680663e+63\n",
      "Gradient Descent(43/49): loss=1.134161165437319e+65\n",
      "Gradient Descent(44/49): loss=4.023445314273815e+66\n",
      "Gradient Descent(45/49): loss=1.4273202689592991e+68\n",
      "Gradient Descent(46/49): loss=5.063429451755197e+69\n",
      "Gradient Descent(47/49): loss=1.7962554284747553e+71\n",
      "Gradient Descent(48/49): loss=6.372229721116343e+72\n",
      "Gradient Descent(49/49): loss=2.260553314132912e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3980875197406497\n",
      "Gradient Descent(2/49): loss=34.535773314640714\n",
      "Gradient Descent(3/49): loss=1218.3496642208834\n",
      "Gradient Descent(4/49): loss=43918.63058036945\n",
      "Gradient Descent(5/49): loss=1587979.478003758\n",
      "Gradient Descent(6/49): loss=57459978.101220205\n",
      "Gradient Descent(7/49): loss=2079579541.305443\n",
      "Gradient Descent(8/49): loss=75268082085.50511\n",
      "Gradient Descent(9/49): loss=2724289881843.989\n",
      "Gradient Descent(10/49): loss=98604745580986.89\n",
      "Gradient Descent(11/49): loss=3568969970818750.0\n",
      "Gradient Descent(12/49): loss=1.2917787468173835e+17\n",
      "Gradient Descent(13/49): loss=4.675557720332881e+18\n",
      "Gradient Descent(14/49): loss=1.6923053361893943e+20\n",
      "Gradient Descent(15/49): loss=6.125252958211482e+21\n",
      "Gradient Descent(16/49): loss=2.217018589519677e+23\n",
      "Gradient Descent(17/49): loss=8.024438277505635e+24\n",
      "Gradient Descent(18/49): loss=2.9044235346851656e+26\n",
      "Gradient Descent(19/49): loss=1.0512481718451402e+28\n",
      "Gradient Descent(20/49): loss=3.8049640681808605e+29\n",
      "Gradient Descent(21/49): loss=1.3771963602850022e+31\n",
      "Gradient Descent(22/49): loss=4.984724640754715e+32\n",
      "Gradient Descent(23/49): loss=1.8042074798262104e+34\n",
      "Gradient Descent(24/49): loss=6.530279734305122e+35\n",
      "Gradient Descent(25/49): loss=2.3636169279369864e+37\n",
      "Gradient Descent(26/49): loss=8.55504696480629e+38\n",
      "Gradient Descent(27/49): loss=3.096475901191069e+40\n",
      "Gradient Descent(28/49): loss=1.1207610017923694e+42\n",
      "Gradient Descent(29/49): loss=4.0565638591130823e+43\n",
      "Gradient Descent(30/49): loss=1.4682622179702675e+45\n",
      "Gradient Descent(31/49): loss=5.314335027355689e+46\n",
      "Gradient Descent(32/49): loss=1.9235090597115245e+48\n",
      "Gradient Descent(33/49): loss=6.962088546821094e+49\n",
      "Gradient Descent(34/49): loss=2.5199089491705748e+51\n",
      "Gradient Descent(35/49): loss=9.120741670269827e+52\n",
      "Gradient Descent(36/49): loss=3.301227555986794e+54\n",
      "Gradient Descent(37/49): loss=1.194870304454548e+56\n",
      "Gradient Descent(38/49): loss=4.3248004575695785e+57\n",
      "Gradient Descent(39/49): loss=1.5653497227326531e+59\n",
      "Gradient Descent(40/49): loss=5.665740601211929e+60\n",
      "Gradient Descent(41/49): loss=2.0506993481420034e+62\n",
      "Gradient Descent(42/49): loss=7.42245032462396e+63\n",
      "Gradient Descent(43/49): loss=2.686535638265362e+65\n",
      "Gradient Descent(44/49): loss=9.723842424011772e+66\n",
      "Gradient Descent(45/49): loss=3.5195182278714174e+68\n",
      "Gradient Descent(46/49): loss=1.273880017402489e+70\n",
      "Gradient Descent(47/49): loss=4.610773957317475e+71\n",
      "Gradient Descent(48/49): loss=1.6688570505113767e+73\n",
      "Gradient Descent(49/49): loss=6.040382549271385e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3953132632517782\n",
      "Gradient Descent(2/49): loss=34.29336939920105\n",
      "Gradient Descent(3/49): loss=1210.028814030392\n",
      "Gradient Descent(4/49): loss=43685.19817037237\n",
      "Gradient Descent(5/49): loss=1582597.6511996654\n",
      "Gradient Descent(6/49): loss=57383707.257684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=2081203652.1565418\n",
      "Gradient Descent(8/49): loss=75486906790.75195\n",
      "Gradient Descent(9/49): loss=2738026675825.6133\n",
      "Gradient Descent(10/49): loss=99313051996884.67\n",
      "Gradient Descent(11/49): loss=3602265664228207.5\n",
      "Gradient Descent(12/49): loss=1.3066081663921344e+17\n",
      "Gradient Descent(13/49): loss=4.739309396187394e+18\n",
      "Gradient Descent(14/49): loss=1.7190352155375436e+20\n",
      "Gradient Descent(15/49): loss=6.235258899031791e+21\n",
      "Gradient Descent(16/49): loss=2.2616438177660698e+23\n",
      "Gradient Descent(17/49): loss=8.203400767463497e+24\n",
      "Gradient Descent(18/49): loss=2.97552530788986e+26\n",
      "Gradient Descent(19/49): loss=1.079278107901962e+28\n",
      "Gradient Descent(20/49): loss=3.914741478210301e+29\n",
      "Gradient Descent(21/49): loss=1.419949198369689e+31\n",
      "Gradient Descent(22/49): loss=5.150418583647648e+32\n",
      "Gradient Descent(23/49): loss=1.8681521576445097e+34\n",
      "Gradient Descent(24/49): loss=6.77613368201379e+35\n",
      "Gradient Descent(25/49): loss=2.4578291167898507e+37\n",
      "Gradient Descent(26/49): loss=8.91500116559796e+38\n",
      "Gradient Descent(27/49): loss=3.233635944813666e+40\n",
      "Gradient Descent(28/49): loss=1.1728996137365849e+42\n",
      "Gradient Descent(29/49): loss=4.2543240098189506e+43\n",
      "Gradient Descent(30/49): loss=1.5431220684660156e+45\n",
      "Gradient Descent(31/49): loss=5.597189383533063e+46\n",
      "Gradient Descent(32/49): loss=2.0302041967605347e+48\n",
      "Gradient Descent(33/49): loss=7.36392642469852e+49\n",
      "Gradient Descent(34/49): loss=2.6710324249600302e+51\n",
      "Gradient Descent(35/49): loss=9.68832902955014e+52\n",
      "Gradient Descent(36/49): loss=3.514136275834572e+54\n",
      "Gradient Descent(37/49): loss=1.2746422760282597e+56\n",
      "Gradient Descent(38/49): loss=4.623363479131586e+57\n",
      "Gradient Descent(39/49): loss=1.6769795151289875e+59\n",
      "Gradient Descent(40/49): loss=6.082715120400795e+60\n",
      "Gradient Descent(41/49): loss=2.2063133688968497e+62\n",
      "Gradient Descent(42/49): loss=8.00270699090741e+63\n",
      "Gradient Descent(43/49): loss=2.902729960537742e+65\n",
      "Gradient Descent(44/49): loss=1.0528738879702492e+67\n",
      "Gradient Descent(45/49): loss=3.818968484978322e+68\n",
      "Gradient Descent(46/49): loss=1.3852105609128438e+70\n",
      "Gradient Descent(47/49): loss=5.024415115264723e+71\n",
      "Gradient Descent(48/49): loss=1.8224483672622713e+73\n",
      "Gradient Descent(49/49): loss=6.610357574250582e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.39706979756474\n",
      "Gradient Descent(2/49): loss=34.12561293022003\n",
      "Gradient Descent(3/49): loss=1192.6391485433041\n",
      "Gradient Descent(4/49): loss=42625.20976280425\n",
      "Gradient Descent(5/49): loss=1528441.974014927\n",
      "Gradient Descent(6/49): loss=54851939.1237013\n",
      "Gradient Descent(7/49): loss=1968961250.3528838\n",
      "Gradient Descent(8/49): loss=70682508486.88382\n",
      "Gradient Descent(9/49): loss=2537437599924.7544\n",
      "Gradient Descent(10/49): loss=91092225127990.25\n",
      "Gradient Descent(11/49): loss=3270152368682538.0\n",
      "Gradient Descent(12/49): loss=1.1739642780414662e+17\n",
      "Gradient Descent(13/49): loss=4.214459056418047e+18\n",
      "Gradient Descent(14/49): loss=1.512964766182241e+20\n",
      "Gradient Descent(15/49): loss=5.431450113956582e+21\n",
      "Gradient Descent(16/49): loss=1.9498570692022707e+23\n",
      "Gradient Descent(17/49): loss=6.999866545040388e+24\n",
      "Gradient Descent(18/49): loss=2.512908891507561e+26\n",
      "Gradient Descent(19/49): loss=9.021187842600807e+27\n",
      "Gradient Descent(20/49): loss=3.238550763567145e+29\n",
      "Gradient Descent(21/49): loss=1.1626197382516305e+31\n",
      "Gradient Descent(22/49): loss=4.1737331122966964e+32\n",
      "Gradient Descent(23/49): loss=1.4983444302167097e+34\n",
      "Gradient Descent(24/49): loss=5.3789640380874864e+35\n",
      "Gradient Descent(25/49): loss=1.9310148948099567e+37\n",
      "Gradient Descent(26/49): loss=6.932224304856576e+38\n",
      "Gradient Descent(27/49): loss=2.4886257450424357e+40\n",
      "Gradient Descent(28/49): loss=8.934012845702586e+41\n",
      "Gradient Descent(29/49): loss=3.2072554776940887e+43\n",
      "Gradient Descent(30/49): loss=1.1513849237576024e+45\n",
      "Gradient Descent(31/49): loss=4.1334008215941065e+46\n",
      "Gradient Descent(32/49): loss=1.4838653867550288e+48\n",
      "Gradient Descent(33/49): loss=5.326985165596591e+49\n",
      "Gradient Descent(34/49): loss=1.9123548003597148e+51\n",
      "Gradient Descent(35/49): loss=6.865235717338999e+52\n",
      "Gradient Descent(36/49): loss=2.4645772555260837e+54\n",
      "Gradient Descent(37/49): loss=8.847680252428205e+55\n",
      "Gradient Descent(38/49): loss=3.1762626094875105e+57\n",
      "Gradient Descent(39/49): loss=1.1402586753357812e+59\n",
      "Gradient Descent(40/49): loss=4.0934582763869745e+60\n",
      "Gradient Descent(41/49): loss=1.4695262595206024e+62\n",
      "Gradient Descent(42/49): loss=5.2755085837266016e+63\n",
      "Gradient Descent(43/49): loss=1.893875025142604e+65\n",
      "Gradient Descent(44/49): loss=6.79889446474035e+66\n",
      "Gradient Descent(45/49): loss=2.440761155250797e+68\n",
      "Gradient Descent(46/49): loss=8.762181922188024e+69\n",
      "Gradient Descent(47/49): loss=3.1455692365617656e+71\n",
      "Gradient Descent(48/49): loss=1.1292399438715351e+73\n",
      "Gradient Descent(49/49): loss=4.0539017104224025e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.409303630309303\n",
      "Gradient Descent(2/49): loss=34.9240540901426\n",
      "Gradient Descent(3/49): loss=1235.1950638189073\n",
      "Gradient Descent(4/49): loss=44671.187423022704\n",
      "Gradient Descent(5/49): loss=1621050.2137971276\n",
      "Gradient Descent(6/49): loss=58878069.45561282\n",
      "Gradient Descent(7/49): loss=2139065371.4270535\n",
      "Gradient Descent(8/49): loss=77719209351.22337\n",
      "Gradient Descent(9/49): loss=2823857874647.3687\n",
      "Gradient Descent(10/49): loss=102603067358299.28\n",
      "Gradient Descent(11/49): loss=3728024611121543.5\n",
      "Gradient Descent(12/49): loss=1.3545575862973258e+17\n",
      "Gradient Descent(13/49): loss=4.921712693758084e+18\n",
      "Gradient Descent(14/49): loss=1.7882781416719788e+20\n",
      "Gradient Descent(15/49): loss=6.497613668783212e+21\n",
      "Gradient Descent(16/49): loss=2.3608734359923235e+23\n",
      "Gradient Descent(17/49): loss=8.578108323484787e+24\n",
      "Gradient Descent(18/49): loss=3.1168101314655564e+26\n",
      "Gradient Descent(19/49): loss=1.1324764191175632e+28\n",
      "Gradient Descent(20/49): loss=4.114792964013308e+29\n",
      "Gradient Descent(21/49): loss=1.4950881847150405e+31\n",
      "Gradient Descent(22/49): loss=5.4323235692105576e+32\n",
      "Gradient Descent(23/49): loss=1.9738059374911995e+34\n",
      "Gradient Descent(24/49): loss=7.171719116582495e+35\n",
      "Gradient Descent(25/49): loss=2.605806078004521e+37\n",
      "Gradient Descent(26/49): loss=9.468058084518577e+38\n",
      "Gradient Descent(27/49): loss=3.440168654471275e+40\n",
      "Gradient Descent(28/49): loss=1.249967022335652e+42\n",
      "Gradient Descent(29/49): loss=4.541688835214384e+43\n",
      "Gradient Descent(30/49): loss=1.6501985338275534e+45\n",
      "Gradient Descent(31/49): loss=5.995908790431417e+46\n",
      "Gradient Descent(32/49): loss=2.178581636464462e+48\n",
      "Gradient Descent(33/49): loss=7.915760750587806e+49\n",
      "Gradient Descent(34/49): loss=2.8761496568122112e+51\n",
      "Gradient Descent(35/49): loss=1.0450337129967867e+53\n",
      "Gradient Descent(36/49): loss=3.7970745323116385e+54\n",
      "Gradient Descent(37/49): loss=1.37964687881548e+56\n",
      "Gradient Descent(38/49): loss=5.0128737111370116e+57\n",
      "Gradient Descent(39/49): loss=1.8214010577390238e+59\n",
      "Gradient Descent(40/49): loss=6.617964074703139e+60\n",
      "Gradient Descent(41/49): loss=2.4046021225236654e+62\n",
      "Gradient Descent(42/49): loss=8.73699419092828e+63\n",
      "Gradient Descent(43/49): loss=3.1745404687657616e+65\n",
      "Gradient Descent(44/49): loss=1.1534524308480557e+67\n",
      "Gradient Descent(45/49): loss=4.1910081894358827e+68\n",
      "Gradient Descent(46/49): loss=1.5227805823777648e+70\n",
      "Gradient Descent(47/49): loss=5.532942426387616e+71\n",
      "Gradient Descent(48/49): loss=2.01036526522543e+73\n",
      "Gradient Descent(49/49): loss=7.304555493565138e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4227736389211971\n",
      "Gradient Descent(2/49): loss=36.21289722335828\n",
      "Gradient Descent(3/49): loss=1308.9591638057389\n",
      "Gradient Descent(4/49): loss=48325.75399013037\n",
      "Gradient Descent(5/49): loss=1789578.4284342674\n",
      "Gradient Descent(6/49): loss=66320872.256546095\n",
      "Gradient Descent(7/49): loss=2458331043.016077\n",
      "Gradient Descent(8/49): loss=91128900257.89287\n",
      "Gradient Descent(9/49): loss=3378151948265.705\n",
      "Gradient Descent(10/49): loss=125228825310268.23\n",
      "Gradient Descent(11/49): loss=4642266036402871.0\n",
      "Gradient Descent(12/49): loss=1.7209011001397584e+17\n",
      "Gradient Descent(13/49): loss=6.379429574460266e+18\n",
      "Gradient Descent(14/49): loss=2.3648728551404624e+20\n",
      "Gradient Descent(15/49): loss=8.766651635666005e+21\n",
      "Gradient Descent(16/49): loss=3.249822963271672e+23\n",
      "Gradient Descent(17/49): loss=1.2047187157952406e+25\n",
      "Gradient Descent(18/49): loss=4.4659269163061274e+26\n",
      "Gradient Descent(19/49): loss=1.6555319479543327e+28\n",
      "Gradient Descent(20/49): loss=6.137104529715432e+29\n",
      "Gradient Descent(21/49): loss=2.2750422941324656e+31\n",
      "Gradient Descent(22/49): loss=8.433647194760501e+32\n",
      "Gradient Descent(23/49): loss=3.126377262926536e+34\n",
      "Gradient Descent(24/49): loss=1.1589570401068565e+36\n",
      "Gradient Descent(25/49): loss=4.296287069193817e+37\n",
      "Gradient Descent(26/49): loss=1.5926459689325905e+39\n",
      "Gradient Descent(27/49): loss=5.903984397469977e+40\n",
      "Gradient Descent(28/49): loss=2.1886239908628638e+42\n",
      "Gradient Descent(29/49): loss=8.113292059906437e+43\n",
      "Gradient Descent(30/49): loss=3.0076206933740554e+45\n",
      "Gradient Descent(31/49): loss=1.1149336383332659e+47\n",
      "Gradient Descent(32/49): loss=4.133091053089297e+48\n",
      "Gradient Descent(33/49): loss=1.5321487365529366e+50\n",
      "Gradient Descent(34/49): loss=5.6797194176646904e+51\n",
      "Gradient Descent(35/49): loss=2.105488318058133e+53\n",
      "Gradient Descent(36/49): loss=7.805105730560857e+54\n",
      "Gradient Descent(37/49): loss=2.8933751350100066e+56\n",
      "Gradient Descent(38/49): loss=1.072582481376915e+58\n",
      "Gradient Descent(39/49): loss=3.976094096601416e+59\n",
      "Gradient Descent(40/49): loss=1.4739495133962716e+61\n",
      "Gradient Descent(41/49): loss=5.463973224119824e+62\n",
      "Gradient Descent(42/49): loss=2.0255105838127745e+64\n",
      "Gradient Descent(43/49): loss=7.508625970249816e+65\n",
      "Gradient Descent(44/49): loss=2.7834692354449383e+67\n",
      "Gradient Descent(45/49): loss=1.031840048414437e+69\n",
      "Gradient Descent(46/49): loss=3.825060726211736e+70\n",
      "Gradient Descent(47/49): loss=1.4179610087522906e+72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=5.256422227662449e+73\n",
      "Gradient Descent(49/49): loss=1.948570832689986e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4198857523248312\n",
      "Gradient Descent(2/49): loss=35.958700932465796\n",
      "Gradient Descent(3/49): loss=1300.0139102945666\n",
      "Gradient Descent(4/49): loss=48068.34615365473\n",
      "Gradient Descent(5/49): loss=1783483.3563934967\n",
      "Gradient Descent(6/49): loss=66231438.30171747\n",
      "Gradient Descent(7/49): loss=2460189328.68967\n",
      "Gradient Descent(8/49): loss=91391208455.57957\n",
      "Gradient Descent(9/49): loss=3395075739304.657\n",
      "Gradient Descent(10/49): loss=126123837812797.48\n",
      "Gradient Descent(11/49): loss=4685389094864749.0\n",
      "Gradient Descent(12/49): loss=1.7405815387368806e+17\n",
      "Gradient Descent(13/49): loss=6.466110902465158e+18\n",
      "Gradient Descent(14/49): loss=2.4021047627870044e+20\n",
      "Gradient Descent(15/49): loss=8.923613362530421e+21\n",
      "Gradient Descent(16/49): loss=3.3150459119990496e+23\n",
      "Gradient Descent(17/49): loss=1.231511156313336e+25\n",
      "Gradient Descent(18/49): loss=4.5749584436027666e+26\n",
      "Gradient Descent(19/49): loss=1.6995578688278347e+28\n",
      "Gradient Descent(20/49): loss=6.313711884336118e+29\n",
      "Gradient Descent(21/49): loss=2.3454898765059777e+31\n",
      "Gradient Descent(22/49): loss=8.713293957003165e+32\n",
      "Gradient Descent(23/49): loss=3.236914059688744e+34\n",
      "Gradient Descent(24/49): loss=1.2024858430710343e+36\n",
      "Gradient Descent(25/49): loss=4.467131892050822e+37\n",
      "Gradient Descent(26/49): loss=1.659501228722504e+39\n",
      "Gradient Descent(27/49): loss=6.1649048979997575e+40\n",
      "Gradient Descent(28/49): loss=2.290209355894181e+42\n",
      "Gradient Descent(29/49): loss=8.50793155872845e+43\n",
      "Gradient Descent(30/49): loss=3.160623688036e+45\n",
      "Gradient Descent(31/49): loss=1.1741446235690285e+47\n",
      "Gradient Descent(32/49): loss=4.3618466895460057e+48\n",
      "Gradient Descent(33/49): loss=1.620388677952746e+50\n",
      "Gradient Descent(34/49): loss=6.019605122597145e+51\n",
      "Gradient Descent(35/49): loss=2.236231734091059e+53\n",
      "Gradient Descent(36/49): loss=8.30740931790281e+54\n",
      "Gradient Descent(37/49): loss=3.0861313934098767e+56\n",
      "Gradient Descent(38/49): loss=1.1464713742784973e+58\n",
      "Gradient Descent(39/49): loss=4.25904293915283e+59\n",
      "Gradient Descent(40/49): loss=1.5821979653842757e+61\n",
      "Gradient Descent(41/49): loss=5.877729897139955e+62\n",
      "Gradient Descent(42/49): loss=2.1835263032551218e+64\n",
      "Gradient Descent(43/49): loss=8.111613157533725e+65\n",
      "Gradient Descent(44/49): loss=3.013394797185847e+67\n",
      "Gradient Descent(45/49): loss=1.1194503519035649e+69\n",
      "Gradient Descent(46/49): loss=4.1586621558759186e+70\n",
      "Gradient Descent(47/49): loss=1.5449073643423439e+72\n",
      "Gradient Descent(48/49): loss=5.739198508892822e+73\n",
      "Gradient Descent(49/49): loss=2.1320630792966313e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4217494881451556\n",
      "Gradient Descent(2/49): loss=35.78386888209658\n",
      "Gradient Descent(3/49): loss=1281.394763274268\n",
      "Gradient Descent(4/49): loss=46905.23873158126\n",
      "Gradient Descent(5/49): loss=1722603.6160227314\n",
      "Gradient Descent(6/49): loss=63315950.7542197\n",
      "Gradient Descent(7/49): loss=2327792792.732897\n",
      "Gradient Descent(8/49): loss=85586556844.12086\n",
      "Gradient Descent(9/49): loss=3146846641330.4175\n",
      "Gradient Descent(10/49): loss=115703947881259.88\n",
      "Gradient Descent(11/49): loss=4254235547707699.0\n",
      "Gradient Descent(12/49): loss=1.564210181710734e+17\n",
      "Gradient Descent(13/49): loss=5.751336125273911e+18\n",
      "Gradient Descent(14/49): loss=2.1146690557859427e+20\n",
      "Gradient Descent(15/49): loss=7.775280692680303e+21\n",
      "Gradient Descent(16/49): loss=2.858839302229156e+23\n",
      "Gradient Descent(17/49): loss=1.0511468959245256e+25\n",
      "Gradient Descent(18/49): loss=3.8648894896281434e+26\n",
      "Gradient Descent(19/49): loss=1.421054547788008e+28\n",
      "Gradient Descent(20/49): loss=5.2249774107771466e+29\n",
      "Gradient Descent(21/49): loss=1.921135890656116e+31\n",
      "Gradient Descent(22/49): loss=7.063691993698176e+32\n",
      "Gradient Descent(23/49): loss=2.597200168116697e+34\n",
      "Gradient Descent(24/49): loss=9.5494660855608e+35\n",
      "Gradient Descent(25/49): loss=3.511177291560392e+37\n",
      "Gradient Descent(26/49): loss=1.2910005504297894e+39\n",
      "Gradient Descent(27/49): loss=4.746790841966677e+40\n",
      "Gradient Descent(28/49): loss=1.745314770770435e+42\n",
      "Gradient Descent(29/49): loss=6.4172274500458e+43\n",
      "Gradient Descent(30/49): loss=2.359506080811021e+45\n",
      "Gradient Descent(31/49): loss=8.675505097367922e+46\n",
      "Gradient Descent(32/49): loss=3.1898366063369844e+48\n",
      "Gradient Descent(33/49): loss=1.1728490112021794e+50\n",
      "Gradient Descent(34/49): loss=4.312367600099589e+51\n",
      "Gradient Descent(35/49): loss=1.585584686585301e+53\n",
      "Gradient Descent(36/49): loss=5.8299269252364974e+54\n",
      "Gradient Descent(37/49): loss=2.1435656033480957e+56\n",
      "Gradient Descent(38/49): loss=7.881528456157631e+57\n",
      "Gradient Descent(39/49): loss=2.8979048137457445e+59\n",
      "Gradient Descent(40/49): loss=1.0655106247785963e+61\n",
      "Gradient Descent(41/49): loss=3.9177024936460803e+62\n",
      "Gradient Descent(42/49): loss=1.4404729968703897e+64\n",
      "Gradient Descent(43/49): loss=5.296375766352948e+65\n",
      "Gradient Descent(44/49): loss=1.9473878593598432e+67\n",
      "Gradient Descent(45/49): loss=7.160216045987768e+68\n",
      "Gradient Descent(46/49): loss=2.6326904308663944e+70\n",
      "Gradient Descent(47/49): loss=9.679957783758944e+71\n",
      "Gradient Descent(48/49): loss=3.5591568836492125e+73\n",
      "Gradient Descent(49/49): loss=1.3086418355750825e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4339553445297128\n",
      "Gradient Descent(2/49): loss=36.602307775667214\n",
      "Gradient Descent(3/49): loss=1326.0740202302838\n",
      "Gradient Descent(4/49): loss=49105.05494801038\n",
      "Gradient Descent(5/49): loss=1824577.9134501985\n",
      "Gradient Descent(6/49): loss=67856270.20648819\n",
      "Gradient Descent(7/49): loss=2524249473.820438\n",
      "Gradient Descent(8/49): loss=93909357803.45634\n",
      "Gradient Descent(9/49): loss=3493781851362.312\n",
      "Gradient Descent(10/49): loss=129982775919128.12\n",
      "Gradient Descent(11/49): loss=4835893948047494.0\n",
      "Gradient Descent(12/49): loss=1.7991526446624192e+17\n",
      "Gradient Descent(13/49): loss=6.693593592939788e+18\n",
      "Gradient Descent(14/49): loss=2.4902944158526708e+20\n",
      "Gradient Descent(15/49): loss=9.264928177062865e+21\n",
      "Gradient Descent(16/49): loss=3.4469376007389495e+23\n",
      "Gradient Descent(17/49): loss=1.2824037721238184e+25\n",
      "Gradient Descent(18/49): loss=4.771073996849155e+26\n",
      "Gradient Descent(19/49): loss=1.7750374399697012e+28\n",
      "Gradient Descent(20/49): loss=6.603875595967992e+29\n",
      "Gradient Descent(21/49): loss=2.456915663049545e+31\n",
      "Gradient Descent(22/49): loss=9.140745442037569e+32\n",
      "Gradient Descent(23/49): loss=3.400736480003014e+34\n",
      "Gradient Descent(24/49): loss=1.2652150396006944e+36\n",
      "Gradient Descent(25/49): loss=4.707124782660454e+37\n",
      "Gradient Descent(26/49): loss=1.7512456796696024e+39\n",
      "Gradient Descent(27/49): loss=6.515360378502829e+40\n",
      "Gradient Descent(28/49): loss=2.4239843303866558e+42\n",
      "Gradient Descent(29/49): loss=9.018227224002373e+43\n",
      "Gradient Descent(30/49): loss=3.355154620606181e+45\n",
      "Gradient Descent(31/49): loss=1.2482566970827516e+47\n",
      "Gradient Descent(32/49): loss=4.6440327138498036e+48\n",
      "Gradient Descent(33/49): loss=1.7277728128926152e+50\n",
      "Gradient Descent(34/49): loss=6.428031577099317e+51\n",
      "Gradient Descent(35/49): loss=2.391494393699194e+53\n",
      "Gradient Descent(36/49): loss=8.897351182079165e+54\n",
      "Gradient Descent(37/49): loss=3.310183718841822e+56\n",
      "Gradient Descent(38/49): loss=1.2315256561475818e+58\n",
      "Gradient Descent(39/49): loss=4.58178630121588e+59\n",
      "Gradient Descent(40/49): loss=1.7046145652927984e+61\n",
      "Gradient Descent(41/49): loss=6.3418732895448855e+62\n",
      "Gradient Descent(42/49): loss=2.3594399367187453e+64\n",
      "Gradient Descent(43/49): loss=8.778095305311371e+65\n",
      "Gradient Descent(44/49): loss=3.265815585722817e+67\n",
      "Gradient Descent(45/49): loss=1.2150188701524629e+69\n",
      "Gradient Descent(46/49): loss=4.520374209984146e+70\n",
      "Gradient Descent(47/49): loss=1.6817667198638564e+72\n",
      "Gradient Descent(48/49): loss=6.256869826826885e+73\n",
      "Gradient Descent(49/49): loss=2.32781512248178e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4477746047335076\n",
      "Gradient Descent(2/49): loss=37.95108864409433\n",
      "Gradient Descent(3/49): loss=1405.118295660429\n",
      "Gradient Descent(4/49): loss=53115.104143246215\n",
      "Gradient Descent(5/49): loss=2013926.3118720409\n",
      "Gradient Descent(6/49): loss=76418659.23351344\n",
      "Gradient Descent(7/49): loss=2900327319.911967\n",
      "Gradient Descent(8/49): loss=110083095132.82635\n",
      "Gradient Descent(9/49): loss=4178319725201.96\n",
      "Gradient Descent(10/49): loss=158593296329446.56\n",
      "Gradient Descent(11/49): loss=6019613238153365.0\n",
      "Gradient Descent(12/49): loss=2.284822803445109e+17\n",
      "Gradient Descent(13/49): loss=8.672344272261204e+18\n",
      "Gradient Descent(14/49): loss=3.2917019874373304e+20\n",
      "Gradient Descent(15/49): loss=1.2494086644894876e+22\n",
      "Gradient Descent(16/49): loss=4.742294475698226e+23\n",
      "Gradient Descent(17/49): loss=1.800000076083157e+25\n",
      "Gradient Descent(18/49): loss=6.832136408250869e+26\n",
      "Gradient Descent(19/49): loss=2.5932269961584214e+28\n",
      "Gradient Descent(20/49): loss=9.842933237696444e+29\n",
      "Gradient Descent(21/49): loss=3.736014427806215e+31\n",
      "Gradient Descent(22/49): loss=1.4180532842934448e+33\n",
      "Gradient Descent(23/49): loss=5.38240725766427e+34\n",
      "Gradient Descent(24/49): loss=2.042963279887991e+36\n",
      "Gradient Descent(25/49): loss=7.754335120271138e+37\n",
      "Gradient Descent(26/49): loss=2.943259614571623e+39\n",
      "Gradient Descent(27/49): loss=1.1171527957467706e+41\n",
      "Gradient Descent(28/49): loss=4.240299981918095e+42\n",
      "Gradient Descent(29/49): loss=1.6094614814650792e+44\n",
      "Gradient Descent(30/49): loss=6.108922178538958e+45\n",
      "Gradient Descent(31/49): loss=2.3187215483699607e+47\n",
      "Gradient Descent(32/49): loss=8.801011801661288e+48\n",
      "Gradient Descent(33/49): loss=3.3405394790691324e+50\n",
      "Gradient Descent(34/49): loss=1.267945579747208e+52\n",
      "Gradient Descent(35/49): loss=4.812653774259453e+53\n",
      "Gradient Descent(36/49): loss=1.8267058713601612e+55\n",
      "Gradient Descent(37/49): loss=6.93350175802988e+56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/49): loss=2.6317015444203674e+58\n",
      "Gradient Descent(39/49): loss=9.988968432702198e+59\n",
      "Gradient Descent(40/49): loss=3.791444001736049e+61\n",
      "Gradient Descent(41/49): loss=1.4390923061923836e+63\n",
      "Gradient Descent(42/49): loss=5.462263625135553e+64\n",
      "Gradient Descent(43/49): loss=2.0732738117001804e+66\n",
      "Gradient Descent(44/49): loss=7.869382719833686e+67\n",
      "Gradient Descent(45/49): loss=2.986927440154851e+69\n",
      "Gradient Descent(46/49): loss=1.1337274917718786e+71\n",
      "Gradient Descent(47/49): loss=4.303211414913782e+72\n",
      "Gradient Descent(48/49): loss=1.633340341117033e+74\n",
      "Gradient Descent(49/49): loss=6.1995575227245794e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4447713527332164\n",
      "Gradient Descent(2/49): loss=37.68467921022836\n",
      "Gradient Descent(3/49): loss=1395.5101337111487\n",
      "Gradient Descent(4/49): loss=52831.59568039504\n",
      "Gradient Descent(5/49): loss=2007034.193172535\n",
      "Gradient Descent(6/49): loss=76314032.0344895\n",
      "Gradient Descent(7/49): loss=2902448731.253727\n",
      "Gradient Descent(8/49): loss=110396861570.52835\n",
      "Gradient Descent(9/49): loss=4199119327804.6396\n",
      "Gradient Descent(10/49): loss=159721147960362.28\n",
      "Gradient Descent(11/49): loss=6075295720824422.0\n",
      "Gradient Descent(12/49): loss=2.3108547762527392e+17\n",
      "Gradient Descent(13/49): loss=8.789778922510176e+18\n",
      "Gradient Descent(14/49): loss=3.34336098521177e+20\n",
      "Gradient Descent(15/49): loss=1.2717114871840082e+22\n",
      "Gradient Descent(16/49): loss=4.837198615539202e+23\n",
      "Gradient Descent(17/49): loss=1.8399212957024634e+25\n",
      "Gradient Descent(18/49): loss=6.998493641795147e+26\n",
      "Gradient Descent(19/49): loss=2.662011324797404e+28\n",
      "Gradient Descent(20/49): loss=1.012547078878389e+30\n",
      "Gradient Descent(21/49): loss=3.8514170747623956e+31\n",
      "Gradient Descent(22/49): loss=1.4649603750018258e+33\n",
      "Gradient Descent(23/49): loss=5.572257843457247e+34\n",
      "Gradient Descent(24/49): loss=2.1195151762339158e+36\n",
      "Gradient Descent(25/49): loss=8.061982608290361e+37\n",
      "Gradient Descent(26/49): loss=3.0665297566712935e+39\n",
      "Gradient Descent(27/49): loss=1.1664134252634737e+41\n",
      "Gradient Descent(28/49): loss=4.43667724298139e+42\n",
      "Gradient Descent(29/49): loss=1.6875753083811265e+44\n",
      "Gradient Descent(30/49): loss=6.419016451924506e+45\n",
      "Gradient Descent(31/49): loss=2.4415960582881283e+47\n",
      "Gradient Descent(32/49): loss=9.287079035388345e+48\n",
      "Gradient Descent(33/49): loss=3.5325186865685954e+50\n",
      "Gradient Descent(34/49): loss=1.3436612548904077e+52\n",
      "Gradient Descent(35/49): loss=5.1108733685069645e+53\n",
      "Gradient Descent(36/49): loss=1.9440187393841527e+55\n",
      "Gradient Descent(37/49): loss=7.39444824120301e+56\n",
      "Gradient Descent(38/49): loss=2.812620253298143e+58\n",
      "Gradient Descent(39/49): loss=1.0698340743238318e+60\n",
      "Gradient Descent(40/49): loss=4.069319152637874e+61\n",
      "Gradient Descent(41/49): loss=1.5478436108413786e+63\n",
      "Gradient Descent(42/49): loss=5.887520132377483e+64\n",
      "Gradient Descent(43/49): loss=2.239431236228585e+66\n",
      "Gradient Descent(44/49): loss=8.518106348743983e+67\n",
      "Gradient Descent(45/49): loss=3.24002517222664e+69\n",
      "Gradient Descent(46/49): loss=1.2324057351326923e+71\n",
      "Gradient Descent(47/49): loss=4.687691654395915e+72\n",
      "Gradient Descent(48/49): loss=1.7830534555510853e+74\n",
      "Gradient Descent(49/49): loss=6.782185902460707e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4467441082834793\n",
      "Gradient Descent(2/49): loss=37.50255249131828\n",
      "Gradient Descent(3/49): loss=1375.5900630482158\n",
      "Gradient Descent(4/49): loss=51556.70378003496\n",
      "Gradient Descent(5/49): loss=1938688.4770365085\n",
      "Gradient Descent(6/49): loss=72962186.20469095\n",
      "Gradient Descent(7/49): loss=2746580265.036168\n",
      "Gradient Descent(8/49): loss=103399215726.87129\n",
      "Gradient Descent(9/49): loss=3892701801366.2095\n",
      "Gradient Descent(10/49): loss=146550618458110.03\n",
      "Gradient Descent(11/49): loss=5517279025559170.0\n",
      "Gradient Descent(12/49): loss=2.0771242121397405e+17\n",
      "Gradient Descent(13/49): loss=7.819879396575064e+18\n",
      "Gradient Descent(14/49): loss=2.9439990248878604e+20\n",
      "Gradient Descent(15/49): loss=1.1083457857489235e+22\n",
      "Gradient Descent(16/49): loss=4.172658941900404e+23\n",
      "Gradient Descent(17/49): loss=1.570907102226393e+25\n",
      "Gradient Descent(18/49): loss=5.914092570291274e+26\n",
      "Gradient Descent(19/49): loss=2.2265155516775463e+28\n",
      "Gradient Descent(20/49): loss=8.382302852192226e+29\n",
      "Gradient Descent(21/49): loss=3.1557381691558715e+31\n",
      "Gradient Descent(22/49): loss=1.1880605566151984e+33\n",
      "Gradient Descent(23/49): loss=4.472766150188694e+34\n",
      "Gradient Descent(24/49): loss=1.6838903474141424e+36\n",
      "Gradient Descent(25/49): loss=6.33944768607027e+37\n",
      "Gradient Descent(26/49): loss=2.386651662095321e+39\n",
      "Gradient Descent(27/49): loss=8.985177318677955e+40\n",
      "Gradient Descent(28/49): loss=3.3827061037138006e+42\n",
      "Gradient Descent(29/49): loss=1.2735085995816765e+44\n",
      "Gradient Descent(30/49): loss=4.794457760985815e+45\n",
      "Gradient Descent(31/49): loss=1.8049996073389671e+47\n",
      "Gradient Descent(32/49): loss=6.795395318748066e+48\n",
      "Gradient Descent(33/49): loss=2.55830512928145e+50\n",
      "Gradient Descent(34/49): loss=9.631411901012978e+51\n",
      "Gradient Descent(35/49): loss=3.625998093238736e+53\n",
      "Gradient Descent(36/49): loss=1.3651022619838514e+55\n",
      "Gradient Descent(37/49): loss=5.139286171022068e+56\n",
      "Gradient Descent(38/49): loss=1.934819323299255e+58\n",
      "Gradient Descent(39/49): loss=7.284135752004003e+59\n",
      "Gradient Descent(40/49): loss=2.742304307936501e+61\n",
      "Gradient Descent(41/49): loss=1.0324125158235968e+63\n",
      "Gradient Descent(42/49): loss=3.886788201238126e+64\n",
      "Gradient Descent(43/49): loss=1.4632835508810567e+66\n",
      "Gradient Descent(44/49): loss=5.508915432019138e+67\n",
      "Gradient Descent(45/49): loss=2.073975971291806e+69\n",
      "Gradient Descent(46/49): loss=7.808027519346461e+70\n",
      "Gradient Descent(47/49): loss=2.939537129974488e+72\n",
      "Gradient Descent(48/49): loss=1.1066659943357749e+74\n",
      "Gradient Descent(49/49): loss=4.166334932567524e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4589176265430088\n",
      "Gradient Descent(2/49): loss=38.34103440835198\n",
      "Gradient Descent(3/49): loss=1422.4602535034642\n",
      "Gradient Descent(4/49): loss=53919.33605700245\n",
      "Gradient Descent(5/49): loss=2050821.9961417387\n",
      "Gradient Descent(6/49): loss=78073916.84155309\n",
      "Gradient Descent(7/49): loss=2973034764.730936\n",
      "Gradient Descent(8/49): loss=113221471151.36894\n",
      "Gradient Descent(9/49): loss=4311894130639.0205\n",
      "Gradient Descent(10/49): loss=164214141416203.7\n",
      "Gradient Descent(11/49): loss=6253943841487713.0\n",
      "Gradient Descent(12/49): loss=2.3817583149058128e+17\n",
      "Gradient Descent(13/49): loss=9.07071435166323e+18\n",
      "Gradient Descent(14/49): loss=3.454500960847665e+20\n",
      "Gradient Descent(15/49): loss=1.3156160194518809e+22\n",
      "Gradient Descent(16/49): loss=5.0104068284425455e+23\n",
      "Gradient Descent(17/49): loss=1.908168968586485e+25\n",
      "Gradient Descent(18/49): loss=7.267092152813875e+26\n",
      "Gradient Descent(19/49): loss=2.767607546032831e+28\n",
      "Gradient Descent(20/49): loss=1.0540187695638808e+30\n",
      "Gradient Descent(21/49): loss=4.01413693282186e+31\n",
      "Gradient Descent(22/49): loss=1.5287484227800403e+33\n",
      "Gradient Descent(23/49): loss=5.82210268176517e+34\n",
      "Gradient Descent(24/49): loss=2.2172961313928236e+36\n",
      "Gradient Descent(25/49): loss=8.444375516921599e+37\n",
      "Gradient Descent(26/49): loss=3.2159654662814366e+39\n",
      "Gradient Descent(27/49): loss=1.224771904043088e+41\n",
      "Gradient Descent(28/49): loss=4.6644350900566006e+42\n",
      "Gradient Descent(29/49): loss=1.7764087041456114e+44\n",
      "Gradient Descent(30/49): loss=6.765294881884627e+45\n",
      "Gradient Descent(31/49): loss=2.576502509362985e+47\n",
      "Gradient Descent(32/49): loss=9.812381125513418e+48\n",
      "Gradient Descent(33/49): loss=3.7369582603720297e+50\n",
      "Gradient Descent(34/49): loss=1.4231873855217798e+52\n",
      "Gradient Descent(35/49): loss=5.42008284060063e+53\n",
      "Gradient Descent(36/49): loss=2.0641904430738652e+55\n",
      "Gradient Descent(37/49): loss=7.861286092087291e+56\n",
      "Gradient Descent(38/49): loss=2.9939010341321487e+58\n",
      "Gradient Descent(39/49): loss=1.1402006360256477e+60\n",
      "Gradient Descent(40/49): loss=4.342352922063582e+61\n",
      "Gradient Descent(41/49): loss=1.6537465691546944e+63\n",
      "Gradient Descent(42/49): loss=6.298147027835912e+64\n",
      "Gradient Descent(43/49): loss=2.3985933953902924e+66\n",
      "Gradient Descent(44/49): loss=9.134830055542224e+67\n",
      "Gradient Descent(45/49): loss=3.478918949080991e+69\n",
      "Gradient Descent(46/49): loss=1.3249154040837125e+71\n",
      "Gradient Descent(47/49): loss=5.045822721572807e+72\n",
      "Gradient Descent(48/49): loss=1.9216567985446905e+74\n",
      "Gradient Descent(49/49): loss=7.31845935768815e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.473090417177582\n",
      "Gradient Descent(2/49): loss=39.75181312486415\n",
      "Gradient Descent(3/49): loss=1507.0957466357406\n",
      "Gradient Descent(4/49): loss=58314.75177575704\n",
      "Gradient Descent(5/49): loss=2263277.026650319\n",
      "Gradient Descent(6/49): loss=87908360.59380057\n",
      "Gradient Descent(7/49): loss=3415195069.3862367\n",
      "Gradient Descent(8/49): loss=132686657426.24507\n",
      "Gradient Descent(9/49): loss=5155212153883.7705\n",
      "Gradient Descent(10/49): loss=200294025227689.25\n",
      "Gradient Descent(11/49): loss=7781979212989057.0\n",
      "Gradient Descent(12/49): loss=3.023516312887191e+17\n",
      "Gradient Descent(13/49): loss=1.1747206870406633e+19\n",
      "Gradient Descent(14/49): loss=4.564118715502322e+20\n",
      "Gradient Descent(15/49): loss=1.7732879041093274e+22\n",
      "Gradient Descent(16/49): loss=6.889719999807852e+23\n",
      "Gradient Descent(17/49): loss=2.6768491232278286e+25\n",
      "Gradient Descent(18/49): loss=1.0400308329449803e+27\n",
      "Gradient Descent(19/49): loss=4.040810982438145e+28\n",
      "Gradient Descent(20/49): loss=1.5699682046794256e+30\n",
      "Gradient Descent(21/49): loss=6.099766048028216e+31\n",
      "Gradient Descent(22/49): loss=2.3699298960218398e+33\n",
      "Gradient Descent(23/49): loss=9.207841198886877e+34\n",
      "Gradient Descent(24/49): loss=3.577504114625851e+36\n",
      "Gradient Descent(25/49): loss=1.3899605144919906e+38\n",
      "Gradient Descent(26/49): loss=5.400385771600756e+39\n",
      "Gradient Descent(27/49): loss=2.0982010767958422e+41\n",
      "Gradient Descent(28/49): loss=8.152098655282311e+42\n",
      "Gradient Descent(29/49): loss=3.167318576870689e+44\n",
      "Gradient Descent(30/49): loss=1.2305919483555145e+46\n",
      "Gradient Descent(31/49): loss=4.781194270813143e+47\n",
      "Gradient Descent(32/49): loss=1.8576278420970394e+49\n",
      "Gradient Descent(33/49): loss=7.217404280766098e+50\n",
      "Gradient Descent(34/49): loss=2.8041636420143326e+52\n",
      "Gradient Descent(35/49): loss=1.0894960882474492e+54\n",
      "Gradient Descent(36/49): loss=4.232997349091341e+55\n",
      "Gradient Descent(37/49): loss=1.6446379891310522e+57\n",
      "Gradient Descent(38/49): loss=6.389879067308264e+58\n",
      "Gradient Descent(39/49): loss=2.4826469268411625e+60\n",
      "Gradient Descent(40/49): loss=9.645778423080995e+61\n",
      "Gradient Descent(41/49): loss=3.747654987959044e+63\n",
      "Gradient Descent(42/49): loss=1.4560688928087746e+65\n",
      "Gradient Descent(43/49): loss=5.6572353309395474e+66\n",
      "Gradient Descent(44/49): loss=2.1979943220882847e+68\n",
      "Gradient Descent(45/49): loss=8.539823354193032e+69\n",
      "Gradient Descent(46/49): loss=3.3179604782387388e+71\n",
      "Gradient Descent(47/49): loss=1.2891205448352677e+73\n",
      "Gradient Descent(48/49): loss=5.008594255464169e+74\n",
      "Gradient Descent(49/49): loss=1.9459791030694062e+76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4699700644769342\n",
      "Gradient Descent(2/49): loss=39.47275990375607\n",
      "Gradient Descent(3/49): loss=1496.784297226066\n",
      "Gradient Descent(4/49): loss=58002.85819388104\n",
      "Gradient Descent(5/49): loss=2255495.435333617\n",
      "Gradient Descent(6/49): loss=87786231.79158469\n",
      "Gradient Descent(7/49): loss=3417611439.2167478\n",
      "Gradient Descent(8/49): loss=133061200427.33278\n",
      "Gradient Descent(9/49): loss=5180714609221.134\n",
      "Gradient Descent(10/49): loss=201711506233523.5\n",
      "Gradient Descent(11/49): loss=7853667069993910.0\n",
      "Gradient Descent(12/49): loss=3.057838459802223e+17\n",
      "Gradient Descent(13/49): loss=1.1905747287897532e+19\n",
      "Gradient Descent(14/49): loss=4.635523654465478e+20\n",
      "Gradient Descent(15/49): loss=1.8048493157090693e+22\n",
      "Gradient Descent(16/49): loss=7.027212691155476e+23\n",
      "Gradient Descent(17/49): loss=2.736057677220017e+25\n",
      "Gradient Descent(18/49): loss=1.0652888911515332e+27\n",
      "Gradient Descent(19/49): loss=4.147721121427775e+28\n",
      "Gradient Descent(20/49): loss=1.6149225477277024e+30\n",
      "Gradient Descent(21/49): loss=6.287729475611553e+31\n",
      "Gradient Descent(22/49): loss=2.4481385818907073e+33\n",
      "Gradient Descent(23/49): loss=9.531870827765424e+34\n",
      "Gradient Descent(24/49): loss=3.711250749827087e+36\n",
      "Gradient Descent(25/49): loss=1.4449820373113411e+38\n",
      "Gradient Descent(26/49): loss=5.62606309543957e+39\n",
      "Gradient Descent(27/49): loss=2.1905176075934396e+41\n",
      "Gradient Descent(28/49): loss=8.528819012119519e+42\n",
      "Gradient Descent(29/49): loss=3.3207107529898525e+44\n",
      "Gradient Descent(30/49): loss=1.2929245994495613e+46\n",
      "Gradient Descent(31/49): loss=5.034024774234536e+47\n",
      "Gradient Descent(32/49): loss=1.9600064410868139e+49\n",
      "Gradient Descent(33/49): loss=7.631319712140188e+50\n",
      "Gradient Descent(34/49): loss=2.9712678146408056e+52\n",
      "Gradient Descent(35/49): loss=1.1568683739295766e+54\n",
      "Gradient Descent(36/49): loss=4.5042874560274697e+55\n",
      "Gradient Descent(37/49): loss=1.75375228018475e+57\n",
      "Gradient Descent(38/49): loss=6.828265492109102e+58\n",
      "Gradient Descent(39/49): loss=2.6585972350562704e+60\n",
      "Gradient Descent(40/49): loss=1.035129531272168e+62\n",
      "Gradient Descent(41/49): loss=4.0302951209872196e+63\n",
      "Gradient Descent(42/49): loss=1.5692025269813853e+65\n",
      "Gradient Descent(43/49): loss=6.109717766974844e+66\n",
      "Gradient Descent(44/49): loss=2.378829408584722e+68\n",
      "Gradient Descent(45/49): loss=9.262014336792305e+69\n",
      "Gradient Descent(46/49): loss=3.6061816482243425e+71\n",
      "Gradient Descent(47/49): loss=1.404073196942804e+73\n",
      "Gradient Descent(48/49): loss=5.466783802596038e+74\n",
      "Gradient Descent(49/49): loss=2.1285019334746053e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4720536579797108\n",
      "Gradient Descent(2/49): loss=39.283115090299944\n",
      "Gradient Descent(3/49): loss=1475.4885858221596\n",
      "Gradient Descent(4/49): loss=56606.90009930644\n",
      "Gradient Descent(5/49): loss=2178869.9730367805\n",
      "Gradient Descent(6/49): loss=83938905.99812645\n",
      "Gradient Descent(7/49): loss=3234455391.0158653\n",
      "Gradient Descent(8/49): loss=124643592723.04881\n",
      "Gradient Descent(9/49): loss=4803389963129.24\n",
      "Gradient Descent(10/49): loss=185109374902333.56\n",
      "Gradient Descent(11/49): loss=7133616740870306.0\n",
      "Gradient Descent(12/49): loss=2.7491052339223942e+17\n",
      "Gradient Descent(13/49): loss=1.0594318505766613e+19\n",
      "Gradient Descent(14/49): loss=4.082768039097827e+20\n",
      "Gradient Descent(15/49): loss=1.5733900280711396e+22\n",
      "Gradient Descent(16/49): loss=6.063426006111934e+23\n",
      "Gradient Descent(17/49): loss=2.336682851668218e+25\n",
      "Gradient Descent(18/49): loss=9.004953215746159e+26\n",
      "Gradient Descent(19/49): loss=3.4702690768829355e+28\n",
      "Gradient Descent(20/49): loss=1.3373492541064973e+30\n",
      "Gradient Descent(21/49): loss=5.153787754931047e+31\n",
      "Gradient Descent(22/49): loss=1.9861325036364506e+33\n",
      "Gradient Descent(23/49): loss=7.654025562519737e+34\n",
      "Gradient Descent(24/49): loss=2.949657548247985e+36\n",
      "Gradient Descent(25/49): loss=1.13671944009978e+38\n",
      "Gradient Descent(26/49): loss=4.3806138996314245e+39\n",
      "Gradient Descent(27/49): loss=1.6881718971886061e+41\n",
      "Gradient Descent(28/49): loss=6.505764762096892e+42\n",
      "Gradient Descent(29/49): loss=2.5071484254789038e+44\n",
      "Gradient Descent(30/49): loss=9.66188212645323e+45\n",
      "Gradient Descent(31/49): loss=3.723431978609141e+47\n",
      "Gradient Descent(32/49): loss=1.4349114921792663e+49\n",
      "Gradient Descent(33/49): loss=5.529766629863962e+50\n",
      "Gradient Descent(34/49): loss=2.1310247459455856e+52\n",
      "Gradient Descent(35/49): loss=8.212401665030379e+53\n",
      "Gradient Descent(36/49): loss=3.164840822994184e+55\n",
      "Gradient Descent(37/49): loss=1.2196453416959663e+57\n",
      "Gradient Descent(38/49): loss=4.7001882328898446e+58\n",
      "Gradient Descent(39/49): loss=1.811327331753391e+60\n",
      "Gradient Descent(40/49): loss=6.98037299825258e+61\n",
      "Gradient Descent(41/49): loss=2.6900497961109308e+63\n",
      "Gradient Descent(42/49): loss=1.036673528387086e+65\n",
      "Gradient Descent(43/49): loss=3.995063608161612e+66\n",
      "Gradient Descent(44/49): loss=1.5395910859313182e+68\n",
      "Gradient Descent(45/49): loss=5.933173897498809e+69\n",
      "Gradient Descent(46/49): loss=2.2864871601063283e+71\n",
      "Gradient Descent(47/49): loss=8.811512393956613e+72\n",
      "Gradient Descent(48/49): loss=3.3957221375885837e+74\n",
      "Gradient Descent(49/49): loss=1.3086208496531997e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.484190476349191\n",
      "Gradient Descent(2/49): loss=40.14167033706315\n",
      "Gradient Descent(3/49): loss=1524.6176148267812\n",
      "Gradient Descent(4/49): loss=59141.67173755436\n",
      "Gradient Descent(5/49): loss=2302008.913204768\n",
      "Gradient Descent(6/49): loss=89684658.45123477\n",
      "Gradient Descent(7/49): loss=3494995480.13404\n",
      "Gradient Descent(8/49): loss=136210422184.81496\n",
      "Gradient Descent(9/49): loss=5308657655287.081\n",
      "Gradient Descent(10/49): loss=206900875811531.78\n",
      "Gradient Descent(11/49): loss=8063821719804148.0\n",
      "Gradient Descent(12/49): loss=3.1428221325733446e+17\n",
      "Gradient Descent(13/49): loss=1.2248947642567764e+19\n",
      "Gradient Descent(14/49): loss=4.7739490640555835e+20\n",
      "Gradient Descent(15/49): loss=1.8606161742048017e+22\n",
      "Gradient Descent(16/49): loss=7.251632812335118e+23\n",
      "Gradient Descent(17/49): loss=2.8262776206188556e+25\n",
      "Gradient Descent(18/49): loss=1.101523670568728e+27\n",
      "Gradient Descent(19/49): loss=4.293118227901766e+28\n",
      "Gradient Descent(20/49): loss=1.6732154389499574e+30\n",
      "Gradient Descent(21/49): loss=6.521250421166116e+31\n",
      "Gradient Descent(22/49): loss=2.5416157456949176e+33\n",
      "Gradient Descent(23/49): loss=9.90578521230607e+34\n",
      "Gradient Descent(24/49): loss=3.860716587021036e+36\n",
      "Gradient Descent(25/49): loss=1.5046896581993763e+38\n",
      "Gradient Descent(26/49): loss=5.864431942773695e+39\n",
      "Gradient Descent(27/49): loss=2.285624934285793e+41\n",
      "Gradient Descent(28/49): loss=8.908077357204639e+42\n",
      "Gradient Descent(29/49): loss=3.471866315928962e+44\n",
      "Gradient Descent(30/49): loss=1.3531377459282226e+46\n",
      "Gradient Descent(31/49): loss=5.2737680337953916e+47\n",
      "Gradient Descent(32/49): loss=2.0554174442309472e+49\n",
      "Gradient Descent(33/49): loss=8.010858352085035e+50\n",
      "Gradient Descent(34/49): loss=3.1221809329920096e+52\n",
      "Gradient Descent(35/49): loss=1.2168500989412313e+54\n",
      "Gradient Descent(36/49): loss=4.7425956248931995e+55\n",
      "Gradient Descent(37/49): loss=1.8483963867715748e+57\n",
      "Gradient Descent(38/49): loss=7.204006988698649e+58\n",
      "Gradient Descent(39/49): loss=2.8077157618698792e+60\n",
      "Gradient Descent(40/49): loss=1.0942893048021087e+62\n",
      "Gradient Descent(41/49): loss=4.2649227491845104e+63\n",
      "Gradient Descent(42/49): loss=1.6622264310443076e+65\n",
      "Gradient Descent(43/49): loss=6.478421464000951e+66\n",
      "Gradient Descent(44/49): loss=2.524923432895978e+68\n",
      "Gradient Descent(45/49): loss=9.84072798815716e+69\n",
      "Gradient Descent(46/49): loss=3.835360948978504e+71\n",
      "Gradient Descent(47/49): loss=1.4948074600427937e+73\n",
      "Gradient Descent(48/49): loss=5.825916705948374e+74\n",
      "Gradient Descent(49/49): loss=2.2706138664625524e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.49872107625342\n",
      "Gradient Descent(2/49): loss=41.616553621789464\n",
      "Gradient Descent(3/49): loss=1615.1698454641516\n",
      "Gradient Descent(4/49): loss=63954.46502157376\n",
      "Gradient Descent(5/49): loss=2540077.5049400385\n",
      "Gradient Descent(6/49): loss=100962270.52894683\n",
      "Gradient Descent(7/49): loss=4013887627.3203907\n",
      "Gradient Descent(8/49): loss=159587215966.53574\n",
      "Gradient Descent(9/49): loss=6345102864063.08\n",
      "Gradient Descent(10/49): loss=252279198121587.66\n",
      "Gradient Descent(11/49): loss=1.003055239739748e+16\n",
      "Gradient Descent(12/49): loss=3.9881220644518874e+17\n",
      "Gradient Descent(13/49): loss=1.5856673582199548e+19\n",
      "Gradient Descent(14/49): loss=6.304573976419761e+20\n",
      "Gradient Descent(15/49): loss=2.506682956865049e+22\n",
      "Gradient Descent(16/49): loss=9.966509216616286e+23\n",
      "Gradient Descent(17/49): loss=3.9626593293298495e+25\n",
      "Gradient Descent(18/49): loss=1.5755435150628966e+27\n",
      "Gradient Descent(19/49): loss=6.264321915470127e+28\n",
      "Gradient Descent(20/49): loss=2.4906788474099182e+30\n",
      "Gradient Descent(21/49): loss=9.902877286127827e+31\n",
      "Gradient Descent(22/49): loss=3.937359433001472e+33\n",
      "Gradient Descent(23/49): loss=1.5654843392202253e+35\n",
      "Gradient Descent(24/49): loss=6.224326882130488e+36\n",
      "Gradient Descent(25/49): loss=2.4747769214294224e+38\n",
      "Gradient Descent(26/49): loss=9.839651623089897e+39\n",
      "Gradient Descent(27/49): loss=3.9122210662872e+41\n",
      "Gradient Descent(28/49): loss=1.5554893870007862e+43\n",
      "Gradient Descent(29/49): loss=6.184587200150982e+44\n",
      "Gradient Descent(30/49): loss=2.458976522496326e+46\n",
      "Gradient Descent(31/49): loss=9.776829629050284e+47\n",
      "Gradient Descent(32/49): loss=3.8872431973623455e+49\n",
      "Gradient Descent(33/49): loss=1.5455582483038158e+51\n",
      "Gradient Descent(34/49): loss=6.145101239152787e+52\n",
      "Gradient Descent(35/49): loss=2.4432770023957136e+54\n",
      "Gradient Descent(36/49): loss=9.714408726745117e+55\n",
      "Gradient Descent(37/49): loss=3.862424801515679e+57\n",
      "Gradient Descent(38/49): loss=1.535690515706977e+59\n",
      "Gradient Descent(39/49): loss=6.105867379235182e+60\n",
      "Gradient Descent(40/49): loss=2.4276777170590974e+62\n",
      "Gradient Descent(41/49): loss=9.652386355373949e+63\n",
      "Gradient Descent(42/49): loss=3.8377648605793665e+65\n",
      "Gradient Descent(43/49): loss=1.5258857843892352e+67\n",
      "Gradient Descent(44/49): loss=6.06688401083976e+68\n",
      "Gradient Descent(45/49): loss=2.4121780265300563e+70\n",
      "Gradient Descent(46/49): loss=9.59075997048613e+71\n",
      "Gradient Descent(47/49): loss=3.813262362886098e+73\n",
      "Gradient Descent(48/49): loss=1.5161436521142117e+75\n",
      "Gradient Descent(49/49): loss=6.028149534684685e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.495481887555984\n",
      "Gradient Descent(2/49): loss=41.324415977480896\n",
      "Gradient Descent(3/49): loss=1604.1127881865682\n",
      "Gradient Descent(4/49): loss=63611.73271774918\n",
      "Gradient Descent(5/49): loss=2531304.624913642\n",
      "Gradient Descent(6/49): loss=100820019.92822373\n",
      "Gradient Descent(7/49): loss=4016633860.2588787\n",
      "Gradient Descent(8/49): loss=160033404269.2354\n",
      "Gradient Descent(9/49): loss=6376299096440.021\n",
      "Gradient Descent(10/49): loss=254056050905475.3\n",
      "Gradient Descent(11/49): loss=1.012258039172815e+16\n",
      "Gradient Descent(12/49): loss=4.033231675323547e+17\n",
      "Gradient Descent(13/49): loss=1.6069974049442988e+19\n",
      "Gradient Descent(14/49): loss=6.402907127511632e+20\n",
      "Gradient Descent(15/49): loss=2.5511690397178275e+22\n",
      "Gradient Descent(16/49): loss=1.0164856941770786e+24\n",
      "Gradient Descent(17/49): loss=4.050077245532286e+25\n",
      "Gradient Descent(18/49): loss=1.6137094495883703e+27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/49): loss=6.429650671075301e+28\n",
      "Gradient Descent(20/49): loss=2.561824730212672e+30\n",
      "Gradient Descent(21/49): loss=1.0207313404930283e+32\n",
      "Gradient Descent(22/49): loss=4.0669935658750654e+33\n",
      "Gradient Descent(23/49): loss=1.6204495746052836e+35\n",
      "Gradient Descent(24/49): loss=6.456505970090104e+36\n",
      "Gradient Descent(25/49): loss=2.5725249335182915e+38\n",
      "Gradient Descent(26/49): loss=1.0249947207097645e+40\n",
      "Gradient Descent(27/49): loss=4.083980542983625e+41\n",
      "Gradient Descent(28/49): loss=1.627217851807021e+43\n",
      "Gradient Descent(29/49): loss=6.4834734381595696e+44\n",
      "Gradient Descent(30/49): loss=2.5832698293372286e+46\n",
      "Gradient Descent(31/49): loss=1.0292759081709669e+48\n",
      "Gradient Descent(32/49): loss=4.101038471126241e+49\n",
      "Gradient Descent(33/49): loss=1.6340143986799632e+51\n",
      "Gradient Descent(34/49): loss=6.510553543673995e+52\n",
      "Gradient Descent(35/49): loss=2.5940596043271388e+54\n",
      "Gradient Descent(36/49): loss=1.0335749772521673e+56\n",
      "Gradient Descent(37/49): loss=4.118167646648636e+57\n",
      "Gradient Descent(38/49): loss=1.6408393333003335e+59\n",
      "Gradient Descent(39/49): loss=6.537746757095996e+60\n",
      "Gradient Descent(40/49): loss=2.6048944459387797e+62\n",
      "Gradient Descent(41/49): loss=1.0378920026410968e+64\n",
      "Gradient Descent(42/49): loss=4.135368367136012e+65\n",
      "Gradient Descent(43/49): loss=1.6476927742377776e+67\n",
      "Gradient Descent(44/49): loss=6.565053550853572e+68\n",
      "Gradient Descent(45/49): loss=2.6157745424060006e+70\n",
      "Gradient Descent(46/49): loss=1.0422270593375042e+72\n",
      "Gradient Descent(47/49): loss=4.1526409314166917e+73\n",
      "Gradient Descent(48/49): loss=1.6545748405571943e+75\n",
      "Gradient Descent(49/49): loss=6.59247439934784e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4976781372338497\n",
      "Gradient Descent(2/49): loss=41.127025264380336\n",
      "Gradient Descent(3/49): loss=1581.3633329243348\n",
      "Gradient Descent(4/49): loss=62084.77439841005\n",
      "Gradient Descent(5/49): loss=2445507.773505971\n",
      "Gradient Descent(6/49): loss=96410899.96310411\n",
      "Gradient Descent(7/49): loss=3801809088.8390875\n",
      "Gradient Descent(8/49): loss=149929066859.17868\n",
      "Gradient Descent(9/49): loss=5912764993061.184\n",
      "Gradient Descent(10/49): loss=233183668402974.8\n",
      "Gradient Descent(11/49): loss=9196158555976270.0\n",
      "Gradient Descent(12/49): loss=3.626728206046537e+17\n",
      "Gradient Descent(13/49): loss=1.4302884778209366e+19\n",
      "Gradient Descent(14/49): loss=5.6406907608036894e+20\n",
      "Gradient Descent(15/49): loss=2.224543734088899e+22\n",
      "Gradient Descent(16/49): loss=8.773029856810902e+23\n",
      "Gradient Descent(17/49): loss=3.4598579378402965e+25\n",
      "Gradient Descent(18/49): loss=1.364479222097522e+27\n",
      "Gradient Descent(19/49): loss=5.381156050879964e+28\n",
      "Gradient Descent(20/49): loss=2.122189915090504e+30\n",
      "Gradient Descent(21/49): loss=8.36937266485418e+31\n",
      "Gradient Descent(22/49): loss=3.300665897303507e+33\n",
      "Gradient Descent(23/49): loss=1.3016979649359838e+35\n",
      "Gradient Descent(24/49): loss=5.1335628768217805e+36\n",
      "Gradient Descent(25/49): loss=2.0245455182516797e+38\n",
      "Gradient Descent(26/49): loss=7.984288210395191e+39\n",
      "Gradient Descent(27/49): loss=3.1487984662210563e+41\n",
      "Gradient Descent(28/49): loss=1.2418053456496335e+43\n",
      "Gradient Descent(29/49): loss=4.897361749336363e+44\n",
      "Gradient Descent(30/49): loss=1.9313938523364937e+46\n",
      "Gradient Descent(31/49): loss=7.616921934240788e+47\n",
      "Gradient Descent(32/49): loss=3.003918630171292e+49\n",
      "Gradient Descent(33/49): loss=1.1846684546058069e+51\n",
      "Gradient Descent(34/49): loss=4.6720285071706017e+52\n",
      "Gradient Descent(35/49): loss=1.842528201620577e+54\n",
      "Gradient Descent(36/49): loss=7.26645860263197e+55\n",
      "Gradient Descent(37/49): loss=2.8657048818749763e+57\n",
      "Gradient Descent(38/49): loss=1.1301604975809758e+59\n",
      "Gradient Descent(39/49): loss=4.4570631064312034e+60\n",
      "Gradient Descent(40/49): loss=1.7577513616190585e+62\n",
      "Gradient Descent(41/49): loss=6.932120491665127e+63\n",
      "Gradient Descent(42/49): loss=2.7338505069739087e+65\n",
      "Gradient Descent(43/49): loss=1.0781605142997463e+67\n",
      "Gradient Descent(44/49): loss=4.2519885108194424e+68\n",
      "Gradient Descent(45/49): loss=1.6768752014520767e+70\n",
      "Gradient Descent(46/49): loss=6.61316566140727e+71\n",
      "Gradient Descent(47/49): loss=2.608062903389898e+73\n",
      "Gradient Descent(48/49): loss=1.028553110007998e+75\n",
      "Gradient Descent(49/49): loss=4.056349633024816e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5097738939482594\n",
      "Gradient Descent(2/49): loss=42.00566879761693\n",
      "Gradient Descent(3/49): loss=1632.8193260385387\n",
      "Gradient Descent(4/49): loss=64801.356858341285\n",
      "Gradient Descent(5/49): loss=2580553.771515025\n",
      "Gradient Descent(6/49): loss=102859079.54729252\n",
      "Gradient Descent(7/49): loss=4101010896.0766172\n",
      "Gradient Descent(8/49): loss=163521530235.7112\n",
      "Gradient Descent(9/49): loss=6520333218712.401\n",
      "Gradient Descent(10/49): loss=259996754903415.1\n",
      "Gradient Descent(11/49): loss=1.0367333365475364e+16\n",
      "Gradient Descent(12/49): loss=4.133962223940454e+17\n",
      "Gradient Descent(13/49): loss=1.648413020204547e+19\n",
      "Gradient Descent(14/49): loss=6.573029732113967e+20\n",
      "Gradient Descent(15/49): loss=2.620988804348832e+22\n",
      "Gradient Descent(16/49): loss=1.0451165799602094e+24\n",
      "Gradient Descent(17/49): loss=4.167391580923849e+25\n",
      "Gradient Descent(18/49): loss=1.6617430946334006e+27\n",
      "Gradient Descent(19/49): loss=6.626183451697352e+28\n",
      "Gradient Descent(20/49): loss=2.6421838176784224e+30\n",
      "Gradient Descent(21/49): loss=1.0535680723904465e+32\n",
      "Gradient Descent(22/49): loss=4.2010918231309193e+33\n",
      "Gradient Descent(23/49): loss=1.6751810318585827e+35\n",
      "Gradient Descent(24/49): loss=6.679767088281876e+36\n",
      "Gradient Descent(25/49): loss=2.663550237564192e+38\n",
      "Gradient Descent(26/49): loss=1.0620879102917035e+40\n",
      "Gradient Descent(27/49): loss=4.235064588905175e+41\n",
      "Gradient Descent(28/49): loss=1.6887276371757698e+43\n",
      "Gradient Descent(29/49): loss=6.733784037278422e+44\n",
      "Gradient Descent(30/49): loss=2.685089440268697e+46\n",
      "Gradient Descent(31/49): loss=1.0706766451566193e+48\n",
      "Gradient Descent(32/49): loss=4.2693120806028635e+49\n",
      "Gradient Descent(33/49): loss=1.7023837891705758e+51\n",
      "Gradient Descent(34/49): loss=6.788237802520898e+52\n",
      "Gradient Descent(35/49): loss=2.706802823000599e+54\n",
      "Gradient Descent(36/49): loss=1.0793348341278096e+56\n",
      "Gradient Descent(37/49): loss=4.3038365198330166e+57\n",
      "Gradient Descent(38/49): loss=1.7161503737082988e+59\n",
      "Gradient Descent(39/49): loss=6.843131916389358e+60\n",
      "Gradient Descent(40/49): loss=2.7286917942929832e+62\n",
      "Gradient Descent(41/49): loss=1.0880630388564047e+64\n",
      "Gradient Descent(42/49): loss=4.338640146173741e+65\n",
      "Gradient Descent(43/49): loss=1.7300282838184623e+67\n",
      "Gradient Descent(44/49): loss=6.898469939829881e+68\n",
      "Gradient Descent(45/49): loss=2.750757774069429e+70\n",
      "Gradient Descent(46/49): loss=1.096861825535473e+72\n",
      "Gradient Descent(47/49): loss=4.373725217314034e+73\n",
      "Gradient Descent(48/49): loss=1.7440184197521864e+75\n",
      "Gradient Descent(49/49): loss=6.954255462584407e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5246665819610214\n",
      "Gradient Descent(2/49): loss=43.546810499098044\n",
      "Gradient Descent(3/49): loss=1729.6287907558437\n",
      "Gradient Descent(4/49): loss=70065.79055438955\n",
      "Gradient Descent(5/49): loss=2846981.7139996164\n",
      "Gradient Descent(6/49): loss=115771645.91275886\n",
      "Gradient Descent(7/49): loss=4708849144.750933\n",
      "Gradient Descent(8/49): loss=191537808119.98553\n",
      "Gradient Descent(9/49): loss=7791158833503.18\n",
      "Gradient Descent(10/49): loss=316921609054985.3\n",
      "Gradient Descent(11/49): loss=1.2891465616793004e+16\n",
      "Gradient Descent(12/49): loss=5.243882162808991e+17\n",
      "Gradient Descent(13/49): loss=2.1330626297445773e+19\n",
      "Gradient Descent(14/49): loss=8.676694486943535e+20\n",
      "Gradient Descent(15/49): loss=3.5294335453042974e+22\n",
      "Gradient Descent(16/49): loss=1.4356735974197153e+24\n",
      "Gradient Descent(17/49): loss=5.839913554509795e+25\n",
      "Gradient Descent(18/49): loss=2.3755114250118906e+27\n",
      "Gradient Descent(19/49): loss=9.662907640827724e+28\n",
      "Gradient Descent(20/49): loss=3.9305971376929884e+30\n",
      "Gradient Descent(21/49): loss=1.5988555860406109e+32\n",
      "Gradient Descent(22/49): loss=6.503691666854618e+33\n",
      "Gradient Descent(23/49): loss=2.645517560612299e+35\n",
      "Gradient Descent(24/49): loss=1.0761216124647306e+37\n",
      "Gradient Descent(25/49): loss=4.377357920639609e+38\n",
      "Gradient Descent(26/49): loss=1.7805852185702195e+40\n",
      "Gradient Descent(27/49): loss=7.242916339195572e+41\n",
      "Gradient Descent(28/49): loss=2.9462132196464425e+43\n",
      "Gradient Descent(29/49): loss=1.198436089706857e+45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=4.874898569914828e+46\n",
      "Gradient Descent(31/49): loss=1.9829706624381304e+48\n",
      "Gradient Descent(32/49): loss=8.066163001539147e+49\n",
      "Gradient Descent(33/49): loss=3.2810866443884575e+51\n",
      "Gradient Descent(34/49): loss=1.3346531139936259e+53\n",
      "Gradient Descent(35/49): loss=5.42899084283366e+54\n",
      "Gradient Descent(36/49): loss=2.208359705045617e+56\n",
      "Gradient Descent(37/49): loss=8.982981788054907e+57\n",
      "Gradient Descent(38/49): loss=3.6540225589227024e+59\n",
      "Gradient Descent(39/49): loss=1.486352881052327e+61\n",
      "Gradient Descent(40/49): loss=6.046062528042823e+62\n",
      "Gradient Descent(41/49): loss=2.4593669887545696e+64\n",
      "Gradient Descent(42/49): loss=1.0004008323304064e+66\n",
      "Gradient Descent(43/49): loss=4.069347234079042e+67\n",
      "Gradient Descent(44/49): loss=1.6552951953200237e+69\n",
      "Gradient Descent(45/49): loss=6.733272011548238e+70\n",
      "Gradient Descent(46/49): loss=2.738904342239312e+72\n",
      "Gradient Descent(47/49): loss=1.1141087101592431e+74\n",
      "Gradient Descent(48/49): loss=4.531878674659606e+75\n",
      "Gradient Descent(49/49): loss=1.843439884685839e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5213068219703663\n",
      "Gradient Descent(2/49): loss=43.24113768900195\n",
      "Gradient Descent(3/49): loss=1717.7817947389315\n",
      "Gradient Descent(4/49): loss=69689.58652019557\n",
      "Gradient Descent(5/49): loss=2837105.5159978867\n",
      "Gradient Descent(6/49): loss=115606304.33159727\n",
      "Gradient Descent(7/49): loss=4711963405.210592\n",
      "Gradient Descent(8/49): loss=192068296630.47678\n",
      "Gradient Descent(9/49): loss=7829233692395.118\n",
      "Gradient Descent(10/49): loss=319143282283664.94\n",
      "Gradient Descent(11/49): loss=1.3009272331895224e+16\n",
      "Gradient Descent(12/49): loss=5.302986874686723e+17\n",
      "Gradient Descent(13/49): loss=2.1616639134978347e+19\n",
      "Gradient Descent(14/49): loss=8.811621106123325e+20\n",
      "Gradient Descent(15/49): loss=3.591893594931038e+22\n",
      "Gradient Descent(16/49): loss=1.4641686810428044e+24\n",
      "Gradient Descent(17/49): loss=5.968411570669539e+25\n",
      "Gradient Descent(18/49): loss=2.4329120784634512e+27\n",
      "Gradient Descent(19/49): loss=9.91731403254762e+28\n",
      "Gradient Descent(20/49): loss=4.0426087935513873e+30\n",
      "Gradient Descent(21/49): loss=1.6478943597247344e+32\n",
      "Gradient Descent(22/49): loss=6.71733516523267e+33\n",
      "Gradient Descent(23/49): loss=2.7381968665543493e+35\n",
      "Gradient Descent(24/49): loss=1.1161750747255132e+37\n",
      "Gradient Descent(25/49): loss=4.549880297709629e+38\n",
      "Gradient Descent(26/49): loss=1.854674162884129e+40\n",
      "Gradient Descent(27/49): loss=7.560234611450236e+41\n",
      "Gradient Descent(28/49): loss=3.081789164048479e+43\n",
      "Gradient Descent(29/49): loss=1.2562340905747101e+45\n",
      "Gradient Descent(30/49): loss=5.120804851714546e+46\n",
      "Gradient Descent(31/49): loss=2.0874009490816046e+48\n",
      "Gradient Descent(32/49): loss=8.508902112854169e+49\n",
      "Gradient Descent(33/49): loss=3.4684958439818865e+51\n",
      "Gradient Descent(34/49): loss=1.4138678833248693e+53\n",
      "Gradient Descent(35/49): loss=5.7633697182195445e+54\n",
      "Gradient Descent(36/49): loss=2.3493305775344293e+56\n",
      "Gradient Descent(37/49): loss=9.576609574586379e+57\n",
      "Gradient Descent(38/49): loss=3.903726951883841e+59\n",
      "Gradient Descent(39/49): loss=1.5912817575130648e+61\n",
      "Gradient Descent(40/49): loss=6.486564411406636e+62\n",
      "Gradient Descent(41/49): loss=2.644127456666427e+64\n",
      "Gradient Descent(42/49): loss=1.0778294276709708e+66\n",
      "Gradient Descent(43/49): loss=4.3935713924250665e+67\n",
      "Gradient Descent(44/49): loss=1.79095774199151e+69\n",
      "Gradient Descent(45/49): loss=7.300506460710827e+70\n",
      "Gradient Descent(46/49): loss=2.97591580935991e+72\n",
      "Gradient Descent(47/49): loss=1.2130767847489776e+74\n",
      "Gradient Descent(48/49): loss=4.944882113494421e+75\n",
      "Gradient Descent(49/49): loss=2.015689313633756e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.523617546045896\n",
      "Gradient Descent(2/49): loss=43.03576885182212\n",
      "Gradient Descent(3/49): loss=1693.4969929774365\n",
      "Gradient Descent(4/49): loss=68021.0039011752\n",
      "Gradient Descent(5/49): loss=2741161.318966086\n",
      "Gradient Descent(6/49): loss=110561105.52266334\n",
      "Gradient Descent(7/49): loss=4460447395.1903305\n",
      "Gradient Descent(8/49): loss=179964260467.01962\n",
      "Gradient Descent(9/49): loss=7261118215620.352\n",
      "Gradient Descent(10/49): loss=292970257380891.8\n",
      "Gradient Descent(11/49): loss=1.1820732331017306e+16\n",
      "Gradient Descent(12/49): loss=4.769419014355189e+17\n",
      "Gradient Descent(13/49): loss=1.924361444116862e+19\n",
      "Gradient Descent(14/49): loss=7.764398850690217e+20\n",
      "Gradient Descent(15/49): loss=3.132773761622256e+22\n",
      "Gradient Descent(16/49): loss=1.2640091877170646e+24\n",
      "Gradient Descent(17/49): loss=5.100014716275082e+25\n",
      "Gradient Descent(18/49): loss=2.057750083541602e+27\n",
      "Gradient Descent(19/49): loss=8.302594487199892e+28\n",
      "Gradient Descent(20/49): loss=3.3499245497574613e+30\n",
      "Gradient Descent(21/49): loss=1.351625025945967e+32\n",
      "Gradient Descent(22/49): loss=5.453526441069067e+33\n",
      "Gradient Descent(23/49): loss=2.2003847274617427e+35\n",
      "Gradient Descent(24/49): loss=8.878095671062701e+36\n",
      "Gradient Descent(25/49): loss=3.582127332590243e+38\n",
      "Gradient Descent(26/49): loss=1.4453140293040448e+40\n",
      "Gradient Descent(27/49): loss=5.831542123860233e+41\n",
      "Gradient Descent(28/49): loss=2.3529062095060096e+43\n",
      "Gradient Descent(29/49): loss=9.493488194281023e+44\n",
      "Gradient Descent(30/49): loss=3.830425442834568e+46\n",
      "Gradient Descent(31/49): loss=1.5454971631979377e+48\n",
      "Gradient Descent(32/49): loss=6.235760275457286e+49\n",
      "Gradient Descent(33/49): loss=2.5159998438632633e+51\n",
      "Gradient Descent(34/49): loss=1.0151537157761767e+53\n",
      "Gradient Descent(35/49): loss=4.0959345413623375e+54\n",
      "Gradient Descent(36/49): loss=1.6526245736388631e+56\n",
      "Gradient Descent(37/49): loss=6.667997141591563e+57\n",
      "Gradient Descent(38/49): loss=2.6903984479895294e+59\n",
      "Gradient Descent(39/49): loss=1.0855199327840164e+61\n",
      "Gradient Descent(40/49): loss=4.379847622020319e+62\n",
      "Gradient Descent(41/49): loss=1.7671776088826485e+64\n",
      "Gradient Descent(42/49): loss=7.130194862568415e+65\n",
      "Gradient Descent(43/49): loss=2.876885635187621e+67\n",
      "Gradient Descent(44/49): loss=1.1607636421548777e+69\n",
      "Gradient Descent(45/49): loss=4.6834403720076754e+70\n",
      "Gradient Descent(46/49): loss=1.8896709822364166e+72\n",
      "Gradient Descent(47/49): loss=7.624430199749931e+73\n",
      "Gradient Descent(48/49): loss=3.076299335562646e+75\n",
      "Gradient Descent(49/49): loss=1.2412229312943987e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.535667879340214\n",
      "Gradient Descent(2/49): loss=43.934499912779835\n",
      "Gradient Descent(3/49): loss=1747.3481989501115\n",
      "Gradient Descent(4/49): loss=70929.41841145723\n",
      "Gradient Descent(5/49): loss=2889073.873389037\n",
      "Gradient Descent(6/49): loss=117786339.49441047\n",
      "Gradient Descent(7/49): loss=4803425094.53633\n",
      "Gradient Descent(8/49): loss=195904004635.74823\n",
      "Gradient Descent(9/49): loss=7989996722144.666\n",
      "Gradient Descent(10/49): loss=325876644737501.3\n",
      "Gradient Descent(11/49): loss=1.3291098943866732e+16\n",
      "Gradient Descent(12/49): loss=5.420868308114572e+17\n",
      "Gradient Descent(13/49): loss=2.2109397988294717e+19\n",
      "Gradient Descent(14/49): loss=9.017476244200227e+20\n",
      "Gradient Descent(15/49): loss=3.6778423142130934e+22\n",
      "Gradient Descent(16/49): loss=1.5000343561206738e+24\n",
      "Gradient Descent(17/49): loss=6.117997677837318e+25\n",
      "Gradient Descent(18/49): loss=2.495269221987314e+27\n",
      "Gradient Descent(19/49): loss=1.017713444685047e+29\n",
      "Gradient Descent(20/49): loss=4.150817260203563e+30\n",
      "Gradient Descent(21/49): loss=1.692940583409107e+32\n",
      "Gradient Descent(22/49): loss=6.904779563403904e+33\n",
      "Gradient Descent(23/49): loss=2.8161638563396167e+35\n",
      "Gradient Descent(24/49): loss=1.1485926223903344e+37\n",
      "Gradient Descent(25/49): loss=4.684617371392729e+38\n",
      "Gradient Descent(26/49): loss=1.9106547864363294e+40\n",
      "Gradient Descent(27/49): loss=7.792742551878401e+41\n",
      "Gradient Descent(28/49): loss=3.178325928417535e+43\n",
      "Gradient Descent(29/49): loss=1.2963030204066332e+45\n",
      "Gradient Descent(30/49): loss=5.287064821423201e+46\n",
      "Gradient Descent(31/49): loss=2.156367298840521e+48\n",
      "Gradient Descent(32/49): loss=8.794898652778529e+49\n",
      "Gradient Descent(33/49): loss=3.587062480229479e+51\n",
      "Gradient Descent(34/49): loss=1.4630091539491477e+53\n",
      "Gradient Descent(35/49): loss=5.9669877409050996e+54\n",
      "Gradient Descent(36/49): loss=2.433678737006008e+56\n",
      "Gradient Descent(37/49): loss=9.925933238228435e+57\n",
      "Gradient Descent(38/49): loss=4.048363046100951e+59\n",
      "Gradient Descent(39/49): loss=1.6511538975413245e+61\n",
      "Gradient Descent(40/49): loss=6.734349568751361e+62\n",
      "Gradient Descent(41/49): loss=2.7466527609372636e+64\n",
      "Gradient Descent(42/49): loss=1.1202420236945045e+66\n",
      "Gradient Descent(43/49): loss=4.568987421704279e+67\n",
      "Gradient Descent(44/49): loss=1.863494282319908e+69\n",
      "Gradient Descent(45/49): loss=7.600395054148906e+70\n",
      "Gradient Descent(46/49): loss=3.0998756222217524e+72\n",
      "Gradient Descent(47/49): loss=1.2643065004889673e+74\n",
      "Gradient Descent(48/49): loss=5.156564720596775e+75\n",
      "Gradient Descent(49/49): loss=2.103141896954553e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.550926934300387\n",
      "Gradient Descent(2/49): loss=45.54410152912448\n",
      "Gradient Descent(3/49): loss=1850.7708816597292\n",
      "Gradient Descent(4/49): loss=76682.13760482696\n",
      "Gradient Descent(5/49): loss=3186865.493723985\n",
      "Gradient Descent(6/49): loss=132548539.32220137\n",
      "Gradient Descent(7/49): loss=5514196830.143454\n",
      "Gradient Descent(8/49): loss=229412523999.52136\n",
      "Gradient Descent(9/49): loss=9544648051293.117\n",
      "Gradient Descent(10/49): loss=397104681205211.9\n",
      "Gradient Descent(11/49): loss=1.652154877669255e+16\n",
      "Gradient Descent(12/49): loss=6.873796941666623e+17\n",
      "Gradient Descent(13/49): loss=2.8598463168215847e+19\n",
      "Gradient Descent(14/49): loss=1.1898403933962484e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=4.950336556588784e+22\n",
      "Gradient Descent(16/49): loss=2.059589860537929e+24\n",
      "Gradient Descent(17/49): loss=8.568933342040884e+25\n",
      "Gradient Descent(18/49): loss=3.5651087649103524e+27\n",
      "Gradient Descent(19/49): loss=1.4832651859046182e+29\n",
      "Gradient Descent(20/49): loss=6.17113181347588e+30\n",
      "Gradient Descent(21/49): loss=2.567502306494509e+32\n",
      "Gradient Descent(22/49): loss=1.0682105476132198e+34\n",
      "Gradient Descent(23/49): loss=4.444295030023459e+35\n",
      "Gradient Descent(24/49): loss=1.8490510469145913e+37\n",
      "Gradient Descent(25/49): loss=7.692985616389244e+38\n",
      "Gradient Descent(26/49): loss=3.2006703001913584e+40\n",
      "Gradient Descent(27/49): loss=1.3316403905269928e+42\n",
      "Gradient Descent(28/49): loss=5.5402961360214605e+43\n",
      "Gradient Descent(29/49): loss=2.3050428248625704e+45\n",
      "Gradient Descent(30/49): loss=9.59014156284048e+46\n",
      "Gradient Descent(31/49): loss=3.98998292800935e+48\n",
      "Gradient Descent(32/49): loss=1.660034282235433e+50\n",
      "Gradient Descent(33/49): loss=6.906580473946407e+51\n",
      "Gradient Descent(34/49): loss=2.8734860691469043e+53\n",
      "Gradient Descent(35/49): loss=1.1955152366252493e+55\n",
      "Gradient Descent(36/49): loss=4.973946790100323e+56\n",
      "Gradient Descent(37/49): loss=2.0694129119246317e+58\n",
      "Gradient Descent(38/49): loss=8.609802196846606e+59\n",
      "Gradient Descent(39/49): loss=3.582112271633706e+61\n",
      "Gradient Descent(40/49): loss=1.490339502954943e+63\n",
      "Gradient Descent(41/49): loss=6.200564543039883e+64\n",
      "Gradient Descent(42/49): loss=2.579747807541373e+66\n",
      "Gradient Descent(43/49): loss=1.0733052941098483e+68\n",
      "Gradient Descent(44/49): loss=4.465491746893399e+69\n",
      "Gradient Descent(45/49): loss=1.857869950982674e+71\n",
      "Gradient Descent(46/49): loss=7.729676708428902e+72\n",
      "Gradient Descent(47/49): loss=3.215935646368911e+74\n",
      "Gradient Descent(48/49): loss=1.3379915450161551e+76\n",
      "Gradient Descent(49/49): loss=5.566720144279193e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5474448677200803\n",
      "Gradient Descent(2/49): loss=45.224432589084685\n",
      "Gradient Descent(3/49): loss=1838.0875348721718\n",
      "Gradient Descent(4/49): loss=76269.6386361973\n",
      "Gradient Descent(5/49): loss=3175762.851285059\n",
      "Gradient Descent(6/49): loss=132356747.58126308\n",
      "Gradient Descent(7/49): loss=5517720725.547522\n",
      "Gradient Descent(8/49): loss=230042022102.21057\n",
      "Gradient Descent(9/49): loss=9591015425850.988\n",
      "Gradient Descent(10/49): loss=399875628098918.75\n",
      "Gradient Descent(11/49): loss=1.6671940275969958e+16\n",
      "Gradient Descent(12/49): loss=6.951005170080998e+17\n",
      "Gradient Descent(13/49): loss=2.8980718768625738e+19\n",
      "Gradient Descent(14/49): loss=1.2082887046229697e+21\n",
      "Gradient Descent(15/49): loss=5.037699814803254e+22\n",
      "Gradient Descent(16/49): loss=2.100360571101309e+24\n",
      "Gradient Descent(17/49): loss=8.75700160113485e+25\n",
      "Gradient Descent(18/49): loss=3.651043450685231e+27\n",
      "Gradient Descent(19/49): loss=1.5222240314066741e+29\n",
      "Gradient Descent(20/49): loss=6.346585662888182e+30\n",
      "Gradient Descent(21/49): loss=2.6460723747447718e+32\n",
      "Gradient Descent(22/49): loss=1.1032229586611538e+34\n",
      "Gradient Descent(23/49): loss=4.599650818831169e+35\n",
      "Gradient Descent(24/49): loss=1.9177254687352583e+37\n",
      "Gradient Descent(25/49): loss=7.995543832109165e+38\n",
      "Gradient Descent(26/49): loss=3.3335700137174085e+40\n",
      "Gradient Descent(27/49): loss=1.3898603109057733e+42\n",
      "Gradient Descent(28/49): loss=5.794723602270966e+43\n",
      "Gradient Descent(29/49): loss=2.4159853593367727e+45\n",
      "Gradient Descent(30/49): loss=1.0072931268442379e+47\n",
      "Gradient Descent(31/49): loss=4.19969201993086e+48\n",
      "Gradient Descent(32/49): loss=1.7509712507944438e+50\n",
      "Gradient Descent(33/49): loss=7.300297989849092e+51\n",
      "Gradient Descent(34/49): loss=3.043702214780174e+53\n",
      "Gradient Descent(35/49): loss=1.2690061673015647e+55\n",
      "Gradient Descent(36/49): loss=5.290848246682739e+56\n",
      "Gradient Descent(37/49): loss=2.2059053683679717e+58\n",
      "Gradient Descent(38/49): loss=9.197047935074584e+59\n",
      "Gradient Descent(39/49): loss=3.834511304654924e+61\n",
      "Gradient Descent(40/49): loss=1.5987170067312632e+63\n",
      "Gradient Descent(41/49): loss=6.665506669673982e+64\n",
      "Gradient Descent(42/49): loss=2.7790396284272915e+66\n",
      "Gradient Descent(43/49): loss=1.1586607949110376e+68\n",
      "Gradient Descent(44/49): loss=4.830786952194787e+69\n",
      "Gradient Descent(45/49): loss=2.0140927077184166e+71\n",
      "Gradient Descent(46/49): loss=8.397326306103148e+72\n",
      "Gradient Descent(47/49): loss=3.5010845737608477e+74\n",
      "Gradient Descent(48/49): loss=1.459701903416254e+76\n",
      "Gradient Descent(49/49): loss=6.085913099060689e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5498718844158508\n",
      "Gradient Descent(2/49): loss=45.01084894381241\n",
      "Gradient Descent(3/49): loss=1812.1821684408876\n",
      "Gradient Descent(4/49): loss=74448.07814411227\n",
      "Gradient Descent(5/49): loss=3068604.146402343\n",
      "Gradient Descent(6/49): loss=126592362.96192832\n",
      "Gradient Descent(7/49): loss=5223764557.110188\n",
      "Gradient Descent(8/49): loss=215571777232.63297\n",
      "Gradient Descent(9/49): loss=8896306893159.566\n",
      "Gradient Descent(10/49): loss=367138936427048.2\n",
      "Gradient Descent(11/49): loss=1.515137256886455e+16\n",
      "Gradient Descent(12/49): loss=6.252788806252049e+17\n",
      "Gradient Descent(13/49): loss=2.580451004590143e+19\n",
      "Gradient Descent(14/49): loss=1.0649212588735034e+21\n",
      "Gradient Descent(15/49): loss=4.39480270233157e+22\n",
      "Gradient Descent(16/49): loss=1.813682534620792e+24\n",
      "Gradient Descent(17/49): loss=7.484851001530233e+25\n",
      "Gradient Descent(18/49): loss=3.088908531018452e+27\n",
      "Gradient Descent(19/49): loss=1.2747556246853683e+29\n",
      "Gradient Descent(20/49): loss=5.26076407380263e+30\n",
      "Gradient Descent(21/49): loss=2.1710544440495476e+32\n",
      "Gradient Descent(22/49): loss=8.959682154373847e+33\n",
      "Gradient Descent(23/49): loss=3.697553717618681e+35\n",
      "Gradient Descent(24/49): loss=1.5259362172803608e+37\n",
      "Gradient Descent(25/49): loss=6.2973563524255656e+38\n",
      "Gradient Descent(26/49): loss=2.5988436856237074e+40\n",
      "Gradient Descent(27/49): loss=1.0725117214789325e+42\n",
      "Gradient Descent(28/49): loss=4.4261276623632174e+43\n",
      "Gradient Descent(29/49): loss=1.82660997462316e+45\n",
      "Gradient Descent(30/49): loss=7.538201005280996e+46\n",
      "Gradient Descent(31/49): loss=3.110925440322444e+48\n",
      "Gradient Descent(32/49): loss=1.283841740020656e+50\n",
      "Gradient Descent(33/49): loss=5.2982613856808737e+51\n",
      "Gradient Descent(34/49): loss=2.186529136414054e+53\n",
      "Gradient Descent(35/49): loss=9.023544359869677e+54\n",
      "Gradient Descent(36/49): loss=3.7239088863948894e+56\n",
      "Gradient Descent(37/49): loss=1.5368126803746387e+58\n",
      "Gradient Descent(38/49): loss=6.342242215401624e+59\n",
      "Gradient Descent(39/49): loss=2.6173675446910393e+61\n",
      "Gradient Descent(40/49): loss=1.0801562966746715e+63\n",
      "Gradient Descent(41/49): loss=4.4576759103340763e+64\n",
      "Gradient Descent(42/49): loss=1.839629559420844e+66\n",
      "Gradient Descent(43/49): loss=7.59193127532973e+67\n",
      "Gradient Descent(44/49): loss=3.1330992804592207e+69\n",
      "Gradient Descent(45/49): loss=1.2929926187704828e+71\n",
      "Gradient Descent(46/49): loss=5.336025968350118e+72\n",
      "Gradient Descent(47/49): loss=2.202114128229307e+74\n",
      "Gradient Descent(48/49): loss=9.087861758001343e+75\n",
      "Gradient Descent(49/49): loss=3.7504519077288856e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5618724325250548\n",
      "Gradient Descent(2/49): loss=45.92965069226913\n",
      "Gradient Descent(3/49): loss=1868.4968572067637\n",
      "Gradient Descent(4/49): loss=77558.69649880087\n",
      "Gradient Descent(5/49): loss=3230403.056826215\n",
      "Gradient Descent(6/49): loss=134675949.17010438\n",
      "Gradient Descent(7/49): loss=5616224122.57989\n",
      "Gradient Descent(8/49): loss=234226197793.6658\n",
      "Gradient Descent(9/49): loss=9768720153470.588\n",
      "Gradient Descent(10/49): loss=407420872749053.75\n",
      "Gradient Descent(11/49): loss=1.6992212141277048e+16\n",
      "Gradient Descent(12/49): loss=7.08690948565179e+17\n",
      "Gradient Descent(13/49): loss=2.9557244737571885e+19\n",
      "Gradient Descent(14/49): loss=1.232738724745973e+21\n",
      "Gradient Descent(15/49): loss=5.141361479091632e+22\n",
      "Gradient Descent(16/49): loss=2.1442985035444194e+24\n",
      "Gradient Descent(17/49): loss=8.94318771392746e+25\n",
      "Gradient Descent(18/49): loss=3.7299194306196963e+27\n",
      "Gradient Descent(19/49): loss=1.5556308786609771e+29\n",
      "Gradient Descent(20/49): loss=6.488042103301114e+30\n",
      "Gradient Descent(21/49): loss=2.7059562080148965e+32\n",
      "Gradient Descent(22/49): loss=1.1285683543895133e+34\n",
      "Gradient Descent(23/49): loss=4.706900011016206e+35\n",
      "Gradient Descent(24/49): loss=1.963098435956008e+37\n",
      "Gradient Descent(25/49): loss=8.187459814812339e+38\n",
      "Gradient Descent(26/49): loss=3.41472933763127e+40\n",
      "Gradient Descent(27/49): loss=1.4241751059570932e+42\n",
      "Gradient Descent(28/49): loss=5.939781844715351e+43\n",
      "Gradient Descent(29/49): loss=2.4772942747865312e+45\n",
      "Gradient Descent(30/49): loss=1.0332007276244054e+47\n",
      "Gradient Descent(31/49): loss=4.3091519422156325e+48\n",
      "Gradient Descent(32/49): loss=1.7972103546418495e+50\n",
      "Gradient Descent(33/49): loss=7.495593337493511e+51\n",
      "Gradient Descent(34/49): loss=3.126173813541919e+53\n",
      "Gradient Descent(35/49): loss=1.303827765520597e+55\n",
      "Gradient Descent(36/49): loss=5.4378513273271424e+56\n",
      "Gradient Descent(37/49): loss=2.2679550044945277e+58\n",
      "Gradient Descent(38/49): loss=9.4589196960263e+59\n",
      "Gradient Descent(39/49): loss=3.945014854287889e+61\n",
      "Gradient Descent(40/49): loss=1.6453403454826031e+63\n",
      "Gradient Descent(41/49): loss=6.86219178498243e+64\n",
      "Gradient Descent(42/49): loss=2.8620021519054365e+66\n",
      "Gradient Descent(43/49): loss=1.1936501593320462e+68\n",
      "Gradient Descent(44/49): loss=4.978335540121225e+69\n",
      "Gradient Descent(45/49): loss=2.0763055704615243e+71\n",
      "Gradient Descent(46/49): loss=8.659610802016319e+72\n",
      "Gradient Descent(47/49): loss=3.6116485121084086e+74\n",
      "Gradient Descent(48/49): loss=1.5063038366548337e+76\n",
      "Gradient Descent(49/49): loss=6.282314684594049e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.577502133271516\n",
      "Gradient Descent(2/49): loss=47.60996189230961\n",
      "Gradient Descent(3/49): loss=1978.90475118927\n",
      "Gradient Descent(4/49): loss=83838.8649199402\n",
      "Gradient Descent(5/49): loss=3562842.269988547\n",
      "Gradient Descent(6/49): loss=151527785.27116114\n",
      "Gradient Descent(7/49): loss=6445923003.27047\n",
      "Gradient Descent(8/49): loss=274224253464.97507\n",
      "Gradient Descent(9/49): loss=11666341261113.234\n",
      "Gradient Descent(10/49): loss=496324626030684.2\n",
      "Gradient Descent(11/49): loss=2.1115319065386984e+16\n",
      "Gradient Descent(12/49): loss=8.983171035330452e+17\n",
      "Gradient Descent(13/49): loss=3.821745347847599e+19\n",
      "Gradient Descent(14/49): loss=1.6259000301951312e+21\n",
      "Gradient Descent(15/49): loss=6.917129993316374e+22\n",
      "Gradient Descent(16/49): loss=2.9427816352959207e+24\n",
      "Gradient Descent(17/49): loss=1.2519590884367518e+26\n",
      "Gradient Descent(18/49): loss=5.326258464914106e+27\n",
      "Gradient Descent(19/49): loss=2.2659709489942485e+29\n",
      "Gradient Descent(20/49): loss=9.640208742450033e+30\n",
      "Gradient Descent(21/49): loss=4.101271670753942e+32\n",
      "Gradient Descent(22/49): loss=1.7448200310531606e+34\n",
      "Gradient Descent(23/49): loss=7.423056030341811e+35\n",
      "Gradient Descent(24/49): loss=3.158019729767973e+37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=1.343528672401123e+39\n",
      "Gradient Descent(26/49): loss=5.715826524290189e+40\n",
      "Gradient Descent(27/49): loss=2.4317064106559972e+42\n",
      "Gradient Descent(28/49): loss=1.0345303592571506e+44\n",
      "Gradient Descent(29/49): loss=4.401242927743098e+45\n",
      "Gradient Descent(30/49): loss=1.872437974939466e+47\n",
      "Gradient Descent(31/49): loss=7.965986035206766e+48\n",
      "Gradient Descent(32/49): loss=3.389000563031235e+50\n",
      "Gradient Descent(33/49): loss=1.4417957507664594e+52\n",
      "Gradient Descent(34/49): loss=6.133887995193748e+53\n",
      "Gradient Descent(35/49): loss=2.609563935639351e+55\n",
      "Gradient Descent(36/49): loss=1.1101969810217302e+57\n",
      "Gradient Descent(37/49): loss=4.7231543931027985e+58\n",
      "Gradient Descent(38/49): loss=2.009390027394573e+60\n",
      "Gradient Descent(39/49): loss=8.548626502849288e+61\n",
      "Gradient Descent(40/49): loss=3.636875573627345e+63\n",
      "Gradient Descent(41/49): loss=1.547250185002082e+65\n",
      "Gradient Descent(42/49): loss=6.582526914995001e+66\n",
      "Gradient Descent(43/49): loss=2.800430144177042e+68\n",
      "Gradient Descent(44/49): loss=1.1913979378573355e+70\n",
      "Gradient Descent(45/49): loss=5.068610796388355e+71\n",
      "Gradient Descent(46/49): loss=2.1563588947843973e+73\n",
      "Gradient Descent(47/49): loss=9.173881897637689e+74\n",
      "Gradient Descent(48/49): loss=3.902880419180821e+76\n",
      "Gradient Descent(49/49): loss=1.6604176657590768e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5738960248051272\n",
      "Gradient Descent(2/49): loss=47.275825521659634\n",
      "Gradient Descent(3/49): loss=1965.3364881079524\n",
      "Gradient Descent(4/49): loss=83387.0463144822\n",
      "Gradient Descent(5/49): loss=3550378.010732425\n",
      "Gradient Descent(6/49): loss=151305748.97757897\n",
      "Gradient Descent(7/49): loss=6449901758.990856\n",
      "Gradient Descent(8/49): loss=274969829943.18115\n",
      "Gradient Descent(9/49): loss=11722685052049.89\n",
      "Gradient Descent(10/49): loss=499772246374966.75\n",
      "Gradient Descent(11/49): loss=2.130679147125435e+16\n",
      "Gradient Descent(12/49): loss=9.083730417809015e+17\n",
      "Gradient Descent(13/49): loss=3.87267003951458e+19\n",
      "Gradient Descent(14/49): loss=1.6510368905129597e+21\n",
      "Gradient Descent(15/49): loss=7.038871962746138e+22\n",
      "Gradient Descent(16/49): loss=3.0008850208265565e+24\n",
      "Gradient Descent(17/49): loss=1.2793684795131164e+26\n",
      "Gradient Descent(18/49): loss=5.454336622930953e+27\n",
      "Gradient Descent(19/49): loss=2.325349457656308e+29\n",
      "Gradient Descent(20/49): loss=9.913671403557445e+30\n",
      "Gradient Descent(21/49): loss=4.2264993923877495e+32\n",
      "Gradient Descent(22/49): loss=1.8018851328377583e+34\n",
      "Gradient Descent(23/49): loss=7.681983907988744e+35\n",
      "Gradient Descent(24/49): loss=3.2750631928282586e+37\n",
      "Gradient Descent(25/49): loss=1.3962589671484405e+39\n",
      "Gradient Descent(26/49): loss=5.952676295259216e+40\n",
      "Gradient Descent(27/49): loss=2.537806804457513e+42\n",
      "Gradient Descent(28/49): loss=1.0819441638175545e+44\n",
      "Gradient Descent(29/49): loss=4.612656769470291e+45\n",
      "Gradient Descent(30/49): loss=1.966515757880458e+47\n",
      "Gradient Descent(31/49): loss=8.383854293230345e+48\n",
      "Gradient Descent(32/49): loss=3.574291867657097e+50\n",
      "Gradient Descent(33/49): loss=1.523829244684687e+52\n",
      "Gradient Descent(34/49): loss=6.496547156565578e+53\n",
      "Gradient Descent(35/49): loss=2.769675480681125e+55\n",
      "Gradient Descent(36/49): loss=1.180796826901131e+57\n",
      "Gradient Descent(37/49): loss=5.03409571318046e+58\n",
      "Gradient Descent(38/49): loss=2.146187986968868e+60\n",
      "Gradient Descent(39/49): loss=9.149851607607694e+61\n",
      "Gradient Descent(40/49): loss=3.900859801171516e+63\n",
      "Gradient Descent(41/49): loss=1.6630550790292493e+65\n",
      "Gradient Descent(42/49): loss=7.090109198629416e+66\n",
      "Gradient Descent(43/49): loss=3.022729017359564e+68\n",
      "Gradient Descent(44/49): loss=1.288681239797241e+70\n",
      "Gradient Descent(45/49): loss=5.494039751059158e+71\n",
      "Gradient Descent(46/49): loss=2.3422761078579393e+73\n",
      "Gradient Descent(47/49): loss=9.985834857464302e+74\n",
      "Gradient Descent(48/49): loss=4.257264865829266e+76\n",
      "Gradient Descent(49/49): loss=1.8150013891203673e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5764411523437134\n",
      "Gradient Descent(2/49): loss=47.053785884462215\n",
      "Gradient Descent(3/49): loss=1937.7216047721784\n",
      "Gradient Descent(4/49): loss=81400.38364064369\n",
      "Gradient Descent(5/49): loss=3430839.0607481105\n",
      "Gradient Descent(6/49): loss=144729317.49534997\n",
      "Gradient Descent(7/49): loss=6106934951.7627\n",
      "Gradient Descent(8/49): loss=257704922413.67102\n",
      "Gradient Descent(9/49): loss=10875064169281.574\n",
      "Gradient Descent(10/49): loss=458927231801578.5\n",
      "Gradient Descent(11/49): loss=1.936674744431924e+16\n",
      "Gradient Descent(12/49): loss=8.172779953484504e+17\n",
      "Gradient Descent(13/49): loss=3.4489190360019825e+19\n",
      "Gradient Descent(14/49): loss=1.4554463975476308e+21\n",
      "Gradient Descent(15/49): loss=6.141994715374984e+22\n",
      "Gradient Descent(16/49): loss=2.591926389271934e+24\n",
      "Gradient Descent(17/49): loss=1.0937948871782162e+26\n",
      "Gradient Descent(18/49): loss=4.615822658643086e+27\n",
      "Gradient Descent(19/49): loss=1.9478806372595085e+29\n",
      "Gradient Descent(20/49): loss=8.220070955381724e+30\n",
      "Gradient Descent(21/49): loss=3.4688761323341446e+32\n",
      "Gradient Descent(22/49): loss=1.4638683396812808e+34\n",
      "Gradient Descent(23/49): loss=6.177535415429823e+35\n",
      "Gradient Descent(24/49): loss=2.6069245965938095e+37\n",
      "Gradient Descent(25/49): loss=1.1001241426073713e+39\n",
      "Gradient Descent(26/49): loss=4.642532165022946e+40\n",
      "Gradient Descent(27/49): loss=1.9591520691647312e+42\n",
      "Gradient Descent(28/49): loss=8.26763648301718e+43\n",
      "Gradient Descent(29/49): loss=3.488948820826282e+45\n",
      "Gradient Descent(30/49): loss=1.472339029340441e+47\n",
      "Gradient Descent(31/49): loss=6.213281789572815e+48\n",
      "Gradient Descent(32/49): loss=2.6220095933971795e+50\n",
      "Gradient Descent(33/49): loss=1.1064900226164485e+52\n",
      "Gradient Descent(34/49): loss=4.669396226592267e+53\n",
      "Gradient Descent(35/49): loss=1.9704887233738673e+55\n",
      "Gradient Descent(36/49): loss=8.31547724913739e+56\n",
      "Gradient Descent(37/49): loss=3.5091376601500037e+58\n",
      "Gradient Descent(38/49): loss=1.4808587347359347e+60\n",
      "Gradient Descent(39/49): loss=6.24923501048962e+61\n",
      "Gradient Descent(40/49): loss=2.637181879694501e+63\n",
      "Gradient Descent(41/49): loss=1.1128927388576672e+65\n",
      "Gradient Descent(42/49): loss=4.696415737338491e+66\n",
      "Gradient Descent(43/49): loss=1.9818909772527293e+68\n",
      "Gradient Descent(44/49): loss=8.363594846357669e+69\n",
      "Gradient Descent(45/49): loss=3.529443322406375e+71\n",
      "Gradient Descent(46/49): loss=1.4894277394969664e+73\n",
      "Gradient Descent(47/49): loss=6.28539627510024e+74\n",
      "Gradient Descent(48/49): loss=2.6524419605872604e+76\n",
      "Gradient Descent(49/49): loss=1.1193325044842624e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.588387553502782\n",
      "Gradient Descent(2/49): loss=47.99262503275241\n",
      "Gradient Descent(3/49): loss=1996.5679606861765\n",
      "Gradient Descent(4/49): loss=84723.92812597101\n",
      "Gradient Descent(5/49): loss=3607606.873504639\n",
      "Gradient Descent(6/49): loss=153759694.58381954\n",
      "Gradient Descent(7/49): loss=6555232208.168474\n",
      "Gradient Descent(8/49): loss=279492886707.71564\n",
      "Gradient Descent(9/49): loss=11916940319009.895\n",
      "Gradient Descent(10/49): loss=508115255416082.1\n",
      "Gradient Descent(11/49): loss=2.1665103327268804e+16\n",
      "Gradient Descent(12/49): loss=9.237609936869821e+17\n",
      "Gradient Descent(13/49): loss=3.938751438423682e+19\n",
      "Gradient Descent(14/49): loss=1.6794131910989078e+21\n",
      "Gradient Descent(15/49): loss=7.160717732876809e+22\n",
      "Gradient Descent(16/49): loss=3.0532020981637327e+24\n",
      "Gradient Descent(17/49): loss=1.301830824615133e+26\n",
      "Gradient Descent(18/49): loss=5.55077404795464e+27\n",
      "Gradient Descent(19/49): loss=2.3667508830407266e+29\n",
      "Gradient Descent(20/49): loss=1.0091402918413975e+31\n",
      "Gradient Descent(21/49): loss=4.3027939101366446e+32\n",
      "Gradient Descent(22/49): loss=1.8346344490345034e+34\n",
      "Gradient Descent(23/49): loss=7.822553512638143e+35\n",
      "Gradient Descent(24/49): loss=3.3353970590883377e+37\n",
      "Gradient Descent(25/49): loss=1.4221537153825223e+39\n",
      "Gradient Descent(26/49): loss=6.063809358665164e+40\n",
      "Gradient Descent(27/49): loss=2.5854999737736145e+42\n",
      "Gradient Descent(28/49): loss=1.1024109959576481e+44\n",
      "Gradient Descent(29/49): loss=4.700483528663741e+45\n",
      "Gradient Descent(30/49): loss=2.0042021972073958e+47\n",
      "Gradient Descent(31/49): loss=8.545560095671417e+48\n",
      "Gradient Descent(32/49): loss=3.6436741487702477e+50\n",
      "Gradient Descent(33/49): loss=1.5535975587067118e+52\n",
      "Gradient Descent(34/49): loss=6.624262422681483e+53\n",
      "Gradient Descent(35/49): loss=2.824467147147056e+55\n",
      "Gradient Descent(36/49): loss=1.2043023292672794e+57\n",
      "Gradient Descent(37/49): loss=5.134929969865515e+58\n",
      "Gradient Descent(38/49): loss=2.189442397862477e+60\n",
      "Gradient Descent(39/49): loss=9.335391216023398e+61\n",
      "Gradient Descent(40/49): loss=3.980444027268726e+63\n",
      "Gradient Descent(41/49): loss=1.6971902181265418e+65\n",
      "Gradient Descent(42/49): loss=7.236515868007074e+66\n",
      "Gradient Descent(43/49): loss=3.085521077638805e+68\n",
      "Gradient Descent(44/49): loss=1.3156110612074494e+70\n",
      "Gradient Descent(45/49): loss=5.609530516303903e+71\n",
      "Gradient Descent(46/49): loss=2.3918035915922686e+73\n",
      "Gradient Descent(47/49): loss=1.019822319198833e+75\n",
      "Gradient Descent(48/49): loss=4.34834016635835e+76\n",
      "Gradient Descent(49/49): loss=1.8540545589568513e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6043921788744082\n",
      "Gradient Descent(2/49): loss=49.74594417719975\n",
      "Gradient Descent(3/49): loss=2114.349602213323\n",
      "Gradient Descent(4/49): loss=91573.37073300187\n",
      "Gradient Descent(5/49): loss=3978279.684666866\n",
      "Gradient Descent(6/49): loss=172969150.5060368\n",
      "Gradient Descent(7/49): loss=7522118852.60389\n",
      "Gradient Descent(8/49): loss=327144789813.95337\n",
      "Gradient Descent(9/49): loss=14228136230364.172\n",
      "Gradient Descent(10/49): loss=618811531907845.8\n",
      "Gradient Descent(11/49): loss=2.691345616330585e+16\n",
      "Gradient Descent(12/49): loss=1.1705251434696128e+18\n",
      "Gradient Descent(13/49): loss=5.090870852980842e+19\n",
      "Gradient Descent(14/49): loss=2.214131596988264e+21\n",
      "Gradient Descent(15/49): loss=9.629744941288782e+22\n",
      "Gradient Descent(16/49): loss=4.1881877330915866e+24\n",
      "Gradient Descent(17/49): loss=1.8215349016348298e+26\n",
      "Gradient Descent(18/49): loss=7.922255663422513e+27\n",
      "Gradient Descent(19/49): loss=3.44556312071598e+29\n",
      "Gradient Descent(20/49): loss=1.4985511353704402e+31\n",
      "Gradient Descent(21/49): loss=6.517528272326934e+32\n",
      "Gradient Descent(22/49): loss=2.834616302243473e+34\n",
      "Gradient Descent(23/49): loss=1.2328369352942425e+36\n",
      "Gradient Descent(24/49): loss=5.361878811686156e+37\n",
      "Gradient Descent(25/49): loss=2.3319989504004287e+39\n",
      "Gradient Descent(26/49): loss=1.0142376013452998e+41\n",
      "Gradient Descent(27/49): loss=4.411142259759768e+42\n",
      "Gradient Descent(28/49): loss=1.918502726582881e+44\n",
      "Gradient Descent(29/49): loss=8.343990048750845e+45\n",
      "Gradient Descent(30/49): loss=3.62898467481772e+47\n",
      "Gradient Descent(31/49): loss=1.5783252009071364e+49\n",
      "Gradient Descent(32/49): loss=6.864483217867793e+50\n",
      "Gradient Descent(33/49): loss=2.985514634202502e+52\n",
      "Gradient Descent(34/49): loss=1.2984659366398604e+54\n",
      "Gradient Descent(35/49): loss=5.647313763927969e+55\n",
      "Gradient Descent(36/49): loss=2.4561408850493305e+57\n",
      "Gradient Descent(37/49): loss=1.0682296573893515e+59\n",
      "Gradient Descent(38/49): loss=4.645965579060216e+60\n",
      "Gradient Descent(39/49): loss=2.0206325496114685e+62\n",
      "Gradient Descent(40/49): loss=8.788175097447166e+63\n",
      "Gradient Descent(41/49): loss=3.822170515774413e+65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=1.6623459694036848e+67\n",
      "Gradient Descent(43/49): loss=7.229908007996866e+68\n",
      "Gradient Descent(44/49): loss=3.144445907541616e+70\n",
      "Gradient Descent(45/49): loss=1.367588640757085e+72\n",
      "Gradient Descent(46/49): loss=5.947943597446214e+73\n",
      "Gradient Descent(47/49): loss=2.586891407551947e+75\n",
      "Gradient Descent(48/49): loss=1.1250959335490879e+77\n",
      "Gradient Descent(49/49): loss=4.893289513403278e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6006602932255063\n",
      "Gradient Descent(2/49): loss=49.396858623823846\n",
      "Gradient Descent(3/49): loss=2099.8456298369215\n",
      "Gradient Descent(4/49): loss=91078.99445675632\n",
      "Gradient Descent(5/49): loss=3964305.572981221\n",
      "Gradient Descent(6/49): loss=172712590.26115003\n",
      "Gradient Descent(7/49): loss=7526601474.51006\n",
      "Gradient Descent(8/49): loss=328026218885.1657\n",
      "Gradient Descent(9/49): loss=14296458074793.412\n",
      "Gradient Descent(10/49): loss=623090874108454.8\n",
      "Gradient Descent(11/49): loss=2.7156589437706e+16\n",
      "Gradient Descent(12/49): loss=1.1835846644176172e+18\n",
      "Gradient Descent(13/49): loss=5.158500909303044e+19\n",
      "Gradient Descent(14/49): loss=2.2482661248176237e+21\n",
      "Gradient Descent(15/49): loss=9.7987782455027e+22\n",
      "Gradient Descent(16/49): loss=4.2706712742105863e+24\n",
      "Gradient Descent(17/49): loss=1.8613170643401842e+26\n",
      "Gradient Descent(18/49): loss=8.112310672026907e+27\n",
      "Gradient Descent(19/49): loss=3.5356461136272e+29\n",
      "Gradient Descent(20/49): loss=1.540965816850589e+31\n",
      "Gradient Descent(21/49): loss=6.716101024861191e+32\n",
      "Gradient Descent(22/49): loss=2.9271261233084514e+34\n",
      "Gradient Descent(23/49): loss=1.2757502172832513e+36\n",
      "Gradient Descent(24/49): loss=5.560192995917691e+37\n",
      "Gradient Descent(25/49): loss=2.4233384978519238e+39\n",
      "Gradient Descent(26/49): loss=1.0561808698876172e+41\n",
      "Gradient Descent(27/49): loss=4.60322827745845e+42\n",
      "Gradient Descent(28/49): loss=2.0062577517284284e+44\n",
      "Gradient Descent(29/49): loss=8.74401598999732e+45\n",
      "Gradient Descent(30/49): loss=3.8109667398148974e+47\n",
      "Gradient Descent(31/49): loss=1.6609607654639976e+49\n",
      "Gradient Descent(32/49): loss=7.239083552182231e+50\n",
      "Gradient Descent(33/49): loss=3.155061321441608e+52\n",
      "Gradient Descent(34/49): loss=1.3750928374153294e+54\n",
      "Gradient Descent(35/49): loss=5.993165009696106e+55\n",
      "Gradient Descent(36/49): loss=2.6120437730559574e+57\n",
      "Gradient Descent(37/49): loss=1.1384256334210983e+59\n",
      "Gradient Descent(38/49): loss=4.9616814855822914e+60\n",
      "Gradient Descent(39/49): loss=2.1624849653454565e+62\n",
      "Gradient Descent(40/49): loss=9.424912177320733e+63\n",
      "Gradient Descent(41/49): loss=4.10772657261082e+65\n",
      "Gradient Descent(42/49): loss=1.7902997161008684e+67\n",
      "Gradient Descent(43/49): loss=7.802790708714809e+68\n",
      "Gradient Descent(44/49): loss=3.4007458246491473e+70\n",
      "Gradient Descent(45/49): loss=1.4821712635393212e+72\n",
      "Gradient Descent(46/49): loss=6.45985253746032e+73\n",
      "Gradient Descent(47/49): loss=2.815443520749756e+75\n",
      "Gradient Descent(48/49): loss=1.2270747935134987e+77\n",
      "Gradient Descent(49/49): loss=5.348047431174247e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6033253498294835\n",
      "Gradient Descent(2/49): loss=49.16611727080597\n",
      "Gradient Descent(3/49): loss=2070.428422207138\n",
      "Gradient Descent(4/49): loss=88914.2914778893\n",
      "Gradient Descent(5/49): loss=3831114.191945004\n",
      "Gradient Descent(6/49): loss=165220478.5344383\n",
      "Gradient Descent(7/49): loss=7127125633.154507\n",
      "Gradient Descent(8/49): loss=307466644700.04865\n",
      "Gradient Descent(9/49): loss=13264517088192.725\n",
      "Gradient Descent(10/49): loss=572252653968109.0\n",
      "Gradient Descent(11/49): loss=2.4687952025896852e+16\n",
      "Gradient Descent(12/49): loss=1.0650808501421661e+18\n",
      "Gradient Descent(13/49): loss=4.594943399248285e+19\n",
      "Gradient Descent(14/49): loss=1.9823383340482356e+21\n",
      "Gradient Descent(15/49): loss=8.552151773245709e+22\n",
      "Gradient Descent(16/49): loss=3.6895467866823394e+24\n",
      "Gradient Descent(17/49): loss=1.5917345565954206e+26\n",
      "Gradient Descent(18/49): loss=6.867019301384199e+27\n",
      "Gradient Descent(19/49): loss=2.962551381212309e+29\n",
      "Gradient Descent(20/49): loss=1.2780961144075182e+31\n",
      "Gradient Descent(21/49): loss=5.513928595607745e+32\n",
      "Gradient Descent(22/49): loss=2.3788045527123205e+34\n",
      "Gradient Descent(23/49): loss=1.0262575950871511e+36\n",
      "Gradient Descent(24/49): loss=4.427453488238545e+37\n",
      "Gradient Descent(25/49): loss=1.9100803233376062e+39\n",
      "Gradient Descent(26/49): loss=8.24041822526995e+40\n",
      "Gradient Descent(27/49): loss=3.555059528004954e+42\n",
      "Gradient Descent(28/49): loss=1.533714418632536e+44\n",
      "Gradient Descent(29/49): loss=6.61671035151809e+45\n",
      "Gradient Descent(30/49): loss=2.854563753460832e+47\n",
      "Gradient Descent(31/49): loss=1.231508376470318e+49\n",
      "Gradient Descent(32/49): loss=5.312941003604603e+50\n",
      "Gradient Descent(33/49): loss=2.2920950151136518e+52\n",
      "Gradient Descent(34/49): loss=9.888495947431776e+53\n",
      "Gradient Descent(35/49): loss=4.2660688783674513e+55\n",
      "Gradient Descent(36/49): loss=1.840456199984779e+57\n",
      "Gradient Descent(37/49): loss=7.940047665987695e+58\n",
      "Gradient Descent(38/49): loss=3.4254744524036104e+60\n",
      "Gradient Descent(39/49): loss=1.4778091666040685e+62\n",
      "Gradient Descent(40/49): loss=6.3755253855902296e+63\n",
      "Gradient Descent(41/49): loss=2.750512370667657e+65\n",
      "Gradient Descent(42/49): loss=1.1866188029452061e+67\n",
      "Gradient Descent(43/49): loss=5.119279587756641e+68\n",
      "Gradient Descent(44/49): loss=2.2085461171334567e+70\n",
      "Gradient Descent(45/49): loss=9.528051492188107e+71\n",
      "Gradient Descent(46/49): loss=4.110566880786637e+73\n",
      "Gradient Descent(47/49): loss=1.7733699377331719e+75\n",
      "Gradient Descent(48/49): loss=7.650625880228775e+76\n",
      "Gradient Descent(49/49): loss=3.3006128678399425e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.615213242273395\n",
      "Gradient Descent(2/49): loss=50.12494371784768\n",
      "Gradient Descent(3/49): loss=2131.874432434632\n",
      "Gradient Descent(4/49): loss=92461.83386783159\n",
      "Gradient Descent(5/49): loss=4023998.643237257\n",
      "Gradient Descent(6/49): loss=175293717.7545002\n",
      "Gradient Descent(7/49): loss=7638328833.525751\n",
      "Gradient Descent(8/49): loss=332864825262.2325\n",
      "Gradient Descent(9/49): loss=14506042554659.68\n",
      "Gradient Descent(10/49): loss=632169461629846.5\n",
      "Gradient Descent(11/49): loss=2.7549844927341028e+16\n",
      "Gradient Descent(12/49): loss=1.2006187920292431e+18\n",
      "Gradient Descent(13/49): loss=5.23228288817816e+19\n",
      "Gradient Descent(14/49): loss=2.2802230295652312e+21\n",
      "Gradient Descent(15/49): loss=9.937186674756863e+22\n",
      "Gradient Descent(16/49): loss=4.330614961728694e+24\n",
      "Gradient Descent(17/49): loss=1.8872772143312443e+26\n",
      "Gradient Descent(18/49): loss=8.224733248024713e+27\n",
      "Gradient Descent(19/49): loss=3.58432966290112e+29\n",
      "Gradient Descent(20/49): loss=1.5620469072614625e+31\n",
      "Gradient Descent(21/49): loss=6.807383165072326e+32\n",
      "Gradient Descent(22/49): loss=2.9666500628727515e+34\n",
      "Gradient Descent(23/49): loss=1.2928628199915203e+36\n",
      "Gradient Descent(24/49): loss=5.634281886616356e+37\n",
      "Gradient Descent(25/49): loss=2.4554138217128498e+39\n",
      "Gradient Descent(26/49): loss=1.0700666308833763e+41\n",
      "Gradient Descent(27/49): loss=4.663338555825835e+42\n",
      "Gradient Descent(28/49): loss=2.0322777908044236e+44\n",
      "Gradient Descent(29/49): loss=8.856644160731465e+45\n",
      "Gradient Descent(30/49): loss=3.859715740866848e+47\n",
      "Gradient Descent(31/49): loss=1.6820598558478097e+49\n",
      "Gradient Descent(32/49): loss=7.330398269224086e+50\n",
      "Gradient Descent(33/49): loss=3.1945794674684637e+52\n",
      "Gradient Descent(34/49): loss=1.392194202710259e+54\n",
      "Gradient Descent(35/49): loss=6.067166955142266e+55\n",
      "Gradient Descent(36/49): loss=2.644064656346724e+57\n",
      "Gradient Descent(37/49): loss=1.1522804562047829e+59\n",
      "Gradient Descent(38/49): loss=5.021625498319113e+60\n",
      "Gradient Descent(39/49): loss=2.188418844525406e+62\n",
      "Gradient Descent(40/49): loss=9.537105147878861e+63\n",
      "Gradient Descent(41/49): loss=4.1562598873262224e+65\n",
      "Gradient Descent(42/49): loss=1.811293467267591e+67\n",
      "Gradient Descent(43/49): loss=7.893596920082883e+68\n",
      "Gradient Descent(44/49): loss=3.440020817318874e+70\n",
      "Gradient Descent(45/49): loss=1.4991572718236732e+72\n",
      "Gradient Descent(46/49): loss=6.533310828663729e+73\n",
      "Gradient Descent(47/49): loss=2.847209641454821e+75\n",
      "Gradient Descent(48/49): loss=1.2408108162904252e+77\n",
      "Gradient Descent(49/49): loss=5.407439829532957e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6315970711090642\n",
      "Gradient Descent(2/49): loss=51.953618380449456\n",
      "Gradient Descent(3/49): loss=2257.4354461118432\n",
      "Gradient Descent(4/49): loss=99925.18581225976\n",
      "Gradient Descent(5/49): loss=4436817.184615194\n",
      "Gradient Descent(6/49): loss=197159659.84492517\n",
      "Gradient Descent(7/49): loss=8763221944.382265\n",
      "Gradient Descent(8/49): loss=389527571329.8043\n",
      "Gradient Descent(9/49): loss=17314936767639.988\n",
      "Gradient Descent(10/49): loss=769672616839221.0\n",
      "Gradient Descent(11/49): loss=3.4213055798745004e+16\n",
      "Gradient Descent(12/49): loss=1.5208202998385134e+18\n",
      "Gradient Descent(13/49): loss=6.760269284848658e+19\n",
      "Gradient Descent(14/49): loss=3.005038967441158e+21\n",
      "Gradient Descent(15/49): loss=1.3357839637361114e+23\n",
      "Gradient Descent(16/49): loss=5.937755959818136e+24\n",
      "Gradient Descent(17/49): loss=2.6394197587655143e+26\n",
      "Gradient Descent(18/49): loss=1.173260860182742e+28\n",
      "Gradient Descent(19/49): loss=5.2153168952415496e+29\n",
      "Gradient Descent(20/49): loss=2.318284981757713e+31\n",
      "Gradient Descent(21/49): loss=1.0305117339246064e+33\n",
      "Gradient Descent(22/49): loss=4.580776056933873e+34\n",
      "Gradient Descent(23/49): loss=2.03622225667218e+36\n",
      "Gradient Descent(24/49): loss=9.05130708647464e+37\n",
      "Gradient Descent(25/49): loss=4.0234389789827054e+39\n",
      "Gradient Descent(26/49): loss=1.788477737296913e+41\n",
      "Gradient Descent(27/49): loss=7.950046299982527e+42\n",
      "Gradient Descent(28/49): loss=3.533912379999256e+44\n",
      "Gradient Descent(29/49): loss=1.5708759720732e+46\n",
      "Gradient Descent(30/49): loss=6.982774484175011e+47\n",
      "Gradient Descent(31/49): loss=3.103945847010118e+49\n",
      "Gradient Descent(32/49): loss=1.3797495312222872e+51\n",
      "Gradient Descent(33/49): loss=6.133189374878635e+52\n",
      "Gradient Descent(34/49): loss=2.7262927840824214e+54\n",
      "Gradient Descent(35/49): loss=1.2118771963872287e+56\n",
      "Gradient Descent(36/49): loss=5.386972183245065e+57\n",
      "Gradient Descent(37/49): loss=2.3945882792057727e+59\n",
      "Gradient Descent(38/49): loss=1.0644296706680732e+61\n",
      "Gradient Descent(39/49): loss=4.731546268882321e+62\n",
      "Gradient Descent(40/49): loss=2.1032418309537686e+64\n",
      "Gradient Descent(41/49): loss=9.349218940468506e+65\n",
      "Gradient Descent(42/49): loss=4.155865174913217e+67\n",
      "Gradient Descent(43/49): loss=1.8473431269533423e+69\n",
      "Gradient Descent(44/49): loss=8.211711605328993e+70\n",
      "Gradient Descent(45/49): loss=3.650226452532662e+72\n",
      "Gradient Descent(46/49): loss=1.6225792861651936e+74\n",
      "Gradient Descent(47/49): loss=7.212603311407249e+75\n",
      "Gradient Descent(48/49): loss=3.206108137289932e+77\n",
      "Gradient Descent(49/49): loss=1.4251621701888834e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6277376729812179\n",
      "Gradient Descent(2/49): loss=51.58909132584146\n",
      "Gradient Descent(3/49): loss=2241.9426683011475\n",
      "Gradient Descent(4/49): loss=99384.78811744996\n",
      "Gradient Descent(5/49): loss=4421170.831670508\n",
      "Gradient Descent(6/49): loss=196863756.47749493\n",
      "Gradient Descent(7/49): loss=8768261364.198214\n",
      "Gradient Descent(8/49): loss=390567724794.8375\n",
      "Gradient Descent(9/49): loss=17397611537213.986\n",
      "Gradient Descent(10/49): loss=774971975718177.5\n",
      "Gradient Descent(11/49): loss=3.452099338726102e+16\n",
      "Gradient Descent(12/49): loss=1.537732679842384e+18\n",
      "Gradient Descent(13/49): loss=6.849809359552317e+19\n",
      "Gradient Descent(14/49): loss=3.051238455164145e+21\n",
      "Gradient Descent(15/49): loss=1.3591701279639525e+23\n",
      "Gradient Descent(16/49): loss=6.054405360135958e+24\n",
      "Gradient Descent(17/49): loss=2.6969268647128005e+26\n",
      "Gradient Descent(18/49): loss=1.2013425074982693e+28\n",
      "Gradient Descent(19/49): loss=5.3513643228196535e+29\n",
      "Gradient Descent(20/49): loss=2.383758165377541e+31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=1.0618419244673338e+33\n",
      "Gradient Descent(22/49): loss=4.72996081958557e+34\n",
      "Gradient Descent(23/49): loss=2.106954796125613e+36\n",
      "Gradient Descent(24/49): loss=9.38540229453108e+37\n",
      "Gradient Descent(25/49): loss=4.1807150486652205e+39\n",
      "Gradient Descent(26/49): loss=1.8622939933348434e+41\n",
      "Gradient Descent(27/49): loss=8.295563981856042e+42\n",
      "Gradient Descent(28/49): loss=3.695248012578094e+44\n",
      "Gradient Descent(29/49): loss=1.646043343686837e+46\n",
      "Gradient Descent(30/49): loss=7.332278320895189e+47\n",
      "Gradient Descent(31/49): loss=3.266153687949177e+49\n",
      "Gradient Descent(32/49): loss=1.4549038438575765e+51\n",
      "Gradient Descent(33/49): loss=6.480849944941371e+52\n",
      "Gradient Descent(34/49): loss=2.886886043099808e+54\n",
      "Gradient Descent(35/49): loss=1.2859595726868588e+56\n",
      "Gradient Descent(36/49): loss=5.728289921722373e+57\n",
      "Gradient Descent(37/49): loss=2.551659175315013e+59\n",
      "Gradient Descent(38/49): loss=1.1366332074567176e+61\n",
      "Gradient Descent(39/49): loss=5.063117601251952e+62\n",
      "Gradient Descent(40/49): loss=2.25535904423095e+64\n",
      "Gradient Descent(41/49): loss=1.00464670564566e+66\n",
      "Gradient Descent(42/49): loss=4.475185473224042e+67\n",
      "Gradient Descent(43/49): loss=1.993465454792332e+69\n",
      "Gradient Descent(44/49): loss=8.879865523400317e+70\n",
      "Gradient Descent(45/49): loss=3.955524362065647e+72\n",
      "Gradient Descent(46/49): loss=1.761983099593563e+74\n",
      "Gradient Descent(47/49): loss=7.848730431360723e+75\n",
      "Gradient Descent(48/49): loss=3.496206598030227e+77\n",
      "Gradient Descent(49/49): loss=1.5573806086229584e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6305244768731615\n",
      "Gradient Descent(2/49): loss=51.34939795280282\n",
      "Gradient Descent(3/49): loss=2210.626350159497\n",
      "Gradient Descent(4/49): loss=97028.24791298379\n",
      "Gradient Descent(5/49): loss=4272939.978460448\n",
      "Gradient Descent(6/49): loss=188340447.16385183\n",
      "Gradient Descent(7/49): loss=8303731453.488124\n",
      "Gradient Descent(8/49): loss=366130964926.3054\n",
      "Gradient Descent(9/49): loss=16143942837804.523\n",
      "Gradient Descent(10/49): loss=711845509003371.9\n",
      "Gradient Descent(11/49): loss=3.138793772155157e+16\n",
      "Gradient Descent(12/49): loss=1.3840127243767345e+18\n",
      "Gradient Descent(13/49): loss=6.102635962533889e+19\n",
      "Gradient Descent(14/49): loss=2.6908833349363717e+21\n",
      "Gradient Descent(15/49): loss=1.1865124000244744e+23\n",
      "Gradient Descent(16/49): loss=5.231782679437137e+24\n",
      "Gradient Descent(17/49): loss=2.306891191787322e+26\n",
      "Gradient Descent(18/49): loss=1.0171957245161288e+28\n",
      "Gradient Descent(19/49): loss=4.485201321136416e+29\n",
      "Gradient Descent(20/49): loss=1.9776951875706346e+31\n",
      "Gradient Descent(21/49): loss=8.720407346165698e+32\n",
      "Gradient Descent(22/49): loss=3.8451579778958377e+34\n",
      "Gradient Descent(23/49): loss=1.69547582906123e+36\n",
      "Gradient Descent(24/49): loss=7.475995273684103e+37\n",
      "Gradient Descent(25/49): loss=3.2964495496876874e+39\n",
      "Gradient Descent(26/49): loss=1.4535294948469836e+41\n",
      "Gradient Descent(27/49): loss=6.40916222300535e+42\n",
      "Gradient Descent(28/49): loss=2.8260424398971887e+44\n",
      "Gradient Descent(29/49): loss=1.2461091784247305e+46\n",
      "Gradient Descent(30/49): loss=5.49456746520361e+47\n",
      "Gradient Descent(31/49): loss=2.422762961094551e+49\n",
      "Gradient Descent(32/49): loss=1.0682879776841768e+51\n",
      "Gradient Descent(33/49): loss=4.7104864222827844e+52\n",
      "Gradient Descent(34/49): loss=2.077031923789944e+54\n",
      "Gradient Descent(35/49): loss=9.158420650646716e+55\n",
      "Gradient Descent(36/49): loss=4.038294638300186e+57\n",
      "Gradient Descent(37/49): loss=1.7806370997571836e+59\n",
      "Gradient Descent(38/49): loss=7.85150358015063e+60\n",
      "Gradient Descent(39/49): loss=3.4620253884143056e+62\n",
      "Gradient Descent(40/49): loss=1.5265381551026894e+64\n",
      "Gradient Descent(41/49): loss=6.731085065934964e+65\n",
      "Gradient Descent(42/49): loss=2.967990417625995e+67\n",
      "Gradient Descent(43/49): loss=1.3086994196077861e+69\n",
      "Gradient Descent(44/49): loss=5.770551551347963e+70\n",
      "Gradient Descent(45/49): loss=2.5444548005334902e+72\n",
      "Gradient Descent(46/49): loss=1.1219465200766821e+74\n",
      "Gradient Descent(47/49): loss=4.947087264620531e+75\n",
      "Gradient Descent(48/49): loss=2.1813581989717294e+77\n",
      "Gradient Descent(49/49): loss=9.618434722691608e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.642349498836895\n",
      "Gradient Descent(2/49): loss=52.32814441812307\n",
      "Gradient Descent(3/49): loss=2274.7396881409054\n",
      "Gradient Descent(4/49): loss=100811.20747061836\n",
      "Gradient Descent(5/49): loss=4483156.42492001\n",
      "Gradient Descent(6/49): loss=199560765.7405369\n",
      "Gradient Descent(7/49): loss=8885688614.187897\n",
      "Gradient Descent(8/49): loss=395680835413.909\n",
      "Gradient Descent(9/49): loss=17620190219561.688\n",
      "Gradient Descent(10/49): loss=784656811683649.4\n",
      "Gradient Descent(11/49): loss=3.4942180839126252e+16\n",
      "Gradient Descent(12/49): loss=1.5560394505476396e+18\n",
      "Gradient Descent(13/49): loss=6.929330953583508e+19\n",
      "Gradient Descent(14/49): loss=3.085759232721043e+21\n",
      "Gradient Descent(15/49): loss=1.3741456891686837e+23\n",
      "Gradient Descent(16/49): loss=6.119325086159838e+24\n",
      "Gradient Descent(17/49): loss=2.7250487256068247e+26\n",
      "Gradient Descent(18/49): loss=1.2135146372012446e+28\n",
      "Gradient Descent(19/49): loss=5.404005297827261e+29\n",
      "Gradient Descent(20/49): loss=2.406503585991196e+31\n",
      "Gradient Descent(21/49): loss=1.0716605906786634e+33\n",
      "Gradient Descent(22/49): loss=4.772302972270551e+34\n",
      "Gradient Descent(23/49): loss=2.1251948478166163e+36\n",
      "Gradient Descent(24/49): loss=9.46388602616318e+37\n",
      "Gradient Descent(25/49): loss=4.214443621874941e+39\n",
      "Gradient Descent(26/49): loss=1.8767697532346646e+41\n",
      "Gradient Descent(27/49): loss=8.357603097059708e+42\n",
      "Gradient Descent(28/49): loss=3.72179535649459e+44\n",
      "Gradient Descent(29/49): loss=1.6573843618510506e+46\n",
      "Gradient Descent(30/49): loss=7.380639341480711e+47\n",
      "Gradient Descent(31/49): loss=3.286735312753506e+49\n",
      "Gradient Descent(32/49): loss=1.463644071508531e+51\n",
      "Gradient Descent(33/49): loss=6.51787796768875e+52\n",
      "Gradient Descent(34/49): loss=2.9025317034828658e+54\n",
      "Gradient Descent(35/49): loss=1.2925510927769908e+56\n",
      "Gradient Descent(36/49): loss=5.755969264467522e+57\n",
      "Gradient Descent(37/49): loss=2.5632396551778613e+59\n",
      "Gradient Descent(38/49): loss=1.1414580634464429e+61\n",
      "Gradient Descent(39/49): loss=5.08312403787502e+62\n",
      "Gradient Descent(40/49): loss=2.2636092215607809e+64\n",
      "Gradient Descent(41/49): loss=1.0080270852640911e+66\n",
      "Gradient Descent(42/49): loss=4.488931194251797e+67\n",
      "Gradient Descent(43/49): loss=1.9990041499180214e+69\n",
      "Gradient Descent(44/49): loss=8.901935490805651e+70\n",
      "Gradient Descent(45/49): loss=3.964196646901152e+72\n",
      "Gradient Descent(46/49): loss=1.7653301432630482e+74\n",
      "Gradient Descent(47/49): loss=7.86134188663229e+75\n",
      "Gradient Descent(48/49): loss=3.500801053806665e+77\n",
      "Gradient Descent(49/49): loss=1.558971508308748e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6591168099754838\n",
      "Gradient Descent(2/49): loss=54.234571906817344\n",
      "Gradient Descent(3/49): loss=2408.503344096376\n",
      "Gradient Descent(4/49): loss=108936.06965811003\n",
      "Gradient Descent(5/49): loss=4942384.613363423\n",
      "Gradient Descent(6/49): loss=224416109.70413792\n",
      "Gradient Descent(7/49): loss=10192289698.68997\n",
      "Gradient Descent(8/49): loss=462933371693.1998\n",
      "Gradient Descent(9/49): loss=21026822938071.207\n",
      "Gradient Descent(10/49): loss=955061386984850.4\n",
      "Gradient Descent(11/49): loss=4.338000866544548e+16\n",
      "Gradient Descent(12/49): loss=1.9703718377738972e+18\n",
      "Gradient Descent(13/49): loss=8.949665631084131e+19\n",
      "Gradient Descent(14/49): loss=4.0650458288569e+21\n",
      "Gradient Descent(15/49): loss=1.8463927559338414e+23\n",
      "Gradient Descent(16/49): loss=8.38653822036852e+24\n",
      "Gradient Descent(17/49): loss=3.809266645339503e+26\n",
      "Gradient Descent(18/49): loss=1.7302147797281136e+28\n",
      "Gradient Descent(19/49): loss=7.858843874617365e+29\n",
      "Gradient Descent(20/49): loss=3.5695815207919335e+31\n",
      "Gradient Descent(21/49): loss=1.621346910176981e+33\n",
      "Gradient Descent(22/49): loss=7.364352901969721e+34\n",
      "Gradient Descent(23/49): loss=3.344977766594333e+36\n",
      "Gradient Descent(24/49): loss=1.519329180438902e+38\n",
      "Gradient Descent(25/49): loss=6.900976088948775e+39\n",
      "Gradient Descent(26/49): loss=3.134506438327341e+41\n",
      "Gradient Descent(27/49): loss=1.423730568730638e+43\n",
      "Gradient Descent(28/49): loss=6.466755682976786e+44\n",
      "Gradient Descent(29/49): loss=2.9372783012306567e+46\n",
      "Gradient Descent(30/49): loss=1.3341471739209005e+48\n",
      "Gradient Descent(31/49): loss=6.059857116485585e+49\n",
      "Gradient Descent(32/49): loss=2.7524600726245008e+51\n",
      "Gradient Descent(33/49): loss=1.2502005089825961e+53\n",
      "Gradient Descent(34/49): loss=5.67856125582237e+54\n",
      "Gradient Descent(35/49): loss=2.579270901302724e+56\n",
      "Gradient Descent(36/49): loss=1.171535901895189e+58\n",
      "Gradient Descent(37/49): loss=5.321257137961631e+59\n",
      "Gradient Descent(38/49): loss=2.416979068460582e+61\n",
      "Gradient Descent(39/49): loss=1.0978209971665357e+63\n",
      "Gradient Descent(40/49): loss=4.9864351642372657e+64\n",
      "Gradient Descent(41/49): loss=2.264898896205935e+66\n",
      "Gradient Descent(42/49): loss=1.0287443516413464e+68\n",
      "Gradient Descent(43/49): loss=4.672680722335191e+69\n",
      "Gradient Descent(44/49): loss=2.1223878505915757e+71\n",
      "Gradient Descent(45/49): loss=9.640141186636779e+72\n",
      "Gradient Descent(46/49): loss=4.3786682096011523e+74\n",
      "Gradient Descent(47/49): loss=1.9888438269295416e+76\n",
      "Gradient Descent(48/49): loss=9.033568150339567e+77\n",
      "Gradient Descent(49/49): loss=4.1031554324109634e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6551281640722608\n",
      "Gradient Descent(2/49): loss=53.85410035114102\n",
      "Gradient Descent(3/49): loss=2391.9662842226294\n",
      "Gradient Descent(4/49): loss=108345.94813301548\n",
      "Gradient Descent(5/49): loss=4924888.310187238\n",
      "Gradient Descent(6/49): loss=224075444.10142007\n",
      "Gradient Descent(7/49): loss=10197942896.590591\n",
      "Gradient Descent(8/49): loss=464158662742.09436\n",
      "Gradient Descent(9/49): loss=21126663171278.305\n",
      "Gradient Descent(10/49): loss=961608955354429.6\n",
      "Gradient Descent(11/49): loss=4.376904047463807e+16\n",
      "Gradient Descent(12/49): loss=1.9922132824987244e+18\n",
      "Gradient Descent(13/49): loss=9.067858190494112e+19\n",
      "Gradient Descent(14/49): loss=4.1273721828250684e+21\n",
      "Gradient Descent(15/49): loss=1.878635623384544e+23\n",
      "Gradient Descent(16/49): loss=8.550893079050149e+24\n",
      "Gradient Descent(17/49): loss=3.8920678203495804e+26\n",
      "Gradient Descent(18/49): loss=1.771533310608711e+28\n",
      "Gradient Descent(19/49): loss=8.063400783014638e+29\n",
      "Gradient Descent(20/49): loss=3.6701783590942373e+31\n",
      "Gradient Descent(21/49): loss=1.6705369793968146e+33\n",
      "Gradient Descent(22/49): loss=7.603700764642993e+34\n",
      "Gradient Descent(23/49): loss=3.4609389694066564e+36\n",
      "Gradient Descent(24/49): loss=1.5752985185397097e+38\n",
      "Gradient Descent(25/49): loss=7.170208560306057e+39\n",
      "Gradient Descent(26/49): loss=3.2636284610961205e+41\n",
      "Gradient Descent(27/49): loss=1.485489667768052e+43\n",
      "Gradient Descent(28/49): loss=6.7614300443516834e+44\n",
      "Gradient Descent(29/49): loss=3.077566760417199e+46\n",
      "Gradient Descent(30/49): loss=1.4008008812776222e+48\n",
      "Gradient Descent(31/49): loss=6.37595627242265e+49\n",
      "Gradient Descent(32/49): loss=2.9021125651183108e+51\n",
      "Gradient Descent(33/49): loss=1.320940260686178e+53\n",
      "Gradient Descent(34/49): loss=6.012458625051724e+54\n",
      "Gradient Descent(35/49): loss=2.7366611340304336e+56\n",
      "Gradient Descent(36/49): loss=1.2456325489388862e+58\n",
      "Gradient Descent(37/49): loss=5.669684228280183e+59\n",
      "Gradient Descent(38/49): loss=2.580642202693937e+61\n",
      "Gradient Descent(39/49): loss=1.1746181815746661e+63\n",
      "Gradient Descent(40/49): loss=5.346451635354467e+64\n",
      "Gradient Descent(41/49): loss=2.4335180178179026e+66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=1.1076523920807422e+68\n",
      "Gradient Descent(43/49): loss=5.041646754612187e+69\n",
      "Gradient Descent(44/49): loss=2.294781483795934e+71\n",
      "Gradient Descent(45/49): loss=1.0445043682513463e+73\n",
      "Gradient Descent(46/49): loss=4.754219009521889e+74\n",
      "Gradient Descent(47/49): loss=2.1639544148905104e+76\n",
      "Gradient Descent(48/49): loss=9.849564566431383e+77\n",
      "Gradient Descent(49/49): loss=4.4831777176419395e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.658038533474747\n",
      "Gradient Descent(2/49): loss=53.60520003333573\n",
      "Gradient Descent(3/49): loss=2358.649964239178\n",
      "Gradient Descent(4/49): loss=105782.86803660596\n",
      "Gradient Descent(5/49): loss=4760107.119543201\n",
      "Gradient Descent(6/49): loss=214392323.4687147\n",
      "Gradient Descent(7/49): loss=9658634865.901304\n",
      "Gradient Descent(8/49): loss=435167185006.6892\n",
      "Gradient Descent(9/49): loss=19606797321507.04\n",
      "Gradient Descent(10/49): loss=883405741675804.9\n",
      "Gradient Descent(11/49): loss=3.9802897419793624e+16\n",
      "Gradient Descent(12/49): loss=1.7933680603405952e+18\n",
      "Gradient Descent(13/49): loss=8.080239924686299e+19\n",
      "Gradient Descent(14/49): loss=3.640651486713489e+21\n",
      "Gradient Descent(15/49): loss=1.6403403361117e+23\n",
      "Gradient Descent(16/49): loss=7.390755313674163e+24\n",
      "Gradient Descent(17/49): loss=3.329995793443868e+26\n",
      "Gradient Descent(18/49): loss=1.5003706006832084e+28\n",
      "Gradient Descent(19/49): loss=6.760104454603267e+29\n",
      "Gradient Descent(20/49): loss=3.045848286966091e+31\n",
      "Gradient Descent(21/49): loss=1.3723444437331357e+33\n",
      "Gradient Descent(22/49): loss=6.183266843301108e+34\n",
      "Gradient Descent(23/49): loss=2.7859469996837475e+36\n",
      "Gradient Descent(24/49): loss=1.2552427190584993e+38\n",
      "Gradient Descent(25/49): loss=5.65565060616155e+39\n",
      "Gradient Descent(26/49): loss=2.5482230084527398e+41\n",
      "Gradient Descent(27/49): loss=1.1481332481419004e+43\n",
      "Gradient Descent(28/49): loss=5.173055698485708e+44\n",
      "Gradient Descent(29/49): loss=2.3307839314769303e+46\n",
      "Gradient Descent(30/49): loss=1.0501633950744557e+48\n",
      "Gradient Descent(31/49): loss=4.731640464225593e+49\n",
      "Gradient Descent(32/49): loss=2.131898863329704e+51\n",
      "Gradient Descent(33/49): loss=9.605532791068407e+52\n",
      "Gradient Descent(34/49): loss=4.327891054652754e+54\n",
      "Gradient Descent(35/49): loss=1.9499845962068716e+56\n",
      "Gradient Descent(36/49): loss=8.785895664716911e+57\n",
      "Gradient Descent(37/49): loss=3.958593456658372e+59\n",
      "Gradient Descent(38/49): loss=1.7835930169338595e+61\n",
      "Gradient Descent(39/49): loss=8.036197919501949e+62\n",
      "Gradient Descent(40/49): loss=3.620807907872769e+64\n",
      "Gradient Descent(41/49): loss=1.6313995793829935e+66\n",
      "Gradient Descent(42/49): loss=7.350471649777799e+67\n",
      "Gradient Descent(43/49): loss=3.311845494935181e+69\n",
      "Gradient Descent(44/49): loss=1.4921927605358597e+71\n",
      "Gradient Descent(45/49): loss=6.723258189431925e+72\n",
      "Gradient Descent(46/49): loss=3.029246748625895e+74\n",
      "Gradient Descent(47/49): loss=1.364864416256483e+76\n",
      "Gradient Descent(48/49): loss=6.14956465863387e+77\n",
      "Gradient Descent(49/49): loss=2.7707620654689307e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6697963231932798\n",
      "Gradient Descent(2/49): loss=54.603781691098035\n",
      "Gradient Descent(3/49): loss=2425.497868147978\n",
      "Gradient Descent(4/49): loss=109813.00845619878\n",
      "Gradient Descent(5/49): loss=4988940.945361226\n",
      "Gradient Descent(6/49): loss=226872619.3378176\n",
      "Gradient Descent(7/49): loss=10320046089.255896\n",
      "Gradient Descent(8/49): loss=469482733535.4465\n",
      "Gradient Descent(9/49): loss=21358434774399.9\n",
      "Gradient Descent(10/49): loss=971679092739681.1\n",
      "Gradient Descent(11/49): loss=4.420561274941267e+16\n",
      "Gradient Descent(12/49): loss=2.0110937536253143e+18\n",
      "Gradient Descent(13/49): loss=9.149288579052252e+19\n",
      "Gradient Descent(14/49): loss=4.1623861487353e+21\n",
      "Gradient Descent(15/49): loss=1.8936400033638995e+23\n",
      "Gradient Descent(16/49): loss=8.614944313237026e+24\n",
      "Gradient Descent(17/49): loss=3.91929118275412e+26\n",
      "Gradient Descent(18/49): loss=1.783046161091163e+28\n",
      "Gradient Descent(19/49): loss=8.111807633081072e+29\n",
      "Gradient Descent(20/49): loss=3.690393693460457e+31\n",
      "Gradient Descent(21/49): loss=1.6789113141029267e+33\n",
      "Gradient Descent(22/49): loss=7.638055543058179e+34\n",
      "Gradient Descent(23/49): loss=3.4748644546555646e+36\n",
      "Gradient Descent(24/49): loss=1.5808582315442596e+38\n",
      "Gradient Descent(25/49): loss=7.191971890855901e+39\n",
      "Gradient Descent(26/49): loss=3.2719227218962694e+41\n",
      "Gradient Descent(27/49): loss=1.4885317212755768e+43\n",
      "Gradient Descent(28/49): loss=6.771940762584686e+44\n",
      "Gradient Descent(29/49): loss=3.0808333498366897e+46\n",
      "Gradient Descent(30/49): loss=1.4015973355684286e+48\n",
      "Gradient Descent(31/49): loss=6.376440618499113e+49\n",
      "Gradient Descent(32/49): loss=2.9009041277005112e+51\n",
      "Gradient Descent(33/49): loss=1.3197401593760426e+53\n",
      "Gradient Descent(34/49): loss=6.004038781007669e+54\n",
      "Gradient Descent(35/49): loss=2.7314832717439767e+56\n",
      "Gradient Descent(36/49): loss=1.2426636695649427e+58\n",
      "Gradient Descent(37/49): loss=5.653386244868596e+59\n",
      "Gradient Descent(38/49): loss=2.571957064203779e+61\n",
      "Gradient Descent(39/49): loss=1.1700886607759902e+63\n",
      "Gradient Descent(40/49): loss=5.32321279049188e+64\n",
      "Gradient Descent(41/49): loss=2.421747630138024e+66\n",
      "Gradient Descent(42/49): loss=1.101752234018287e+68\n",
      "Gradient Descent(43/49): loss=5.0123223826386526e+69\n",
      "Gradient Descent(44/49): loss=2.2803108441061176e+71\n",
      "Gradient Descent(45/49): loss=1.0374068443320287e+73\n",
      "Gradient Descent(46/49): loss=4.719588837849024e+74\n",
      "Gradient Descent(47/49): loss=2.1471343591039565e+76\n",
      "Gradient Descent(48/49): loss=9.768194040703503e+77\n",
      "Gradient Descent(49/49): loss=4.4439517449041014e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6869513954736675\n",
      "Gradient Descent(2/49): loss=56.59040956917077\n",
      "Gradient Descent(3/49): loss=2567.905651195814\n",
      "Gradient Descent(4/49): loss=118650.10991897019\n",
      "Gradient Descent(5/49): loss=5499221.8507344145\n",
      "Gradient Descent(6/49): loss=255087782.1480907\n",
      "Gradient Descent(7/49): loss=11835301226.638922\n",
      "Gradient Descent(8/49): loss=549159282887.362\n",
      "Gradient Descent(9/49): loss=25481553651410.367\n",
      "Gradient Descent(10/49): loss=1182377022489952.5\n",
      "Gradient Descent(11/49): loss=5.4863914452370184e+16\n",
      "Gradient Descent(12/49): loss=2.545762084608719e+18\n",
      "Gradient Descent(13/49): loss=1.181269253481519e+20\n",
      "Gradient Descent(14/49): loss=5.481254971146215e+21\n",
      "Gradient Descent(15/49): loss=2.5433791950117947e+23\n",
      "Gradient Descent(16/49): loss=1.1801636280513532e+25\n",
      "Gradient Descent(17/49): loss=5.476124807428159e+26\n",
      "Gradient Descent(18/49): loss=2.540998739754249e+28\n",
      "Gradient Descent(19/49): loss=1.1790590651367304e+30\n",
      "Gradient Descent(20/49): loss=5.470999482874401e+31\n",
      "Gradient Descent(21/49): loss=2.5386205175712117e+33\n",
      "Gradient Descent(22/49): loss=1.1779555367207408e+35\n",
      "Gradient Descent(23/49): loss=5.465878956258958e+36\n",
      "Gradient Descent(24/49): loss=2.536244521389169e+38\n",
      "Gradient Descent(25/49): loss=1.1768530411583823e+40\n",
      "Gradient Descent(26/49): loss=5.460763222172107e+41\n",
      "Gradient Descent(27/49): loss=2.5338707489998896e+43\n",
      "Gradient Descent(28/49): loss=1.1757515774660116e+45\n",
      "Gradient Descent(29/49): loss=5.455652276105462e+46\n",
      "Gradient Descent(30/49): loss=2.531499198318955e+48\n",
      "Gradient Descent(31/49): loss=1.1746511446774657e+50\n",
      "Gradient Descent(32/49): loss=5.4505461135770436e+51\n",
      "Gradient Descent(33/49): loss=2.5291298672668653e+53\n",
      "Gradient Descent(34/49): loss=1.1735517418278486e+55\n",
      "Gradient Descent(35/49): loss=5.445444730109831e+56\n",
      "Gradient Descent(36/49): loss=2.5267627537662217e+58\n",
      "Gradient Descent(37/49): loss=1.1724533679531969e+60\n",
      "Gradient Descent(38/49): loss=5.440348121230752e+61\n",
      "Gradient Descent(39/49): loss=2.5243978557414603e+63\n",
      "Gradient Descent(40/49): loss=1.1713560220904461e+65\n",
      "Gradient Descent(41/49): loss=5.4352562824711896e+66\n",
      "Gradient Descent(42/49): loss=2.522035171119084e+68\n",
      "Gradient Descent(43/49): loss=1.1702597032774546e+70\n",
      "Gradient Descent(44/49): loss=5.430169209366547e+71\n",
      "Gradient Descent(45/49): loss=2.5196746978274352e+73\n",
      "Gradient Descent(46/49): loss=1.1691644105529461e+75\n",
      "Gradient Descent(47/49): loss=5.4250868974564703e+76\n",
      "Gradient Descent(48/49): loss=2.5173164337969025e+78\n",
      "Gradient Descent(49/49): loss=1.1680701429565978e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6828317664986367\n",
      "Gradient Descent(2/49): loss=56.1934797163179\n",
      "Gradient Descent(3/49): loss=2550.266373078306\n",
      "Gradient Descent(4/49): loss=118006.30995061524\n",
      "Gradient Descent(5/49): loss=5479681.319875974\n",
      "Gradient Descent(6/49): loss=254696269.22608483\n",
      "Gradient Descent(7/49): loss=11841629324.655397\n",
      "Gradient Descent(8/49): loss=550600167700.1095\n",
      "Gradient Descent(9/49): loss=25601884238898.723\n",
      "Gradient Descent(10/49): loss=1190448780251781.8\n",
      "Gradient Descent(11/49): loss=5.535418393535266e+16\n",
      "Gradient Descent(12/49): loss=2.573892919807573e+18\n",
      "Gradient Descent(13/49): loss=1.196824808456145e+20\n",
      "Gradient Descent(14/49): loss=5.56507108265868e+21\n",
      "Gradient Descent(15/49): loss=2.587681712160853e+23\n",
      "Gradient Descent(16/49): loss=1.2032365047723707e+25\n",
      "Gradient Descent(17/49): loss=5.594884725826206e+26\n",
      "Gradient Descent(18/49): loss=2.6015446660446224e+28\n",
      "Gradient Descent(19/49): loss=1.2096825907996428e+30\n",
      "Gradient Descent(20/49): loss=5.624858145407516e+31\n",
      "Gradient Descent(21/49): loss=2.615481895580006e+33\n",
      "Gradient Descent(22/49): loss=1.2161632114611174e+35\n",
      "Gradient Descent(23/49): loss=5.65499214279616e+36\n",
      "Gradient Descent(24/49): loss=2.6294937911069244e+38\n",
      "Gradient Descent(25/49): loss=1.222678550717092e+40\n",
      "Gradient Descent(26/49): loss=5.6852875767939156e+41\n",
      "Gradient Descent(27/49): loss=2.6435807524300353e+43\n",
      "Gradient Descent(28/49): loss=1.2292287945369887e+45\n",
      "Gradient Descent(29/49): loss=5.715745312224433e+46\n",
      "Gradient Descent(30/49): loss=2.6577431816931267e+48\n",
      "Gradient Descent(31/49): loss=1.235814129913938e+50\n",
      "Gradient Descent(32/49): loss=5.746366218582575e+51\n",
      "Gradient Descent(33/49): loss=2.671981483199776e+53\n",
      "Gradient Descent(34/49): loss=1.2424347448435946e+55\n",
      "Gradient Descent(35/49): loss=5.777151170022337e+56\n",
      "Gradient Descent(36/49): loss=2.686296063419592e+58\n",
      "Gradient Descent(37/49): loss=1.2490908283287428e+60\n",
      "Gradient Descent(38/49): loss=5.808101045380876e+61\n",
      "Gradient Descent(39/49): loss=2.7006873309997494e+63\n",
      "Gradient Descent(40/49): loss=1.2557825703847157e+65\n",
      "Gradient Descent(41/49): loss=5.839216728203335e+66\n",
      "Gradient Descent(42/49): loss=2.715155696776712e+68\n",
      "Gradient Descent(43/49): loss=1.2625101620448538e+70\n",
      "Gradient Descent(44/49): loss=5.870499106768565e+71\n",
      "Gradient Descent(45/49): loss=2.729701573787889e+73\n",
      "Gradient Descent(46/49): loss=1.2692737953658687e+75\n",
      "Gradient Descent(47/49): loss=5.901949074113935e+76\n",
      "Gradient Descent(48/49): loss=2.7443253772834457e+78\n",
      "Gradient Descent(49/49): loss=1.2760736634334028e+80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.68586751963424\n",
      "Gradient Descent(2/49): loss=55.9351128682118\n",
      "Gradient Descent(3/49): loss=2514.844925889848\n",
      "Gradient Descent(4/49): loss=115221.03257221117\n",
      "Gradient Descent(5/49): loss=5296705.539929765\n",
      "Gradient Descent(6/49): loss=243710306.02010402\n",
      "Gradient Descent(7/49): loss=11216492685.425459\n",
      "Gradient Descent(8/49): loss=516267201254.6306\n",
      "Gradient Descent(9/49): loss=23763054587442.55\n",
      "Gradient Descent(10/49): loss=1093787821347024.0\n",
      "Gradient Descent(11/49): loss=5.034598358668476e+16\n",
      "Gradient Descent(12/49): loss=2.3173778723568655e+18\n",
      "Gradient Descent(13/49): loss=1.0666672613547423e+20\n",
      "Gradient Descent(14/49): loss=4.909769469781447e+21\n",
      "Gradient Descent(15/49): loss=2.2599209277971998e+23\n",
      "Gradient Descent(16/49): loss=1.0402204539948195e+25\n",
      "Gradient Descent(17/49): loss=4.788037410157692e+26\n",
      "Gradient Descent(18/49): loss=2.2038888164519895e+28\n",
      "Gradient Descent(19/49): loss=1.0144293997530936e+30\n",
      "Gradient Descent(20/49): loss=4.669323604013334e+31\n",
      "Gradient Descent(21/49): loss=2.1492459627636e+33\n",
      "Gradient Descent(22/49): loss=9.89277805569844e+34\n",
      "Gradient Descent(23/49): loss=4.553553169576874e+36\n",
      "Gradient Descent(24/49): loss=2.0959579151002164e+38\n",
      "Gradient Descent(25/49): loss=9.647498158630103e+39\n",
      "Gradient Descent(26/49): loss=4.440653128110362e+41\n",
      "Gradient Descent(27/49): loss=2.0439910824503794e+43\n",
      "Gradient Descent(28/49): loss=9.408299690623469e+44\n",
      "Gradient Descent(29/49): loss=4.3305523115331375e+46\n",
      "Gradient Descent(30/49): loss=1.9933127068236807e+48\n",
      "Gradient Descent(31/49): loss=9.175031869729524e+49\n",
      "Gradient Descent(32/49): loss=4.223181316327129e+51\n",
      "Gradient Descent(33/49): loss=1.9438908424304254e+53\n",
      "Gradient Descent(34/49): loss=8.947547652467803e+54\n",
      "Gradient Descent(35/49): loss=4.118472459754285e+56\n",
      "Gradient Descent(36/49): loss=1.8956943355396645e+58\n",
      "Gradient Descent(37/49): loss=8.72570364112999e+59\n",
      "Gradient Descent(38/49): loss=4.0163597371912074e+61\n",
      "Gradient Descent(39/49): loss=1.8486928048408478e+63\n",
      "Gradient Descent(40/49): loss=8.509359993386542e+64\n",
      "Gradient Descent(41/49): loss=3.916778780522223e+66\n",
      "Gradient Descent(42/49): loss=1.8028566222926527e+68\n",
      "Gradient Descent(43/49): loss=8.298380334135494e+69\n",
      "Gradient Descent(44/49): loss=3.8196668175639415e+71\n",
      "Gradient Descent(45/49): loss=1.7581568944464452e+73\n",
      "Gradient Descent(46/49): loss=8.09263166953653e+74\n",
      "Gradient Descent(47/49): loss=3.724962632496174e+76\n",
      "Gradient Descent(48/49): loss=1.7145654442329878e+78\n",
      "Gradient Descent(49/49): loss=7.891984303176444e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6975537153425517\n",
      "Gradient Descent(2/49): loss=56.95342698124131\n",
      "Gradient Descent(3/49): loss=2584.494072002169\n",
      "Gradient Descent(4/49): loss=119510.45779478886\n",
      "Gradient Descent(5/49): loss=5545514.528765497\n",
      "Gradient Descent(6/49): loss=257572713.6114414\n",
      "Gradient Descent(7/49): loss=11966987696.54979\n",
      "Gradient Descent(8/49): loss=556043418946.0265\n",
      "Gradient Descent(9/49): loss=25837145711939.363\n",
      "Gradient Descent(10/49): loss=1200560520678563.8\n",
      "Gradient Descent(11/49): loss=5.5785935754647224e+16\n",
      "Gradient Descent(12/49): loss=2.5921834851300603e+18\n",
      "Gradient Descent(13/49): loss=1.2045001693687834e+20\n",
      "Gradient Descent(14/49): loss=5.596906155882826e+21\n",
      "Gradient Descent(15/49): loss=2.6006936375035508e+23\n",
      "Gradient Descent(16/49): loss=1.2084546815458616e+25\n",
      "Gradient Descent(17/49): loss=5.615281629615827e+26\n",
      "Gradient Descent(18/49): loss=2.60923212791691e+28\n",
      "Gradient Descent(19/49): loss=1.2124222341646595e+30\n",
      "Gradient Descent(20/49): loss=5.633717515082612e+31\n",
      "Gradient Descent(21/49): loss=2.6177986633739995e+33\n",
      "Gradient Descent(22/49): loss=1.2164028145998476e+35\n",
      "Gradient Descent(23/49): loss=5.652213931006896e+36\n",
      "Gradient Descent(24/49): loss=2.6263933245170742e+38\n",
      "Gradient Descent(25/49): loss=1.2203964639817445e+40\n",
      "Gradient Descent(26/49): loss=5.670771073761728e+41\n",
      "Gradient Descent(27/49): loss=2.6350162033486805e+43\n",
      "Gradient Descent(28/49): loss=1.2244032251692096e+45\n",
      "Gradient Descent(29/49): loss=5.689389142653357e+46\n",
      "Gradient Descent(30/49): loss=2.6436673925020736e+48\n",
      "Gradient Descent(31/49): loss=1.228423141209014e+50\n",
      "Gradient Descent(32/49): loss=5.708068337710261e+51\n",
      "Gradient Descent(33/49): loss=2.6523469849243663e+53\n",
      "Gradient Descent(34/49): loss=1.2324562552906293e+55\n",
      "Gradient Descent(35/49): loss=5.726808859619504e+56\n",
      "Gradient Descent(36/49): loss=2.661055073868151e+58\n",
      "Gradient Descent(37/49): loss=1.2365026107454017e+60\n",
      "Gradient Descent(38/49): loss=5.745610909727213e+61\n",
      "Gradient Descent(39/49): loss=2.6697917528921274e+63\n",
      "Gradient Descent(40/49): loss=1.2405622510469035e+65\n",
      "Gradient Descent(41/49): loss=5.764474690040511e+66\n",
      "Gradient Descent(42/49): loss=2.6785571158622375e+68\n",
      "Gradient Descent(43/49): loss=1.2446352198114683e+70\n",
      "Gradient Descent(44/49): loss=5.783400403229689e+71\n",
      "Gradient Descent(45/49): loss=2.6873512569525295e+73\n",
      "Gradient Descent(46/49): loss=1.2487215607986242e+75\n",
      "Gradient Descent(47/49): loss=5.802388252630621e+76\n",
      "Gradient Descent(48/49): loss=2.6961742706463433e+78\n",
      "Gradient Descent(49/49): loss=1.2528213179115732e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7151008276036144\n",
      "Gradient Descent(2/49): loss=59.02275358848095\n",
      "Gradient Descent(3/49): loss=2736.0062629064664\n",
      "Gradient Descent(4/49): loss=129113.8250967887\n",
      "Gradient Descent(5/49): loss=6111899.54712025\n",
      "Gradient Descent(6/49): loss=289559373.0217273\n",
      "Gradient Descent(7/49): loss=13721490113.69435\n",
      "Gradient Descent(8/49): loss=650271369856.2626\n",
      "Gradient Descent(9/49): loss=30817448073543.89\n",
      "Gradient Descent(10/49): loss=1460498973750459.0\n",
      "Gradient Descent(11/49): loss=6.92160139328245e+16\n",
      "Gradient Descent(12/49): loss=3.2802891913139497e+18\n",
      "Gradient Descent(13/49): loss=1.5545967120539533e+20\n",
      "Gradient Descent(14/49): loss=7.36755481268714e+21\n",
      "Gradient Descent(15/49): loss=3.4916363959204855e+23\n",
      "Gradient Descent(16/49): loss=1.6547586107911653e+25\n",
      "Gradient Descent(17/49): loss=7.842242869794188e+26\n",
      "Gradient Descent(18/49): loss=3.716600889549837e+28\n",
      "Gradient Descent(19/49): loss=1.76137393381505e+30\n",
      "Gradient Descent(20/49): loss=8.347514912166254e+31\n",
      "Gradient Descent(21/49): loss=3.9560597481124545e+33\n",
      "Gradient Descent(22/49): loss=1.8748584333599342e+35\n",
      "Gradient Descent(23/49): loss=8.885341397638055e+36\n",
      "Gradient Descent(24/49): loss=4.210946829255363e+38\n",
      "Gradient Descent(25/49): loss=1.9956546862150334e+40\n",
      "Gradient Descent(26/49): loss=9.457819792315866e+41\n",
      "Gradient Descent(27/49): loss=4.482256165948933e+43\n",
      "Gradient Descent(28/49): loss=2.124233785201766e+45\n",
      "Gradient Descent(29/49): loss=1.0067182702480106e+47\n",
      "Gradient Descent(30/49): loss=4.771045836439706e+48\n",
      "Gradient Descent(31/49): loss=2.2610971754591375e+50\n",
      "Gradient Descent(32/49): loss=1.0715806580228526e+52\n",
      "Gradient Descent(33/49): loss=5.078442090466628e+53\n",
      "Gradient Descent(34/49): loss=2.4067786100029817e+55\n",
      "Gradient Descent(35/49): loss=1.1406220991358439e+57\n",
      "Gradient Descent(36/49): loss=5.40564374151324e+58\n",
      "Gradient Descent(37/49): loss=2.561846231306555e+60\n",
      "Gradient Descent(38/49): loss=1.214111848040939e+62\n",
      "Gradient Descent(39/49): loss=5.753926842055661e+63\n",
      "Gradient Descent(40/49): loss=2.726904786997203e+65\n",
      "Gradient Descent(41/49): loss=1.2923365071307848e+67\n",
      "Gradient Descent(42/49): loss=6.124649660034875e+68\n",
      "Gradient Descent(43/49): loss=2.902597988309351e+70\n",
      "Gradient Descent(44/49): loss=1.3756011444562383e+72\n",
      "Gradient Descent(45/49): loss=6.519257975960643e+73\n",
      "Gradient Descent(46/49): loss=3.0896110204914365e+75\n",
      "Gradient Descent(47/49): loss=1.4642304834601409e+77\n",
      "Gradient Descent(48/49): loss=6.9392907213053926e+78\n",
      "Gradient Descent(49/49): loss=3.2886732149574133e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.710848480260345\n",
      "Gradient Descent(2/49): loss=58.60884073113475\n",
      "Gradient Descent(3/49): loss=2717.20429002115\n",
      "Gradient Descent(4/49): loss=128412.12572675693\n",
      "Gradient Descent(5/49): loss=6090102.608234321\n",
      "Gradient Descent(6/49): loss=289110189.34322643\n",
      "Gradient Descent(7/49): loss=13728558432.556902\n",
      "Gradient Descent(8/49): loss=651962913789.3036\n",
      "Gradient Descent(9/49): loss=30962192692863.35\n",
      "Gradient Descent(10/49): loss=1470428027818646.8\n",
      "Gradient Descent(11/49): loss=6.9832373943051144e+16\n",
      "Gradient Descent(12/49): loss=3.316424618863446e+18\n",
      "Gradient Descent(13/49): loss=1.575010819810779e+20\n",
      "Gradient Descent(14/49): loss=7.47992044483201e+21\n",
      "Gradient Descent(15/49): loss=3.552306445109054e+23\n",
      "Gradient Descent(16/49): loss=1.6870341390225696e+25\n",
      "Gradient Descent(17/49): loss=8.011933189804407e+26\n",
      "Gradient Descent(18/49): loss=3.804965885627373e+28\n",
      "Gradient Descent(19/49): loss=1.8070252272311607e+30\n",
      "Gradient Descent(20/49): loss=8.581785672046164e+31\n",
      "Gradient Descent(21/49): loss=4.075595858433603e+33\n",
      "Gradient Descent(22/49): loss=1.9355507392186685e+35\n",
      "Gradient Descent(23/49): loss=9.192169180224113e+36\n",
      "Gradient Descent(24/49): loss=4.3654745146076354e+38\n",
      "Gradient Descent(25/49): loss=2.0732176882351682e+40\n",
      "Gradient Descent(26/49): loss=9.84596650015633e+41\n",
      "Gradient Descent(27/49): loss=4.675970925403645e+43\n",
      "Gradient Descent(28/49): loss=2.2206762632061673e+45\n",
      "Gradient Descent(29/49): loss=1.0546265459384988e+47\n",
      "Gradient Descent(30/49): loss=5.0085515382253076e+48\n",
      "Gradient Descent(31/49): loss=2.378622898092869e+50\n",
      "Gradient Descent(32/49): loss=1.129637350869015e+52\n",
      "Gradient Descent(33/49): loss=5.3647871022409555e+53\n",
      "Gradient Descent(34/49): loss=2.5478035610481674e+55\n",
      "Gradient Descent(35/49): loss=1.209983334283329e+57\n",
      "Gradient Descent(36/49): loss=5.746360086886326e+58\n",
      "Gradient Descent(37/49): loss=2.729017277557652e+60\n",
      "Gradient Descent(38/49): loss=1.2960439632392773e+62\n",
      "Gradient Descent(39/49): loss=6.155072628020365e+63\n",
      "Gradient Descent(40/49): loss=2.9231199041672597e+65\n",
      "Gradient Descent(41/49): loss=1.3882256945661754e+67\n",
      "Gradient Descent(42/49): loss=6.592855039255574e+68\n",
      "Gradient Descent(43/49): loss=3.13102817061895e+70\n",
      "Gradient Descent(44/49): loss=1.486963894524882e+72\n",
      "Gradient Descent(45/49): loss=7.061774928660325e+73\n",
      "Gradient Descent(46/49): loss=3.3537240094850897e+75\n",
      "Gradient Descent(47/49): loss=1.5927248950046112e+77\n",
      "Gradient Descent(48/49): loss=7.564046963891137e+78\n",
      "Gradient Descent(49/49): loss=3.5922591937501775e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7140114353516411\n",
      "Gradient Descent(2/49): loss=58.34074306616164\n",
      "Gradient Descent(3/49): loss=2679.568224645399\n",
      "Gradient Descent(4/49): loss=125387.98788018168\n",
      "Gradient Descent(5/49): loss=5887144.412177707\n",
      "Gradient Descent(6/49): loss=276662496.5195806\n",
      "Gradient Descent(7/49): loss=13005052266.709293\n",
      "Gradient Descent(8/49): loss=611376279878.6719\n",
      "Gradient Descent(9/49): loss=28741900583707.086\n",
      "Gradient Descent(10/49): loss=1351218296358595.8\n",
      "Gradient Descent(11/49): loss=6.352380401113138e+16\n",
      "Gradient Descent(12/49): loss=2.986398523912916e+18\n",
      "Gradient Descent(13/49): loss=1.4039741556596315e+20\n",
      "Gradient Descent(14/49): loss=6.600403568871592e+21\n",
      "Gradient Descent(15/49): loss=3.1030007127665686e+23\n",
      "Gradient Descent(16/49): loss=1.4587916292257536e+25\n",
      "Gradient Descent(17/49): loss=6.858113220000705e+26\n",
      "Gradient Descent(18/49): loss=3.224155939706295e+28\n",
      "Gradient Descent(19/49): loss=1.5157494768063916e+30\n",
      "Gradient Descent(20/49): loss=7.12588509832309e+31\n",
      "Gradient Descent(21/49): loss=3.350041627075466e+33\n",
      "Gradient Descent(22/49): loss=1.5749312188356135e+35\n",
      "Gradient Descent(23/49): loss=7.404112008696005e+36\n",
      "Gradient Descent(24/49): loss=3.480842463575469e+38\n",
      "Gradient Descent(25/49): loss=1.6364236848390625e+40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=7.693202161041374e+41\n",
      "Gradient Descent(27/49): loss=3.61675035866235e+43\n",
      "Gradient Descent(28/49): loss=1.7003170959326037e+45\n",
      "Gradient Descent(29/49): loss=7.993579705594997e+46\n",
      "Gradient Descent(30/49): loss=3.7579647150846976e+48\n",
      "Gradient Descent(31/49): loss=1.7667051959132817e+50\n",
      "Gradient Descent(32/49): loss=8.305685353399219e+51\n",
      "Gradient Descent(33/49): loss=3.904692721187657e+53\n",
      "Gradient Descent(34/49): loss=1.835685388762777e+55\n",
      "Gradient Descent(35/49): loss=8.62997702285821e+56\n",
      "Gradient Descent(36/49): loss=4.057149654890327e+58\n",
      "Gradient Descent(37/49): loss=1.9073588815564468e+60\n",
      "Gradient Descent(38/49): loss=8.966930511589895e+61\n",
      "Gradient Descent(39/49): loss=4.215559199539254e+63\n",
      "Gradient Descent(40/49): loss=1.9818308329534648e+65\n",
      "Gradient Descent(41/49): loss=9.317040194511608e+66\n",
      "Gradient Descent(42/49): loss=4.380153772094583e+68\n",
      "Gradient Descent(43/49): loss=2.0592105074845734e+70\n",
      "Gradient Descent(44/49): loss=9.680819749182352e+71\n",
      "Gradient Descent(45/49): loss=4.5511748641298566e+73\n",
      "Gradient Descent(46/49): loss=2.1396114358638752e+75\n",
      "Gradient Descent(47/49): loss=1.005880290946532e+77\n",
      "Gradient Descent(48/49): loss=4.728873396146159e+78\n",
      "Gradient Descent(49/49): loss=2.223151581560086e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7256216752847096\n",
      "Gradient Descent(2/49): loss=59.3786686199729\n",
      "Gradient Descent(3/49): loss=2752.0845955403647\n",
      "Gradient Descent(4/49): loss=129949.13671321853\n",
      "Gradient Descent(5/49): loss=6157361.071065537\n",
      "Gradient Descent(6/49): loss=292038963.1056112\n",
      "Gradient Descent(7/49): loss=13855273385.929083\n",
      "Gradient Descent(8/49): loss=657398485226.9313\n",
      "Gradient Descent(9/49): loss=31192804587897.195\n",
      "Gradient Descent(10/49): loss=1480075604101379.2\n",
      "Gradient Descent(11/49): loss=7.022868506038526e+16\n",
      "Gradient Descent(12/49): loss=3.332311048238661e+18\n",
      "Gradient Descent(13/49): loss=1.581162990487003e+20\n",
      "Gradient Descent(14/49): loss=7.50253072054202e+21\n",
      "Gradient Descent(15/49): loss=3.559909315831751e+23\n",
      "Gradient Descent(16/49): loss=1.6891572864564243e+25\n",
      "Gradient Descent(17/49): loss=8.014957944098327e+26\n",
      "Gradient Descent(18/49): loss=3.8030532388943222e+28\n",
      "Gradient Descent(19/49): loss=1.8045277395107504e+30\n",
      "Gradient Descent(20/49): loss=8.56238437471553e+31\n",
      "Gradient Descent(21/49): loss=4.0628040554168146e+33\n",
      "Gradient Descent(22/49): loss=1.9277780663018616e+35\n",
      "Gradient Descent(23/49): loss=9.147200362683973e+36\n",
      "Gradient Descent(24/49): loss=4.340296009055149e+38\n",
      "Gradient Descent(25/49): loss=2.0594464644148617e+40\n",
      "Gradient Descent(26/49): loss=9.771959633496078e+41\n",
      "Gradient Descent(27/49): loss=4.636740829570952e+43\n",
      "Gradient Descent(28/49): loss=2.2001078930898828e+45\n",
      "Gradient Descent(29/49): loss=1.0439390337208703e+47\n",
      "Gradient Descent(30/49): loss=4.9534330091217174e+48\n",
      "Gradient Descent(31/49): loss=2.3503765817053763e+50\n",
      "Gradient Descent(32/49): loss=1.1152406958277477e+52\n",
      "Gradient Descent(33/49): loss=5.291755454472368e+53\n",
      "Gradient Descent(34/49): loss=2.5109087118771324e+55\n",
      "Gradient Descent(35/49): loss=1.1914123042197771e+57\n",
      "Gradient Descent(36/49): loss=5.653185525749752e+58\n",
      "Gradient Descent(37/49): loss=2.682405282818962e+60\n",
      "Gradient Descent(38/49): loss=1.2727864791489954e+62\n",
      "Gradient Descent(39/49): loss=6.039301487663493e+63\n",
      "Gradient Descent(40/49): loss=2.865615172411414e+65\n",
      "Gradient Descent(41/49): loss=1.3597185590301542e+67\n",
      "Gradient Descent(42/49): loss=6.451789401349544e+68\n",
      "Gradient Descent(43/49): loss=3.0613384073434044e+70\n",
      "Gradient Descent(44/49): loss=1.4525881521048251e+72\n",
      "Gradient Descent(45/49): loss=6.892450486930524e+73\n",
      "Gradient Descent(46/49): loss=3.270429656606509e+75\n",
      "Gradient Descent(47/49): loss=1.5518007940851424e+77\n",
      "Gradient Descent(48/49): loss=7.363208988943596e+78\n",
      "Gradient Descent(49/49): loss=3.4938019635969633e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7435651063653246\n",
      "Gradient Descent(2/49): loss=61.53324359382764\n",
      "Gradient Descent(3/49): loss=2913.1808645077645\n",
      "Gradient Descent(4/49): loss=140376.27061407498\n",
      "Gradient Descent(5/49): loss=6785341.000699179\n",
      "Gradient Descent(6/49): loss=328254148.48074996\n",
      "Gradient Descent(7/49): loss=15883710938.089205\n",
      "Gradient Descent(8/49): loss=768641415053.8997\n",
      "Gradient Descent(9/49): loss=37196698195645.45\n",
      "Gradient Descent(10/49): loss=1800062505383054.8\n",
      "Gradient Descent(11/49): loss=8.711070732073142e+16\n",
      "Gradient Descent(12/49): loss=4.215564297254026e+18\n",
      "Gradient Descent(13/49): loss=2.0400460030929014e+20\n",
      "Gradient Descent(14/49): loss=9.872433643879307e+21\n",
      "Gradient Descent(15/49): loss=4.777585759238105e+23\n",
      "Gradient Descent(16/49): loss=2.312026253897635e+25\n",
      "Gradient Descent(17/49): loss=1.1188633075627096e+27\n",
      "Gradient Descent(18/49): loss=5.414536704828464e+28\n",
      "Gradient Descent(19/49): loss=2.6202671526210224e+30\n",
      "Gradient Descent(20/49): loss=1.268030918541741e+32\n",
      "Gradient Descent(21/49): loss=6.136406391932233e+33\n",
      "Gradient Descent(22/49): loss=2.9696029376274858e+35\n",
      "Gradient Descent(23/49): loss=1.4370856563160956e+37\n",
      "Gradient Descent(24/49): loss=6.954516233203455e+38\n",
      "Gradient Descent(25/49): loss=3.365512405285274e+40\n",
      "Gradient Descent(26/49): loss=1.6286788858226394e+42\n",
      "Gradient Descent(27/49): loss=7.881697030617969e+43\n",
      "Gradient Descent(28/49): loss=3.814204790349117e+45\n",
      "Gradient Descent(29/49): loss=1.8458154539824524e+47\n",
      "Gradient Descent(30/49): loss=8.932490197645113e+48\n",
      "Gradient Descent(31/49): loss=4.3227171469864776e+50\n",
      "Gradient Descent(32/49): loss=2.091900815942375e+52\n",
      "Gradient Descent(33/49): loss=1.0123375818820375e+54\n",
      "Gradient Descent(34/49): loss=4.899024714176539e+55\n",
      "Gradient Descent(35/49): loss=2.370794444427651e+57\n",
      "Gradient Descent(36/49): loss=1.1473031114671142e+59\n",
      "Gradient Descent(37/49): loss=5.5521659951413104e+60\n",
      "Gradient Descent(38/49): loss=2.6868703596719206e+62\n",
      "Gradient Descent(39/49): loss=1.300262336536965e+64\n",
      "Gradient Descent(40/49): loss=6.292384512451929e+65\n",
      "Gradient Descent(41/49): loss=3.04508572923808e+67\n",
      "Gradient Descent(42/49): loss=1.4736141887165441e+69\n",
      "Gradient Descent(43/49): loss=7.131289462021409e+70\n",
      "Gradient Descent(44/49): loss=3.451058613613785e+72\n",
      "Gradient Descent(45/49): loss=1.6700774268139212e+74\n",
      "Gradient Descent(46/49): loss=8.082037785596228e+75\n",
      "Gradient Descent(47/49): loss=3.911156076902254e+77\n",
      "Gradient Descent(48/49): loss=1.8927332763961946e+79\n",
      "Gradient Descent(49/49): loss=9.159540517275623e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7391783053573857\n",
      "Gradient Descent(2/49): loss=61.10181199851823\n",
      "Gradient Descent(3/49): loss=2893.1530974473953\n",
      "Gradient Descent(4/49): loss=139612.1697672328\n",
      "Gradient Descent(5/49): loss=6761056.145161172\n",
      "Gradient Descent(6/49): loss=327739652.9947583\n",
      "Gradient Descent(7/49): loss=15891589009.065512\n",
      "Gradient Descent(8/49): loss=770623930915.9977\n",
      "Gradient Descent(9/49): loss=37370479190881.836\n",
      "Gradient Descent(10/49): loss=1812250127638581.5\n",
      "Gradient Descent(11/49): loss=8.7883753951863e+16\n",
      "Gradient Descent(12/49): loss=4.2618617375543316e+18\n",
      "Gradient Descent(13/49): loss=2.066760734370523e+20\n",
      "Gradient Descent(14/49): loss=1.0022615603699597e+22\n",
      "Gradient Descent(15/49): loss=4.8603993542556897e+23\n",
      "Gradient Descent(16/49): loss=2.3570176635768507e+25\n",
      "Gradient Descent(17/49): loss=1.1430197131309269e+27\n",
      "Gradient Descent(18/49): loss=5.542996496985629e+28\n",
      "Gradient Descent(19/49): loss=2.688038519126224e+30\n",
      "Gradient Descent(20/49): loss=1.3035460304833355e+32\n",
      "Gradient Descent(21/49): loss=6.321457975847539e+33\n",
      "Gradient Descent(22/49): loss=3.0655481284193542e+35\n",
      "Gradient Descent(23/49): loss=1.486616752586537e+37\n",
      "Gradient Descent(24/49): loss=7.209247013881559e+38\n",
      "Gradient Descent(25/49): loss=3.496075395137197e+40\n",
      "Gradient Descent(26/49): loss=1.695398027692678e+42\n",
      "Gradient Descent(27/49): loss=8.221717633156079e+43\n",
      "Gradient Descent(28/49): loss=3.9870661481979035e+45\n",
      "Gradient Descent(29/49): loss=1.9335006600078873e+47\n",
      "Gradient Descent(30/49): loss=9.376380183560848e+48\n",
      "Gradient Descent(31/49): loss=4.547011912905898e+50\n",
      "Gradient Descent(32/49): loss=2.2050425570794517e+52\n",
      "Gradient Descent(33/49): loss=1.0693204178178937e+54\n",
      "Gradient Descent(34/49): loss=5.185596769056089e+55\n",
      "Gradient Descent(35/49): loss=2.5147199476578543e+57\n",
      "Gradient Descent(36/49): loss=1.2194963659504624e+59\n",
      "Gradient Descent(37/49): loss=5.913864833941027e+60\n",
      "Gradient Descent(38/49): loss=2.8678886014445825e+62\n",
      "Gradient Descent(39/49): loss=1.3907631069096872e+64\n",
      "Gradient Descent(40/49): loss=6.744411266765385e+65\n",
      "Gradient Descent(41/49): loss=3.270656455386224e+67\n",
      "Gradient Descent(42/49): loss=1.5860826432504908e+69\n",
      "Gradient Descent(43/49): loss=7.691600097826183e+70\n",
      "Gradient Descent(44/49): loss=3.7299892484566256e+72\n",
      "Gradient Descent(45/49): loss=1.8088329627971819e+74\n",
      "Gradient Descent(46/49): loss=8.77181264974274e+75\n",
      "Gradient Descent(47/49): loss=4.253830991845684e+77\n",
      "Gradient Descent(48/49): loss=2.062866459843676e+79\n",
      "Gradient Descent(49/49): loss=1.0003730847100732e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7424702806269503\n",
      "Gradient Descent(2/49): loss=60.82371448884017\n",
      "Gradient Descent(3/49): loss=2853.188423005654\n",
      "Gradient Descent(4/49): loss=136331.44923686527\n",
      "Gradient Descent(5/49): loss=6536173.283295025\n",
      "Gradient Descent(6/49): loss=313653923.32853204\n",
      "Gradient Descent(7/49): loss=15055499751.063778\n",
      "Gradient Descent(8/49): loss=722727689144.1193\n",
      "Gradient Descent(9/49): loss=34694830216725.04\n",
      "Gradient Descent(10/49): loss=1665551342984588.2\n",
      "Gradient Descent(11/49): loss=7.995621844264211e+16\n",
      "Gradient Descent(12/49): loss=3.838369304291676e+18\n",
      "Gradient Descent(13/49): loss=1.842643660011895e+20\n",
      "Gradient Descent(14/49): loss=8.845776934008401e+21\n",
      "Gradient Descent(15/49): loss=4.246495006728794e+23\n",
      "Gradient Descent(16/49): loss=2.0385682430819213e+25\n",
      "Gradient Descent(17/49): loss=9.78633080256993e+26\n",
      "Gradient Descent(18/49): loss=4.6980164118279115e+28\n",
      "Gradient Descent(19/49): loss=2.255325172885047e+30\n",
      "Gradient Descent(20/49): loss=1.0826892011021405e+32\n",
      "Gradient Descent(21/49): loss=5.197547210898368e+33\n",
      "Gradient Descent(22/49): loss=2.495129440852724e+35\n",
      "Gradient Descent(23/49): loss=1.1978094039365157e+37\n",
      "Gradient Descent(24/49): loss=5.750192132994014e+38\n",
      "Gradient Descent(25/49): loss=2.760431622734424e+40\n",
      "Gradient Descent(26/49): loss=1.3251701104159224e+42\n",
      "Gradient Descent(27/49): loss=6.36159869738139e+43\n",
      "Gradient Descent(28/49): loss=3.0539428612544484e+45\n",
      "Gradient Descent(29/49): loss=1.4660728290902873e+47\n",
      "Gradient Descent(30/49): loss=7.038014913330481e+48\n",
      "Gradient Descent(31/49): loss=3.3786625696486318e+50\n",
      "Gradient Descent(32/49): loss=1.6219574553505427e+52\n",
      "Gradient Descent(33/49): loss=7.786353128601463e+53\n",
      "Gradient Descent(34/49): loss=3.7379090828358543e+55\n",
      "Gradient Descent(35/49): loss=1.794416985818937e+57\n",
      "Gradient Descent(36/49): loss=8.614260667230143e+58\n",
      "Gradient Descent(37/49): loss=4.135353567728421e+60\n",
      "Gradient Descent(38/49): loss=1.9852137973000123e+62\n",
      "Gradient Descent(39/49): loss=9.53019797810233e+63\n",
      "Gradient Descent(40/49): loss=4.575057539160325e+65\n",
      "Gradient Descent(41/49): loss=2.19629765664066e+67\n",
      "Gradient Descent(42/49): loss=1.0543525092911943e+69\n",
      "Gradient Descent(43/49): loss=5.061514364810504e+70\n",
      "Gradient Descent(44/49): loss=2.4298256455429506e+72\n",
      "Gradient Descent(45/49): loss=1.1664597277023117e+74\n",
      "Gradient Descent(46/49): loss=5.599695183262077e+75\n",
      "Gradient Descent(47/49): loss=2.6881842039428634e+77\n",
      "Gradient Descent(48/49): loss=1.2904870850699213e+79\n",
      "Gradient Descent(49/49): loss=6.19509970443846e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7540002030197537\n",
      "Gradient Descent(2/49): loss=61.8811118256627\n",
      "Gradient Descent(3/49): loss=2928.637170514559\n",
      "Gradient Descent(4/49): loss=141177.08870647565\n",
      "Gradient Descent(5/49): loss=6829307.104739722\n",
      "Gradient Descent(6/49): loss=330686805.2850213\n",
      "Gradient Descent(7/49): loss=16017190515.631199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8/49): loss=775881751518.7119\n",
      "Gradient Descent(9/49): loss=37585212948585.5\n",
      "Gradient Descent(10/49): loss=1820716378681786.8\n",
      "Gradient Descent(11/49): loss=8.820003622436592e+16\n",
      "Gradient Descent(12/49): loss=4.2726330379438525e+18\n",
      "Gradient Descent(13/49): loss=2.069772149488402e+20\n",
      "Gradient Descent(14/49): loss=1.0026503705861823e+22\n",
      "Gradient Descent(15/49): loss=4.8570940102586825e+23\n",
      "Gradient Descent(16/49): loss=2.3529001834749408e+25\n",
      "Gradient Descent(17/49): loss=1.1398048452741243e+27\n",
      "Gradient Descent(18/49): loss=5.521505312525303e+28\n",
      "Gradient Descent(19/49): loss=2.6747579684175764e+30\n",
      "Gradient Descent(20/49): loss=1.2957209646984532e+32\n",
      "Gradient Descent(21/49): loss=6.27680275456917e+33\n",
      "Gradient Descent(22/49): loss=3.0406433092800864e+35\n",
      "Gradient Descent(23/49): loss=1.472965153725699e+37\n",
      "Gradient Descent(24/49): loss=7.135418802560288e+38\n",
      "Gradient Descent(25/49): loss=3.4565788171675424e+40\n",
      "Gradient Descent(26/49): loss=1.6744549198716982e+42\n",
      "Gradient Descent(27/49): loss=8.111486608542402e+43\n",
      "Gradient Descent(28/49): loss=3.929410951571316e+45\n",
      "Gradient Descent(29/49): loss=1.90350686273316e+47\n",
      "Gradient Descent(30/49): loss=9.221072626733819e+48\n",
      "Gradient Descent(31/49): loss=4.4669227126090884e+50\n",
      "Gradient Descent(32/49): loss=2.1638912660305844e+52\n",
      "Gradient Descent(33/49): loss=1.0482441072879979e+54\n",
      "Gradient Descent(34/49): loss=5.077961752115554e+55\n",
      "Gradient Descent(35/49): loss=2.45989415792289e+57\n",
      "Gradient Descent(36/49): loss=1.191635456029612e+59\n",
      "Gradient Descent(37/49): loss=5.772586009415689e+60\n",
      "Gradient Descent(38/49): loss=2.7963878606909827e+62\n",
      "Gradient Descent(39/49): loss=1.354641586052604e+64\n",
      "Gradient Descent(40/49): loss=6.562229268902854e+65\n",
      "Gradient Descent(41/49): loss=3.1789111910501416e+67\n",
      "Gradient Descent(42/49): loss=1.5399456414105672e+69\n",
      "Gradient Descent(43/49): loss=7.45988936455947e+70\n",
      "Gradient Descent(44/49): loss=3.61376063121897e+72\n",
      "Gradient Descent(45/49): loss=1.7505977986470144e+74\n",
      "Gradient Descent(46/49): loss=8.480342129340346e+75\n",
      "Gradient Descent(47/49): loss=4.1080939714563077e+77\n",
      "Gradient Descent(48/49): loss=1.9900654738829923e+79\n",
      "Gradient Descent(49/49): loss=9.640384611107726e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7723442317587987\n",
      "Gradient Descent(2/49): loss=64.12353662239622\n",
      "Gradient Descent(3/49): loss=3099.817183042229\n",
      "Gradient Descent(4/49): loss=152489.14831492366\n",
      "Gradient Descent(5/49): loss=7524845.227445831\n",
      "Gradient Descent(6/49): loss=371637345.0233693\n",
      "Gradient Descent(7/49): loss=18358842530.75397\n",
      "Gradient Descent(8/49): loss=906988213401.8331\n",
      "Gradient Descent(9/49): loss=44809171448455.266\n",
      "Gradient Descent(10/49): loss=2213781783441213.2\n",
      "Gradient Descent(11/49): loss=1.0937132625094037e+17\n",
      "Gradient Descent(12/49): loss=5.403465404097362e+18\n",
      "Gradient Descent(13/49): loss=2.6695701592998642e+20\n",
      "Gradient Descent(14/49): loss=1.3188953064115299e+22\n",
      "Gradient Descent(15/49): loss=6.515973541820211e+23\n",
      "Gradient Descent(16/49): loss=3.219202552247199e+25\n",
      "Gradient Descent(17/49): loss=1.5904400191233478e+27\n",
      "Gradient Descent(18/49): loss=7.857534324862513e+28\n",
      "Gradient Descent(19/49): loss=3.8819977446659013e+30\n",
      "Gradient Descent(20/49): loss=1.917892543257007e+32\n",
      "Gradient Descent(21/49): loss=9.475306400080534e+33\n",
      "Gradient Descent(22/49): loss=4.681254520296066e+35\n",
      "Gradient Descent(23/49): loss=2.3127636150766275e+37\n",
      "Gradient Descent(24/49): loss=1.1426158342882015e+39\n",
      "Gradient Descent(25/49): loss=5.645068679978167e+40\n",
      "Gradient Descent(26/49): loss=2.788933904589414e+42\n",
      "Gradient Descent(27/49): loss=1.377866730258897e+44\n",
      "Gradient Descent(28/49): loss=6.807320615344038e+45\n",
      "Gradient Descent(29/49): loss=3.36314194562061e+47\n",
      "Gradient Descent(30/49): loss=1.6615529641571275e+49\n",
      "Gradient Descent(31/49): loss=8.208866284381162e+50\n",
      "Gradient Descent(32/49): loss=4.055572535362029e+52\n",
      "Gradient Descent(33/49): loss=2.003646791138192e+54\n",
      "Gradient Descent(34/49): loss=9.89897339681042e+55\n",
      "Gradient Descent(35/49): loss=4.8905662786551466e+57\n",
      "Gradient Descent(36/49): loss=2.4161736340887025e+59\n",
      "Gradient Descent(37/49): loss=1.1937053292877104e+61\n",
      "Gradient Descent(38/49): loss=5.897475218941853e+62\n",
      "Gradient Descent(39/49): loss=2.913634806237044e+64\n",
      "Gradient Descent(40/49): loss=1.4394749395215195e+66\n",
      "Gradient Descent(41/49): loss=7.11169463336618e+67\n",
      "Gradient Descent(42/49): loss=3.513517267279456e+69\n",
      "Gradient Descent(43/49): loss=1.7358455647901957e+71\n",
      "Gradient Descent(44/49): loss=8.575907262111987e+72\n",
      "Gradient Descent(45/49): loss=4.2369083321784257e+74\n",
      "Gradient Descent(46/49): loss=2.0932353471907946e+76\n",
      "Gradient Descent(47/49): loss=1.0341583709638861e+78\n",
      "Gradient Descent(48/49): loss=5.109236941130236e+79\n",
      "Gradient Descent(49/49): loss=2.5242073993250423e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.767821241789758\n",
      "Gradient Descent(2/49): loss=63.67403941456192\n",
      "Gradient Descent(3/49): loss=3078.497815210156\n",
      "Gradient Descent(4/49): loss=151657.8473804931\n",
      "Gradient Descent(5/49): loss=7497820.096896567\n",
      "Gradient Descent(6/49): loss=371048992.36293805\n",
      "Gradient Descent(7/49): loss=18367604052.888714\n",
      "Gradient Descent(8/49): loss=909307980103.4891\n",
      "Gradient Descent(9/49): loss=45017425073378.18\n",
      "Gradient Descent(10/49): loss=2228710432785549.2\n",
      "Gradient Descent(11/49): loss=1.1033864521947795e+17\n",
      "Gradient Descent(12/49): loss=5.462632076025292e+18\n",
      "Gradient Descent(13/49): loss=2.7044337345779977e+20\n",
      "Gradient Descent(14/49): loss=1.3389080908958475e+22\n",
      "Gradient Descent(15/49): loss=6.628651635229924e+23\n",
      "Gradient Descent(16/49): loss=3.281705671156701e+25\n",
      "Gradient Descent(17/49): loss=1.624703291575052e+27\n",
      "Gradient Descent(18/49): loss=8.043563474346506e+28\n",
      "Gradient Descent(19/49): loss=3.982198701221381e+30\n",
      "Gradient Descent(20/49): loss=1.9715026240942687e+32\n",
      "Gradient Descent(21/49): loss=9.760493859014383e+33\n",
      "Gradient Descent(22/49): loss=4.8322147385435364e+35\n",
      "Gradient Descent(23/49): loss=2.392327644144917e+37\n",
      "Gradient Descent(24/49): loss=1.1843909814875326e+39\n",
      "Gradient Descent(25/49): loss=5.863670055656413e+40\n",
      "Gradient Descent(26/49): loss=2.902979426474546e+42\n",
      "Gradient Descent(27/49): loss=1.4372039133417893e+44\n",
      "Gradient Descent(28/49): loss=7.115293583163416e+45\n",
      "Gradient Descent(29/49): loss=3.522631848175772e+47\n",
      "Gradient Descent(30/49): loss=1.7439807637937798e+49\n",
      "Gradient Descent(31/49): loss=8.634081094956813e+50\n",
      "Gradient Descent(32/49): loss=4.2745515261374515e+52\n",
      "Gradient Descent(33/49): loss=2.1162403443577497e+54\n",
      "Gradient Descent(34/49): loss=1.0477059798444408e+56\n",
      "Gradient Descent(35/49): loss=5.1869714284978056e+57\n",
      "Gradient Descent(36/49): loss=2.5679602023506015e+59\n",
      "Gradient Descent(37/49): loss=1.2713429583641149e+61\n",
      "Gradient Descent(38/49): loss=6.294150961928901e+62\n",
      "Gradient Descent(39/49): loss=3.1161014477577554e+64\n",
      "Gradient Descent(40/49): loss=1.5427161330338073e+66\n",
      "Gradient Descent(41/49): loss=7.637662338738453e+67\n",
      "Gradient Descent(42/49): loss=3.781245606466051e+69\n",
      "Gradient Descent(43/49): loss=1.8720149834197132e+71\n",
      "Gradient Descent(44/49): loss=9.267951524109393e+72\n",
      "Gradient Descent(45/49): loss=4.58836741233409e+74\n",
      "Gradient Descent(46/49): loss=2.2716039737370854e+76\n",
      "Gradient Descent(47/49): loss=1.1246232373691148e+78\n",
      "Gradient Descent(48/49): loss=5.56777255478147e+79\n",
      "Gradient Descent(49/49): loss=2.756486811912026e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7712440554601672\n",
      "Gradient Descent(2/49): loss=63.3856682508265\n",
      "Gradient Descent(3/49): loss=3036.08590393076\n",
      "Gradient Descent(4/49): loss=148101.7074592465\n",
      "Gradient Descent(5/49): loss=7248904.353866222\n",
      "Gradient Descent(6/49): loss=355129798.36510414\n",
      "Gradient Descent(7/49): loss=17402843242.368988\n",
      "Gradient Descent(8/49): loss=852881622695.1969\n",
      "Gradient Descent(9/49): loss=41799202826411.664\n",
      "Gradient Descent(10/49): loss=2048568432242663.2\n",
      "Gradient Descent(11/49): loss=1.0040005072060584e+17\n",
      "Gradient Descent(12/49): loss=4.920595724322144e+18\n",
      "Gradient Descent(13/49): loss=2.411579189202859e+20\n",
      "Gradient Descent(14/49): loss=1.1819126943415664e+22\n",
      "Gradient Descent(15/49): loss=5.792543117908608e+23\n",
      "Gradient Descent(16/49): loss=2.8389200087701687e+25\n",
      "Gradient Descent(17/49): loss=1.3913520652656567e+27\n",
      "Gradient Descent(18/49): loss=6.819003580801298e+28\n",
      "Gradient Descent(19/49): loss=3.341987337586024e+30\n",
      "Gradient Descent(20/49): loss=1.6379048980957025e+32\n",
      "Gradient Descent(21/49): loss=8.027356731947086e+33\n",
      "Gradient Descent(22/49): loss=3.934200097767677e+35\n",
      "Gradient Descent(23/49): loss=1.9281478232675827e+37\n",
      "Gradient Descent(24/49): loss=9.449834619450475e+38\n",
      "Gradient Descent(25/49): loss=4.63135519265539e+40\n",
      "Gradient Descent(26/49): loss=2.2698228894278963e+42\n",
      "Gradient Descent(27/49): loss=1.112438095342223e+44\n",
      "Gradient Descent(28/49): loss=5.45204879963367e+45\n",
      "Gradient Descent(29/49): loss=2.6720440659165394e+47\n",
      "Gradient Descent(30/49): loss=1.309566321321173e+49\n",
      "Gradient Descent(31/49): loss=6.418172408958481e+50\n",
      "Gradient Descent(32/49): loss=3.1455403518286845e+52\n",
      "Gradient Descent(33/49): loss=1.541626412399253e+54\n",
      "Gradient Descent(34/49): loss=7.555496765524942e+55\n",
      "Gradient Descent(35/49): loss=3.70294196536338e+57\n",
      "Gradient Descent(36/49): loss=1.8148084268151424e+59\n",
      "Gradient Descent(37/49): loss=8.894359287416053e+60\n",
      "Gradient Descent(38/49): loss=4.359117247018517e+62\n",
      "Gradient Descent(39/49): loss=2.136399324472825e+64\n",
      "Gradient Descent(40/49): loss=1.0470473297614796e+66\n",
      "Gradient Descent(41/49): loss=5.1315692633031937e+67\n",
      "Gradient Descent(42/49): loss=2.5149773420535717e+69\n",
      "Gradient Descent(43/49): loss=1.232588065462719e+71\n",
      "Gradient Descent(44/49): loss=6.040902690123603e+72\n",
      "Gradient Descent(45/49): loss=2.9606408121307856e+74\n",
      "Gradient Descent(46/49): loss=1.4510073192844388e+76\n",
      "Gradient Descent(47/49): loss=7.111373429665487e+77\n",
      "Gradient Descent(48/49): loss=3.485277529895005e+79\n",
      "Gradient Descent(49/49): loss=1.708131288636666e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.782689298547684\n",
      "Gradient Descent(2/49): loss=64.46237870363115\n",
      "Gradient Descent(3/49): loss=3114.53120675429\n",
      "Gradient Descent(4/49): loss=153244.9248210631\n",
      "Gradient Descent(5/49): loss=7566544.001235136\n",
      "Gradient Descent(6/49): loss=373972476.50092196\n",
      "Gradient Descent(7/49): loss=18488942880.750496\n",
      "Gradient Descent(8/49): loss=914165150859.5361\n",
      "Gradient Descent(9/49): loss=45201170125544.78\n",
      "Gradient Descent(10/49): loss=2235005289906260.5\n",
      "Gradient Descent(11/49): loss=1.105117951082273e+17\n",
      "Gradient Descent(12/49): loss=5.464357247031447e+18\n",
      "Gradient Descent(13/49): loss=2.701902369185354e+20\n",
      "Gradient Descent(14/49): loss=1.3359809194916072e+22\n",
      "Gradient Descent(15/49): loss=6.605882891008075e+23\n",
      "Gradient Descent(16/49): loss=3.2663407451277184e+25\n",
      "Gradient Descent(17/49): loss=1.6150728170606454e+27\n",
      "Gradient Descent(18/49): loss=7.985879030922272e+28\n",
      "Gradient Descent(19/49): loss=3.9486927919454977e+30\n",
      "Gradient Descent(20/49): loss=1.95246818865652e+32\n",
      "Gradient Descent(21/49): loss=9.654162095316134e+33\n",
      "Gradient Descent(22/49): loss=4.773591001625966e+35\n",
      "Gradient Descent(23/49): loss=2.360346845834652e+37\n",
      "Gradient Descent(24/49): loss=1.1670956373823974e+39\n",
      "Gradient Descent(25/49): loss=5.770813849672401e+40\n",
      "Gradient Descent(26/49): loss=2.8534330367530594e+42\n",
      "Gradient Descent(27/49): loss=1.4109067295068191e+44\n",
      "Gradient Descent(28/49): loss=6.976360663549455e+45\n",
      "Gradient Descent(29/49): loss=3.449526966600602e+47\n",
      "Gradient Descent(30/49): loss=1.7056509643311018e+49\n",
      "Gradient Descent(31/49): loss=8.433751178906072e+50\n",
      "Gradient Descent(32/49): loss=4.170147376875177e+52\n",
      "Gradient Descent(33/49): loss=2.0619684854295872e+54\n",
      "Gradient Descent(34/49): loss=1.019559658366495e+56\n",
      "Gradient Descent(35/49): loss=5.0413083629250656e+57\n",
      "Gradient Descent(36/49): loss=2.492722206252923e+59\n",
      "Gradient Descent(37/49): loss=1.2325498759891683e+61\n",
      "Gradient Descent(38/49): loss=6.094458471907115e+62\n",
      "Gradient Descent(39/49): loss=3.0134621559222666e+64\n",
      "Gradient Descent(40/49): loss=1.4900346285129404e+66\n",
      "Gradient Descent(41/49): loss=7.367615982183202e+67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=3.642986828775542e+69\n",
      "Gradient Descent(43/49): loss=1.8013090077883622e+71\n",
      "Gradient Descent(44/49): loss=8.90674134726441e+72\n",
      "Gradient Descent(45/49): loss=4.404021802148801e+74\n",
      "Gradient Descent(46/49): loss=2.1776098886894366e+76\n",
      "Gradient Descent(47/49): loss=1.0767396348956087e+78\n",
      "Gradient Descent(48/49): loss=5.324040120211266e+79\n",
      "Gradient Descent(49/49): loss=2.6325215755958627e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8014382037840369\n",
      "Gradient Descent(2/49): loss=66.79530711947721\n",
      "Gradient Descent(3/49): loss=3296.31524196073\n",
      "Gradient Descent(4/49): loss=165506.9194734372\n",
      "Gradient Descent(5/49): loss=8336111.275411438\n",
      "Gradient Descent(6/49): loss=420219828.95116925\n",
      "Gradient Descent(7/49): loss=21188231215.107574\n",
      "Gradient Descent(8/49): loss=1068423924330.5468\n",
      "Gradient Descent(9/49): loss=53876769540353.52\n",
      "Gradient Descent(10/49): loss=2716828076899400.0\n",
      "Gradient Descent(11/49): loss=1.3700094185586755e+17\n",
      "Gradient Descent(12/49): loss=6.908522576953613e+18\n",
      "Gradient Descent(13/49): loss=3.483749168835598e+20\n",
      "Gradient Descent(14/49): loss=1.7567444706086028e+22\n",
      "Gradient Descent(15/49): loss=8.858706605803222e+23\n",
      "Gradient Descent(16/49): loss=4.467165507539557e+25\n",
      "Gradient Descent(17/49): loss=2.2526502551314164e+27\n",
      "Gradient Descent(18/49): loss=1.135940265740552e+29\n",
      "Gradient Descent(19/49): loss=5.72818742956808e+30\n",
      "Gradient Descent(20/49): loss=2.8885437217848806e+32\n",
      "Gradient Descent(21/49): loss=1.4566012260126206e+34\n",
      "Gradient Descent(22/49): loss=7.345179218253782e+35\n",
      "Gradient Descent(23/49): loss=3.703941530792215e+37\n",
      "Gradient Descent(24/49): loss=1.8677805477416472e+39\n",
      "Gradient Descent(25/49): loss=9.418626469992347e+40\n",
      "Gradient Descent(26/49): loss=4.7495153907937937e+42\n",
      "Gradient Descent(27/49): loss=2.3950303708567767e+44\n",
      "Gradient Descent(28/49): loss=1.2077380543802487e+46\n",
      "Gradient Descent(29/49): loss=6.090240966240451e+47\n",
      "Gradient Descent(30/49): loss=3.0711158675799574e+49\n",
      "Gradient Descent(31/49): loss=1.548666583864861e+51\n",
      "Gradient Descent(32/49): loss=7.809435695011952e+52\n",
      "Gradient Descent(33/49): loss=3.938051386265914e+54\n",
      "Gradient Descent(34/49): loss=1.9858347422946605e+56\n",
      "Gradient Descent(35/49): loss=1.0013936429214092e+58\n",
      "Gradient Descent(36/49): loss=5.049711371877149e+59\n",
      "Gradient Descent(37/49): loss=2.546409708061899e+61\n",
      "Gradient Descent(38/49): loss=1.2840738655725286e+63\n",
      "Gradient Descent(39/49): loss=6.475178314888451e+64\n",
      "Gradient Descent(40/49): loss=3.2652275958367163e+66\n",
      "Gradient Descent(41/49): loss=1.64655098811705e+68\n",
      "Gradient Descent(42/49): loss=8.303035782026383e+69\n",
      "Gradient Descent(43/49): loss=4.18695829616846e+71\n",
      "Gradient Descent(44/49): loss=2.1113506233229078e+73\n",
      "Gradient Descent(45/49): loss=1.0646873312985741e+75\n",
      "Gradient Descent(46/49): loss=5.368881420764e+76\n",
      "Gradient Descent(47/49): loss=2.7073570674563996e+78\n",
      "Gradient Descent(48/49): loss=1.3652345276910682e+80\n",
      "Gradient Descent(49/49): loss=6.884445860519551e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7967772895574632\n",
      "Gradient Descent(2/49): loss=66.32718616852708\n",
      "Gradient Descent(3/49): loss=3273.635673479206\n",
      "Gradient Descent(4/49): loss=164603.3072173407\n",
      "Gradient Descent(5/49): loss=8306071.038899401\n",
      "Gradient Descent(6/49): loss=419548074.6875545\n",
      "Gradient Descent(7/49): loss=21197953946.96787\n",
      "Gradient Descent(8/49): loss=1071133995042.201\n",
      "Gradient Descent(9/49): loss=54125878733390.07\n",
      "Gradient Descent(10/49): loss=2735076735940063.0\n",
      "Gradient Descent(11/49): loss=1.3820860822607405e+17\n",
      "Gradient Descent(12/49): loss=6.983947719324507e+18\n",
      "Gradient Descent(13/49): loss=3.529124327827785e+20\n",
      "Gradient Descent(14/49): loss=1.7833351311615072e+22\n",
      "Gradient Descent(15/49): loss=9.011539283373275e+23\n",
      "Gradient Descent(16/49): loss=4.553706104382003e+25\n",
      "Gradient Descent(17/49): loss=2.301076283355533e+27\n",
      "Gradient Descent(18/49): loss=1.1627786122441972e+29\n",
      "Gradient Descent(19/49): loss=5.87574654124987e+30\n",
      "Gradient Descent(20/49): loss=2.969129037558779e+32\n",
      "Gradient Descent(21/49): loss=1.5003586659009988e+34\n",
      "Gradient Descent(22/49): loss=7.58160422763184e+35\n",
      "Gradient Descent(23/49): loss=3.831132113333817e+37\n",
      "Gradient Descent(24/49): loss=1.935945590028258e+39\n",
      "Gradient Descent(25/49): loss=9.78270969697499e+40\n",
      "Gradient Descent(26/49): loss=4.9433935286319013e+42\n",
      "Gradient Descent(27/49): loss=2.4979929217849118e+44\n",
      "Gradient Descent(28/49): loss=1.262284420842872e+46\n",
      "Gradient Descent(29/49): loss=6.378568750963889e+47\n",
      "Gradient Descent(30/49): loss=3.2232148823959874e+49\n",
      "Gradient Descent(31/49): loss=1.628753186446257e+51\n",
      "Gradient Descent(32/49): loss=8.230406718607731e+52\n",
      "Gradient Descent(33/49): loss=4.158984634222118e+54\n",
      "Gradient Descent(34/49): loss=2.101615847074656e+56\n",
      "Gradient Descent(35/49): loss=1.0619873736324684e+58\n",
      "Gradient Descent(36/49): loss=5.366428804410875e+59\n",
      "Gradient Descent(37/49): loss=2.711760876620134e+61\n",
      "Gradient Descent(38/49): loss=1.370305527192185e+63\n",
      "Gradient Descent(39/49): loss=6.924420416426287e+64\n",
      "Gradient Descent(40/49): loss=3.4990443482825345e+66\n",
      "Gradient Descent(41/49): loss=1.768135181711964e+68\n",
      "Gradient Descent(42/49): loss=8.934731056901603e+69\n",
      "Gradient Descent(43/49): loss=4.514893424713626e+71\n",
      "Gradient Descent(44/49): loss=2.281463482974869e+73\n",
      "Gradient Descent(45/49): loss=1.152867882917507e+75\n",
      "Gradient Descent(46/49): loss=5.825665698272067e+76\n",
      "Gradient Descent(47/49): loss=2.9438222133604493e+78\n",
      "Gradient Descent(48/49): loss=1.487570635308649e+80\n",
      "Gradient Descent(49/49): loss=7.516983821202072e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8003327598512915\n",
      "Gradient Descent(2/49): loss=66.02826271962368\n",
      "Gradient Descent(3/49): loss=3228.653120955084\n",
      "Gradient Descent(4/49): loss=160751.73894673336\n",
      "Gradient Descent(5/49): loss=8030835.959432666\n",
      "Gradient Descent(6/49): loss=401579022.64134544\n",
      "Gradient Descent(7/49): loss=20086333992.067776\n",
      "Gradient Descent(8/49): loss=1004768892105.0034\n",
      "Gradient Descent(9/49): loss=50262318019331.45\n",
      "Gradient Descent(10/49): loss=2514329143987678.5\n",
      "Gradient Descent(11/49): loss=1.2577743706808186e+17\n",
      "Gradient Descent(12/49): loss=6.291926705057915e+18\n",
      "Gradient Descent(13/49): loss=3.147492185330791e+20\n",
      "Gradient Descent(14/49): loss=1.5745109815411715e+22\n",
      "Gradient Descent(15/49): loss=7.876381364261944e+23\n",
      "Gradient Descent(16/49): loss=3.940104863628007e+25\n",
      "Gradient Descent(17/49): loss=1.9710099913000177e+27\n",
      "Gradient Descent(18/49): loss=9.859840086449573e+28\n",
      "Gradient Descent(19/49): loss=4.9323162731570327e+30\n",
      "Gradient Descent(20/49): loss=2.467356834022628e+32\n",
      "Gradient Descent(21/49): loss=1.234278057070805e+34\n",
      "Gradient Descent(22/49): loss=6.1743899429812394e+35\n",
      "Gradient Descent(23/49): loss=3.08869552931334e+37\n",
      "Gradient Descent(24/49): loss=1.5450984082482522e+39\n",
      "Gradient Descent(25/49): loss=7.729247083484213e+40\n",
      "Gradient Descent(26/49): loss=3.866501975449109e+42\n",
      "Gradient Descent(27/49): loss=1.934190661092555e+44\n",
      "Gradient Descent(28/49): loss=9.675653955984626e+45\n",
      "Gradient Descent(29/49): loss=4.840178445649183e+47\n",
      "Gradient Descent(30/49): loss=2.4212655281286255e+49\n",
      "Gradient Descent(31/49): loss=1.2112212025930124e+51\n",
      "Gradient Descent(32/49): loss=6.059049635686797e+52\n",
      "Gradient Descent(33/49): loss=3.0309973445908904e+54\n",
      "Gradient Descent(34/49): loss=1.5162352935363734e+56\n",
      "Gradient Descent(35/49): loss=7.584861364092153e+57\n",
      "Gradient Descent(36/49): loss=3.7942740257891363e+59\n",
      "Gradient Descent(37/49): loss=1.8980591327526716e+61\n",
      "Gradient Descent(38/49): loss=9.494908504075536e+62\n",
      "Gradient Descent(39/49): loss=4.749761793249345e+64\n",
      "Gradient Descent(40/49): loss=2.3760352280306495e+66\n",
      "Gradient Descent(41/49): loss=1.188595060254697e+68\n",
      "Gradient Descent(42/49): loss=5.945863935834038e+69\n",
      "Gradient Descent(43/49): loss=2.974376987220201e+71\n",
      "Gradient Descent(44/49): loss=1.4879113544437558e+73\n",
      "Gradient Descent(45/49): loss=7.443172833150858e+74\n",
      "Gradient Descent(46/49): loss=3.723395325850319e+76\n",
      "Gradient Descent(47/49): loss=1.862602557180614e+78\n",
      "Gradient Descent(48/49): loss=9.317539456338701e+79\n",
      "Gradient Descent(49/49): loss=4.661034163500848e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8116889618685001\n",
      "Gradient Descent(2/49): loss=67.12410824614955\n",
      "Gradient Descent(3/49): loss=3310.15803686689\n",
      "Gradient Descent(4/49): loss=166205.9322793898\n",
      "Gradient Descent(5/49): loss=8374651.359625697\n",
      "Gradient Descent(6/49): loss=422396535.5468739\n",
      "Gradient Descent(7/49): loss=21311077940.631805\n",
      "Gradient Descent(8/49): loss=1075303456005.3932\n",
      "Gradient Descent(9/49): loss=54258683729420.8\n",
      "Gradient Descent(10/49): loss=2737860921571431.5\n",
      "Gradient Descent(11/49): loss=1.3815122234097322e+17\n",
      "Gradient Descent(12/49): loss=6.971054221634351e+18\n",
      "Gradient Descent(13/49): loss=3.5175664310364806e+20\n",
      "Gradient Descent(14/49): loss=1.774950280785961e+22\n",
      "Gradient Descent(15/49): loss=8.956330020890485e+23\n",
      "Gradient Descent(16/49): loss=4.519329300279497e+25\n",
      "Gradient Descent(17/49): loss=2.2804359965789096e+27\n",
      "Gradient Descent(18/49): loss=1.1506991398785491e+29\n",
      "Gradient Descent(19/49): loss=5.806383133901803e+30\n",
      "Gradient Descent(20/49): loss=2.929878361066731e+32\n",
      "Gradient Descent(21/49): loss=1.4784052331535923e+34\n",
      "Gradient Descent(22/49): loss=7.459975343947946e+35\n",
      "Gradient Descent(23/49): loss=3.76427456318812e+37\n",
      "Gradient Descent(24/49): loss=1.8994383136361053e+39\n",
      "Gradient Descent(25/49): loss=9.58449190341197e+40\n",
      "Gradient Descent(26/49): loss=4.836297361545986e+42\n",
      "Gradient Descent(27/49): loss=2.4403768509596855e+44\n",
      "Gradient Descent(28/49): loss=1.2314046737598046e+46\n",
      "Gradient Descent(29/49): loss=6.213620121667602e+47\n",
      "Gradient Descent(30/49): loss=3.135368562351552e+49\n",
      "Gradient Descent(31/49): loss=1.582094790040719e+51\n",
      "Gradient Descent(32/49): loss=7.983188817829727e+52\n",
      "Gradient Descent(33/49): loss=4.0282860484915204e+54\n",
      "Gradient Descent(34/49): loss=2.032657482963416e+56\n",
      "Gradient Descent(35/49): loss=1.0256710653888069e+58\n",
      "Gradient Descent(36/49): loss=5.17549633026268e+59\n",
      "Gradient Descent(37/49): loss=2.611535332178703e+61\n",
      "Gradient Descent(38/49): loss=1.3177705781257026e+63\n",
      "Gradient Descent(39/49): loss=6.649419118235838e+64\n",
      "Gradient Descent(40/49): loss=3.3552710421603436e+66\n",
      "Gradient Descent(41/49): loss=1.69305672663728e+68\n",
      "Gradient Descent(42/49): loss=8.543098437037554e+69\n",
      "Gradient Descent(43/49): loss=4.3108142660921955e+71\n",
      "Gradient Descent(44/49): loss=2.1752201234366133e+73\n",
      "Gradient Descent(45/49): loss=1.0976076196604166e+75\n",
      "Gradient Descent(46/49): loss=5.53848538709376e+76\n",
      "Gradient Descent(47/49): loss=2.7946981993930965e+78\n",
      "Gradient Descent(48/49): loss=1.4101938490063212e+80\n",
      "Gradient Descent(49/49): loss=7.115783350800227e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8308470224410378\n",
      "Gradient Descent(2/49): loss=69.55024693846984\n",
      "Gradient Descent(3/49): loss=3503.0876184325443\n",
      "Gradient Descent(4/49): loss=179486.9213836214\n",
      "Gradient Descent(5/49): loss=9225263.836391428\n",
      "Gradient Descent(6/49): loss=474562032.0452762\n",
      "Gradient Descent(7/49): loss=24418177512.096157\n",
      "Gradient Descent(8/49): loss=1256506037803.8333\n",
      "Gradient Descent(9/49): loss=64658417800662.86\n",
      "Gradient Descent(10/49): loss=3327271751607403.5\n",
      "Gradient Descent(11/49): loss=1.7121912485262506e+17\n",
      "Gradient Descent(12/49): loss=8.810823667226814e+18\n",
      "Gradient Descent(13/49): loss=4.5339926998273825e+20\n",
      "Gradient Descent(14/49): loss=2.3331633408464795e+22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=1.2006307932817484e+24\n",
      "Gradient Descent(16/49): loss=6.178368580380726e+25\n",
      "Gradient Descent(17/49): loss=3.179348603735129e+27\n",
      "Gradient Descent(18/49): loss=1.63607227640445e+29\n",
      "Gradient Descent(19/49): loss=8.419122366022465e+30\n",
      "Gradient Descent(20/49): loss=4.332426044899682e+32\n",
      "Gradient Descent(21/49): loss=2.2294384876093296e+34\n",
      "Gradient Descent(22/49): loss=1.1472546602149223e+36\n",
      "Gradient Descent(23/49): loss=5.903698454570643e+37\n",
      "Gradient Descent(24/49): loss=3.038005130960122e+39\n",
      "Gradient Descent(25/49): loss=1.5633378375881021e+41\n",
      "Gradient Descent(26/49): loss=8.044835637464135e+42\n",
      "Gradient Descent(27/49): loss=4.13982050953631e+44\n",
      "Gradient Descent(28/49): loss=2.1303249219122239e+46\n",
      "Gradient Descent(29/49): loss=1.0962514588413014e+48\n",
      "Gradient Descent(30/49): loss=5.641239271298369e+49\n",
      "Gradient Descent(31/49): loss=2.9029453287729516e+51\n",
      "Gradient Descent(32/49): loss=1.4938369348593775e+53\n",
      "Gradient Descent(33/49): loss=7.687188476585337e+54\n",
      "Gradient Descent(34/49): loss=3.955777588275327e+56\n",
      "Gradient Descent(35/49): loss=2.0356176221728673e+58\n",
      "Gradient Descent(36/49): loss=1.0475156934966445e+60\n",
      "Gradient Descent(37/49): loss=5.390448167522145e+61\n",
      "Gradient Descent(38/49): loss=2.7738898450055657e+63\n",
      "Gradient Descent(39/49): loss=1.4274258156464145e+65\n",
      "Gradient Descent(40/49): loss=7.345441142309458e+66\n",
      "Gradient Descent(41/49): loss=3.779916615190149e+68\n",
      "Gradient Descent(42/49): loss=1.9451207001705607e+70\n",
      "Gradient Descent(43/49): loss=1.0009465613679095e+72\n",
      "Gradient Descent(44/49): loss=5.150806418472548e+73\n",
      "Gradient Descent(45/49): loss=2.6505717472390056e+75\n",
      "Gradient Descent(46/49): loss=1.36396711824884e+77\n",
      "Gradient Descent(47/49): loss=7.018886780189798e+78\n",
      "Gradient Descent(48/49): loss=3.611873847543545e+80\n",
      "Gradient Descent(49/49): loss=1.858646976239769e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8260464486605004\n",
      "Gradient Descent(2/49): loss=69.0629327428384\n",
      "Gradient Descent(3/49): loss=3478.976368246821\n",
      "Gradient Descent(4/49): loss=178505.5571705984\n",
      "Gradient Descent(5/49): loss=9191909.460548554\n",
      "Gradient Descent(6/49): loss=473796229.25527424\n",
      "Gradient Descent(7/49): loss=24428943085.710773\n",
      "Gradient Descent(8/49): loss=1259667147687.1309\n",
      "Gradient Descent(9/49): loss=64955864939134.73\n",
      "Gradient Descent(10/49): loss=3349533959317693.0\n",
      "Gradient Descent(11/49): loss=1.7272350419149504e+17\n",
      "Gradient Descent(12/49): loss=8.906740703414095e+18\n",
      "Gradient Descent(13/49): loss=4.5928924162332203e+20\n",
      "Gradient Descent(14/49): loss=2.368392977529411e+22\n",
      "Gradient Descent(15/49): loss=1.2212969299089393e+24\n",
      "Gradient Descent(16/49): loss=6.297798608973601e+25\n",
      "Gradient Descent(17/49): loss=3.247553188710012e+27\n",
      "Gradient Descent(18/49): loss=1.6746489327596267e+29\n",
      "Gradient Descent(19/49): loss=8.635575417837808e+30\n",
      "Gradient Descent(20/49): loss=4.453062450407457e+32\n",
      "Gradient Descent(21/49): loss=2.296287650567599e+34\n",
      "Gradient Descent(22/49): loss=1.184114759875113e+36\n",
      "Gradient Descent(23/49): loss=6.10606325479055e+37\n",
      "Gradient Descent(24/49): loss=3.1486820141867865e+39\n",
      "Gradient Descent(25/49): loss=1.6236645466596344e+41\n",
      "Gradient Descent(26/49): loss=8.372666875224034e+42\n",
      "Gradient Descent(27/49): loss=4.3174897639844885e+44\n",
      "Gradient Descent(28/49): loss=2.226377585530305e+46\n",
      "Gradient Descent(29/49): loss=1.1480645987165701e+48\n",
      "Gradient Descent(30/49): loss=5.920165255851194e+49\n",
      "Gradient Descent(31/49): loss=3.0528209558737787e+51\n",
      "Gradient Descent(32/49): loss=1.5742323712012883e+53\n",
      "Gradient Descent(33/49): loss=8.117762536220269e+54\n",
      "Gradient Descent(34/49): loss=4.186044563686289e+56\n",
      "Gradient Descent(35/49): loss=2.158595919871092e+58\n",
      "Gradient Descent(36/49): loss=1.1131119782396363e+60\n",
      "Gradient Descent(37/49): loss=5.739926888097498e+61\n",
      "Gradient Descent(38/49): loss=2.9598783702614752e+63\n",
      "Gradient Descent(39/49): loss=1.526305149445121e+65\n",
      "Gradient Descent(40/49): loss=7.870618714028124e+66\n",
      "Gradient Descent(41/49): loss=4.0586011888992084e+68\n",
      "Gradient Descent(42/49): loss=2.092877854847026e+70\n",
      "Gradient Descent(43/49): loss=1.0792234840144736e+72\n",
      "Gradient Descent(44/49): loss=5.5651758450732525e+73\n",
      "Gradient Descent(45/49): loss=2.869765404972536e+75\n",
      "Gradient Descent(46/49): loss=1.479837063345991e+77\n",
      "Gradient Descent(47/49): loss=7.63099914110727e+78\n",
      "Gradient Descent(48/49): loss=3.935037804764389e+80\n",
      "Gradient Descent(49/49): loss=2.029160564507434e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8297363938003237\n",
      "Gradient Descent(2/49): loss=68.75317351565907\n",
      "Gradient Descent(3/49): loss=3431.2948509196963\n",
      "Gradient Descent(4/49): loss=174337.3192123107\n",
      "Gradient Descent(5/49): loss=8887877.30548352\n",
      "Gradient Descent(6/49): loss=453537956.53731656\n",
      "Gradient Descent(7/49): loss=23149928908.587635\n",
      "Gradient Descent(8/49): loss=1181739914134.5342\n",
      "Gradient Descent(9/49): loss=60326081389138.17\n",
      "Gradient Descent(10/49): loss=3079581186253033.0\n",
      "Gradient Descent(11/49): loss=1.57209654193732e+17\n",
      "Gradient Descent(12/49): loss=8.025407158171509e+18\n",
      "Gradient Descent(13/49): loss=4.096896865305874e+20\n",
      "Gradient Descent(14/49): loss=2.0914284715314652e+22\n",
      "Gradient Descent(15/49): loss=1.0676551747935868e+24\n",
      "Gradient Descent(16/49): loss=5.450282401245148e+25\n",
      "Gradient Descent(17/49): loss=2.7823195175295317e+27\n",
      "Gradient Descent(18/49): loss=1.4203487694890072e+29\n",
      "Gradient Descent(19/49): loss=7.25075108959842e+30\n",
      "Gradient Descent(20/49): loss=3.7014423848137654e+32\n",
      "Gradient Descent(21/49): loss=1.8895526213783354e+34\n",
      "Gradient Descent(22/49): loss=9.64599401472134e+35\n",
      "Gradient Descent(23/49): loss=4.92419208014985e+37\n",
      "Gradient Descent(24/49): loss=2.5137552029594385e+39\n",
      "Gradient Descent(25/49): loss=1.2832491335744548e+41\n",
      "Gradient Descent(26/49): loss=6.550869937060568e+42\n",
      "Gradient Descent(27/49): loss=3.3441594316724007e+44\n",
      "Gradient Descent(28/49): loss=1.7071629282662332e+46\n",
      "Gradient Descent(29/49): loss=8.714911245092882e+47\n",
      "Gradient Descent(30/49): loss=4.448882807394393e+49\n",
      "Gradient Descent(31/49): loss=2.2711141487612984e+51\n",
      "Gradient Descent(32/49): loss=1.1593830855986584e+53\n",
      "Gradient Descent(33/49): loss=5.918545044974508e+54\n",
      "Gradient Descent(34/49): loss=3.0213633340445623e+56\n",
      "Gradient Descent(35/49): loss=1.5423784607435042e+58\n",
      "Gradient Descent(36/49): loss=7.873701548436197e+59\n",
      "Gradient Descent(37/49): loss=4.019452919743292e+61\n",
      "Gradient Descent(38/49): loss=2.0518941027478646e+63\n",
      "Gradient Descent(39/49): loss=1.0474732489615426e+65\n",
      "Gradient Descent(40/49): loss=5.347255522693356e+66\n",
      "Gradient Descent(41/49): loss=2.72972523673722e+68\n",
      "Gradient Descent(42/49): loss=1.3934998685693834e+70\n",
      "Gradient Descent(43/49): loss=7.113689896584357e+71\n",
      "Gradient Descent(44/49): loss=3.6314738943404835e+73\n",
      "Gradient Descent(45/49): loss=1.8538343443405484e+75\n",
      "Gradient Descent(46/49): loss=9.463655464004627e+76\n",
      "Gradient Descent(47/49): loss=4.831109910915132e+78\n",
      "Gradient Descent(48/49): loss=2.46623760344147e+80\n",
      "Gradient Descent(49/49): loss=1.2589918318535066e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.840999192982203\n",
      "Gradient Descent(2/49): loss=69.86795633243841\n",
      "Gradient Descent(3/49): loss=3515.9211634751355\n",
      "Gradient Descent(4/49): loss=180116.1865151325\n",
      "Gradient Descent(5/49): loss=9259621.631676994\n",
      "Gradient Descent(6/49): loss=476507650.6738772\n",
      "Gradient Descent(7/49): loss=24528955543.695644\n",
      "Gradient Descent(8/49): loss=1262784370340.2063\n",
      "Gradient Descent(9/49): loss=65011783293879.57\n",
      "Gradient Descent(10/49): loss=3347024809645986.0\n",
      "Gradient Descent(11/49): loss=1.723165628598427e+17\n",
      "Gradient Descent(12/49): loss=8.871468236134434e+18\n",
      "Gradient Descent(13/49): loss=4.567347981238558e+20\n",
      "Gradient Descent(14/49): loss=2.351433701581628e+22\n",
      "Gradient Descent(15/49): loss=1.2106020004475737e+24\n",
      "Gradient Descent(16/49): loss=6.2326112571529195e+25\n",
      "Gradient Descent(17/49): loss=3.208770774465089e+27\n",
      "Gradient Descent(18/49): loss=1.6519897466316274e+29\n",
      "Gradient Descent(19/49): loss=8.505032972117261e+30\n",
      "Gradient Descent(20/49): loss=4.3786946020767604e+32\n",
      "Gradient Descent(21/49): loss=2.254308299755452e+34\n",
      "Gradient Descent(22/49): loss=1.1605983911222941e+36\n",
      "Gradient Descent(23/49): loss=5.975174849088214e+37\n",
      "Gradient Descent(24/49): loss=3.076233325007387e+39\n",
      "Gradient Descent(25/49): loss=1.5837547367057087e+41\n",
      "Gradient Descent(26/49): loss=8.153734782233843e+42\n",
      "Gradient Descent(27/49): loss=4.197833752800661e+44\n",
      "Gradient Descent(28/49): loss=2.1611946778731098e+46\n",
      "Gradient Descent(29/49): loss=1.112660174441373e+48\n",
      "Gradient Descent(30/49): loss=5.728371795761903e+49\n",
      "Gradient Descent(31/49): loss=2.9491703023301957e+51\n",
      "Gradient Descent(32/49): loss=1.518338156503959e+53\n",
      "Gradient Descent(33/49): loss=7.816946873750721e+54\n",
      "Gradient Descent(34/49): loss=4.024443314244118e+56\n",
      "Gradient Descent(35/49): loss=2.0719270900958428e+58\n",
      "Gradient Descent(36/49): loss=1.0667020334163457e+60\n",
      "Gradient Descent(37/49): loss=5.491762878788998e+61\n",
      "Gradient Descent(38/49): loss=2.8273555849755754e+63\n",
      "Gradient Descent(39/49): loss=1.4556235912456812e+65\n",
      "Gradient Descent(40/49): loss=7.494069902810855e+66\n",
      "Gradient Descent(41/49): loss=3.8582147229528433e+68\n",
      "Gradient Descent(42/49): loss=1.9863466769674354e+70\n",
      "Gradient Descent(43/49): loss=1.0226421815320594e+72\n",
      "Gradient Descent(44/49): loss=5.2649270319986236e+73\n",
      "Gradient Descent(45/49): loss=2.7105723930478117e+75\n",
      "Gradient Descent(46/49): loss=1.3954994349017195e+77\n",
      "Gradient Descent(47/49): loss=7.184529281733429e+78\n",
      "Gradient Descent(48/49): loss=3.6988521606760904e+80\n",
      "Gradient Descent(49/49): loss=1.904301140691735e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8605706877298027\n",
      "Gradient Descent(2/49): loss=72.3900653408772\n",
      "Gradient Descent(3/49): loss=3720.559703320217\n",
      "Gradient Descent(4/49): loss=194489.48760564864\n",
      "Gradient Descent(5/49): loss=10198880.209783973\n",
      "Gradient Descent(6/49): loss=535278181.1396874\n",
      "Gradient Descent(7/49): loss=28100470058.911728\n",
      "Gradient Descent(8/49): loss=1475295565828.2627\n",
      "Gradient Descent(9/49): loss=77455768304584.84\n",
      "Gradient Descent(10/49): loss=4066597989927598.0\n",
      "Gradient Descent(11/49): loss=2.135057280030514e+17\n",
      "Gradient Descent(12/49): loss=1.1209546978330313e+19\n",
      "Gradient Descent(13/49): loss=5.8852736798212686e+20\n",
      "Gradient Descent(14/49): loss=3.0899061348625415e+22\n",
      "Gradient Descent(15/49): loss=1.6222729102968035e+24\n",
      "Gradient Descent(16/49): loss=8.517311827319142e+25\n",
      "Gradient Descent(17/49): loss=4.4717877242258507e+27\n",
      "Gradient Descent(18/49): loss=2.3477930435681505e+29\n",
      "Gradient Descent(19/49): loss=1.2326462068361669e+31\n",
      "Gradient Descent(20/49): loss=6.471680608478064e+32\n",
      "Gradient Descent(21/49): loss=3.3977835380731884e+34\n",
      "Gradient Descent(22/49): loss=1.7839157508036767e+36\n",
      "Gradient Descent(23/49): loss=9.365974525183707e+37\n",
      "Gradient Descent(24/49): loss=4.917355473031351e+39\n",
      "Gradient Descent(25/49): loss=2.5817265232929666e+41\n",
      "Gradient Descent(26/49): loss=1.3554667498881723e+43\n",
      "Gradient Descent(27/49): loss=7.116517158095273e+44\n",
      "Gradient Descent(28/49): loss=3.73633779402134e+46\n",
      "Gradient Descent(29/49): loss=1.9616646459078676e+48\n",
      "Gradient Descent(30/49): loss=1.0299197757660974e+50\n",
      "Gradient Descent(31/49): loss=5.407319475970764e+51\n",
      "Gradient Descent(32/49): loss=2.838969073437141e+53\n",
      "Gradient Descent(33/49): loss=1.4905250994968374e+55\n",
      "Gradient Descent(34/49): loss=7.825605051555868e+56\n",
      "Gradient Descent(35/49): loss=4.108625506783502e+58\n",
      "Gradient Descent(36/49): loss=2.157124394060206e+60\n",
      "Gradient Descent(37/49): loss=1.132540710699241e+62\n",
      "Gradient Descent(38/49): loss=5.9461033629910495e+63\n",
      "Gradient Descent(39/49): loss=3.121843203459237e+65\n",
      "Gradient Descent(40/49): loss=1.6390406274542538e+67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(41/49): loss=8.605346275779736e+68\n",
      "Gradient Descent(42/49): loss=4.5180078690967494e+70\n",
      "Gradient Descent(43/49): loss=2.372059699988159e+72\n",
      "Gradient Descent(44/49): loss=1.24538676853451e+74\n",
      "Gradient Descent(45/49): loss=6.538571534470972e+75\n",
      "Gradient Descent(46/49): loss=3.4329028372208415e+77\n",
      "Gradient Descent(47/49): loss=1.8023542034632405e+79\n",
      "Gradient Descent(48/49): loss=9.462780710017071e+80\n",
      "Gradient Descent(49/49): loss=4.968180982062838e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8556287190988698\n",
      "Gradient Descent(2/49): loss=71.88297691308773\n",
      "Gradient Descent(3/49): loss=3694.9423194800966\n",
      "Gradient Descent(4/49): loss=193424.58390921153\n",
      "Gradient Descent(5/49): loss=10161886.616230583\n",
      "Gradient Descent(6/49): loss=534406467.59965765\n",
      "Gradient Descent(7/49): loss=28112363702.033024\n",
      "Gradient Descent(8/49): loss=1478977150417.6577\n",
      "Gradient Descent(9/49): loss=77810310697373.14\n",
      "Gradient Descent(10/49): loss=4093703009607642.5\n",
      "Gradient Descent(11/49): loss=2.1537563554851046e+17\n",
      "Gradient Descent(12/49): loss=1.133123199428575e+19\n",
      "Gradient Descent(13/49): loss=5.96153073412121e+20\n",
      "Gradient Descent(14/49): loss=3.1364507497662106e+22\n",
      "Gradient Descent(15/49): loss=1.6501338237613194e+24\n",
      "Gradient Descent(16/49): loss=8.681601817811216e+25\n",
      "Gradient Descent(17/49): loss=4.5675210808875713e+27\n",
      "Gradient Descent(18/49): loss=2.4030414287463896e+29\n",
      "Gradient Descent(19/49): loss=1.2642761811032343e+31\n",
      "Gradient Descent(20/49): loss=6.651546840043339e+32\n",
      "Gradient Descent(21/49): loss=3.4994786761995844e+34\n",
      "Gradient Descent(22/49): loss=1.841128281846892e+36\n",
      "Gradient Descent(23/49): loss=9.686452365820859e+37\n",
      "Gradient Descent(24/49): loss=5.0961880473225677e+39\n",
      "Gradient Descent(25/49): loss=2.6811810591581763e+41\n",
      "Gradient Descent(26/49): loss=1.4106096174699963e+43\n",
      "Gradient Descent(27/49): loss=7.421429023236584e+44\n",
      "Gradient Descent(28/49): loss=3.9045252538206084e+46\n",
      "Gradient Descent(29/49): loss=2.0542293687630448e+48\n",
      "Gradient Descent(30/49): loss=1.0807609184649093e+50\n",
      "Gradient Descent(31/49): loss=5.686045485682333e+51\n",
      "Gradient Descent(32/49): loss=2.9915139151377563e+53\n",
      "Gradient Descent(33/49): loss=1.573880393147959e+55\n",
      "Gradient Descent(34/49): loss=8.280421091811964e+56\n",
      "Gradient Descent(35/49): loss=4.356453880245933e+58\n",
      "Gradient Descent(36/49): loss=2.2919958055607614e+60\n",
      "Gradient Descent(37/49): loss=1.2058534113097408e+62\n",
      "Gradient Descent(38/49): loss=6.3441758751891975e+63\n",
      "Gradient Descent(39/49): loss=3.337766195943879e+65\n",
      "Gradient Descent(40/49): loss=1.7560489176150692e+67\n",
      "Gradient Descent(41/49): loss=9.238837054567985e+68\n",
      "Gradient Descent(42/49): loss=4.8606909104093875e+70\n",
      "Gradient Descent(43/49): loss=2.557282479059953e+72\n",
      "Gradient Descent(44/49): loss=1.3454247139438426e+74\n",
      "Gradient Descent(45/49): loss=7.078481457223615e+75\n",
      "Gradient Descent(46/49): loss=3.7240953894317907e+77\n",
      "Gradient Descent(47/49): loss=1.9593025076634112e+79\n",
      "Gradient Descent(48/49): loss=1.030818471360868e+81\n",
      "Gradient Descent(49/49): loss=5.4232907718060997e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8594549573072634\n",
      "Gradient Descent(2/49): loss=71.56209351228344\n",
      "Gradient Descent(3/49): loss=3644.4284493242367\n",
      "Gradient Descent(4/49): loss=188917.13997605268\n",
      "Gradient Descent(5/49): loss=9826374.509036148\n",
      "Gradient Descent(6/49): loss=511594471.7680627\n",
      "Gradient Descent(7/49): loss=26642797956.875366\n",
      "Gradient Descent(8/49): loss=1387619569731.6946\n",
      "Gradient Descent(9/49): loss=72272338077603.48\n",
      "Gradient Descent(10/49): loss=3764238843747930.5\n",
      "Gradient Descent(11/49): loss=1.9605741318451248e+17\n",
      "Gradient Descent(12/49): loss=1.021150320585878e+19\n",
      "Gradient Descent(13/49): loss=5.3185859681979446e+20\n",
      "Gradient Descent(14/49): loss=2.770146375951734e+22\n",
      "Gradient Descent(15/49): loss=1.4428103911447627e+24\n",
      "Gradient Descent(16/49): loss=7.514771984045414e+25\n",
      "Gradient Descent(17/49): loss=3.914013818176282e+27\n",
      "Gradient Descent(18/49): loss=2.0385853636374016e+29\n",
      "Gradient Descent(19/49): loss=1.0617822211743443e+31\n",
      "Gradient Descent(20/49): loss=5.5302147527000316e+32\n",
      "Gradient Descent(21/49): loss=2.880371756239944e+34\n",
      "Gradient Descent(22/49): loss=1.500220484229047e+36\n",
      "Gradient Descent(23/49): loss=7.813788259894275e+37\n",
      "Gradient Descent(24/49): loss=4.069754253613306e+39\n",
      "Gradient Descent(25/49): loss=2.119701626651196e+41\n",
      "Gradient Descent(26/49): loss=1.1040310313672465e+43\n",
      "Gradient Descent(27/49): loss=5.750264579206332e+44\n",
      "Gradient Descent(28/49): loss=2.9949830929956933e+46\n",
      "Gradient Descent(29/49): loss=1.559914957611937e+48\n",
      "Gradient Descent(30/49): loss=8.124702542302309e+49\n",
      "Gradient Descent(31/49): loss=4.231691675163422e+51\n",
      "Gradient Descent(32/49): loss=2.20404554387207e+53\n",
      "Gradient Descent(33/49): loss=1.1479609414773236e+55\n",
      "Gradient Descent(34/49): loss=5.979070291090087e+56\n",
      "Gradient Descent(35/49): loss=3.114154868352044e+58\n",
      "Gradient Descent(36/49): loss=1.6219847019581705e+60\n",
      "Gradient Descent(37/49): loss=8.447988249147486e+61\n",
      "Gradient Descent(38/49): loss=4.4000726623113656e+63\n",
      "Gradient Descent(39/49): loss=2.291745544931779e+65\n",
      "Gradient Descent(40/49): loss=1.1936388432175862e+67\n",
      "Gradient Descent(41/49): loss=6.216980289058343e+68\n",
      "Gradient Descent(42/49): loss=3.2380685442803025e+70\n",
      "Gradient Descent(43/49): loss=1.6865242304066656e+72\n",
      "Gradient Descent(44/49): loss=8.784137645180706e+73\n",
      "Gradient Descent(45/49): loss=4.575153607539657e+75\n",
      "Gradient Descent(46/49): loss=2.3829351699727813e+77\n",
      "Gradient Descent(47/49): loss=1.2411342899909475e+79\n",
      "Gradient Descent(48/49): loss=6.464356836904352e+80\n",
      "Gradient Descent(49/49): loss=3.3669128032178205e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8706199918887916\n",
      "Gradient Descent(2/49): loss=72.69559572867024\n",
      "Gradient Descent(3/49): loss=3732.2365089928476\n",
      "Gradient Descent(4/49): loss=195034.66669025813\n",
      "Gradient Descent(5/49): loss=10227886.035065966\n",
      "Gradient Descent(6/49): loss=536906666.7734824\n",
      "Gradient Descent(7/49): loss=28193261694.821045\n",
      "Gradient Descent(8/49): loss=1480584562723.338\n",
      "Gradient Descent(9/49): loss=77756015969694.28\n",
      "Gradient Descent(10/49): loss=4083558761155779.5\n",
      "Gradient Descent(11/49): loss=2.144592964607323e+17\n",
      "Gradient Descent(12/49): loss=1.1262928786806405e+19\n",
      "Gradient Descent(13/49): loss=5.915043223286425e+20\n",
      "Gradient Descent(14/49): loss=3.106451270225107e+22\n",
      "Gradient Descent(15/49): loss=1.6314402782404333e+24\n",
      "Gradient Descent(16/49): loss=8.567967668044706e+25\n",
      "Gradient Descent(17/49): loss=4.4997093144678103e+27\n",
      "Gradient Descent(18/49): loss=2.363148965584918e+29\n",
      "Gradient Descent(19/49): loss=1.2410741771858826e+31\n",
      "Gradient Descent(20/49): loss=6.5178502745145e+32\n",
      "Gradient Descent(21/49): loss=3.423032481299517e+34\n",
      "Gradient Descent(22/49): loss=1.7977018303016607e+36\n",
      "Gradient Descent(23/49): loss=9.441137027865034e+37\n",
      "Gradient Descent(24/49): loss=4.9582787799708434e+39\n",
      "Gradient Descent(25/49): loss=2.6039796252671135e+41\n",
      "Gradient Descent(26/49): loss=1.3675531751457092e+43\n",
      "Gradient Descent(27/49): loss=7.182090323227137e+44\n",
      "Gradient Descent(28/49): loss=3.771876834368621e+46\n",
      "Gradient Descent(29/49): loss=1.9809072586619987e+48\n",
      "Gradient Descent(30/49): loss=1.0403291888179115e+50\n",
      "Gradient Descent(31/49): loss=5.4635814795164254e+51\n",
      "Gradient Descent(32/49): loss=2.8693535569479865e+53\n",
      "Gradient Descent(33/49): loss=1.506921762883415e+55\n",
      "Gradient Descent(34/49): loss=7.914023679490488e+56\n",
      "Gradient Descent(35/49): loss=4.156272232719867e+58\n",
      "Gradient Descent(36/49): loss=2.1827833188376646e+60\n",
      "Gradient Descent(37/49): loss=1.1463500825301063e+62\n",
      "Gradient Descent(38/49): loss=6.0203800366980046e+63\n",
      "Gradient Descent(39/49): loss=3.1617719873387984e+65\n",
      "Gradient Descent(40/49): loss=1.6604935301398843e+67\n",
      "Gradient Descent(41/49): loss=8.72054902971392e+68\n",
      "Gradient Descent(42/49): loss=4.5798417156878586e+70\n",
      "Gradient Descent(43/49): loss=2.4052327518927764e+72\n",
      "Gradient Descent(44/49): loss=1.2631756619363528e+74\n",
      "Gradient Descent(45/49): loss=6.633922441196945e+75\n",
      "Gradient Descent(46/49): loss=3.483991045897306e+77\n",
      "Gradient Descent(47/49): loss=1.8297159358562645e+79\n",
      "Gradient Descent(48/49): loss=9.609268111836185e+80\n",
      "Gradient Descent(49/49): loss=5.0465775498609863e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8906091996503316\n",
      "Gradient Descent(2/49): loss=75.31648899631111\n",
      "Gradient Descent(3/49): loss=3949.169963819639\n",
      "Gradient Descent(4/49): loss=210578.07194418425\n",
      "Gradient Descent(5/49): loss=11264018.675163668\n",
      "Gradient Descent(6/49): loss=603040840.2072309\n",
      "Gradient Descent(7/49): loss=32292970769.717667\n",
      "Gradient Descent(8/49): loss=1729422130260.9082\n",
      "Gradient Descent(9/49): loss=92619710030769.22\n",
      "Gradient Descent(10/49): loss=4960307590006098.0\n",
      "Gradient Descent(11/49): loss=2.6565293980071312e+17\n",
      "Gradient Descent(12/49): loss=1.4227247619072883e+19\n",
      "Gradient Descent(13/49): loss=7.619513225912468e+20\n",
      "Gradient Descent(14/49): loss=4.080689832129077e+22\n",
      "Gradient Descent(15/49): loss=2.1854453505489523e+24\n",
      "Gradient Descent(16/49): loss=1.1704323528036092e+26\n",
      "Gradient Descent(17/49): loss=6.268342028952949e+27\n",
      "Gradient Descent(18/49): loss=3.3570596124670416e+29\n",
      "Gradient Descent(19/49): loss=1.797899538810239e+31\n",
      "Gradient Descent(20/49): loss=9.628791635873072e+32\n",
      "Gradient Descent(21/49): loss=5.156774689945055e+34\n",
      "Gradient Descent(22/49): loss=2.761751028432889e+36\n",
      "Gradient Descent(23/49): loss=1.4790773694125096e+38\n",
      "Gradient Descent(24/49): loss=7.921314565238589e+39\n",
      "Gradient Descent(25/49): loss=4.2423219866036665e+41\n",
      "Gradient Descent(26/49): loss=2.2720087290813584e+43\n",
      "Gradient Descent(27/49): loss=1.2167920495715466e+45\n",
      "Gradient Descent(28/49): loss=6.51662501534126e+46\n",
      "Gradient Descent(29/49): loss=3.490029508782926e+48\n",
      "Gradient Descent(30/49): loss=1.8691126071395994e+50\n",
      "Gradient Descent(31/49): loss=1.0010178794695914e+52\n",
      "Gradient Descent(32/49): loss=5.3610295665988096e+53\n",
      "Gradient Descent(33/49): loss=2.8711413255850587e+55\n",
      "Gradient Descent(34/49): loss=1.537662198851144e+57\n",
      "Gradient Descent(35/49): loss=8.235070202592412e+58\n",
      "Gradient Descent(36/49): loss=4.4103562728078156e+60\n",
      "Gradient Descent(37/49): loss=2.362000805648493e+62\n",
      "Gradient Descent(38/49): loss=1.2649880102163015e+64\n",
      "Gradient Descent(39/49): loss=6.774742253111354e+65\n",
      "Gradient Descent(40/49): loss=3.628266214811356e+67\n",
      "Gradient Descent(41/49): loss=1.943146356526806e+69\n",
      "Gradient Descent(42/49): loss=1.0406672331455946e+71\n",
      "Gradient Descent(43/49): loss=5.573374782117021e+72\n",
      "Gradient Descent(44/49): loss=2.98486446700608e+74\n",
      "Gradient Descent(45/49): loss=1.598567516934737e+76\n",
      "Gradient Descent(46/49): loss=8.561253398423315e+77\n",
      "Gradient Descent(47/49): loss=4.5850462351792834e+79\n",
      "Gradient Descent(48/49): loss=2.4555573816566747e+81\n",
      "Gradient Descent(49/49): loss=1.3150929664230102e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8855241008725723\n",
      "Gradient Descent(2/49): loss=74.78903374803448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3/49): loss=3921.9689319195086\n",
      "Gradient Descent(4/49): loss=209423.47612195002\n",
      "Gradient Descent(5/49): loss=11223032.77908521\n",
      "Gradient Descent(6/49): loss=602050015.4811981\n",
      "Gradient Descent(7/49): loss=32306080920.805042\n",
      "Gradient Descent(8/49): loss=1733703466675.0186\n",
      "Gradient Descent(9/49): loss=93041581256191.39\n",
      "Gradient Descent(10/49): loss=4993245217946326.0\n",
      "Gradient Descent(11/49): loss=2.679722241128695e+17\n",
      "Gradient Descent(12/49): loss=1.4381261794746266e+19\n",
      "Gradient Descent(13/49): loss=7.717991609905257e+20\n",
      "Gradient Descent(14/49): loss=4.142014757504415e+22\n",
      "Gradient Descent(15/49): loss=2.222895213653586e+24\n",
      "Gradient Descent(16/49): loss=1.1929612643478397e+26\n",
      "Gradient Descent(17/49): loss=6.402265712811315e+27\n",
      "Gradient Descent(18/49): loss=3.43590420796965e+29\n",
      "Gradient Descent(19/49): loss=1.843946855375171e+31\n",
      "Gradient Descent(20/49): loss=9.895910362524105e+32\n",
      "Gradient Descent(21/49): loss=5.310838629607013e+34\n",
      "Gradient Descent(22/49): loss=2.850167990271648e+36\n",
      "Gradient Descent(23/49): loss=1.5295997749760738e+38\n",
      "Gradient Descent(24/49): loss=8.208903754423653e+39\n",
      "Gradient Descent(25/49): loss=4.405472722461307e+41\n",
      "Gradient Descent(26/49): loss=2.3642852308863545e+43\n",
      "Gradient Descent(27/49): loss=1.2688410541024487e+45\n",
      "Gradient Descent(28/49): loss=6.809489817657315e+46\n",
      "Gradient Descent(29/49): loss=3.6544491862757e+48\n",
      "Gradient Descent(30/49): loss=1.9612333981968853e+50\n",
      "Gradient Descent(31/49): loss=1.0525352101345956e+52\n",
      "Gradient Descent(32/49): loss=5.6486411540390185e+53\n",
      "Gradient Descent(33/49): loss=3.031456485244144e+55\n",
      "Gradient Descent(34/49): loss=1.6268918791836792e+57\n",
      "Gradient Descent(35/49): loss=8.731041330915343e+58\n",
      "Gradient Descent(36/49): loss=4.6856883175544674e+60\n",
      "Gradient Descent(37/49): loss=2.514668546067309e+62\n",
      "Gradient Descent(38/49): loss=1.349547274173074e+64\n",
      "Gradient Descent(39/49): loss=7.242615922787445e+65\n",
      "Gradient Descent(40/49): loss=3.886894991296671e+67\n",
      "Gradient Descent(41/49): loss=2.0859800981345753e+69\n",
      "Gradient Descent(42/49): loss=1.119483026826491e+71\n",
      "Gradient Descent(43/49): loss=6.0079300299812746e+72\n",
      "Gradient Descent(44/49): loss=3.224276061377502e+74\n",
      "Gradient Descent(45/49): loss=1.7303723692009066e+76\n",
      "Gradient Descent(46/49): loss=9.286390120127556e+77\n",
      "Gradient Descent(47/49): loss=4.983727375572171e+79\n",
      "Gradient Descent(48/49): loss=2.67461717984407e+81\n",
      "Gradient Descent(49/49): loss=1.4353869141760186e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8894884503721112\n",
      "Gradient Descent(2/49): loss=74.45673283577115\n",
      "Gradient Descent(3/49): loss=3868.4841082976227\n",
      "Gradient Descent(4/49): loss=204552.92989475964\n",
      "Gradient Descent(5/49): loss=10853138.001452958\n",
      "Gradient Descent(6/49): loss=576392302.8950664\n",
      "Gradient Descent(7/49): loss=30619880280.102974\n",
      "Gradient Descent(8/49): loss=1626768567797.6282\n",
      "Gradient Descent(9/49): loss=86428961471465.33\n",
      "Gradient Descent(10/49): loss=4591940399409596.0\n",
      "Gradient Descent(11/49): loss=2.4396876968103667e+17\n",
      "Gradient Descent(12/49): loss=1.2962015869956889e+19\n",
      "Gradient Descent(13/49): loss=6.886696741863539e+20\n",
      "Gradient Descent(14/49): loss=3.658890387233541e+22\n",
      "Gradient Descent(15/49): loss=1.9439623447789336e+24\n",
      "Gradient Descent(16/49): loss=1.0328239499247184e+26\n",
      "Gradient Descent(17/49): loss=5.487376422112746e+27\n",
      "Gradient Descent(18/49): loss=2.915433943811556e+29\n",
      "Gradient Descent(19/49): loss=1.5489651936443182e+31\n",
      "Gradient Descent(20/49): loss=8.229626249457437e+32\n",
      "Gradient Descent(21/49): loss=4.3723867059637415e+34\n",
      "Gradient Descent(22/49): loss=2.3230417672792847e+36\n",
      "Gradient Descent(23/49): loss=1.2342282180055213e+38\n",
      "Gradient Descent(24/49): loss=6.557433945347946e+39\n",
      "Gradient Descent(25/49): loss=3.4839537226830214e+41\n",
      "Gradient Descent(26/49): loss=1.8510188044530005e+43\n",
      "Gradient Descent(27/49): loss=9.834432048081489e+44\n",
      "Gradient Descent(28/49): loss=5.225017351291186e+46\n",
      "Gradient Descent(29/49): loss=2.7760430076508664e+48\n",
      "Gradient Descent(30/49): loss=1.474907021777228e+50\n",
      "Gradient Descent(31/49): loss=7.836156417218432e+51\n",
      "Gradient Descent(32/49): loss=4.163336840116344e+53\n",
      "Gradient Descent(33/49): loss=2.2119739220854615e+55\n",
      "Gradient Descent(34/49): loss=1.1752180570259688e+57\n",
      "Gradient Descent(35/49): loss=6.2439139438757e+58\n",
      "Gradient Descent(36/49): loss=3.31738106859809e+60\n",
      "Gradient Descent(37/49): loss=1.762519031045758e+62\n",
      "Gradient Descent(38/49): loss=9.364234227427064e+63\n",
      "Gradient Descent(39/49): loss=4.975202033086029e+65\n",
      "Gradient Descent(40/49): loss=2.6433165455777354e+67\n",
      "Gradient Descent(41/49): loss=1.4043896737578429e+69\n",
      "Gradient Descent(42/49): loss=7.461498922848769e+70\n",
      "Gradient Descent(43/49): loss=3.9642819379824976e+72\n",
      "Gradient Descent(44/49): loss=2.106216384443854e+74\n",
      "Gradient Descent(45/49): loss=1.119029253594758e+76\n",
      "Gradient Descent(46/49): loss=5.945383768019118e+77\n",
      "Gradient Descent(47/49): loss=3.158772483871619e+79\n",
      "Gradient Descent(48/49): loss=1.678250554411043e+81\n",
      "Gradient Descent(49/49): loss=8.916517215981408e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9005513585882663\n",
      "Gradient Descent(2/49): loss=75.60871608796722\n",
      "Gradient Descent(3/49): loss=3959.5326679381246\n",
      "Gradient Descent(4/49): loss=211023.37476509935\n",
      "Gradient Descent(5/49): loss=11286341.808110204\n",
      "Gradient Descent(6/49): loss=604250970.3113711\n",
      "Gradient Descent(7/49): loss=32360571172.643692\n",
      "Gradient Descent(8/49): loss=1733232280738.9177\n",
      "Gradient Descent(9/49): loss=92834712529935.16\n",
      "Gradient Descent(10/49): loss=4972423432133634.0\n",
      "Gradient Descent(11/49): loss=2.6633429306731747e+17\n",
      "Gradient Descent(12/49): loss=1.4265482822478172e+19\n",
      "Gradient Descent(13/49): loss=7.64092586833138e+20\n",
      "Gradient Descent(14/49): loss=4.0926587721003e+22\n",
      "Gradient Descent(15/49): loss=2.1921239112647307e+24\n",
      "Gradient Descent(16/49): loss=1.1741529291022334e+26\n",
      "Gradient Descent(17/49): loss=6.289038213934217e+27\n",
      "Gradient Descent(18/49): loss=3.3685562340777016e+29\n",
      "Gradient Descent(19/49): loss=1.8042776525207912e+31\n",
      "Gradient Descent(20/49): loss=9.664133894313974e+32\n",
      "Gradient Descent(21/49): loss=5.176336568778986e+34\n",
      "Gradient Descent(22/49): loss=2.772567160857448e+36\n",
      "Gradient Descent(23/49): loss=1.4850519395995725e+38\n",
      "Gradient Descent(24/49): loss=7.954286173642255e+39\n",
      "Gradient Descent(25/49): loss=4.260502063602747e+41\n",
      "Gradient Descent(26/49): loss=2.2820247395817536e+43\n",
      "Gradient Descent(27/49): loss=1.222305924119105e+45\n",
      "Gradient Descent(28/49): loss=6.546956946709078e+46\n",
      "Gradient Descent(29/49): loss=3.506703552382158e+48\n",
      "Gradient Descent(30/49): loss=1.8782725935705025e+50\n",
      "Gradient Descent(31/49): loss=1.0060468137837016e+52\n",
      "Gradient Descent(32/49): loss=5.388622476785063e+53\n",
      "Gradient Descent(33/49): loss=2.8862724675907616e+55\n",
      "Gradient Descent(34/49): loss=1.5459551662900442e+57\n",
      "Gradient Descent(35/49): loss=8.280498126962549e+58\n",
      "Gradient Descent(36/49): loss=4.435228829770991e+60\n",
      "Gradient Descent(37/49): loss=2.3756124898306626e+62\n",
      "Gradient Descent(38/49): loss=1.2724337161496283e+64\n",
      "Gradient Descent(39/49): loss=6.815453147031428e+65\n",
      "Gradient Descent(40/49): loss=3.650516408818416e+67\n",
      "Gradient Descent(41/49): loss=1.9553021293759453e+69\n",
      "Gradient Descent(42/49): loss=1.047305638157532e+71\n",
      "Gradient Descent(43/49): loss=5.609614408115143e+72\n",
      "Gradient Descent(44/49): loss=3.0046409244098553e+74\n",
      "Gradient Descent(45/49): loss=1.6093560854340368e+76\n",
      "Gradient Descent(46/49): loss=8.620088306333234e+77\n",
      "Gradient Descent(47/49): loss=4.617121287296876e+79\n",
      "Gradient Descent(48/49): loss=2.473038352280883e+81\n",
      "Gradient Descent(49/49): loss=1.3246172910118125e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9209625582026226\n",
      "Gradient Descent(2/49): loss=78.33126198248726\n",
      "Gradient Descent(3/49): loss=4189.370208764522\n",
      "Gradient Descent(4/49): loss=227819.3762351482\n",
      "Gradient Descent(5/49): loss=12428248.331841262\n",
      "Gradient Descent(6/49): loss=678585784.5455842\n",
      "Gradient Descent(7/49): loss=37060255564.2849\n",
      "Gradient Descent(8/49): loss=2024156682098.3567\n",
      "Gradient Descent(9/49): loss=110557790351658.27\n",
      "Gradient Descent(10/49): loss=6038615804934432.0\n",
      "Gradient Descent(11/49): loss=3.2982706421187354e+17\n",
      "Gradient Descent(12/49): loss=1.8015048226505124e+19\n",
      "Gradient Descent(13/49): loss=9.839763120053237e+20\n",
      "Gradient Descent(14/49): loss=5.374448182552249e+22\n",
      "Gradient Descent(15/49): loss=2.935507019149835e+24\n",
      "Gradient Descent(16/49): loss=1.603364886169164e+26\n",
      "Gradient Descent(17/49): loss=8.757529601835954e+27\n",
      "Gradient Descent(18/49): loss=4.783335684804714e+29\n",
      "Gradient Descent(19/49): loss=2.6126432129282116e+31\n",
      "Gradient Descent(20/49): loss=1.42701767304061e+33\n",
      "Gradient Descent(21/49): loss=7.794326562138656e+34\n",
      "Gradient Descent(22/49): loss=4.257237153062381e+36\n",
      "Gradient Descent(23/49): loss=2.3252898159887557e+38\n",
      "Gradient Descent(24/49): loss=1.2700661330208217e+40\n",
      "Gradient Descent(25/49): loss=6.937062086433735e+41\n",
      "Gradient Descent(26/49): loss=3.789001937763553e+43\n",
      "Gradient Descent(27/49): loss=2.0695411840773228e+45\n",
      "Gradient Descent(28/49): loss=1.1303770182604357e+47\n",
      "Gradient Descent(29/49): loss=6.174084445587045e+48\n",
      "Gradient Descent(30/49): loss=3.3722659011506367e+50\n",
      "Gradient Descent(31/49): loss=1.8419212448886344e+52\n",
      "Gradient Descent(32/49): loss=1.006051708797484e+54\n",
      "Gradient Descent(33/49): loss=5.495023435899035e+55\n",
      "Gradient Descent(34/49): loss=3.001364869920236e+57\n",
      "Gradient Descent(35/49): loss=1.639336244417209e+59\n",
      "Gradient Descent(36/49): loss=8.954004057265303e+60\n",
      "Gradient Descent(37/49): loss=4.890649427813126e+62\n",
      "Gradient Descent(38/49): loss=2.671257648846073e+64\n",
      "Gradient Descent(39/49): loss=1.459032697362927e+66\n",
      "Gradient Descent(40/49): loss=7.969191638604174e+67\n",
      "Gradient Descent(41/49): loss=4.352747919055099e+69\n",
      "Gradient Descent(42/49): loss=2.3774575020957825e+71\n",
      "Gradient Descent(43/49): loss=1.2985599624382873e+73\n",
      "Gradient Descent(44/49): loss=7.092694504786119e+74\n",
      "Gradient Descent(45/49): loss=3.8740078851471446e+76\n",
      "Gradient Descent(46/49): loss=2.1159711706256312e+78\n",
      "Gradient Descent(47/49): loss=1.1557369338572595e+80\n",
      "Gradient Descent(48/49): loss=6.312599523210155e+81\n",
      "Gradient Descent(49/49): loss=3.4479224097682666e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9157325939816052\n",
      "Gradient Descent(2/49): loss=77.7828356096026\n",
      "Gradient Descent(3/49): loss=4160.504858523206\n",
      "Gradient Descent(4/49): loss=226568.55154670804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=12382886.955427613\n",
      "Gradient Descent(6/49): loss=677461176.1867558\n",
      "Gradient Descent(7/49): loss=37074673362.73985\n",
      "Gradient Descent(8/49): loss=2029128166461.118\n",
      "Gradient Descent(9/49): loss=111058930934703.92\n",
      "Gradient Descent(10/49): loss=6078565400389828.0\n",
      "Gradient Descent(11/49): loss=3.326976932660111e+17\n",
      "Gradient Descent(12/49): loss=1.8209533366863178e+19\n",
      "Gradient Descent(13/49): loss=9.966621015672415e+20\n",
      "Gradient Descent(14/49): loss=5.455029486184315e+22\n",
      "Gradient Descent(15/49): loss=2.9857006986281843e+24\n",
      "Gradient Descent(16/49): loss=1.6341632510043666e+26\n",
      "Gradient Descent(17/49): loss=8.944264029562386e+27\n",
      "Gradient Descent(18/49): loss=4.895463108049147e+29\n",
      "Gradient Descent(19/49): loss=2.6794333180904e+31\n",
      "Gradient Descent(20/49): loss=1.4665339617579547e+33\n",
      "Gradient Descent(21/49): loss=8.026778821120803e+34\n",
      "Gradient Descent(22/49): loss=4.39329602474989e+36\n",
      "Gradient Descent(23/49): loss=2.4045822603615528e+38\n",
      "Gradient Descent(24/49): loss=1.3160997606978586e+40\n",
      "Gradient Descent(25/49): loss=7.203407463584802e+41\n",
      "Gradient Descent(26/49): loss=3.942640264512885e+43\n",
      "Gradient Descent(27/49): loss=2.157924889566459e+45\n",
      "Gradient Descent(28/49): loss=1.181096807366412e+47\n",
      "Gradient Descent(29/49): loss=6.46449593827793e+48\n",
      "Gradient Descent(30/49): loss=3.5382118955341224e+50\n",
      "Gradient Descent(31/49): loss=1.9365691520620149e+52\n",
      "Gradient Descent(32/49): loss=1.0599421943755733e+54\n",
      "Gradient Descent(33/49): loss=5.801380519882026e+55\n",
      "Gradient Descent(34/49): loss=3.17526900193777e+57\n",
      "Gradient Descent(35/49): loss=1.7379196555222543e+59\n",
      "Gradient Descent(36/49): loss=9.512153859113534e+60\n",
      "Gradient Descent(37/49): loss=5.206286191190928e+62\n",
      "Gradient Descent(38/49): loss=2.8495560843578943e+64\n",
      "Gradient Descent(39/49): loss=1.559647237917909e+66\n",
      "Gradient Descent(40/49): loss=8.536415619603751e+67\n",
      "Gradient Descent(41/49): loss=4.672235481139663e+69\n",
      "Gradient Descent(42/49): loss=2.557254164275754e+71\n",
      "Gradient Descent(43/49): loss=1.3996616581299851e+73\n",
      "Gradient Descent(44/49): loss=7.660766710664467e+74\n",
      "Gradient Descent(45/49): loss=4.1929666540722486e+76\n",
      "Gradient Descent(46/49): loss=2.294936006560227e+78\n",
      "Gradient Descent(47/49): loss=1.2560870879074347e+80\n",
      "Gradient Descent(48/49): loss=6.874940163462786e+81\n",
      "Gradient Descent(49/49): loss=3.762860290995729e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9198368729948658\n",
      "Gradient Descent(2/49): loss=77.438818865322\n",
      "Gradient Descent(3/49): loss=4103.905117187972\n",
      "Gradient Descent(4/49): loss=221309.57900223357\n",
      "Gradient Descent(5/49): loss=11975471.348832475\n",
      "Gradient Descent(6/49): loss=648635717.1665776\n",
      "Gradient Descent(7/49): loss=35142493158.854004\n",
      "Gradient Descent(8/49): loss=1904152007524.354\n",
      "Gradient Descent(9/49): loss=103176794679235.48\n",
      "Gradient Descent(10/49): loss=5590696565403640.0\n",
      "Gradient Descent(11/49): loss=3.0293599067905946e+17\n",
      "Gradient Descent(12/49): loss=1.6414820947326962e+19\n",
      "Gradient Descent(13/49): loss=8.894499719040395e+20\n",
      "Gradient Descent(14/49): loss=4.8195549038256216e+22\n",
      "Gradient Descent(15/49): loss=2.611513935855181e+24\n",
      "Gradient Descent(16/49): loss=1.4150694862864188e+26\n",
      "Gradient Descent(17/49): loss=7.667665975449011e+27\n",
      "Gradient Descent(18/49): loss=4.154785479930641e+29\n",
      "Gradient Descent(19/49): loss=2.2513033878572104e+31\n",
      "Gradient Descent(20/49): loss=1.2198865546611493e+33\n",
      "Gradient Descent(21/49): loss=6.610051822849272e+34\n",
      "Gradient Descent(22/49): loss=3.581708883833444e+36\n",
      "Gradient Descent(23/49): loss=1.9407772998401004e+38\n",
      "Gradient Descent(24/49): loss=1.0516255367871217e+40\n",
      "Gradient Descent(25/49): loss=5.69831618349043e+41\n",
      "Gradient Descent(26/49): loss=3.087677713326899e+43\n",
      "Gradient Descent(27/49): loss=1.673082600961602e+45\n",
      "Gradient Descent(28/49): loss=9.065730460010971e+46\n",
      "Gradient Descent(29/49): loss=4.912337784538174e+48\n",
      "Gradient Descent(30/49): loss=2.661789098610845e+50\n",
      "Gradient Descent(31/49): loss=1.4423114851312242e+52\n",
      "Gradient Descent(32/49): loss=7.815278908562322e+53\n",
      "Gradient Descent(33/49): loss=4.2347707168858885e+55\n",
      "Gradient Descent(34/49): loss=2.2946440215904238e+57\n",
      "Gradient Descent(35/49): loss=1.243371020023643e+59\n",
      "Gradient Descent(36/49): loss=6.737304256732238e+60\n",
      "Gradient Descent(37/49): loss=3.650661622057053e+62\n",
      "Gradient Descent(38/49): loss=1.9781398866531592e+64\n",
      "Gradient Descent(39/49): loss=1.0718707500924966e+66\n",
      "Gradient Descent(40/49): loss=5.808016473737373e+67\n",
      "Gradient Descent(41/49): loss=3.1471196836273067e+69\n",
      "Gradient Descent(42/49): loss=1.70529170291784e+71\n",
      "Gradient Descent(43/49): loss=9.240258027583808e+72\n",
      "Gradient Descent(44/49): loss=5.006906928019044e+74\n",
      "Gradient Descent(45/49): loss=2.7130321373071154e+76\n",
      "Gradient Descent(46/49): loss=1.4700779311217185e+78\n",
      "Gradient Descent(47/49): loss=7.965733593248895e+79\n",
      "Gradient Descent(48/49): loss=4.316295778292343e+81\n",
      "Gradient Descent(49/49): loss=2.3388190212002336e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9307932930806273\n",
      "Gradient Descent(2/49): loss=78.60902395040212\n",
      "Gradient Descent(3/49): loss=4198.251161784206\n",
      "Gradient Descent(4/49): loss=228147.45819361394\n",
      "Gradient Descent(5/49): loss=12442380.861002075\n",
      "Gradient Descent(6/49): loss=679259170.5039119\n",
      "Gradient Descent(7/49): loss=37093963082.823586\n",
      "Gradient Descent(8/49): loss=2025877237127.778\n",
      "Gradient Descent(9/49): loss=110646122306994.11\n",
      "Gradient Descent(10/49): loss=6043150425702419.0\n",
      "Gradient Descent(11/49): loss=3.300592497899719e+17\n",
      "Gradient Descent(12/49): loss=1.8026890415670286e+19\n",
      "Gradient Descent(13/49): loss=9.845773869497648e+20\n",
      "Gradient Descent(14/49): loss=5.377481629475184e+22\n",
      "Gradient Descent(15/49): loss=2.9370275914948424e+24\n",
      "Gradient Descent(16/49): loss=1.6041209893685937e+26\n",
      "Gradient Descent(17/49): loss=8.761252918920113e+27\n",
      "Gradient Descent(18/49): loss=4.7851473353291356e+29\n",
      "Gradient Descent(19/49): loss=2.6135114736634125e+31\n",
      "Gradient Descent(20/49): loss=1.427425687218743e+33\n",
      "Gradient Descent(21/49): loss=7.796193409216191e+34\n",
      "Gradient Descent(22/49): loss=4.2580592614151384e+36\n",
      "Gradient Descent(23/49): loss=2.3256309486036525e+38\n",
      "Gradient Descent(24/49): loss=1.2701935264541798e+40\n",
      "Gradient Descent(25/49): loss=6.937436034789774e+41\n",
      "Gradient Descent(26/49): loss=3.789030390601677e+43\n",
      "Gradient Descent(27/49): loss=2.069460709822398e+45\n",
      "Gradient Descent(28/49): loss=1.130280622747544e+47\n",
      "Gradient Descent(29/49): loss=6.173271519942139e+48\n",
      "Gradient Descent(30/49): loss=3.371665451212527e+50\n",
      "Gradient Descent(31/49): loss=1.8415078420212977e+52\n",
      "Gradient Descent(32/49): loss=1.005779245092778e+54\n",
      "Gradient Descent(33/49): loss=5.493280380218461e+55\n",
      "Gradient Descent(34/49): loss=3.0002736169913002e+57\n",
      "Gradient Descent(35/49): loss=1.6386641776431696e+59\n",
      "Gradient Descent(36/49): loss=8.949918007090758e+60\n",
      "Gradient Descent(37/49): loss=4.888190846330314e+62\n",
      "Gradient Descent(38/49): loss=2.669790911069413e+64\n",
      "Gradient Descent(39/49): loss=1.4581639164477118e+66\n",
      "Gradient Descent(40/49): loss=7.964076881130908e+67\n",
      "Gradient Descent(41/49): loss=4.3497524423097376e+69\n",
      "Gradient Descent(42/49): loss=2.3757111579632096e+71\n",
      "Gradient Descent(43/49): loss=1.2975459134575214e+73\n",
      "Gradient Descent(44/49): loss=7.086826998673381e+74\n",
      "Gradient Descent(45/49): loss=3.8706234891756665e+76\n",
      "Gradient Descent(46/49): loss=2.114024541273958e+78\n",
      "Gradient Descent(47/49): loss=1.1546201209201983e+80\n",
      "Gradient Descent(48/49): loss=6.306206941336628e+81\n",
      "Gradient Descent(49/49): loss=3.444271000168261e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9516307633866785\n",
      "Gradient Descent(2/49): loss=81.43614578523069\n",
      "Gradient Descent(3/49): loss=4441.625856596514\n",
      "Gradient Descent(4/49): loss=246283.4820182099\n",
      "Gradient Descent(5/49): loss=13699680.465479907\n",
      "Gradient Descent(6/49): loss=762717227.664071\n",
      "Gradient Descent(7/49): loss=42474315306.73339\n",
      "Gradient Descent(8/49): loss=2365492657235.022\n",
      "Gradient Descent(9/49): loss=131742664383659.44\n",
      "Gradient Descent(10/49): loss=7337263995925660.0\n",
      "Gradient Descent(11/49): loss=4.0864171863922963e+17\n",
      "Gradient Descent(12/49): loss=2.275891288611472e+19\n",
      "Gradient Descent(13/49): loss=1.2675362859031533e+21\n",
      "Gradient Descent(14/49): loss=7.059424709279703e+22\n",
      "Gradient Descent(15/49): loss=3.931680580281303e+24\n",
      "Gradient Descent(16/49): loss=2.1897127448252515e+26\n",
      "Gradient Descent(17/49): loss=1.219540044918481e+28\n",
      "Gradient Descent(18/49): loss=6.79211428662496e+29\n",
      "Gradient Descent(19/49): loss=3.7828045647597478e+31\n",
      "Gradient Descent(20/49): loss=2.1067976438309766e+33\n",
      "Gradient Descent(21/49): loss=1.1733612551517706e+35\n",
      "Gradient Descent(22/49): loss=6.534925834606081e+36\n",
      "Gradient Descent(23/49): loss=3.639565860585678e+38\n",
      "Gradient Descent(24/49): loss=2.0270221864489046e+40\n",
      "Gradient Descent(25/49): loss=1.1289310598421913e+42\n",
      "Gradient Descent(26/49): loss=6.287476014799742e+43\n",
      "Gradient Descent(27/49): loss=3.5017509963990745e+45\n",
      "Gradient Descent(28/49): loss=1.9502674860179875e+47\n",
      "Gradient Descent(29/49): loss=1.0861832468756926e+49\n",
      "Gradient Descent(30/49): loss=6.049396066189385e+50\n",
      "Gradient Descent(31/49): loss=3.3691545943918903e+52\n",
      "Gradient Descent(32/49): loss=1.8764191593198647e+54\n",
      "Gradient Descent(33/49): loss=1.0450541115932863e+56\n",
      "Gradient Descent(34/49): loss=5.82033119164001e+57\n",
      "Gradient Descent(35/49): loss=3.2415790536176295e+59\n",
      "Gradient Descent(36/49): loss=1.8053671543546146e+61\n",
      "Gradient Descent(37/49): loss=1.005482361562347e+63\n",
      "Gradient Descent(38/49): loss=5.599940028677458e+64\n",
      "Gradient Descent(39/49): loss=3.118834255437066e+66\n",
      "Gradient Descent(40/49): loss=1.7370055863232028e+68\n",
      "Gradient Descent(41/49): loss=9.67409025233754e+69\n",
      "Gradient Descent(42/49): loss=5.387894140770969e+71\n",
      "Gradient Descent(43/49): loss=3.000737280194368e+73\n",
      "Gradient Descent(44/49): loss=1.6712325798330996e+75\n",
      "Gradient Descent(45/49): loss=9.307773640599025e+76\n",
      "Gradient Descent(46/49): loss=5.183877527883157e+78\n",
      "Gradient Descent(47/49): loss=2.8871121346224054e+80\n",
      "Gradient Descent(48/49): loss=1.6079501170791972e+82\n",
      "Gradient Descent(49/49): loss=8.955327879404119e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9462541984259716\n",
      "Gradient Descent(2/49): loss=80.86613215288111\n",
      "Gradient Descent(3/49): loss=4411.012266558113\n",
      "Gradient Descent(4/49): loss=244929.4878621358\n",
      "Gradient Descent(5/49): loss=13649528.119647387\n",
      "Gradient Descent(6/49): loss=761442545.6978464\n",
      "Gradient Descent(7/49): loss=42490133938.84162\n",
      "Gradient Descent(8/49): loss=2371257232990.8433\n",
      "Gradient Descent(9/49): loss=132336985737837.75\n",
      "Gradient Descent(10/49): loss=7385628398490994.0\n",
      "Gradient Descent(11/49): loss=4.121874596494803e+17\n",
      "Gradient Descent(12/49): loss=2.3003950981572833e+19\n",
      "Gradient Descent(13/49): loss=1.2838379071537585e+21\n",
      "Gradient Descent(14/49): loss=7.1650295697582984e+22\n",
      "Gradient Descent(15/49): loss=3.998764140629811e+24\n",
      "Gradient Descent(16/49): loss=2.2316885928737662e+26\n",
      "Gradient Descent(17/49): loss=1.2454933100463538e+28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=6.951030673702175e+29\n",
      "Gradient Descent(19/49): loss=3.8793325540995963e+31\n",
      "Gradient Descent(20/49): loss=2.165034477992777e+33\n",
      "Gradient Descent(21/49): loss=1.20829401078925e+35\n",
      "Gradient Descent(22/49): loss=6.743423402062306e+36\n",
      "Gradient Descent(23/49): loss=3.7634680610434514e+38\n",
      "Gradient Descent(24/49): loss=2.1003711322900801e+40\n",
      "Gradient Descent(25/49): loss=1.1722057479438331e+42\n",
      "Gradient Descent(26/49): loss=6.54201676260156e+43\n",
      "Gradient Descent(27/49): loss=3.6510641068969956e+45\n",
      "Gradient Descent(28/49): loss=2.0376390945489595e+47\n",
      "Gradient Descent(29/49): loss=1.1371953375979081e+49\n",
      "Gradient Descent(30/49): loss=6.346625559521226e+50\n",
      "Gradient Descent(31/49): loss=3.5420173351968607e+52\n",
      "Gradient Descent(32/49): loss=1.976780682139597e+54\n",
      "Gradient Descent(33/49): loss=1.1032305873972003e+56\n",
      "Gradient Descent(34/49): loss=6.1570701290514624e+57\n",
      "Gradient Descent(35/49): loss=3.436227476569163e+59\n",
      "Gradient Descent(36/49): loss=1.9177399352681838e+61\n",
      "Gradient Descent(37/49): loss=1.0702802664840974e+63\n",
      "Gradient Descent(38/49): loss=5.973176173468373e+64\n",
      "Gradient Descent(39/49): loss=3.333597256398671e+66\n",
      "Gradient Descent(40/49): loss=1.860462565499061e+68\n",
      "Gradient Descent(41/49): loss=1.0383140767768222e+70\n",
      "Gradient Descent(42/49): loss=5.794774600819276e+71\n",
      "Gradient Descent(43/49): loss=3.2340323053828713e+73\n",
      "Gradient Descent(44/49): loss=1.8048959058358037e+75\n",
      "Gradient Descent(45/49): loss=1.0073026251100434e+77\n",
      "Gradient Descent(46/49): loss=5.6217013694411636e+78\n",
      "Gradient Descent(47/49): loss=3.1374410727584495e+80\n",
      "Gradient Descent(48/49): loss=1.7509888622935144e+82\n",
      "Gradient Descent(49/49): loss=9.772173962076505e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.950500225175529\n",
      "Gradient Descent(2/49): loss=80.51009623305893\n",
      "Gradient Descent(3/49): loss=4351.148125771379\n",
      "Gradient Descent(4/49): loss=239255.2669354539\n",
      "Gradient Descent(5/49): loss=13201201.548048615\n",
      "Gradient Descent(6/49): loss=729094522.4430928\n",
      "Gradient Descent(7/49): loss=40278998224.11265\n",
      "Gradient Descent(8/49): loss=2225415899016.2134\n",
      "Gradient Descent(9/49): loss=122957553884636.64\n",
      "Gradient Descent(10/49): loss=6793643645409870.0\n",
      "Gradient Descent(11/49): loss=3.753629370805293e+17\n",
      "Gradient Descent(12/49): loss=2.073959852664823e+19\n",
      "Gradient Descent(13/49): loss=1.1459070805302924e+21\n",
      "Gradient Descent(14/49): loss=6.3313816292603734e+22\n",
      "Gradient Descent(15/49): loss=3.498223799742378e+24\n",
      "Gradient Descent(16/49): loss=1.9328435011352657e+26\n",
      "Gradient Descent(17/49): loss=1.0679373937861987e+28\n",
      "Gradient Descent(18/49): loss=5.9005826239804374e+29\n",
      "Gradient Descent(19/49): loss=3.260198164400289e+31\n",
      "Gradient Descent(20/49): loss=1.8013292498597232e+33\n",
      "Gradient Descent(21/49): loss=9.952729566842507e+34\n",
      "Gradient Descent(22/49): loss=5.499096061365109e+36\n",
      "Gradient Descent(23/49): loss=3.0383682475325796e+38\n",
      "Gradient Descent(24/49): loss=1.6787634739603736e+40\n",
      "Gradient Descent(25/49): loss=9.27552742756775e+41\n",
      "Gradient Descent(26/49): loss=5.124927388168639e+43\n",
      "Gradient Descent(27/49): loss=2.8316320488622014e+45\n",
      "Gradient Descent(28/49): loss=1.5645373003048211e+47\n",
      "Gradient Descent(29/49): loss=8.644403375179524e+48\n",
      "Gradient Descent(30/49): loss=4.77621784397575e+50\n",
      "Gradient Descent(31/49): loss=2.6389625637568816e+52\n",
      "Gradient Descent(32/49): loss=1.4580832869032894e+54\n",
      "Gradient Descent(33/49): loss=8.056222171337162e+55\n",
      "Gradient Descent(34/49): loss=4.451235142526493e+57\n",
      "Gradient Descent(35/49): loss=2.4594026669915145e+59\n",
      "Gradient Descent(36/49): loss=1.3588726015880176e+61\n",
      "Gradient Descent(37/49): loss=7.508061905151041e+62\n",
      "Gradient Descent(38/49): loss=4.148364865529238e+64\n",
      "Gradient Descent(39/49): loss=2.2920603579135123e+66\n",
      "Gradient Descent(40/49): loss=1.2664123949107908e+68\n",
      "Gradient Descent(41/49): loss=6.997199477956302e+69\n",
      "Gradient Descent(42/49): loss=3.8661024427008327e+71\n",
      "Gradient Descent(43/49): loss=2.1361043292455677e+73\n",
      "Gradient Descent(44/49): loss=1.1802433518119645e+75\n",
      "Gradient Descent(45/49): loss=6.52109707576089e+76\n",
      "Gradient Descent(46/49): loss=3.6030456775046527e+78\n",
      "Gradient Descent(47/49): loss=1.9907598373959574e+80\n",
      "Gradient Descent(48/49): loss=1.0999374098786098e+82\n",
      "Gradient Descent(49/49): loss=6.077389562133409e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9613457953658744\n",
      "Gradient Descent(2/49): loss=81.69824274299708\n",
      "Gradient Descent(3/49): loss=4448.846696348059\n",
      "Gradient Descent(4/49): loss=246475.33631667186\n",
      "Gradient Descent(5/49): loss=13703919.880218819\n",
      "Gradient Descent(6/49): loss=762716116.177869\n",
      "Gradient Descent(7/49): loss=42463693729.345894\n",
      "Gradient Descent(8/49): loss=2364368529545.4766\n",
      "Gradient Descent(9/49): loss=131651527136018.39\n",
      "Gradient Descent(10/49): loss=7330621871289654.0\n",
      "Gradient Descent(11/49): loss=4.081850093766863e+17\n",
      "Gradient Descent(12/49): loss=2.2728652978907705e+19\n",
      "Gradient Descent(13/49): loss=1.2655825402230197e+21\n",
      "Gradient Descent(14/49): loss=7.047049020385602e+22\n",
      "Gradient Descent(15/49): loss=3.9239559483351146e+24\n",
      "Gradient Descent(16/49): loss=2.1849472571480804e+26\n",
      "Gradient Descent(17/49): loss=1.2166279625412174e+28\n",
      "Gradient Descent(18/49): loss=6.774459180928414e+29\n",
      "Gradient Descent(19/49): loss=3.77217182412345e+31\n",
      "Gradient Descent(20/49): loss=2.100430439092272e+33\n",
      "Gradient Descent(21/49): loss=1.1695670916627037e+35\n",
      "Gradient Descent(22/49): loss=6.512413629388585e+36\n",
      "Gradient Descent(23/49): loss=3.6262589451001705e+38\n",
      "Gradient Descent(24/49): loss=2.0191828537410258e+40\n",
      "Gradient Descent(25/49): loss=1.1243266017590636e+42\n",
      "Gradient Descent(26/49): loss=6.260504367304465e+43\n",
      "Gradient Descent(27/49): loss=3.485990180408293e+45\n",
      "Gradient Descent(28/49): loss=1.941078038595069e+47\n",
      "Gradient Descent(29/49): loss=1.0808360772475859e+49\n",
      "Gradient Descent(30/49): loss=6.018339307601901e+50\n",
      "Gradient Descent(31/49): loss=3.35114720760094e+52\n",
      "Gradient Descent(32/49): loss=1.8659944268722813e+54\n",
      "Gradient Descent(33/49): loss=1.039027827014235e+56\n",
      "Gradient Descent(34/49): loss=5.785541530900929e+57\n",
      "Gradient Descent(35/49): loss=3.2215201494618648e+59\n",
      "Gradient Descent(36/49): loss=1.7938151542009076e+61\n",
      "Gradient Descent(37/49): loss=9.9883677833843e+62\n",
      "Gradient Descent(38/49): loss=5.561748697601593e+64\n",
      "Gradient Descent(39/49): loss=3.096907247121026e+66\n",
      "Gradient Descent(40/49): loss=1.724427876686816e+68\n",
      "Gradient Descent(41/49): loss=9.602003756034329e+69\n",
      "Gradient Descent(42/49): loss=5.346612483906264e+71\n",
      "Gradient Descent(43/49): loss=2.977114546023536e+73\n",
      "Gradient Descent(44/49): loss=1.6577245960547737e+75\n",
      "Gradient Descent(45/49): loss=9.230584829312217e+76\n",
      "Gradient Descent(46/49): loss=5.139798039668665e+78\n",
      "Gradient Descent(47/49): loss=2.8619555940476906e+80\n",
      "Gradient Descent(48/49): loss=1.593601491553722e+82\n",
      "Gradient Descent(49/49): loss=8.873532905835601e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.982613815202498\n",
      "Gradient Descent(2/49): loss=84.63291929846947\n",
      "Gradient Descent(3/49): loss=4706.416205999215\n",
      "Gradient Descent(4/49): loss=266043.9861728962\n",
      "Gradient Descent(5/49): loss=15087001.503653046\n",
      "Gradient Descent(6/49): loss=856313422.5252969\n",
      "Gradient Descent(7/49): loss=48615321932.408676\n",
      "Gradient Descent(8/49): loss=2760236449312.5264\n",
      "Gradient Descent(9/49): loss=156721702240574.75\n",
      "Gradient Descent(10/49): loss=8898460919374826.0\n",
      "Gradient Descent(11/49): loss=5.0524441537665664e+17\n",
      "Gradient Descent(12/49): loss=2.8687216118969037e+19\n",
      "Gradient Descent(13/49): loss=1.6288285238952004e+21\n",
      "Gradient Descent(14/49): loss=9.248309388094533e+22\n",
      "Gradient Descent(15/49): loss=5.251088536569136e+24\n",
      "Gradient Descent(16/49): loss=2.9815104353514876e+26\n",
      "Gradient Descent(17/49): loss=1.6928689026721472e+28\n",
      "Gradient Descent(18/49): loss=9.61192383857216e+29\n",
      "Gradient Descent(19/49): loss=5.4575448662560245e+31\n",
      "Gradient Descent(20/49): loss=3.098734079519145e+33\n",
      "Gradient Descent(21/49): loss=1.7594272023400387e+35\n",
      "Gradient Descent(22/49): loss=9.989834561166791e+36\n",
      "Gradient Descent(23/49): loss=5.6721184273412e+38\n",
      "Gradient Descent(24/49): loss=3.220566592649934e+40\n",
      "Gradient Descent(25/49): loss=1.8286023662160506e+42\n",
      "Gradient Descent(26/49): loss=1.0382603549829671e+44\n",
      "Gradient Descent(27/49): loss=5.895128348543352e+45\n",
      "Gradient Descent(28/49): loss=3.34718917841853e+47\n",
      "Gradient Descent(29/49): loss=1.9004972807573313e+49\n",
      "Gradient Descent(30/49): loss=1.0790814984268459e+51\n",
      "Gradient Descent(31/49): loss=6.126906320976798e+52\n",
      "Gradient Descent(32/49): loss=3.478790167448172e+54\n",
      "Gradient Descent(33/49): loss=1.9752188780331507e+56\n",
      "Gradient Descent(34/49): loss=1.121507601305095e+58\n",
      "Gradient Descent(35/49): loss=6.36779707693741e+59\n",
      "Gradient Descent(36/49): loss=3.615565294953525e+61\n",
      "Gradient Descent(37/49): loss=2.052878294350338e+63\n",
      "Gradient Descent(38/49): loss=1.1656017655930398e+65\n",
      "Gradient Descent(39/49): loss=6.618158902515795e+66\n",
      "Gradient Descent(40/49): loss=3.7577179918446825e+68\n",
      "Gradient Descent(41/49): loss=2.133591035547005e+70\n",
      "Gradient Descent(42/49): loss=1.2114295742379133e+72\n",
      "Gradient Descent(43/49): loss=6.878364170488703e+73\n",
      "Gradient Descent(44/49): loss=3.905459687297617e+75\n",
      "Gradient Descent(45/49): loss=2.2174771487888785e+77\n",
      "Gradient Descent(46/49): loss=1.2590591887029073e+79\n",
      "Gradient Descent(47/49): loss=7.148799894163649e+80\n",
      "Gradient Descent(48/49): loss=4.059010123220894e+82\n",
      "Gradient Descent(49/49): loss=2.3046614011200037e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9770889142056707\n",
      "Gradient Descent(2/49): loss=84.04069032612813\n",
      "Gradient Descent(3/49): loss=4673.967106337121\n",
      "Gradient Descent(4/49): loss=264579.45751912944\n",
      "Gradient Descent(5/49): loss=15031608.031202972\n",
      "Gradient Descent(6/49): loss=854870601.3283497\n",
      "Gradient Descent(7/49): loss=48632635811.20565\n",
      "Gradient Descent(8/49): loss=2766911202537.9507\n",
      "Gradient Descent(9/49): loss=157425388278981.2\n",
      "Gradient Descent(10/49): loss=8956906016896630.0\n",
      "Gradient Descent(11/49): loss=5.0961521508378054e+17\n",
      "Gradient Descent(12/49): loss=2.8995265973660398e+19\n",
      "Gradient Descent(13/49): loss=1.6497263542921247e+21\n",
      "Gradient Descent(14/49): loss=9.386350329496248e+22\n",
      "Gradient Descent(15/49): loss=5.340496273431705e+24\n",
      "Gradient Descent(16/49): loss=3.0385506228860346e+26\n",
      "Gradient Descent(17/49): loss=1.7288262053257436e+28\n",
      "Gradient Descent(18/49): loss=9.8364003798205e+29\n",
      "Gradient Descent(19/49): loss=5.59655864497347e+31\n",
      "Gradient Descent(20/49): loss=3.1842409274755764e+33\n",
      "Gradient Descent(21/49): loss=1.8117187592589712e+35\n",
      "Gradient Descent(22/49): loss=1.0308029252267936e+37\n",
      "Gradient Descent(23/49): loss=5.8648985402816266e+38\n",
      "Gradient Descent(24/49): loss=3.3369166933870316e+40\n",
      "Gradient Descent(25/49): loss=1.8985857883352323e+42\n",
      "Gradient Descent(26/49): loss=1.0802271458595925e+44\n",
      "Gradient Descent(27/49): loss=6.146104610185444e+45\n",
      "Gradient Descent(28/49): loss=3.496912850610118e+47\n",
      "Gradient Descent(29/49): loss=1.9896178572191892e+49\n",
      "Gradient Descent(30/49): loss=1.1320211245970333e+51\n",
      "Gradient Descent(31/49): loss=6.440793752851618e+52\n",
      "Gradient Descent(32/49): loss=3.6645803921317486e+54\n",
      "Gradient Descent(33/49): loss=2.0850146683319583e+56\n",
      "Gradient Descent(34/49): loss=1.1862984849489292e+58\n",
      "Gradient Descent(35/49): loss=6.749612445259175e+59\n",
      "Gradient Descent(36/49): loss=3.8402871401423584e+61\n",
      "Gradient Descent(37/49): loss=2.1849854992935807e+63\n",
      "Gradient Descent(38/49): loss=1.243178298367092e+65\n",
      "Gradient Descent(39/49): loss=7.073238161216984e+66\n",
      "Gradient Descent(40/49): loss=4.024418552914797e+68\n",
      "Gradient Descent(41/49): loss=2.289749662021606e+70\n",
      "Gradient Descent(42/49): loss=1.3027853454583413e+72\n",
      "Gradient Descent(43/49): loss=7.412380857575954e+73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=4.2173785704066816e+75\n",
      "Gradient Descent(45/49): loss=2.399536983848696e+77\n",
      "Gradient Descent(46/49): loss=1.3652503897231287e+79\n",
      "Gradient Descent(47/49): loss=7.767784531703953e+80\n",
      "Gradient Descent(48/49): loss=4.4195905004074474e+82\n",
      "Gradient Descent(49/49): loss=2.514588311708366e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9814785069141003\n",
      "Gradient Descent(2/49): loss=83.6723268240291\n",
      "Gradient Descent(3/49): loss=4610.6834100798515\n",
      "Gradient Descent(4/49): loss=258461.59502267418\n",
      "Gradient Descent(5/49): loss=14538710.858272009\n",
      "Gradient Descent(6/49): loss=818609433.9752898\n",
      "Gradient Descent(7/49): loss=46105529660.06125\n",
      "Gradient Descent(8/49): loss=2596972473324.951\n",
      "Gradient Descent(9/49): loss=146282815334363.12\n",
      "Gradient Descent(10/49): loss=8239917051055062.0\n",
      "Gradient Descent(11/49): loss=4.641447728765293e+17\n",
      "Gradient Descent(12/49): loss=2.614474581901098e+19\n",
      "Gradient Descent(13/49): loss=1.4727040705865974e+21\n",
      "Gradient Descent(14/49): loss=8.295576682247853e+22\n",
      "Gradient Descent(15/49): loss=4.6728053115272504e+24\n",
      "Gradient Descent(16/49): loss=2.6321388453287927e+26\n",
      "Gradient Descent(17/49): loss=1.4826543058933345e+28\n",
      "Gradient Descent(18/49): loss=8.351625508330544e+29\n",
      "Gradient Descent(19/49): loss=4.704377033005242e+31\n",
      "Gradient Descent(20/49): loss=2.649922850102388e+33\n",
      "Gradient Descent(21/49): loss=1.4926718378142122e+35\n",
      "Gradient Descent(22/49): loss=8.408053145133753e+36\n",
      "Gradient Descent(23/49): loss=4.736162088717475e+38\n",
      "Gradient Descent(24/49): loss=2.6678270157699105e+40\n",
      "Gradient Descent(25/49): loss=1.50275705365487e+42\n",
      "Gradient Descent(26/49): loss=8.464862035508947e+43\n",
      "Gradient Descent(27/49): loss=4.7681618998847286e+45\n",
      "Gradient Descent(28/49): loss=2.685852150707332e+47\n",
      "Gradient Descent(29/49): loss=1.5129104101170582e+49\n",
      "Gradient Descent(30/49): loss=8.522054754345983e+50\n",
      "Gradient Descent(31/49): loss=4.800377917318423e+52\n",
      "Gradient Descent(32/49): loss=2.7039990722104733e+54\n",
      "Gradient Descent(33/49): loss=1.5231323675864907e+56\n",
      "Gradient Descent(34/49): loss=8.579633894966953e+57\n",
      "Gradient Descent(35/49): loss=4.8328116018115085e+59\n",
      "Gradient Descent(36/49): loss=2.722268603128303e+61\n",
      "Gradient Descent(37/49): loss=1.5334233895648432e+63\n",
      "Gradient Descent(38/49): loss=8.637602068225139e+64\n",
      "Gradient Descent(39/49): loss=4.865464424028337e+66\n",
      "Gradient Descent(40/49): loss=2.740661571869515e+68\n",
      "Gradient Descent(41/49): loss=1.5437839426854664e+70\n",
      "Gradient Descent(42/49): loss=8.695961902613784e+71\n",
      "Gradient Descent(43/49): loss=4.898337864569763e+73\n",
      "Gradient Descent(44/49): loss=2.759178812440048e+75\n",
      "Gradient Descent(45/49): loss=1.5542144967345466e+77\n",
      "Gradient Descent(46/49): loss=8.754716044385775e+78\n",
      "Gradient Descent(47/49): loss=4.931433414040297e+80\n",
      "Gradient Descent(48/49): loss=2.7778211644806716e+82\n",
      "Gradient Descent(49/49): loss=1.5647155246723348e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.992208865444008\n",
      "Gradient Descent(2/49): loss=84.87811277972614\n",
      "Gradient Descent(3/49): loss=4711.7874217168\n",
      "Gradient Descent(4/49): loss=266078.83052695304\n",
      "Gradient Descent(5/49): loss=15079431.944484416\n",
      "Gradient Descent(6/49): loss=855478268.738902\n",
      "Gradient Descent(7/49): loss=48547931500.09425\n",
      "Gradient Descent(8/49): loss=2755341424559.934\n",
      "Gradient Descent(9/49): loss=156384457029912.06\n",
      "Gradient Descent(10/49): loss=8875973361197912.0\n",
      "Gradient Descent(11/49): loss=5.037786154017117e+17\n",
      "Gradient Descent(12/49): loss=2.8593274080834347e+19\n",
      "Gradient Descent(13/49): loss=1.622886615402033e+21\n",
      "Gradient Descent(14/49): loss=9.211121511167203e+22\n",
      "Gradient Descent(15/49): loss=5.228015392943425e+24\n",
      "Gradient Descent(16/49): loss=2.9672983009258303e+26\n",
      "Gradient Descent(17/49): loss=1.6841685747599653e+28\n",
      "Gradient Descent(18/49): loss=9.558943871642247e+29\n",
      "Gradient Descent(19/49): loss=5.425431238765081e+31\n",
      "Gradient Descent(20/49): loss=3.07934689486576e+33\n",
      "Gradient Descent(21/49): loss=1.7477647180280075e+35\n",
      "Gradient Descent(22/49): loss=9.919900595496962e+36\n",
      "Gradient Descent(23/49): loss=5.630301768298632e+38\n",
      "Gradient Descent(24/49): loss=3.1956265788113047e+40\n",
      "Gradient Descent(25/49): loss=1.8137623259746006e+42\n",
      "Gradient Descent(26/49): loss=1.029448746276481e+44\n",
      "Gradient Descent(27/49): loss=5.84290844524407e+45\n",
      "Gradient Descent(28/49): loss=3.3162971175580812e+47\n",
      "Gradient Descent(29/49): loss=1.8822520795916134e+49\n",
      "Gradient Descent(30/49): loss=1.0683219161423417e+51\n",
      "Gradient Descent(31/49): loss=6.063543395089063e+52\n",
      "Gradient Descent(32/49): loss=3.441524314776792e+54\n",
      "Gradient Descent(33/49): loss=1.9533280851577075e+56\n",
      "Gradient Descent(34/49): loss=1.1086629816571107e+58\n",
      "Gradient Descent(35/49): loss=6.292509774657731e+59\n",
      "Gradient Descent(36/49): loss=3.571480235136869e+61\n",
      "Gradient Descent(37/49): loss=2.0270880025239485e+63\n",
      "Gradient Descent(38/49): loss=1.1505273722504756e+65\n",
      "Gradient Descent(39/49): loss=6.530122188328316e+66\n",
      "Gradient Descent(40/49): loss=3.7063434406682065e+68\n",
      "Gradient Descent(41/49): loss=2.103633179289842e+70\n",
      "Gradient Descent(42/49): loss=1.1939726104311814e+72\n",
      "Gradient Descent(43/49): loss=6.776707120302744e+73\n",
      "Gradient Descent(44/49): loss=3.846299236108705e+75\n",
      "Gradient Descent(45/49): loss=2.1830687900570664e+77\n",
      "Gradient Descent(46/49): loss=1.2390583908242114e+79\n",
      "Gradient Descent(47/49): loss=7.032603383202352e+80\n",
      "Gradient Descent(48/49): loss=3.991539923516466e+82\n",
      "Gradient Descent(49/49): loss=2.265503980941251e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0139117136500806\n",
      "Gradient Descent(2/49): loss=87.92337882424073\n",
      "Gradient Descent(3/49): loss=4984.234709198234\n",
      "Gradient Descent(4/49): loss=287178.14059721737\n",
      "Gradient Descent(5/49): loss=16599507.624105154\n",
      "Gradient Descent(6/49): loss=960332659.8933388\n",
      "Gradient Descent(7/49): loss=55572465097.45638\n",
      "Gradient Descent(8/49): loss=3216108162171.106\n",
      "Gradient Descent(9/49): loss=186127899160855.72\n",
      "Gradient Descent(10/49): loss=1.077197277220102e+16\n",
      "Gradient Descent(11/49): loss=6.234188056309798e+17\n",
      "Gradient Descent(12/49): loss=3.6079857532635038e+19\n",
      "Gradient Descent(13/49): loss=2.0880928585632836e+21\n",
      "Gradient Descent(14/49): loss=1.2084670841663621e+23\n",
      "Gradient Descent(15/49): loss=6.993906958136073e+24\n",
      "Gradient Descent(16/49): loss=4.047667943810221e+26\n",
      "Gradient Descent(17/49): loss=2.342555585212151e+28\n",
      "Gradient Descent(18/49): loss=1.355735388471125e+30\n",
      "Gradient Descent(19/49): loss=7.846210588965089e+31\n",
      "Gradient Descent(20/49): loss=4.540931890715128e+33\n",
      "Gradient Descent(21/49): loss=2.6280281675532776e+35\n",
      "Gradient Descent(22/49): loss=1.5209503722351464e+37\n",
      "Gradient Descent(23/49): loss=8.802379150138134e+38\n",
      "Gradient Descent(24/49): loss=5.094306830599971e+40\n",
      "Gradient Descent(25/49): loss=2.948289506921861e+42\n",
      "Gradient Descent(26/49): loss=1.7062990718213258e+44\n",
      "Gradient Descent(27/49): loss=9.875069987743573e+45\n",
      "Gradient Descent(28/49): loss=5.7151181099069086e+47\n",
      "Gradient Descent(29/49): loss=3.3075790906520257e+49\n",
      "Gradient Descent(30/49): loss=1.9142350570068423e+51\n",
      "Gradient Descent(31/49): loss=1.1078482941889835e+53\n",
      "Gradient Descent(32/49): loss=6.41158377308442e+54\n",
      "Gradient Descent(33/49): loss=3.7106530465322893e+56\n",
      "Gradient Descent(34/49): loss=2.1475108988735094e+58\n",
      "Gradient Descent(35/49): loss=1.2428548298500638e+60\n",
      "Gradient Descent(36/49): loss=7.192923346241953e+61\n",
      "Gradient Descent(37/49): loss=4.162847101873856e+63\n",
      "Gradient Descent(38/49): loss=2.409214607108727e+65\n",
      "Gradient Descent(39/49): loss=1.3943137667711424e+67\n",
      "Gradient Descent(40/49): loss=8.069479881415114e+68\n",
      "Gradient Descent(41/49): loss=4.6701472156698026e+70\n",
      "Gradient Descent(42/49): loss=2.7028105078073143e+72\n",
      "Gradient Descent(43/49): loss=1.5642300560895514e+74\n",
      "Gradient Descent(44/49): loss=9.052856873636066e+75\n",
      "Gradient Descent(45/49): loss=5.239268818259253e+77\n",
      "Gradient Descent(46/49): loss=3.0321851027960216e+79\n",
      "Gradient Descent(47/49): loss=1.7548529797852439e+81\n",
      "Gradient Descent(48/49): loss=1.0156071863229794e+83\n",
      "Gradient Descent(49/49): loss=5.877745707433067e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0082367413207014\n",
      "Gradient Descent(2/49): loss=87.30829437076603\n",
      "Gradient Descent(3/49): loss=4949.8593826021215\n",
      "Gradient Descent(4/49): loss=285595.2665904276\n",
      "Gradient Descent(5/49): loss=16538385.697180346\n",
      "Gradient Descent(6/49): loss=958701686.5264571\n",
      "Gradient Descent(7/49): loss=55591368852.246925\n",
      "Gradient Descent(8/49): loss=3223826101522.313\n",
      "Gradient Descent(9/49): loss=186959750513339.56\n",
      "Gradient Descent(10/49): loss=1.0842473592391326e+16\n",
      "Gradient Descent(11/49): loss=6.287959956557428e+17\n",
      "Gradient Descent(12/49): loss=3.6466283987692954e+19\n",
      "Gradient Descent(13/49): loss=2.1148197546442542e+21\n",
      "Gradient Descent(14/49): loss=1.2264652274831741e+23\n",
      "Gradient Descent(15/49): loss=7.112743226505559e+24\n",
      "Gradient Descent(16/49): loss=4.124953192679756e+26\n",
      "Gradient Descent(17/49): loss=2.392218914148833e+28\n",
      "Gradient Descent(18/49): loss=1.3873397029271094e+30\n",
      "Gradient Descent(19/49): loss=8.045716219008567e+31\n",
      "Gradient Descent(20/49): loss=4.666020106261405e+33\n",
      "Gradient Descent(21/49): loss=2.7060044177297303e+35\n",
      "Gradient Descent(22/49): loss=1.5693159785119358e+37\n",
      "Gradient Descent(23/49): loss=9.10106659206151e+38\n",
      "Gradient Descent(24/49): loss=5.278058354552871e+40\n",
      "Gradient Descent(25/49): loss=3.060948924202893e+42\n",
      "Gradient Descent(26/49): loss=1.775161941606288e+44\n",
      "Gradient Descent(27/49): loss=1.0294846457616292e+46\n",
      "Gradient Descent(28/49): loss=5.970377186545225e+47\n",
      "Gradient Descent(29/49): loss=3.4624512270650457e+49\n",
      "Gradient Descent(30/49): loss=2.0080085604677628e+51\n",
      "Gradient Descent(31/49): loss=1.1645213504796792e+53\n",
      "Gradient Descent(32/49): loss=6.753506944248858e+54\n",
      "Gradient Descent(33/49): loss=3.9166182764472423e+56\n",
      "Gradient Descent(34/49): loss=2.271397490227466e+58\n",
      "Gradient Descent(35/49): loss=1.3172707153099388e+60\n",
      "Gradient Descent(36/49): loss=7.639359226549907e+61\n",
      "Gradient Descent(37/49): loss=4.430358066416273e+63\n",
      "Gradient Descent(38/49): loss=2.5693349421826135e+65\n",
      "Gradient Descent(39/49): loss=1.4900560961792487e+67\n",
      "Gradient Descent(40/49): loss=8.641408067547897e+68\n",
      "Gradient Descent(41/49): loss=5.011484707277693e+70\n",
      "Gradient Descent(42/49): loss=2.906352619267609e+72\n",
      "Gradient Descent(43/49): loss=1.6855056018143845e+74\n",
      "Gradient Descent(44/49): loss=9.774894879973657e+75\n",
      "Gradient Descent(45/49): loss=5.668837280141959e+77\n",
      "Gradient Descent(46/49): loss=3.2875766443857516e+79\n",
      "Gradient Descent(47/49): loss=1.9065920679310785e+81\n",
      "Gradient Descent(48/49): loss=1.1057060280877203e+83\n",
      "Gradient Descent(49/49): loss=6.412414281552088e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.012771718210579\n",
      "Gradient Descent(2/49): loss=86.9272897762032\n",
      "Gradient Descent(3/49): loss=4882.9951408482175\n",
      "Gradient Descent(4/49): loss=279003.722310241\n",
      "Gradient Descent(5/49): loss=15996970.229619237\n",
      "Gradient Descent(6/49): loss=918097821.8547316\n",
      "Gradient Descent(7/49): loss=52706789472.04562\n",
      "Gradient Descent(8/49): loss=3026095190353.012\n",
      "Gradient Descent(9/49): loss=173744221700931.6\n",
      "Gradient Descent(10/49): loss=9975662936968134.0\n",
      "Gradient Descent(11/49): loss=5.7276210114950445e+17\n",
      "Gradient Descent(12/49): loss=3.288570239415284e+19\n",
      "Gradient Descent(13/49): loss=1.8881655777647843e+21\n",
      "Gradient Descent(14/49): loss=1.0841092799983154e+23\n",
      "Gradient Descent(15/49): loss=6.224522741760216e+24\n",
      "Gradient Descent(16/49): loss=3.573872519182753e+26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=2.05197496112886e+28\n",
      "Gradient Descent(18/49): loss=1.1781621256239765e+30\n",
      "Gradient Descent(19/49): loss=6.7645367065111745e+31\n",
      "Gradient Descent(20/49): loss=3.8839269962436645e+33\n",
      "Gradient Descent(21/49): loss=2.229995869181047e+35\n",
      "Gradient Descent(22/49): loss=1.280374626352094e+37\n",
      "Gradient Descent(23/49): loss=7.351400092105669e+38\n",
      "Gradient Descent(24/49): loss=4.22088052995987e+40\n",
      "Gradient Descent(25/49): loss=2.4234611400526165e+42\n",
      "Gradient Descent(26/49): loss=1.3914546634659957e+44\n",
      "Gradient Descent(27/49): loss=7.989177331885169e+45\n",
      "Gradient Descent(28/49): loss=4.587066766611042e+47\n",
      "Gradient Descent(29/49): loss=2.633710662219403e+49\n",
      "Gradient Descent(30/49): loss=1.5121715477912755e+51\n",
      "Gradient Descent(31/49): loss=8.682285502168714e+52\n",
      "Gradient Descent(32/49): loss=4.9850218152348166e+54\n",
      "Gradient Descent(33/49): loss=2.862200568290421e+56\n",
      "Gradient Descent(34/49): loss=1.6433613325594165e+58\n",
      "Gradient Descent(35/49): loss=9.435524886938739e+59\n",
      "Gradient Descent(36/49): loss=5.41750180731002e+61\n",
      "Gradient Descent(37/49): loss=3.1105133189606085e+63\n",
      "Gradient Descent(38/49): loss=1.7859326035436004e+65\n",
      "Gradient Descent(39/49): loss=1.0254112223077836e+67\n",
      "Gradient Descent(40/49): loss=5.887501984948634e+68\n",
      "Gradient Descent(41/49): loss=3.380368662707086e+70\n",
      "Gradient Descent(42/49): loss=1.940872771682263e+72\n",
      "Gradient Descent(43/49): loss=1.1143716830107244e+74\n",
      "Gradient Descent(44/49): loss=6.398277445150557e+75\n",
      "Gradient Descent(45/49): loss=3.673635546312472e+77\n",
      "Gradient Descent(46/49): loss=2.109254911626129e+79\n",
      "Gradient Descent(47/49): loss=1.2110499874394857e+81\n",
      "Gradient Descent(48/49): loss=6.953365683745e+82\n",
      "Gradient Descent(49/49): loss=3.9923450586963175e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0233825033150277\n",
      "Gradient Descent(2/49): loss=88.15039126151268\n",
      "Gradient Descent(3/49): loss=4987.555194711664\n",
      "Gradient Descent(4/49): loss=287033.2982797486\n",
      "Gradient Descent(5/49): loss=16577979.712403178\n",
      "Gradient Descent(6/49): loss=958479452.7003906\n",
      "Gradient Descent(7/49): loss=55433558796.27447\n",
      "Gradient Descent(8/49): loss=3206313913373.306\n",
      "Gradient Descent(9/49): loss=185461144270178.7\n",
      "Gradient Descent(10/49): loss=1.072763827603577e+16\n",
      "Gradient Descent(11/49): loss=6.205212530495727e+17\n",
      "Gradient Descent(12/49): loss=3.589298914250667e+19\n",
      "Gradient Descent(13/49): loss=2.076169129010473e+21\n",
      "Gradient Descent(14/49): loss=1.2009249654472675e+23\n",
      "Gradient Descent(15/49): loss=6.946547836994745e+24\n",
      "Gradient Descent(16/49): loss=4.0181134301537575e+26\n",
      "Gradient Descent(17/49): loss=2.3242099480474555e+28\n",
      "Gradient Descent(18/49): loss=1.3444000478727632e+30\n",
      "Gradient Descent(19/49): loss=7.776455353328018e+31\n",
      "Gradient Descent(20/49): loss=4.498159455130279e+33\n",
      "Gradient Descent(21/49): loss=2.601884478863205e+35\n",
      "Gradient Descent(22/49): loss=1.5050161980570742e+37\n",
      "Gradient Descent(23/49): loss=8.705512388508123e+38\n",
      "Gradient Descent(24/49): loss=5.035556829513073e+40\n",
      "Gradient Descent(25/49): loss=2.912732927327319e+42\n",
      "Gradient Descent(26/49): loss=1.6848212408632466e+44\n",
      "Gradient Descent(27/49): loss=9.74556433592669e+45\n",
      "Gradient Descent(28/49): loss=5.637157338841742e+47\n",
      "Gradient Descent(29/49): loss=3.26071859642962e+49\n",
      "Gradient Descent(30/49): loss=1.8861076826510283e+51\n",
      "Gradient Descent(31/49): loss=1.0909871813073518e+53\n",
      "Gradient Descent(32/49): loss=6.310631363867822e+54\n",
      "Gradient Descent(33/49): loss=3.650278288596417e+56\n",
      "Gradient Descent(34/49): loss=2.1114419169671904e+58\n",
      "Gradient Descent(35/49): loss=1.2213279690629568e+60\n",
      "Gradient Descent(36/49): loss=7.064565669691713e+61\n",
      "Gradient Descent(37/49): loss=4.08637887329133e+63\n",
      "Gradient Descent(38/49): loss=2.363696945690697e+65\n",
      "Gradient Descent(39/49): loss=1.3672406363454857e+67\n",
      "Gradient Descent(40/49): loss=7.908572886564165e+68\n",
      "Gradient Descent(41/49): loss=4.5745806143735455e+70\n",
      "Gradient Descent(42/49): loss=2.6460890096814642e+72\n",
      "Gradient Descent(43/49): loss=1.5305855634409576e+74\n",
      "Gradient Descent(44/49): loss=8.853414070511133e+75\n",
      "Gradient Descent(45/49): loss=5.121108063224491e+77\n",
      "Gradient Descent(46/49): loss=2.962218595713861e+79\n",
      "Gradient Descent(47/49): loss=1.7134453912046554e+81\n",
      "Gradient Descent(48/49): loss=9.911135906338962e+82\n",
      "Gradient Descent(49/49): loss=5.732929421512519e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.045524458729427\n",
      "Gradient Descent(2/49): loss=91.30933807268678\n",
      "Gradient Descent(3/49): loss=5275.58924792532\n",
      "Gradient Descent(4/49): loss=309766.99600819446\n",
      "Gradient Descent(5/49): loss=18247141.08136064\n",
      "Gradient Descent(6/49): loss=1075819687.6802378\n",
      "Gradient Descent(7/49): loss=63444865063.765976\n",
      "Gradient Descent(8/49): loss=3741853692999.2\n",
      "Gradient Descent(9/49): loss=220692249947985.5\n",
      "Gradient Descent(10/49): loss=1.3016383712859548e+16\n",
      "Gradient Descent(11/49): loss=7.67705225278632e+17\n",
      "Gradient Descent(12/49): loss=4.527921805307155e+19\n",
      "Gradient Descent(13/49): loss=2.6705666434742847e+21\n",
      "Gradient Descent(14/49): loss=1.575099327997673e+23\n",
      "Gradient Descent(15/49): loss=9.289930810687952e+24\n",
      "Gradient Descent(16/49): loss=5.479198255232281e+26\n",
      "Gradient Descent(17/49): loss=3.23162940358041e+28\n",
      "Gradient Descent(18/49): loss=1.9060140042929477e+30\n",
      "Gradient Descent(19/49): loss=1.1241664595032425e+32\n",
      "Gradient Descent(20/49): loss=6.630330238270412e+33\n",
      "Gradient Descent(21/49): loss=3.910566686759261e+35\n",
      "Gradient Descent(22/49): loss=2.306451000491346e+37\n",
      "Gradient Descent(23/49): loss=1.3603440738359718e+39\n",
      "Gradient Descent(24/49): loss=8.02330506404439e+40\n",
      "Gradient Descent(25/49): loss=4.732142800402179e+42\n",
      "Gradient Descent(26/49): loss=2.791016333624356e+44\n",
      "Gradient Descent(27/49): loss=1.646140554738954e+46\n",
      "Gradient Descent(28/49): loss=9.708931808497968e+47\n",
      "Gradient Descent(29/49): loss=5.726324923512482e+49\n",
      "Gradient Descent(30/49): loss=3.3773846367876704e+51\n",
      "Gradient Descent(31/49): loss=1.991980395309556e+53\n",
      "Gradient Descent(32/49): loss=1.1748694099206027e+55\n",
      "Gradient Descent(33/49): loss=6.929376080293564e+56\n",
      "Gradient Descent(34/49): loss=4.086943830241495e+58\n",
      "Gradient Descent(35/49): loss=2.410478184183277e+60\n",
      "Gradient Descent(36/49): loss=1.421699274022114e+62\n",
      "Gradient Descent(37/49): loss=8.38517784154865e+63\n",
      "Gradient Descent(38/49): loss=4.945575250628243e+65\n",
      "Gradient Descent(39/49): loss=2.9168987255622935e+67\n",
      "Gradient Descent(40/49): loss=1.7203859498662256e+69\n",
      "Gradient Descent(41/49): loss=1.0146830915175361e+71\n",
      "Gradient Descent(42/49): loss=5.9845976787455275e+72\n",
      "Gradient Descent(43/49): loss=3.529713826499434e+74\n",
      "Gradient Descent(44/49): loss=2.0818241034362784e+76\n",
      "Gradient Descent(45/49): loss=1.2278592006838332e+78\n",
      "Gradient Descent(46/49): loss=7.24190969936133e+79\n",
      "Gradient Descent(47/49): loss=4.271276060357382e+81\n",
      "Gradient Descent(48/49): loss=2.5191972754632653e+83\n",
      "Gradient Descent(49/49): loss=1.485821759825685e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0396976797710646\n",
      "Gradient Descent(2/49): loss=90.670745821382\n",
      "Gradient Descent(3/49): loss=5239.193428553574\n",
      "Gradient Descent(4/49): loss=308057.49771740026\n",
      "Gradient Descent(5/49): loss=18179763.54578246\n",
      "Gradient Descent(6/49): loss=1073978415.673279\n",
      "Gradient Descent(7/49): loss=63465452312.37426\n",
      "Gradient Descent(8/49): loss=3750765733605.319\n",
      "Gradient Descent(9/49): loss=221674076312111.34\n",
      "Gradient Descent(10/49): loss=1.310127803638971e+16\n",
      "Gradient Descent(11/49): loss=7.743076977858124e+17\n",
      "Gradient Descent(12/49): loss=4.576293226483212e+19\n",
      "Gradient Descent(13/49): loss=2.704669599906335e+21\n",
      "Gradient Descent(14/49): loss=1.5985073158110964e+23\n",
      "Gradient Descent(15/49): loss=9.44745967602554e+24\n",
      "Gradient Descent(16/49): loss=5.583615044038375e+26\n",
      "Gradient Descent(17/49): loss=3.3000148290663606e+28\n",
      "Gradient Descent(18/49): loss=1.9503668847899773e+30\n",
      "Gradient Descent(19/49): loss=1.1527011795689027e+32\n",
      "Gradient Descent(20/49): loss=6.812666990115127e+33\n",
      "Gradient Descent(21/49): loss=4.0264061789583144e+35\n",
      "Gradient Descent(22/49): loss=2.3796769666814537e+37\n",
      "Gradient Descent(23/49): loss=1.4064309992762258e+39\n",
      "Gradient Descent(24/49): loss=8.312254912836617e+40\n",
      "Gradient Descent(25/49): loss=4.9126890527544203e+42\n",
      "Gradient Descent(26/49): loss=2.9034857547238196e+44\n",
      "Gradient Descent(27/49): loss=1.716011218572382e+46\n",
      "Gradient Descent(28/49): loss=1.01419285335753e+48\n",
      "Gradient Descent(29/49): loss=5.99405838766726e+49\n",
      "Gradient Descent(30/49): loss=3.542594077233e+51\n",
      "Gradient Descent(31/49): loss=2.0937354934459446e+53\n",
      "Gradient Descent(32/49): loss=1.2374345524620027e+55\n",
      "Gradient Descent(33/49): loss=7.313456147732678e+56\n",
      "Gradient Descent(34/49): loss=4.3223813912737097e+58\n",
      "Gradient Descent(35/49): loss=2.5546035300179264e+60\n",
      "Gradient Descent(36/49): loss=1.509815679096514e+62\n",
      "Gradient Descent(37/49): loss=8.923276579163234e+63\n",
      "Gradient Descent(38/49): loss=5.273813619149288e+65\n",
      "Gradient Descent(39/49): loss=3.116916733755747e+67\n",
      "Gradient Descent(40/49): loss=1.8421526862251273e+69\n",
      "Gradient Descent(41/49): loss=1.0887446824020342e+71\n",
      "Gradient Descent(42/49): loss=6.434672827732344e+72\n",
      "Gradient Descent(43/49): loss=3.803004971616245e+74\n",
      "Gradient Descent(44/49): loss=2.2476429185032597e+76\n",
      "Gradient Descent(45/49): loss=1.3283965513594369e+78\n",
      "Gradient Descent(46/49): loss=7.851057581863385e+79\n",
      "Gradient Descent(47/49): loss=4.640113307329462e+81\n",
      "Gradient Descent(48/49): loss=2.742388688447971e+83\n",
      "Gradient Descent(49/49): loss=1.6208000150875194e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.044379859064965\n",
      "Gradient Descent(2/49): loss=90.27678148047654\n",
      "Gradient Descent(3/49): loss=5168.581654580233\n",
      "Gradient Descent(4/49): loss=300960.50560565\n",
      "Gradient Descent(5/49): loss=17585574.392399695\n",
      "Gradient Descent(6/49): loss=1028559862.0516013\n",
      "Gradient Descent(7/49): loss=60176915254.98796\n",
      "Gradient Descent(8/49): loss=3521024436732.607\n",
      "Gradient Descent(9/49): loss=206025058989498.97\n",
      "Gradient Descent(10/49): loss=1.2055208128766968e+16\n",
      "Gradient Descent(11/49): loss=7.053919590799049e+17\n",
      "Gradient Descent(12/49): loss=4.127495836677449e+19\n",
      "Gradient Descent(13/49): loss=2.4151432205705743e+21\n",
      "Gradient Descent(14/49): loss=1.4131854881297031e+23\n",
      "Gradient Descent(15/49): loss=8.269046959967275e+24\n",
      "Gradient Descent(16/49): loss=4.838511199958739e+26\n",
      "Gradient Descent(17/49): loss=2.831183665725466e+28\n",
      "Gradient Descent(18/49): loss=1.6566254831924468e+30\n",
      "Gradient Descent(19/49): loss=9.693500373594107e+31\n",
      "Gradient Descent(20/49): loss=5.672009180850658e+33\n",
      "Gradient Descent(21/49): loss=3.3188927537402447e+35\n",
      "Gradient Descent(22/49): loss=1.9420012837929947e+37\n",
      "Gradient Descent(23/49): loss=1.1363334901408509e+39\n",
      "Gradient Descent(24/49): loss=6.649088296658864e+40\n",
      "Gradient Descent(25/49): loss=3.8906162284536267e+42\n",
      "Gradient Descent(26/49): loss=2.2765368666729704e+44\n",
      "Gradient Descent(27/49): loss=1.3320820664394306e+46\n",
      "Gradient Descent(28/49): loss=7.794482302071372e+47\n",
      "Gradient Descent(29/49): loss=4.5608266853779793e+49\n",
      "Gradient Descent(30/49): loss=2.6687006587375308e+51\n",
      "Gradient Descent(31/49): loss=1.561550941801653e+53\n",
      "Gradient Descent(32/49): loss=9.137185678198813e+54\n",
      "Gradient Descent(33/49): loss=5.3464898187411455e+56\n",
      "Gradient Descent(34/49): loss=3.128419886454328e+58\n",
      "Gradient Descent(35/49): loss=1.8305488867960216e+60\n",
      "Gradient Descent(36/49): loss=1.0711187591726977e+62\n",
      "Gradient Descent(37/49): loss=6.267493889549998e+63\n",
      "Gradient Descent(38/49): loss=3.667331873254313e+65\n",
      "Gradient Descent(39/49): loss=2.1458853100776867e+67\n",
      "Gradient Descent(40/49): loss=1.2556332295939676e+69\n",
      "Gradient Descent(41/49): loss=7.347153176622907e+70\n",
      "Gradient Descent(42/49): loss=4.29907862650432e+72\n",
      "Gradient Descent(43/49): loss=2.5155426316239685e+74\n",
      "Gradient Descent(44/49): loss=1.471932774735739e+76\n",
      "Gradient Descent(45/49): loss=8.61279815378268e+77\n",
      "Gradient Descent(46/49): loss=5.0396521710116723e+79\n",
      "Gradient Descent(47/49): loss=2.948878349555671e+81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=1.7254927970023997e+83\n",
      "Gradient Descent(49/49): loss=1.0096467332929427e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.054866708978934\n",
      "Gradient Descent(2/49): loss=91.51685227623182\n",
      "Gradient Descent(3/49): loss=5276.645843889754\n",
      "Gradient Descent(4/49): loss=309417.77102470177\n",
      "Gradient Descent(5/49): loss=18209250.243659373\n",
      "Gradient Descent(6/49): loss=1072737006.29\n",
      "Gradient Descent(7/49): loss=63217046388.63073\n",
      "Gradient Descent(8/49): loss=3725794029530.059\n",
      "Gradient Descent(9/49): loss=219592367985569.0\n",
      "Gradient Descent(10/49): loss=1.2942553953118204e+16\n",
      "Gradient Descent(11/49): loss=7.628235607572197e+17\n",
      "Gradient Descent(12/49): loss=4.496024263014168e+19\n",
      "Gradient Descent(13/49): loss=2.649923468017166e+21\n",
      "Gradient Descent(14/49): loss=1.5618454609163983e+23\n",
      "Gradient Descent(15/49): loss=9.205402911530942e+24\n",
      "Gradient Descent(16/49): loss=5.425597200866483e+26\n",
      "Gradient Descent(17/49): loss=3.197807348776512e+28\n",
      "Gradient Descent(18/49): loss=1.8847642888895527e+30\n",
      "Gradient Descent(19/49): loss=1.1108663025473288e+32\n",
      "Gradient Descent(20/49): loss=6.547364832476333e+33\n",
      "Gradient Descent(21/49): loss=3.858969000373772e+35\n",
      "Gradient Descent(22/49): loss=2.274448137077778e+37\n",
      "Gradient Descent(23/49): loss=1.3405431159907784e+39\n",
      "Gradient Descent(24/49): loss=7.901063192150532e+40\n",
      "Gradient Descent(25/49): loss=4.656828924166037e+42\n",
      "Gradient Descent(26/49): loss=2.7447009473985962e+44\n",
      "Gradient Descent(27/49): loss=1.6177066869597483e+46\n",
      "Gradient Descent(28/49): loss=9.534645031236146e+47\n",
      "Gradient Descent(29/49): loss=5.619650125977249e+49\n",
      "Gradient Descent(30/49): loss=3.312180729847472e+51\n",
      "Gradient Descent(31/49): loss=1.952175125006586e+53\n",
      "Gradient Descent(32/49): loss=1.150597757046301e+55\n",
      "Gradient Descent(33/49): loss=6.781539123010342e+56\n",
      "Gradient Descent(34/49): loss=3.996989616508447e+58\n",
      "Gradient Descent(35/49): loss=2.355796479927214e+60\n",
      "Gradient Descent(36/49): loss=1.38848923497716e+62\n",
      "Gradient Descent(37/49): loss=8.183654114751946e+63\n",
      "Gradient Descent(38/49): loss=4.823385949477549e+65\n",
      "Gradient Descent(39/49): loss=2.842868441333504e+67\n",
      "Gradient Descent(40/49): loss=1.6755658907215105e+69\n",
      "Gradient Descent(41/49): loss=9.875662951298726e+70\n",
      "Gradient Descent(42/49): loss=5.820643596752735e+72\n",
      "Gradient Descent(43/49): loss=3.430644813162962e+74\n",
      "Gradient Descent(44/49): loss=2.0219969902723323e+76\n",
      "Gradient Descent(45/49): loss=1.1917502543496794e+78\n",
      "Gradient Descent(46/49): loss=7.024088935717156e+79\n",
      "Gradient Descent(47/49): loss=4.139946704168068e+81\n",
      "Gradient Descent(48/49): loss=2.4400543430195117e+83\n",
      "Gradient Descent(49/49): loss=1.4381502039371966e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0774520504405376\n",
      "Gradient Descent(2/49): loss=94.79262816205603\n",
      "Gradient Descent(3/49): loss=5581.002412047676\n",
      "Gradient Descent(4/49): loss=333895.5499447146\n",
      "Gradient Descent(5/49): loss=20040528.31927805\n",
      "Gradient Descent(6/49): loss=1203912576.3692412\n",
      "Gradient Descent(7/49): loss=72342567932.90388\n",
      "Gradient Descent(8/49): loss=4347369293042.92\n",
      "Gradient Descent(9/49): loss=261257767316048.06\n",
      "Gradient Descent(10/49): loss=1.5700551489659278e+16\n",
      "Gradient Descent(11/49): loss=9.435425941765585e+17\n",
      "Gradient Descent(12/49): loss=5.67033066096992e+19\n",
      "Gradient Descent(13/49): loss=3.4076528242944235e+21\n",
      "Gradient Descent(14/49): loss=2.047869713790756e+23\n",
      "Gradient Descent(15/49): loss=1.2306918140344104e+25\n",
      "Gradient Descent(16/49): loss=7.395989787164717e+26\n",
      "Gradient Descent(17/49): loss=4.444708609576946e+28\n",
      "Gradient Descent(18/49): loss=2.6711008535240597e+30\n",
      "Gradient Descent(19/49): loss=1.6052300380977611e+32\n",
      "Gradient Descent(20/49): loss=9.646822103025092e+33\n",
      "Gradient Descent(21/49): loss=5.797373241211991e+35\n",
      "Gradient Descent(22/49): loss=3.4840008594651455e+37\n",
      "Gradient Descent(23/49): loss=2.0937520293638945e+39\n",
      "Gradient Descent(24/49): loss=1.2582653498942954e+41\n",
      "Gradient Descent(25/49): loss=7.561696268424768e+42\n",
      "Gradient Descent(26/49): loss=4.544291906370491e+44\n",
      "Gradient Descent(27/49): loss=2.73094662854086e+46\n",
      "Gradient Descent(28/49): loss=1.641195073204574e+48\n",
      "Gradient Descent(29/49): loss=9.862958287654688e+49\n",
      "Gradient Descent(30/49): loss=5.927262869128199e+51\n",
      "Gradient Descent(31/49): loss=3.562059586495522e+53\n",
      "Gradient Descent(32/49): loss=2.140662355946923e+55\n",
      "Gradient Descent(33/49): loss=1.286456672297442e+57\n",
      "Gradient Descent(34/49): loss=7.731115395667048e+58\n",
      "Gradient Descent(35/49): loss=4.646106359289839e+60\n",
      "Gradient Descent(36/49): loss=2.7921332430158177e+62\n",
      "Gradient Descent(37/49): loss=1.6779659017417936e+64\n",
      "Gradient Descent(38/49): loss=1.0083936984207168e+66\n",
      "Gradient Descent(39/49): loss=6.060062662531309e+67\n",
      "Gradient Descent(40/49): loss=3.6418672123121556e+69\n",
      "Gradient Descent(41/49): loss=2.1886237041935637e+71\n",
      "Gradient Descent(42/49): loss=1.3152796187527086e+73\n",
      "Gradient Descent(43/49): loss=7.904330343272562e+74\n",
      "Gradient Descent(44/49): loss=4.7502019559025966e+76\n",
      "Gradient Descent(45/49): loss=2.854690738104789e+78\n",
      "Gradient Descent(46/49): loss=1.7155605773971364e+80\n",
      "Gradient Descent(47/49): loss=1.0309866688652713e+82\n",
      "Gradient Descent(48/49): loss=6.195837823404649e+83\n",
      "Gradient Descent(49/49): loss=3.723462920833187e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.07147172955676\n",
      "Gradient Descent(2/49): loss=94.12986350573232\n",
      "Gradient Descent(3/49): loss=5542.488182526626\n",
      "Gradient Descent(4/49): loss=332050.6572338739\n",
      "Gradient Descent(5/49): loss=19966325.37804019\n",
      "Gradient Descent(6/49): loss=1201836523.8939734\n",
      "Gradient Descent(7/49): loss=72364929806.79396\n",
      "Gradient Descent(8/49): loss=4357646465463.2134\n",
      "Gradient Descent(9/49): loss=262414834124326.34\n",
      "Gradient Descent(10/49): loss=1.5802602123466012e+16\n",
      "Gradient Descent(11/49): loss=9.51634116911157e+17\n",
      "Gradient Descent(12/49): loss=5.730753929320807e+19\n",
      "Gradient Descent(13/49): loss=3.451068886764287e+21\n",
      "Gradient Descent(14/49): loss=2.0782391868301473e+23\n",
      "Gradient Descent(15/49): loss=1.2515189821227567e+25\n",
      "Gradient Descent(16/49): loss=7.536667501744972e+26\n",
      "Gradient Descent(17/49): loss=4.538593338972261e+28\n",
      "Gradient Descent(18/49): loss=2.7331482390422e+30\n",
      "Gradient Descent(19/49): loss=1.645906284222127e+32\n",
      "Gradient Descent(20/49): loss=9.911674229440693e+33\n",
      "Gradient Descent(21/49): loss=5.968826231131301e+35\n",
      "Gradient Descent(22/49): loss=3.5944367977543545e+37\n",
      "Gradient Descent(23/49): loss=2.1645756456584913e+39\n",
      "Gradient Descent(24/49): loss=1.3035109502294619e+41\n",
      "Gradient Descent(25/49): loss=7.849763997744281e+42\n",
      "Gradient Descent(26/49): loss=4.727140559075357e+44\n",
      "Gradient Descent(27/49): loss=2.8466916803711533e+46\n",
      "Gradient Descent(28/49): loss=1.7142823281479688e+48\n",
      "Gradient Descent(29/49): loss=1.0323435870713145e+50\n",
      "Gradient Descent(30/49): loss=6.216789756671164e+51\n",
      "Gradient Descent(31/49): loss=3.7437608333766698e+53\n",
      "Gradient Descent(32/49): loss=2.2544988211134184e+55\n",
      "Gradient Descent(33/49): loss=1.3576628317406215e+57\n",
      "Gradient Descent(34/49): loss=8.175867502914228e+58\n",
      "Gradient Descent(35/49): loss=4.923520616640064e+60\n",
      "Gradient Descent(36/49): loss=2.9649520682470883e+62\n",
      "Gradient Descent(37/49): loss=1.7854989247515107e+64\n",
      "Gradient Descent(38/49): loss=1.07523033658132e+66\n",
      "Gradient Descent(39/49): loss=6.475054454965183e+67\n",
      "Gradient Descent(40/49): loss=3.899288251861321e+69\n",
      "Gradient Descent(41/49): loss=2.348157683746523e+71\n",
      "Gradient Descent(42/49): loss=1.414064350104365e+73\n",
      "Gradient Descent(43/49): loss=8.515518357547992e+74\n",
      "Gradient Descent(44/49): loss=5.128058909934682e+76\n",
      "Gradient Descent(45/49): loss=3.0881253588574805e+78\n",
      "Gradient Descent(46/49): loss=1.8596740793174995e+80\n",
      "Gradient Descent(47/49): loss=1.1198987344752327e+82\n",
      "Gradient Descent(48/49): loss=6.744048268605803e+83\n",
      "Gradient Descent(49/49): loss=4.061276760938333e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0763029294772593\n",
      "Gradient Descent(2/49): loss=93.7226155806688\n",
      "Gradient Descent(3/49): loss=5467.955727233494\n",
      "Gradient Descent(4/49): loss=324414.64361513103\n",
      "Gradient Descent(5/49): loss=19314778.67231668\n",
      "Gradient Descent(6/49): loss=1151085115.092508\n",
      "Gradient Descent(7/49): loss=68620426279.18669\n",
      "Gradient Descent(8/49): loss=4091084996221.9814\n",
      "Gradient Descent(9/49): loss=243913372163686.22\n",
      "Gradient Descent(10/49): loss=1.4542411226812188e+16\n",
      "Gradient Descent(11/49): loss=8.670384846223122e+17\n",
      "Gradient Descent(12/49): loss=5.169406451655468e+19\n",
      "Gradient Descent(13/49): loss=3.0820742371624584e+21\n",
      "Gradient Descent(14/49): loss=1.8375769882646505e+23\n",
      "Gradient Descent(15/49): loss=1.0955898556155159e+25\n",
      "Gradient Descent(16/49): loss=6.532064497158563e+26\n",
      "Gradient Descent(17/49): loss=3.894510931581523e+28\n",
      "Gradient Descent(18/49): loss=2.3219635099247337e+30\n",
      "Gradient Descent(19/49): loss=1.3843880880836537e+32\n",
      "Gradient Descent(20/49): loss=8.253921176472547e+33\n",
      "Gradient Descent(21/49): loss=4.921106687875237e+35\n",
      "Gradient Descent(22/49): loss=2.934034686766287e+37\n",
      "Gradient Descent(23/49): loss=1.7493137396055166e+39\n",
      "Gradient Descent(24/49): loss=1.0429660471898345e+41\n",
      "Gradient Descent(25/49): loss=6.218313793363887e+42\n",
      "Gradient Descent(26/49): loss=3.707448246942199e+44\n",
      "Gradient Descent(27/49): loss=2.2104340437794724e+46\n",
      "Gradient Descent(28/49): loss=1.3178926141259425e+48\n",
      "Gradient Descent(29/49): loss=7.857465583537625e+49\n",
      "Gradient Descent(30/49): loss=4.68473415320154e+51\n",
      "Gradient Descent(31/49): loss=2.7931059770919004e+53\n",
      "Gradient Descent(32/49): loss=1.6652900130811189e+55\n",
      "Gradient Descent(33/49): loss=9.928698912294981e+56\n",
      "Gradient Descent(34/49): loss=5.919633296101778e+58\n",
      "Gradient Descent(35/49): loss=3.529370632533054e+60\n",
      "Gradient Descent(36/49): loss=2.1042616051892397e+62\n",
      "Gradient Descent(37/49): loss=1.254591077020334e+64\n",
      "Gradient Descent(38/49): loss=7.48005270189539e+65\n",
      "Gradient Descent(39/49): loss=4.459715157230123e+67\n",
      "Gradient Descent(40/49): loss=2.658946410710216e+69\n",
      "Gradient Descent(41/49): loss=1.585302147283289e+71\n",
      "Gradient Descent(42/49): loss=9.451799735631889e+72\n",
      "Gradient Descent(43/49): loss=5.635299138122387e+74\n",
      "Gradient Descent(44/49): loss=3.3598465122367425e+76\n",
      "Gradient Descent(45/49): loss=2.0031888829863616e+78\n",
      "Gradient Descent(46/49): loss=1.1943300642768835e+80\n",
      "Gradient Descent(47/49): loss=7.120767864431687e+81\n",
      "Gradient Descent(48/49): loss=4.2455043622989834e+83\n",
      "Gradient Descent(49/49): loss=2.5312308494609646e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0866614824357255\n",
      "Gradient Descent(2/49): loss=94.9792867987076\n",
      "Gradient Descent(3/49): loss=5579.569437083321\n",
      "Gradient Descent(4/49): loss=333315.0961341967\n",
      "Gradient Descent(5/49): loss=19983591.517486684\n",
      "Gradient Descent(6/49): loss=1199358355.759737\n",
      "Gradient Descent(7/49): loss=72005405957.86588\n",
      "Gradient Descent(8/49): loss=4323399008297.9854\n",
      "Gradient Descent(9/49): loss=259596858237570.16\n",
      "Gradient Descent(10/49): loss=1.5587552871996986e+16\n",
      "Gradient Descent(11/49): loss=9.359611830387945e+17\n",
      "Gradient Descent(12/49): loss=5.6200240763071955e+19\n",
      "Gradient Descent(13/49): loss=3.374571652261518e+21\n",
      "Gradient Descent(14/49): loss=2.026278684849866e+23\n",
      "Gradient Descent(15/49): loss=1.216689365047442e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=7.305673386320675e+26\n",
      "Gradient Descent(17/49): loss=4.386728883581411e+28\n",
      "Gradient Descent(18/49): loss=2.634033757075164e+30\n",
      "Gradient Descent(19/49): loss=1.5816190194993574e+32\n",
      "Gradient Descent(20/49): loss=9.496912165151993e+33\n",
      "Gradient Descent(21/49): loss=5.702469404047291e+35\n",
      "Gradient Descent(22/49): loss=3.424076872454638e+37\n",
      "Gradient Descent(23/49): loss=2.0560044426009052e+39\n",
      "Gradient Descent(24/49): loss=1.2345383662396281e+41\n",
      "Gradient Descent(25/49): loss=7.412848659947872e+42\n",
      "Gradient Descent(26/49): loss=4.451082830473215e+44\n",
      "Gradient Descent(27/49): loss=2.672675414349222e+46\n",
      "Gradient Descent(28/49): loss=1.604821600164962e+48\n",
      "Gradient Descent(29/49): loss=9.636233246015533e+49\n",
      "Gradient Descent(30/49): loss=5.786125458560028e+51\n",
      "Gradient Descent(31/49): loss=3.474308577580332e+53\n",
      "Gradient Descent(32/49): loss=2.086166326447462e+55\n",
      "Gradient Descent(33/49): loss=1.2526492234130423e+57\n",
      "Gradient Descent(34/49): loss=7.521596226650766e+58\n",
      "Gradient Descent(35/49): loss=4.516380862203446e+60\n",
      "Gradient Descent(36/49): loss=2.7118839509363554e+62\n",
      "Gradient Descent(37/49): loss=1.6283645661712712e+64\n",
      "Gradient Descent(38/49): loss=9.777598187586977e+65\n",
      "Gradient Descent(39/49): loss=5.871008759585619e+67\n",
      "Gradient Descent(40/49): loss=3.5252771891250755e+69\n",
      "Gradient Descent(41/49): loss=2.116770689513112e+71\n",
      "Gradient Descent(42/49): loss=1.2710257694924398e+73\n",
      "Gradient Descent(43/49): loss=7.631939135955461e+74\n",
      "Gradient Descent(44/49): loss=4.582636825545084e+76\n",
      "Gradient Descent(45/49): loss=2.7516676824510527e+78\n",
      "Gradient Descent(46/49): loss=1.6522529109089725e+80\n",
      "Gradient Descent(47/49): loss=9.921036973387255e+81\n",
      "Gradient Descent(48/49): loss=5.957137310968208e+83\n",
      "Gradient Descent(49/49): loss=3.576993517605385e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1096944887834113\n",
      "Gradient Descent(2/49): loss=98.37509761870372\n",
      "Gradient Descent(3/49): loss=5901.011780862291\n",
      "Gradient Descent(4/49): loss=359652.89905376337\n",
      "Gradient Descent(5/49): loss=21991019.9391117\n",
      "Gradient Descent(6/49): loss=1345850056.8258016\n",
      "Gradient Descent(7/49): loss=82387629767.40376\n",
      "Gradient Descent(8/49): loss=5043839856176.498\n",
      "Gradient Descent(9/49): loss=308795343722061.2\n",
      "Gradient Descent(10/49): loss=1.8905286079604364e+16\n",
      "Gradient Descent(11/49): loss=1.1574351931069394e+18\n",
      "Gradient Descent(12/49): loss=7.086150792884689e+19\n",
      "Gradient Descent(13/49): loss=4.3383459377972474e+21\n",
      "Gradient Descent(14/49): loss=2.6560606895925345e+23\n",
      "Gradient Descent(15/49): loss=1.626117098635121e+25\n",
      "Gradient Descent(16/49): loss=9.955558781646967e+26\n",
      "Gradient Descent(17/49): loss=6.095080784392972e+28\n",
      "Gradient Descent(18/49): loss=3.731584595164908e+30\n",
      "Gradient Descent(19/49): loss=2.2845839268464285e+32\n",
      "Gradient Descent(20/49): loss=1.3986883014233483e+34\n",
      "Gradient Descent(21/49): loss=8.563173983562983e+35\n",
      "Gradient Descent(22/49): loss=5.242622577055522e+37\n",
      "Gradient Descent(23/49): loss=3.209685046484159e+39\n",
      "Gradient Descent(24/49): loss=1.965061941081739e+41\n",
      "Gradient Descent(25/49): loss=1.2030677080039782e+43\n",
      "Gradient Descent(26/49): loss=7.365528178950089e+44\n",
      "Gradient Descent(27/49): loss=4.509389205111068e+46\n",
      "Gradient Descent(28/49): loss=2.7607783867131563e+48\n",
      "Gradient Descent(29/49): loss=1.6902283111654422e+50\n",
      "Gradient Descent(30/49): loss=1.0348066174432893e+52\n",
      "Gradient Descent(31/49): loss=6.3353851573227255e+53\n",
      "Gradient Descent(32/49): loss=3.8787058775090174e+55\n",
      "Gradient Descent(33/49): loss=2.374655827646728e+57\n",
      "Gradient Descent(34/49): loss=1.4538329220770993e+59\n",
      "Gradient Descent(35/49): loss=8.900785287314014e+60\n",
      "Gradient Descent(36/49): loss=5.449317973737839e+62\n",
      "Gradient Descent(37/49): loss=3.336229941556466e+64\n",
      "Gradient Descent(38/49): loss=2.0425363828242805e+66\n",
      "Gradient Descent(39/49): loss=1.2504998001470367e+68\n",
      "Gradient Descent(40/49): loss=7.655921154293111e+69\n",
      "Gradient Descent(41/49): loss=4.687176176586405e+71\n",
      "Gradient Descent(42/49): loss=2.8696247084571366e+73\n",
      "Gradient Descent(43/49): loss=1.7568671748508847e+75\n",
      "Gradient Descent(44/49): loss=1.0756048555659571e+77\n",
      "Gradient Descent(45/49): loss=6.585163761257323e+78\n",
      "Gradient Descent(46/49): loss=4.031627557106874e+80\n",
      "Gradient Descent(47/49): loss=2.4682788991294872e+82\n",
      "Gradient Descent(48/49): loss=1.511151672020965e+84\n",
      "Gradient Descent(49/49): loss=9.251707238825919e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.103558890677788\n",
      "Gradient Descent(2/49): loss=97.68748354473799\n",
      "Gradient Descent(3/49): loss=5860.277467312641\n",
      "Gradient Descent(4/49): loss=357663.32654750586\n",
      "Gradient Descent(5/49): loss=21909376.166999754\n",
      "Gradient Descent(6/49): loss=1343512188.1261592\n",
      "Gradient Descent(7/49): loss=82411853156.63242\n",
      "Gradient Descent(8/49): loss=5055675763497.291\n",
      "Gradient Descent(9/49): loss=310156880032897.75\n",
      "Gradient Descent(10/49): loss=1.9027753088376468e+16\n",
      "Gradient Descent(11/49): loss=1.1673330603040468e+18\n",
      "Gradient Descent(12/49): loss=7.1614741530368975e+19\n",
      "Gradient Descent(13/49): loss=4.3934954827300266e+21\n",
      "Gradient Descent(14/49): loss=2.6953674162290515e+23\n",
      "Gradient Descent(15/49): loss=1.653582145108213e+25\n",
      "Gradient Descent(16/49): loss=1.0144568471538932e+27\n",
      "Gradient Descent(17/49): loss=6.223595846955235e+28\n",
      "Gradient Descent(18/49): loss=3.818116601220324e+30\n",
      "Gradient Descent(19/49): loss=2.342378062801535e+32\n",
      "Gradient Descent(20/49): loss=1.437026566371071e+34\n",
      "Gradient Descent(21/49): loss=8.816020715410137e+35\n",
      "Gradient Descent(22/49): loss=5.408544495547909e+37\n",
      "Gradient Descent(23/49): loss=3.318090383935367e+39\n",
      "Gradient Descent(24/49): loss=2.0356167551239682e+41\n",
      "Gradient Descent(25/49): loss=1.2488314344311333e+43\n",
      "Gradient Descent(26/49): loss=7.6614615580153e+44\n",
      "Gradient Descent(27/49): loss=4.700233481205171e+46\n",
      "Gradient Descent(28/49): loss=2.8835483426435473e+48\n",
      "Gradient Descent(29/49): loss=1.7690293636711814e+50\n",
      "Gradient Descent(30/49): loss=1.0852826162997041e+52\n",
      "Gradient Descent(31/49): loss=6.658105181465264e+53\n",
      "Gradient Descent(32/49): loss=4.0846839285604766e+55\n",
      "Gradient Descent(33/49): loss=2.505914572014688e+57\n",
      "Gradient Descent(34/49): loss=1.5373546526643012e+59\n",
      "Gradient Descent(35/49): loss=9.431523941250754e+60\n",
      "Gradient Descent(36/49): loss=5.786149845139886e+62\n",
      "Gradient Descent(37/49): loss=3.5497476589104225e+64\n",
      "Gradient Descent(38/49): loss=2.1777362804601424e+66\n",
      "Gradient Descent(39/49): loss=1.3360204056555592e+68\n",
      "Gradient Descent(40/49): loss=8.196357567918618e+69\n",
      "Gradient Descent(41/49): loss=5.028387073789702e+71\n",
      "Gradient Descent(42/49): loss=3.084867437070117e+73\n",
      "Gradient Descent(43/49): loss=1.8925367050399767e+75\n",
      "Gradient Descent(44/49): loss=1.1610531904493517e+77\n",
      "Gradient Descent(45/49): loss=7.1229504160351114e+78\n",
      "Gradient Descent(46/49): loss=4.369862039624415e+80\n",
      "Gradient Descent(47/49): loss=2.680868619043367e+82\n",
      "Gradient Descent(48/49): loss=1.6446872893015236e+84\n",
      "Gradient Descent(49/49): loss=1.0089999414276521e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1085409294474617\n",
      "Gradient Descent(2/49): loss=97.2666229735223\n",
      "Gradient Descent(3/49): loss=5781.644850523574\n",
      "Gradient Descent(4/49): loss=349452.82525483635\n",
      "Gradient Descent(5/49): loss=21195537.598881148\n",
      "Gradient Descent(6/49): loss=1286859557.6139553\n",
      "Gradient Descent(7/49): loss=78153254114.7798\n",
      "Gradient Descent(8/49): loss=4746816472847.286\n",
      "Gradient Descent(9/49): loss=288316806393538.06\n",
      "Gradient Descent(10/49): loss=1.7512220809390696e+16\n",
      "Gradient Descent(11/49): loss=1.063686502986852e+18\n",
      "Gradient Descent(12/49): loss=6.46080198440022e+19\n",
      "Gradient Descent(13/49): loss=3.9242740044548633e+21\n",
      "Gradient Descent(14/49): loss=2.383593819113846e+23\n",
      "Gradient Descent(15/49): loss=1.4477887187919233e+25\n",
      "Gradient Descent(16/49): loss=8.793831286462251e+26\n",
      "Gradient Descent(17/49): loss=5.341350424371928e+28\n",
      "Gradient Descent(18/49): loss=3.2443224627754967e+30\n",
      "Gradient Descent(19/49): loss=1.9705930913635436e+32\n",
      "Gradient Descent(20/49): loss=1.1969331583259913e+34\n",
      "Gradient Descent(21/49): loss=7.270141115431927e+35\n",
      "Gradient Descent(22/49): loss=4.415864952105168e+37\n",
      "Gradient Descent(23/49): loss=2.682184976283238e+39\n",
      "Gradient Descent(24/49): loss=1.629152232921982e+41\n",
      "Gradient Descent(25/49): loss=9.895428620710197e+42\n",
      "Gradient Descent(26/49): loss=6.010457807982265e+44\n",
      "Gradient Descent(27/49): loss=3.650736561923977e+46\n",
      "Gradient Descent(28/49): loss=2.2174479665872105e+48\n",
      "Gradient Descent(29/49): loss=1.3468721725378134e+50\n",
      "Gradient Descent(30/49): loss=8.180866818483644e+51\n",
      "Gradient Descent(31/49): loss=4.969037393924441e+53\n",
      "Gradient Descent(32/49): loss=3.018180489924667e+55\n",
      "Gradient Descent(33/49): loss=1.833235040835041e+57\n",
      "Gradient Descent(34/49): loss=1.1135022329394691e+59\n",
      "Gradient Descent(35/49): loss=6.763383827730077e+60\n",
      "Gradient Descent(36/49): loss=4.1080618833107744e+62\n",
      "Gradient Descent(37/49): loss=2.495226186619502e+64\n",
      "Gradient Descent(38/49): loss=1.5155939465483158e+66\n",
      "Gradient Descent(39/49): loss=9.205678519773342e+67\n",
      "Gradient Descent(40/49): loss=5.591505376649011e+69\n",
      "Gradient Descent(41/49): loss=3.396265936284795e+71\n",
      "Gradient Descent(42/49): loss=2.0628831652632912e+73\n",
      "Gradient Descent(43/49): loss=1.2529899110850543e+75\n",
      "Gradient Descent(44/49): loss=7.610628385154123e+76\n",
      "Gradient Descent(45/49): loss=4.6226760410827954e+78\n",
      "Gradient Descent(46/49): loss=2.8078014980320443e+80\n",
      "Gradient Descent(47/49): loss=1.7054513840654873e+82\n",
      "Gradient Descent(48/49): loss=1.0358867695773597e+84\n",
      "Gradient Descent(49/49): loss=6.29194950622067e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.118766823685404\n",
      "Gradient Descent(2/49): loss=98.53950269071532\n",
      "Gradient Descent(3/49): loss=5896.850551477118\n",
      "Gradient Descent(4/49): loss=358812.0829049451\n",
      "Gradient Descent(5/49): loss=21912050.71396103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=1339548038.1768248\n",
      "Gradient Descent(7/49): loss=81917226973.25566\n",
      "Gradient Descent(8/49): loss=5009987463940.76\n",
      "Gradient Descent(9/49): loss=306416447278688.0\n",
      "Gradient Descent(10/49): loss=1.8740965081897196e+16\n",
      "Gradient Descent(11/49): loss=1.146233875003645e+18\n",
      "Gradient Descent(12/49): loss=7.01059653192522e+19\n",
      "Gradient Descent(13/49): loss=4.287823004185299e+21\n",
      "Gradient Descent(14/49): loss=2.6225197720262487e+23\n",
      "Gradient Descent(15/49): loss=1.6039864924070755e+25\n",
      "Gradient Descent(16/49): loss=9.810308094039512e+26\n",
      "Gradient Descent(17/49): loss=6.000184270481805e+28\n",
      "Gradient Descent(18/49): loss=3.6698349299914564e+30\n",
      "Gradient Descent(19/49): loss=2.244545802388052e+32\n",
      "Gradient Descent(20/49): loss=1.3728099371382738e+34\n",
      "Gradient Descent(21/49): loss=8.39638523555e+35\n",
      "Gradient Descent(22/49): loss=5.13540025587747e+37\n",
      "Gradient Descent(23/49): loss=3.1409154116101954e+39\n",
      "Gradient Descent(24/49): loss=1.921047850478659e+41\n",
      "Gradient Descent(25/49): loss=1.1749519997222754e+43\n",
      "Gradient Descent(26/49): loss=7.186245784078498e+44\n",
      "Gradient Descent(27/49): loss=4.395254315188562e+46\n",
      "Gradient Descent(28/49): loss=2.6882270765055806e+48\n",
      "Gradient Descent(29/49): loss=1.6441744428496643e+50\n",
      "Gradient Descent(30/49): loss=1.005610583326926e+52\n",
      "Gradient Descent(31/49): loss=6.150519184244426e+53\n",
      "Gradient Descent(32/49): loss=3.7617828275640056e+55\n",
      "Gradient Descent(33/49): loss=2.3007830099946005e+57\n",
      "Gradient Descent(34/49): loss=1.4072057590064956e+59\n",
      "Gradient Descent(35/49): loss=8.606757089125519e+60\n",
      "Gradient Descent(36/49): loss=5.2640679671117095e+62\n",
      "Gradient Descent(37/49): loss=3.219611204943062e+64\n",
      "Gradient Descent(38/49): loss=1.9691798008228305e+66\n",
      "Gradient Descent(39/49): loss=1.2043904810665577e+68\n",
      "Gradient Descent(40/49): loss=7.366297533001366e+69\n",
      "Gradient Descent(41/49): loss=4.505377632729998e+71\n",
      "Gradient Descent(42/49): loss=2.7555807408763763e+73\n",
      "Gradient Descent(43/49): loss=1.6853693160650272e+75\n",
      "Gradient Descent(44/49): loss=1.0308062069812992e+77\n",
      "Gradient Descent(45/49): loss=6.304620751207353e+78\n",
      "Gradient Descent(46/49): loss=3.856034485178018e+80\n",
      "Gradient Descent(47/49): loss=2.3584292438263913e+82\n",
      "Gradient Descent(48/49): loss=1.4424633699505761e+84\n",
      "Gradient Descent(49/49): loss=8.822399820116607e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1422517737580486\n",
      "Gradient Descent(2/49): loss=102.05861237709097\n",
      "Gradient Descent(3/49): loss=6236.170207054915\n",
      "Gradient Descent(4/49): loss=387132.3957418659\n",
      "Gradient Descent(5/49): loss=24110732.594658047\n",
      "Gradient Descent(6/49): loss=1502979358.0880961\n",
      "Gradient Descent(7/49): loss=93715296587.63914\n",
      "Gradient Descent(8/49): loss=5843892297289.781\n",
      "Gradient Descent(9/49): loss=364421678259142.94\n",
      "Gradient Descent(10/49): loss=2.272528290970075e+16\n",
      "Gradient Descent(11/49): loss=1.417148382223133e+18\n",
      "Gradient Descent(12/49): loss=8.837341406767732e+19\n",
      "Gradient Descent(13/49): loss=5.510969729649281e+21\n",
      "Gradient Descent(14/49): loss=3.43664318808606e+23\n",
      "Gradient Descent(15/49): loss=2.143092267040602e+25\n",
      "Gradient Descent(16/49): loss=1.336433327005488e+27\n",
      "Gradient Descent(17/49): loss=8.334004409776019e+28\n",
      "Gradient Descent(18/49): loss=5.197089007912878e+30\n",
      "Gradient Descent(19/49): loss=3.2409071116448096e+32\n",
      "Gradient Descent(20/49): loss=2.0210311754871043e+34\n",
      "Gradient Descent(21/49): loss=1.2603159768661646e+36\n",
      "Gradient Descent(22/49): loss=7.859336267624148e+37\n",
      "Gradient Descent(23/49): loss=4.9010857357580287e+39\n",
      "Gradient Descent(24/49): loss=3.056319334268779e+41\n",
      "Gradient Descent(25/49): loss=1.9059221520803363e+43\n",
      "Gradient Descent(26/49): loss=1.1885339365756186e+45\n",
      "Gradient Descent(27/49): loss=7.411703131998708e+46\n",
      "Gradient Descent(28/49): loss=4.621941505107754e+48\n",
      "Gradient Descent(29/49): loss=2.8822448627778524e+50\n",
      "Gradient Descent(30/49): loss=1.797369231053387e+52\n",
      "Gradient Descent(31/49): loss=1.1208402847577418e+54\n",
      "Gradient Descent(32/49): loss=6.989565205806625e+55\n",
      "Gradient Descent(33/49): loss=4.358696098863198e+57\n",
      "Gradient Descent(34/49): loss=2.718084905547823e+59\n",
      "Gradient Descent(35/49): loss=1.6949990057103836e+61\n",
      "Gradient Descent(36/49): loss=1.0570021648312527e+63\n",
      "Gradient Descent(37/49): loss=6.591470394342266e+64\n",
      "Gradient Descent(38/49): loss=4.110443990095994e+66\n",
      "Gradient Descent(39/49): loss=2.563274775567316e+68\n",
      "Gradient Descent(40/49): loss=1.598459336969638e+70\n",
      "Gradient Descent(41/49): loss=9.967999827017747e+71\n",
      "Gradient Descent(42/49): loss=6.216049307816315e+73\n",
      "Gradient Descent(43/49): loss=3.876331226699443e+75\n",
      "Gradient Descent(44/49): loss=2.417281947907163e+77\n",
      "Gradient Descent(45/49): loss=1.5074181420386965e+79\n",
      "Gradient Descent(46/49): loss=9.400266513861674e+80\n",
      "Gradient Descent(47/49): loss=5.8620105508429266e+82\n",
      "Gradient Descent(48/49): loss=3.6555524939129826e+84\n",
      "Gradient Descent(49/49): loss=2.279604227909802e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1359591631341486\n",
      "Gradient Descent(2/49): loss=101.34545935248407\n",
      "Gradient Descent(3/49): loss=6193.11027212747\n",
      "Gradient Descent(4/49): loss=384988.31786014413\n",
      "Gradient Descent(5/49): loss=24020983.775646806\n",
      "Gradient Descent(6/49): loss=1500349846.968339\n",
      "Gradient Descent(7/49): loss=93741462069.1557\n",
      "Gradient Descent(8/49): loss=5857505845557.735\n",
      "Gradient Descent(9/49): loss=366021453581361.6\n",
      "Gradient Descent(10/49): loss=2.28720072832477e+16\n",
      "Gradient Descent(11/49): loss=1.4292336294669484e+18\n",
      "Gradient Descent(12/49): loss=8.93105045843284e+19\n",
      "Gradient Descent(13/49): loss=5.580870864647721e+21\n",
      "Gradient Descent(14/49): loss=3.487397396721216e+23\n",
      "Gradient Descent(15/49): loss=2.1792191933717374e+25\n",
      "Gradient Descent(16/49): loss=1.361759440832169e+27\n",
      "Gradient Descent(17/49): loss=8.509418348897876e+28\n",
      "Gradient Descent(18/49): loss=5.317400307089757e+30\n",
      "Gradient Descent(19/49): loss=3.3227589563450505e+32\n",
      "Gradient Descent(20/49): loss=2.0763392720311873e+34\n",
      "Gradient Descent(21/49): loss=1.2974714173703351e+36\n",
      "Gradient Descent(22/49): loss=8.107692714673579e+37\n",
      "Gradient Descent(23/49): loss=5.0663683435068514e+39\n",
      "Gradient Descent(24/49): loss=3.1658930716070508e+41\n",
      "Gradient Descent(25/49): loss=1.9783162733710205e+43\n",
      "Gradient Descent(26/49): loss=1.2362184031370857e+45\n",
      "Gradient Descent(27/49): loss=7.724932362057238e+46\n",
      "Gradient Descent(28/49): loss=4.827187481348476e+48\n",
      "Gradient Descent(29/49): loss=3.01643275150724e+50\n",
      "Gradient Descent(30/49): loss=1.8849208943141804e+52\n",
      "Gradient Descent(31/49): loss=1.1778571148476098e+54\n",
      "Gradient Descent(32/49): loss=7.36024194533594e+55\n",
      "Gradient Descent(33/49): loss=4.599298234989317e+57\n",
      "Gradient Descent(34/49): loss=2.874028382692026e+59\n",
      "Gradient Descent(35/49): loss=1.7959346670935272e+61\n",
      "Gradient Descent(36/49): loss=1.122251035477669e+63\n",
      "Gradient Descent(37/49): loss=7.012768391340988e+64\n",
      "Gradient Descent(38/49): loss=4.382167532566278e+66\n",
      "Gradient Descent(39/49): loss=2.738346857025735e+68\n",
      "Gradient Descent(40/49): loss=1.711149437728464e+70\n",
      "Gradient Descent(41/49): loss=1.0692700929124554e+72\n",
      "Gradient Descent(42/49): loss=6.681698900095932e+73\n",
      "Gradient Descent(43/49): loss=4.175287468289717e+75\n",
      "Gradient Descent(44/49): loss=2.609070792251173e+77\n",
      "Gradient Descent(45/49): loss=1.6303668790897784e+79\n",
      "Gradient Descent(46/49): loss=1.018790355680414e+81\n",
      "Gradient Descent(47/49): loss=6.366259043528238e+82\n",
      "Gradient Descent(48/49): loss=3.978174114363014e+84\n",
      "Gradient Descent(49/49): loss=2.485897789578015e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1410938589755717\n",
      "Gradient Descent(2/49): loss=100.91065180870469\n",
      "Gradient Descent(3/49): loss=6110.191510847121\n",
      "Gradient Descent(4/49): loss=376165.8822153958\n",
      "Gradient Descent(5/49): loss=23239545.37624665\n",
      "Gradient Descent(6/49): loss=1437173093.256402\n",
      "Gradient Descent(7/49): loss=88903864444.40108\n",
      "Gradient Descent(8/49): loss=5500117952467.552\n",
      "Gradient Descent(9/49): loss=340279381434216.0\n",
      "Gradient Descent(10/49): loss=2.1052470069112424e+16\n",
      "Gradient Descent(11/49): loss=1.3024817788287424e+18\n",
      "Gradient Descent(12/49): loss=8.058247791267558e+19\n",
      "Gradient Descent(13/49): loss=4.985511149536324e+21\n",
      "Gradient Descent(14/49): loss=3.0844575722520654e+23\n",
      "Gradient Descent(15/49): loss=1.9083055812005333e+25\n",
      "Gradient Descent(16/49): loss=1.1806387781369059e+27\n",
      "Gradient Descent(17/49): loss=7.304427244263856e+28\n",
      "Gradient Descent(18/49): loss=4.5191347594412984e+30\n",
      "Gradient Descent(19/49): loss=2.7959179133053173e+32\n",
      "Gradient Descent(20/49): loss=1.7297906335201254e+34\n",
      "Gradient Descent(21/49): loss=1.0701943793272869e+36\n",
      "Gradient Descent(22/49): loss=6.621125050381411e+37\n",
      "Gradient Descent(23/49): loss=4.096386392950192e+39\n",
      "Gradient Descent(24/49): loss=2.534370118774877e+41\n",
      "Gradient Descent(25/49): loss=1.5679751084990966e+43\n",
      "Gradient Descent(26/49): loss=9.700816477671287e+44\n",
      "Gradient Descent(27/49): loss=6.001743256214145e+46\n",
      "Gradient Descent(28/49): loss=3.713184575383192e+48\n",
      "Gradient Descent(29/49): loss=2.2972891545448817e+50\n",
      "Gradient Descent(30/49): loss=1.4212968282205358e+52\n",
      "Gradient Descent(31/49): loss=8.79334092494747e+53\n",
      "Gradient Descent(32/49): loss=5.440302341289599e+55\n",
      "Gradient Descent(33/49): loss=3.3658298725427616e+57\n",
      "Gradient Descent(34/49): loss=2.0823862388897695e+59\n",
      "Gradient Descent(35/49): loss=1.288339759324055e+61\n",
      "Gradient Descent(36/49): loss=7.970756358532679e+62\n",
      "Gradient Descent(37/49): loss=4.931382150343867e+64\n",
      "Gradient Descent(38/49): loss=3.050968919241528e+66\n",
      "Gradient Descent(39/49): loss=1.8875866972769015e+68\n",
      "Gradient Descent(40/49): loss=1.1678203331623782e+70\n",
      "Gradient Descent(41/49): loss=7.225121540191842e+71\n",
      "Gradient Descent(42/49): loss=4.470069563627465e+73\n",
      "Gradient Descent(43/49): loss=2.765562045221744e+75\n",
      "Gradient Descent(44/49): loss=1.7110099333139797e+77\n",
      "Gradient Descent(45/49): loss=1.0585750541945867e+79\n",
      "Gradient Descent(46/49): loss=6.549238105197169e+80\n",
      "Gradient Descent(47/49): loss=4.051911065597633e+82\n",
      "Gradient Descent(48/49): loss=2.5068539301516495e+84\n",
      "Gradient Descent(49/49): loss=1.5509512734554234e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1511827327279684\n",
      "Gradient Descent(2/49): loss=102.19932470098038\n",
      "Gradient Descent(3/49): loss=6229.02854622299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=385999.6527098894\n",
      "Gradient Descent(5/49): loss=24006414.325546205\n",
      "Gradient Descent(6/49): loss=1494615198.6685188\n",
      "Gradient Descent(7/49): loss=93083804483.20941\n",
      "Gradient Descent(8/49): loss=5797805865230.515\n",
      "Gradient Descent(9/49): loss=361133176148334.0\n",
      "Gradient Descent(10/49): loss=2.249446151349226e+16\n",
      "Gradient Descent(11/49): loss=1.4011519716448696e+18\n",
      "Gradient Descent(12/49): loss=8.727610109671136e+19\n",
      "Gradient Descent(13/49): loss=5.436327038835924e+21\n",
      "Gradient Descent(14/49): loss=3.386225371675141e+23\n",
      "Gradient Descent(15/49): loss=2.1092407741070784e+25\n",
      "Gradient Descent(16/49): loss=1.313821793344912e+27\n",
      "Gradient Descent(17/49): loss=8.18364471089532e+28\n",
      "Gradient Descent(18/49): loss=5.097498090329763e+30\n",
      "Gradient Descent(19/49): loss=3.175172885400072e+32\n",
      "Gradient Descent(20/49): loss=1.9777786425067173e+34\n",
      "Gradient Descent(21/49): loss=1.231935551250339e+36\n",
      "Gradient Descent(22/49): loss=7.673584747216917e+37\n",
      "Gradient Descent(23/49): loss=4.779787612522521e+39\n",
      "Gradient Descent(24/49): loss=2.9772746862716213e+41\n",
      "Gradient Descent(25/49): loss=1.8545101322690152e+43\n",
      "Gradient Descent(26/49): loss=1.1551530151205713e+45\n",
      "Gradient Descent(27/49): loss=7.195315167728793e+46\n",
      "Gradient Descent(28/49): loss=4.481878996571238e+48\n",
      "Gradient Descent(29/49): loss=2.7917108384631294e+50\n",
      "Gradient Descent(30/49): loss=1.7389245473951507e+52\n",
      "Gradient Descent(31/49): loss=1.0831560847462624e+54\n",
      "Gradient Descent(32/49): loss=6.74685457560715e+55\n",
      "Gradient Descent(33/49): loss=4.202538055727648e+57\n",
      "Gradient Descent(34/49): loss=2.617712581755127e+59\n",
      "Gradient Descent(35/49): loss=1.6305430360921867e+61\n",
      "Gradient Descent(36/49): loss=1.0156464888769969e+63\n",
      "Gradient Descent(37/49): loss=6.326345073604353e+64\n",
      "Gradient Descent(38/49): loss=3.9406075271890286e+66\n",
      "Gradient Descent(39/49): loss=2.4545590704700173e+68\n",
      "Gradient Descent(40/49): loss=1.5289165918851117e+70\n",
      "Gradient Descent(41/49): loss=9.523445465478061e+71\n",
      "Gradient Descent(42/49): loss=5.932044561182318e+73\n",
      "Gradient Descent(43/49): loss=3.695002276582703e+75\n",
      "Gradient Descent(44/49): loss=2.3015743868974184e+77\n",
      "Gradient Descent(45/49): loss=1.4336241934122296e+79\n",
      "Gradient Descent(46/49): loss=8.929880083986458e+80\n",
      "Gradient Descent(47/49): loss=5.562319517263379e+82\n",
      "Gradient Descent(48/49): loss=3.4647048024319355e+84\n",
      "Gradient Descent(49/49): loss=2.1581247410793883e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1751239053644498\n",
      "Gradient Descent(2/49): loss=105.84505577978753\n",
      "Gradient Descent(3/49): loss=6587.04610332443\n",
      "Gradient Descent(4/49): loss=416431.8092744518\n",
      "Gradient Descent(5/49): loss=26412592.888119824\n",
      "Gradient Descent(6/49): loss=1676764574.0592685\n",
      "Gradient Descent(7/49): loss=106475287708.61142\n",
      "Gradient Descent(8/49): loss=6761765502879.391\n",
      "Gradient Descent(9/49): loss=429419514358561.75\n",
      "Gradient Descent(10/49): loss=2.7271346342258664e+16\n",
      "Gradient Descent(11/49): loss=1.731937941053394e+18\n",
      "Gradient Descent(12/49): loss=1.0999130605246664e+20\n",
      "Gradient Descent(13/49): loss=6.98529059258429e+21\n",
      "Gradient Descent(14/49): loss=4.4361949445160486e+23\n",
      "Gradient Descent(15/49): loss=2.8173238723295807e+25\n",
      "Gradient Descent(16/49): loss=1.7892166471979172e+27\n",
      "Gradient Descent(17/49): loss=1.1362897419536484e+29\n",
      "Gradient Descent(18/49): loss=7.216311005903693e+30\n",
      "Gradient Descent(19/49): loss=4.582910732981136e+32\n",
      "Gradient Descent(20/49): loss=2.9104996680776046e+34\n",
      "Gradient Descent(21/49): loss=1.8483904251143193e+36\n",
      "Gradient Descent(22/49): loss=1.1738696283463546e+38\n",
      "Gradient Descent(23/49): loss=7.454972096982706e+39\n",
      "Gradient Descent(24/49): loss=4.73447882326622e+41\n",
      "Gradient Descent(25/49): loss=3.006757025560299e+43\n",
      "Gradient Descent(26/49): loss=1.9095212267777388e+45\n",
      "Gradient Descent(27/49): loss=1.2126923740488538e+47\n",
      "Gradient Descent(28/49): loss=7.701526295980948e+48\n",
      "Gradient Descent(29/49): loss=4.891059641915153e+50\n",
      "Gradient Descent(30/49): loss=3.1061978498021953e+52\n",
      "Gradient Descent(31/49): loss=1.9726737738854937e+54\n",
      "Gradient Descent(32/49): loss=1.2527990831052359e+56\n",
      "Gradient Descent(33/49): loss=7.95623464663361e+57\n",
      "Gradient Descent(34/49): loss=5.0528189720087225e+59\n",
      "Gradient Descent(35/49): loss=3.2089274258262086e+61\n",
      "Gradient Descent(36/49): loss=2.037914930509781e+63\n",
      "Gradient Descent(37/49): loss=1.2942322193296022e+65\n",
      "Gradient Descent(38/49): loss=8.21936682671941e+66\n",
      "Gradient Descent(39/49): loss=5.219928079612287e+68\n",
      "Gradient Descent(40/49): loss=3.315054520714246e+70\n",
      "Gradient Descent(41/49): loss=2.105313772086345e+72\n",
      "Gradient Descent(42/49): loss=1.3370356509193872e+74\n",
      "Gradient Descent(43/49): loss=8.491201432924074e+75\n",
      "Gradient Descent(44/49): loss=5.392563894980115e+77\n",
      "Gradient Descent(45/49): loss=3.4246914987422645e+79\n",
      "Gradient Descent(46/49): loss=2.1749416585449293e+81\n",
      "Gradient Descent(47/49): loss=1.3812546969008455e+83\n",
      "Gradient Descent(48/49): loss=8.772026275808402e+84\n",
      "Gradient Descent(49/49): loss=5.570909199883602e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1686725469258405\n",
      "Gradient Descent(2/49): loss=105.10566163622552\n",
      "Gradient Descent(3/49): loss=6541.551037226096\n",
      "Gradient Descent(4/49): loss=414122.83430932084\n",
      "Gradient Descent(5/49): loss=26314022.666876994\n",
      "Gradient Descent(6/49): loss=1673810548.1580462\n",
      "Gradient Descent(7/49): loss=106503467117.68839\n",
      "Gradient Descent(8/49): loss=6777403932360.669\n",
      "Gradient Descent(9/49): loss=431296493006015.44\n",
      "Gradient Descent(10/49): loss=2.7446846779042404e+16\n",
      "Gradient Descent(11/49): loss=1.7466673884424868e+18\n",
      "Gradient Descent(12/49): loss=1.1115483078540978e+20\n",
      "Gradient Descent(13/49): loss=7.073699196469819e+21\n",
      "Gradient Descent(14/49): loss=4.5015789306162396e+23\n",
      "Gradient Descent(15/49): loss=2.8647264773125128e+25\n",
      "Gradient Descent(16/49): loss=1.823062081138217e+27\n",
      "Gradient Descent(17/49): loss=1.1601649881635019e+29\n",
      "Gradient Descent(18/49): loss=7.383088127817732e+30\n",
      "Gradient Descent(19/49): loss=4.698468827419137e+32\n",
      "Gradient Descent(20/49): loss=2.99002381409963e+34\n",
      "Gradient Descent(21/49): loss=1.9027991325391072e+36\n",
      "Gradient Descent(22/49): loss=1.2109082615804548e+38\n",
      "Gradient Descent(23/49): loss=7.70600949355148e+39\n",
      "Gradient Descent(24/49): loss=4.903970366610405e+41\n",
      "Gradient Descent(25/49): loss=3.120801418259618e+43\n",
      "Gradient Descent(26/49): loss=1.9860237244752632e+45\n",
      "Gradient Descent(27/49): loss=1.2638709438866737e+47\n",
      "Gradient Descent(28/49): loss=8.043054788900084e+48\n",
      "Gradient Descent(29/49): loss=5.118460128398136e+50\n",
      "Gradient Descent(30/49): loss=3.2572989708036416e+52\n",
      "Gradient Descent(31/49): loss=2.07288839202484e+54\n",
      "Gradient Descent(32/49): loss=1.3191501069768846e+56\n",
      "Gradient Descent(33/49): loss=8.394841764911931e+57\n",
      "Gradient Descent(34/49): loss=5.342331239271506e+59\n",
      "Gradient Descent(35/49): loss=3.399766650681554e+61\n",
      "Gradient Descent(36/49): loss=2.1635523447368032e+63\n",
      "Gradient Descent(37/49): loss=1.3768470690415453e+65\n",
      "Gradient Descent(38/49): loss=8.762015192929889e+66\n",
      "Gradient Descent(39/49): loss=5.575994020496226e+68\n",
      "Gradient Descent(40/49): loss=3.548465579208057e+70\n",
      "Gradient Descent(41/49): loss=2.258181755672647e+72\n",
      "Gradient Descent(42/49): loss=1.4370675797257942e+74\n",
      "Gradient Descent(43/49): loss=9.145248045296482e+75\n",
      "Gradient Descent(44/49): loss=5.819876739962214e+77\n",
      "Gradient Descent(45/49): loss=3.703668298616939e+79\n",
      "Gradient Descent(46/49): loss=2.3569500659680907e+81\n",
      "Gradient Descent(47/49): loss=1.4999220139507287e+83\n",
      "Gradient Descent(48/49): loss=9.545242728805721e+84\n",
      "Gradient Descent(49/49): loss=6.074426397132129e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1739617180615887\n",
      "Gradient Descent(2/49): loss=104.65656748880663\n",
      "Gradient Descent(3/49): loss=6454.153470823999\n",
      "Gradient Descent(4/49): loss=404648.9458604357\n",
      "Gradient Descent(5/49): loss=25459278.28765796\n",
      "Gradient Descent(6/49): loss=1603427570.6399345\n",
      "Gradient Descent(7/49): loss=101014477166.77057\n",
      "Gradient Descent(8/49): loss=6364408302176.478\n",
      "Gradient Descent(9/49): loss=401000429294190.2\n",
      "Gradient Descent(10/49): loss=2.5265941036412184e+16\n",
      "Gradient Descent(11/49): loss=1.5919422483053002e+18\n",
      "Gradient Descent(12/49): loss=1.0030428993606574e+20\n",
      "Gradient Descent(13/49): loss=6.319923503879689e+21\n",
      "Gradient Descent(14/49): loss=3.98202672962513e+23\n",
      "Gradient Descent(15/49): loss=2.508976140013097e+25\n",
      "Gradient Descent(16/49): loss=1.5808435623764488e+27\n",
      "Gradient Descent(17/49): loss=9.960502752339335e+28\n",
      "Gradient Descent(18/49): loss=6.27586546295998e+30\n",
      "Gradient Descent(19/49): loss=3.954266998138593e+32\n",
      "Gradient Descent(20/49): loss=2.491485451049696e+34\n",
      "Gradient Descent(21/49): loss=1.5698231191406837e+36\n",
      "Gradient Descent(22/49): loss=9.891065686808115e+37\n",
      "Gradient Descent(23/49): loss=6.23211489422576e+39\n",
      "Gradient Descent(24/49): loss=3.926700851522744e+41\n",
      "Gradient Descent(25/49): loss=2.4741167066157345e+43\n",
      "Gradient Descent(26/49): loss=1.5588795045544955e+45\n",
      "Gradient Descent(27/49): loss=9.822112688629726e+46\n",
      "Gradient Descent(28/49): loss=6.188669322181681e+48\n",
      "Gradient Descent(29/49): loss=3.899326875331929e+50\n",
      "Gradient Descent(30/49): loss=2.4568690439135914e+52\n",
      "Gradient Descent(31/49): loss=1.548012180545153e+54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=9.753640378402016e+55\n",
      "Gradient Descent(33/49): loss=6.145526619673748e+57\n",
      "Gradient Descent(34/49): loss=3.872143729714382e+59\n",
      "Gradient Descent(35/49): loss=2.4397416188171187e+61\n",
      "Gradient Descent(36/49): loss=1.5372206152655967e+63\n",
      "Gradient Descent(37/49): loss=9.68564540511972e+64\n",
      "Gradient Descent(38/49): loss=6.102684675322828e+66\n",
      "Gradient Descent(39/49): loss=3.8451500843437945e+68\n",
      "Gradient Descent(40/49): loss=2.422733593120953e+70\n",
      "Gradient Descent(41/49): loss=1.526504280583487e+72\n",
      "Gradient Descent(42/49): loss=9.618124441152162e+73\n",
      "Gradient Descent(43/49): loss=6.060141392471505e+75\n",
      "Gradient Descent(44/49): loss=3.818344618168302e+77\n",
      "Gradient Descent(45/49): loss=2.4058441344631257e+79\n",
      "Gradient Descent(46/49): loss=1.5158626520482225e+81\n",
      "Gradient Descent(47/49): loss=9.551074182066421e+82\n",
      "Gradient Descent(48/49): loss=6.0178946890785594e+84\n",
      "Gradient Descent(49/49): loss=3.7917260193454504e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.183909209563419\n",
      "Gradient Descent(2/49): loss=105.96059446517891\n",
      "Gradient Descent(3/49): loss=6576.657837592563\n",
      "Gradient Descent(4/49): loss=414972.9933784039\n",
      "Gradient Descent(5/49): loss=26279250.168239787\n",
      "Gradient Descent(6/49): loss=1665981589.3353357\n",
      "Gradient Descent(7/49): loss=105650364835.56627\n",
      "Gradient Descent(8/49): loss=6700650701791.182\n",
      "Gradient Descent(9/49): loss=424988587285704.25\n",
      "Gradient Descent(10/49): loss=2.6955171642224212e+16\n",
      "Gradient Descent(11/49): loss=1.709654608049834e+18\n",
      "Gradient Descent(12/49): loss=1.0843640740640652e+20\n",
      "Gradient Descent(13/49): loss=6.877680648316729e+21\n",
      "Gradient Descent(14/49): loss=4.362233801674377e+23\n",
      "Gradient Descent(15/49): loss=2.766788015144908e+25\n",
      "Gradient Descent(16/49): loss=1.7548614659152418e+27\n",
      "Gradient Descent(17/49): loss=1.11303748546642e+29\n",
      "Gradient Descent(18/49): loss=7.059545550905365e+30\n",
      "Gradient Descent(19/49): loss=4.477583554327807e+32\n",
      "Gradient Descent(20/49): loss=2.839949730992028e+34\n",
      "Gradient Descent(21/49): loss=1.801264985228924e+36\n",
      "Gradient Descent(22/49): loss=1.1424693583977535e+38\n",
      "Gradient Descent(23/49): loss=7.246219993093224e+39\n",
      "Gradient Descent(24/49): loss=4.595983586115872e+41\n",
      "Gradient Descent(25/49): loss=2.9150460714672924e+43\n",
      "Gradient Descent(26/49): loss=1.848895549681286e+45\n",
      "Gradient Descent(27/49): loss=1.172679494533931e+47\n",
      "Gradient Descent(28/49): loss=7.437830639688813e+48\n",
      "Gradient Descent(29/49): loss=4.717514451523739e+50\n",
      "Gradient Descent(30/49): loss=2.992128172639658e+52\n",
      "Gradient Descent(31/49): loss=1.8977856016132098e+54\n",
      "Gradient Descent(32/49): loss=1.2036884725138874e+56\n",
      "Gradient Descent(33/49): loss=7.63450801624383e+57\n",
      "Gradient Descent(34/49): loss=4.842258938341628e+59\n",
      "Gradient Descent(35/49): loss=3.0712485435944864e+61\n",
      "Gradient Descent(36/49): loss=1.9479684454383397e+63\n",
      "Gradient Descent(37/49): loss=1.2355174159830122e+65\n",
      "Gradient Descent(38/49): loss=7.836386101489674e+66\n",
      "Gradient Descent(39/49): loss=4.970302023849823e+68\n",
      "Gradient Descent(40/49): loss=3.1524610819762195e+70\n",
      "Gradient Descent(41/49): loss=1.9994782662477063e+72\n",
      "Gradient Descent(42/49): loss=1.2681880071587448e+74\n",
      "Gradient Descent(43/49): loss=8.043602416941821e+75\n",
      "Gradient Descent(44/49): loss=5.101730932370616e+77\n",
      "Gradient Descent(45/49): loss=3.235821110636502e+79\n",
      "Gradient Descent(46/49): loss=2.0523501530833433e+81\n",
      "Gradient Descent(47/49): loss=1.301722501597959e+83\n",
      "Gradient Descent(48/49): loss=8.256298120575279e+84\n",
      "Gradient Descent(49/49): loss=5.236635194685186e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2083108836026146\n",
      "Gradient Descent(2/49): loss=109.73632857746595\n",
      "Gradient Descent(3/49): loss=6954.223731671374\n",
      "Gradient Descent(4/49): loss=447653.49140644347\n",
      "Gradient Descent(5/49): loss=28910383.342402164\n",
      "Gradient Descent(6/49): loss=1868795589.4042487\n",
      "Gradient Descent(7/49): loss=120833190384.60861\n",
      "Gradient Descent(8/49): loss=7813498466035.402\n",
      "Gradient Descent(9/49): loss=505260460597167.94\n",
      "Gradient Descent(10/49): loss=3.2672943704287336e+16\n",
      "Gradient Descent(11/49): loss=2.112818412654838e+18\n",
      "Gradient Descent(12/49): loss=1.3662694925372811e+20\n",
      "Gradient Descent(13/49): loss=8.835083467197743e+21\n",
      "Gradient Descent(14/49): loss=5.7132728982002435e+23\n",
      "Gradient Descent(15/49): loss=3.6945307797815124e+25\n",
      "Gradient Descent(16/49): loss=2.389096057921319e+27\n",
      "Gradient Descent(17/49): loss=1.5449268997418755e+29\n",
      "Gradient Descent(18/49): loss=9.990385773107832e+30\n",
      "Gradient Descent(19/49): loss=6.460357957881562e+32\n",
      "Gradient Descent(20/49): loss=4.177638971485836e+34\n",
      "Gradient Descent(21/49): loss=2.7015016025592936e+36\n",
      "Gradient Descent(22/49): loss=1.7469462915475444e+38\n",
      "Gradient Descent(23/49): loss=1.1296759338082457e+40\n",
      "Gradient Descent(24/49): loss=7.305134231090388e+41\n",
      "Gradient Descent(25/49): loss=4.723919890402211e+43\n",
      "Gradient Descent(26/49): loss=3.0547582597407757e+45\n",
      "Gradient Descent(27/49): loss=1.9753823608257893e+47\n",
      "Gradient Descent(28/49): loss=1.277395832884269e+49\n",
      "Gradient Descent(29/49): loss=8.260376047845018e+50\n",
      "Gradient Descent(30/49): loss=5.341634182236613e+52\n",
      "Gradient Descent(31/49): loss=3.454207843755775e+54\n",
      "Gradient Descent(32/49): loss=2.2336894330094344e+56\n",
      "Gradient Descent(33/49): loss=1.444432040230972e+58\n",
      "Gradient Descent(34/49): loss=9.340528222112086e+59\n",
      "Gradient Descent(35/49): loss=6.040122694462083e+61\n",
      "Gradient Descent(36/49): loss=3.905890683761155e+63\n",
      "Gradient Descent(37/49): loss=2.5257735322959137e+65\n",
      "Gradient Descent(38/49): loss=1.63331041571892e+67\n",
      "Gradient Descent(39/49): loss=1.0561924416362825e+69\n",
      "Gradient Descent(40/49): loss=6.82994771253318e+70\n",
      "Gradient Descent(41/49): loss=4.4166369609376175e+72\n",
      "Gradient Descent(42/49): loss=2.8560514466201205e+74\n",
      "Gradient Descent(43/49): loss=1.8468871084231456e+76\n",
      "Gradient Descent(44/49): loss=1.1943034133002793e+78\n",
      "Gradient Descent(45/49): loss=7.723052678831642e+79\n",
      "Gradient Descent(46/49): loss=4.9941699919610184e+81\n",
      "Gradient Descent(47/49): loss=3.2295175166896875e+83\n",
      "Gradient Descent(48/49): loss=2.0883917462549417e+85\n",
      "Gradient Descent(49/49): loss=1.3504742003369694e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2016990420528653\n",
      "Gradient Descent(2/49): loss=108.96997839638162\n",
      "Gradient Descent(3/49): loss=6906.17994116324\n",
      "Gradient Descent(4/49): loss=445168.63461369276\n",
      "Gradient Descent(5/49): loss=28802219.68088564\n",
      "Gradient Descent(6/49): loss=1865480853.905611\n",
      "Gradient Descent(7/49): loss=120863443985.11803\n",
      "Gradient Descent(8/49): loss=7831440713266.001\n",
      "Gradient Descent(9/49): loss=507459543149184.7\n",
      "Gradient Descent(10/49): loss=3.288252841952542e+16\n",
      "Gradient Descent(11/49): loss=2.1307388422702339e+18\n",
      "Gradient Descent(12/49): loss=1.3806882329362735e+20\n",
      "Gradient Descent(13/49): loss=8.946664300150151e+21\n",
      "Gradient Descent(14/49): loss=5.79731230264099e+23\n",
      "Gradient Descent(15/49): loss=3.756576716510786e+25\n",
      "Gradient Descent(16/49): loss=2.4342088199595047e+27\n",
      "Gradient Descent(17/49): loss=1.577333045706357e+29\n",
      "Gradient Descent(18/49): loss=1.0220896081535873e+31\n",
      "Gradient Descent(19/49): loss=6.622996774096072e+32\n",
      "Gradient Descent(20/49): loss=4.291608673341006e+34\n",
      "Gradient Descent(21/49): loss=2.780902004594021e+36\n",
      "Gradient Descent(22/49): loss=1.801985350446072e+38\n",
      "Gradient Descent(23/49): loss=1.1676611393946415e+40\n",
      "Gradient Descent(24/49): loss=7.566279804193599e+41\n",
      "Gradient Descent(25/49): loss=4.902842797786236e+43\n",
      "Gradient Descent(26/49): loss=3.176973112530554e+45\n",
      "Gradient Descent(27/49): loss=2.0586338526496457e+47\n",
      "Gradient Descent(28/49): loss=1.3339657558195266e+49\n",
      "Gradient Descent(29/49): loss=8.643910306870929e+50\n",
      "Gradient Descent(30/49): loss=5.601132193031955e+52\n",
      "Gradient Descent(31/49): loss=3.629454810386132e+54\n",
      "Gradient Descent(32/49): loss=2.3518356229875747e+56\n",
      "Gradient Descent(33/49): loss=1.5239563754108086e+58\n",
      "Gradient Descent(34/49): loss=9.875022775635142e+59\n",
      "Gradient Descent(35/49): loss=6.398875741638332e+61\n",
      "Gradient Descent(36/49): loss=4.146381399540018e+63\n",
      "Gradient Descent(37/49): loss=2.686796775655076e+65\n",
      "Gradient Descent(38/49): loss=1.7410064868782643e+67\n",
      "Gradient Descent(39/49): loss=1.1281476942420273e+69\n",
      "Gradient Descent(40/49): loss=7.310238242165645e+70\n",
      "Gradient Descent(41/49): loss=4.736931470052411e+72\n",
      "Gradient Descent(42/49): loss=3.069464907798349e+74\n",
      "Gradient Descent(43/49): loss=1.988970049444537e+76\n",
      "Gradient Descent(44/49): loss=1.2888245920442698e+78\n",
      "Gradient Descent(45/49): loss=8.351401920415909e+79\n",
      "Gradient Descent(46/49): loss=5.411590876435592e+81\n",
      "Gradient Descent(47/49): loss=3.506634705525306e+83\n",
      "Gradient Descent(48/49): loss=2.272249924054456e+85\n",
      "Gradient Descent(49/49): loss=1.4723859628806166e+87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2071445067055144\n",
      "Gradient Descent(2/49): loss=108.50625266934394\n",
      "Gradient Descent(3/49): loss=6814.104053458629\n",
      "Gradient Descent(4/49): loss=435001.60854036553\n",
      "Gradient Descent(5/49): loss=27868039.106729954\n",
      "Gradient Descent(6/49): loss=1787145337.4884872\n",
      "Gradient Descent(7/49): loss=114642392373.49065\n",
      "Gradient Descent(8/49): loss=7354803629558.7295\n",
      "Gradient Descent(9/49): loss=471855950228131.7\n",
      "Gradient Descent(10/49): loss=3.0272735818928196e+16\n",
      "Gradient Descent(11/49): loss=1.94220519308442e+18\n",
      "Gradient Descent(12/49): loss=1.2460599156638704e+20\n",
      "Gradient Descent(13/49): loss=7.994344443847033e+21\n",
      "Gradient Descent(14/49): loss=5.12893060799573e+23\n",
      "Gradient Descent(15/49): loss=3.290567481449137e+25\n",
      "Gradient Descent(16/49): loss=2.111129056841899e+27\n",
      "Gradient Descent(17/49): loss=1.3544368656502118e+29\n",
      "Gradient Descent(18/49): loss=8.689659301085692e+30\n",
      "Gradient Descent(19/49): loss=5.57502388647198e+32\n",
      "Gradient Descent(20/49): loss=3.576767541758835e+34\n",
      "Gradient Descent(21/49): loss=2.2947464098084602e+36\n",
      "Gradient Descent(22/49): loss=1.4722402347616983e+38\n",
      "Gradient Descent(23/49): loss=9.445450266711506e+39\n",
      "Gradient Descent(24/49): loss=6.05991662464133e+41\n",
      "Gradient Descent(25/49): loss=3.8878601295518323e+43\n",
      "Gradient Descent(26/49): loss=2.4943340516429243e+45\n",
      "Gradient Descent(27/49): loss=1.6002896590579479e+47\n",
      "Gradient Descent(28/49): loss=1.0266976835765247e+49\n",
      "Gradient Descent(29/49): loss=6.586983347014375e+50\n",
      "Gradient Descent(30/49): loss=4.226010276238309e+52\n",
      "Gradient Descent(31/49): loss=2.7112810089259875e+54\n",
      "Gradient Descent(32/49): loss=1.7394762977022587e+56\n",
      "Gradient Descent(33/49): loss=1.115995642025524e+58\n",
      "Gradient Descent(34/49): loss=7.159892173668134e+59\n",
      "Gradient Descent(35/49): loss=4.593571337385349e+61\n",
      "Gradient Descent(36/49): loss=2.9470971237878005e+63\n",
      "Gradient Descent(37/49): loss=1.8907688199704838e+65\n",
      "Gradient Descent(38/49): loss=1.2130603710738048e+67\n",
      "Gradient Descent(39/49): loss=7.782630263030751e+68\n",
      "Gradient Descent(40/49): loss=4.9931013538448915e+70\n",
      "Gradient Descent(41/49): loss=3.203423558253307e+72\n",
      "Gradient Descent(42/49): loss=2.0552201460261017e+74\n",
      "Gradient Descent(43/49): loss=1.3185673926100123e+76\n",
      "Gradient Descent(44/49): loss=8.459531560237479e+77\n",
      "Gradient Descent(45/49): loss=5.427380854383059e+79\n",
      "Gradient Descent(46/49): loss=3.482044215897077e+81\n",
      "Gradient Descent(47/49): loss=2.233974774714895e+83\n",
      "Gradient Descent(48/49): loss=1.4332509826492045e+85\n",
      "Gradient Descent(49/49): loss=9.195306959216147e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2169462541917557\n",
      "Gradient Descent(2/49): loss=109.82517050593707\n",
      "Gradient Descent(3/49): loss=6940.308176667378\n",
      "Gradient Descent(4/49): loss=445831.71788340603\n",
      "Gradient Descent(5/49): loss=28743951.36362062\n",
      "Gradient Descent(6/49): loss=1855190098.336503\n",
      "Gradient Descent(7/49): loss=119777396815.0098\n",
      "Gradient Descent(8/49): loss=7734047854775.597\n",
      "Gradient Descent(9/49): loss=499405458546472.44\n",
      "Gradient Descent(10/49): loss=3.224811324631224e+16\n",
      "Gradient Descent(11/49): loss=2.0823647293634043e+18\n",
      "Gradient Descent(12/49): loss=1.3446515425900057e+20\n",
      "Gradient Descent(13/49): loss=8.682861192440215e+21\n",
      "Gradient Descent(14/49): loss=5.606812197924531e+23\n",
      "Gradient Descent(15/49): loss=3.620505202004743e+25\n",
      "Gradient Descent(16/49): loss=2.3378807065833336e+27\n",
      "Gradient Descent(17/49): loss=1.5096473869137753e+29\n",
      "Gradient Descent(18/49): loss=9.748295663516383e+30\n",
      "Gradient Descent(19/49): loss=6.294798984264203e+32\n",
      "Gradient Descent(20/49): loss=4.064761228480296e+34\n",
      "Gradient Descent(21/49): loss=2.6247516221663787e+36\n",
      "Gradient Descent(22/49): loss=1.6948894881879043e+38\n",
      "Gradient Descent(23/49): loss=1.0944465575030341e+40\n",
      "Gradient Descent(24/49): loss=7.067205712109855e+41\n",
      "Gradient Descent(25/49): loss=4.563529962691561e+43\n",
      "Gradient Descent(26/49): loss=2.9468231955807555e+45\n",
      "Gradient Descent(27/49): loss=1.902861823414303e+47\n",
      "Gradient Descent(28/49): loss=1.22874121679162e+49\n",
      "Gradient Descent(29/49): loss=7.934391027581352e+50\n",
      "Gradient Descent(30/49): loss=5.123500385455039e+52\n",
      "Gradient Descent(31/49): loss=3.308414736368196e+54\n",
      "Gradient Descent(32/49): loss=2.1363535170001177e+56\n",
      "Gradient Descent(33/49): loss=1.379514575191649e+58\n",
      "Gradient Descent(34/49): loss=8.90798478820345e+59\n",
      "Gradient Descent(35/49): loss=5.752182283093276e+61\n",
      "Gradient Descent(36/49): loss=3.7143755635673445e+63\n",
      "Gradient Descent(37/49): loss=2.398495935668946e+65\n",
      "Gradient Descent(38/49): loss=1.5487886604270508e+67\n",
      "Gradient Descent(39/49): loss=1.0001043900032355e+69\n",
      "Gradient Descent(40/49): loss=6.458006934451253e+70\n",
      "Gradient Descent(41/49): loss=4.170150034566453e+72\n",
      "Gradient Descent(42/49): loss=2.692804682203768e+74\n",
      "Gradient Descent(43/49): loss=1.7388336142328638e+76\n",
      "Gradient Descent(44/49): loss=1.122822742387573e+78\n",
      "Gradient Descent(45/49): loss=7.250440183024416e+79\n",
      "Gradient Descent(46/49): loss=4.681850559584506e+81\n",
      "Gradient Descent(47/49): loss=3.023226743336622e+83\n",
      "Gradient Descent(48/49): loss=1.952198137318782e+85\n",
      "Gradient Descent(49/49): loss=1.2605993168559895e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.241812708472542\n",
      "Gradient Descent(2/49): loss=113.73434892890698\n",
      "Gradient Descent(3/49): loss=7338.303495351913\n",
      "Gradient Descent(4/49): loss=480904.54662837635\n",
      "Gradient Descent(5/49): loss=31618790.5276938\n",
      "Gradient Descent(6/49): loss=2080797596.3876376\n",
      "Gradient Descent(7/49): loss=136971974262.22527\n",
      "Gradient Descent(8/49): loss=9017138357829.28\n",
      "Gradient Descent(9/49): loss=593630696037492.1\n",
      "Gradient Descent(10/49): loss=3.908113528142127e+16\n",
      "Gradient Descent(11/49): loss=2.572876634517267e+18\n",
      "Gradient Descent(12/49): loss=1.6938347896780443e+20\n",
      "Gradient Descent(13/49): loss=1.1151241549468804e+22\n",
      "Gradient Descent(14/49): loss=7.341341468157218e+23\n",
      "Gradient Descent(15/49): loss=4.833120628491423e+25\n",
      "Gradient Descent(16/49): loss=3.181851061417653e+27\n",
      "Gradient Descent(17/49): loss=2.0947493292105856e+29\n",
      "Gradient Descent(18/49): loss=1.3790635293182336e+31\n",
      "Gradient Descent(19/49): loss=9.07896802580086e+32\n",
      "Gradient Descent(20/49): loss=5.9770749254910325e+34\n",
      "Gradient Descent(21/49): loss=3.9349653577287496e+36\n",
      "Gradient Descent(22/49): loss=2.5905568458842004e+38\n",
      "Gradient Descent(23/49): loss=1.7054749309508637e+40\n",
      "Gradient Descent(24/49): loss=1.1227874596630377e+42\n",
      "Gradient Descent(25/49): loss=7.391792495442188e+43\n",
      "Gradient Descent(26/49): loss=4.866334747991859e+45\n",
      "Gradient Descent(27/49): loss=3.2037173519299086e+47\n",
      "Gradient Descent(28/49): loss=2.1091448497849922e+49\n",
      "Gradient Descent(29/49): loss=1.3885407196408245e+51\n",
      "Gradient Descent(30/49): loss=9.141360444244528e+52\n",
      "Gradient Descent(31/49): loss=6.018150536717038e+54\n",
      "Gradient Descent(32/49): loss=3.9620071983257855e+56\n",
      "Gradient Descent(33/49): loss=2.608359651991773e+58\n",
      "Gradient Descent(34/49): loss=1.7171952834950943e+60\n",
      "Gradient Descent(35/49): loss=1.1305034715615573e+62\n",
      "Gradient Descent(36/49): loss=7.442590318623924e+63\n",
      "Gradient Descent(37/49): loss=4.899777138619684e+65\n",
      "Gradient Descent(38/49): loss=3.225733915255851e+67\n",
      "Gradient Descent(39/49): loss=2.1236392998402988e+69\n",
      "Gradient Descent(40/49): loss=1.3980830391797876e+71\n",
      "Gradient Descent(41/49): loss=9.204181635691056e+72\n",
      "Gradient Descent(42/49): loss=6.059508427517509e+74\n",
      "Gradient Descent(43/49): loss=3.989234875676061e+76\n",
      "Gradient Descent(44/49): loss=2.6262848024175398e+78\n",
      "Gradient Descent(45/49): loss=1.7289961805621683e+80\n",
      "Gradient Descent(46/49): loss=1.1382725093815944e+82\n",
      "Gradient Descent(47/49): loss=7.493737234240753e+83\n",
      "Gradient Descent(48/49): loss=4.933449351803706e+85\n",
      "Gradient Descent(49/49): loss=3.2479017806498336e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2350386485152223\n",
      "Gradient Descent(2/49): loss=112.94031492653544\n",
      "Gradient Descent(3/49): loss=7287.593190699708\n",
      "Gradient Descent(4/49): loss=478232.20230616414\n",
      "Gradient Descent(5/49): loss=31500201.95748673\n",
      "Gradient Descent(6/49): loss=2077082335.7404783\n",
      "Gradient Descent(7/49): loss=137004347467.08669\n",
      "Gradient Descent(8/49): loss=9037698781098.05\n",
      "Gradient Descent(9/49): loss=596203558561333.1\n",
      "Gradient Descent(10/49): loss=3.933103100476173e+16\n",
      "Gradient Descent(11/49): loss=2.5946414559167206e+18\n",
      "Gradient Descent(12/49): loss=1.711668908162963e+20\n",
      "Gradient Descent(13/49): loss=1.1291776948353267e+22\n",
      "Gradient Descent(14/49): loss=7.449118044251374e+23\n",
      "Gradient Descent(15/49): loss=4.9141389657626225e+25\n",
      "Gradient Descent(16/49): loss=3.241828338232517e+27\n",
      "Gradient Descent(17/49): loss=2.1386149382651515e+29\n",
      "Gradient Descent(18/49): loss=1.410831598875532e+31\n",
      "Gradient Descent(19/49): loss=9.307172437857351e+32\n",
      "Gradient Descent(20/49): loss=6.13988649413878e+34\n",
      "Gradient Descent(21/49): loss=4.050446729495148e+36\n",
      "Gradient Descent(22/49): loss=2.6720557007454194e+38\n",
      "Gradient Descent(23/49): loss=1.7627393087062144e+40\n",
      "Gradient Descent(24/49): loss=1.1628686743293973e+42\n",
      "Gradient Descent(25/49): loss=7.671375722195741e+43\n",
      "Gradient Descent(26/49): loss=5.060761096263547e+45\n",
      "Gradient Descent(27/49): loss=3.3385541004533905e+47\n",
      "Gradient Descent(28/49): loss=2.2024243527092174e+49\n",
      "Gradient Descent(29/49): loss=1.4529262918782322e+51\n",
      "Gradient Descent(30/49): loss=9.58486863366854e+52\n",
      "Gradient Descent(31/49): loss=6.323081028833262e+54\n",
      "Gradient Descent(32/49): loss=4.171299078294034e+56\n",
      "Gradient Descent(33/49): loss=2.751781278973636e+58\n",
      "Gradient Descent(34/49): loss=1.8153338001375534e+60\n",
      "Gradient Descent(35/49): loss=1.1975649486033912e+62\n",
      "Gradient Descent(36/49): loss=7.900264987159777e+63\n",
      "Gradient Descent(37/49): loss=5.211757987750977e+65\n",
      "Gradient Descent(38/49): loss=3.4381658548204187e+67\n",
      "Gradient Descent(39/49): loss=2.268137636673757e+69\n",
      "Gradient Descent(40/49): loss=1.49627695583193e+71\n",
      "Gradient Descent(41/49): loss=9.870850394409746e+72\n",
      "Gradient Descent(42/49): loss=6.5117415014018e+74\n",
      "Gradient Descent(43/49): loss=4.2957572738710364e+76\n",
      "Gradient Descent(44/49): loss=2.8338856129413933e+78\n",
      "Gradient Descent(45/49): loss=1.8694975426298507e+80\n",
      "Gradient Descent(46/49): loss=1.2332964485011269e+82\n",
      "Gradient Descent(47/49): loss=8.135983574205991e+83\n",
      "Gradient Descent(48/49): loss=5.3672601425389516e+85\n",
      "Gradient Descent(49/49): loss=3.540749704684422e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.240642224907347\n",
      "Gradient Descent(2/49): loss=112.4616072587559\n",
      "Gradient Descent(3/49): loss=7190.632428920051\n",
      "Gradient Descent(4/49): loss=467328.08940347\n",
      "Gradient Descent(5/49): loss=30480003.59082564\n",
      "Gradient Descent(6/49): loss=1989978361.3440616\n",
      "Gradient Descent(7/49): loss=129961430288.19965\n",
      "Gradient Descent(8/49): loss=8488313555866.77\n",
      "Gradient Descent(9/49): loss=554422669386499.06\n",
      "Gradient Descent(10/49): loss=3.621299706426264e+16\n",
      "Gradient Descent(11/49): loss=2.365316056250594e+18\n",
      "Gradient Descent(12/49): loss=1.544949435932227e+20\n",
      "Gradient Descent(13/49): loss=1.00911225076848e+22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/49): loss=6.591203551716221e+23\n",
      "Gradient Descent(15/49): loss=4.305166781293161e+25\n",
      "Gradient Descent(16/49): loss=2.8119995113953195e+27\n",
      "Gradient Descent(17/49): loss=1.8367096268274137e+29\n",
      "Gradient Descent(18/49): loss=1.1996809538173725e+31\n",
      "Gradient Descent(19/49): loss=7.835938627896675e+32\n",
      "Gradient Descent(20/49): loss=5.118188630861992e+34\n",
      "Gradient Descent(21/49): loss=3.343039820201515e+36\n",
      "Gradient Descent(22/49): loss=2.1835684546964945e+38\n",
      "Gradient Descent(23/49): loss=1.4262382301115776e+40\n",
      "Gradient Descent(24/49): loss=9.31573949357135e+41\n",
      "Gradient Descent(25/49): loss=6.08474800912574e+43\n",
      "Gradient Descent(26/49): loss=3.974366002839816e+45\n",
      "Gradient Descent(27/49): loss=2.59593085873721e+47\n",
      "Gradient Descent(28/49): loss=1.695580381507148e+49\n",
      "Gradient Descent(29/49): loss=1.107499770448613e+51\n",
      "Gradient Descent(30/49): loss=7.23384013474773e+52\n",
      "Gradient Descent(31/49): loss=4.724916834419782e+54\n",
      "Gradient Descent(32/49): loss=3.086167053228914e+56\n",
      "Gradient Descent(33/49): loss=2.0157872432913047e+58\n",
      "Gradient Descent(34/49): loss=1.3166488204080194e+60\n",
      "Gradient Descent(35/49): loss=8.599935940914725e+61\n",
      "Gradient Descent(36/49): loss=5.6172076442462275e+63\n",
      "Gradient Descent(37/49): loss=3.6689833430575397e+65\n",
      "Gradient Descent(38/49): loss=2.39646451122782e+67\n",
      "Gradient Descent(39/49): loss=1.5652952375598552e+69\n",
      "Gradient Descent(40/49): loss=1.022401612562266e+71\n",
      "Gradient Descent(41/49): loss=6.678005735195671e+72\n",
      "Gradient Descent(42/49): loss=4.36186328849226e+74\n",
      "Gradient Descent(43/49): loss=2.8490318969363693e+76\n",
      "Gradient Descent(44/49): loss=1.860898018325242e+78\n",
      "Gradient Descent(45/49): loss=1.2154800507255109e+80\n",
      "Gradient Descent(46/49): loss=7.939133360146721e+81\n",
      "Gradient Descent(47/49): loss=5.185592184139335e+83\n",
      "Gradient Descent(48/49): loss=3.387065701048994e+85\n",
      "Gradient Descent(49/49): loss=2.2123247752323256e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.250293866612979\n",
      "Gradient Descent(2/49): loss=113.79492823283124\n",
      "Gradient Descent(3/49): loss=7320.564929566829\n",
      "Gradient Descent(4/49): loss=478680.0274147682\n",
      "Gradient Descent(5/49): loss=31414782.365085624\n",
      "Gradient Descent(6/49): loss=2063913838.9877777\n",
      "Gradient Descent(7/49): loss=135642096181.13747\n",
      "Gradient Descent(8/49): loss=8915450815526.45\n",
      "Gradient Descent(9/49): loss=586012261099992.9\n",
      "Gradient Descent(10/49): loss=3.851897677797446e+16\n",
      "Gradient Descent(11/49): loss=2.531886671785507e+18\n",
      "Gradient Descent(12/49): loss=1.664233485203066e+20\n",
      "Gradient Descent(13/49): loss=1.0939170723613288e+22\n",
      "Gradient Descent(14/49): loss=7.190425529526981e+23\n",
      "Gradient Descent(15/49): loss=4.72633826019841e+25\n",
      "Gradient Descent(16/49): loss=3.1066692103955025e+27\n",
      "Gradient Descent(17/49): loss=2.0420446202310055e+29\n",
      "Gradient Descent(18/49): loss=1.3422562730083753e+31\n",
      "Gradient Descent(19/49): loss=8.822784209723254e+32\n",
      "Gradient Descent(20/49): loss=5.79930396197809e+34\n",
      "Gradient Descent(21/49): loss=3.811940272505991e+36\n",
      "Gradient Descent(22/49): loss=2.5056263193989227e+38\n",
      "Gradient Descent(23/49): loss=1.6469731432495068e+40\n",
      "Gradient Descent(24/49): loss=1.0825718558221224e+42\n",
      "Gradient Descent(25/49): loss=7.11585266476368e+43\n",
      "Gradient Descent(26/49): loss=4.677320851665478e+45\n",
      "Gradient Descent(27/49): loss=3.0744495958661353e+47\n",
      "Gradient Descent(28/49): loss=2.0208663500507714e+49\n",
      "Gradient Descent(29/49): loss=1.328335585744743e+51\n",
      "Gradient Descent(30/49): loss=8.731282146944043e+52\n",
      "Gradient Descent(31/49): loss=5.739158744798803e+54\n",
      "Gradient Descent(32/49): loss=3.772406222095212e+56\n",
      "Gradient Descent(33/49): loss=2.4796401942009101e+58\n",
      "Gradient Descent(34/49): loss=1.6298922042604734e+60\n",
      "Gradient Descent(35/49): loss=1.0713443844481534e+62\n",
      "Gradient Descent(36/49): loss=7.042053376832813e+63\n",
      "Gradient Descent(37/49): loss=4.628811844447786e+65\n",
      "Gradient Descent(38/49): loss=3.0425641421275895e+67\n",
      "Gradient Descent(39/49): loss=1.9999077236342056e+69\n",
      "Gradient Descent(40/49): loss=1.3145592717907077e+71\n",
      "Gradient Descent(41/49): loss=8.640729062792422e+72\n",
      "Gradient Descent(42/49): loss=5.679637300407161e+74\n",
      "Gradient Descent(43/49): loss=3.73328218368549e+76\n",
      "Gradient Descent(44/49): loss=2.453923573962072e+78\n",
      "Gradient Descent(45/49): loss=1.6129884135632399e+80\n",
      "Gradient Descent(46/49): loss=1.060233354410689e+82\n",
      "Gradient Descent(47/49): loss=6.969019469406534e+83\n",
      "Gradient Descent(48/49): loss=4.5808059294609394e+85\n",
      "Gradient Descent(49/49): loss=3.011009375924662e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.275629379974234\n",
      "Gradient Descent(2/49): loss=117.84105240099748\n",
      "Gradient Descent(3/49): loss=7739.902233496997\n",
      "Gradient Descent(4/49): loss=516297.0071129564\n",
      "Gradient Descent(5/49): loss=34553455.42235226\n",
      "Gradient Descent(6/49): loss=2314641235.8762746\n",
      "Gradient Descent(7/49): loss=155093634704.94196\n",
      "Gradient Descent(8/49): loss=10392970437599.162\n",
      "Gradient Descent(9/49): loss=696459893465081.5\n",
      "Gradient Descent(10/49): loss=4.6671931436919336e+16\n",
      "Gradient Descent(11/49): loss=3.1276375487328794e+18\n",
      "Gradient Descent(12/49): loss=2.0959328339354252e+20\n",
      "Gradient Descent(13/49): loss=1.4045538496635065e+22\n",
      "Gradient Descent(14/49): loss=9.412380059845583e+23\n",
      "Gradient Descent(15/49): loss=6.307547418793296e+25\n",
      "Gradient Descent(16/49): loss=4.2268963236593926e+27\n",
      "Gradient Descent(17/49): loss=2.83258315415595e+29\n",
      "Gradient Descent(18/49): loss=1.8982077427820221e+31\n",
      "Gradient Descent(19/49): loss=1.272051847818433e+33\n",
      "Gradient Descent(20/49): loss=8.524440540141266e+34\n",
      "Gradient Descent(21/49): loss=5.71250980438527e+36\n",
      "Gradient Descent(22/49): loss=3.828141930450411e+38\n",
      "Gradient Descent(23/49): loss=2.5653646368261764e+40\n",
      "Gradient Descent(24/49): loss=1.7191357685911875e+42\n",
      "Gradient Descent(25/49): loss=1.1520497898912396e+44\n",
      "Gradient Descent(26/49): loss=7.720267023913624e+45\n",
      "Gradient Descent(27/49): loss=5.173606509329413e+47\n",
      "Gradient Descent(28/49): loss=3.467004992245351e+49\n",
      "Gradient Descent(29/49): loss=2.323354819230778e+51\n",
      "Gradient Descent(30/49): loss=1.556956978174693e+53\n",
      "Gradient Descent(31/49): loss=1.0433684135639015e+55\n",
      "Gradient Descent(32/49): loss=6.991957142573668e+56\n",
      "Gradient Descent(33/49): loss=4.68554194741233e+58\n",
      "Gradient Descent(34/49): loss=3.1399367721065995e+60\n",
      "Gradient Descent(35/49): loss=2.10417557744247e+62\n",
      "Gradient Descent(36/49): loss=1.4100777124040276e+64\n",
      "Gradient Descent(37/49): loss=9.449397551868238e+65\n",
      "Gradient Descent(38/49): loss=6.332354118342978e+67\n",
      "Gradient Descent(39/49): loss=4.243520124959444e+69\n",
      "Gradient Descent(40/49): loss=2.843723315910812e+71\n",
      "Gradient Descent(41/49): loss=1.905673134407983e+73\n",
      "Gradient Descent(42/49): loss=1.2770546539761303e+75\n",
      "Gradient Descent(43/49): loss=8.557966000547835e+76\n",
      "Gradient Descent(44/49): loss=5.734976325288994e+78\n",
      "Gradient Descent(45/49): loss=3.8431974898614947e+80\n",
      "Gradient Descent(46/49): loss=2.5754538655978377e+82\n",
      "Gradient Descent(47/49): loss=1.7258968947916194e+84\n",
      "Gradient Descent(48/49): loss=1.156580644382807e+86\n",
      "Gradient Descent(49/49): loss=7.750629779784489e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.268691366312911\n",
      "Gradient Descent(2/49): loss=117.01859381344029\n",
      "Gradient Descent(3/49): loss=7686.403313356223\n",
      "Gradient Descent(4/49): loss=513424.91963919497\n",
      "Gradient Descent(5/49): loss=34423547.08302492\n",
      "Gradient Descent(6/49): loss=2310481692.009135\n",
      "Gradient Descent(7/49): loss=155128154293.33307\n",
      "Gradient Descent(8/49): loss=10416502941391.787\n",
      "Gradient Descent(9/49): loss=699465936347648.0\n",
      "Gradient Descent(10/49): loss=4.696943205300943e+16\n",
      "Gradient Descent(11/49): loss=3.154026360834187e+18\n",
      "Gradient Descent(12/49): loss=2.1179500899362655e+20\n",
      "Gradient Descent(13/49): loss=1.422218245898098e+22\n",
      "Gradient Descent(14/49): loss=9.550295465385504e+23\n",
      "Gradient Descent(15/49): loss=6.413090689849562e+25\n",
      "Gradient Descent(16/49): loss=4.30643561612499e+27\n",
      "Gradient Descent(17/49): loss=2.8918018877636668e+29\n",
      "Gradient Descent(18/49): loss=1.9418653638712983e+31\n",
      "Gradient Descent(19/49): loss=1.3039762883765954e+33\n",
      "Gradient Descent(20/49): loss=8.756292750324167e+34\n",
      "Gradient Descent(21/49): loss=5.879912343086423e+36\n",
      "Gradient Descent(22/49): loss=3.948402611521957e+38\n",
      "Gradient Descent(23/49): loss=2.651380203148582e+40\n",
      "Gradient Descent(24/49): loss=1.780420507558968e+42\n",
      "Gradient Descent(25/49): loss=1.1955649287766738e+44\n",
      "Gradient Descent(26/49): loss=8.028302824261463e+45\n",
      "Gradient Descent(27/49): loss=5.39106197302026e+47\n",
      "Gradient Descent(28/49): loss=3.62013613999655e+49\n",
      "Gradient Descent(29/49): loss=2.4309469521395707e+51\n",
      "Gradient Descent(30/49): loss=1.6323980247113744e+53\n",
      "Gradient Descent(31/49): loss=1.0961667874884224e+55\n",
      "Gradient Descent(32/49): loss=7.360837294600048e+56\n",
      "Gradient Descent(33/49): loss=4.942854161976434e+58\n",
      "Gradient Descent(34/49): loss=3.3191614335085396e+60\n",
      "Gradient Descent(35/49): loss=2.2288403138492097e+62\n",
      "Gradient Descent(36/49): loss=1.496681991568049e+64\n",
      "Gradient Descent(37/49): loss=1.0050325139783284e+66\n",
      "Gradient Descent(38/49): loss=6.748864219949195e+67\n",
      "Gradient Descent(39/49): loss=4.531909925880564e+69\n",
      "Gradient Descent(40/49): loss=3.0432094804315624e+71\n",
      "Gradient Descent(41/49): loss=2.04353663096892e+73\n",
      "Gradient Descent(42/49): loss=1.3722492614999262e+75\n",
      "Gradient Descent(43/49): loss=9.214750580684518e+76\n",
      "Gradient Descent(44/49): loss=6.1877700099042935e+78\n",
      "Gradient Descent(45/49): loss=4.1551312062346375e+80\n",
      "Gradient Descent(46/49): loss=2.7901999126324926e+82\n",
      "Gradient Descent(47/49): loss=1.8736389216236861e+84\n",
      "Gradient Descent(48/49): loss=1.258161751324493e+86\n",
      "Gradient Descent(49/49): loss=8.44864490285101e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.274454872667088\n",
      "Gradient Descent(2/49): loss=116.52454841840508\n",
      "Gradient Descent(3/49): loss=7584.343903941242\n",
      "Gradient Descent(4/49): loss=501737.40478718764\n",
      "Gradient Descent(5/49): loss=33310269.133911517\n",
      "Gradient Descent(6/49): loss=2213717948.740194\n",
      "Gradient Descent(7/49): loss=147163493793.0478\n",
      "Gradient Descent(8/49): loss=9784058099333.451\n",
      "Gradient Descent(9/49): loss=650505106381981.9\n",
      "Gradient Descent(10/49): loss=4.3250025190216136e+16\n",
      "Gradient Descent(11/49): loss=2.875565439336838e+18\n",
      "Gradient Descent(12/49): loss=1.9118795638129466e+20\n",
      "Gradient Descent(13/49): loss=1.2711532928840665e+22\n",
      "Gradient Descent(14/49): loss=8.451530440869643e+23\n",
      "Gradient Descent(15/49): loss=5.619178199220822e+25\n",
      "Gradient Descent(16/49): loss=3.7360291275120536e+27\n",
      "Gradient Descent(17/49): loss=2.4839777603091843e+29\n",
      "Gradient Descent(18/49): loss=1.6515250037262548e+31\n",
      "Gradient Descent(19/49): loss=1.0980512314595292e+33\n",
      "Gradient Descent(20/49): loss=7.3006252057028595e+34\n",
      "Gradient Descent(21/49): loss=4.853974647850788e+36\n",
      "Gradient Descent(22/49): loss=3.2272674213921897e+38\n",
      "Gradient Descent(23/49): loss=2.145716812471618e+40\n",
      "Gradient Descent(24/49): loss=1.4266250787920988e+42\n",
      "Gradient Descent(25/49): loss=9.485217730547807e+43\n",
      "Gradient Descent(26/49): loss=6.306447064009308e+45\n",
      "Gradient Descent(27/49): loss=4.192974341861075e+47\n",
      "Gradient Descent(28/49): loss=2.7877874265907695e+49\n",
      "Gradient Descent(29/49): loss=1.8535192687127412e+51\n",
      "Gradient Descent(30/49): loss=1.2323513789897447e+53\n",
      "Gradient Descent(31/49): loss=8.193548062506224e+54\n",
      "Gradient Descent(32/49): loss=5.447653242181208e+56\n",
      "Gradient Descent(33/49): loss=3.6219871563150343e+58\n",
      "Gradient Descent(34/49): loss=2.4081545534014867e+60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=1.6011123460107348e+62\n",
      "Gradient Descent(36/49): loss=1.064533312833687e+64\n",
      "Gradient Descent(37/49): loss=7.077774254606024e+65\n",
      "Gradient Descent(38/49): loss=4.7058074928454515e+67\n",
      "Gradient Descent(39/49): loss=3.1287553633558228e+69\n",
      "Gradient Descent(40/49): loss=2.080218992938172e+71\n",
      "Gradient Descent(41/49): loss=1.3830774720396669e+73\n",
      "Gradient Descent(42/49): loss=9.195682282285915e+74\n",
      "Gradient Descent(43/49): loss=6.11394331454502e+76\n",
      "Gradient Descent(44/49): loss=4.0649841638697326e+78\n",
      "Gradient Descent(45/49): loss=2.702690457270191e+80\n",
      "Gradient Descent(46/49): loss=1.796940753851714e+82\n",
      "Gradient Descent(47/49): loss=1.1947339600682763e+84\n",
      "Gradient Descent(48/49): loss=7.943440718793016e+85\n",
      "Gradient Descent(49/49): loss=5.281364099617032e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.283952046827088\n",
      "Gradient Descent(2/49): loss=117.87175994238946\n",
      "Gradient Descent(3/49): loss=7718.029360213849\n",
      "Gradient Descent(4/49): loss=513626.87891913427\n",
      "Gradient Descent(5/49): loss=34306927.10359156\n",
      "Gradient Descent(6/49): loss=2293965830.0990987\n",
      "Gradient Descent(7/49): loss=153439932114.91318\n",
      "Gradient Descent(8/49): loss=10264459535882.36\n",
      "Gradient Descent(9/49): loss=686670653361772.2\n",
      "Gradient Descent(10/49): loss=4.593731219422066e+16\n",
      "Gradient Descent(11/49): loss=3.0731529394657526e+18\n",
      "Gradient Descent(12/49): loss=2.055905963578574e+20\n",
      "Gradient Descent(13/49): loss=1.3753792619225576e+22\n",
      "Gradient Descent(14/49): loss=9.201142274439185e+23\n",
      "Gradient Descent(15/49): loss=6.155467408099038e+25\n",
      "Gradient Descent(16/49): loss=4.1179430030035063e+27\n",
      "Gradient Descent(17/49): loss=2.754860600207873e+29\n",
      "Gradient Descent(18/49): loss=1.8429727973352544e+31\n",
      "Gradient Descent(19/49): loss=1.232929438513063e+33\n",
      "Gradient Descent(20/49): loss=8.2481684088498715e+34\n",
      "Gradient Descent(21/49): loss=5.51793800831083e+36\n",
      "Gradient Descent(22/49): loss=3.691442554842466e+38\n",
      "Gradient Descent(23/49): loss=2.469536286060395e+40\n",
      "Gradient Descent(24/49): loss=1.652093829870503e+42\n",
      "Gradient Descent(25/49): loss=1.1052334149142867e+44\n",
      "Gradient Descent(26/49): loss=7.393895427470962e+45\n",
      "Gradient Descent(27/49): loss=4.946438359051815e+47\n",
      "Gradient Descent(28/49): loss=3.309115293813176e+49\n",
      "Gradient Descent(29/49): loss=2.2137633652524405e+51\n",
      "Gradient Descent(30/49): loss=1.480984433058715e+53\n",
      "Gradient Descent(31/49): loss=9.907630261611667e+54\n",
      "Gradient Descent(32/49): loss=6.6281005532292155e+56\n",
      "Gradient Descent(33/49): loss=4.4341296337971023e+58\n",
      "Gradient Descent(34/49): loss=2.9663861390483378e+60\n",
      "Gradient Descent(35/49): loss=1.984481161504254e+62\n",
      "Gradient Descent(36/49): loss=1.3275970476415054e+64\n",
      "Gradient Descent(37/49): loss=8.881484768393816e+65\n",
      "Gradient Descent(38/49): loss=5.941619999181552e+67\n",
      "Gradient Descent(39/49): loss=3.974881355458161e+69\n",
      "Gradient Descent(40/49): loss=2.6591538658051593e+71\n",
      "Gradient Descent(41/49): loss=1.778945998555849e+73\n",
      "Gradient Descent(42/49): loss=1.1900961830276234e+75\n",
      "Gradient Descent(43/49): loss=7.961618430276678e+76\n",
      "Gradient Descent(44/49): loss=5.32623908330352e+78\n",
      "Gradient Descent(45/49): loss=3.563197988065869e+80\n",
      "Gradient Descent(46/49): loss=2.3837420182576364e+82\n",
      "Gradient Descent(47/49): loss=1.5946983660852767e+84\n",
      "Gradient Descent(48/49): loss=1.06683645265182e+86\n",
      "Gradient Descent(49/49): loss=7.137023784006604e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.30976089810769\n",
      "Gradient Descent(2/49): loss=122.05839196873119\n",
      "Gradient Descent(3/49): loss=8159.653518395536\n",
      "Gradient Descent(4/49): loss=553948.0124477436\n",
      "Gradient Descent(5/49): loss=37731026.09031217\n",
      "Gradient Descent(6/49): loss=2572353397.2715015\n",
      "Gradient Descent(7/49): loss=175420974646.7816\n",
      "Gradient Descent(8/49): loss=11963771866498.95\n",
      "Gradient Descent(9/49): loss=815953728871666.2\n",
      "Gradient Descent(10/49): loss=5.565013441961068e+16\n",
      "Gradient Descent(11/49): loss=3.7954904176586163e+18\n",
      "Gradient Descent(12/49): loss=2.588629424247297e+20\n",
      "Gradient Descent(13/49): loss=1.7655172261484893e+22\n",
      "Gradient Descent(14/49): loss=1.2041319035765328e+24\n",
      "Gradient Descent(15/49): loss=8.212514996015231e+25\n",
      "Gradient Descent(16/49): loss=5.60116402198305e+27\n",
      "Gradient Descent(17/49): loss=3.820149914247626e+29\n",
      "Gradient Descent(18/49): loss=2.60544867424981e+31\n",
      "Gradient Descent(19/49): loss=1.7769885862237778e+33\n",
      "Gradient Descent(20/49): loss=1.2119557245313107e+35\n",
      "Gradient Descent(21/49): loss=8.265875704740438e+36\n",
      "Gradient Descent(22/49): loss=5.637557526530983e+38\n",
      "Gradient Descent(23/49): loss=3.844971301318323e+40\n",
      "Gradient Descent(24/49): loss=2.622377552405214e+42\n",
      "Gradient Descent(25/49): loss=1.788534552911184e+44\n",
      "Gradient Descent(26/49): loss=1.2198303955215634e+46\n",
      "Gradient Descent(27/49): loss=8.319583154914913e+47\n",
      "Gradient Descent(28/49): loss=5.674187503907001e+49\n",
      "Gradient Descent(29/49): loss=3.869953966440493e+51\n",
      "Gradient Descent(30/49): loss=2.6394164260621047e+53\n",
      "Gradient Descent(31/49): loss=1.8001555394660516e+55\n",
      "Gradient Descent(32/49): loss=1.2277562321248055e+57\n",
      "Gradient Descent(33/49): loss=8.373639568769679e+58\n",
      "Gradient Descent(34/49): loss=5.71105548422402e+60\n",
      "Gradient Descent(35/49): loss=3.895098956197025e+62\n",
      "Gradient Descent(36/49): loss=2.656566009641685e+64\n",
      "Gradient Descent(37/49): loss=1.811852033272588e+66\n",
      "Gradient Descent(38/49): loss=1.235733566777363e+68\n",
      "Gradient Descent(39/49): loss=8.428047213668654e+69\n",
      "Gradient Descent(40/49): loss=5.748163013898773e+71\n",
      "Gradient Descent(41/49): loss=3.920407325289658e+73\n",
      "Gradient Descent(42/49): loss=2.6738270224803842e+75\n",
      "Gradient Descent(43/49): loss=1.8236245249383887e+77\n",
      "Gradient Descent(44/49): loss=1.2437627340873229e+79\n",
      "Gradient Descent(45/49): loss=8.482808371732307e+80\n",
      "Gradient Descent(46/49): loss=5.785511649401103e+82\n",
      "Gradient Descent(47/49): loss=3.945880135274186e+84\n",
      "Gradient Descent(48/49): loss=2.6912001885888636e+86\n",
      "Gradient Descent(49/49): loss=1.8354735082588856e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3026571954459336\n",
      "Gradient Descent(2/49): loss=121.20675493701364\n",
      "Gradient Descent(3/49): loss=8103.23945261237\n",
      "Gradient Descent(4/49): loss=550863.2462473697\n",
      "Gradient Descent(5/49): loss=37588835.5437335\n",
      "Gradient Descent(6/49): loss=2567701522.701999\n",
      "Gradient Descent(7/49): loss=175457644419.57465\n",
      "Gradient Descent(8/49): loss=11990674463544.39\n",
      "Gradient Descent(9/49): loss=819461148419432.6\n",
      "Gradient Descent(10/49): loss=5.60037720285069e+16\n",
      "Gradient Descent(11/49): loss=3.8274319378487444e+18\n",
      "Gradient Descent(12/49): loss=2.615761072016197e+20\n",
      "Gradient Descent(13/49): loss=1.7876759249642557e+22\n",
      "Gradient Descent(14/49): loss=1.2217421262637546e+24\n",
      "Gradient Descent(15/49): loss=8.349689353256955e+25\n",
      "Gradient Descent(16/49): loss=5.706385282609476e+27\n",
      "Gradient Descent(17/49): loss=3.8998855767069045e+29\n",
      "Gradient Descent(18/49): loss=2.66527876589044e+31\n",
      "Gradient Descent(19/49): loss=1.821517775833741e+33\n",
      "Gradient Descent(20/49): loss=1.2448705367701894e+35\n",
      "Gradient Descent(21/49): loss=8.507754763175374e+36\n",
      "Gradient Descent(22/49): loss=5.814411135367356e+38\n",
      "Gradient Descent(23/49): loss=3.973713134926339e+40\n",
      "Gradient Descent(24/49): loss=2.715734355737084e+42\n",
      "Gradient Descent(25/49): loss=1.8560003806286564e+44\n",
      "Gradient Descent(26/49): loss=1.268436806279235e+46\n",
      "Gradient Descent(27/49): loss=8.66881250842704e+47\n",
      "Gradient Descent(28/49): loss=5.924482002907013e+49\n",
      "Gradient Descent(29/49): loss=4.048938302523951e+51\n",
      "Gradient Descent(30/49): loss=2.76714510561453e+53\n",
      "Gradient Descent(31/49): loss=1.891135765332198e+55\n",
      "Gradient Descent(32/49): loss=1.2924492017646972e+57\n",
      "Gradient Descent(33/49): loss=8.832919189431032e+58\n",
      "Gradient Descent(34/49): loss=6.036636588926706e+60\n",
      "Gradient Descent(35/49): loss=4.125587535134668e+62\n",
      "Gradient Descent(36/49): loss=2.8195290969279224e+64\n",
      "Gradient Descent(37/49): loss=1.926936287430812e+66\n",
      "Gradient Descent(38/49): loss=1.31691616868338e+68\n",
      "Gradient Descent(39/49): loss=9.000132524630622e+69\n",
      "Gradient Descent(40/49): loss=6.150914339665085e+71\n",
      "Gradient Descent(41/49): loss=4.203687791302867e+73\n",
      "Gradient Descent(42/49): loss=2.8729047538176403e+75\n",
      "Gradient Descent(43/49): loss=1.9634145384402787e+77\n",
      "Gradient Descent(44/49): loss=1.3418463123902651e+79\n",
      "Gradient Descent(45/49): loss=9.170511325161552e+80\n",
      "Gradient Descent(46/49): loss=6.267355448114626e+82\n",
      "Gradient Descent(47/49): loss=4.283266539918923e+84\n",
      "Gradient Descent(48/49): loss=2.9272908492062794e+86\n",
      "Gradient Descent(49/49): loss=2.0005833482426843e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.308582449984737\n",
      "Gradient Descent(2/49): loss=120.697010562579\n",
      "Gradient Descent(3/49): loss=7995.860213837575\n",
      "Gradient Descent(4/49): loss=538343.5432731359\n",
      "Gradient Descent(5/49): loss=36374905.65839744\n",
      "Gradient Descent(6/49): loss=2460305096.183936\n",
      "Gradient Descent(7/49): loss=166460262733.3035\n",
      "Gradient Descent(8/49): loss=11263507117732.062\n",
      "Gradient Descent(9/49): loss=762166002817664.0\n",
      "Gradient Descent(10/49): loss=5.157384588007783e+16\n",
      "Gradient Descent(11/49): loss=3.489881773232018e+18\n",
      "Gradient Descent(12/49): loss=2.3615236627838095e+20\n",
      "Gradient Descent(13/49): loss=1.5979898282629447e+22\n",
      "Gradient Descent(14/49): loss=1.081323789087058e+24\n",
      "Gradient Descent(15/49): loss=7.3170751663559396e+25\n",
      "Gradient Descent(16/49): loss=4.9513004326717546e+27\n",
      "Gradient Descent(17/49): loss=3.350433811684257e+29\n",
      "Gradient Descent(18/49): loss=2.2671633217322806e+31\n",
      "Gradient Descent(19/49): loss=1.5341385079210346e+33\n",
      "Gradient Descent(20/49): loss=1.0381170774583084e+35\n",
      "Gradient Descent(21/49): loss=7.02470514216042e+36\n",
      "Gradient Descent(22/49): loss=4.753460221991934e+38\n",
      "Gradient Descent(23/49): loss=3.2165597878982853e+40\n",
      "Gradient Descent(24/49): loss=2.1765737769856865e+42\n",
      "Gradient Descent(25/49): loss=1.4728385974626513e+44\n",
      "Gradient Descent(26/49): loss=9.966368046481316e+45\n",
      "Gradient Descent(27/49): loss=6.744017450998701e+47\n",
      "Gradient Descent(28/49): loss=4.563525164559111e+49\n",
      "Gradient Descent(29/49): loss=3.088034999743414e+51\n",
      "Gradient Descent(30/49): loss=2.0896039390113945e+53\n",
      "Gradient Descent(31/49): loss=1.4139880611115934e+55\n",
      "Gradient Descent(32/49): loss=9.568139682546863e+56\n",
      "Gradient Descent(33/49): loss=6.474545259792221e+58\n",
      "Gradient Descent(34/49): loss=4.381179384072229e+60\n",
      "Gradient Descent(35/49): loss=2.9646457048684676e+62\n",
      "Gradient Descent(36/49): loss=2.006109174015529e+64\n",
      "Gradient Descent(37/49): loss=1.3574890286081817e+66\n",
      "Gradient Descent(38/49): loss=9.185823417092403e+67\n",
      "Gradient Descent(39/49): loss=6.215840428302827e+69\n",
      "Gradient Descent(40/49): loss=4.2061196341126627e+71\n",
      "Gradient Descent(41/49): loss=2.84618670323532e+73\n",
      "Gradient Descent(42/49): loss=1.9259506277411078e+75\n",
      "Gradient Descent(43/49): loss=1.3032475403950048e+77\n",
      "Gradient Descent(44/49): loss=8.818783446892842e+78\n",
      "Gradient Descent(45/49): loss=5.967472722766051e+80\n",
      "Gradient Descent(46/49): loss=4.038054785153351e+82\n",
      "Gradient Descent(47/49): loss=2.7324609940306318e+84\n",
      "Gradient Descent(48/49): loss=1.848994994161602e+86\n",
      "Gradient Descent(49/49): loss=1.2511733912774479e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3179207948340834\n",
      "Gradient Descent(2/49): loss=122.05757481808918\n",
      "Gradient Descent(3/49): loss=8133.3189156381695\n",
      "Gradient Descent(4/49): loss=550786.1571869382\n",
      "Gradient Descent(5/49): loss=37436539.330276296\n",
      "Gradient Descent(6/49): loss=2547309300.216944\n",
      "Gradient Descent(7/49): loss=173386344634.54837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8/49): loss=11803061844296.645\n",
      "Gradient Descent(9/49): loss=803506355601817.0\n",
      "Gradient Descent(10/49): loss=5.470017203186732e+16\n",
      "Gradient Descent(11/49): loss=3.723827759537269e+18\n",
      "Gradient Descent(12/49): loss=2.535075881819853e+20\n",
      "Gradient Descent(13/49): loss=1.725808077431997e+22\n",
      "Gradient Descent(14/49): loss=1.1748815420849277e+24\n",
      "Gradient Descent(15/49): loss=7.998262996948454e+25\n",
      "Gradient Descent(16/49): loss=5.44499248885474e+27\n",
      "Gradient Descent(17/49): loss=3.706797754267237e+29\n",
      "Gradient Descent(18/49): loss=2.5234836660841017e+31\n",
      "Gradient Descent(19/49): loss=1.717916713453224e+33\n",
      "Gradient Descent(20/49): loss=1.1695093867223671e+35\n",
      "Gradient Descent(21/49): loss=7.961691011995469e+36\n",
      "Gradient Descent(22/49): loss=5.420095340058929e+38\n",
      "Gradient Descent(23/49): loss=3.689848482085762e+40\n",
      "Gradient Descent(24/49): loss=2.511945079663531e+42\n",
      "Gradient Descent(25/49): loss=1.7100615686207485e+44\n",
      "Gradient Descent(26/49): loss=1.1641618250928228e+46\n",
      "Gradient Descent(27/49): loss=7.9252863164255e+47\n",
      "Gradient Descent(28/49): loss=5.395312047130037e+49\n",
      "Gradient Descent(29/49): loss=3.6729767132294324e+51\n",
      "Gradient Descent(30/49): loss=2.5004592539002213e+53\n",
      "Gradient Descent(31/49): loss=1.7022423414489877e+55\n",
      "Gradient Descent(32/49): loss=1.1588387151288282e+57\n",
      "Gradient Descent(33/49): loss=7.889048080770418e+58\n",
      "Gradient Descent(34/49): loss=5.370642075397743e+60\n",
      "Gradient Descent(35/49): loss=3.6561820902498613e+62\n",
      "Gradient Descent(36/49): loss=2.4890259468787606e+64\n",
      "Gradient Descent(37/49): loss=1.6944588675593944e+66\n",
      "Gradient Descent(38/49): loss=1.1535399449937938e+68\n",
      "Gradient Descent(39/49): loss=7.852975543825913e+69\n",
      "Gradient Descent(40/49): loss=5.3460849066878e+71\n",
      "Gradient Descent(41/49): loss=3.639464260395616e+73\n",
      "Gradient Descent(42/49): loss=2.4776449184574132e+75\n",
      "Gradient Descent(43/49): loss=1.6867109834705715e+77\n",
      "Gradient Descent(44/49): loss=1.1482654033942625e+79\n",
      "Gradient Descent(45/49): loss=7.817067947937517e+80\n",
      "Gradient Descent(46/49): loss=5.321640025210332e+82\n",
      "Gradient Descent(47/49): loss=3.6228228725314643e+84\n",
      "Gradient Descent(48/49): loss=2.4663159295932264e+86\n",
      "Gradient Descent(49/49): loss=1.6789985264487898e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.344207262872909\n",
      "Gradient Descent(2/49): loss=126.38833801520835\n",
      "Gradient Descent(3/49): loss=8598.207955443262\n",
      "Gradient Descent(4/49): loss=593979.9942405531\n",
      "Gradient Descent(5/49): loss=41169212.759500146\n",
      "Gradient Descent(6/49): loss=2856128713.7362356\n",
      "Gradient Descent(7/49): loss=198199535260.05502\n",
      "Gradient Descent(8/49): loss=13755091662679.926\n",
      "Gradient Descent(9/49): loss=954630383824030.2\n",
      "Gradient Descent(10/49): loss=6.62537295609181e+16\n",
      "Gradient Descent(11/49): loss=4.5981847442982467e+18\n",
      "Gradient Descent(12/49): loss=3.1912645541329804e+20\n",
      "Gradient Descent(13/49): loss=2.214824372965619e+22\n",
      "Gradient Descent(14/49): loss=1.537148437782338e+24\n",
      "Gradient Descent(15/49): loss=1.066822902825394e+26\n",
      "Gradient Descent(16/49): loss=7.404041697697225e+27\n",
      "Gradient Descent(17/49): loss=5.13860673755301e+29\n",
      "Gradient Descent(18/49): loss=3.566333132113556e+31\n",
      "Gradient Descent(19/49): loss=2.4751323969657779e+33\n",
      "Gradient Descent(20/49): loss=1.717809345308544e+35\n",
      "Gradient Descent(21/49): loss=1.1922065060008478e+37\n",
      "Gradient Descent(22/49): loss=8.27423809772261e+38\n",
      "Gradient Descent(23/49): loss=5.74254676126189e+40\n",
      "Gradient Descent(24/49): loss=3.985483970345328e+42\n",
      "Gradient Descent(25/49): loss=2.7660345031985946e+44\n",
      "Gradient Descent(26/49): loss=1.9197033358592305e+46\n",
      "Gradient Descent(27/49): loss=1.332326438244902e+48\n",
      "Gradient Descent(28/49): loss=9.246708618401498e+49\n",
      "Gradient Descent(29/49): loss=6.417467808133664e+51\n",
      "Gradient Descent(30/49): loss=4.453897572426284e+53\n",
      "Gradient Descent(31/49): loss=3.0911263100568533e+55\n",
      "Gradient Descent(32/49): loss=2.1453259104745233e+57\n",
      "Gradient Descent(33/49): loss=1.488914654564436e+59\n",
      "Gradient Descent(34/49): loss=1.0333473519118512e+61\n",
      "Gradient Descent(35/49): loss=7.171712269939428e+62\n",
      "Gradient Descent(36/49): loss=4.977363786498303e+64\n",
      "Gradient Descent(37/49): loss=3.454426130142819e+66\n",
      "Gradient Descent(38/49): loss=2.397465887661928e+68\n",
      "Gradient Descent(39/49): loss=1.6639066710235347e+70\n",
      "Gradient Descent(40/49): loss=1.1547965808917475e+72\n",
      "Gradient Descent(41/49): loss=8.014603021087445e+73\n",
      "Gradient Descent(42/49): loss=5.562352941504367e+75\n",
      "Gradient Descent(43/49): loss=3.860424548097484e+77\n",
      "Gradient Descent(44/49): loss=2.6792398555570993e+79\n",
      "Gradient Descent(45/49): loss=1.8594654847336026e+81\n",
      "Gradient Descent(46/49): loss=1.290519727729501e+83\n",
      "Gradient Descent(47/49): loss=8.956558652647591e+84\n",
      "Gradient Descent(48/49): loss=6.216095823614609e+86\n",
      "Gradient Descent(49/49): loss=4.3141399266041516e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3369361359142875\n",
      "Gradient Descent(2/49): loss=125.50675547033764\n",
      "Gradient Descent(3/49): loss=8538.747665751886\n",
      "Gradient Descent(4/49): loss=590668.9026533124\n",
      "Gradient Descent(5/49): loss=41013705.56964474\n",
      "Gradient Descent(6/49): loss=2850931797.881617\n",
      "Gradient Descent(7/49): loss=198238331069.41208\n",
      "Gradient Descent(8/49): loss=13785811515597.361\n",
      "Gradient Descent(9/49): loss=958717381233906.0\n",
      "Gradient Descent(10/49): loss=6.667347109816319e+16\n",
      "Gradient Descent(11/49): loss=4.636783646954807e+18\n",
      "Gradient Descent(12/49): loss=3.2246382593961715e+20\n",
      "Gradient Descent(13/49): loss=2.242566333625634e+22\n",
      "Gradient Descent(14/49): loss=1.5595870935310268e+24\n",
      "Gradient Descent(15/49): loss=1.0846109358491882e+26\n",
      "Gradient Descent(16/49): loss=7.542899637808532e+27\n",
      "Gradient Descent(17/49): loss=5.245690709681095e+29\n",
      "Gradient Descent(18/49): loss=3.6481025024524134e+31\n",
      "Gradient Descent(19/49): loss=2.5370637745109823e+33\n",
      "Gradient Descent(20/49): loss=1.7643946660276753e+35\n",
      "Gradient Descent(21/49): loss=1.227043864194531e+37\n",
      "Gradient Descent(22/49): loss=8.533445910141884e+38\n",
      "Gradient Descent(23/49): loss=5.934563647347008e+40\n",
      "Gradient Descent(24/49): loss=4.127177467965383e+42\n",
      "Gradient Descent(25/49): loss=2.870235263160361e+44\n",
      "Gradient Descent(26/49): loss=1.9960979458320032e+46\n",
      "Gradient Descent(27/49): loss=1.3881813315078779e+48\n",
      "Gradient Descent(28/49): loss=9.654072402463115e+49\n",
      "Gradient Descent(29/49): loss=6.713900542860812e+51\n",
      "Gradient Descent(30/49): loss=4.66916536568819e+53\n",
      "Gradient Descent(31/49): loss=3.247159393107806e+55\n",
      "Gradient Descent(32/49): loss=2.2582288906990038e+57\n",
      "Gradient Descent(33/49): loss=1.5704796424874278e+59\n",
      "Gradient Descent(34/49): loss=1.092186145357482e+61\n",
      "Gradient Descent(35/49): loss=7.5955812723652305e+62\n",
      "Gradient Descent(36/49): loss=5.282328027171777e+64\n",
      "Gradient Descent(37/49): loss=3.673581834765303e+66\n",
      "Gradient Descent(38/49): loss=2.5547833128309354e+68\n",
      "Gradient Descent(39/49): loss=1.776717674763977e+70\n",
      "Gradient Descent(40/49): loss=1.2356138698591969e+72\n",
      "Gradient Descent(41/49): loss=8.593045800544735e+73\n",
      "Gradient Descent(42/49): loss=5.976012242293298e+75\n",
      "Gradient Descent(43/49): loss=4.156002789811178e+77\n",
      "Gradient Descent(44/49): loss=2.890281761251895e+79\n",
      "Gradient Descent(45/49): loss=2.0100392328670524e+81\n",
      "Gradient Descent(46/49): loss=1.3978767647603862e+83\n",
      "Gradient Descent(47/49): loss=9.721499050890504e+84\n",
      "Gradient Descent(48/49): loss=6.76079223712287e+86\n",
      "Gradient Descent(49/49): loss=4.701776077358547e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3430249568602934\n",
      "Gradient Descent(2/49): loss=124.98094535848936\n",
      "Gradient Descent(3/49): loss=8425.819817144076\n",
      "Gradient Descent(4/49): loss=577265.6454901972\n",
      "Gradient Descent(5/49): loss=39691008.827643186\n",
      "Gradient Descent(6/49): loss=2731841507.8293242\n",
      "Gradient Descent(7/49): loss=188085029788.26938\n",
      "Gradient Descent(8/49): loss=12950744423658.145\n",
      "Gradient Descent(9/49): loss=891760488706626.1\n",
      "Gradient Descent(10/49): loss=6.140528797574727e+16\n",
      "Gradient Descent(11/49): loss=4.2282882377767265e+18\n",
      "Gradient Descent(12/49): loss=2.911546921446535e+20\n",
      "Gradient Descent(13/49): loss=2.0048557276193377e+22\n",
      "Gradient Descent(14/49): loss=1.3805193442472688e+24\n",
      "Gradient Descent(15/49): loss=9.506089075101282e+25\n",
      "Gradient Descent(16/49): loss=6.545777910882233e+27\n",
      "Gradient Descent(17/49): loss=4.5073434760439284e+29\n",
      "Gradient Descent(18/49): loss=3.103702188638696e+31\n",
      "Gradient Descent(19/49): loss=2.1371717796803888e+33\n",
      "Gradient Descent(20/49): loss=1.4716306329139686e+35\n",
      "Gradient Descent(21/49): loss=1.0133470507041232e+37\n",
      "Gradient Descent(22/49): loss=6.977785201071868e+38\n",
      "Gradient Descent(23/49): loss=4.804818475426544e+40\n",
      "Gradient Descent(24/49): loss=3.3085398757004367e+42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=2.2782205332181333e+44\n",
      "Gradient Descent(26/49): loss=1.568755098313049e+46\n",
      "Gradient Descent(27/49): loss=1.0802257826229632e+48\n",
      "Gradient Descent(28/49): loss=7.43830405841039e+49\n",
      "Gradient Descent(29/49): loss=5.121926189450737e+51\n",
      "Gradient Descent(30/49): loss=3.5268964113558803e+53\n",
      "Gradient Descent(31/49): loss=2.428578202094106e+55\n",
      "Gradient Descent(32/49): loss=1.6722895701434046e+57\n",
      "Gradient Descent(33/49): loss=1.1515183674130888e+59\n",
      "Gradient Descent(34/49): loss=7.929216172627266e+60\n",
      "Gradient Descent(35/49): loss=5.459962332472251e+62\n",
      "Gradient Descent(36/49): loss=3.7596640100351997e+64\n",
      "Gradient Descent(37/49): loss=2.588859154629668e+66\n",
      "Gradient Descent(38/49): loss=1.7826570950543675e+68\n",
      "Gradient Descent(39/49): loss=1.2275161098913823e+70\n",
      "Gradient Descent(40/49): loss=8.452527433476529e+71\n",
      "Gradient Descent(41/49): loss=5.820308135914839e+73\n",
      "Gradient Descent(42/49): loss=4.007793770988475e+75\n",
      "Gradient Descent(43/49): loss=2.759718306262728e+77\n",
      "Gradient Descent(44/49): loss=1.9003086398937178e+79\n",
      "Gradient Descent(45/49): loss=1.3085295403736484e+81\n",
      "Gradient Descent(46/49): loss=9.010376115146412e+82\n",
      "Gradient Descent(47/49): loss=6.204435989516745e+84\n",
      "Gradient Descent(48/49): loss=4.2722995639772254e+86\n",
      "Gradient Descent(49/49): loss=2.941853795445734e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.352200110633965\n",
      "Gradient Descent(2/49): loss=126.35429893035747\n",
      "Gradient Descent(3/49): loss=8567.067513817263\n",
      "Gradient Descent(4/49): loss=590276.8515681955\n",
      "Gradient Descent(5/49): loss=40820795.23543636\n",
      "Gradient Descent(6/49): loss=2826068649.9268827\n",
      "Gradient Descent(7/49): loss=195718582625.81598\n",
      "Gradient Descent(8/49): loss=13555899523633.58\n",
      "Gradient Descent(9/49): loss=938943785400714.4\n",
      "Gradient Descent(10/49): loss=6.5036271123703896e+16\n",
      "Gradient Descent(11/49): loss=4.50477608283199e+18\n",
      "Gradient Descent(12/49): loss=3.12026353719986e+20\n",
      "Gradient Descent(13/49): loss=2.1612723689440586e+22\n",
      "Gradient Descent(14/49): loss=1.497020601213069e+24\n",
      "Gradient Descent(15/49): loss=1.0369219522158976e+26\n",
      "Gradient Descent(16/49): loss=7.182313636685716e+27\n",
      "Gradient Descent(17/49): loss=4.974880632367896e+29\n",
      "Gradient Descent(18/49): loss=3.4458864634266656e+31\n",
      "Gradient Descent(19/49): loss=2.386817775335473e+33\n",
      "Gradient Descent(20/49): loss=1.6532463137837233e+35\n",
      "Gradient Descent(21/49): loss=1.145132821782364e+37\n",
      "Gradient Descent(22/49): loss=7.931843964249253e+38\n",
      "Gradient Descent(23/49): loss=5.494048155526198e+40\n",
      "Gradient Descent(24/49): loss=3.805491544126494e+42\n",
      "Gradient Descent(25/49): loss=2.6359007934537285e+44\n",
      "Gradient Descent(26/49): loss=1.825775438564462e+46\n",
      "Gradient Descent(27/49): loss=1.2646363475985375e+48\n",
      "Gradient Descent(28/49): loss=8.759593638333006e+49\n",
      "Gradient Descent(29/49): loss=6.067394856587105e+51\n",
      "Gradient Descent(30/49): loss=4.202624215881468e+53\n",
      "Gradient Descent(31/49): loss=2.910977564075704e+55\n",
      "Gradient Descent(32/49): loss=2.0163093208596134e+57\n",
      "Gradient Descent(33/49): loss=1.3966109967859616e+59\n",
      "Gradient Descent(34/49): loss=9.67372543569807e+60\n",
      "Gradient Descent(35/49): loss=6.70057474992183e+62\n",
      "Gradient Descent(36/49): loss=4.641200773965345e+64\n",
      "Gradient Descent(37/49): loss=3.214760737428946e+66\n",
      "Gradient Descent(38/49): loss=2.22672689724754e+68\n",
      "Gradient Descent(39/49): loss=1.5423582281557725e+70\n",
      "Gradient Descent(40/49): loss=1.0683254003444874e+72\n",
      "Gradient Descent(41/49): loss=7.399831894993041e+73\n",
      "Gradient Descent(42/49): loss=5.125546210592751e+75\n",
      "Gradient Descent(43/49): loss=3.5502460501430574e+77\n",
      "Gradient Descent(44/49): loss=2.4591031860190096e+79\n",
      "Gradient Descent(45/49): loss=1.7033153178904842e+81\n",
      "Gradient Descent(46/49): loss=1.1798134737311228e+83\n",
      "Gradient Descent(47/49): loss=8.172061967489465e+84\n",
      "Gradient Descent(48/49): loss=5.660436864590945e+86\n",
      "Gradient Descent(49/49): loss=3.920741867289504e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3789684742698913\n",
      "Gradient Descent(2/49): loss=130.8328783316344\n",
      "Gradient Descent(3/49): loss=9056.233485756056\n",
      "Gradient Descent(4/49): loss=636520.8656848515\n",
      "Gradient Descent(5/49): loss=44886845.38801681\n",
      "Gradient Descent(6/49): loss=3168341790.738757\n",
      "Gradient Descent(7/49): loss=223699686384.30176\n",
      "Gradient Descent(8/49): loss=15795559223398.918\n",
      "Gradient Descent(9/49): loss=1115361489234673.5\n",
      "Gradient Descent(10/49): loss=7.87588985063553e+16\n",
      "Gradient Descent(11/49): loss=5.56140652618375e+18\n",
      "Gradient Descent(12/49): loss=3.9270819268661104e+20\n",
      "Gradient Descent(13/49): loss=2.773035158031694e+22\n",
      "Gradient Descent(14/49): loss=1.958126833691436e+24\n",
      "Gradient Descent(15/49): loss=1.3826946129582326e+26\n",
      "Gradient Descent(16/49): loss=9.763639268015395e+27\n",
      "Gradient Descent(17/49): loss=6.894411176706347e+29\n",
      "Gradient Descent(18/49): loss=4.868359450405034e+31\n",
      "Gradient Descent(19/49): loss=3.437700934138221e+33\n",
      "Gradient Descent(20/49): loss=2.4274681920255904e+35\n",
      "Gradient Descent(21/49): loss=1.7141112436149265e+37\n",
      "Gradient Descent(22/49): loss=1.2103875820716393e+39\n",
      "Gradient Descent(23/49): loss=8.546925436098258e+40\n",
      "Gradient Descent(24/49): loss=6.03525147583128e+42\n",
      "Gradient Descent(25/49): loss=4.261679904529133e+44\n",
      "Gradient Descent(26/49): loss=3.00930552461636e+46\n",
      "Gradient Descent(27/49): loss=2.1249647893222638e+48\n",
      "Gradient Descent(28/49): loss=1.5005041259262256e+50\n",
      "Gradient Descent(29/49): loss=1.0595529127048391e+52\n",
      "Gradient Descent(30/49): loss=7.481834640929876e+53\n",
      "Gradient Descent(31/49): loss=5.28315753965679e+55\n",
      "Gradient Descent(32/49): loss=3.7306028438718997e+57\n",
      "Gradient Descent(33/49): loss=2.634295395175579e+59\n",
      "Gradient Descent(34/49): loss=1.8601584032035268e+61\n",
      "Gradient Descent(35/49): loss=1.3135160511405268e+63\n",
      "Gradient Descent(36/49): loss=9.275147824144898e+64\n",
      "Gradient Descent(37/49): loss=6.5494720894382054e+66\n",
      "Gradient Descent(38/49): loss=4.624787169285318e+68\n",
      "Gradient Descent(39/49): loss=3.2657069255517336e+70\n",
      "Gradient Descent(40/49): loss=2.306017841085781e+72\n",
      "Gradient Descent(41/49): loss=1.6283513507592303e+74\n",
      "Gradient Descent(42/49): loss=1.1498298383810218e+76\n",
      "Gradient Descent(43/49): loss=8.119308259945761e+77\n",
      "Gradient Descent(44/49): loss=5.733297608004693e+79\n",
      "Gradient Descent(45/49): loss=4.0484608305993736e+81\n",
      "Gradient Descent(46/49): loss=2.858744865086722e+83\n",
      "Gradient Descent(47/49): loss=2.0186491967244113e+85\n",
      "Gradient Descent(48/49): loss=1.4254313594761884e+87\n",
      "Gradient Descent(49/49): loss=1.0065416833569386e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3715281877179732\n",
      "Gradient Descent(2/49): loss=129.92056987966316\n",
      "Gradient Descent(3/49): loss=8993.5912243547\n",
      "Gradient Descent(4/49): loss=632969.0587036966\n",
      "Gradient Descent(5/49): loss=44716910.45542981\n",
      "Gradient Descent(6/49): loss=3162542057.6374574\n",
      "Gradient Descent(7/49): loss=223740550467.37592\n",
      "Gradient Descent(8/49): loss=15830598211552.807\n",
      "Gradient Descent(9/49): loss=1120117633125509.1\n",
      "Gradient Descent(10/49): loss=7.925637146904032e+16\n",
      "Gradient Descent(11/49): loss=5.607974814184649e+18\n",
      "Gradient Descent(12/49): loss=3.9680609353757957e+20\n",
      "Gradient Descent(13/49): loss=2.807700233596421e+22\n",
      "Gradient Descent(14/49): loss=1.9866583318632634e+24\n",
      "Gradient Descent(15/49): loss=1.4057096950161427e+26\n",
      "Gradient Descent(16/49): loss=9.946449937131007e+27\n",
      "Gradient Descent(17/49): loss=7.037859023978117e+29\n",
      "Gradient Descent(18/49): loss=4.979812897710063e+31\n",
      "Gradient Descent(19/49): loss=3.523590969673799e+33\n",
      "Gradient Descent(20/49): loss=2.4932047805031776e+35\n",
      "Gradient Descent(21/49): loss=1.7641293019482423e+37\n",
      "Gradient Descent(22/49): loss=1.2482537408695627e+39\n",
      "Gradient Descent(23/49): loss=8.832331053500022e+40\n",
      "Gradient Descent(24/49): loss=6.249536395087154e+42\n",
      "Gradient Descent(25/49): loss=4.422015537795197e+44\n",
      "Gradient Descent(26/49): loss=3.128907518944206e+46\n",
      "Gradient Descent(27/49): loss=2.21393664912066e+48\n",
      "Gradient Descent(28/49): loss=1.5665261618130574e+50\n",
      "Gradient Descent(29/49): loss=1.108434704588061e+52\n",
      "Gradient Descent(30/49): loss=7.843006547131237e+53\n",
      "Gradient Descent(31/49): loss=5.549515135508495e+55\n",
      "Gradient Descent(32/49): loss=3.9266980148706906e+57\n",
      "Gradient Descent(33/49): loss=2.7784332366861095e+59\n",
      "Gradient Descent(34/49): loss=1.9659498187757237e+61\n",
      "Gradient Descent(35/49): loss=1.3910568873535835e+63\n",
      "Gradient Descent(36/49): loss=9.84277037680889e+64\n",
      "Gradient Descent(37/49): loss=6.964497970668614e+66\n",
      "Gradient Descent(38/49): loss=4.927904454393299e+68\n",
      "Gradient Descent(39/49): loss=3.486861854781757e+70\n",
      "Gradient Descent(40/49): loss=2.467216178165329e+72\n",
      "Gradient Descent(41/49): loss=1.7457404174051255e+74\n",
      "Gradient Descent(42/49): loss=1.2352422264141012e+76\n",
      "Gradient Descent(43/49): loss=8.740264833785821e+77\n",
      "Gradient Descent(44/49): loss=6.184392642281942e+79\n",
      "Gradient Descent(45/49): loss=4.3759214487491265e+81\n",
      "Gradient Descent(46/49): loss=3.0962924951927146e+83\n",
      "Gradient Descent(47/49): loss=2.190859074613228e+85\n",
      "Gradient Descent(48/49): loss=1.5501970476844099e+87\n",
      "Gradient Descent(49/49): loss=1.0968806321208553e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.377782393293757\n",
      "Gradient Descent(2/49): loss=129.37832172627057\n",
      "Gradient Descent(3/49): loss=8874.878192871849\n",
      "Gradient Descent(4/49): loss=618628.1887508072\n",
      "Gradient Descent(5/49): loss=43276755.6630373\n",
      "Gradient Descent(6/49): loss=3030601316.317092\n",
      "Gradient Descent(7/49): loss=212294688327.1269\n",
      "Gradient Descent(8/49): loss=14872758862577.45\n",
      "Gradient Descent(9/49): loss=1041974407959231.6\n",
      "Gradient Descent(10/49): loss=7.300063928926558e+16\n",
      "Gradient Descent(11/49): loss=5.114433731408222e+18\n",
      "Gradient Descent(12/49): loss=3.583181847003625e+20\n",
      "Gradient Descent(13/49): loss=2.510384636124507e+22\n",
      "Gradient Descent(14/49): loss=1.758780843706418e+24\n",
      "Gradient Descent(15/49): loss=1.2322056546261517e+26\n",
      "Gradient Descent(16/49): loss=8.632859484520296e+27\n",
      "Gradient Descent(17/49): loss=6.048200055483649e+29\n",
      "Gradient Descent(18/49): loss=4.23738206426407e+31\n",
      "Gradient Descent(19/49): loss=2.968719056699491e+33\n",
      "Gradient Descent(20/49): loss=2.0798910045639976e+35\n",
      "Gradient Descent(21/49): loss=1.457176145118773e+37\n",
      "Gradient Descent(22/49): loss=1.0209007651177844e+39\n",
      "Gradient Descent(23/49): loss=7.152452884382538e+40\n",
      "Gradient Descent(24/49): loss=5.0110239909019867e+42\n",
      "Gradient Descent(25/49): loss=3.510734267432988e+44\n",
      "Gradient Descent(26/49): loss=2.4596280358878725e+46\n",
      "Gradient Descent(27/49): loss=1.723220732211465e+48\n",
      "Gradient Descent(28/49): loss=1.2072921793849735e+50\n",
      "Gradient Descent(29/49): loss=8.458315172041798e+51\n",
      "Gradient Descent(30/49): loss=5.9259139395765986e+53\n",
      "Gradient Descent(31/49): loss=4.1517081481360296e+55\n",
      "Gradient Descent(32/49): loss=2.908695725765236e+57\n",
      "Gradient Descent(33/49): loss=2.0378385289157104e+59\n",
      "Gradient Descent(34/49): loss=1.427714089565288e+61\n",
      "Gradient Descent(35/49): loss=1.0002595851535954e+63\n",
      "Gradient Descent(36/49): loss=7.007840330246201e+64\n",
      "Gradient Descent(37/49): loss=4.909708122085578e+66\n",
      "Gradient Descent(38/49): loss=3.439752150178663e+68\n",
      "Gradient Descent(39/49): loss=2.4098978107139885e+70\n",
      "Gradient Descent(40/49): loss=1.6883796286841275e+72\n",
      "Gradient Descent(41/49): loss=1.1828824267494513e+74\n",
      "Gradient Descent(42/49): loss=8.287299916092761e+75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=5.806100280650987e+77\n",
      "Gradient Descent(44/49): loss=4.0677664390441265e+79\n",
      "Gradient Descent(45/49): loss=2.8498859824650605e+81\n",
      "Gradient Descent(46/49): loss=1.9966362953128112e+83\n",
      "Gradient Descent(47/49): loss=1.3988477154135985e+85\n",
      "Gradient Descent(48/49): loss=9.80035740866499e+86\n",
      "Gradient Descent(49/49): loss=6.866151638899246e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3867899942267328\n",
      "Gradient Descent(2/49): loss=130.76387523657434\n",
      "Gradient Descent(3/49): loss=9019.925834055366\n",
      "Gradient Descent(4/49): loss=632223.2373993349\n",
      "Gradient Descent(5/49): loss=44477948.425474636\n",
      "Gradient Descent(6/49): loss=3132541107.915911\n",
      "Gradient Descent(7/49): loss=220697692747.71643\n",
      "Gradient Descent(8/49): loss=15550561320112.025\n",
      "Gradient Descent(9/49): loss=1095744872956226.1\n",
      "Gradient Descent(10/49): loss=7.721073070851715e+16\n",
      "Gradient Descent(11/49): loss=5.440607926765148e+18\n",
      "Gradient Descent(12/49): loss=3.833696209361218e+20\n",
      "Gradient Descent(13/49): loss=2.7013951813773706e+22\n",
      "Gradient Descent(14/49): loss=1.90352505520602e+24\n",
      "Gradient Descent(15/49): loss=1.3413097788433456e+26\n",
      "Gradient Descent(16/49): loss=9.451474987523248e+27\n",
      "Gradient Descent(17/49): loss=6.65993651863404e+29\n",
      "Gradient Descent(18/49): loss=4.6928923312487065e+31\n",
      "Gradient Descent(19/49): loss=3.3068240785708315e+33\n",
      "Gradient Descent(20/49): loss=2.330137730886685e+35\n",
      "Gradient Descent(21/49): loss=1.6419203792280126e+37\n",
      "Gradient Descent(22/49): loss=1.1569713223494216e+39\n",
      "Gradient Descent(23/49): loss=8.152542947143795e+40\n",
      "Gradient Descent(24/49): loss=5.744650296962355e+42\n",
      "Gradient Descent(25/49): loss=4.047940286652036e+44\n",
      "Gradient Descent(26/49): loss=2.8523617134653745e+46\n",
      "Gradient Descent(27/49): loss=2.009903004565459e+48\n",
      "Gradient Descent(28/49): loss=1.4162685148558552e+50\n",
      "Gradient Descent(29/49): loss=9.979668181080641e+51\n",
      "Gradient Descent(30/49): loss=7.032125332152112e+53\n",
      "Gradient Descent(31/49): loss=4.955153396867853e+55\n",
      "Gradient Descent(32/49): loss=3.491625081570691e+57\n",
      "Gradient Descent(33/49): loss=2.460356871688327e+59\n",
      "Gradient Descent(34/49): loss=1.73367867243665e+61\n",
      "Gradient Descent(35/49): loss=1.2216283636931033e+63\n",
      "Gradient Descent(36/49): loss=8.608145688739364e+64\n",
      "Gradient Descent(37/49): loss=6.065688584255652e+66\n",
      "Gradient Descent(38/49): loss=4.2741583764432635e+68\n",
      "Gradient Descent(39/49): loss=3.0117652057407746e+70\n",
      "Gradient Descent(40/49): loss=2.1222259110713972e+72\n",
      "Gradient Descent(41/49): loss=1.495416312346647e+74\n",
      "Gradient Descent(42/49): loss=1.0537379341031014e+76\n",
      "Gradient Descent(43/49): loss=7.42511382683437e+77\n",
      "Gradient Descent(44/49): loss=5.232070855299827e+79\n",
      "Gradient Descent(45/49): loss=3.6867536408595134e+81\n",
      "Gradient Descent(46/49): loss=2.597853275366604e+83\n",
      "Gradient Descent(47/49): loss=1.8305648540051646e+85\n",
      "Gradient Descent(48/49): loss=1.2898987469744944e+87\n",
      "Gradient Descent(49/49): loss=9.089209671025774e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4140445322986377\n",
      "Gradient Descent(2/49): loss=135.39401811732066\n",
      "Gradient Descent(3/49): loss=9534.415691448305\n",
      "Gradient Descent(4/49): loss=681704.2161731425\n",
      "Gradient Descent(5/49): loss=48903933.807186134\n",
      "Gradient Descent(6/49): loss=3511560207.651963\n",
      "Gradient Descent(7/49): loss=252218888361.7225\n",
      "Gradient Descent(8/49): loss=18117224040000.926\n",
      "Gradient Descent(9/49): loss=1301418004810465.2\n",
      "Gradient Descent(10/49): loss=9.348573596447902e+16\n",
      "Gradient Descent(11/49): loss=6.715446976858222e+18\n",
      "Gradient Descent(12/49): loss=4.823972379686633e+20\n",
      "Gradient Descent(13/49): loss=3.4652517924416615e+22\n",
      "Gradient Descent(14/49): loss=2.489228762457872e+24\n",
      "Gradient Descent(15/49): loss=1.7881124751859885e+26\n",
      "Gradient Descent(16/49): loss=1.2844726415995886e+28\n",
      "Gradient Descent(17/49): loss=9.226880261094636e+29\n",
      "Gradient Descent(18/49): loss=6.628036802445607e+31\n",
      "Gradient Descent(19/49): loss=4.7611836958243624e+33\n",
      "Gradient Descent(20/49): loss=3.420148508870666e+35\n",
      "Gradient Descent(21/49): loss=2.4568293454387057e+37\n",
      "Gradient Descent(22/49): loss=1.7648386954497102e+39\n",
      "Gradient Descent(23/49): loss=1.267754159133637e+41\n",
      "Gradient Descent(24/49): loss=9.106784728515147e+42\n",
      "Gradient Descent(25/49): loss=6.541767384001644e+44\n",
      "Gradient Descent(26/49): loss=4.699212925544506e+46\n",
      "Gradient Descent(27/49): loss=3.375632428265393e+48\n",
      "Gradient Descent(28/49): loss=2.4248516658641474e+50\n",
      "Gradient Descent(29/49): loss=1.7418678503647458e+52\n",
      "Gradient Descent(30/49): loss=1.251253283178877e+54\n",
      "Gradient Descent(31/49): loss=8.98825234266813e+55\n",
      "Gradient Descent(32/49): loss=6.456620834610805e+57\n",
      "Gradient Descent(33/49): loss=4.638048756601262e+59\n",
      "Gradient Descent(34/49): loss=3.3316957615503413e+61\n",
      "Gradient Descent(35/49): loss=2.3932902024226966e+63\n",
      "Gradient Descent(36/49): loss=1.7191959899565192e+65\n",
      "Gradient Descent(37/49): loss=1.234967179864193e+67\n",
      "Gradient Descent(38/49): loss=8.871262754517563e+68\n",
      "Gradient Descent(39/49): loss=6.372582538456198e+70\n",
      "Gradient Descent(40/49): loss=4.577680690244113e+72\n",
      "Gradient Descent(41/49): loss=3.288330967135691e+74\n",
      "Gradient Descent(42/49): loss=2.362139537707036e+76\n",
      "Gradient Descent(43/49): loss=1.6968192226888294e+78\n",
      "Gradient Descent(44/49): loss=1.2188930537444882e+80\n",
      "Gradient Descent(45/49): loss=8.755795883266096e+81\n",
      "Gradient Descent(46/49): loss=6.289638070699044e+83\n",
      "Gradient Descent(47/49): loss=4.518098364535007e+85\n",
      "Gradient Descent(48/49): loss=3.2455306016273003e+87\n",
      "Gradient Descent(49/49): loss=2.3313943248296956e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4064333508569917\n",
      "Gradient Descent(2/49): loss=134.45018992440575\n",
      "Gradient Descent(3/49): loss=9468.450917435033\n",
      "Gradient Descent(4/49): loss=677896.5270228377\n",
      "Gradient Descent(5/49): loss=48718378.4468682\n",
      "Gradient Descent(6/49): loss=3505094383.204961\n",
      "Gradient Descent(7/49): loss=252261722902.06378\n",
      "Gradient Descent(8/49): loss=18157144901071.12\n",
      "Gradient Descent(9/49): loss=1306945765249472.8\n",
      "Gradient Descent(10/49): loss=9.40744872445436e+16\n",
      "Gradient Descent(11/49): loss=6.771540602395313e+18\n",
      "Gradient Descent(12/49): loss=4.874202055163248e+20\n",
      "Gradient Descent(13/49): loss=3.5084856055121175e+22\n",
      "Gradient Descent(14/49): loss=2.525433341125234e+24\n",
      "Gradient Descent(15/49): loss=1.8178252559439647e+26\n",
      "Gradient Descent(16/49): loss=1.3084838300390154e+28\n",
      "Gradient Descent(17/49): loss=9.418561727205079e+29\n",
      "Gradient Descent(18/49): loss=6.779549204160299e+31\n",
      "Gradient Descent(19/49): loss=4.8799688057734013e+33\n",
      "Gradient Descent(20/49): loss=3.512637024996217e+35\n",
      "Gradient Descent(21/49): loss=2.5284216684128023e+37\n",
      "Gradient Descent(22/49): loss=1.8199762992471803e+39\n",
      "Gradient Descent(23/49): loss=1.3100321719316413e+41\n",
      "Gradient Descent(24/49): loss=9.429706816551224e+42\n",
      "Gradient Descent(25/49): loss=6.787571523149863e+44\n",
      "Gradient Descent(26/49): loss=4.885743329900073e+46\n",
      "Gradient Descent(27/49): loss=3.51679356957794e+48\n",
      "Gradient Descent(28/49): loss=2.5314135794517433e+50\n",
      "Gradient Descent(29/49): loss=1.8221298985717116e+52\n",
      "Gradient Descent(30/49): loss=1.3115823483842016e+54\n",
      "Gradient Descent(31/49): loss=9.440865099362277e+55\n",
      "Gradient Descent(32/49): loss=6.795603336241869e+57\n",
      "Gradient Descent(33/49): loss=4.89152468735741e+59\n",
      "Gradient Descent(34/49): loss=3.520955032707841e+61\n",
      "Gradient Descent(35/49): loss=2.5344090308676542e+63\n",
      "Gradient Descent(36/49): loss=1.8242860462786542e+65\n",
      "Gradient Descent(37/49): loss=1.313134359179452e+67\n",
      "Gradient Descent(38/49): loss=9.452036585901956e+68\n",
      "Gradient Descent(39/49): loss=6.803644653472933e+70\n",
      "Gradient Descent(40/49): loss=4.8973128859629625e+72\n",
      "Gradient Descent(41/49): loss=3.5251214201459527e+74\n",
      "Gradient Descent(42/49): loss=2.5374080268364037e+76\n",
      "Gradient Descent(43/49): loss=1.8264447453805026e+78\n",
      "Gradient Descent(44/49): loss=1.314688206487307e+80\n",
      "Gradient Descent(45/49): loss=9.463221291792925e+81\n",
      "Gradient Descent(46/49): loss=6.811695486089176e+83\n",
      "Gradient Descent(47/49): loss=4.903107933811944e+85\n",
      "Gradient Descent(48/49): loss=3.5292927377192408e+87\n",
      "Gradient Descent(49/49): loss=2.5404105715522945e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.412854759285129\n",
      "Gradient Descent(2/49): loss=133.8911258389829\n",
      "Gradient Descent(3/49): loss=9343.708140383616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=662561.1766062392\n",
      "Gradient Descent(5/49): loss=47151462.651794665\n",
      "Gradient Descent(6/49): loss=3359043544.9020624\n",
      "Gradient Descent(7/49): loss=239371883333.1318\n",
      "Gradient Descent(8/49): loss=17059764833246.893\n",
      "Gradient Descent(9/49): loss=1215867266002893.5\n",
      "Gradient Descent(10/49): loss=8.665695607121117e+16\n",
      "Gradient Descent(11/49): loss=6.176209080991599e+18\n",
      "Gradient Descent(12/49): loss=4.4019079408157576e+20\n",
      "Gradient Descent(13/49): loss=3.137328893944925e+22\n",
      "Gradient Descent(14/49): loss=2.2360380150623817e+24\n",
      "Gradient Descent(15/49): loss=1.593669748407813e+26\n",
      "Gradient Descent(16/49): loss=1.1358408367242088e+28\n",
      "Gradient Descent(17/49): loss=8.095368637831512e+29\n",
      "Gradient Descent(18/49): loss=5.769733867795004e+31\n",
      "Gradient Descent(19/49): loss=4.1122066707256875e+33\n",
      "Gradient Descent(20/49): loss=2.9308533273310024e+35\n",
      "Gradient Descent(21/49): loss=2.0888787734608654e+37\n",
      "Gradient Descent(22/49): loss=1.4887863850309174e+39\n",
      "Gradient Descent(23/49): loss=1.0610883352446159e+41\n",
      "Gradient Descent(24/49): loss=7.562592367270775e+42\n",
      "Gradient Descent(25/49): loss=5.390013386617111e+44\n",
      "Gradient Descent(26/49): loss=3.841572161636752e+46\n",
      "Gradient Descent(27/49): loss=2.7379666087109747e+48\n",
      "Gradient Descent(28/49): loss=1.951404486235758e+50\n",
      "Gradient Descent(29/49): loss=1.3908056646073677e+52\n",
      "Gradient Descent(30/49): loss=9.912554830881188e+53\n",
      "Gradient Descent(31/49): loss=7.064879427491046e+55\n",
      "Gradient Descent(32/49): loss=5.0352832520523004e+57\n",
      "Gradient Descent(33/49): loss=3.5887487803033096e+59\n",
      "Gradient Descent(34/49): loss=2.5577742429642247e+61\n",
      "Gradient Descent(35/49): loss=1.822977722452405e+63\n",
      "Gradient Descent(36/49): loss=1.299273298141603e+65\n",
      "Gradient Descent(37/49): loss=9.260185039413339e+66\n",
      "Gradient Descent(38/49): loss=6.59992220934789e+68\n",
      "Gradient Descent(39/49): loss=4.7038987864764333e+70\n",
      "Gradient Descent(40/49): loss=3.3525643320576053e+72\n",
      "Gradient Descent(41/49): loss=2.3894407832282703e+74\n",
      "Gradient Descent(42/49): loss=1.7030030421669677e+76\n",
      "Gradient Descent(43/49): loss=1.2137649034815462e+78\n",
      "Gradient Descent(44/49): loss=8.6507493201479e+79\n",
      "Gradient Descent(45/49): loss=6.1655649776478535e+81\n",
      "Gradient Descent(46/49): loss=4.394323553574864e+83\n",
      "Gradient Descent(47/49): loss=3.131923767490582e+85\n",
      "Gradient Descent(48/49): loss=2.232185765518485e+87\n",
      "Gradient Descent(49/49): loss=1.5909241928246632e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.421690445612387\n",
      "Gradient Descent(2/49): loss=135.28826358106792\n",
      "Gradient Descent(3/49): loss=9492.561609899538\n",
      "Gradient Descent(4/49): loss=676755.0622240696\n",
      "Gradient Descent(5/49): loss=48427387.34160528\n",
      "Gradient Descent(6/49): loss=3469209118.093962\n",
      "Gradient Descent(7/49): loss=248610670123.66656\n",
      "Gradient Descent(8/49): loss=17817905339128.223\n",
      "Gradient Descent(9/49): loss=1277052517646452.0\n",
      "Gradient Descent(10/49): loss=9.153048286278619e+16\n",
      "Gradient Descent(11/49): loss=6.560309346955306e+18\n",
      "Gradient Descent(12/49): loss=4.7020082671592066e+20\n",
      "Gradient Descent(13/49): loss=3.3700986994970033e+22\n",
      "Gradient Descent(14/49): loss=2.41547142458398e+24\n",
      "Gradient Descent(15/49): loss=1.73125565401889e+26\n",
      "Gradient Descent(16/49): loss=1.2408535022387438e+28\n",
      "Gradient Descent(17/49): loss=8.893645583680412e+29\n",
      "Gradient Descent(18/49): loss=6.374397277908025e+31\n",
      "Gradient Descent(19/49): loss=4.5687609530911345e+33\n",
      "Gradient Descent(20/49): loss=3.274596128734995e+35\n",
      "Gradient Descent(21/49): loss=2.3470214171586514e+37\n",
      "Gradient Descent(22/49): loss=1.682195090972901e+39\n",
      "Gradient Descent(23/49): loss=1.2056900305288046e+41\n",
      "Gradient Descent(24/49): loss=8.641616287667694e+42\n",
      "Gradient Descent(25/49): loss=6.193758774845683e+44\n",
      "Gradient Descent(26/49): loss=4.4392908090272826e+46\n",
      "Gradient Descent(27/49): loss=3.1818001965383296e+48\n",
      "Gradient Descent(28/49): loss=2.280511218166812e+50\n",
      "Gradient Descent(29/49): loss=1.6345248271223648e+52\n",
      "Gradient Descent(30/49): loss=1.1715230292210547e+54\n",
      "Gradient Descent(31/49): loss=8.396729038442023e+55\n",
      "Gradient Descent(32/49): loss=6.0182392310200135e+57\n",
      "Gradient Descent(33/49): loss=4.313489607199319e+59\n",
      "Gradient Descent(34/49): loss=3.0916339276634267e+61\n",
      "Gradient Descent(35/49): loss=2.2158857938887097e+63\n",
      "Gradient Descent(36/49): loss=1.5882054494300279e+65\n",
      "Gradient Descent(37/49): loss=1.1383242568528874e+67\n",
      "Gradient Descent(38/49): loss=8.158781436020798e+68\n",
      "Gradient Descent(39/49): loss=5.847693582915545e+70\n",
      "Gradient Descent(40/49): loss=4.191253376233307e+72\n",
      "Gradient Descent(41/49): loss=3.004022802273537e+74\n",
      "Gradient Descent(42/49): loss=2.1530917333109158e+76\n",
      "Gradient Descent(43/49): loss=1.543198676302754e+78\n",
      "Gradient Descent(44/49): loss=1.1060662756251851e+80\n",
      "Gradient Descent(45/49): loss=7.92757682378516e+81\n",
      "Gradient Descent(46/49): loss=5.681980879626111e+83\n",
      "Gradient Descent(47/49): loss=4.072481091519923e+85\n",
      "Gradient Descent(48/49): loss=2.918894412379408e+87\n",
      "Gradient Descent(49/49): loss=2.0920771390101044e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4494354369591482\n",
      "Gradient Descent(2/49): loss=140.07377997968717\n",
      "Gradient Descent(3/49): loss=10033.458103576711\n",
      "Gradient Descent(4/49): loss=729669.5110472877\n",
      "Gradient Descent(5/49): loss=53241730.5329355\n",
      "Gradient Descent(6/49): loss=3888558333.926408\n",
      "Gradient Descent(7/49): loss=284084137660.66595\n",
      "Gradient Descent(8/49): loss=20755929447037.156\n",
      "Gradient Descent(9/49): loss=1516521578423250.8\n",
      "Gradient Descent(10/49): loss=1.1080476127733787e+17\n",
      "Gradient Descent(11/49): loss=8.09597755154823e+18\n",
      "Gradient Descent(12/49): loss=5.91535056312948e+20\n",
      "Gradient Descent(13/49): loss=4.322069808803542e+22\n",
      "Gradient Descent(14/49): loss=3.1579343466062133e+24\n",
      "Gradient Descent(15/49): loss=2.3073550377678164e+26\n",
      "Gradient Descent(16/49): loss=1.6858765003200366e+28\n",
      "Gradient Descent(17/49): loss=1.2317911779455208e+30\n",
      "Gradient Descent(18/49): loss=9.000122527877742e+31\n",
      "Gradient Descent(19/49): loss=6.575968961457864e+33\n",
      "Gradient Descent(20/49): loss=4.8047532298074044e+35\n",
      "Gradient Descent(21/49): loss=3.5106086623958804e+37\n",
      "Gradient Descent(22/49): loss=2.565037701436267e+39\n",
      "Gradient Descent(23/49): loss=1.8741531861058325e+41\n",
      "Gradient Descent(24/49): loss=1.3693561552820796e+43\n",
      "Gradient Descent(25/49): loss=1.0005245536547744e+45\n",
      "Gradient Descent(26/49): loss=7.310365375762446e+46\n",
      "Gradient Descent(27/49): loss=5.341342372052084e+48\n",
      "Gradient Descent(28/49): loss=3.902669274243131e+50\n",
      "Gradient Descent(29/49): loss=2.851498069813842e+52\n",
      "Gradient Descent(30/49): loss=2.0834563911975314e+54\n",
      "Gradient Descent(31/49): loss=1.5222842266574658e+56\n",
      "Gradient Descent(32/49): loss=1.1122619491921111e+58\n",
      "Gradient Descent(33/49): loss=8.126778310887695e+59\n",
      "Gradient Descent(34/49): loss=5.937857153369832e+61\n",
      "Gradient Descent(35/49): loss=4.338514750253354e+63\n",
      "Gradient Descent(36/49): loss=3.169949992394781e+65\n",
      "Gradient Descent(37/49): loss=2.316134330002431e+67\n",
      "Gradient Descent(38/49): loss=1.6922911236726233e+69\n",
      "Gradient Descent(39/49): loss=1.2364780445433544e+71\n",
      "Gradient Descent(40/49): loss=9.034367274348041e+72\n",
      "Gradient Descent(41/49): loss=6.600989998003069e+74\n",
      "Gradient Descent(42/49): loss=4.8230349321149506e+76\n",
      "Gradient Descent(43/49): loss=3.5239662480079746e+78\n",
      "Gradient Descent(44/49): loss=2.574797465058766e+80\n",
      "Gradient Descent(45/49): loss=1.8812841893195185e+82\n",
      "Gradient Descent(46/49): loss=1.3745664461040715e+84\n",
      "Gradient Descent(47/49): loss=1.0043314696854018e+86\n",
      "Gradient Descent(48/49): loss=7.338180732254442e+87\n",
      "Gradient Descent(49/49): loss=5.361665753249591e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.441651625331343\n",
      "Gradient Descent(2/49): loss=139.097624657147\n",
      "Gradient Descent(3/49): loss=9964.02535722504\n",
      "Gradient Descent(4/49): loss=725589.9615721744\n",
      "Gradient Descent(5/49): loss=53039275.28400064\n",
      "Gradient Descent(6/49): loss=3881357180.660146\n",
      "Gradient Descent(7/49): loss=284128797493.8739\n",
      "Gradient Descent(8/49): loss=20801362546977.906\n",
      "Gradient Descent(9/49): loss=1522938052294863.5\n",
      "Gradient Descent(10/49): loss=1.1150055361185933e+17\n",
      "Gradient Descent(11/49): loss=8.163439108100276e+18\n",
      "Gradient Descent(12/49): loss=5.976813599554032e+20\n",
      "Gradient Descent(13/49): loss=4.3758900008781106e+22\n",
      "Gradient Descent(14/49): loss=3.203783216281531e+24\n",
      "Gradient Descent(15/49): loss=2.345631904602972e+26\n",
      "Gradient Descent(16/49): loss=1.717341252698102e+28\n",
      "Gradient Descent(17/49): loss=1.2573417767739124e+30\n",
      "Gradient Descent(18/49): loss=9.205557384331815e+31\n",
      "Gradient Descent(19/49): loss=6.739797272704206e+33\n",
      "Gradient Descent(20/49): loss=4.934504819704438e+35\n",
      "Gradient Descent(21/49): loss=3.6127700628253542e+37\n",
      "Gradient Descent(22/49): loss=2.6450693643739723e+39\n",
      "Gradient Descent(23/49): loss=1.9365727186347343e+41\n",
      "Gradient Descent(24/49): loss=1.4178508681381893e+43\n",
      "Gradient Descent(25/49): loss=1.038071570944144e+45\n",
      "Gradient Descent(26/49): loss=7.600182858564952e+46\n",
      "Gradient Descent(27/49): loss=5.564431307090856e+48\n",
      "Gradient Descent(28/49): loss=4.073967212044076e+50\n",
      "Gradient Descent(29/49): loss=2.9827322737653495e+52\n",
      "Gradient Descent(30/49): loss=2.183790726312109e+54\n",
      "Gradient Descent(31/49): loss=1.598850147655563e+56\n",
      "Gradient Descent(32/49): loss=1.1705891795663191e+58\n",
      "Gradient Descent(33/49): loss=8.570403107051748e+59\n",
      "Gradient Descent(34/49): loss=6.274772627282857e+61\n",
      "Gradient Descent(35/49): loss=4.5940396306099314e+63\n",
      "Gradient Descent(36/49): loss=3.3635003818064843e+65\n",
      "Gradient Descent(37/49): loss=2.4625679637226678e+67\n",
      "Gradient Descent(38/49): loss=1.8029553404409597e+69\n",
      "Gradient Descent(39/49): loss=1.320023653158619e+71\n",
      "Gradient Descent(40/49): loss=9.664479234810464e+72\n",
      "Gradient Descent(41/49): loss=7.075794335698859e+74\n",
      "Gradient Descent(42/49): loss=5.180503187463243e+76\n",
      "Gradient Descent(43/49): loss=3.792876389851458e+78\n",
      "Gradient Descent(44/49): loss=2.7769332028414236e+80\n",
      "Gradient Descent(45/49): loss=2.033116089329032e+82\n",
      "Gradient Descent(46/49): loss=1.4885345561999897e+84\n",
      "Gradient Descent(47/49): loss=1.0898222372204737e+86\n",
      "Gradient Descent(48/49): loss=7.979072462867642e+87\n",
      "Gradient Descent(49/49): loss=5.841833208511907e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.448242054834409\n",
      "Gradient Descent(2/49): loss=138.5213611226085\n",
      "Gradient Descent(3/49): loss=9833.000081888025\n",
      "Gradient Descent(4/49): loss=709200.3334075661\n",
      "Gradient Descent(5/49): loss=51335646.43393225\n",
      "Gradient Descent(6/49): loss=3719825350.6978974\n",
      "Gradient Descent(7/49): loss=269627337180.55713\n",
      "Gradient Descent(8/49): loss=19545554933500.145\n",
      "Gradient Descent(9/49): loss=1416920309431454.2\n",
      "Gradient Descent(10/49): loss=1.0271811100715696e+17\n",
      "Gradient Descent(11/49): loss=7.446461252723952e+18\n",
      "Gradient Descent(12/49): loss=5.3982532582105154e+20\n",
      "Gradient Descent(13/49): loss=3.913422218085745e+22\n",
      "Gradient Descent(14/49): loss=2.837005630494159e+24\n",
      "Gradient Descent(15/49): loss=2.056665684194621e+26\n",
      "Gradient Descent(16/49): loss=1.490964180709017e+28\n",
      "Gradient Descent(17/49): loss=1.0808631717233561e+30\n",
      "Gradient Descent(18/49): loss=7.8356355717049435e+31\n",
      "Gradient Descent(19/49): loss=5.680384570066115e+33\n",
      "Gradient Descent(20/49): loss=4.1179517055715076e+35\n",
      "Gradient Descent(21/49): loss=2.9852778523584208e+37\n",
      "Gradient Descent(22/49): loss=2.1641545343533205e+39\n",
      "Gradient Descent(23/49): loss=1.5688874135702118e+41\n",
      "Gradient Descent(24/49): loss=1.1373530297348552e+43\n",
      "Gradient Descent(25/49): loss=8.245154515604875e+44\n",
      "Gradient Descent(26/49): loss=5.977262222799347e+46\n",
      "Gradient Descent(27/49): loss=4.3331709081359923e+48\n",
      "Gradient Descent(28/49): loss=3.1412993807594133e+50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=2.2772611578813475e+52\n",
      "Gradient Descent(30/49): loss=1.6508832023331043e+54\n",
      "Gradient Descent(31/49): loss=1.1967952548232087e+56\n",
      "Gradient Descent(32/49): loss=8.676076417417817e+57\n",
      "Gradient Descent(33/49): loss=6.289655786778118e+59\n",
      "Gradient Descent(34/49): loss=4.559638252693666e+61\n",
      "Gradient Descent(35/49): loss=3.3054751643375774e+63\n",
      "Gradient Descent(36/49): loss=2.396279146837539e+65\n",
      "Gradient Descent(37/49): loss=1.7371643906207236e+67\n",
      "Gradient Descent(38/49): loss=1.2593441477898274e+69\n",
      "Gradient Descent(39/49): loss=9.129519872358176e+70\n",
      "Gradient Descent(40/49): loss=6.6183761798600276e+72\n",
      "Gradient Descent(41/49): loss=4.797941608163025e+74\n",
      "Gradient Descent(42/49): loss=3.478231374244577e+76\n",
      "Gradient Descent(43/49): loss=2.5215174507743197e+78\n",
      "Gradient Descent(44/49): loss=1.8279549490695724e+80\n",
      "Gradient Descent(45/49): loss=1.3251620744491969e+82\n",
      "Gradient Descent(46/49): loss=9.606661939082939e+83\n",
      "Gradient Descent(47/49): loss=6.96427670179018e+85\n",
      "Gradient Descent(48/49): loss=5.048699567721782e+87\n",
      "Gradient Descent(49/49): loss=3.66001645491225e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.456901464790927\n",
      "Gradient Descent(2/49): loss=139.92944069511645\n",
      "Gradient Descent(3/49): loss=9985.659924593874\n",
      "Gradient Descent(4/49): loss=724007.7368919775\n",
      "Gradient Descent(5/49): loss=52689695.20631884\n",
      "Gradient Descent(6/49): loss=3838753496.729953\n",
      "Gradient Descent(7/49): loss=279772782412.66473\n",
      "Gradient Descent(8/49): loss=20392413483435.125\n",
      "Gradient Descent(9/49): loss=1486439193526013.5\n",
      "Gradient Descent(10/49): loss=1.0835042025936189e+17\n",
      "Gradient Descent(11/49): loss=7.897972894804387e+18\n",
      "Gradient Descent(12/49): loss=5.7570657352151296e+20\n",
      "Gradient Descent(13/49): loss=4.1964968561533675e+22\n",
      "Gradient Descent(14/49): loss=3.058952045914996e+24\n",
      "Gradient Descent(15/49): loss=2.229761705284494e+26\n",
      "Gradient Descent(16/49): loss=1.6253400676116003e+28\n",
      "Gradient Descent(17/49): loss=1.1847590439197014e+30\n",
      "Gradient Descent(18/49): loss=8.636063442704262e+31\n",
      "Gradient Descent(19/49): loss=6.295085249662524e+33\n",
      "Gradient Descent(20/49): loss=4.588676144959409e+35\n",
      "Gradient Descent(21/49): loss=3.3448234502253347e+37\n",
      "Gradient Descent(22/49): loss=2.4381419738292636e+39\n",
      "Gradient Descent(23/49): loss=1.7772346950552825e+41\n",
      "Gradient Descent(24/49): loss=1.2954795886443448e+43\n",
      "Gradient Descent(25/49): loss=9.443138653911419e+44\n",
      "Gradient Descent(26/49): loss=6.883386540294658e+46\n",
      "Gradient Descent(27/49): loss=5.017506572720513e+48\n",
      "Gradient Descent(28/49): loss=3.6574107904473748e+50\n",
      "Gradient Descent(29/49): loss=2.665996246583538e+52\n",
      "Gradient Descent(30/49): loss=1.9433244975820957e+54\n",
      "Gradient Descent(31/49): loss=1.4165474192779842e+56\n",
      "Gradient Descent(32/49): loss=1.0325638325250168e+58\n",
      "Gradient Descent(33/49): loss=7.526666977249406e+59\n",
      "Gradient Descent(34/49): loss=5.48641294629544e+61\n",
      "Gradient Descent(35/49): loss=3.9992106875809266e+63\n",
      "Gradient Descent(36/49): loss=2.915144426826425e+65\n",
      "Gradient Descent(37/49): loss=2.1249360669211385e+67\n",
      "Gradient Descent(38/49): loss=1.5489295305405762e+69\n",
      "Gradient Descent(39/49): loss=1.1290611176160619e+71\n",
      "Gradient Descent(40/49): loss=8.230064584458754e+72\n",
      "Gradient Descent(41/49): loss=5.999140525481731e+74\n",
      "Gradient Descent(42/49): loss=4.3729531737136656e+76\n",
      "Gradient Descent(43/49): loss=3.1875765167139152e+78\n",
      "Gradient Descent(44/49): loss=2.3235199752384358e+80\n",
      "Gradient Descent(45/49): loss=1.6936832879223347e+82\n",
      "Gradient Descent(46/49): loss=1.2345764660332006e+84\n",
      "Gradient Descent(47/49): loss=8.999197555717427e+85\n",
      "Gradient Descent(48/49): loss=6.559784579973734e+87\n",
      "Gradient Descent(49/49): loss=4.7816234135589866e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.485141188251421\n",
      "Gradient Descent(2/49): loss=144.87420393425882\n",
      "Gradient Descent(3/49): loss=10554.082512747867\n",
      "Gradient Descent(4/49): loss=780562.2965752354\n",
      "Gradient Descent(5/49): loss=57922796.33938956\n",
      "Gradient Descent(6/49): loss=4302332003.195911\n",
      "Gradient Descent(7/49): loss=319654609444.19916\n",
      "Gradient Descent(8/49): loss=23751723477603.67\n",
      "Gradient Descent(9/49): loss=1764901984162472.0\n",
      "Gradient Descent(10/49): loss=1.3114432702014797e+17\n",
      "Gradient Descent(11/49): loss=9.744947036998556e+18\n",
      "Gradient Descent(12/49): loss=7.241187296253504e+20\n",
      "Gradient Descent(13/49): loss=5.380717331208754e+22\n",
      "Gradient Descent(14/49): loss=3.9982560641754624e+24\n",
      "Gradient Descent(15/49): loss=2.970988980145515e+26\n",
      "Gradient Descent(16/49): loss=2.207656397241993e+28\n",
      "Gradient Descent(17/49): loss=1.6404459291551576e+30\n",
      "Gradient Descent(18/49): loss=1.2189681562283436e+32\n",
      "Gradient Descent(19/49): loss=9.057801539017509e+33\n",
      "Gradient Descent(20/49): loss=6.730591632417859e+35\n",
      "Gradient Descent(21/49): loss=5.00130892999054e+37\n",
      "Gradient Descent(22/49): loss=3.716328724040009e+39\n",
      "Gradient Descent(23/49): loss=2.7614969158031503e+41\n",
      "Gradient Descent(24/49): loss=2.0519888799568264e+43\n",
      "Gradient Descent(25/49): loss=1.5247738787506395e+45\n",
      "Gradient Descent(26/49): loss=1.1330155850402506e+47\n",
      "Gradient Descent(27/49): loss=8.419112721133281e+48\n",
      "Gradient Descent(28/49): loss=6.256000354013728e+50\n",
      "Gradient Descent(29/49): loss=4.648653810178662e+52\n",
      "Gradient Descent(30/49): loss=3.454280854224068e+54\n",
      "Gradient Descent(31/49): loss=2.566776685700398e+56\n",
      "Gradient Descent(32/49): loss=1.90729788117796e+58\n",
      "Gradient Descent(33/49): loss=1.4172581618853563e+60\n",
      "Gradient Descent(34/49): loss=1.0531237502293642e+62\n",
      "Gradient Descent(35/49): loss=7.825459490187602e+63\n",
      "Gradient Descent(36/49): loss=5.814873723931319e+65\n",
      "Gradient Descent(37/49): loss=4.320865307355415e+67\n",
      "Gradient Descent(38/49): loss=3.2107106518016114e+69\n",
      "Gradient Descent(39/49): loss=2.3857866784330986e+71\n",
      "Gradient Descent(40/49): loss=1.7728094158204302e+73\n",
      "Gradient Descent(41/49): loss=1.3173236539679674e+75\n",
      "Gradient Descent(42/49): loss=9.788652935941394e+76\n",
      "Gradient Descent(43/49): loss=7.273666271132166e+78\n",
      "Gradient Descent(44/49): loss=5.404852063918606e+80\n",
      "Gradient Descent(45/49): loss=4.0161900125640705e+82\n",
      "Gradient Descent(46/49): loss=2.9843152090503306e+84\n",
      "Gradient Descent(47/49): loss=2.21755874077361e+86\n",
      "Gradient Descent(48/49): loss=1.6478040770855074e+88\n",
      "Gradient Descent(49/49): loss=1.2244357845115682e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4771830111410256\n",
      "Gradient Descent(2/49): loss=143.86490042363477\n",
      "Gradient Descent(3/49): loss=10481.03128760575\n",
      "Gradient Descent(4/49): loss=776194.0614047289\n",
      "Gradient Descent(5/49): loss=57702069.49444146\n",
      "Gradient Descent(6/49): loss=4294319820.439753\n",
      "Gradient Descent(7/49): loss=319700893817.1585\n",
      "Gradient Descent(8/49): loss=23803374267185.824\n",
      "Gradient Descent(9/49): loss=1772340833828721.2\n",
      "Gradient Descent(10/49): loss=1.3196547813654043e+17\n",
      "Gradient Descent(11/49): loss=9.825955464321536e+18\n",
      "Gradient Descent(12/49): loss=7.316269126381067e+20\n",
      "Gradient Descent(13/49): loss=5.447593415027886e+22\n",
      "Gradient Descent(14/49): loss=4.056203555606312e+24\n",
      "Gradient Descent(15/49): loss=3.0201937847311064e+26\n",
      "Gradient Descent(16/49): loss=2.248795075719733e+28\n",
      "Gradient Descent(17/49): loss=1.6744221290659575e+30\n",
      "Gradient Descent(18/49): loss=1.2467518713063741e+32\n",
      "Gradient Descent(19/49): loss=9.283144328193219e+33\n",
      "Gradient Descent(20/49): loss=6.912102609105802e+35\n",
      "Gradient Descent(21/49): loss=5.146657295310676e+37\n",
      "Gradient Descent(22/49): loss=3.832130802095865e+39\n",
      "Gradient Descent(23/49): loss=2.8533523103998794e+41\n",
      "Gradient Descent(24/49): loss=2.1245671997464858e+43\n",
      "Gradient Descent(25/49): loss=1.581923749754978e+45\n",
      "Gradient Descent(26/49): loss=1.177878840611703e+47\n",
      "Gradient Descent(27/49): loss=8.770325139727511e+48\n",
      "Gradient Descent(28/49): loss=6.53026443845373e+50\n",
      "Gradient Descent(29/49): loss=4.862345803231909e+52\n",
      "Gradient Descent(30/49): loss=3.6204363441988256e+54\n",
      "Gradient Descent(31/49): loss=2.6957275053706556e+56\n",
      "Gradient Descent(32/49): loss=2.007201920524287e+58\n",
      "Gradient Descent(33/49): loss=1.4945351641550484e+60\n",
      "Gradient Descent(34/49): loss=1.1128104920866763e+62\n",
      "Gradient Descent(35/49): loss=8.285835094407447e+63\n",
      "Gradient Descent(36/49): loss=6.1695197609950895e+65\n",
      "Gradient Descent(37/49): loss=4.5937402383254845e+67\n",
      "Gradient Descent(38/49): loss=3.420436305370894e+69\n",
      "Gradient Descent(39/49): loss=2.5468102052204696e+71\n",
      "Gradient Descent(40/49): loss=1.8963201306307515e+73\n",
      "Gradient Descent(41/49): loss=1.4119740962496108e+75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=1.0513366473712286e+77\n",
      "Gradient Descent(43/49): loss=7.828109234026465e+78\n",
      "Gradient Descent(44/49): loss=5.828703330476844e+80\n",
      "Gradient Descent(45/49): loss=4.339972974193771e+82\n",
      "Gradient Descent(46/49): loss=3.2314846628489354e+84\n",
      "Gradient Descent(47/49): loss=2.4061193902175772e+86\n",
      "Gradient Descent(48/49): loss=1.7915636693373508e+88\n",
      "Gradient Descent(49/49): loss=1.3339738644470501e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.483944279941596\n",
      "Gradient Descent(2/49): loss=143.27104825605593\n",
      "Gradient Descent(3/49): loss=10343.462367553522\n",
      "Gradient Descent(4/49): loss=758687.3039595829\n",
      "Gradient Descent(5/49): loss=55851087.15920056\n",
      "Gradient Descent(6/49): loss=4115816090.6357846\n",
      "Gradient Descent(7/49): loss=303402362787.2799\n",
      "Gradient Descent(8/49): loss=22367887632470.438\n",
      "Gradient Descent(9/49): loss=1649090298680975.5\n",
      "Gradient Descent(10/49): loss=1.2158167475344072e+17\n",
      "Gradient Descent(11/49): loss=8.963820099658711e+18\n",
      "Gradient Descent(12/49): loss=6.608738369431024e+20\n",
      "Gradient Descent(13/49): loss=4.8724132714602905e+22\n",
      "Gradient Descent(14/49): loss=3.592276167392175e+24\n",
      "Gradient Descent(15/49): loss=2.6484716564508015e+26\n",
      "Gradient Descent(16/49): loss=1.9526344451291035e+28\n",
      "Gradient Descent(17/49): loss=1.4396156659913057e+30\n",
      "Gradient Descent(18/49): loss=1.0613831343914244e+32\n",
      "Gradient Descent(19/49): loss=7.825242423461571e+33\n",
      "Gradient Descent(20/49): loss=5.769303940058955e+35\n",
      "Gradient Descent(21/49): loss=4.2535254695684434e+37\n",
      "Gradient Descent(22/49): loss=3.135989905958542e+39\n",
      "Gradient Descent(23/49): loss=2.312066252021244e+41\n",
      "Gradient Descent(24/49): loss=1.7046133801589508e+43\n",
      "Gradient Descent(25/49): loss=1.2567575748651346e+45\n",
      "Gradient Descent(26/49): loss=9.265676430591943e+46\n",
      "Gradient Descent(27/49): loss=6.831290412205536e+48\n",
      "Gradient Descent(28/49): loss=5.036494533935571e+50\n",
      "Gradient Descent(29/49): loss=3.713248253220333e+52\n",
      "Gradient Descent(30/49): loss=2.7376605885581197e+54\n",
      "Gradient Descent(31/49): loss=2.0183906345729793e+56\n",
      "Gradient Descent(32/49): loss=1.4880956283472452e+58\n",
      "Gradient Descent(33/49): loss=1.0971258789925433e+60\n",
      "Gradient Descent(34/49): loss=8.088762384794028e+61\n",
      "Gradient Descent(35/49): loss=5.9635888798593694e+63\n",
      "Gradient Descent(36/49): loss=4.396765615817764e+65\n",
      "Gradient Descent(37/49): loss=3.24159633903087e+67\n",
      "Gradient Descent(38/49): loss=2.3899265376837472e+69\n",
      "Gradient Descent(39/49): loss=1.7620173081860852e+71\n",
      "Gradient Descent(40/49): loss=1.2990796768825937e+73\n",
      "Gradient Descent(41/49): loss=9.57770391385483e+74\n",
      "Gradient Descent(42/49): loss=7.061338414715284e+76\n",
      "Gradient Descent(43/49): loss=5.206101656056011e+78\n",
      "Gradient Descent(44/49): loss=3.838294224322617e+80\n",
      "Gradient Descent(45/49): loss=2.829853031265891e+82\n",
      "Gradient Descent(46/49): loss=2.086361209054526e+84\n",
      "Gradient Descent(47/49): loss=1.5382081848611952e+86\n",
      "Gradient Descent(48/49): loss=1.1340722832199342e+88\n",
      "Gradient Descent(49/49): loss=8.361156547114177e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4924230517623527\n",
      "Gradient Descent(2/49): loss=144.68940019695097\n",
      "Gradient Descent(3/49): loss=10499.92350907167\n",
      "Gradient Descent(4/49): loss=774122.5316193879\n",
      "Gradient Descent(5/49): loss=57286712.585863166\n",
      "Gradient Descent(6/49): loss=4244067400.273032\n",
      "Gradient Descent(7/49): loss=314530079575.48267\n",
      "Gradient Descent(8/49): loss=23312580803197.37\n",
      "Gradient Descent(9/49): loss=1727961261881518.2\n",
      "Gradient Descent(10/49): loss=1.2808038632861842e+17\n",
      "Gradient Descent(11/49): loss=9.493642191076213e+18\n",
      "Gradient Descent(12/49): loss=7.03693609058992e+20\n",
      "Gradient Descent(13/49): loss=5.2159632245837995e+22\n",
      "Gradient Descent(14/49): loss=3.866210432158628e+24\n",
      "Gradient Descent(15/49): loss=2.865737939065158e+26\n",
      "Gradient Descent(16/49): loss=2.124161161171068e+28\n",
      "Gradient Descent(17/49): loss=1.5744847411923918e+30\n",
      "Gradient Descent(18/49): loss=1.1670499625323e+32\n",
      "Gradient Descent(19/49): loss=8.650484695083975e+33\n",
      "Gradient Descent(20/49): loss=6.411969313455395e+35\n",
      "Gradient Descent(21/49): loss=4.752722180052699e+37\n",
      "Gradient Descent(22/49): loss=3.522844077511328e+39\n",
      "Gradient Descent(23/49): loss=2.611225719567862e+41\n",
      "Gradient Descent(24/49): loss=1.9355099483596214e+43\n",
      "Gradient Descent(25/49): loss=1.434651448217517e+45\n",
      "Gradient Descent(26/49): loss=1.0634018076824254e+47\n",
      "Gradient Descent(27/49): loss=7.882217008091408e+48\n",
      "Gradient Descent(28/49): loss=5.842508872356647e+50\n",
      "Gradient Descent(29/49): loss=4.3306229565267543e+52\n",
      "Gradient Descent(30/49): loss=3.2099729074149784e+54\n",
      "Gradient Descent(31/49): loss=2.3793172875530422e+56\n",
      "Gradient Descent(32/49): loss=1.763613250994007e+58\n",
      "Gradient Descent(33/49): loss=1.3072370445727282e+60\n",
      "Gradient Descent(34/49): loss=9.689588631408148e+61\n",
      "Gradient Descent(35/49): loss=7.182180786240003e+63\n",
      "Gradient Descent(36/49): loss=5.323623407399368e+65\n",
      "Gradient Descent(37/49): loss=3.946011250246935e+67\n",
      "Gradient Descent(38/49): loss=2.92488848205023e+69\n",
      "Gradient Descent(39/49): loss=2.16800512971034e+71\n",
      "Gradient Descent(40/49): loss=1.606983059797094e+73\n",
      "Gradient Descent(41/49): loss=1.1911385813094752e+75\n",
      "Gradient Descent(42/49): loss=8.829035945550563e+76\n",
      "Gradient Descent(43/49): loss=6.544316249258626e+78\n",
      "Gradient Descent(44/49): loss=4.850821248711061e+80\n",
      "Gradient Descent(45/49): loss=3.595557716149847e+82\n",
      "Gradient Descent(46/49): loss=2.665123002336957e+84\n",
      "Gradient Descent(47/49): loss=1.975460047736728e+86\n",
      "Gradient Descent(48/49): loss=1.4642635243409308e+88\n",
      "Gradient Descent(49/49): loss=1.0853510660323282e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5211617861754583\n",
      "Gradient Descent(2/49): loss=149.79734740466773\n",
      "Gradient Descent(3/49): loss=11097.029282391552\n",
      "Gradient Descent(4/49): loss=834534.4102446882\n",
      "Gradient Descent(5/49): loss=62971068.69101845\n",
      "Gradient Descent(6/49): loss=4756114090.583001\n",
      "Gradient Descent(7/49): loss=359324511057.925\n",
      "Gradient Descent(8/49): loss=27149310144369.45\n",
      "Gradient Descent(9/49): loss=2051361297268967.2\n",
      "Gradient Descent(10/49): loss=1.5499903893298454e+17\n",
      "Gradient Descent(11/49): loss=1.1711618636133229e+19\n",
      "Gradient Descent(12/49): loss=8.849223549015378e+20\n",
      "Gradient Descent(13/49): loss=6.686417818107826e+22\n",
      "Gradient Descent(14/49): loss=5.052215729422813e+24\n",
      "Gradient Descent(15/49): loss=3.81742287510239e+26\n",
      "Gradient Descent(16/49): loss=2.884421070854098e+28\n",
      "Gradient Descent(17/49): loss=2.1794506930957946e+30\n",
      "Gradient Descent(18/49): loss=1.6467794437529423e+32\n",
      "Gradient Descent(19/49): loss=1.2442963474598413e+34\n",
      "Gradient Descent(20/49): loss=9.40182612957924e+35\n",
      "Gradient Descent(21/49): loss=7.1039615886429175e+37\n",
      "Gradient Descent(22/49): loss=5.367709374503357e+39\n",
      "Gradient Descent(23/49): loss=4.0558079558372574e+41\n",
      "Gradient Descent(24/49): loss=3.0645433697988237e+43\n",
      "Gradient Descent(25/49): loss=2.3155499884709772e+45\n",
      "Gradient Descent(26/49): loss=1.7496152287967739e+47\n",
      "Gradient Descent(27/49): loss=1.32199842978082e+49\n",
      "Gradient Descent(28/49): loss=9.988938250982543e+50\n",
      "Gradient Descent(29/49): loss=7.547579871065786e+52\n",
      "Gradient Descent(30/49): loss=5.70290460094833e+54\n",
      "Gradient Descent(31/49): loss=4.309079392746482e+56\n",
      "Gradient Descent(32/49): loss=3.255913698767583e+58\n",
      "Gradient Descent(33/49): loss=2.460148223694168e+60\n",
      "Gradient Descent(34/49): loss=1.8588727596915277e+62\n",
      "Gradient Descent(35/49): loss=1.4045527433849329e+64\n",
      "Gradient Descent(36/49): loss=1.0612713531169866e+66\n",
      "Gradient Descent(37/49): loss=8.018900609117904e+67\n",
      "Gradient Descent(38/49): loss=6.0590316312649e+69\n",
      "Gradient Descent(39/49): loss=4.5781667710066695e+71\n",
      "Gradient Descent(40/49): loss=3.459234455056623e+73\n",
      "Gradient Descent(41/49): loss=2.6137761277795765e+75\n",
      "Gradient Descent(42/49): loss=1.9749530524488697e+77\n",
      "Gradient Descent(43/49): loss=1.4922622935922863e+79\n",
      "Gradient Descent(44/49): loss=1.1275441459816538e+81\n",
      "Gradient Descent(45/49): loss=8.519653727073576e+82\n",
      "Gradient Descent(46/49): loss=6.4373975855327966e+84\n",
      "Gradient Descent(47/49): loss=4.8640577424568063e+86\n",
      "Gradient Descent(48/49): loss=3.6752519022787264e+88\n",
      "Gradient Descent(49/49): loss=2.7769975728908553e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.513027508286041\n",
      "Gradient Descent(2/49): loss=148.7540608627836\n",
      "Gradient Descent(3/49): loss=11020.203895183413\n",
      "Gradient Descent(4/49): loss=829859.7797043602\n",
      "Gradient Descent(5/49): loss=62730600.53274537\n",
      "Gradient Descent(6/49): loss=4747208177.835243\n",
      "Gradient Descent(7/49): loss=359372154342.7542\n",
      "Gradient Descent(8/49): loss=27207967365311.094\n",
      "Gradient Descent(9/49): loss=2059974926770445.2\n",
      "Gradient Descent(10/49): loss=1.5596680910077594e+17\n",
      "Gradient Descent(11/49): loss=1.1808747016381876e+19\n",
      "Gradient Descent(12/49): loss=8.940789721617665e+20\n",
      "Gradient Descent(13/49): loss=6.7693672747217394e+22\n",
      "Gradient Descent(14/49): loss=5.125312098987151e+24\n",
      "Gradient Descent(15/49): loss=3.880543607722598e+26\n",
      "Gradient Descent(16/49): loss=2.93808816767592e+28\n",
      "Gradient Descent(17/49): loss=2.2245239273523028e+30\n",
      "Gradient Descent(18/49): loss=1.6842607932490747e+32\n",
      "Gradient Descent(19/49): loss=1.2752096687355182e+34\n",
      "Gradient Descent(20/49): loss=9.655035050990679e+35\n",
      "Gradient Descent(21/49): loss=7.310147038871992e+37\n",
      "Gradient Descent(22/49): loss=5.534754607122397e+39\n",
      "Gradient Descent(23/49): loss=4.190546154304653e+41\n",
      "Gradient Descent(24/49): loss=3.172801382878765e+43\n",
      "Gradient Descent(25/49): loss=2.4022330847876013e+45\n",
      "Gradient Descent(26/49): loss=1.8188102869561671e+47\n",
      "Gradient Descent(27/49): loss=1.377081550032083e+49\n",
      "Gradient Descent(28/49): loss=1.0426340828610478e+51\n",
      "Gradient Descent(29/49): loss=7.89412820699088e+52\n",
      "Gradient Descent(30/49): loss=5.976906104719565e+54\n",
      "Gradient Descent(31/49): loss=4.525313707598282e+56\n",
      "Gradient Descent(32/49): loss=3.426264992854161e+58\n",
      "Gradient Descent(33/49): loss=2.5941387845768412e+60\n",
      "Gradient Descent(34/49): loss=1.9641084526973162e+62\n",
      "Gradient Descent(35/49): loss=1.4870916070075892e+64\n",
      "Gradient Descent(36/49): loss=1.125926343117878e+66\n",
      "Gradient Descent(37/49): loss=8.524761515383418e+67\n",
      "Gradient Descent(38/49): loss=6.454379483912121e+69\n",
      "Gradient Descent(39/49): loss=4.88682462813414e+71\n",
      "Gradient Descent(40/49): loss=3.6999768925367575e+73\n",
      "Gradient Descent(41/49): loss=2.8013751356027105e+75\n",
      "Gradient Descent(42/49): loss=2.1210139626014295e+77\n",
      "Gradient Descent(43/49): loss=1.6058899689570978e+79\n",
      "Gradient Descent(44/49): loss=1.2158725203458953e+81\n",
      "Gradient Descent(45/49): loss=9.205773834507185e+82\n",
      "Gradient Descent(46/49): loss=6.969996481867043e+84\n",
      "Gradient Descent(47/49): loss=5.277215346648776e+86\n",
      "Gradient Descent(48/49): loss=3.995554644447317e+88\n",
      "Gradient Descent(49/49): loss=3.0251668480617233e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5199614346066914\n",
      "Gradient Descent(2/49): loss=148.1422251711559\n",
      "Gradient Descent(3/49): loss=10875.821583240593\n",
      "Gradient Descent(4/49): loss=811169.8583558386\n",
      "Gradient Descent(5/49): loss=60720894.60713046\n",
      "Gradient Descent(6/49): loss=4550112253.563655\n",
      "Gradient Descent(7/49): loss=341071577443.8472\n",
      "Gradient Descent(8/49): loss=25568913104023.9\n",
      "Gradient Descent(9/49): loss=1916869590442062.2\n",
      "Gradient Descent(10/49): loss=1.437067372718567e+17\n",
      "Gradient Descent(11/49): loss=1.0773654184327537e+19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=8.076986470727628e+20\n",
      "Gradient Descent(13/49): loss=6.055302063002831e+22\n",
      "Gradient Descent(14/49): loss=4.5396493778783454e+24\n",
      "Gradient Descent(15/49): loss=3.403367310985317e+26\n",
      "Gradient Descent(16/49): loss=2.5514986290802535e+28\n",
      "Gradient Descent(17/49): loss=1.9128541467667934e+30\n",
      "Gradient Descent(18/49): loss=1.4340634749192354e+32\n",
      "Gradient Descent(19/49): loss=1.0751149292732411e+34\n",
      "Gradient Descent(20/49): loss=8.060118198881986e+35\n",
      "Gradient Descent(21/49): loss=6.042656800030393e+37\n",
      "Gradient Descent(22/49): loss=4.5301694469392023e+39\n",
      "Gradient Descent(23/49): loss=3.3962602704721807e+41\n",
      "Gradient Descent(24/49): loss=2.546170504194218e+43\n",
      "Gradient Descent(25/49): loss=1.908859663317036e+45\n",
      "Gradient Descent(26/49): loss=1.431068818147459e+47\n",
      "Gradient Descent(27/49): loss=1.0728698403712288e+49\n",
      "Gradient Descent(28/49): loss=8.043286806208605e+50\n",
      "Gradient Descent(29/49): loss=6.03003833387133e+52\n",
      "Gradient Descent(30/49): loss=4.520709404504935e+54\n",
      "Gradient Descent(31/49): loss=3.389168092876608e+56\n",
      "Gradient Descent(32/49): loss=2.540853510806619e+58\n",
      "Gradient Descent(33/49): loss=1.90487352248697e+60\n",
      "Gradient Descent(34/49): loss=1.4280804151987484e+62\n",
      "Gradient Descent(35/49): loss=1.0706294398021827e+64\n",
      "Gradient Descent(36/49): loss=8.026490561538911e+65\n",
      "Gradient Descent(37/49): loss=6.017446218028215e+67\n",
      "Gradient Descent(38/49): loss=4.511269116837975e+69\n",
      "Gradient Descent(39/49): loss=3.382090725391638e+71\n",
      "Gradient Descent(40/49): loss=2.5355476205324524e+73\n",
      "Gradient Descent(41/49): loss=1.90089570564176e+75\n",
      "Gradient Descent(42/49): loss=1.425098252727156e+77\n",
      "Gradient Descent(43/49): loss=1.0683937177081155e+79\n",
      "Gradient Descent(44/49): loss=8.009729391315928e+80\n",
      "Gradient Descent(45/49): loss=6.004880397437649e+82\n",
      "Gradient Descent(46/49): loss=4.501848542676777e+84\n",
      "Gradient Descent(47/49): loss=3.375028137088144e+86\n",
      "Gradient Descent(48/49): loss=2.5302528101853366e+88\n",
      "Gradient Descent(49/49): loss=1.8969261953988748e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5282552065266657\n",
      "Gradient Descent(2/49): loss=149.5701525917515\n",
      "Gradient Descent(3/49): loss=11036.073042484475\n",
      "Gradient Descent(4/49): loss=827246.7770975248\n",
      "Gradient Descent(5/49): loss=62241602.6592802\n",
      "Gradient Descent(6/49): loss=4688271146.304006\n",
      "Gradient Descent(7/49): loss=353262102409.74347\n",
      "Gradient Descent(8/49): loss=26621342856704.668\n",
      "Gradient Descent(9/49): loss=2006219603941305.0\n",
      "Gradient Descent(10/49): loss=1.511931120593015e+17\n",
      "Gradient Descent(11/49): loss=1.1394287244439208e+19\n",
      "Gradient Descent(12/49): loss=8.587027304177116e+20\n",
      "Gradient Descent(13/49): loss=6.471406679260566e+22\n",
      "Gradient Descent(14/49): loss=4.877020670604381e+24\n",
      "Gradient Descent(15/49): loss=3.675449985354766e+26\n",
      "Gradient Descent(16/49): loss=2.7699150124194385e+28\n",
      "Gradient Descent(17/49): loss=2.0874802375118277e+30\n",
      "Gradient Descent(18/49): loss=1.5731795843546322e+32\n",
      "Gradient Descent(19/49): loss=1.1855891908552227e+34\n",
      "Gradient Descent(20/49): loss=8.934909552928014e+35\n",
      "Gradient Descent(21/49): loss=6.733581019330394e+37\n",
      "Gradient Descent(22/49): loss=5.074602386959573e+39\n",
      "Gradient Descent(23/49): loss=3.824352794142138e+41\n",
      "Gradient Descent(24/49): loss=2.8821320723904835e+43\n",
      "Gradient Descent(25/49): loss=2.172049946707198e+45\n",
      "Gradient Descent(26/49): loss=1.6369135252981213e+47\n",
      "Gradient Descent(27/49): loss=1.233620752306425e+49\n",
      "Gradient Descent(28/49): loss=9.296887935750513e+50\n",
      "Gradient Descent(29/49): loss=7.006377375567546e+52\n",
      "Gradient Descent(30/49): loss=5.280188840407072e+54\n",
      "Gradient Descent(31/49): loss=3.979288110797917e+56\n",
      "Gradient Descent(32/49): loss=2.9988953704763566e+58\n",
      "Gradient Descent(33/49): loss=2.2600458153961553e+60\n",
      "Gradient Descent(34/49): loss=1.703229508430083e+62\n",
      "Gradient Descent(35/49): loss=1.2835982078878608e+64\n",
      "Gradient Descent(36/49): loss=9.67353108396763e+65\n",
      "Gradient Descent(37/49): loss=7.290225481575527e+67\n",
      "Gradient Descent(38/49): loss=5.4941041808711264e+69\n",
      "Gradient Descent(39/49): loss=4.140500294065256e+71\n",
      "Gradient Descent(40/49): loss=3.1203890790502415e+73\n",
      "Gradient Descent(41/49): loss=2.3516066448810935e+75\n",
      "Gradient Descent(42/49): loss=1.7722321390549522e+77\n",
      "Gradient Descent(43/49): loss=1.3356003911351872e+79\n",
      "Gradient Descent(44/49): loss=1.0065433108281718e+81\n",
      "Gradient Descent(45/49): loss=7.585573074831454e+82\n",
      "Gradient Descent(46/49): loss=5.716685835035122e+84\n",
      "Gradient Descent(47/49): loss=4.308243637507567e+86\n",
      "Gradient Descent(48/49): loss=3.246804840380098e+88\n",
      "Gradient Descent(49/49): loss=2.446876861777079e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5574972307312587\n",
      "Gradient Descent(2/49): loss=154.84528522264984\n",
      "Gradient Descent(3/49): loss=11663.057664698914\n",
      "Gradient Descent(4/49): loss=891744.1964647735\n",
      "Gradient Descent(5/49): loss=68411933.13217588\n",
      "Gradient Descent(6/49): loss=5253391040.442365\n",
      "Gradient Descent(7/49): loss=403526161271.05426\n",
      "Gradient Descent(8/49): loss=30998544730671.773\n",
      "Gradient Descent(9/49): loss=2381345528902796.0\n",
      "Gradient Descent(10/49): loss=1.829393148916998e+17\n",
      "Gradient Descent(11/49): loss=1.4053767425435701e+19\n",
      "Gradient Descent(12/49): loss=1.0796396051992471e+21\n",
      "Gradient Descent(13/49): loss=8.294017647504083e+22\n",
      "Gradient Descent(14/49): loss=6.371638587275232e+24\n",
      "Gradient Descent(15/49): loss=4.8948267184042335e+26\n",
      "Gradient Descent(16/49): loss=3.760308818140425e+28\n",
      "Gradient Descent(17/49): loss=2.888748315454574e+30\n",
      "Gradient Descent(18/49): loss=2.2191972093142568e+32\n",
      "Gradient Descent(19/49): loss=1.7048339685556453e+34\n",
      "Gradient Descent(20/49): loss=1.3096893094122364e+36\n",
      "Gradient Descent(21/49): loss=1.0061308718957743e+38\n",
      "Gradient Descent(22/49): loss=7.72930895986162e+39\n",
      "Gradient Descent(23/49): loss=5.9378177000506975e+41\n",
      "Gradient Descent(24/49): loss=4.561556436952848e+43\n",
      "Gradient Descent(25/49): loss=3.504283590136464e+45\n",
      "Gradient Descent(26/49): loss=2.6920643534344686e+47\n",
      "Gradient Descent(27/49): loss=2.0681004538078654e+49\n",
      "Gradient Descent(28/49): loss=1.5887582633690568e+51\n",
      "Gradient Descent(29/49): loss=1.2205175115047684e+53\n",
      "Gradient Descent(30/49): loss=9.376272213564354e+54\n",
      "Gradient Descent(31/49): loss=7.203049509258518e+56\n",
      "Gradient Descent(32/49): loss=5.533534122203794e+58\n",
      "Gradient Descent(33/49): loss=4.250977289859786e+60\n",
      "Gradient Descent(34/49): loss=3.265690157469698e+62\n",
      "Gradient Descent(35/49): loss=2.5087718605399247e+64\n",
      "Gradient Descent(36/49): loss=1.9272913058946046e+66\n",
      "Gradient Descent(37/49): loss=1.4805857145485985e+68\n",
      "Gradient Descent(38/49): loss=1.137417084496136e+70\n",
      "Gradient Descent(39/49): loss=8.737877256218913e+71\n",
      "Gradient Descent(40/49): loss=6.712621076776831e+73\n",
      "Gradient Descent(41/49): loss=5.156776686044536e+75\n",
      "Gradient Descent(42/49): loss=3.9615443037194583e+77\n",
      "Gradient Descent(43/49): loss=3.043341650376941e+79\n",
      "Gradient Descent(44/49): loss=2.3379590611224805e+81\n",
      "Gradient Descent(45/49): loss=1.7960693209740938e+83\n",
      "Gradient Descent(46/49): loss=1.3797782259692671e+85\n",
      "Gradient Descent(47/49): loss=1.0599746516612056e+87\n",
      "Gradient Descent(48/49): loss=8.142948200063326e+88\n",
      "Gradient Descent(49/49): loss=6.255584063731761e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.549185116766388\n",
      "Gradient Descent(2/49): loss=153.76716690667305\n",
      "Gradient Descent(3/49): loss=11582.29712301245\n",
      "Gradient Descent(4/49): loss=886744.5382004336\n",
      "Gradient Descent(5/49): loss=68150149.8642319\n",
      "Gradient Descent(6/49): loss=5243501121.580154\n",
      "Gradient Descent(7/49): loss=403574822526.4766\n",
      "Gradient Descent(8/49): loss=31065089439563.25\n",
      "Gradient Descent(9/49): loss=2391307524470986.0\n",
      "Gradient Descent(10/49): loss=1.8407834929932147e+17\n",
      "Gradient Descent(11/49): loss=1.4170050103120327e+19\n",
      "Gradient Descent(12/49): loss=1.0907883639620166e+21\n",
      "Gradient Descent(13/49): loss=8.39672139397151e+22\n",
      "Gradient Descent(14/49): loss=6.463667877541324e+24\n",
      "Gradient Descent(15/49): loss=4.975632958952426e+26\n",
      "Gradient Descent(16/49): loss=3.830166408492622e+28\n",
      "Gradient Descent(17/49): loss=2.948403727101826e+30\n",
      "Gradient Descent(18/49): loss=2.269636255722989e+32\n",
      "Gradient Descent(19/49): loss=1.7471314011232953e+34\n",
      "Gradient Descent(20/49): loss=1.3449151269835442e+36\n",
      "Gradient Descent(21/49): loss=1.0352951687997471e+38\n",
      "Gradient Descent(22/49): loss=7.969544434775833e+39\n",
      "Gradient Descent(23/49): loss=6.134833853390727e+41\n",
      "Gradient Descent(24/49): loss=4.722501608060983e+43\n",
      "Gradient Descent(25/49): loss=3.635309768954574e+45\n",
      "Gradient Descent(26/49): loss=2.7984060595553427e+47\n",
      "Gradient Descent(27/49): loss=2.154170338119025e+49\n",
      "Gradient Descent(28/49): loss=1.6582474976377256e+51\n",
      "Gradient Descent(29/49): loss=1.2764936526898932e+53\n",
      "Gradient Descent(30/49): loss=9.826253606164495e+54\n",
      "Gradient Descent(31/49): loss=7.564100278069886e+56\n",
      "Gradient Descent(32/49): loss=5.82272912036411e+58\n",
      "Gradient Descent(33/49): loss=4.482248140923307e+60\n",
      "Gradient Descent(34/49): loss=3.450366311313849e+62\n",
      "Gradient Descent(35/49): loss=2.6560394043238225e+64\n",
      "Gradient Descent(36/49): loss=2.044578656529539e+66\n",
      "Gradient Descent(37/49): loss=1.5738854912810887e+68\n",
      "Gradient Descent(38/49): loss=1.2115530658379087e+70\n",
      "Gradient Descent(39/49): loss=9.326350865249095e+71\n",
      "Gradient Descent(40/49): loss=7.179282766419894e+73\n",
      "Gradient Descent(41/49): loss=5.526502464352346e+75\n",
      "Gradient Descent(42/49): loss=4.2542173754946047e+77\n",
      "Gradient Descent(43/49): loss=3.2748317031794026e+79\n",
      "Gradient Descent(44/49): loss=2.5209155380552323e+81\n",
      "Gradient Descent(45/49): loss=1.9405623635066403e+83\n",
      "Gradient Descent(46/49): loss=1.4938153340764477e+85\n",
      "Gradient Descent(47/49): loss=1.1499162790571667e+87\n",
      "Gradient Descent(48/49): loss=8.851880273796967e+88\n",
      "Gradient Descent(49/49): loss=6.814042535851455e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5562935188296945\n",
      "Gradient Descent(2/49): loss=153.13694705266315\n",
      "Gradient Descent(3/49): loss=11430.822860853425\n",
      "Gradient Descent(4/49): loss=866802.1020836716\n",
      "Gradient Descent(5/49): loss=65969577.16578042\n",
      "Gradient Descent(6/49): loss=5026053303.806321\n",
      "Gradient Descent(7/49): loss=383045831438.78375\n",
      "Gradient Descent(8/49): loss=29195640606461.965\n",
      "Gradient Descent(9/49): loss=2225353207218394.5\n",
      "Gradient Descent(10/49): loss=1.6962278768956918e+17\n",
      "Gradient Descent(11/49): loss=1.292917447316117e+19\n",
      "Gradient Descent(12/49): loss=9.855027101613076e+20\n",
      "Gradient Descent(13/49): loss=7.511816743866585e+22\n",
      "Gradient Descent(14/49): loss=5.725747451467685e+24\n",
      "Gradient Descent(15/49): loss=4.364348333666865e+26\n",
      "Gradient Descent(16/49): loss=3.3266462980152232e+28\n",
      "Gradient Descent(17/49): loss=2.5356765271608146e+30\n",
      "Gradient Descent(18/49): loss=1.9327739953741836e+32\n",
      "Gradient Descent(19/49): loss=1.4732223445379909e+34\n",
      "Gradient Descent(20/49): loss=1.122937333516081e+36\n",
      "Gradient Descent(21/49): loss=8.559388606326752e+37\n",
      "Gradient Descent(22/49): loss=6.524240590115504e+39\n",
      "Gradient Descent(23/49): loss=4.972985482441586e+41\n",
      "Gradient Descent(24/49): loss=3.790569073443779e+43\n",
      "Gradient Descent(25/49): loss=2.8892933533154505e+45\n",
      "Gradient Descent(26/49): loss=2.202312085538488e+47\n",
      "Gradient Descent(27/49): loss=1.6786729241402371e+49\n",
      "Gradient Descent(28/49): loss=1.27953835641444e+51\n",
      "Gradient Descent(29/49): loss=9.75305184227179e+52\n",
      "Gradient Descent(30/49): loss=7.434089002583338e+54\n",
      "Gradient Descent(31/49): loss=5.6665011313481365e+56\n",
      "Gradient Descent(32/49): loss=4.319188949770696e+58\n",
      "Gradient Descent(33/49): loss=3.2922243817470004e+60\n",
      "Gradient Descent(34/49): loss=2.5094390418703045e+62\n",
      "Gradient Descent(35/49): loss=1.912774943219811e+64\n",
      "Gradient Descent(36/49): loss=1.457978425601715e+66\n",
      "Gradient Descent(37/49): loss=1.1113179295112585e+68\n",
      "Gradient Descent(38/49): loss=8.470821781491684e+69\n",
      "Gradient Descent(39/49): loss=6.456732114935848e+71\n",
      "Gradient Descent(40/49): loss=4.9215283569220146e+73\n",
      "Gradient Descent(41/49): loss=3.7513468015744386e+75\n",
      "Gradient Descent(42/49): loss=2.859396879404323e+77\n",
      "Gradient Descent(43/49): loss=2.179524034012443e+79\n",
      "Gradient Descent(44/49): loss=1.6613031402018928e+81\n",
      "Gradient Descent(45/49): loss=1.266298549855265e+83\n",
      "Gradient Descent(46/49): loss=9.652133789205304e+84\n",
      "Gradient Descent(47/49): loss=7.357166024976239e+86\n",
      "Gradient Descent(48/49): loss=5.607867970044081e+88\n",
      "Gradient Descent(49/49): loss=4.274496873209803e+90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5643979290838637\n",
      "Gradient Descent(2/49): loss=154.57372527164793\n",
      "Gradient Descent(3/49): loss=11594.847455269666\n",
      "Gradient Descent(4/49): loss=883534.0707340004\n",
      "Gradient Descent(5/49): loss=67578919.28688204\n",
      "Gradient Descent(6/49): loss=5174727931.898131\n",
      "Gradient Descent(7/49): loss=396384803726.1272\n",
      "Gradient Descent(8/49): loss=30366544425903.133\n",
      "Gradient Descent(9/49): loss=2326427246679598.5\n",
      "Gradient Descent(10/49): loss=1.7823321803673907e+17\n",
      "Gradient Descent(11/49): loss=1.3654929395501255e+19\n",
      "Gradient Descent(12/49): loss=1.0461423858250501e+21\n",
      "Gradient Descent(13/49): loss=8.014792977499675e+22\n",
      "Gradient Descent(14/49): loss=6.140360577630022e+24\n",
      "Gradient Descent(15/49): loss=4.7043048622143415e+26\n",
      "Gradient Descent(16/49): loss=3.604101787016391e+28\n",
      "Gradient Descent(17/49): loss=2.7612049234524164e+30\n",
      "Gradient Descent(18/49): loss=2.1154376569906743e+32\n",
      "Gradient Descent(19/49): loss=1.620696980732121e+34\n",
      "Gradient Descent(20/49): loss=1.2416620717018256e+36\n",
      "Gradient Descent(21/49): loss=9.512726430073811e+37\n",
      "Gradient Descent(22/49): loss=7.28797039042732e+39\n",
      "Gradient Descent(23/49): loss=5.583521485920979e+41\n",
      "Gradient Descent(24/49): loss=4.2776946822862185e+43\n",
      "Gradient Descent(25/49): loss=3.2772636124011996e+45\n",
      "Gradient Descent(26/49): loss=2.5108049037830125e+47\n",
      "Gradient Descent(27/49): loss=1.9235990785136297e+49\n",
      "Gradient Descent(28/49): loss=1.4737239875879695e+51\n",
      "Gradient Descent(29/49): loss=1.1290618798125095e+53\n",
      "Gradient Descent(30/49): loss=8.650064321285651e+54\n",
      "Gradient Descent(31/49): loss=6.6270604029961685e+56\n",
      "Gradient Descent(32/49): loss=5.077179539219009e+58\n",
      "Gradient Descent(33/49): loss=3.88977170961196e+60\n",
      "Gradient Descent(34/49): loss=2.98006478518679e+62\n",
      "Gradient Descent(35/49): loss=2.2831124258437346e+64\n",
      "Gradient Descent(36/49): loss=1.7491573924676644e+66\n",
      "Gradient Descent(37/49): loss=1.3400792483942721e+68\n",
      "Gradient Descent(38/49): loss=1.0266728424269833e+70\n",
      "Gradient Descent(39/49): loss=7.86563277239092e+71\n",
      "Gradient Descent(40/49): loss=6.026085073396745e+73\n",
      "Gradient Descent(41/49): loss=4.616755239232542e+75\n",
      "Gradient Descent(42/49): loss=3.5370275526108095e+77\n",
      "Gradient Descent(43/49): loss=2.7098174496267495e+79\n",
      "Gradient Descent(44/49): loss=2.0760682525306916e+81\n",
      "Gradient Descent(45/49): loss=1.5905349601167792e+83\n",
      "Gradient Descent(46/49): loss=1.2185540895728701e+85\n",
      "Gradient Descent(47/49): loss=9.335689604117535e+86\n",
      "Gradient Descent(48/49): loss=7.152337440759659e+88\n",
      "Gradient Descent(49/49): loss=5.479609223932377e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5941475219188233\n",
      "Gradient Descent(2/49): loss=160.0201096280502\n",
      "Gradient Descent(3/49): loss=12252.946119225562\n",
      "Gradient Descent(4/49): loss=952356.7277676847\n",
      "Gradient Descent(5/49): loss=74272297.73539844\n",
      "Gradient Descent(6/49): loss=5797920393.823636\n",
      "Gradient Descent(7/49): loss=452733311009.90717\n",
      "Gradient Descent(8/49): loss=35354976959608.96\n",
      "Gradient Descent(9/49): loss=2761024514193389.0\n",
      "Gradient Descent(10/49): loss=2.1562222546797594e+17\n",
      "Gradient Descent(11/49): loss=1.6839061319149668e+19\n",
      "Gradient Descent(12/49): loss=1.3150509171882498e+21\n",
      "Gradient Descent(13/49): loss=1.026992712392535e+23\n",
      "Gradient Descent(14/49): loss=8.020329057850558e+24\n",
      "Gradient Descent(15/49): loss=6.263499133418729e+26\n",
      "Gradient Descent(16/49): loss=4.891497765520729e+28\n",
      "Gradient Descent(17/49): loss=3.820029336831988e+30\n",
      "Gradient Descent(18/49): loss=2.9832629683774946e+32\n",
      "Gradient Descent(19/49): loss=2.329787851225914e+34\n",
      "Gradient Descent(20/49): loss=1.8194545668921274e+36\n",
      "Gradient Descent(21/49): loss=1.4209083111603731e+38\n",
      "Gradient Descent(22/49): loss=1.1096624590000554e+40\n",
      "Gradient Descent(23/49): loss=8.665941097287807e+41\n",
      "Gradient Descent(24/49): loss=6.767691787048145e+43\n",
      "Gradient Descent(25/49): loss=5.285248492956114e+45\n",
      "Gradient Descent(26/49): loss=4.127530110894769e+47\n",
      "Gradient Descent(27/49): loss=3.2234065889330533e+49\n",
      "Gradient Descent(28/49): loss=2.5173287071004737e+51\n",
      "Gradient Descent(29/49): loss=1.9659151412511366e+53\n",
      "Gradient Descent(30/49): loss=1.5352871207082298e+55\n",
      "Gradient Descent(31/49): loss=1.1989869214357256e+57\n",
      "Gradient Descent(32/49): loss=9.363523072549201e+58\n",
      "Gradient Descent(33/49): loss=7.312470450067516e+60\n",
      "Gradient Descent(34/49): loss=5.7106949669269e+62\n",
      "Gradient Descent(35/49): loss=4.4597837663717545e+64\n",
      "Gradient Descent(36/49): loss=3.482881043022375e+66\n",
      "Gradient Descent(37/49): loss=2.7199660331768313e+68\n",
      "Gradient Descent(38/49): loss=2.1241653476673656e+70\n",
      "Gradient Descent(39/49): loss=1.6588730775291526e+72\n",
      "Gradient Descent(40/49): loss=1.295501732185303e+74\n",
      "Gradient Descent(41/49): loss=1.0117258281115376e+76\n",
      "Gradient Descent(42/49): loss=7.901102143193144e+77\n",
      "Gradient Descent(43/49): loss=6.170388591709373e+79\n",
      "Gradient Descent(44/49): loss=4.818782833417466e+81\n",
      "Gradient Descent(45/49): loss=3.763242403701854e+83\n",
      "Gradient Descent(46/49): loss=2.9389150494205047e+85\n",
      "Gradient Descent(47/49): loss=2.295154215740651e+87\n",
      "Gradient Descent(48/49): loss=1.7924073290484367e+89\n",
      "Gradient Descent(49/49): loss=1.399785692478972e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5856558365820685\n",
      "Gradient Descent(2/49): loss=158.90629678054975\n",
      "Gradient Descent(3/49): loss=12168.083986964548\n",
      "Gradient Descent(4/49): loss=947012.447049224\n",
      "Gradient Descent(5/49): loss=73987515.09418915\n",
      "Gradient Descent(6/49): loss=5786947999.684097\n",
      "Gradient Descent(7/49): loss=452782562273.53876\n",
      "Gradient Descent(8/49): loss=35430392443402.44\n",
      "Gradient Descent(9/49): loss=2772532378603618.0\n",
      "Gradient Descent(10/49): loss=2.1696105863009558e+17\n",
      "Gradient Descent(11/49): loss=1.6978071752803072e+19\n",
      "Gradient Descent(12/49): loss=1.3286034604400564e+21\n",
      "Gradient Descent(13/49): loss=1.0396868045259148e+23\n",
      "Gradient Descent(14/49): loss=8.135977298211962e+24\n",
      "Gradient Descent(15/49): loss=6.366737403575678e+26\n",
      "Gradient Descent(16/49): loss=4.982234349580389e+28\n",
      "Gradient Descent(17/49): loss=3.898803675529717e+30\n",
      "Gradient Descent(18/49): loss=3.050974532379001e+32\n",
      "Gradient Descent(19/49): loss=2.387513292461267e+34\n",
      "Gradient Descent(20/49): loss=1.8683275332919092e+36\n",
      "Gradient Descent(21/49): loss=1.462043282766873e+38\n",
      "Gradient Descent(22/49): loss=1.1441091150314565e+40\n",
      "Gradient Descent(23/49): loss=8.953125277007053e+41\n",
      "Gradient Descent(24/49): loss=7.006189459792175e+43\n",
      "Gradient Descent(25/49): loss=5.4826319556343867e+45\n",
      "Gradient Descent(26/49): loss=4.290385427549898e+47\n",
      "Gradient Descent(27/49): loss=3.357403390540566e+49\n",
      "Gradient Descent(28/49): loss=2.627306501283406e+51\n",
      "Gradient Descent(29/49): loss=2.0559756004102673e+53\n",
      "Gradient Descent(30/49): loss=1.6088856277017893e+55\n",
      "Gradient Descent(31/49): loss=1.2590193008656574e+57\n",
      "Gradient Descent(32/49): loss=9.852344832096773e+58\n",
      "Gradient Descent(33/49): loss=7.709865815703002e+60\n",
      "Gradient Descent(34/49): loss=6.033287700456499e+62\n",
      "Gradient Descent(35/49): loss=4.721296238689542e+64\n",
      "Gradient Descent(36/49): loss=3.6946088567560775e+66\n",
      "Gradient Descent(37/49): loss=2.8911836737889377e+68\n",
      "Gradient Descent(38/49): loss=2.262470361456066e+70\n",
      "Gradient Descent(39/49): loss=1.770476287229075e+72\n",
      "Gradient Descent(40/49): loss=1.3854706505958855e+74\n",
      "Gradient Descent(41/49): loss=1.0841878750416875e+76\n",
      "Gradient Descent(42/49): loss=8.484216882413585e+77\n",
      "Gradient Descent(43/49): loss=6.639249318764429e+79\n",
      "Gradient Descent(44/49): loss=5.19548617481524e+81\n",
      "Gradient Descent(45/49): loss=4.065682021671659e+83\n",
      "Gradient Descent(46/49): loss=3.181563716110168e+85\n",
      "Gradient Descent(47/49): loss=2.489704710233776e+87\n",
      "Gradient Descent(48/49): loss=1.948296528770699e+89\n",
      "Gradient Descent(49/49): loss=1.524622317023092e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.592940532610605\n",
      "Gradient Descent(2/49): loss=158.2572863382576\n",
      "Gradient Descent(3/49): loss=12009.230191311039\n",
      "Gradient Descent(4/49): loss=925744.6914889058\n",
      "Gradient Descent(5/49): loss=71623113.76721549\n",
      "Gradient Descent(6/49): loss=5547238483.466927\n",
      "Gradient Descent(7/49): loss=429775366461.75714\n",
      "Gradient Descent(8/49): loss=33300451061449.062\n",
      "Gradient Descent(9/49): loss=2580313637523570.0\n",
      "Gradient Descent(10/49): loss=1.999397852154359e+17\n",
      "Gradient Descent(11/49): loss=1.549270724286453e+19\n",
      "Gradient Descent(12/49): loss=1.2004825171048006e+21\n",
      "Gradient Descent(13/49): loss=9.302174914664498e+22\n",
      "Gradient Descent(14/49): loss=7.207973918449625e+24\n",
      "Gradient Descent(15/49): loss=5.585241098274783e+26\n",
      "Gradient Descent(16/49): loss=4.327834532613881e+28\n",
      "Gradient Descent(17/49): loss=3.3535081960877585e+30\n",
      "Gradient Descent(18/49): loss=2.5985321637349166e+32\n",
      "Gradient Descent(19/49): loss=2.0135240510058085e+34\n",
      "Gradient Descent(20/49): loss=1.560218942431815e+36\n",
      "Gradient Descent(21/49): loss=1.208966511814279e+38\n",
      "Gradient Descent(22/49): loss=9.367916174794119e+39\n",
      "Gradient Descent(23/49): loss=7.258915164369936e+41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=5.624714011142328e+43\n",
      "Gradient Descent(25/49): loss=4.35842091975965e+45\n",
      "Gradient Descent(26/49): loss=3.3772086680621277e+47\n",
      "Gradient Descent(27/49): loss=2.6168969444702843e+49\n",
      "Gradient Descent(28/49): loss=2.0277543649405303e+51\n",
      "Gradient Descent(29/49): loss=1.57124558275934e+53\n",
      "Gradient Descent(30/49): loss=1.2175107222186454e+55\n",
      "Gradient Descent(31/49): loss=9.434122679372506e+56\n",
      "Gradient Descent(32/49): loss=7.31021658415152e+58\n",
      "Gradient Descent(33/49): loss=5.664465931108536e+60\n",
      "Gradient Descent(34/49): loss=4.389223481319547e+62\n",
      "Gradient Descent(35/49): loss=3.4010766422239893e+64\n",
      "Gradient Descent(36/49): loss=2.6353915164064634e+66\n",
      "Gradient Descent(37/49): loss=2.0420852498653543e+68\n",
      "Gradient Descent(38/49): loss=1.582350152437261e+70\n",
      "Gradient Descent(39/49): loss=1.2261153177044422e+72\n",
      "Gradient Descent(40/49): loss=9.500797089657282e+73\n",
      "Gradient Descent(41/49): loss=7.361880569915427e+75\n",
      "Gradient Descent(42/49): loss=5.704498792495878e+77\n",
      "Gradient Descent(43/49): loss=4.420243735896507e+79\n",
      "Gradient Descent(44/49): loss=3.4251133001263712e+81\n",
      "Gradient Descent(45/49): loss=2.6540167962758684e+83\n",
      "Gradient Descent(46/49): loss=2.0565174164178884e+85\n",
      "Gradient Descent(47/49): loss=1.5935332021879553e+87\n",
      "Gradient Descent(48/49): loss=1.2347807250271285e+89\n",
      "Gradient Descent(49/49): loss=9.567942712490031e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.6008512194339484\n",
      "Gradient Descent(2/49): loss=159.7021625157222\n",
      "Gradient Descent(3/49): loss=12177.00423475527\n",
      "Gradient Descent(4/49): loss=943144.4881141565\n",
      "Gradient Descent(5/49): loss=73324677.97340336\n",
      "Gradient Descent(6/49): loss=5707060495.579088\n",
      "Gradient Descent(7/49): loss=444353696877.8768\n",
      "Gradient Descent(8/49): loss=34601453193435.402\n",
      "Gradient Descent(9/49): loss=2694484719767614.5\n",
      "Gradient Descent(10/49): loss=2.0982741399268118e+17\n",
      "Gradient Descent(11/49): loss=1.6339937300480254e+19\n",
      "Gradient Descent(12/49): loss=1.2724451725081405e+21\n",
      "Gradient Descent(13/49): loss=9.908957010260322e+22\n",
      "Gradient Descent(14/49): loss=7.716437911881143e+24\n",
      "Gradient Descent(15/49): loss=6.009049839679962e+26\n",
      "Gradient Descent(16/49): loss=4.679449361993175e+28\n",
      "Gradient Descent(17/49): loss=3.6440447344271943e+30\n",
      "Gradient Descent(18/49): loss=2.837740301914167e+32\n",
      "Gradient Descent(19/49): loss=2.2098438992025004e+34\n",
      "Gradient Descent(20/49): loss=1.7208798339552194e+36\n",
      "Gradient Descent(21/49): loss=1.3401070564868742e+38\n",
      "Gradient Descent(22/49): loss=1.0435864767769269e+40\n",
      "Gradient Descent(23/49): loss=8.126759196180786e+41\n",
      "Gradient Descent(24/49): loss=6.328580956394761e+43\n",
      "Gradient Descent(25/49): loss=4.928279029171397e+45\n",
      "Gradient Descent(26/49): loss=3.837816780210874e+47\n",
      "Gradient Descent(27/49): loss=2.9886371188169914e+49\n",
      "Gradient Descent(28/49): loss=2.3273523306342176e+51\n",
      "Gradient Descent(29/49): loss=1.812387605308401e+53\n",
      "Gradient Descent(30/49): loss=1.411367238487878e+55\n",
      "Gradient Descent(31/49): loss=1.0990791793336856e+57\n",
      "Gradient Descent(32/49): loss=8.558899551466242e+58\n",
      "Gradient Descent(33/49): loss=6.665103198160847e+60\n",
      "Gradient Descent(34/49): loss=5.190340227152599e+62\n",
      "Gradient Descent(35/49): loss=4.041892656820712e+64\n",
      "Gradient Descent(36/49): loss=3.1475578737202707e+66\n",
      "Gradient Descent(37/49): loss=2.4511092722119123e+68\n",
      "Gradient Descent(38/49): loss=1.908761301733293e+70\n",
      "Gradient Descent(39/49): loss=1.4864166801126616e+72\n",
      "Gradient Descent(40/49): loss=1.1575227059092293e+74\n",
      "Gradient Descent(41/49): loss=9.014018966699588e+75\n",
      "Gradient Descent(42/49): loss=7.01952000744526e+77\n",
      "Gradient Descent(43/49): loss=5.466336527242236e+79\n",
      "Gradient Descent(44/49): loss=4.256820266538086e+81\n",
      "Gradient Descent(45/49): loss=3.3149292384951866e+83\n",
      "Gradient Descent(46/49): loss=2.581446988168703e+85\n",
      "Gradient Descent(47/49): loss=2.010259668695182e+87\n",
      "Gradient Descent(48/49): loss=1.565456875195903e+89\n",
      "Gradient Descent(49/49): loss=1.2190739665432304e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.6311126597381507\n",
      "Gradient Descent(2/49): loss=165.32393026882104\n",
      "Gradient Descent(3/49): loss=12867.492634158969\n",
      "Gradient Descent(4/49): loss=1016544.0316030242\n",
      "Gradient Descent(5/49): loss=80580670.7124402\n",
      "Gradient Descent(6/49): loss=6393749367.045489\n",
      "Gradient Descent(7/49): loss=507464722274.8621\n",
      "Gradient Descent(8/49): loss=40280446212375.23\n",
      "Gradient Descent(9/49): loss=3197380923710861.0\n",
      "Gradient Descent(10/49): loss=2.538037750496724e+17\n",
      "Gradient Descent(11/49): loss=2.0146651775496958e+19\n",
      "Gradient Descent(12/49): loss=1.5992192998377733e+21\n",
      "Gradient Descent(13/49): loss=1.2694431876537035e+23\n",
      "Gradient Descent(14/49): loss=1.0076705091860982e+25\n",
      "Gradient Descent(15/49): loss=7.99878157710692e+26\n",
      "Gradient Descent(16/49): loss=6.34934798473495e+28\n",
      "Gradient Descent(17/49): loss=5.0400451030976224e+30\n",
      "Gradient Descent(18/49): loss=4.0007343618967725e+32\n",
      "Gradient Descent(19/49): loss=3.175740516271277e+34\n",
      "Gradient Descent(20/49): loss=2.5208691492683107e+36\n",
      "Gradient Descent(21/49): loss=2.001039201816605e+38\n",
      "Gradient Descent(22/49): loss=1.5884037013158635e+40\n",
      "Gradient Descent(23/49): loss=1.2608580162088335e+42\n",
      "Gradient Descent(24/49): loss=1.0008557243489013e+44\n",
      "Gradient Descent(25/49): loss=7.944686618832001e+45\n",
      "Gradient Descent(26/49): loss=6.306408000265243e+47\n",
      "Gradient Descent(27/49): loss=5.005959803567079e+49\n",
      "Gradient Descent(28/49): loss=3.973677813721434e+51\n",
      "Gradient Descent(29/49): loss=3.15426331550055e+53\n",
      "Gradient Descent(30/49): loss=2.503820775090646e+55\n",
      "Gradient Descent(31/49): loss=1.9875063831760927e+57\n",
      "Gradient Descent(32/49): loss=1.5776614933721484e+59\n",
      "Gradient Descent(33/49): loss=1.2523309654441095e+61\n",
      "Gradient Descent(34/49): loss=9.94087041864708e+62\n",
      "Gradient Descent(35/49): loss=7.890957534958612e+64\n",
      "Gradient Descent(36/49): loss=6.263758423177887e+66\n",
      "Gradient Descent(37/49): loss=4.972105021489934e+68\n",
      "Gradient Descent(38/49): loss=3.946804246671936e+70\n",
      "Gradient Descent(39/49): loss=3.132931362917942e+72\n",
      "Gradient Descent(40/49): loss=2.4868876973139464e+74\n",
      "Gradient Descent(41/49): loss=1.9740650855789037e+76\n",
      "Gradient Descent(42/49): loss=1.5669919338580006e+78\n",
      "Gradient Descent(43/49): loss=1.2438615822314507e+80\n",
      "Gradient Descent(44/49): loss=9.873641352716341e+81\n",
      "Gradient Descent(45/49): loss=7.837591815254715e+83\n",
      "Gradient Descent(46/49): loss=6.221397280715286e+85\n",
      "Gradient Descent(47/49): loss=4.938479195759696e+87\n",
      "Gradient Descent(48/49): loss=3.920112422742978e+89\n",
      "Gradient Descent(49/49): loss=3.111743676097388e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.622439667733081\n",
      "Gradient Descent(2/49): loss=164.17354600282607\n",
      "Gradient Descent(3/49): loss=12778.35689474375\n",
      "Gradient Descent(4/49): loss=1010834.5302742934\n",
      "Gradient Descent(5/49): loss=80271087.24595107\n",
      "Gradient Descent(6/49): loss=6381587173.772491\n",
      "Gradient Descent(7/49): loss=507514035459.9966\n",
      "Gradient Descent(8/49): loss=40365828875177.58\n",
      "Gradient Descent(9/49): loss=3210659137021562.0\n",
      "Gradient Descent(10/49): loss=2.5537540532658643e+17\n",
      "Gradient Descent(11/49): loss=2.0312592727235437e+19\n",
      "Gradient Descent(12/49): loss=1.61566791014809e+21\n",
      "Gradient Descent(13/49): loss=1.2851060780302212e+23\n",
      "Gradient Descent(14/49): loss=1.0221765180891753e+25\n",
      "Gradient Descent(15/49): loss=8.130417287208382e+26\n",
      "Gradient Descent(16/49): loss=6.466954068297528e+28\n",
      "Gradient Descent(17/49): loss=5.143831315739186e+30\n",
      "Gradient Descent(18/49): loss=4.091416198606409e+32\n",
      "Gradient Descent(19/49): loss=3.2543226027140037e+34\n",
      "Gradient Descent(20/49): loss=2.5884962784188113e+36\n",
      "Gradient Descent(21/49): loss=2.058896366943633e+38\n",
      "Gradient Descent(22/49): loss=1.6376512824052676e+40\n",
      "Gradient Descent(23/49): loss=1.3025918962365266e+42\n",
      "Gradient Descent(24/49): loss=1.0360848285421224e+44\n",
      "Gradient Descent(25/49): loss=8.241044451734793e+45\n",
      "Gradient Descent(26/49): loss=6.554947219045585e+47\n",
      "Gradient Descent(27/49): loss=5.213821293662613e+49\n",
      "Gradient Descent(28/49): loss=4.147086402658755e+51\n",
      "Gradient Descent(29/49): loss=3.298602821700398e+53\n",
      "Gradient Descent(30/49): loss=2.6237168746602424e+55\n",
      "Gradient Descent(31/49): loss=2.0869109166736165e+57\n",
      "Gradient Descent(32/49): loss=1.6599341248264445e+59\n",
      "Gradient Descent(33/49): loss=1.3203157244273746e+61\n",
      "Gradient Descent(34/49): loss=1.0501824054930207e+63\n",
      "Gradient Descent(35/49): loss=8.353176928839786e+64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=6.644137669754974e+66\n",
      "Gradient Descent(37/49): loss=5.2847635996127e+68\n",
      "Gradient Descent(38/49): loss=4.203514088957976e+70\n",
      "Gradient Descent(39/49): loss=3.3434855434901797e+72\n",
      "Gradient Descent(40/49): loss=2.6594167030136047e+74\n",
      "Gradient Descent(41/49): loss=2.1153066487869095e+76\n",
      "Gradient Descent(42/49): loss=1.6825201606546575e+78\n",
      "Gradient Descent(43/49): loss=1.3382807134052266e+80\n",
      "Gradient Descent(44/49): loss=1.064471802332248e+82\n",
      "Gradient Descent(45/49): loss=8.466835146098118e+83\n",
      "Gradient Descent(46/49): loss=6.734541697970397e+85\n",
      "Gradient Descent(47/49): loss=5.356671188124335e+87\n",
      "Gradient Descent(48/49): loss=4.2607095634033566e+89\n",
      "Gradient Descent(49/49): loss=3.3889789658777694e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.629902475949423\n",
      "Gradient Descent(2/49): loss=163.5053327185432\n",
      "Gradient Descent(3/49): loss=12611.826740136234\n",
      "Gradient Descent(4/49): loss=988165.0546904007\n",
      "Gradient Descent(5/49): loss=77709028.88024752\n",
      "Gradient Descent(6/49): loss=6117544622.773845\n",
      "Gradient Descent(7/49): loss=481753219672.53345\n",
      "Gradient Descent(8/49): loss=37941658771690.26\n",
      "Gradient Descent(9/49): loss=2988284182100565.5\n",
      "Gradient Descent(10/49): loss=2.3535956891755846e+17\n",
      "Gradient Descent(11/49): loss=1.8537160400759427e+19\n",
      "Gradient Descent(12/49): loss=1.4600071854827965e+21\n",
      "Gradient Descent(13/49): loss=1.149918118093144e+23\n",
      "Gradient Descent(14/49): loss=9.05688564108014e+24\n",
      "Gradient Descent(15/49): loss=7.133306175035741e+26\n",
      "Gradient Descent(16/49): loss=5.618273158704314e+28\n",
      "Gradient Descent(17/49): loss=4.425015919923578e+30\n",
      "Gradient Descent(18/49): loss=3.485192933514306e+32\n",
      "Gradient Descent(19/49): loss=2.7449776469708587e+34\n",
      "Gradient Descent(20/49): loss=2.1619756572642036e+36\n",
      "Gradient Descent(21/49): loss=1.7027966504243e+38\n",
      "Gradient Descent(22/49): loss=1.3411420350562457e+40\n",
      "Gradient Descent(23/49): loss=1.0562987411047162e+42\n",
      "Gradient Descent(24/49): loss=8.319529187031524e+43\n",
      "Gradient Descent(25/49): loss=6.552555939004929e+45\n",
      "Gradient Descent(26/49): loss=5.1608676847629385e+47\n",
      "Gradient Descent(27/49): loss=4.0647581657541703e+49\n",
      "Gradient Descent(28/49): loss=3.201449824967609e+51\n",
      "Gradient Descent(29/49): loss=2.5214983435265635e+53\n",
      "Gradient Descent(30/49): loss=1.985960812761305e+55\n",
      "Gradient Descent(31/49): loss=1.5641653542819381e+57\n",
      "Gradient Descent(32/49): loss=1.2319544473458815e+59\n",
      "Gradient Descent(33/49): loss=9.703013534857607e+60\n",
      "Gradient Descent(34/49): loss=7.642203967887197e+62\n",
      "Gradient Descent(35/49): loss=6.019086882336086e+64\n",
      "Gradient Descent(36/49): loss=4.740701380040064e+66\n",
      "Gradient Descent(37/49): loss=3.733830398871265e+68\n",
      "Gradient Descent(38/49): loss=2.9408073468270696e+70\n",
      "Gradient Descent(39/49): loss=2.316213359280194e+72\n",
      "Gradient Descent(40/49): loss=1.8242760211736849e+74\n",
      "Gradient Descent(41/49): loss=1.4368205709958942e+76\n",
      "Gradient Descent(42/49): loss=1.13165624569727e+78\n",
      "Gradient Descent(43/49): loss=8.913053475689067e+79\n",
      "Gradient Descent(44/49): loss=7.020022428413681e+81\n",
      "Gradient Descent(45/49): loss=5.529049615807576e+83\n",
      "Gradient Descent(46/49): loss=4.354742447877044e+85\n",
      "Gradient Descent(47/49): loss=3.429844748204934e+87\n",
      "Gradient Descent(48/49): loss=2.701384786263061e+89\n",
      "Gradient Descent(49/49): loss=2.12764142379119e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.6376150775769194\n",
      "Gradient Descent(2/49): loss=164.95752549000403\n",
      "Gradient Descent(3/49): loss=12783.31973330227\n",
      "Gradient Descent(4/49): loss=1006244.799769663\n",
      "Gradient Descent(5/49): loss=79506429.82349552\n",
      "Gradient Descent(6/49): loss=6289168771.008538\n",
      "Gradient Descent(7/49): loss=497667247238.87164\n",
      "Gradient Descent(8/49): loss=39385322268545.64\n",
      "Gradient Descent(9/49): loss=3117063952511606.5\n",
      "Gradient Descent(10/49): loss=2.466960432605877e+17\n",
      "Gradient Descent(11/49): loss=1.952451821680442e+19\n",
      "Gradient Descent(12/49): loss=1.5452509153645465e+21\n",
      "Gradient Descent(13/49): loss=1.2229758137635876e+23\n",
      "Gradient Descent(14/49): loss=9.679140273064915e+24\n",
      "Gradient Descent(15/49): loss=7.660475321078965e+26\n",
      "Gradient Descent(16/49): loss=6.062819764475535e+28\n",
      "Gradient Descent(17/49): loss=4.798368524066841e+30\n",
      "Gradient Descent(18/49): loss=3.7976290604671165e+32\n",
      "Gradient Descent(19/49): loss=3.0056020948172846e+34\n",
      "Gradient Descent(20/49): loss=2.3787589071761524e+36\n",
      "Gradient Descent(21/49): loss=1.882649053414157e+38\n",
      "Gradient Descent(22/49): loss=1.490007014870236e+40\n",
      "Gradient Descent(23/49): loss=1.1792537224862973e+42\n",
      "Gradient Descent(24/49): loss=9.333106006358058e+43\n",
      "Gradient Descent(25/49): loss=7.38660951964693e+45\n",
      "Gradient Descent(26/49): loss=5.846070982005219e+47\n",
      "Gradient Descent(27/49): loss=4.6268245039544194e+49\n",
      "Gradient Descent(28/49): loss=3.66186196785634e+51\n",
      "Gradient Descent(29/49): loss=2.8981503534816034e+53\n",
      "Gradient Descent(30/49): loss=2.2937171158044772e+55\n",
      "Gradient Descent(31/49): loss=1.815343431376529e+57\n",
      "Gradient Descent(32/49): loss=1.436738537256846e+59\n",
      "Gradient Descent(33/49): loss=1.1370948266652224e+61\n",
      "Gradient Descent(34/49): loss=8.999442913931255e+62\n",
      "Gradient Descent(35/49): loss=7.122534626125047e+64\n",
      "Gradient Descent(36/49): loss=5.637071092680525e+66\n",
      "Gradient Descent(37/49): loss=4.461413270969563e+68\n",
      "Gradient Descent(38/49): loss=3.530948616246485e+70\n",
      "Gradient Descent(39/49): loss=2.7945400646247357e+72\n",
      "Gradient Descent(40/49): loss=2.2117156100375487e+74\n",
      "Gradient Descent(41/49): loss=1.7504440181789298e+76\n",
      "Gradient Descent(42/49): loss=1.3853744337077752e+78\n",
      "Gradient Descent(43/49): loss=1.0964431319362194e+80\n",
      "Gradient Descent(44/49): loss=8.677708439823142e+81\n",
      "Gradient Descent(45/49): loss=6.867900538863323e+83\n",
      "Gradient Descent(46/49): loss=5.435543051349682e+85\n",
      "Gradient Descent(47/49): loss=4.3019155702516685e+87\n",
      "Gradient Descent(48/49): loss=3.4047154808162764e+89\n",
      "Gradient Descent(49/49): loss=2.69463389413565e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.6683926441892423\n",
      "Gradient Descent(2/49): loss=170.7588742010171\n",
      "Gradient Descent(3/49): loss=13507.51505025137\n",
      "Gradient Descent(4/49): loss=1084485.3228182923\n",
      "Gradient Descent(5/49): loss=87367241.29461984\n",
      "Gradient Descent(6/49): loss=7045234534.955017\n",
      "Gradient Descent(7/49): loss=568288022932.6298\n",
      "Gradient Descent(8/49): loss=45843733291358.7\n",
      "Gradient Descent(9/49): loss=3698309351867999.0\n",
      "Gradient Descent(10/49): loss=2.9835280064485037e+17\n",
      "Gradient Descent(11/49): loss=2.406900396579944e+19\n",
      "Gradient Descent(12/49): loss=1.941719365378303e+21\n",
      "Gradient Descent(13/49): loss=1.5664441368264164e+23\n",
      "Gradient Descent(14/49): loss=1.2636982780017282e+25\n",
      "Gradient Descent(15/49): loss=1.0194639804316616e+27\n",
      "Gradient Descent(16/49): loss=8.224327222070707e+28\n",
      "Gradient Descent(17/49): loss=6.634815913363935e+30\n",
      "Gradient Descent(18/49): loss=5.352508605929859e+32\n",
      "Gradient Descent(19/49): loss=4.318032143867463e+34\n",
      "Gradient Descent(20/49): loss=3.4834883919698347e+36\n",
      "Gradient Descent(21/49): loss=2.8102364625663585e+38\n",
      "Gradient Descent(22/49): loss=2.267103571750405e+40\n",
      "Gradient Descent(23/49): loss=1.8289416828502086e+42\n",
      "Gradient Descent(24/49): loss=1.475463106736776e+44\n",
      "Gradient Descent(25/49): loss=1.1903011450584473e+46\n",
      "Gradient Descent(26/49): loss=9.602522824586617e+47\n",
      "Gradient Descent(27/49): loss=7.74664839897978e+49\n",
      "Gradient Descent(28/49): loss=6.249457826204111e+51\n",
      "Gradient Descent(29/49): loss=5.041628470790935e+53\n",
      "Gradient Descent(30/49): loss=4.067235645772599e+55\n",
      "Gradient Descent(31/49): loss=3.2811631983758834e+57\n",
      "Gradient Descent(32/49): loss=2.6470145504272057e+59\n",
      "Gradient Descent(33/49): loss=2.1354274708559215e+61\n",
      "Gradient Descent(34/49): loss=1.72271455121022e+63\n",
      "Gradient Descent(35/49): loss=1.3897664357393973e+65\n",
      "Gradient Descent(36/49): loss=1.1211670236087128e+67\n",
      "Gradient Descent(37/49): loss=9.044796755066613e+68\n",
      "Gradient Descent(38/49): loss=7.29671374717624e+70\n",
      "Gradient Descent(39/49): loss=5.886481802745467e+72\n",
      "Gradient Descent(40/49): loss=4.748804628311304e+74\n",
      "Gradient Descent(41/49): loss=3.831005709956174e+76\n",
      "Gradient Descent(42/49): loss=3.090589295297213e+78\n",
      "Gradient Descent(43/49): loss=2.493272763176072e+80\n",
      "Gradient Descent(44/49): loss=2.0113992762010824e+82\n",
      "Gradient Descent(45/49): loss=1.6226572190796285e+84\n",
      "Gradient Descent(46/49): loss=1.3090471304156857e+86\n",
      "Gradient Descent(47/49): loss=1.0560482950437966e+88\n",
      "Gradient Descent(48/49): loss=8.51946408614612e+89\n",
      "Gradient Descent(49/49): loss=6.872911840847504e+91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.659536610219425\n",
      "Gradient Descent(2/49): loss=169.57102738508004\n",
      "Gradient Descent(3/49): loss=13413.92796754822\n",
      "Gradient Descent(4/49): loss=1078388.956858687\n",
      "Gradient Descent(5/49): loss=87030931.29397038\n",
      "Gradient Descent(6/49): loss=7031765655.369477\n",
      "Gradient Descent(7/49): loss=568336755192.7003\n",
      "Gradient Descent(8/49): loss=45940304598667.44\n",
      "Gradient Descent(9/49): loss=3713612794388838.5\n",
      "Gradient Descent(10/49): loss=3.0019534392939725e+17\n",
      "Gradient Descent(11/49): loss=2.4266811859219382e+19\n",
      "Gradient Descent(12/49): loss=1.961651911326243e+21\n",
      "Gradient Descent(13/49): loss=1.5857375497682494e+23\n",
      "Gradient Descent(14/49): loss=1.2818603647819861e+25\n",
      "Gradient Descent(15/49): loss=1.0362156385609216e+27\n",
      "Gradient Descent(16/49): loss=8.376441693338118e+28\n",
      "Gradient Descent(17/49): loss=6.771252339160505e+30\n",
      "Gradient Descent(18/49): loss=5.473667693860907e+32\n",
      "Gradient Descent(19/49): loss=4.424741027435979e+34\n",
      "Gradient Descent(20/49): loss=3.576821659042348e+36\n",
      "Gradient Descent(21/49): loss=2.8913902761001943e+38\n",
      "Gradient Descent(22/49): loss=2.3373090765297543e+40\n",
      "Gradient Descent(23/49): loss=1.8894072392791307e+42\n",
      "Gradient Descent(24/49): loss=1.5273374632780964e+44\n",
      "Gradient Descent(25/49): loss=1.2346516294832722e+46\n",
      "Gradient Descent(26/49): loss=9.980535951197886e+47\n",
      "Gradient Descent(27/49): loss=8.067951760193772e+49\n",
      "Gradient Descent(28/49): loss=6.521878777161457e+51\n",
      "Gradient Descent(29/49): loss=5.272081942017876e+53\n",
      "Gradient Descent(30/49): loss=4.261785438374603e+55\n",
      "Gradient Descent(31/49): loss=3.4450934796719076e+57\n",
      "Gradient Descent(32/49): loss=2.784905353706513e+59\n",
      "Gradient Descent(33/49): loss=2.2512300101191565e+61\n",
      "Gradient Descent(34/49): loss=1.8198236258607065e+63\n",
      "Gradient Descent(35/49): loss=1.4710882559110582e+65\n",
      "Gradient Descent(36/49): loss=1.1891815371151174e+67\n",
      "Gradient Descent(37/49): loss=9.612970007293468e+68\n",
      "Gradient Descent(38/49): loss=7.770822996907867e+70\n",
      "Gradient Descent(39/49): loss=6.2816892181559344e+72\n",
      "Gradient Descent(40/49): loss=5.077920247211696e+74\n",
      "Gradient Descent(41/49): loss=4.104831223186816e+76\n",
      "Gradient Descent(42/49): loss=3.318216622268066e+78\n",
      "Gradient Descent(43/49): loss=2.6823420875628427e+80\n",
      "Gradient Descent(44/49): loss=2.1683210874258983e+82\n",
      "Gradient Descent(45/49): loss=1.7528026570420337e+84\n",
      "Gradient Descent(46/49): loss=1.4169106099414906e+86\n",
      "Gradient Descent(47/49): loss=1.145386029909821e+88\n",
      "Gradient Descent(48/49): loss=9.258940883834287e+89\n",
      "Gradient Descent(49/49): loss=7.484636974059112e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.6671793488461497\n",
      "Gradient Descent(2/49): loss=168.88319313704582\n",
      "Gradient Descent(3/49): loss=13239.415165664966\n",
      "Gradient Descent(4/49): loss=1054237.6180359677\n",
      "Gradient Descent(5/49): loss=84256470.66353832\n",
      "Gradient Descent(6/49): loss=6741145009.877349\n",
      "Gradient Descent(7/49): loss=539518890277.12616\n",
      "Gradient Descent(8/49): loss=43184126523300.695\n",
      "Gradient Descent(9/49): loss=3456651739707592.5\n",
      "Gradient Descent(10/49): loss=2.7668877105145206e+17\n",
      "Gradient Descent(11/49): loss=2.2147710638060073e+19\n",
      "Gradient Descent(12/49): loss=1.7728279480870938e+21\n",
      "Gradient Descent(13/49): loss=1.4190721552863637e+23\n",
      "Gradient Descent(14/49): loss=1.1359060475296348e+25\n",
      "Gradient Descent(15/49): loss=9.092438222399722e+26\n",
      "Gradient Descent(16/49): loss=7.278104897286314e+28\n",
      "Gradient Descent(17/49): loss=5.82580929028906e+30\n",
      "Gradient Descent(18/49): loss=4.6633092543453786e+32\n",
      "Gradient Descent(19/49): loss=3.732778078570031e+34\n",
      "Gradient Descent(20/49): loss=2.9879279767475915e+36\n",
      "Gradient Descent(21/49): loss=2.3917075718418783e+38\n",
      "Gradient Descent(22/49): loss=1.9144588336065823e+40\n",
      "Gradient Descent(23/49): loss=1.5324417870883948e+42\n",
      "Gradient Descent(24/49): loss=1.2266536054959559e+44\n",
      "Gradient Descent(25/49): loss=9.818833449690645e+45\n",
      "Gradient Descent(26/49): loss=7.859553005087662e+47\n",
      "Gradient Descent(27/49): loss=6.291233450113275e+49\n",
      "Gradient Descent(28/49): loss=5.035861237681542e+51\n",
      "Gradient Descent(29/49): loss=4.030989885572746e+53\n",
      "Gradient Descent(30/49): loss=3.226633676084085e+55\n",
      "Gradient Descent(31/49): loss=2.5827811964754335e+57\n",
      "Gradient Descent(32/49): loss=2.0674050352573196e+59\n",
      "Gradient Descent(33/49): loss=1.6548686298475537e+61\n",
      "Gradient Descent(34/49): loss=1.3246510167818438e+63\n",
      "Gradient Descent(35/49): loss=1.060326049218068e+65\n",
      "Gradient Descent(36/49): loss=8.487453045420101e+66\n",
      "Gradient Descent(37/49): loss=6.79384037120797e+68\n",
      "Gradient Descent(38/49): loss=5.438176416700347e+70\n",
      "Gradient Descent(39/49): loss=4.3530258474261905e+72\n",
      "Gradient Descent(40/49): loss=3.4844095844646726e+74\n",
      "Gradient Descent(41/49): loss=2.7891197015262295e+76\n",
      "Gradient Descent(42/49): loss=2.2325701157881754e+78\n",
      "Gradient Descent(43/49): loss=1.78707615853955e+80\n",
      "Gradient Descent(44/49): loss=1.430477445629067e+82\n",
      "Gradient Descent(45/49): loss=1.1450355446103284e+84\n",
      "Gradient Descent(46/49): loss=9.165516047996775e+85\n",
      "Gradient Descent(47/49): loss=7.336600581659246e+87\n",
      "Gradient Descent(48/49): loss=5.872632573325454e+89\n",
      "Gradient Descent(49/49): loss=4.7007892766438736e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.6746895035127767\n",
      "Gradient Descent(2/49): loss=170.3418922474769\n",
      "Gradient Descent(3/49): loss=13414.58947898567\n",
      "Gradient Descent(4/49): loss=1073008.693342443\n",
      "Gradient Descent(5/49): loss=86153338.58965999\n",
      "Gradient Descent(6/49): loss=6925248582.58412\n",
      "Gradient Descent(7/49): loss=556870523153.7118\n",
      "Gradient Descent(8/49): loss=44784005749141.47\n",
      "Gradient Descent(9/49): loss=3601701596527533.0\n",
      "Gradient Descent(10/49): loss=2.896661362316064e+17\n",
      "Gradient Descent(11/49): loss=2.329643205161602e+19\n",
      "Gradient Descent(12/49): loss=1.8736205663134176e+21\n",
      "Gradient Descent(13/49): loss=1.506863992523459e+23\n",
      "Gradient Descent(14/49): loss=1.2118992680976263e+25\n",
      "Gradient Descent(15/49): loss=9.746731665699152e+26\n",
      "Gradient Descent(16/49): loss=7.838834628369427e+28\n",
      "Gradient Descent(17/49): loss=6.304403436619934e+30\n",
      "Gradient Descent(18/49): loss=5.070333108722812e+32\n",
      "Gradient Descent(19/49): loss=4.0778287912741597e+34\n",
      "Gradient Descent(20/49): loss=3.2796045736929274e+36\n",
      "Gradient Descent(21/49): loss=2.6376306389670612e+38\n",
      "Gradient Descent(22/49): loss=2.121321406713693e+40\n",
      "Gradient Descent(23/49): loss=1.70607834323906e+42\n",
      "Gradient Descent(24/49): loss=1.37211801288684e+44\n",
      "Gradient Descent(25/49): loss=1.1035295352938952e+46\n",
      "Gradient Descent(26/49): loss=8.875165429131441e+47\n",
      "Gradient Descent(27/49): loss=7.137875233532062e+49\n",
      "Gradient Descent(28/49): loss=5.740655005960541e+51\n",
      "Gradient Descent(29/49): loss=4.616936948217395e+53\n",
      "Gradient Descent(30/49): loss=3.713183732811393e+55\n",
      "Gradient Descent(31/49): loss=2.9863378227286764e+57\n",
      "Gradient Descent(32/49): loss=2.4017700801213844e+59\n",
      "Gradient Descent(33/49): loss=1.9316299294282558e+61\n",
      "Gradient Descent(34/49): loss=1.553518471707508e+63\n",
      "Gradient Descent(35/49): loss=1.2494213333352008e+65\n",
      "Gradient Descent(36/49): loss=1.0048504067526947e+67\n",
      "Gradient Descent(37/49): loss=8.081535931963807e+68\n",
      "Gradient Descent(38/49): loss=6.499596614652695e+70\n",
      "Gradient Descent(39/49): loss=5.227317741188232e+72\n",
      "Gradient Descent(40/49): loss=4.2040840974253973e+74\n",
      "Gradient Descent(41/49): loss=3.381145737317964e+76\n",
      "Gradient Descent(42/49): loss=2.719295388021502e+78\n",
      "Gradient Descent(43/49): loss=2.187000496814025e+80\n",
      "Gradient Descent(44/49): loss=1.758900924899073e+82\n",
      "Gradient Descent(45/49): loss=1.4146007136796308e+84\n",
      "Gradient Descent(46/49): loss=1.137696359593269e+86\n",
      "Gradient Descent(47/49): loss=9.149953015822585e+87\n",
      "Gradient Descent(48/49): loss=7.358873875775775e+89\n",
      "Gradient Descent(49/49): loss=5.918393747588748e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7059874752720976\n",
      "Gradient Descent(2/49): loss=176.32708588880345\n",
      "Gradient Descent(3/49): loss=14173.851387417848\n",
      "Gradient Descent(4/49): loss=1156367.2419198179\n",
      "Gradient Descent(5/49): loss=94663963.99174702\n",
      "Gradient Descent(6/49): loss=7757062674.704748\n",
      "Gradient Descent(7/49): loss=635823856127.6686\n",
      "Gradient Descent(8/49): loss=52121273570016.86\n",
      "Gradient Descent(9/49): loss=4272726526333280.5\n",
      "Gradient Descent(10/49): loss=3.502666855420107e+17\n",
      "Gradient Descent(11/49): loss=2.8714000041605267e+19\n",
      "Gradient Descent(12/49): loss=2.353904876189558e+21\n",
      "Gradient Descent(13/49): loss=1.9296752553456073e+23\n",
      "Gradient Descent(14/49): loss=1.581902018600586e+25\n",
      "Gradient Descent(15/49): loss=1.2968057959245036e+27\n",
      "Gradient Descent(16/49): loss=1.0630906748912922e+29\n",
      "Gradient Descent(17/49): loss=8.714965564833395e+30\n",
      "Gradient Descent(18/49): loss=7.14432236521835e+32\n",
      "Gradient Descent(19/49): loss=5.856746270464415e+34\n",
      "Gradient Descent(20/49): loss=4.801221882967503e+36\n",
      "Gradient Descent(21/49): loss=3.935927988925388e+38\n",
      "Gradient Descent(22/49): loss=3.226580547976784e+40\n",
      "Gradient Descent(23/49): loss=2.645074315865414e+42\n",
      "Gradient Descent(24/49): loss=2.168369279001007e+44\n",
      "Gradient Descent(25/49): loss=1.7775777799187786e+46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=1.4572161643596482e+48\n",
      "Gradient Descent(27/49): loss=1.1945912992724906e+50\n",
      "Gradient Descent(28/49): loss=9.792976548023967e+51\n",
      "Gradient Descent(29/49): loss=8.02805024015764e+53\n",
      "Gradient Descent(30/49): loss=6.581205452952929e+55\n",
      "Gradient Descent(31/49): loss=5.395116363039416e+57\n",
      "Gradient Descent(32/49): loss=4.422788618105786e+59\n",
      "Gradient Descent(33/49): loss=3.625697361127169e+61\n",
      "Gradient Descent(34/49): loss=2.972260826725792e+63\n",
      "Gradient Descent(35/49): loss=2.4365890316179673e+65\n",
      "Gradient Descent(36/49): loss=1.9974579806783168e+67\n",
      "Gradient Descent(37/49): loss=1.6374687453657884e+69\n",
      "Gradient Descent(38/49): loss=1.342358096133398e+71\n",
      "Gradient Descent(39/49): loss=1.100433375204581e+73\n",
      "Gradient Descent(40/49): loss=9.021092186594917e+74\n",
      "Gradient Descent(41/49): loss=7.395277721735271e+76\n",
      "Gradient Descent(42/49): loss=6.062473528744336e+78\n",
      "Gradient Descent(43/49): loss=4.96987221706417e+80\n",
      "Gradient Descent(44/49): loss=4.074183538589754e+82\n",
      "Gradient Descent(45/49): loss=3.339919173197827e+84\n",
      "Gradient Descent(46/49): loss=2.7379866365460083e+86\n",
      "Gradient Descent(47/49): loss=2.2445365989880593e+88\n",
      "Gradient Descent(48/49): loss=1.8400179449203912e+90\n",
      "Gradient Descent(49/49): loss=1.508403132813918e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.6969466640411017\n",
      "Gradient Descent(2/49): loss=175.10087103205663\n",
      "Gradient Descent(3/49): loss=14075.629364377732\n",
      "Gradient Descent(4/49): loss=1149861.277582668\n",
      "Gradient Descent(5/49): loss=94298870.06065215\n",
      "Gradient Descent(6/49): loss=7742159899.810943\n",
      "Gradient Descent(7/49): loss=635871233540.2062\n",
      "Gradient Descent(8/49): loss=52230393143405.85\n",
      "Gradient Descent(9/49): loss=4290344303265484.0\n",
      "Gradient Descent(10/49): loss=3.524241182515402e+17\n",
      "Gradient Descent(11/49): loss=2.894946557182539e+19\n",
      "Gradient Descent(12/49): loss=2.378022374167616e+21\n",
      "Gradient Descent(13/49): loss=1.9534012702916318e+23\n",
      "Gradient Descent(14/49): loss=1.6046009327052996e+25\n",
      "Gradient Descent(15/49): loss=1.3180826062682992e+27\n",
      "Gradient Descent(16/49): loss=1.0827251433057702e+29\n",
      "Gradient Descent(17/49): loss=8.893932247965279e+30\n",
      "Gradient Descent(18/49): loss=7.305827467007474e+32\n",
      "Gradient Descent(19/49): loss=6.001295434531635e+34\n",
      "Gradient Descent(20/49): loss=4.9297012636549506e+36\n",
      "Gradient Descent(21/49): loss=4.049451458399344e+38\n",
      "Gradient Descent(22/49): loss=3.3263794775949228e+40\n",
      "Gradient Descent(23/49): loss=2.7324195740261958e+42\n",
      "Gradient Descent(24/49): loss=2.2445174336895273e+44\n",
      "Gradient Descent(25/49): loss=1.8437353318745694e+46\n",
      "Gradient Descent(26/49): loss=1.5145170730151635e+48\n",
      "Gradient Descent(27/49): loss=1.2440841832338063e+50\n",
      "Gradient Descent(28/49): loss=1.0219399190338815e+52\n",
      "Gradient Descent(29/49): loss=8.394618404361677e+53\n",
      "Gradient Descent(30/49): loss=6.895671344502143e+55\n",
      "Gradient Descent(31/49): loss=5.6643769854603386e+57\n",
      "Gradient Descent(32/49): loss=4.65294313352012e+59\n",
      "Gradient Descent(33/49): loss=3.8221113918343113e+61\n",
      "Gradient Descent(34/49): loss=3.1396333615918758e+63\n",
      "Gradient Descent(35/49): loss=2.579018933430383e+65\n",
      "Gradient Descent(36/49): loss=2.1185080845299687e+67\n",
      "Gradient Descent(37/49): loss=1.740226272107746e+69\n",
      "Gradient Descent(38/49): loss=1.429490640252113e+71\n",
      "Gradient Descent(39/49): loss=1.1742401107951307e+73\n",
      "Gradient Descent(40/49): loss=9.645672374300954e+74\n",
      "Gradient Descent(41/49): loss=7.923336521808288e+76\n",
      "Gradient Descent(42/49): loss=6.508541779325245e+78\n",
      "Gradient Descent(43/49): loss=5.346373459795254e+80\n",
      "Gradient Descent(44/49): loss=4.39172246883333e+82\n",
      "Gradient Descent(45/49): loss=3.6075344134291264e+84\n",
      "Gradient Descent(46/49): loss=2.9633713506338044e+86\n",
      "Gradient Descent(47/49): loss=2.4342303510861982e+88\n",
      "Gradient Descent(48/49): loss=1.999573020398528e+90\n",
      "Gradient Descent(49/49): loss=1.6425283096653348e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7047711513007835\n",
      "Gradient Descent(2/49): loss=174.39299179021947\n",
      "Gradient Descent(3/49): loss=13892.8179398741\n",
      "Gradient Descent(4/49): loss=1124144.0381911017\n",
      "Gradient Descent(5/49): loss=91296292.3846935\n",
      "Gradient Descent(6/49): loss=7422529373.6596155\n",
      "Gradient Descent(7/49): loss=603662286453.7607\n",
      "Gradient Descent(8/49): loss=49099938646164.805\n",
      "Gradient Descent(9/49): loss=3993760010921727.0\n",
      "Gradient Descent(10/49): loss=3.248534172775394e+17\n",
      "Gradient Descent(11/49): loss=2.6423742447613792e+19\n",
      "Gradient Descent(12/49): loss=2.1493228860768715e+21\n",
      "Gradient Descent(13/49): loss=1.748272556871241e+23\n",
      "Gradient Descent(14/49): loss=1.4220558805921897e+25\n",
      "Gradient Descent(15/49): loss=1.1567092247232726e+27\n",
      "Gradient Descent(16/49): loss=9.408745906063636e+28\n",
      "Gradient Descent(17/49): loss=7.653133382574504e+30\n",
      "Gradient Descent(18/49): loss=6.2251070671292366e+32\n",
      "Gradient Descent(19/49): loss=5.063541437587929e+34\n",
      "Gradient Descent(20/49): loss=4.11871661264209e+36\n",
      "Gradient Descent(21/49): loss=3.350190127781256e+38\n",
      "Gradient Descent(22/49): loss=2.7250658270468907e+40\n",
      "Gradient Descent(23/49): loss=2.2165857693221723e+42\n",
      "Gradient Descent(24/49): loss=1.8029848761823116e+44\n",
      "Gradient Descent(25/49): loss=1.4665592952612492e+46\n",
      "Gradient Descent(26/49): loss=1.1929086011367711e+48\n",
      "Gradient Descent(27/49): loss=9.7031939674327e+49\n",
      "Gradient Descent(28/49): loss=7.892639308653015e+51\n",
      "Gradient Descent(29/49): loss=6.419922704376963e+53\n",
      "Gradient Descent(30/49): loss=5.222005709166132e+55\n",
      "Gradient Descent(31/49): loss=4.247612453024087e+57\n",
      "Gradient Descent(32/49): loss=3.4550348191722573e+59\n",
      "Gradient Descent(33/49): loss=2.810347161778842e+61\n",
      "Gradient Descent(34/49): loss=2.2859541460744337e+63\n",
      "Gradient Descent(35/49): loss=1.8594095523228146e+65\n",
      "Gradient Descent(36/49): loss=1.512455483504159e+67\n",
      "Gradient Descent(37/49): loss=1.2302408507712496e+69\n",
      "Gradient Descent(38/49): loss=1.0006856845794887e+71\n",
      "Gradient Descent(39/49): loss=8.139640613417713e+72\n",
      "Gradient Descent(40/49): loss=6.620835127009979e+74\n",
      "Gradient Descent(41/49): loss=5.385429143737472e+76\n",
      "Gradient Descent(42/49): loss=4.380542107731796e+78\n",
      "Gradient Descent(43/49): loss=3.5631606405825137e+80\n",
      "Gradient Descent(44/49): loss=2.898297388395665e+82\n",
      "Gradient Descent(45/49): loss=2.357493416352941e+84\n",
      "Gradient Descent(46/49): loss=1.9176000469792834e+86\n",
      "Gradient Descent(47/49): loss=1.5597879996897676e+88\n",
      "Gradient Descent(48/49): loss=1.2687414186335284e+90\n",
      "Gradient Descent(49/49): loss=1.0320022898473213e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7120744972415203\n",
      "Gradient Descent(2/49): loss=175.85735772807192\n",
      "Gradient Descent(3/49): loss=14071.628488811775\n",
      "Gradient Descent(4/49): loss=1143617.0012326338\n",
      "Gradient Descent(5/49): loss=93296260.91522671\n",
      "Gradient Descent(6/49): loss=7619811435.218482\n",
      "Gradient Descent(7/49): loss=622559123858.3738\n",
      "Gradient Descent(8/49): loss=50870631827629.945\n",
      "Gradient Descent(9/49): loss=4156902743401878.5\n",
      "Gradient Descent(10/49): loss=3.3968615533975066e+17\n",
      "Gradient Descent(11/49): loss=2.7757959223030874e+19\n",
      "Gradient Descent(12/49): loss=2.268285797413213e+21\n",
      "Gradient Descent(13/49): loss=1.8535665917776488e+23\n",
      "Gradient Descent(14/49): loss=1.5146722585267574e+25\n",
      "Gradient Descent(15/49): loss=1.2377392649605572e+27\n",
      "Gradient Descent(16/49): loss=1.011438944044476e+29\n",
      "Gradient Descent(17/49): loss=8.265139284173642e+30\n",
      "Gradient Descent(18/49): loss=6.75399418730724e+32\n",
      "Gradient Descent(19/49): loss=5.51913717796382e+34\n",
      "Gradient Descent(20/49): loss=4.510053510444497e+36\n",
      "Gradient Descent(21/49): loss=3.6854642333095455e+38\n",
      "Gradient Descent(22/49): loss=3.01163757453762e+40\n",
      "Gradient Descent(23/49): loss=2.4610090632384342e+42\n",
      "Gradient Descent(24/49): loss=2.011053939743129e+44\n",
      "Gradient Descent(25/49): loss=1.6433657270797538e+46\n",
      "Gradient Descent(26/49): loss=1.3429032705536546e+48\n",
      "Gradient Descent(27/49): loss=1.0973754437902373e+50\n",
      "Gradient Descent(28/49): loss=8.9673835118248e+51\n",
      "Gradient Descent(29/49): loss=7.327844586206999e+53\n",
      "Gradient Descent(30/49): loss=5.988068449263434e+55\n",
      "Gradient Descent(31/49): loss=4.893248394017058e+57\n",
      "Gradient Descent(32/49): loss=3.9985982205156694e+59\n",
      "Gradient Descent(33/49): loss=3.26752015055283e+61\n",
      "Gradient Descent(34/49): loss=2.6701077091190958e+63\n",
      "Gradient Descent(35/49): loss=2.181922329412716e+65\n",
      "Gradient Descent(36/49): loss=1.7829936355490517e+67\n",
      "Gradient Descent(37/49): loss=1.4570025071717787e+69\n",
      "Gradient Descent(38/49): loss=1.1906135072945148e+71\n",
      "Gradient Descent(39/49): loss=9.729293647571052e+72\n",
      "Gradient Descent(40/49): loss=7.950451956131838e+74\n",
      "Gradient Descent(41/49): loss=6.496842278220383e+76\n",
      "Gradient Descent(42/49): loss=5.309001277030279e+78\n",
      "Gradient Descent(43/49): loss=4.338337511131592e+80\n",
      "Gradient Descent(44/49): loss=3.5451436868027084e+82\n",
      "Gradient Descent(45/49): loss=2.8969723374055523e+84\n",
      "Gradient Descent(46/49): loss=2.367308483132868e+86\n",
      "Gradient Descent(47/49): loss=1.9344849731398538e+88\n",
      "Gradient Descent(48/49): loss=1.5807961395683667e+90\n",
      "Gradient Descent(49/49): loss=1.2917735053884997e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.743897152986716\n",
      "Gradient Descent(2/49): loss=182.03072720444698\n",
      "Gradient Descent(3/49): loss=14867.360173998017\n",
      "Gradient Descent(4/49): loss=1232384.099209143\n",
      "Gradient Descent(5/49): loss=102504646.34160924\n",
      "Gradient Descent(6/49): loss=8534272828.210514\n",
      "Gradient Descent(7/49): loss=710750344161.558\n",
      "Gradient Descent(8/49): loss=59197936741592.695\n",
      "Gradient Descent(9/49): loss=4930693780861944.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=4.10689107125264e+17\n",
      "Gradient Descent(11/49): loss=3.42073535261401e+19\n",
      "Gradient Descent(12/49): loss=2.8492208910344745e+21\n",
      "Gradient Descent(13/49): loss=2.3731919842285622e+23\n",
      "Gradient Descent(14/49): loss=1.9766949791647364e+25\n",
      "Gradient Descent(15/49): loss=1.646442074663369e+27\n",
      "Gradient Descent(16/49): loss=1.3713656147381038e+29\n",
      "Gradient Descent(17/49): loss=1.1422470808544048e+31\n",
      "Gradient Descent(18/49): loss=9.514081296913406e+32\n",
      "Gradient Descent(19/49): loss=7.924532656831578e+34\n",
      "Gradient Descent(20/49): loss=6.600555100763644e+36\n",
      "Gradient Descent(21/49): loss=5.497778799809101e+38\n",
      "Gradient Descent(22/49): loss=4.579246937629247e+40\n",
      "Gradient Descent(23/49): loss=3.8141771939890955e+42\n",
      "Gradient Descent(24/49): loss=3.1769301514643914e+44\n",
      "Gradient Descent(25/49): loss=2.6461500538546866e+46\n",
      "Gradient Descent(26/49): loss=2.2040491209062058e+48\n",
      "Gradient Descent(27/49): loss=1.835811434915056e+50\n",
      "Gradient Descent(28/49): loss=1.5290964219432788e+52\n",
      "Gradient Descent(29/49): loss=1.2736252880503047e+54\n",
      "Gradient Descent(30/49): loss=1.0608365509741572e+56\n",
      "Gradient Descent(31/49): loss=8.835991232597851e+57\n",
      "Gradient Descent(32/49): loss=7.359733315264348e+59\n",
      "Gradient Descent(33/49): loss=6.130118630265608e+61\n",
      "Gradient Descent(34/49): loss=5.1059396871338896e+63\n",
      "Gradient Descent(35/49): loss=4.252873665434962e+65\n",
      "Gradient Descent(36/49): loss=3.542332170457521e+67\n",
      "Gradient Descent(37/49): loss=2.950503163976507e+69\n",
      "Gradient Descent(38/49): loss=2.4575529627734063e+71\n",
      "Gradient Descent(39/49): loss=2.0469615618702126e+73\n",
      "Gradient Descent(40/49): loss=1.7049690074819635e+75\n",
      "Gradient Descent(41/49): loss=1.42011426624842e+77\n",
      "Gradient Descent(42/49): loss=1.1828511370894355e+79\n",
      "Gradient Descent(43/49): loss=9.852283339212786e+80\n",
      "Gradient Descent(44/49): loss=8.206230179985107e+82\n",
      "Gradient Descent(45/49): loss=6.835188498778911e+84\n",
      "Gradient Descent(46/49): loss=5.693211229656722e+86\n",
      "Gradient Descent(47/49): loss=4.742027833070025e+88\n",
      "Gradient Descent(48/49): loss=3.9497617535203486e+90\n",
      "Gradient Descent(49/49): loss=3.289862155758824e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.734669829198111\n",
      "Gradient Descent(2/49): loss=180.76522434166648\n",
      "Gradient Descent(3/49): loss=14764.31360898789\n",
      "Gradient Descent(4/49): loss=1225444.6677014392\n",
      "Gradient Descent(5/49): loss=102108571.58842456\n",
      "Gradient Descent(6/49): loss=8517797815.8000145\n",
      "Gradient Descent(7/49): loss=710795443571.3503\n",
      "Gradient Descent(8/49): loss=59321116704166.55\n",
      "Gradient Descent(9/49): loss=4950953491980402.0\n",
      "Gradient Descent(10/49): loss=4.132121098371281e+17\n",
      "Gradient Descent(11/49): loss=3.4487260891743744e+19\n",
      "Gradient Descent(12/49): loss=2.878358113435616e+21\n",
      "Gradient Descent(13/49): loss=2.4023213178393884e+23\n",
      "Gradient Descent(14/49): loss=2.0050140028140164e+25\n",
      "Gradient Descent(15/49): loss=1.6734153150367973e+27\n",
      "Gradient Descent(16/49): loss=1.396657999362182e+29\n",
      "Gradient Descent(17/49): loss=1.1656721174190517e+31\n",
      "Gradient Descent(18/49): loss=9.728877702031114e+32\n",
      "Gradient Descent(19/49): loss=8.119870069597139e+34\n",
      "Gradient Descent(20/49): loss=6.776967701027333e+36\n",
      "Gradient Descent(21/49): loss=5.656160853388046e+38\n",
      "Gradient Descent(22/49): loss=4.720718322868104e+40\n",
      "Gradient Descent(23/49): loss=3.9399836853270116e+42\n",
      "Gradient Descent(24/49): loss=3.2883706205176136e+44\n",
      "Gradient Descent(25/49): loss=2.744524394392075e+46\n",
      "Gradient Descent(26/49): loss=2.2906220194329215e+48\n",
      "Gradient Descent(27/49): loss=1.9117881577705358e+50\n",
      "Gradient Descent(28/49): loss=1.5956076249963443e+52\n",
      "Gradient Descent(29/49): loss=1.3317185183924933e+54\n",
      "Gradient Descent(30/49): loss=1.1114726355319075e+56\n",
      "Gradient Descent(31/49): loss=9.276520544502561e+57\n",
      "Gradient Descent(32/49): loss=7.742325871243373e+59\n",
      "Gradient Descent(33/49): loss=6.461863541287394e+61\n",
      "Gradient Descent(34/49): loss=5.393170104258825e+63\n",
      "Gradient Descent(35/49): loss=4.501222222912535e+65\n",
      "Gradient Descent(36/49): loss=3.7567888845267726e+67\n",
      "Gradient Descent(37/49): loss=3.1354734389833553e+69\n",
      "Gradient Descent(38/49): loss=2.616914069103601e+71\n",
      "Gradient Descent(39/49): loss=2.1841164909669357e+73\n",
      "Gradient Descent(40/49): loss=1.82289701539485e+75\n",
      "Gradient Descent(41/49): loss=1.5214177185505006e+77\n",
      "Gradient Descent(42/49): loss=1.2697984882146722e+79\n",
      "Gradient Descent(43/49): loss=1.0597932316763324e+81\n",
      "Gradient Descent(44/49): loss=8.845196338878317e+82\n",
      "Gradient Descent(45/49): loss=7.3823360948959605e+84\n",
      "Gradient Descent(46/49): loss=6.161410570216367e+86\n",
      "Gradient Descent(47/49): loss=5.142407461104482e+88\n",
      "Gradient Descent(48/49): loss=4.291931887131889e+90\n",
      "Gradient Descent(49/49): loss=3.5821119705327635e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7426778833133247\n",
      "Gradient Descent(2/49): loss=180.03687012743592\n",
      "Gradient Descent(3/49): loss=14572.877671827338\n",
      "Gradient Descent(4/49): loss=1198073.439953501\n",
      "Gradient Descent(5/49): loss=98861137.21367316\n",
      "Gradient Descent(6/49): loss=8166525035.3670845\n",
      "Gradient Descent(7/49): loss=674827971524.1201\n",
      "Gradient Descent(8/49): loss=55769136955271.03\n",
      "Gradient Descent(9/49): loss=4609024190598874.0\n",
      "Gradient Descent(10/49): loss=3.809154176894056e+17\n",
      "Gradient Descent(11/49): loss=3.148107409069534e+19\n",
      "Gradient Descent(12/49): loss=2.6017824758657256e+21\n",
      "Gradient Descent(13/49): loss=2.1502678943485543e+23\n",
      "Gradient Descent(14/49): loss=1.777109556787094e+25\n",
      "Gradient Descent(15/49): loss=1.4687093130437498e+27\n",
      "Gradient Descent(16/49): loss=1.2138289734443225e+29\n",
      "Gradient Descent(17/49): loss=1.0031806625232527e+31\n",
      "Gradient Descent(18/49): loss=8.290883351349405e+32\n",
      "Gradient Descent(19/49): loss=6.852080521169665e+34\n",
      "Gradient Descent(20/49): loss=5.662968043807175e+36\n",
      "Gradient Descent(21/49): loss=4.680214566538379e+38\n",
      "Gradient Descent(22/49): loss=3.86800847534859e+40\n",
      "Gradient Descent(23/49): loss=3.196752916498399e+42\n",
      "Gradient Descent(24/49): loss=2.64198728474376e+44\n",
      "Gradient Descent(25/49): loss=2.1834958769337178e+46\n",
      "Gradient Descent(26/49): loss=1.804571230194041e+48\n",
      "Gradient Descent(27/49): loss=1.4914053006673055e+50\n",
      "Gradient Descent(28/49): loss=1.2325862973108548e+52\n",
      "Gradient Descent(29/49): loss=1.0186828353357187e+54\n",
      "Gradient Descent(30/49): loss=8.419002558048942e+55\n",
      "Gradient Descent(31/49): loss=6.957965876501256e+57\n",
      "Gradient Descent(32/49): loss=5.750478017407223e+59\n",
      "Gradient Descent(33/49): loss=4.7525380284434036e+61\n",
      "Gradient Descent(34/49): loss=3.927780898114719e+63\n",
      "Gradient Descent(35/49): loss=3.246152411882514e+65\n",
      "Gradient Descent(36/49): loss=2.682813974228694e+67\n",
      "Gradient Descent(37/49): loss=2.217237488286254e+69\n",
      "Gradient Descent(38/49): loss=1.83245731037886e+71\n",
      "Gradient Descent(39/49): loss=1.5144520206341616e+73\n",
      "Gradient Descent(40/49): loss=1.2516334813435275e+75\n",
      "Gradient Descent(41/49): loss=1.034424564314773e+77\n",
      "Gradient Descent(42/49): loss=8.549101595694065e+78\n",
      "Gradient Descent(43/49): loss=7.065487481140161e+80\n",
      "Gradient Descent(44/49): loss=5.839340284748974e+82\n",
      "Gradient Descent(45/49): loss=4.825979106481966e+84\n",
      "Gradient Descent(46/49): loss=3.98847698549592e+86\n",
      "Gradient Descent(47/49): loss=3.2963152787926444e+88\n",
      "Gradient Descent(48/49): loss=2.7242715594736844e+90\n",
      "Gradient Descent(49/49): loss=2.251500509525154e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7497700587631484\n",
      "Gradient Descent(2/49): loss=181.50603375867175\n",
      "Gradient Descent(3/49): loss=14755.271584474289\n",
      "Gradient Descent(4/49): loss=1218257.9338202742\n",
      "Gradient Descent(5/49): loss=100967829.87750246\n",
      "Gradient Descent(6/49): loss=8377705452.737243\n",
      "Gradient Descent(7/49): loss=695383402894.0232\n",
      "Gradient Descent(8/49): loss=57726338290068.69\n",
      "Gradient Descent(9/49): loss=4792256097185383.0\n",
      "Gradient Descent(10/49): loss=3.9784263443337626e+17\n",
      "Gradient Descent(11/49): loss=3.3028157406940955e+19\n",
      "Gradient Descent(12/49): loss=2.741939882293554e+21\n",
      "Gradient Descent(13/49): loss=2.2763115037769265e+23\n",
      "Gradient Descent(14/49): loss=1.8897550602767509e+25\n",
      "Gradient Descent(15/49): loss=1.5688425507597298e+27\n",
      "Gradient Descent(16/49): loss=1.3024264552777238e+29\n",
      "Gradient Descent(17/49): loss=1.0812523401568494e+31\n",
      "Gradient Descent(18/49): loss=8.976373446166591e+32\n",
      "Gradient Descent(19/49): loss=7.452032915205875e+34\n",
      "Gradient Descent(20/49): loss=6.186551273884933e+36\n",
      "Gradient Descent(21/49): loss=5.1359698892785706e+38\n",
      "Gradient Descent(22/49): loss=4.263795050947748e+40\n",
      "Gradient Descent(23/49): loss=3.539730299926026e+42\n",
      "Gradient Descent(24/49): loss=2.938624030123561e+44\n",
      "Gradient Descent(25/49): loss=2.439595804969985e+46\n",
      "Gradient Descent(26/49): loss=2.025311040343617e+48\n",
      "Gradient Descent(27/49): loss=1.6813788586541868e+50\n",
      "Gradient Descent(28/49): loss=1.3958521975220636e+52\n",
      "Gradient Descent(29/49): loss=1.158812808486681e+54\n",
      "Gradient Descent(30/49): loss=9.620267299765892e+55\n",
      "Gradient Descent(31/49): loss=7.986582668153908e+57\n",
      "Gradient Descent(32/49): loss=6.630325408609888e+59\n",
      "Gradient Descent(33/49): loss=5.504383645755144e+61\n",
      "Gradient Descent(34/49): loss=4.569645900080962e+63\n",
      "Gradient Descent(35/49): loss=3.793642484972896e+65\n",
      "Gradient Descent(36/49): loss=3.149417617574324e+67\n",
      "Gradient Descent(37/49): loss=2.6145930643642118e+69\n",
      "Gradient Descent(38/49): loss=2.1705907956044879e+71\n",
      "Gradient Descent(39/49): loss=1.8019876462529365e+73\n",
      "Gradient Descent(40/49): loss=1.4959795664036757e+75\n",
      "Gradient Descent(41/49): loss=1.2419368510938123e+77\n",
      "Gradient Descent(42/49): loss=1.0310348996361931e+79\n",
      "Gradient Descent(43/49): loss=8.559476782829743e+80\n",
      "Gradient Descent(44/49): loss=7.105932381304776e+82\n",
      "Gradient Descent(45/49): loss=5.899224484020666e+84\n",
      "Gradient Descent(46/49): loss=4.897436064045274e+86\n",
      "Gradient Descent(47/49): loss=4.065768316899843e+88\n",
      "Gradient Descent(48/49): loss=3.375331865599194e+90\n",
      "Gradient Descent(49/49): loss=2.802143239587337e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7821216773330977\n",
      "Gradient Descent(2/49): loss=187.87197742832566\n",
      "Gradient Descent(3/49): loss=15588.920778684485\n",
      "Gradient Descent(4/49): loss=1312738.1248907174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=110925040.26475409\n",
      "Gradient Descent(6/49): loss=9382279643.863333\n",
      "Gradient Descent(7/49): loss=793807887849.5697\n",
      "Gradient Descent(8/49): loss=67167878773679.72\n",
      "Gradient Descent(9/49): loss=5683553040624476.0\n",
      "Gradient Descent(10/49): loss=4.809300625419462e+17\n",
      "Gradient Descent(11/49): loss=4.0695377778158805e+19\n",
      "Gradient Descent(12/49): loss=3.4435674937758135e+21\n",
      "Gradient Descent(13/49): loss=2.9138837722661483e+23\n",
      "Gradient Descent(14/49): loss=2.4656752974308326e+25\n",
      "Gradient Descent(15/49): loss=2.0864095122116736e+27\n",
      "Gradient Descent(16/49): loss=1.7654817282209905e+29\n",
      "Gradient Descent(17/49): loss=1.4939184861272174e+31\n",
      "Gradient Descent(18/49): loss=1.264126617184532e+33\n",
      "Gradient Descent(19/49): loss=1.069680922667007e+35\n",
      "Gradient Descent(20/49): loss=9.051445170459908e+36\n",
      "Gradient Descent(21/49): loss=7.659168069606807e+38\n",
      "Gradient Descent(22/49): loss=6.481048541322632e+40\n",
      "Gradient Descent(23/49): loss=5.484145250937413e+42\n",
      "Gradient Descent(24/49): loss=4.640583840969164e+44\n",
      "Gradient Descent(25/49): loss=3.926777537737019e+46\n",
      "Gradient Descent(26/49): loss=3.3227676428870796e+48\n",
      "Gradient Descent(27/49): loss=2.811665469335524e+50\n",
      "Gradient Descent(28/49): loss=2.3791801176277614e+52\n",
      "Gradient Descent(29/49): loss=2.013218888893271e+54\n",
      "Gradient Descent(30/49): loss=1.7035491615649052e+56\n",
      "Gradient Descent(31/49): loss=1.441512277616203e+58\n",
      "Gradient Descent(32/49): loss=1.219781438305785e+60\n",
      "Gradient Descent(33/49): loss=1.0321568399651577e+62\n",
      "Gradient Descent(34/49): loss=8.733923216331115e+63\n",
      "Gradient Descent(35/49): loss=7.390486774407554e+65\n",
      "Gradient Descent(36/49): loss=6.253695322230806e+67\n",
      "Gradient Descent(37/49): loss=5.2917630972184995e+69\n",
      "Gradient Descent(38/49): loss=4.477793565915922e+71\n",
      "Gradient Descent(39/49): loss=3.789027371519558e+73\n",
      "Gradient Descent(40/49): loss=3.2062059607671537e+75\n",
      "Gradient Descent(41/49): loss=2.7130330966007766e+77\n",
      "Gradient Descent(42/49): loss=2.2957191999886413e+79\n",
      "Gradient Descent(43/49): loss=1.9425957802725817e+81\n",
      "Gradient Descent(44/49): loss=1.6437891731495326e+83\n",
      "Gradient Descent(45/49): loss=1.3909444636930428e+85\n",
      "Gradient Descent(46/49): loss=1.176991875041588e+87\n",
      "Gradient Descent(47/49): loss=9.959490907608404e+88\n",
      "Gradient Descent(48/49): loss=8.427539836265214e+90\n",
      "Gradient Descent(49/49): loss=7.131230737665489e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.772706105690452\n",
      "Gradient Descent(2/49): loss=186.5662520049855\n",
      "Gradient Descent(3/49): loss=15480.853919489917\n",
      "Gradient Descent(4/49): loss=1305340.175558037\n",
      "Gradient Descent(5/49): loss=110495640.10127728\n",
      "Gradient Descent(6/49): loss=9364082051.021975\n",
      "Gradient Descent(7/49): loss=793849616697.9906\n",
      "Gradient Descent(8/49): loss=67306799454504.85\n",
      "Gradient Descent(9/49): loss=5706825542657239.0\n",
      "Gradient Descent(10/49): loss=4.838769769430391e+17\n",
      "Gradient Descent(11/49): loss=4.1027665227228e+19\n",
      "Gradient Descent(12/49): loss=3.4787171212834966e+21\n",
      "Gradient Descent(13/49): loss=2.9495894648049535e+23\n",
      "Gradient Descent(14/49): loss=2.5009446405548683e+25\n",
      "Gradient Descent(15/49): loss=2.1205406313046455e+27\n",
      "Gradient Descent(16/49): loss=1.7979976620989252e+29\n",
      "Gradient Descent(17/49): loss=1.524514812877715e+31\n",
      "Gradient Descent(18/49): loss=1.2926298326898235e+33\n",
      "Gradient Descent(19/49): loss=1.0960155131332626e+35\n",
      "Gradient Descent(20/49): loss=9.293070411774685e+36\n",
      "Gradient Descent(21/49): loss=7.879556141880614e+38\n",
      "Gradient Descent(22/49): loss=6.681043212050599e+40\n",
      "Gradient Descent(23/49): loss=5.664829033222717e+42\n",
      "Gradient Descent(24/49): loss=4.803185214817204e+44\n",
      "Gradient Descent(25/49): loss=4.0726009686335407e+46\n",
      "Gradient Descent(26/49): loss=3.4531415941551914e+48\n",
      "Gradient Descent(27/49): loss=2.9279045408875296e+50\n",
      "Gradient Descent(28/49): loss=2.4825582058551937e+52\n",
      "Gradient Descent(29/49): loss=2.104950881899579e+54\n",
      "Gradient Descent(30/49): loss=1.7847791865502236e+56\n",
      "Gradient Descent(31/49): loss=1.5133069242300962e+58\n",
      "Gradient Descent(32/49): loss=1.2831267106768864e+60\n",
      "Gradient Descent(33/49): loss=1.0879578552712306e+62\n",
      "Gradient Descent(34/49): loss=9.22474986294981e+63\n",
      "Gradient Descent(35/49): loss=7.821627429931838e+65\n",
      "Gradient Descent(36/49): loss=6.631925695717372e+67\n",
      "Gradient Descent(37/49): loss=5.623182493352175e+69\n",
      "Gradient Descent(38/49): loss=4.767873285124646e+71\n",
      "Gradient Descent(39/49): loss=4.042660128829218e+73\n",
      "Gradient Descent(40/49): loss=3.427754879353565e+75\n",
      "Gradient Descent(41/49): loss=2.906379249925938e+77\n",
      "Gradient Descent(42/49): loss=2.464307000269829e+79\n",
      "Gradient Descent(43/49): loss=2.0894757598250845e+81\n",
      "Gradient Descent(44/49): loss=1.7716578942552965e+83\n",
      "Gradient Descent(45/49): loss=1.502181434514396e+85\n",
      "Gradient Descent(46/49): loss=1.2736934537512713e+87\n",
      "Gradient Descent(47/49): loss=1.0799594355613111e+89\n",
      "Gradient Descent(48/49): loss=9.156931591529249e+90\n",
      "Gradient Descent(49/49): loss=7.764124596806331e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.780899544883774\n",
      "Gradient Descent(2/49): loss=185.81698685099798\n",
      "Gradient Descent(3/49): loss=15280.457433742955\n",
      "Gradient Descent(4/49): loss=1276222.659886853\n",
      "Gradient Descent(5/49): loss=106985526.50147419\n",
      "Gradient Descent(6/49): loss=8978319287.182047\n",
      "Gradient Descent(7/49): loss=753719729370.2023\n",
      "Gradient Descent(8/49): loss=63280524869196.76\n",
      "Gradient Descent(9/49): loss=5313058319639995.0\n",
      "Gradient Descent(10/49): loss=4.4609117527144416e+17\n",
      "Gradient Descent(11/49): loss=3.7454510140333875e+19\n",
      "Gradient Descent(12/49): loss=3.1447423428430864e+21\n",
      "Gradient Descent(13/49): loss=2.640378335034838e+23\n",
      "Gradient Descent(14/49): loss=2.2169061033690747e+25\n",
      "Gradient Descent(15/49): loss=1.8613517509868572e+27\n",
      "Gradient Descent(16/49): loss=1.5628223373221165e+29\n",
      "Gradient Descent(17/49): loss=1.3121720089005744e+31\n",
      "Gradient Descent(18/49): loss=1.1017217643294865e+33\n",
      "Gradient Descent(19/49): loss=9.250241875165582e+34\n",
      "Gradient Descent(20/49): loss=7.766659199158569e+36\n",
      "Gradient Descent(21/49): loss=6.521018145473297e+38\n",
      "Gradient Descent(22/49): loss=5.47515689354163e+40\n",
      "Gradient Descent(23/49): loss=4.597034134893647e+42\n",
      "Gradient Descent(24/49): loss=3.859747446201703e+44\n",
      "Gradient Descent(25/49): loss=3.240709098804734e+46\n",
      "Gradient Descent(26/49): loss=2.720954054497004e+48\n",
      "Gradient Descent(27/49): loss=2.2845589471189027e+50\n",
      "Gradient Descent(28/49): loss=1.9181542496960328e+52\n",
      "Gradient Descent(29/49): loss=1.6105146817362784e+54\n",
      "Gradient Descent(30/49): loss=1.3522153082835468e+56\n",
      "Gradient Descent(31/49): loss=1.1353427948791445e+58\n",
      "Gradient Descent(32/49): loss=9.532529723540878e+59\n",
      "Gradient Descent(33/49): loss=8.003672841369715e+61\n",
      "Gradient Descent(34/49): loss=6.720018799782406e+63\n",
      "Gradient Descent(35/49): loss=5.642241201565748e+65\n",
      "Gradient Descent(36/49): loss=4.7373209398873865e+67\n",
      "Gradient Descent(37/49): loss=3.9775346153701695e+69\n",
      "Gradient Descent(38/49): loss=3.339605193994727e+71\n",
      "Gradient Descent(39/49): loss=2.8039888851397528e+73\n",
      "Gradient Descent(40/49): loss=2.354276392348821e+75\n",
      "Gradient Descent(41/49): loss=1.9766901933688292e+77\n",
      "Gradient Descent(42/49): loss=1.6596624479856605e+79\n",
      "Gradient Descent(43/49): loss=1.393480602319051e+81\n",
      "Gradient Descent(44/49): loss=1.1699898322072838e+83\n",
      "Gradient Descent(45/49): loss=9.823432096509375e+84\n",
      "Gradient Descent(46/49): loss=8.247919383425377e+86\n",
      "Gradient Descent(47/49): loss=6.925092318768876e+88\n",
      "Gradient Descent(48/49): loss=5.814424389239726e+90\n",
      "Gradient Descent(49/49): loss=4.8818888502840467e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.787776188077664\n",
      "Gradient Descent(2/49): loss=187.29004905311052\n",
      "Gradient Descent(3/49): loss=15466.373710648108\n",
      "Gradient Descent(4/49): loss=1297127.3183507505\n",
      "Gradient Descent(5/49): loss=109202541.93878734\n",
      "Gradient Descent(6/49): loss=9204137521.573534\n",
      "Gradient Descent(7/49): loss=776053006611.4634\n",
      "Gradient Descent(8/49): loss=65441075622952.6\n",
      "Gradient Descent(9/49): loss=5518561759794813.0\n",
      "Gradient Descent(10/49): loss=4.653789377908761e+17\n",
      "Gradient Descent(11/49): loss=3.92454467474418e+19\n",
      "Gradient Descent(12/49): loss=3.309576281584947e+21\n",
      "Gradient Descent(13/49): loss=2.7909734021531404e+23\n",
      "Gradient Descent(14/49): loss=2.353634708127037e+25\n",
      "Gradient Descent(15/49): loss=1.984826005034285e+27\n",
      "Gradient Descent(16/49): loss=1.673808732606561e+29\n",
      "Gradient Descent(17/49): loss=1.4115270958625349e+31\n",
      "Gradient Descent(18/49): loss=1.1903443366180072e+33\n",
      "Gradient Descent(19/49): loss=1.0038203620648138e+35\n",
      "Gradient Descent(20/49): loss=8.465242270352492e+36\n",
      "Gradient Descent(21/49): loss=7.138760021981429e+38\n",
      "Gradient Descent(22/49): loss=6.020134217635962e+40\n",
      "Gradient Descent(23/49): loss=5.0767942733697936e+42\n",
      "Gradient Descent(24/49): loss=4.2812733341830434e+44\n",
      "Gradient Descent(25/49): loss=3.610408532436254e+46\n",
      "Gradient Descent(26/49): loss=3.044666563803601e+48\n",
      "Gradient Descent(27/49): loss=2.567574943794233e+50\n",
      "Gradient Descent(28/49): loss=2.1652423849540906e+52\n",
      "Gradient Descent(29/49): loss=1.8259543297589616e+54\n",
      "Gradient Descent(30/49): loss=1.5398318624897048e+56\n",
      "Gradient Descent(31/49): loss=1.29854406876185e+58\n",
      "Gradient Descent(32/49): loss=1.095065467596043e+60\n",
      "Gradient Descent(33/49): loss=9.234714532751573e+61\n",
      "Gradient Descent(34/49): loss=7.787657909496998e+63\n",
      "Gradient Descent(35/49): loss=6.567351432495278e+65\n",
      "Gradient Descent(36/49): loss=5.538263922109489e+67\n",
      "Gradient Descent(37/49): loss=4.670431845503564e+69\n",
      "Gradient Descent(38/49): loss=3.938586880342361e+71\n",
      "Gradient Descent(39/49): loss=3.321420187073193e+73\n",
      "Gradient Descent(40/49): loss=2.8009619678970764e+75\n",
      "Gradient Descent(41/49): loss=2.3620582472942336e+77\n",
      "Gradient Descent(42/49): loss=1.991929639730035e+79\n",
      "Gradient Descent(43/49): loss=1.679799257355383e+81\n",
      "Gradient Descent(44/49): loss=1.4165789236381487e+83\n",
      "Gradient Descent(45/49): loss=1.1946045565320094e+85\n",
      "Gradient Descent(46/49): loss=1.0074130164395737e+87\n",
      "Gradient Descent(47/49): loss=8.495539215404713e+88\n",
      "Gradient Descent(48/49): loss=7.164309511858301e+90\n",
      "Gradient Descent(49/49): loss=6.041680166531738e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.820661048311244\n",
      "Gradient Descent(2/49): loss=193.85303324892166\n",
      "Gradient Descent(3/49): loss=16339.433745114822\n",
      "Gradient Descent(4/49): loss=1397639.725247454\n",
      "Gradient Descent(5/49): loss=119962937.14211468\n",
      "Gradient Descent(6/49): loss=10306898060.555634\n",
      "Gradient Descent(7/49): loss=885804323581.6707\n",
      "Gradient Descent(8/49): loss=76135472097007.05\n",
      "Gradient Descent(9/49): loss=6544077684765922.0\n",
      "Gradient Descent(10/49): loss=5.624884428038045e+17\n",
      "Gradient Descent(11/49): loss=4.834815698758961e+19\n",
      "Gradient Descent(12/49): loss=4.1557231116056653e+21\n",
      "Gradient Descent(13/49): loss=3.572015996133803e+23\n",
      "Gradient Descent(14/49): loss=3.0702958182476098e+25\n",
      "Gradient Descent(15/49): loss=2.6390465927308095e+27\n",
      "Gradient Descent(16/49): loss=2.268370015149667e+29\n",
      "Gradient Descent(17/49): loss=1.9497581255165955e+31\n",
      "Gradient Descent(18/49): loss=1.6758979908094954e+33\n",
      "Gradient Descent(19/49): loss=1.440503844799856e+35\n",
      "Gradient Descent(20/49): loss=1.238172811476835e+37\n",
      "Gradient Descent(21/49): loss=1.0642608949973522e+39\n",
      "Gradient Descent(22/49): loss=9.147763883437502e+40\n",
      "Gradient Descent(23/49): loss=7.862882537604313e+42\n",
      "Gradient Descent(24/49): loss=6.758473719696723e+44\n",
      "Gradient Descent(25/49): loss=5.80918852614036e+46\n",
      "Gradient Descent(26/49): loss=4.9932385227588016e+48\n",
      "Gradient Descent(27/49): loss=4.2918956465212586e+50\n",
      "Gradient Descent(28/49): loss=3.689062350350284e+52\n",
      "Gradient Descent(29/49): loss=3.1709021247529266e+54\n",
      "Gradient Descent(30/49): loss=2.725521915835294e+56\n",
      "Gradient Descent(31/49): loss=2.3426991504120807e+58\n",
      "Gradient Descent(32/49): loss=2.0136471027639794e+60\n",
      "Gradient Descent(33/49): loss=1.730813217632545e+62\n",
      "Gradient Descent(34/49): loss=1.4877057604679253e+64\n",
      "Gradient Descent(35/49): loss=1.2787448161256895e+66\n",
      "Gradient Descent(36/49): loss=1.0991342160656903e+68\n",
      "Gradient Descent(37/49): loss=9.44751454466577e+69\n",
      "Gradient Descent(38/49): loss=8.120530665595906e+71\n",
      "Gradient Descent(39/49): loss=6.979932973812225e+73\n",
      "Gradient Descent(40/49): loss=5.9995419419226004e+75\n",
      "Gradient Descent(41/49): loss=5.156855180119222e+77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=4.432530950887959e+79\n",
      "Gradient Descent(43/49): loss=3.809944228475601e+81\n",
      "Gradient Descent(44/49): loss=3.2748051135856525e+83\n",
      "Gradient Descent(45/49): loss=2.8148308449800242e+85\n",
      "Gradient Descent(46/49): loss=2.4194638798446116e+87\n",
      "Gradient Descent(47/49): loss=2.079629572168587e+89\n",
      "Gradient Descent(48/49): loss=1.787527887259007e+91\n",
      "Gradient Descent(49/49): loss=1.536454371725785e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8110554935181247\n",
      "Gradient Descent(2/49): loss=192.50613600625775\n",
      "Gradient Descent(3/49): loss=16226.14454059782\n",
      "Gradient Descent(4/49): loss=1389756.9772275456\n",
      "Gradient Descent(5/49): loss=119497710.67278641\n",
      "Gradient Descent(6/49): loss=10286814616.714832\n",
      "Gradient Descent(7/49): loss=885841397535.2975\n",
      "Gradient Descent(8/49): loss=76291999211105.61\n",
      "Gradient Descent(9/49): loss=6570782398793607.0\n",
      "Gradient Descent(10/49): loss=5.6592635610072576e+17\n",
      "Gradient Descent(11/49): loss=4.87421017332175e+19\n",
      "Gradient Descent(12/49): loss=4.1980640036889244e+21\n",
      "Gradient Descent(13/49): loss=3.6157134794461436e+23\n",
      "Gradient Descent(14/49): loss=3.1141462753849403e+25\n",
      "Gradient Descent(15/49): loss=2.682155931603259e+27\n",
      "Gradient Descent(16/49): loss=2.3100907545285698e+29\n",
      "Gradient Descent(17/49): loss=1.9896379809401294e+31\n",
      "Gradient Descent(18/49): loss=1.713637999585755e+33\n",
      "Gradient Descent(19/49): loss=1.4759243755651797e+35\n",
      "Gradient Descent(20/49): loss=1.2711860749744852e+37\n",
      "Gradient Descent(21/49): loss=1.0948488039197969e+39\n",
      "Gradient Descent(22/49): loss=9.429728086672434e+40\n",
      "Gradient Descent(23/49): loss=8.121648529963763e+42\n",
      "Gradient Descent(24/49): loss=6.995024059872921e+44\n",
      "Gradient Descent(25/49): loss=6.024683463916839e+46\n",
      "Gradient Descent(26/49): loss=5.188947247317446e+48\n",
      "Gradient Descent(27/49): loss=4.469143266481153e+50\n",
      "Gradient Descent(28/49): loss=3.849189553171822e+52\n",
      "Gradient Descent(29/49): loss=3.3152350087700274e+54\n",
      "Gradient Descent(30/49): loss=2.855349940955154e+56\n",
      "Gradient Descent(31/49): loss=2.4592595287347165e+58\n",
      "Gradient Descent(32/49): loss=2.1181142608563734e+60\n",
      "Gradient Descent(33/49): loss=1.8242922187034802e+62\n",
      "Gradient Descent(34/49): loss=1.5712287862490036e+64\n",
      "Gradient Descent(35/49): loss=1.3532699824220477e+66\n",
      "Gradient Descent(36/49): loss=1.1655461390167883e+68\n",
      "Gradient Descent(37/49): loss=1.0038631018368814e+70\n",
      "Gradient Descent(38/49): loss=8.646085242748621e+71\n",
      "Gradient Descent(39/49): loss=7.446711597237509e+73\n",
      "Gradient Descent(40/49): loss=6.413713496398868e+75\n",
      "Gradient Descent(41/49): loss=5.524011542107896e+77\n",
      "Gradient Descent(42/49): loss=4.7577278801858786e+79\n",
      "Gradient Descent(43/49): loss=4.097742086407798e+81\n",
      "Gradient Descent(44/49): loss=3.529308659422904e+83\n",
      "Gradient Descent(45/49): loss=3.0397275745572412e+85\n",
      "Gradient Descent(46/49): loss=2.618060537962266e+87\n",
      "Gradient Descent(47/49): loss=2.2548866016171398e+89\n",
      "Gradient Descent(48/49): loss=1.942091679098438e+91\n",
      "Gradient Descent(49/49): loss=1.6726872594472772e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.819436136012131\n",
      "Gradient Descent(2/49): loss=191.73551791612715\n",
      "Gradient Descent(3/49): loss=16016.441089676639\n",
      "Gradient Descent(4/49): loss=1358796.495867483\n",
      "Gradient Descent(5/49): loss=115705951.65772933\n",
      "Gradient Descent(6/49): loss=9863483058.2384\n",
      "Gradient Descent(7/49): loss=841105470256.9583\n",
      "Gradient Descent(8/49): loss=71732545400048.72\n",
      "Gradient Descent(9/49): loss=6117816575147424.0\n",
      "Gradient Descent(10/49): loss=5.217725634142649e+17\n",
      "Gradient Descent(11/49): loss=4.450076531594882e+19\n",
      "Gradient Descent(12/49): loss=3.795370552804557e+21\n",
      "Gradient Descent(13/49): loss=3.236987590996194e+23\n",
      "Gradient Descent(14/49): loss=2.760755411707943e+25\n",
      "Gradient Descent(15/49): loss=2.3545875571710214e+27\n",
      "Gradient Descent(16/49): loss=2.008175951159688e+29\n",
      "Gradient Descent(17/49): loss=1.7127291157723625e+31\n",
      "Gradient Descent(18/49): loss=1.4607490073550166e+33\n",
      "Gradient Descent(19/49): loss=1.245840713273172e+35\n",
      "Gradient Descent(20/49): loss=1.0625501542066585e+37\n",
      "Gradient Descent(21/49): loss=9.062256661033804e+38\n",
      "Gradient Descent(22/49): loss=7.728999470401211e+40\n",
      "Gradient Descent(23/49): loss=6.591893724499961e+42\n",
      "Gradient Descent(24/49): loss=5.622081233354787e+44\n",
      "Gradient Descent(25/49): loss=4.794949481205304e+46\n",
      "Gradient Descent(26/49): loss=4.089506994475465e+48\n",
      "Gradient Descent(27/49): loss=3.487850606855688e+50\n",
      "Gradient Descent(28/49): loss=2.9747111014060284e+52\n",
      "Gradient Descent(29/49): loss=2.5370656986956313e+54\n",
      "Gradient Descent(30/49): loss=2.16380755645669e+56\n",
      "Gradient Descent(31/49): loss=1.845463893105431e+58\n",
      "Gradient Descent(32/49): loss=1.573955581490274e+60\n",
      "Gradient Descent(33/49): loss=1.3423921116850934e+62\n",
      "Gradient Descent(34/49): loss=1.1448967192632933e+64\n",
      "Gradient Descent(35/49): loss=9.764572410474231e+65\n",
      "Gradient Descent(36/49): loss=8.327989132569745e+67\n",
      "Gradient Descent(37/49): loss=7.10275883845194e+69\n",
      "Gradient Descent(38/49): loss=6.057786857562844e+71\n",
      "Gradient Descent(39/49): loss=5.166553229006863e+73\n",
      "Gradient Descent(40/49): loss=4.406439661183561e+75\n",
      "Gradient Descent(41/49): loss=3.758155510455053e+77\n",
      "Gradient Descent(42/49): loss=3.2052482109717594e+79\n",
      "Gradient Descent(43/49): loss=2.733685731033969e+81\n",
      "Gradient Descent(44/49): loss=2.3315004593023644e+83\n",
      "Gradient Descent(45/49): loss=1.9884854831762635e+85\n",
      "Gradient Descent(46/49): loss=1.6959355512998102e+87\n",
      "Gradient Descent(47/49): loss=1.446426146178538e+89\n",
      "Gradient Descent(48/49): loss=1.2336250600710736e+91\n",
      "Gradient Descent(49/49): loss=1.0521316922098232e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.826092885185066\n",
      "Gradient Descent(2/49): loss=193.21154921216984\n",
      "Gradient Descent(3/49): loss=16205.810255819117\n",
      "Gradient Descent(4/49): loss=1380428.843575134\n",
      "Gradient Descent(5/49): loss=118036847.41558911\n",
      "Gradient Descent(6/49): loss=10104696698.74881\n",
      "Gradient Descent(7/49): loss=865341748491.981\n",
      "Gradient Descent(8/49): loss=74114483330975.86\n",
      "Gradient Descent(9/49): loss=6347972893841679.0\n",
      "Gradient Descent(10/49): loss=5.4371638870424806e+17\n",
      "Gradient Descent(11/49): loss=4.657056814994135e+19\n",
      "Gradient Descent(12/49): loss=3.9888822816483983e+21\n",
      "Gradient Descent(13/49): loss=3.416576027696791e+23\n",
      "Gradient Descent(14/49): loss=2.9263820185043718e+25\n",
      "Gradient Descent(15/49): loss=2.5065188184107225e+27\n",
      "Gradient Descent(16/49): loss=2.1468956002698793e+29\n",
      "Gradient Descent(17/49): loss=1.8388693940321042e+31\n",
      "Gradient Descent(18/49): loss=1.5750373018655806e+33\n",
      "Gradient Descent(19/49): loss=1.3490585636984805e+35\n",
      "Gradient Descent(20/49): loss=1.15550216267079e+37\n",
      "Gradient Descent(21/49): loss=9.897162984157431e+38\n",
      "Gradient Descent(22/49): loss=8.477165885203612e+40\n",
      "Gradient Descent(23/49): loss=7.260903105335097e+42\n",
      "Gradient Descent(24/49): loss=6.2191438293292325e+44\n",
      "Gradient Descent(25/49): loss=5.326851138047631e+46\n",
      "Gradient Descent(26/49): loss=4.5625802884808704e+48\n",
      "Gradient Descent(27/49): loss=3.9079633256778684e+50\n",
      "Gradient Descent(28/49): loss=3.3472676400678796e+52\n",
      "Gradient Descent(29/49): loss=2.8670178608449923e+54\n",
      "Gradient Descent(30/49): loss=2.4556719982623038e+56\n",
      "Gradient Descent(31/49): loss=2.1033440514640875e+58\n",
      "Gradient Descent(32/49): loss=1.801566415205253e+60\n",
      "Gradient Descent(33/49): loss=1.5430863753061646e+62\n",
      "Gradient Descent(34/49): loss=1.3216918019557154e+64\n",
      "Gradient Descent(35/49): loss=1.1320618516966304e+66\n",
      "Gradient Descent(36/49): loss=9.696390899682286e+67\n",
      "Gradient Descent(37/49): loss=8.305199608884638e+69\n",
      "Gradient Descent(38/49): loss=7.113609719022178e+71\n",
      "Gradient Descent(39/49): loss=6.092983386026315e+73\n",
      "Gradient Descent(40/49): loss=5.218791585251059e+75\n",
      "Gradient Descent(41/49): loss=4.4700245979251525e+77\n",
      "Gradient Descent(42/49): loss=3.8286870781590644e+79\n",
      "Gradient Descent(43/49): loss=3.279365565296088e+81\n",
      "Gradient Descent(44/49): loss=2.808858047500881e+83\n",
      "Gradient Descent(45/49): loss=2.4058566737734737e+85\n",
      "Gradient Descent(46/49): loss=2.060675988909508e+87\n",
      "Gradient Descent(47/49): loss=1.7650201599947905e+89\n",
      "Gradient Descent(48/49): loss=1.5117836001168862e+91\n",
      "Gradient Descent(49/49): loss=1.2948801976228517e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8595152659211536\n",
      "Gradient Descent(2/49): loss=199.97610876282485\n",
      "Gradient Descent(3/49): loss=17119.821129128562\n",
      "Gradient Descent(4/49): loss=1487307.7449814475\n",
      "Gradient Descent(5/49): loss=129658266.73586118\n",
      "Gradient Descent(6/49): loss=11314369399.653383\n",
      "Gradient Descent(7/49): loss=987620461593.51\n",
      "Gradient Descent(8/49): loss=86216320506070.61\n",
      "Gradient Descent(9/49): loss=7526639776237098.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=6.570774555012918e+17\n",
      "Gradient Descent(11/49): loss=5.736317432823014e+19\n",
      "Gradient Descent(12/49): loss=5.007836593819008e+21\n",
      "Gradient Descent(13/49): loss=4.371869960373634e+23\n",
      "Gradient Descent(14/49): loss=3.816667763170296e+25\n",
      "Gradient Descent(15/49): loss=3.331973116896852e+27\n",
      "Gradient Descent(16/49): loss=2.9088318991032412e+29\n",
      "Gradient Descent(17/49): loss=2.5394271624449157e+31\n",
      "Gradient Descent(18/49): loss=2.2169346809486627e+33\n",
      "Gradient Descent(19/49): loss=1.9353968695828673e+35\n",
      "Gradient Descent(20/49): loss=1.689612723031138e+37\n",
      "Gradient Descent(21/49): loss=1.4750417336923243e+39\n",
      "Gradient Descent(22/49): loss=1.2877200121018426e+41\n",
      "Gradient Descent(23/49): loss=1.124187059724912e+43\n",
      "Gradient Descent(24/49): loss=9.814218412207369e+44\n",
      "Gradient Descent(25/49): loss=8.567869751685199e+46\n",
      "Gradient Descent(26/49): loss=7.479800122497618e+48\n",
      "Gradient Descent(27/49): loss=6.5299090081886714e+50\n",
      "Gradient Descent(28/49): loss=5.700648540991494e+52\n",
      "Gradient Descent(29/49): loss=4.976699330290168e+54\n",
      "Gradient Descent(30/49): loss=4.344687458982125e+56\n",
      "Gradient Descent(31/49): loss=3.792937419656424e+58\n",
      "Gradient Descent(32/49): loss=3.311256426440481e+60\n",
      "Gradient Descent(33/49): loss=2.8907461179880586e+62\n",
      "Gradient Descent(34/49): loss=2.523638173092479e+64\n",
      "Gradient Descent(35/49): loss=2.2031508021611297e+66\n",
      "Gradient Descent(36/49): loss=1.923363463437891e+68\n",
      "Gradient Descent(37/49): loss=1.6791074895368106e+70\n",
      "Gradient Descent(38/49): loss=1.4658706037698813e+72\n",
      "Gradient Descent(39/49): loss=1.2797135623458036e+74\n",
      "Gradient Descent(40/49): loss=1.117197382524814e+76\n",
      "Gradient Descent(41/49): loss=9.753198123745697e+77\n",
      "Gradient Descent(42/49): loss=8.514598684975315e+79\n",
      "Gradient Descent(43/49): loss=7.433294171444581e+81\n",
      "Gradient Descent(44/49): loss=6.489309042448705e+83\n",
      "Gradient Descent(45/49): loss=5.665204534777999e+85\n",
      "Gradient Descent(46/49): loss=4.9457565067294945e+87\n",
      "Gradient Descent(47/49): loss=4.317674194055489e+89\n",
      "Gradient Descent(48/49): loss=3.7693546822709165e+91\n",
      "Gradient Descent(49/49): loss=3.2906685595497475e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.849717992681131\n",
      "Gradient Descent(2/49): loss=198.58707562289175\n",
      "Gradient Descent(3/49): loss=17001.10107851976\n",
      "Gradient Descent(4/49): loss=1478912.6372891008\n",
      "Gradient Descent(5/49): loss=129154547.72046603\n",
      "Gradient Descent(6/49): loss=11292222916.656765\n",
      "Gradient Descent(7/49): loss=987651379770.4778\n",
      "Gradient Descent(8/49): loss=86392523935550.3\n",
      "Gradient Descent(9/49): loss=7557250597518062.0\n",
      "Gradient Descent(10/49): loss=6.610834280720056e+17\n",
      "Gradient Descent(11/49): loss=5.782960529766104e+19\n",
      "Gradient Descent(12/49): loss=5.058766838894955e+21\n",
      "Gradient Descent(13/49): loss=4.4252646516913064e+23\n",
      "Gradient Descent(14/49): loss=3.871095468359798e+25\n",
      "Gradient Descent(15/49): loss=3.386324169063448e+27\n",
      "Gradient Descent(16/49): loss=2.9622600623748522e+29\n",
      "Gradient Descent(17/49): loss=2.5913008529045488e+31\n",
      "Gradient Descent(18/49): loss=2.266796289392095e+33\n",
      "Gradient Descent(19/49): loss=1.982928926842852e+35\n",
      "Gradient Descent(20/49): loss=1.7346098313906584e+37\n",
      "Gradient Descent(21/49): loss=1.5173873488504362e+39\n",
      "Gradient Descent(22/49): loss=1.3273672988642878e+41\n",
      "Gradient Descent(23/49): loss=1.1611431632372219e+43\n",
      "Gradient Descent(24/49): loss=1.0157350167414852e+45\n",
      "Gradient Descent(25/49): loss=8.885361055382579e+46\n",
      "Gradient Descent(26/49): loss=7.77266115505118e+48\n",
      "Gradient Descent(27/49): loss=6.799302926991988e+50\n",
      "Gradient Descent(28/49): loss=5.947836831013641e+52\n",
      "Gradient Descent(29/49): loss=5.202998505614913e+54\n",
      "Gradient Descent(30/49): loss=4.551435121467111e+56\n",
      "Gradient Descent(31/49): loss=3.9814660032227067e+58\n",
      "Gradient Descent(32/49): loss=3.482873228281549e+60\n",
      "Gradient Descent(33/49): loss=3.0467184485467575e+62\n",
      "Gradient Descent(34/49): loss=2.665182651306305e+64\n",
      "Gradient Descent(35/49): loss=2.3314259866093805e+66\n",
      "Gradient Descent(36/49): loss=2.0394651482416618e+68\n",
      "Gradient Descent(37/49): loss=1.784066110089753e+70\n",
      "Gradient Descent(38/49): loss=1.5606502949633313e+72\n",
      "Gradient Descent(39/49): loss=1.365212493749245e+74\n",
      "Gradient Descent(40/49): loss=1.1942490634218686e+76\n",
      "Gradient Descent(41/49): loss=1.044695116704654e+78\n",
      "Gradient Descent(42/49): loss=9.138695773722498e+79\n",
      "Gradient Descent(43/49): loss=7.994271161914915e+81\n",
      "Gradient Descent(44/49): loss=6.993161058494475e+83\n",
      "Gradient Descent(45/49): loss=6.117418411202515e+85\n",
      "Gradient Descent(46/49): loss=5.351343649130282e+87\n",
      "Gradient Descent(47/49): loss=4.6812032341364566e+89\n",
      "Gradient Descent(48/49): loss=4.0949834576314374e+91\n",
      "Gradient Descent(49/49): loss=3.5821750690062643e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.858287656698396\n",
      "Gradient Descent(2/49): loss=197.7946565309718\n",
      "Gradient Descent(3/49): loss=16781.73362682693\n",
      "Gradient Descent(4/49): loss=1446007.9626395928\n",
      "Gradient Descent(5/49): loss=125060969.74371058\n",
      "Gradient Descent(6/49): loss=10827995931.058043\n",
      "Gradient Descent(7/49): loss=937822499438.7765\n",
      "Gradient Descent(8/49): loss=81234239133757.52\n",
      "Gradient Descent(9/49): loss=7036749895551787.0\n",
      "Gradient Descent(10/49): loss=6.095505516531063e+17\n",
      "Gradient Descent(11/49): loss=5.280181001587825e+19\n",
      "Gradient Descent(12/49): loss=4.573917838119674e+21\n",
      "Gradient Descent(13/49): loss=3.9621239410103434e+23\n",
      "Gradient Descent(14/49): loss=3.43216218410225e+25\n",
      "Gradient Descent(15/49): loss=2.9730866178840743e+27\n",
      "Gradient Descent(16/49): loss=2.5754156304831325e+29\n",
      "Gradient Descent(17/49): loss=2.2309359078955706e+31\n",
      "Gradient Descent(18/49): loss=1.932532742217139e+33\n",
      "Gradient Descent(19/49): loss=1.674043072174056e+35\n",
      "Gradient Descent(20/49): loss=1.4501281901169824e+37\n",
      "Gradient Descent(21/49): loss=1.2561634779906076e+39\n",
      "Gradient Descent(22/49): loss=1.088142892614478e+41\n",
      "Gradient Descent(23/49): loss=9.425962269224701e+42\n",
      "Gradient Descent(24/49): loss=8.16517438141557e+44\n",
      "Gradient Descent(25/49): loss=7.07302562589547e+46\n",
      "Gradient Descent(26/49): loss=6.126959348038676e+48\n",
      "Gradient Descent(27/49): loss=5.307436002363862e+50\n",
      "Gradient Descent(28/49): loss=4.597529593240348e+52\n",
      "Gradient Descent(29/49): loss=3.982578094452129e+54\n",
      "Gradient Descent(30/49): loss=3.449880627572243e+56\n",
      "Gradient Descent(31/49): loss=2.9884351448318756e+58\n",
      "Gradient Descent(32/49): loss=2.588711198726623e+60\n",
      "Gradient Descent(33/49): loss=2.242453105265441e+62\n",
      "Gradient Descent(34/49): loss=1.9425094355014026e+64\n",
      "Gradient Descent(35/49): loss=1.6826853137539006e+66\n",
      "Gradient Descent(36/49): loss=1.4576144719689364e+68\n",
      "Gradient Descent(37/49): loss=1.2626484176969687e+70\n",
      "Gradient Descent(38/49): loss=1.0937604266230423e+72\n",
      "Gradient Descent(39/49): loss=9.474623767625288e+73\n",
      "Gradient Descent(40/49): loss=8.207327066605255e+75\n",
      "Gradient Descent(41/49): loss=7.109540096821614e+77\n",
      "Gradient Descent(42/49): loss=6.158589754998154e+79\n",
      "Gradient Descent(43/49): loss=5.334835622816775e+81\n",
      "Gradient Descent(44/49): loss=4.62126432425169e+83\n",
      "Gradient Descent(45/49): loss=4.003138140426072e+85\n",
      "Gradient Descent(46/49): loss=3.4676906246709545e+87\n",
      "Gradient Descent(47/49): loss=3.003862931183008e+89\n",
      "Gradient Descent(48/49): loss=2.6020754115547925e+91\n",
      "Gradient Descent(49/49): loss=2.2540297618545324e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8647201500853545\n",
      "Gradient Descent(2/49): loss=199.2726967235859\n",
      "Gradient Descent(3/49): loss=16974.477375654977\n",
      "Gradient Descent(4/49): loss=1468374.31023709\n",
      "Gradient Descent(5/49): loss=127509244.57900557\n",
      "Gradient Descent(6/49): loss=11085378945.516455\n",
      "Gradient Descent(7/49): loss=964092841193.2478\n",
      "Gradient Descent(8/49): loss=83856845483628.84\n",
      "Gradient Descent(9/49): loss=7294152642662855.0\n",
      "Gradient Descent(10/49): loss=6.344780446917686e+17\n",
      "Gradient Descent(11/49): loss=5.518996491121536e+19\n",
      "Gradient Descent(12/49): loss=4.800696096323896e+21\n",
      "Gradient Descent(13/49): loss=4.1758846979830295e+23\n",
      "Gradient Descent(14/49): loss=3.632393120979198e+25\n",
      "Gradient Descent(15/49): loss=3.159637142368124e+27\n",
      "Gradient Descent(16/49): loss=2.748410397553954e+29\n",
      "Gradient Descent(17/49): loss=2.3907048215104777e+31\n",
      "Gradient Descent(18/49): loss=2.079554624680451e+33\n",
      "Gradient Descent(19/49): loss=1.8089006222687943e+35\n",
      "Gradient Descent(20/49): loss=1.5734722343759387e+37\n",
      "Gradient Descent(21/49): loss=1.3686848475869254e+39\n",
      "Gradient Descent(22/49): loss=1.1905505360195073e+41\n",
      "Gradient Descent(23/49): loss=1.0356004023305417e+43\n",
      "Gradient Descent(24/49): loss=9.008170261255332e+44\n",
      "Gradient Descent(25/49): loss=7.835757042310948e+46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=6.815933385520005e+48\n",
      "Gradient Descent(27/49): loss=5.9288397617478605e+50\n",
      "Gradient Descent(28/49): loss=5.157201359267904e+52\n",
      "Gradient Descent(29/49): loss=4.485991682830386e+54\n",
      "Gradient Descent(30/49): loss=3.90213993530788e+56\n",
      "Gradient Descent(31/49): loss=3.3942764836152293e+58\n",
      "Gradient Descent(32/49): loss=2.9525114522358424e+60\n",
      "Gradient Descent(33/49): loss=2.5682421327973293e+62\n",
      "Gradient Descent(34/49): loss=2.2339854592877214e+64\n",
      "Gradient Descent(35/49): loss=1.943232286619784e+66\n",
      "Gradient Descent(36/49): loss=1.6903206348377485e+68\n",
      "Gradient Descent(37/49): loss=1.4703254305888084e+70\n",
      "Gradient Descent(38/49): loss=1.2789625987401338e+72\n",
      "Gradient Descent(39/49): loss=1.1125056364705983e+74\n",
      "Gradient Descent(40/49): loss=9.677130452431118e+75\n",
      "Gradient Descent(41/49): loss=8.417652075045879e+77\n",
      "Gradient Descent(42/49): loss=7.322094788824801e+79\n",
      "Gradient Descent(43/49): loss=6.369124266310707e+81\n",
      "Gradient Descent(44/49): loss=5.540182842432042e+83\n",
      "Gradient Descent(45/49): loss=4.819128131936639e+85\n",
      "Gradient Descent(46/49): loss=4.191918680761114e+87\n",
      "Gradient Descent(47/49): loss=3.646340529869319e+89\n",
      "Gradient Descent(48/49): loss=3.171769366803077e+91\n",
      "Gradient Descent(49/49): loss=2.7589636332048796e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8986843301628267\n",
      "Gradient Descent(2/49): loss=206.24343547472762\n",
      "Gradient Descent(3/49): loss=17931.026838689613\n",
      "Gradient Descent(4/49): loss=1581969.7358181213\n",
      "Gradient Descent(5/49): loss=140053200.07679492\n",
      "Gradient Descent(6/49): loss=12411388933.197626\n",
      "Gradient Descent(7/49): loss=1100216030293.3386\n",
      "Gradient Descent(8/49): loss=97538365728029.31\n",
      "Gradient Descent(9/49): loss=8647395284622144.0\n",
      "Gradient Descent(10/49): loss=7.666532288731764e+17\n",
      "Gradient Descent(11/49): loss=6.796945878484775e+19\n",
      "Gradient Descent(12/49): loss=6.025998544100478e+21\n",
      "Gradient Descent(13/49): loss=5.3424977413179654e+23\n",
      "Gradient Descent(14/49): loss=4.736523621412122e+25\n",
      "Gradient Descent(15/49): loss=4.199282371526764e+27\n",
      "Gradient Descent(16/49): loss=3.722977859739431e+29\n",
      "Gradient Descent(17/49): loss=3.300698298351267e+31\n",
      "Gradient Descent(18/49): loss=2.92631588890826e+33\n",
      "Gradient Descent(19/49): loss=2.594397884827236e+35\n",
      "Gradient Descent(20/49): loss=2.3001277513480716e+37\n",
      "Gradient Descent(21/49): loss=2.0392352705738124e+39\n",
      "Gradient Descent(22/49): loss=1.807934575087997e+41\n",
      "Gradient Descent(23/49): loss=1.6028692103221732e+43\n",
      "Gradient Descent(24/49): loss=1.4210634282911031e+45\n",
      "Gradient Descent(25/49): loss=1.2598790058616197e+47\n",
      "Gradient Descent(26/49): loss=1.1169769609226879e+49\n",
      "Gradient Descent(27/49): loss=9.90283610908219e+50\n",
      "Gradient Descent(28/49): loss=8.779604811395079e+52\n",
      "Gradient Descent(29/49): loss=7.783776263203849e+54\n",
      "Gradient Descent(30/49): loss=6.90089978047523e+56\n",
      "Gradient Descent(31/49): loss=6.118163751094428e+58\n",
      "Gradient Descent(32/49): loss=5.4242097227831944e+60\n",
      "Gradient Descent(33/49): loss=4.80896757813528e+62\n",
      "Gradient Descent(34/49): loss=4.263509404959009e+64\n",
      "Gradient Descent(35/49): loss=3.779919941407136e+66\n",
      "Gradient Descent(36/49): loss=3.351181715895554e+68\n",
      "Gradient Descent(37/49): loss=2.9710732150512005e+70\n",
      "Gradient Descent(38/49): loss=2.63407860198227e+72\n",
      "Gradient Descent(39/49): loss=2.335307674772757e+74\n",
      "Gradient Descent(40/49): loss=2.070424903701959e+76\n",
      "Gradient Descent(41/49): loss=1.8355865174323875e+78\n",
      "Gradient Descent(42/49): loss=1.627384725210304e+80\n",
      "Gradient Descent(43/49): loss=1.442798265674968e+82\n",
      "Gradient Descent(44/49): loss=1.279148564679872e+84\n",
      "Gradient Descent(45/49): loss=1.1340608659223189e+86\n",
      "Gradient Descent(46/49): loss=1.0054297703397292e+88\n",
      "Gradient Descent(47/49): loss=8.913886842072241e+89\n",
      "Gradient Descent(48/49): loss=7.902827325912596e+91\n",
      "Gradient Descent(49/49): loss=7.006447451005797e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.888693603179469\n",
      "Gradient Descent(2/49): loss=204.81128742546142\n",
      "Gradient Descent(3/49): loss=17806.660838498152\n",
      "Gradient Descent(4/49): loss=1573033.375823596\n",
      "Gradient Descent(5/49): loss=139508147.44921085\n",
      "Gradient Descent(6/49): loss=12386987248.675655\n",
      "Gradient Descent(7/49): loss=1100239047870.2922\n",
      "Gradient Descent(8/49): loss=97736540039518.22\n",
      "Gradient Descent(9/49): loss=8682447157781874.0\n",
      "Gradient Descent(10/49): loss=7.713156827493344e+17\n",
      "Gradient Descent(11/49): loss=6.8521001125977465e+19\n",
      "Gradient Descent(12/49): loss=6.08717405063738e+21\n",
      "Gradient Descent(13/49): loss=5.407641500862899e+23\n",
      "Gradient Descent(14/49): loss=4.8039680823134375e+25\n",
      "Gradient Descent(15/49): loss=4.2676849312759826e+27\n",
      "Gradient Descent(16/49): loss=3.791268918331584e+29\n",
      "Gradient Descent(17/49): loss=3.3680368380340793e+31\n",
      "Gradient Descent(18/49): loss=2.992051579387185e+33\n",
      "Gradient Descent(19/49): loss=2.6580388182718586e+35\n",
      "Gradient Descent(20/49): loss=2.3613130231109776e+37\n",
      "Gradient Descent(21/49): loss=2.0977117245168977e+39\n",
      "Gradient Descent(22/49): loss=1.8635371236919893e+41\n",
      "Gradient Descent(23/49): loss=1.655504219575388e+43\n",
      "Gradient Descent(24/49): loss=1.4706947268134775e+45\n",
      "Gradient Descent(25/49): loss=1.3065161380455276e+47\n",
      "Gradient Descent(26/49): loss=1.160665356210321e+49\n",
      "Gradient Descent(27/49): loss=1.0310963867022085e+51\n",
      "Gradient Descent(28/49): loss=9.159916361609037e+52\n",
      "Gradient Descent(29/49): loss=8.137364152737107e+54\n",
      "Gradient Descent(30/49): loss=7.228962879156635e+56\n",
      "Gradient Descent(31/49): loss=6.421969488811264e+58\n",
      "Gradient Descent(32/49): loss=5.705063479318107e+60\n",
      "Gradient Descent(33/49): loss=5.0681880939727455e+62\n",
      "Gradient Descent(34/49): loss=4.5024092455772284e+64\n",
      "Gradient Descent(35/49): loss=3.99979018907508e+66\n",
      "Gradient Descent(36/49): loss=3.5532801849002745e+68\n",
      "Gradient Descent(37/49): loss=3.156615591210447e+70\n",
      "Gradient Descent(38/49): loss=2.8042319975261074e+72\n",
      "Gradient Descent(39/49): loss=2.491186167186668e+74\n",
      "Gradient Descent(40/49): loss=2.2130866936320437e+76\n",
      "Gradient Descent(41/49): loss=1.9660323977562455e+78\n",
      "Gradient Descent(42/49): loss=1.7465576021712926e+80\n",
      "Gradient Descent(43/49): loss=1.5515835146886308e+82\n",
      "Gradient Descent(44/49): loss=1.3783750390257112e+84\n",
      "Gradient Descent(45/49): loss=1.2245024068784412e+86\n",
      "Gradient Descent(46/49): loss=1.0878070931340458e+88\n",
      "Gradient Descent(47/49): loss=9.663715360832375e+89\n",
      "Gradient Descent(48/49): loss=8.584922378666669e+91\n",
      "Gradient Descent(49/49): loss=7.626558678088311e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8974541069425697\n",
      "Gradient Descent(2/49): loss=203.9966131566036\n",
      "Gradient Descent(3/49): loss=17577.261489457447\n",
      "Gradient Descent(4/49): loss=1538078.5534741574\n",
      "Gradient Descent(5/49): loss=135091302.89994112\n",
      "Gradient Descent(6/49): loss=11878272573.925959\n",
      "Gradient Descent(7/49): loss=1044783172204.0625\n",
      "Gradient Descent(8/49): loss=91906288772027.36\n",
      "Gradient Descent(9/49): loss=8084979464508163.0\n",
      "Gradient Descent(10/49): loss=7.112417890893812e+17\n",
      "Gradient Descent(11/49): loss=6.256869431674222e+19\n",
      "Gradient Descent(12/49): loss=5.504240366528943e+21\n",
      "Gradient Descent(13/49): loss=4.842145556464325e+23\n",
      "Gradient Descent(14/49): loss=4.259693367351934e+25\n",
      "Gradient Descent(15/49): loss=3.7473033414145147e+27\n",
      "Gradient Descent(16/49): loss=3.296547727590513e+29\n",
      "Gradient Descent(17/49): loss=2.900012614169299e+31\n",
      "Gradient Descent(18/49): loss=2.5511759170241823e+33\n",
      "Gradient Descent(19/49): loss=2.2443000868944682e+35\n",
      "Gradient Descent(20/49): loss=1.9743377346140927e+37\n",
      "Gradient Descent(21/49): loss=1.7368486118357512e+39\n",
      "Gradient Descent(22/49): loss=1.5279265789148394e+41\n",
      "Gradient Descent(23/49): loss=1.3441353579433108e+43\n",
      "Gradient Descent(24/49): loss=1.182452013995798e+45\n",
      "Gradient Descent(25/49): loss=1.0402172349239154e+47\n",
      "Gradient Descent(26/49): loss=9.150915919000986e+48\n",
      "Gradient Descent(27/49): loss=8.050170612944424e+50\n",
      "Gradient Descent(28/49): loss=7.081831750082344e+52\n",
      "Gradient Descent(29/49): loss=6.229972425159735e+54\n",
      "Gradient Descent(30/49): loss=5.480581548382457e+56\n",
      "Gradient Descent(31/49): loss=4.821333395821575e+58\n",
      "Gradient Descent(32/49): loss=4.241384880136517e+60\n",
      "Gradient Descent(33/49): loss=3.731197207196094e+62\n",
      "Gradient Descent(34/49): loss=3.282378985266656e+64\n",
      "Gradient Descent(35/49): loss=2.887548206281109e+66\n",
      "Gradient Descent(36/49): loss=2.5402108291038383e+68\n",
      "Gradient Descent(37/49): loss=2.234653967771117e+70\n",
      "Gradient Descent(38/49): loss=1.9658519279035164e+72\n",
      "Gradient Descent(39/49): loss=1.7293835458098181e+74\n",
      "Gradient Descent(40/49): loss=1.5213594707040026e+76\n",
      "Gradient Descent(41/49): loss=1.3383581939985099e+78\n",
      "Gradient Descent(42/49): loss=1.1773697735053272e+80\n",
      "Gradient Descent(43/49): loss=1.0357463269399773e+82\n",
      "Gradient Descent(44/49): loss=9.111584804625545e+83\n",
      "Gradient Descent(45/49): loss=8.015570559362851e+85\n",
      "Gradient Descent(46/49): loss=7.051393667488902e+87\n",
      "Gradient Descent(47/49): loss=6.203195678419042e+89\n",
      "Gradient Descent(48/49): loss=5.4570257227547456e+91\n",
      "Gradient Descent(49/49): loss=4.800611053171946e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9036579827785287\n",
      "Gradient Descent(2/49): loss=205.47567096204088\n",
      "Gradient Descent(3/49): loss=17773.292318910193\n",
      "Gradient Descent(4/49): loss=1561183.8874986772\n",
      "Gradient Descent(5/49): loss=137660377.50194132\n",
      "Gradient Descent(6/49): loss=12152613250.504946\n",
      "Gradient Descent(7/49): loss=1073224509469.7638\n",
      "Gradient Descent(8/49): loss=94790131950124.92\n",
      "Gradient Descent(9/49): loss=8372447812061790.0\n",
      "Gradient Descent(10/49): loss=7.395154262369222e+17\n",
      "Gradient Descent(11/49): loss=6.531964423470361e+19\n",
      "Gradient Descent(12/49): loss=5.769537044650365e+21\n",
      "Gradient Descent(13/49): loss=5.096104347262846e+23\n",
      "Gradient Descent(14/49): loss=4.5012767981249015e+25\n",
      "Gradient Descent(15/49): loss=3.9758789027630035e+27\n",
      "Gradient Descent(16/49): loss=3.511806538407805e+29\n",
      "Gradient Descent(17/49): loss=3.1019016238536816e+31\n",
      "Gradient Descent(18/49): loss=2.739841615565699e+33\n",
      "Gradient Descent(19/49): loss=2.4200419589888515e+35\n",
      "Gradient Descent(20/49): loss=2.137569942369028e+37\n",
      "Gradient Descent(21/49): loss=1.8880686104666447e+39\n",
      "Gradient Descent(22/49): loss=1.6676895605786717e+41\n",
      "Gradient Descent(23/49): loss=1.4730335831347553e+43\n",
      "Gradient Descent(24/49): loss=1.3010982309526509e+45\n",
      "Gradient Descent(25/49): loss=1.1492315083452902e+47\n",
      "Gradient Descent(26/49): loss=1.0150909657350309e+49\n",
      "Gradient Descent(27/49): loss=8.966075688270795e+50\n",
      "Gradient Descent(28/49): loss=7.91953785044197e+52\n",
      "Gradient Descent(29/49): loss=6.99515394975208e+54\n",
      "Gradient Descent(30/49): loss=6.178665940462846e+56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=5.457480003737267e+58\n",
      "Gradient Descent(32/49): loss=4.820472295830384e+60\n",
      "Gradient Descent(33/49): loss=4.257817369730272e+62\n",
      "Gradient Descent(34/49): loss=3.7608366237595093e+64\n",
      "Gradient Descent(35/49): loss=3.3218644395512853e+66\n",
      "Gradient Descent(36/49): loss=2.9341299446622773e+68\n",
      "Gradient Descent(37/49): loss=2.591652576083684e+70\n",
      "Gradient Descent(38/49): loss=2.2891498337829392e+72\n",
      "Gradient Descent(39/49): loss=2.0219558014319407e+74\n",
      "Gradient Descent(40/49): loss=1.785949177554779e+76\n",
      "Gradient Descent(41/49): loss=1.5774897070201525e+78\n",
      "Gradient Descent(42/49): loss=1.393362032374067e+80\n",
      "Gradient Descent(43/49): loss=1.2307260989543836e+82\n",
      "Gradient Descent(44/49): loss=1.0870733488170942e+84\n",
      "Gradient Descent(45/49): loss=9.601880277930278e+85\n",
      "Gradient Descent(46/49): loss=8.481130088602563e+87\n",
      "Gradient Descent(47/49): loss=7.491196046791853e+89\n",
      "Gradient Descent(48/49): loss=6.616809036673635e+91\n",
      "Gradient Descent(49/49): loss=5.844482183396556e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9381682410362635\n",
      "Gradient Descent(2/49): loss=212.65726229743322\n",
      "Gradient Descent(3/49): loss=18774.016976473864\n",
      "Gradient Descent(4/49): loss=1681862.2314726717\n",
      "Gradient Descent(5/49): loss=151192256.44449893\n",
      "Gradient Descent(6/49): loss=13605134999.356962\n",
      "Gradient Descent(7/49): loss=1224636052896.658\n",
      "Gradient Descent(8/49): loss=110243093126257.58\n",
      "Gradient Descent(9/49): loss=9924489074508480.0\n",
      "Gradient Descent(10/49): loss=8.934469657700618e+17\n",
      "Gradient Descent(11/49): loss=8.04323198690056e+19\n",
      "Gradient Descent(12/49): loss=7.240903899084199e+21\n",
      "Gradient Descent(13/49): loss=6.518611338481243e+23\n",
      "Gradient Descent(14/49): loss=5.868369182818177e+25\n",
      "Gradient Descent(15/49): loss=5.282989882847223e+27\n",
      "Gradient Descent(16/49): loss=4.75600315111301e+29\n",
      "Gradient Descent(17/49): loss=4.2815842034923945e+31\n",
      "Gradient Descent(18/49): loss=3.8544893103083427e+33\n",
      "Gradient Descent(19/49): loss=3.469997818735157e+35\n",
      "Gradient Descent(20/49): loss=3.123859970430825e+37\n",
      "Gradient Descent(21/49): loss=2.8122499277661333e+39\n",
      "Gradient Descent(22/49): loss=2.5317234866922972e+41\n",
      "Gradient Descent(23/49): loss=2.2791800080781665e+43\n",
      "Gradient Descent(24/49): loss=2.051828146530179e+45\n",
      "Gradient Descent(25/49): loss=1.8471549978381572e+47\n",
      "Gradient Descent(26/49): loss=1.6628983240183399e+49\n",
      "Gradient Descent(27/49): loss=1.4970215489546884e+51\n",
      "Gradient Descent(28/49): loss=1.3476912482654003e+53\n",
      "Gradient Descent(29/49): loss=1.2132568845915302e+55\n",
      "Gradient Descent(30/49): loss=1.0922325643231203e+57\n",
      "Gradient Descent(31/49): loss=9.832806141211411e+58\n",
      "Gradient Descent(32/49): loss=8.851967957077125e+60\n",
      "Gradient Descent(33/49): loss=7.968969955047524e+62\n",
      "Gradient Descent(34/49): loss=7.174052420024669e+64\n",
      "Gradient Descent(35/49): loss=6.4584291841460616e+66\n",
      "Gradient Descent(36/49): loss=5.814190513886169e+68\n",
      "Gradient Descent(37/49): loss=5.234215684325627e+70\n",
      "Gradient Descent(38/49): loss=4.7120942742773285e+72\n",
      "Gradient Descent(39/49): loss=4.2420553123496095e+74\n",
      "Gradient Descent(40/49): loss=3.818903490803636e+76\n",
      "Gradient Descent(41/49): loss=3.4379617421806554e+78\n",
      "Gradient Descent(42/49): loss=3.095019544002825e+80\n",
      "Gradient Descent(43/49): loss=2.786286380163018e+82\n",
      "Gradient Descent(44/49): loss=2.508349844615669e+84\n",
      "Gradient Descent(45/49): loss=2.2581379242916565e+86\n",
      "Gradient Descent(46/49): loss=2.032885044352942e+88\n",
      "Gradient Descent(47/49): loss=1.8301015004874904e+90\n",
      "Gradient Descent(48/49): loss=1.647545940381804e+92\n",
      "Gradient Descent(49/49): loss=1.4832005902107207e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9279823250131396\n",
      "Gradient Descent(2/49): loss=211.18100527771028\n",
      "Gradient Descent(3/49): loss=18643.783164994173\n",
      "Gradient Descent(4/49): loss=1672354.341735122\n",
      "Gradient Descent(5/49): loss=150602844.36948553\n",
      "Gradient Descent(6/49): loss=13578269849.517982\n",
      "Gradient Descent(7/49): loss=1224649150862.5112\n",
      "Gradient Descent(8/49): loss=110465779970066.23\n",
      "Gradient Descent(9/49): loss=9964585303049376.0\n",
      "Gradient Descent(10/49): loss=8.988672535381151e+17\n",
      "Gradient Descent(11/49): loss=8.108367565426904e+19\n",
      "Gradient Descent(12/49): loss=7.314283402833667e+21\n",
      "Gradient Descent(13/49): loss=6.597969353504202e+23\n",
      "Gradient Descent(14/49): loss=5.9518071819338456e+25\n",
      "Gradient Descent(15/49): loss=5.368926118808082e+27\n",
      "Gradient Descent(16/49): loss=4.8431286688847554e+29\n",
      "Gradient Descent(17/49): loss=4.36882439209418e+31\n",
      "Gradient Descent(18/49): loss=3.9409703717318694e+33\n",
      "Gradient Descent(19/49): loss=3.555017570525602e+35\n",
      "Gradient Descent(20/49): loss=3.2068624567270545e+37\n",
      "Gradient Descent(21/49): loss=2.89280337233741e+39\n",
      "Gradient Descent(22/49): loss=2.609501175688933e+41\n",
      "Gradient Descent(23/49): loss=2.3539437388184416e+43\n",
      "Gradient Descent(24/49): loss=2.1234139218450174e+45\n",
      "Gradient Descent(25/49): loss=1.9154606837587863e+47\n",
      "Gradient Descent(26/49): loss=1.7278730224383952e+49\n",
      "Gradient Descent(27/49): loss=1.5586564668149846e+51\n",
      "Gradient Descent(28/49): loss=1.40601187124028e+53\n",
      "Gradient Descent(29/49): loss=1.2683162865953435e+55\n",
      "Gradient Descent(30/49): loss=1.1441057047576644e+57\n",
      "Gradient Descent(31/49): loss=1.032059492961991e+59\n",
      "Gradient Descent(32/49): loss=9.309863525578512e+60\n",
      "Gradient Descent(33/49): loss=8.398116528742536e+62\n",
      "Gradient Descent(34/49): loss=7.575660055226835e+64\n",
      "Gradient Descent(35/49): loss=6.833749576579503e+66\n",
      "Gradient Descent(36/49): loss=6.164496946134672e+68\n",
      "Gradient Descent(37/49): loss=5.560786530595182e+70\n",
      "Gradient Descent(38/49): loss=5.016199555137756e+72\n",
      "Gradient Descent(39/49): loss=4.524945857662885e+74\n",
      "Gradient Descent(40/49): loss=4.0818023265859056e+76\n",
      "Gradient Descent(41/49): loss=3.6820573676272737e+78\n",
      "Gradient Descent(42/49): loss=3.3214608091612444e+80\n",
      "Gradient Descent(43/49): loss=2.996178713506351e+82\n",
      "Gradient Descent(44/49): loss=2.7027526137017776e+84\n",
      "Gradient Descent(45/49): loss=2.4380627423666265e+86\n",
      "Gradient Descent(46/49): loss=2.1992948616836045e+88\n",
      "Gradient Descent(47/49): loss=1.9839103418367036e+90\n",
      "Gradient Descent(48/49): loss=1.7896191697704475e+92\n",
      "Gradient Descent(49/49): loss=1.6143556012943524e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.936935486744649\n",
      "Gradient Descent(2/49): loss=210.34361550701666\n",
      "Gradient Descent(3/49): loss=18403.972915439263\n",
      "Gradient Descent(4/49): loss=1635238.508028138\n",
      "Gradient Descent(5/49): loss=145839941.7305054\n",
      "Gradient Descent(6/49): loss=13021190657.350853\n",
      "Gradient Descent(7/49): loss=1162980960351.186\n",
      "Gradient Descent(8/49): loss=103882157289196.69\n",
      "Gradient Descent(9/49): loss=9279488715036348.0\n",
      "Gradient Descent(10/49): loss=8.289184882609907e+17\n",
      "Gradient Descent(11/49): loss=7.404591431436042e+19\n",
      "Gradient Descent(12/49): loss=6.61440603431107e+21\n",
      "Gradient Descent(13/49): loss=5.9085478156869706e+23\n",
      "Gradient Descent(14/49): loss=5.278016048104264e+25\n",
      "Gradient Descent(15/49): loss=4.714771762595289e+27\n",
      "Gradient Descent(16/49): loss=4.211634224733396e+29\n",
      "Gradient Descent(17/49): loss=3.7621890943368387e+31\n",
      "Gradient Descent(18/49): loss=3.3607065672689954e+33\n",
      "Gradient Descent(19/49): loss=3.0020683044550393e+35\n",
      "Gradient Descent(20/49): loss=2.6817021734225904e+37\n",
      "Gradient Descent(21/49): loss=2.3955239581121964e+39\n",
      "Gradient Descent(22/49): loss=2.1398852903321643e+41\n",
      "Gradient Descent(23/49): loss=1.9115271380571696e+43\n",
      "Gradient Descent(24/49): loss=1.7075382573270387e+45\n",
      "Gradient Descent(25/49): loss=1.5253180779845071e+47\n",
      "Gradient Descent(26/49): loss=1.3625435500746427e+49\n",
      "Gradient Descent(27/49): loss=1.2171395282374256e+51\n",
      "Gradient Descent(28/49): loss=1.0872523165346767e+53\n",
      "Gradient Descent(29/49): loss=9.712260364445576e+54\n",
      "Gradient Descent(30/49): loss=8.675815167488042e+56\n",
      "Gradient Descent(31/49): loss=7.749974361886062e+58\n",
      "Gradient Descent(32/49): loss=6.922934784845295e+60\n",
      "Gradient Descent(33/49): loss=6.184152849707933e+62\n",
      "Gradient Descent(34/49): loss=5.524210130112502e+64\n",
      "Gradient Descent(35/49): loss=4.934693288358598e+66\n",
      "Gradient Descent(36/49): loss=4.408086817232567e+68\n",
      "Gradient Descent(37/49): loss=3.937677227903847e+70\n",
      "Gradient Descent(38/49): loss=3.5174674624232676e+72\n",
      "Gradient Descent(39/49): loss=3.1421004397033055e+74\n",
      "Gradient Descent(40/49): loss=2.8067907602994897e+76\n",
      "Gradient Descent(41/49): loss=2.5072636993254405e+78\n",
      "Gradient Descent(42/49): loss=2.2397007097473623e+80\n",
      "Gradient Descent(43/49): loss=2.0006907413019302e+82\n",
      "Gradient Descent(44/49): loss=1.7871867544225425e+84\n",
      "Gradient Descent(45/49): loss=1.5964668747878925e+86\n",
      "Gradient Descent(46/49): loss=1.4260996932682316e+88\n",
      "Gradient Descent(47/49): loss=1.2739132688925635e+90\n",
      "Gradient Descent(48/49): loss=1.137967439668535e+92\n",
      "Gradient Descent(49/49): loss=1.0165290882569328e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9429063832645888\n",
      "Gradient Descent(2/49): loss=211.82266818917066\n",
      "Gradient Descent(3/49): loss=18603.19375587032\n",
      "Gradient Descent(4/49): loss=1659086.375398172\n",
      "Gradient Descent(5/49): loss=148533137.7715799\n",
      "Gradient Descent(6/49): loss=13313289208.744106\n",
      "Gradient Descent(7/49): loss=1193736008417.7363\n",
      "Gradient Descent(8/49): loss=107049132252340.38\n",
      "Gradient Descent(9/49): loss=9600080953173938.0\n",
      "Gradient Descent(10/49): loss=8.60938538728601e+17\n",
      "Gradient Descent(11/49): loss=7.72095822031434e+19\n",
      "Gradient Descent(12/49): loss=6.924219781443019e+21\n",
      "Gradient Descent(13/49): loss=6.209700849564008e+23\n",
      "Gradient Descent(14/49): loss=5.568914829935025e+25\n",
      "Gradient Descent(15/49): loss=4.994252477437478e+27\n",
      "Gradient Descent(16/49): loss=4.478890224837929e+29\n",
      "Gradient Descent(17/49): loss=4.0167087744146276e+31\n",
      "Gradient Descent(18/49): loss=3.602220325855253e+33\n",
      "Gradient Descent(19/49): loss=3.2305033826916837e+35\n",
      "Gradient Descent(20/49): loss=2.897144306394828e+37\n",
      "Gradient Descent(21/49): loss=2.5981849075077195e+39\n",
      "Gradient Descent(22/49): loss=2.3300754466487395e+41\n",
      "Gradient Descent(23/49): loss=2.0896324859022182e+43\n",
      "Gradient Descent(24/49): loss=1.8740010897187792e+45\n",
      "Gradient Descent(25/49): loss=1.6806209263889873e+47\n",
      "Gradient Descent(26/49): loss=1.5071958675547233e+49\n",
      "Gradient Descent(27/49): loss=1.3516667247831277e+51\n",
      "Gradient Descent(28/49): loss=1.2121867994835545e+53\n",
      "Gradient Descent(29/49): loss=1.0870999558548399e+55\n",
      "Gradient Descent(30/49): loss=9.749209565085889e+56\n",
      "Gradient Descent(31/49): loss=8.743178272804042e+58\n",
      "Gradient Descent(32/49): loss=7.840960418349525e+60\n",
      "Gradient Descent(33/49): loss=7.031843382784662e+62\n",
      "Gradient Descent(34/49): loss=6.306219993700806e+64\n",
      "Gradient Descent(35/49): loss=5.655474453016495e+66\n",
      "Gradient Descent(36/49): loss=5.071880036007483e+68\n",
      "Gradient Descent(37/49): loss=4.5485073468823934e+70\n",
      "Gradient Descent(38/49): loss=4.079142041563186e+72\n",
      "Gradient Descent(39/49): loss=3.658211040739112e+74\n",
      "Gradient Descent(40/49): loss=3.280716357074253e+76\n",
      "Gradient Descent(41/49): loss=2.9421757508555307e+78\n",
      "Gradient Descent(42/49): loss=2.6385695094476416e+80\n",
      "Gradient Descent(43/49): loss=2.366292718632588e+82\n",
      "Gradient Descent(44/49): loss=2.1221124591202275e+84\n",
      "Gradient Descent(45/49): loss=1.903129419996545e+86\n",
      "Gradient Descent(46/49): loss=1.7067434733208888e+88\n",
      "Gradient Descent(47/49): loss=1.5306228011170976e+90\n",
      "Gradient Descent(48/49): loss=1.372676208183199e+92\n",
      "Gradient Descent(49/49): loss=1.231028291971756e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.977966998541463\n",
      "Gradient Descent(2/49): loss=219.21985555184824\n",
      "Gradient Descent(3/49): loss=19649.780185120286\n",
      "Gradient Descent(4/49): loss=1787231.0290783707\n",
      "Gradient Descent(5/49): loss=163122414.5694819\n",
      "Gradient Descent(6/49): loss=14903299738.97606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=1362017684093.3955\n",
      "Gradient Descent(8/49): loss=124486844547904.11\n",
      "Gradient Descent(9/49): loss=1.1378281590727392e+16\n",
      "Gradient Descent(10/49): loss=1.0400010553729656e+18\n",
      "Gradient Descent(11/49): loss=9.505874802646016e+19\n",
      "Gradient Descent(12/49): loss=8.688619445845964e+21\n",
      "Gradient Descent(13/49): loss=7.941628704817667e+23\n",
      "Gradient Descent(14/49): loss=7.258859956068641e+25\n",
      "Gradient Descent(15/49): loss=6.634791324817532e+27\n",
      "Gradient Descent(16/49): loss=6.064376024265903e+29\n",
      "Gradient Descent(17/49): loss=5.543001257478559e+31\n",
      "Gradient Descent(18/49): loss=5.066450833790177e+33\n",
      "Gradient Descent(19/49): loss=4.630871050743583e+35\n",
      "Gradient Descent(20/49): loss=4.232739523933141e+37\n",
      "Gradient Descent(21/49): loss=3.868836700821868e+39\n",
      "Gradient Descent(22/49): loss=3.536219824797564e+41\n",
      "Gradient Descent(23/49): loss=3.2321991379640347e+43\n",
      "Gradient Descent(24/49): loss=2.954316129954113e+45\n",
      "Gradient Descent(25/49): loss=2.7003236567926137e+47\n",
      "Gradient Descent(26/49): loss=2.4681677690151506e+49\n",
      "Gradient Descent(27/49): loss=2.2559711020867443e+51\n",
      "Gradient Descent(28/49): loss=2.062017694802536e+53\n",
      "Gradient Descent(29/49): loss=1.884739112901675e+55\n",
      "Gradient Descent(30/49): loss=1.7227017656807996e+57\n",
      "Gradient Descent(31/49): loss=1.5745953130408523e+59\n",
      "Gradient Descent(32/49): loss=1.439222069218923e+61\n",
      "Gradient Descent(33/49): loss=1.315487317516905e+63\n",
      "Gradient Descent(34/49): loss=1.2023904577053777e+65\n",
      "Gradient Descent(35/49): loss=1.09901691451493e+67\n",
      "Gradient Descent(36/49): loss=1.0045307417816057e+69\n",
      "Gradient Descent(37/49): loss=9.181678624388484e+70\n",
      "Gradient Descent(38/49): loss=8.392298896899319e+72\n",
      "Gradient Descent(39/49): loss=7.670784793950273e+74\n",
      "Gradient Descent(40/49): loss=7.011301680024583e+76\n",
      "Gradient Descent(41/49): loss=6.4085165428034875e+78\n",
      "Gradient Descent(42/49): loss=5.857554866936205e+80\n",
      "Gradient Descent(43/49): loss=5.353961215516847e+82\n",
      "Gradient Descent(44/49): loss=4.893663200503951e+84\n",
      "Gradient Descent(45/49): loss=4.472938550724006e+86\n",
      "Gradient Descent(46/49): loss=4.0883850111492614e+88\n",
      "Gradient Descent(47/49): loss=3.7368928300355256e+90\n",
      "Gradient Descent(48/49): loss=3.41561961143317e+92\n",
      "Gradient Descent(49/49): loss=3.1219673297122756e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.967584158182142\n",
      "Gradient Descent(2/49): loss=217.6984803365438\n",
      "Gradient Descent(3/49): loss=19513.44978451847\n",
      "Gradient Descent(4/49): loss=1777119.8924952443\n",
      "Gradient Descent(5/49): loss=162485422.0188952\n",
      "Gradient Descent(6/49): loss=14873745556.72786\n",
      "Gradient Descent(7/49): loss=1362018535898.1294\n",
      "Gradient Descent(8/49): loss=124736857095255.7\n",
      "Gradient Descent(9/49): loss=1.1424101956351902e+16\n",
      "Gradient Descent(10/49): loss=1.0462952312904012e+18\n",
      "Gradient Descent(11/49): loss=9.582701817356481e+19\n",
      "Gradient Descent(12/49): loss=8.776516938276618e+21\n",
      "Gradient Descent(13/49): loss=8.03815862290641e+23\n",
      "Gradient Descent(14/49): loss=7.3619183853401734e+25\n",
      "Gradient Descent(15/49): loss=6.742569637202576e+27\n",
      "Gradient Descent(16/49): loss=6.17532597531023e+29\n",
      "Gradient Descent(17/49): loss=5.655803809482511e+31\n",
      "Gradient Descent(18/49): loss=5.179988375685931e+33\n",
      "Gradient Descent(19/49): loss=4.7442026785029504e+35\n",
      "Gradient Descent(20/49): loss=4.345079066772749e+37\n",
      "Gradient Descent(21/49): loss=3.979533206443189e+39\n",
      "Gradient Descent(22/49): loss=3.64474024476863e+41\n",
      "Gradient Descent(23/49): loss=3.3381129802804396e+43\n",
      "Gradient Descent(24/49): loss=3.0572818694346053e+45\n",
      "Gradient Descent(25/49): loss=2.8000767153159118e+47\n",
      "Gradient Descent(26/49): loss=2.564509896859853e+49\n",
      "Gradient Descent(27/49): loss=2.348761008981983e+51\n",
      "Gradient Descent(28/49): loss=2.151162794914198e+53\n",
      "Gradient Descent(29/49): loss=1.9701882620355553e+55\n",
      "Gradient Descent(30/49): loss=1.8044388816316903e+57\n",
      "Gradient Descent(31/49): loss=1.6526337813931413e+59\n",
      "Gradient Descent(32/49): loss=1.5135998471347944e+61\n",
      "Gradient Descent(33/49): loss=1.3862626572447377e+63\n",
      "Gradient Descent(34/49): loss=1.2696381798062478e+65\n",
      "Gradient Descent(35/49): loss=1.1628251682301077e+67\n",
      "Gradient Descent(36/49): loss=1.0649981966324623e+69\n",
      "Gradient Descent(37/49): loss=9.754012811373451e+70\n",
      "Gradient Descent(38/49): loss=8.933420378107121e+72\n",
      "Gradient Descent(39/49): loss=8.181863320798937e+74\n",
      "Gradient Descent(40/49): loss=7.493533782904696e+76\n",
      "Gradient Descent(41/49): loss=6.863112515310354e+78\n",
      "Gradient Descent(42/49): loss=6.285727770423365e+80\n",
      "Gradient Descent(43/49): loss=5.756917654450676e+82\n",
      "Gradient Descent(44/49): loss=5.272595646930716e+84\n",
      "Gradient Descent(45/49): loss=4.8290190210625163e+86\n",
      "Gradient Descent(46/49): loss=4.422759920791231e+88\n",
      "Gradient Descent(47/49): loss=4.0506788711412747e+90\n",
      "Gradient Descent(48/49): loss=3.709900517090448e+92\n",
      "Gradient Descent(49/49): loss=3.3977914035012555e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9767317961046373\n",
      "Gradient Descent(2/49): loss=216.83790854913133\n",
      "Gradient Descent(3/49): loss=19262.838275411945\n",
      "Gradient Descent(4/49): loss=1737727.0865011697\n",
      "Gradient Descent(5/49): loss=157352252.7690423\n",
      "Gradient Descent(6/49): loss=14264120325.471098\n",
      "Gradient Descent(7/49): loss=1293496956492.1255\n",
      "Gradient Descent(8/49): loss=117309327270381.64\n",
      "Gradient Descent(9/49): loss=1.0639335664267362e+16\n",
      "Gradient Descent(10/49): loss=9.649419888138911e+17\n",
      "Gradient Descent(11/49): loss=8.751639261005657e+19\n",
      "Gradient Descent(12/49): loss=7.937396822783244e+21\n",
      "Gradient Descent(13/49): loss=7.198913116164784e+23\n",
      "Gradient Descent(14/49): loss=6.529137585500206e+25\n",
      "Gradient Descent(15/49): loss=5.921677129061436e+27\n",
      "Gradient Descent(16/49): loss=5.370733878318019e+29\n",
      "Gradient Descent(17/49): loss=4.871049513460741e+31\n",
      "Gradient Descent(18/49): loss=4.417854976284639e+33\n",
      "Gradient Descent(19/49): loss=4.006824924468495e+35\n",
      "Gradient Descent(20/49): loss=3.6340364415451e+37\n",
      "Gradient Descent(21/49): loss=3.2959315937017043e+39\n",
      "Gradient Descent(22/49): loss=2.9892834717629348e+41\n",
      "Gradient Descent(23/49): loss=2.7111653930154286e+43\n",
      "Gradient Descent(24/49): loss=2.458922968573075e+45\n",
      "Gradient Descent(25/49): loss=2.2301487695864356e+47\n",
      "Gradient Descent(26/49): loss=2.0226593504776165e+49\n",
      "Gradient Descent(27/49): loss=1.834474409899301e+51\n",
      "Gradient Descent(28/49): loss=1.663797890525036e+53\n",
      "Gradient Descent(29/49): loss=1.5090008372848028e+55\n",
      "Gradient Descent(30/49): loss=1.3686058504423679e+57\n",
      "Gradient Descent(31/49): loss=1.2412729851332518e+59\n",
      "Gradient Descent(32/49): loss=1.1257869627866902e+61\n",
      "Gradient Descent(33/49): loss=1.0210455723761837e+63\n",
      "Gradient Descent(34/49): loss=9.260491507988296e+64\n",
      "Gradient Descent(35/49): loss=8.398910419830759e+66\n",
      "Gradient Descent(36/49): loss=7.617489436655813e+68\n",
      "Gradient Descent(37/49): loss=6.908770592499343e+70\n",
      "Gradient Descent(38/49): loss=6.265989798436545e+72\n",
      "Gradient Descent(39/49): loss=5.683012285389416e+74\n",
      "Gradient Descent(40/49): loss=5.154274053230258e+76\n",
      "Gradient Descent(41/49): loss=4.674728767365738e+78\n",
      "Gradient Descent(42/49): loss=4.239799595976284e+80\n",
      "Gradient Descent(43/49): loss=3.845335528240789e+82\n",
      "Gradient Descent(44/49): loss=3.4875717566424164e+84\n",
      "Gradient Descent(45/49): loss=3.163093745240605e+86\n",
      "Gradient Descent(46/49): loss=2.8688046409724557e+88\n",
      "Gradient Descent(47/49): loss=2.6018957169538635e+90\n",
      "Gradient Descent(48/49): loss=2.3598195656878336e+92\n",
      "Gradient Descent(49/49): loss=2.140265786333151e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9824653515435355\n",
      "Gradient Descent(2/49): loss=218.3159015535604\n",
      "Gradient Descent(3/49): loss=19465.14210933538\n",
      "Gradient Descent(4/49): loss=1762319.4734341463\n",
      "Gradient Descent(5/49): loss=160172770.18833995\n",
      "Gradient Descent(6/49): loss=14574786125.575886\n",
      "Gradient Descent(7/49): loss=1326714072856.8994\n",
      "Gradient Descent(8/49): loss=120782689465815.28\n",
      "Gradient Descent(9/49): loss=1.0996362631678254e+16\n",
      "Gradient Descent(10/49): loss=1.001149563260309e+18\n",
      "Gradient Descent(11/49): loss=9.114874361206096e+19\n",
      "Gradient Descent(12/49): loss=8.298565102202226e+21\n",
      "Gradient Descent(13/49): loss=7.555366191903315e+23\n",
      "Gradient Descent(14/49): loss=6.878727341166509e+25\n",
      "Gradient Descent(15/49): loss=6.262686798063443e+27\n",
      "Gradient Descent(16/49): loss=5.701817298676236e+29\n",
      "Gradient Descent(17/49): loss=5.191177800033193e+31\n",
      "Gradient Descent(18/49): loss=4.726269816323453e+33\n",
      "Gradient Descent(19/49): loss=4.30299774915395e+35\n",
      "Gradient Descent(20/49): loss=3.917632795447983e+37\n",
      "Gradient Descent(21/49): loss=3.566780095084555e+39\n",
      "Gradient Descent(22/49): loss=3.247348822921704e+41\n",
      "Gradient Descent(23/49): loss=2.956524959957682e+43\n",
      "Gradient Descent(24/49): loss=2.6917465032310386e+45\n",
      "Gradient Descent(25/49): loss=2.4506808959140627e+47\n",
      "Gradient Descent(26/49): loss=2.2312044787242817e+49\n",
      "Gradient Descent(27/49): loss=2.0313837816174858e+51\n",
      "Gradient Descent(28/49): loss=1.8494584909483792e+53\n",
      "Gradient Descent(29/49): loss=1.6838259420470068e+55\n",
      "Gradient Descent(30/49): loss=1.5330270005987517e+57\n",
      "Gradient Descent(31/49): loss=1.395733208450122e+59\n",
      "Gradient Descent(32/49): loss=1.2707350806017204e+61\n",
      "Gradient Descent(33/49): loss=1.1569314502912484e+63\n",
      "Gradient Descent(34/49): loss=1.0533197683023005e+65\n",
      "Gradient Descent(35/49): loss=9.589872710410899e+66\n",
      "Gradient Descent(36/49): loss=8.731029395765598e+68\n",
      "Gradient Descent(37/49): loss=7.949101788073321e+70\n",
      "Gradient Descent(38/49): loss=7.237201522628649e+72\n",
      "Gradient Descent(39/49): loss=6.589057138219617e+74\n",
      "Gradient Descent(40/49): loss=5.998958828902904e+76\n",
      "Gradient Descent(41/49): loss=5.46170814366259e+78\n",
      "Gradient Descent(42/49): loss=4.972572190832257e+80\n",
      "Gradient Descent(43/49): loss=4.52724194384667e+82\n",
      "Gradient Descent(44/49): loss=4.12179428101861e+84\n",
      "Gradient Descent(45/49): loss=3.7526574249315604e+86\n",
      "Gradient Descent(46/49): loss=3.416579476987821e+88\n",
      "Gradient Descent(47/49): loss=3.1105997699183236e+90\n",
      "Gradient Descent(48/49): loss=2.8320227858848125e+92\n",
      "Gradient Descent(49/49): loss=2.578394410407014e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0180806026784275\n",
      "Gradient Descent(2/49): loss=225.93349896698808\n",
      "Gradient Descent(3/49): loss=20559.327995148542\n",
      "Gradient Descent(4/49): loss=1898331.4771773783\n",
      "Gradient Descent(5/49): loss=175893228.18957007\n",
      "Gradient Descent(6/49): loss=16314121529.982752\n",
      "Gradient Descent(7/49): loss=1513597536016.241\n",
      "Gradient Descent(8/49): loss=140442246902602.08\n",
      "Gradient Descent(9/49): loss=1.3031599343402848e+16\n",
      "Gradient Descent(10/49): loss=1.2092095936927916e+18\n",
      "Gradient Descent(11/49): loss=1.1220356811416466e+20\n",
      "Gradient Descent(12/49): loss=1.041147189582299e+22\n",
      "Gradient Descent(13/49): loss=9.66090280140318e+23\n",
      "Gradient Descent(14/49): loss=8.964443432168227e+25\n",
      "Gradient Descent(15/49): loss=8.318192390974529e+27\n",
      "Gradient Descent(16/49): loss=7.718529961915518e+29\n",
      "Gradient Descent(17/49): loss=7.162097499752153e+31\n",
      "Gradient Descent(18/49): loss=6.645778521307556e+33\n",
      "Gradient Descent(19/49): loss=6.166681222477532e+35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/49): loss=5.722122273663547e+37\n",
      "Gradient Descent(21/49): loss=5.309611788623254e+39\n",
      "Gradient Descent(22/49): loss=4.9268393784339205e+41\n",
      "Gradient Descent(23/49): loss=4.571661211268707e+43\n",
      "Gradient Descent(24/49): loss=4.2420880051660546e+45\n",
      "Gradient Descent(25/49): loss=3.936273886442222e+47\n",
      "Gradient Descent(26/49): loss=3.6525060513169365e+49\n",
      "Gradient Descent(27/49): loss=3.389195172840231e+51\n",
      "Gradient Descent(28/49): loss=3.14486649938939e+53\n",
      "Gradient Descent(29/49): loss=2.918151594879533e+55\n",
      "Gradient Descent(30/49): loss=2.7077806744264955e+57\n",
      "Gradient Descent(31/49): loss=2.5125754925354745e+59\n",
      "Gradient Descent(32/49): loss=2.3314427439833195e+61\n",
      "Gradient Descent(33/49): loss=2.1633679404344237e+63\n",
      "Gradient Descent(34/49): loss=2.0074097284942618e+65\n",
      "Gradient Descent(35/49): loss=1.8626946173771205e+67\n",
      "Gradient Descent(36/49): loss=1.7284120866586635e+69\n",
      "Gradient Descent(37/49): loss=1.603810046713056e+71\n",
      "Gradient Descent(38/49): loss=1.4881906264091757e+73\n",
      "Gradient Descent(39/49): loss=1.3809062644738386e+75\n",
      "Gradient Descent(40/49): loss=1.2813560826304934e+77\n",
      "Gradient Descent(41/49): loss=1.1889825202000688e+79\n",
      "Gradient Descent(42/49): loss=1.1032682113149676e+81\n",
      "Gradient Descent(43/49): loss=1.0237330872562458e+83\n",
      "Gradient Descent(44/49): loss=9.499316876846104e+84\n",
      "Gradient Descent(45/49): loss=8.814506657060507e+86\n",
      "Gradient Descent(46/49): loss=8.179064727984983e+88\n",
      "Gradient Descent(47/49): loss=7.589432106331545e+90\n",
      "Gradient Descent(48/49): loss=7.04230637759074e+92\n",
      "Gradient Descent(49/49): loss=6.53462319986772e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0074991026864777\n",
      "Gradient Descent(2/49): loss=224.3659810520344\n",
      "Gradient Descent(3/49): loss=20416.665151109668\n",
      "Gradient Descent(4/49): loss=1887583.880410179\n",
      "Gradient Descent(5/49): loss=175205228.0188339\n",
      "Gradient Descent(6/49): loss=16281634164.098991\n",
      "Gradient Descent(7/49): loss=1513583470844.3994\n",
      "Gradient Descent(8/49): loss=140722696488942.31\n",
      "Gradient Descent(9/49): loss=1.3083909117746568e+16\n",
      "Gradient Descent(10/49): loss=1.2165104112530483e+18\n",
      "Gradient Descent(11/49): loss=1.1310862119287621e+20\n",
      "Gradient Descent(12/49): loss=1.0516617626764672e+22\n",
      "Gradient Descent(13/49): loss=9.77814823691489e+23\n",
      "Gradient Descent(14/49): loss=9.091534689339518e+25\n",
      "Gradient Descent(15/49): loss=8.453134891186359e+27\n",
      "Gradient Descent(16/49): loss=7.859563072720098e+29\n",
      "Gradient Descent(17/49): loss=7.307671380737948e+31\n",
      "Gradient Descent(18/49): loss=6.794533051934008e+33\n",
      "Gradient Descent(19/49): loss=6.317426852644194e+35\n",
      "Gradient Descent(20/49): loss=5.873822636252504e+37\n",
      "Gradient Descent(21/49): loss=5.461367922131581e+39\n",
      "Gradient Descent(22/49): loss=5.077875419178726e+41\n",
      "Gradient Descent(23/49): loss=4.72131142608184e+43\n",
      "Gradient Descent(24/49): loss=4.389785046298526e+45\n",
      "Gradient Descent(25/49): loss=4.08153816040593e+47\n",
      "Gradient Descent(26/49): loss=3.7949361025999753e+49\n",
      "Gradient Descent(27/49): loss=3.5284589918877395e+51\n",
      "Gradient Descent(28/49): loss=3.2806936719972342e+53\n",
      "Gradient Descent(29/49): loss=3.050326217260228e+55\n",
      "Gradient Descent(30/49): loss=2.8361349647255027e+57\n",
      "Gradient Descent(31/49): loss=2.636984035551212e+59\n",
      "Gradient Descent(32/49): loss=2.4518173113192916e+61\n",
      "Gradient Descent(33/49): loss=2.2796528333279976e+63\n",
      "Gradient Descent(34/49): loss=2.1195775951610464e+65\n",
      "Gradient Descent(35/49): loss=1.9707427009182206e+67\n",
      "Gradient Descent(36/49): loss=1.8323588634306747e+69\n",
      "Gradient Descent(37/49): loss=1.7036922185877357e+71\n",
      "Gradient Descent(38/49): loss=1.584060433577961e+73\n",
      "Gradient Descent(39/49): loss=1.472829088406133e+75\n",
      "Gradient Descent(40/49): loss=1.369408311497028e+77\n",
      "Gradient Descent(41/49): loss=1.2732496515441104e+79\n",
      "Gradient Descent(42/49): loss=1.183843169014324e+81\n",
      "Gradient Descent(43/49): loss=1.1007147318848718e+83\n",
      "Gradient Descent(44/49): loss=1.0234235012709841e+85\n",
      "Gradient Descent(45/49): loss=9.515595936107777e+86\n",
      "Gradient Descent(46/49): loss=8.847419070093782e+88\n",
      "Gradient Descent(47/49): loss=8.226161002153437e+90\n",
      "Gradient Descent(48/49): loss=7.648527135115384e+92\n",
      "Gradient Descent(49/49): loss=7.111454215554794e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.016843035022534\n",
      "Gradient Descent(2/49): loss=223.48175450279183\n",
      "Gradient Descent(3/49): loss=20154.85041456385\n",
      "Gradient Descent(4/49): loss=1845792.8501877787\n",
      "Gradient Descent(5/49): loss=169676090.1543468\n",
      "Gradient Descent(6/49): loss=15614955296.06428\n",
      "Gradient Descent(7/49): loss=1437506844050.697\n",
      "Gradient Descent(8/49): loss=132350649543314.78\n",
      "Gradient Descent(9/49): loss=1.2185887550085406e+16\n",
      "Gradient Descent(10/49): loss=1.1220004204909755e+18\n",
      "Gradient Descent(11/49): loss=1.0330715358659574e+20\n",
      "Gradient Descent(12/49): loss=9.511921520459918e+21\n",
      "Gradient Descent(13/49): loss=8.758026943056822e+23\n",
      "Gradient Descent(14/49): loss=8.06388536408987e+25\n",
      "Gradient Descent(15/49): loss=7.424760162771013e+27\n",
      "Gradient Descent(16/49): loss=6.836290650573652e+29\n",
      "Gradient Descent(17/49): loss=6.294461909891618e+31\n",
      "Gradient Descent(18/49): loss=5.795577281278542e+33\n",
      "Gradient Descent(19/49): loss=5.336233107297023e+35\n",
      "Gradient Descent(20/49): loss=4.913295500501505e+37\n",
      "Gradient Descent(21/49): loss=4.5238789593332033e+39\n",
      "Gradient Descent(22/49): loss=4.165326680812591e+41\n",
      "Gradient Descent(23/49): loss=3.8351924341751854e+43\n",
      "Gradient Descent(24/49): loss=3.531223871330075e+45\n",
      "Gradient Descent(25/49): loss=3.2513471601427823e+47\n",
      "Gradient Descent(26/49): loss=2.993652835663495e+49\n",
      "Gradient Descent(27/49): loss=2.7563827727589935e+51\n",
      "Gradient Descent(28/49): loss=2.537918191265076e+53\n",
      "Gradient Descent(29/49): loss=2.3367686118235025e+55\n",
      "Gradient Descent(30/49): loss=2.1515616870540916e+57\n",
      "Gradient Descent(31/49): loss=1.981033838684878e+59\n",
      "Gradient Descent(32/49): loss=1.8240216367618725e+61\n",
      "Gradient Descent(33/49): loss=1.6794538621228893e+63\n",
      "Gradient Descent(34/49): loss=1.5463441979815278e+65\n",
      "Gradient Descent(35/49): loss=1.4237845007594317e+67\n",
      "Gradient Descent(36/49): loss=1.310938604256979e+69\n",
      "Gradient Descent(37/49): loss=1.207036614891216e+71\n",
      "Gradient Descent(38/49): loss=1.1113696590801142e+73\n",
      "Gradient Descent(39/49): loss=1.0232850469371755e+75\n",
      "Gradient Descent(40/49): loss=9.421818192804747e+76\n",
      "Gradient Descent(41/49): loss=8.675066475755562e+78\n",
      "Gradient Descent(42/49): loss=7.987500588394924e+80\n",
      "Gradient Descent(43/49): loss=7.354429597503713e+82\n",
      "Gradient Descent(44/49): loss=6.771534362479192e+84\n",
      "Gradient Descent(45/49): loss=6.234838067904025e+86\n",
      "Gradient Descent(46/49): loss=5.7406790916369e+88\n",
      "Gradient Descent(47/49): loss=5.285686023315708e+90\n",
      "Gradient Descent(48/49): loss=4.8667546628370654e+92\n",
      "Gradient Descent(49/49): loss=4.481026841883548e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0223348876153686\n",
      "Gradient Descent(2/49): loss=224.9576010907459\n",
      "Gradient Descent(3/49): loss=20360.11988813778\n",
      "Gradient Descent(4/49): loss=1871130.0553696696\n",
      "Gradient Descent(5/49): loss=172626982.57527775\n",
      "Gradient Descent(6/49): loss=15945003717.155293\n",
      "Gradient Descent(7/49): loss=1473339825088.3345\n",
      "Gradient Descent(8/49): loss=136155042131066.16\n",
      "Gradient Descent(9/49): loss=1.2582925826080938e+16\n",
      "Gradient Descent(10/49): loss=1.162880631419936e+18\n",
      "Gradient Descent(11/49): loss=1.0747079679228202e+20\n",
      "Gradient Descent(12/49): loss=9.932221591320862e+21\n",
      "Gradient Descent(13/49): loss=9.179151297588528e+23\n",
      "Gradient Descent(14/49): loss=8.4831807531151e+25\n",
      "Gradient Descent(15/49): loss=7.839979629302537e+27\n",
      "Gradient Descent(16/49): loss=7.245546612245221e+29\n",
      "Gradient Descent(17/49): loss=6.69618397833665e+31\n",
      "Gradient Descent(18/49): loss=6.188474430990242e+33\n",
      "Gradient Descent(19/49): loss=5.719259794591305e+35\n",
      "Gradient Descent(20/49): loss=5.285621354470631e+37\n",
      "Gradient Descent(21/49): loss=4.884861696777351e+39\n",
      "Gradient Descent(22/49): loss=4.51448792807124e+41\n",
      "Gradient Descent(23/49): loss=4.1721961680672213e+43\n",
      "Gradient Descent(24/49): loss=3.8558572184085305e+45\n",
      "Gradient Descent(25/49): loss=3.563503318122007e+47\n",
      "Gradient Descent(26/49): loss=3.2933159033079456e+49\n",
      "Gradient Descent(27/49): loss=3.0436142949062064e+51\n",
      "Gradient Descent(28/49): loss=2.8128452441664866e+53\n",
      "Gradient Descent(29/49): loss=2.599573270789187e+55\n",
      "Gradient Descent(30/49): loss=2.4024717336356993e+57\n",
      "Gradient Descent(31/49): loss=2.2203145784639894e+59\n",
      "Gradient Descent(32/49): loss=2.0519687113568485e+61\n",
      "Gradient Descent(33/49): loss=1.896386950402497e+63\n",
      "Gradient Descent(34/49): loss=1.7526015117836994e+65\n",
      "Gradient Descent(35/49): loss=1.6197179897565697e+67\n",
      "Gradient Descent(36/49): loss=1.4969097930715604e+69\n",
      "Gradient Descent(37/49): loss=1.3834130032292325e+71\n",
      "Gradient Descent(38/49): loss=1.2785216225866622e+73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=1.1815831827560016e+75\n",
      "Gradient Descent(40/49): loss=1.0919946859773682e+77\n",
      "Gradient Descent(41/49): loss=1.009198854219858e+79\n",
      "Gradient Descent(42/49): loss=9.326806626784114e+80\n",
      "Gradient Descent(43/49): loss=8.61964136103479e+82\n",
      "Gradient Descent(44/49): loss=7.966093880353146e+84\n",
      "Gradient Descent(45/49): loss=7.36209884525653e+86\n",
      "Gradient Descent(46/49): loss=6.803899153260394e+88\n",
      "Gradient Descent(47/49): loss=6.288022568124626e+90\n",
      "Gradient Descent(48/49): loss=5.811260121087744e+92\n",
      "Gradient Descent(49/49): loss=5.370646149734905e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0585090534471555\n",
      "Gradient Descent(2/49): loss=232.80049367997236\n",
      "Gradient Descent(3/49): loss=21503.695175540804\n",
      "Gradient Descent(4/49): loss=2015428.7703752173\n",
      "Gradient Descent(5/49): loss=189556946.09590027\n",
      "Gradient Descent(6/49): loss=17846419199.416847\n",
      "Gradient Descent(7/49): loss=1680719524395.1184\n",
      "Gradient Descent(8/49): loss=158299765673939.66\n",
      "Gradient Descent(9/49): loss=1.4910011481100152e+16\n",
      "Gradient Descent(10/49): loss=1.4043638112232668e+18\n",
      "Gradient Descent(11/49): loss=1.3227644399100838e+20\n",
      "Gradient Descent(12/49): loss=1.245907431009295e+22\n",
      "Gradient Descent(13/49): loss=1.173516392807918e+24\n",
      "Gradient Descent(14/49): loss=1.1053315906017677e+26\n",
      "Gradient Descent(15/49): loss=1.0411085566409384e+28\n",
      "Gradient Descent(16/49): loss=9.806170790593199e+29\n",
      "Gradient Descent(17/49): loss=9.236403371342216e+31\n",
      "Gradient Descent(18/49): loss=8.699741125183741e+33\n",
      "Gradient Descent(19/49): loss=8.194260538602753e+35\n",
      "Gradient Descent(20/49): loss=7.718149863728946e+37\n",
      "Gradient Descent(21/49): loss=7.269702621709208e+39\n",
      "Gradient Descent(22/49): loss=6.847311485470334e+41\n",
      "Gradient Descent(23/49): loss=6.449462518474104e+43\n",
      "Gradient Descent(24/49): loss=6.074729748383355e+45\n",
      "Gradient Descent(25/49): loss=5.7217700560624195e+47\n",
      "Gradient Descent(26/49): loss=5.389318361556353e+49\n",
      "Gradient Descent(27/49): loss=5.076183089782696e+51\n",
      "Gradient Descent(28/49): loss=4.78124189968149e+53\n",
      "Gradient Descent(29/49): loss=4.503437661514402e+55\n",
      "Gradient Descent(30/49): loss=4.241774667894817e+57\n",
      "Gradient Descent(31/49): loss=3.995315064968341e+59\n",
      "Gradient Descent(32/49): loss=3.763175490952043e+61\n",
      "Gradient Descent(33/49): loss=3.544523909984655e+63\n",
      "Gradient Descent(34/49): loss=3.338576629939316e+65\n",
      "Gradient Descent(35/49): loss=3.1445954935101975e+67\n",
      "Gradient Descent(36/49): loss=2.9618852325053448e+69\n",
      "Gradient Descent(37/49): loss=2.7897909758626786e+71\n",
      "Gradient Descent(38/49): loss=2.6276959024578882e+73\n",
      "Gradient Descent(39/49): loss=2.475019030290903e+75\n",
      "Gradient Descent(40/49): loss=2.331213134127244e+77\n",
      "Gradient Descent(41/49): loss=2.1957627841304277e+79\n",
      "Gradient Descent(42/49): loss=2.0681824984557653e+81\n",
      "Gradient Descent(43/49): loss=1.9480150031838338e+83\n",
      "Gradient Descent(44/49): loss=1.8348295933568268e+85\n",
      "Gradient Descent(45/49): loss=1.7282205892437286e+87\n",
      "Gradient Descent(46/49): loss=1.6278058823008515e+89\n",
      "Gradient Descent(47/49): loss=1.5332255656164837e+91\n",
      "Gradient Descent(48/49): loss=1.4441406439306085e+93\n",
      "Gradient Descent(49/49): loss=1.3602318186063824e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0477271585261447\n",
      "Gradient Descent(2/49): loss=231.1857931674243\n",
      "Gradient Descent(3/49): loss=21354.456794457736\n",
      "Gradient Descent(4/49): loss=2004009.9455112014\n",
      "Gradient Descent(5/49): loss=188814293.6009209\n",
      "Gradient Descent(6/49): loss=17810734550.25555\n",
      "Gradient Descent(7/49): loss=1680687486773.4485\n",
      "Gradient Descent(8/49): loss=158614090828818.8\n",
      "Gradient Descent(9/49): loss=1.4969671420186432e+16\n",
      "Gradient Descent(10/49): loss=1.4128229740921027e+18\n",
      "Gradient Descent(11/49): loss=1.3334133831323065e+20\n",
      "Gradient Descent(12/49): loss=1.2584685685945653e+22\n",
      "Gradient Descent(13/49): loss=1.1877364813424087e+24\n",
      "Gradient Descent(14/49): loss=1.1209800140853321e+26\n",
      "Gradient Descent(15/49): loss=1.0579756185950752e+28\n",
      "Gradient Descent(16/49): loss=9.985123808327987e+29\n",
      "Gradient Descent(17/49): loss=9.42391262034236e+31\n",
      "Gradient Descent(18/49): loss=8.894244166249803e+33\n",
      "Gradient Descent(19/49): loss=8.394345587204644e+35\n",
      "Gradient Descent(20/49): loss=7.92254367305015e+37\n",
      "Gradient Descent(21/49): loss=7.47725925764327e+39\n",
      "Gradient Descent(22/49): loss=7.057001931952133e+41\n",
      "Gradient Descent(23/49): loss=6.660365055131796e+43\n",
      "Gradient Descent(24/49): loss=6.286021046249623e+45\n",
      "Gradient Descent(25/49): loss=5.932716940712255e+47\n",
      "Gradient Descent(26/49): loss=5.599270196465206e+49\n",
      "Gradient Descent(27/49): loss=5.284564735910157e+51\n",
      "Gradient Descent(28/49): loss=4.987547210287412e+53\n",
      "Gradient Descent(29/49): loss=4.707223474018349e+55\n",
      "Gradient Descent(30/49): loss=4.442655257206589e+57\n",
      "Gradient Descent(31/49): loss=4.1929570251604325e+59\n",
      "Gradient Descent(32/49): loss=3.9572930144250244e+61\n",
      "Gradient Descent(33/49): loss=3.734874435403477e+63\n",
      "Gradient Descent(34/49): loss=3.5249568322039337e+65\n",
      "Gradient Descent(35/49): loss=3.326837590875775e+67\n",
      "Gradient Descent(36/49): loss=3.1398535876946007e+69\n",
      "Gradient Descent(37/49): loss=2.9633789696248417e+71\n",
      "Gradient Descent(38/49): loss=2.796823059530816e+73\n",
      "Gradient Descent(39/49): loss=2.6396283791247884e+75\n",
      "Gradient Descent(40/49): loss=2.491268783034782e+77\n",
      "Gradient Descent(41/49): loss=2.351247697746532e+79\n",
      "Gradient Descent(42/49): loss=2.219096459525303e+81\n",
      "Gradient Descent(43/49): loss=2.0943727457542386e+83\n",
      "Gradient Descent(44/49): loss=1.9766590944390296e+85\n",
      "Gradient Descent(45/49): loss=1.8655615069233803e+87\n",
      "Gradient Descent(46/49): loss=1.7607081291384345e+89\n",
      "Gradient Descent(47/49): loss=1.661748006972307e+91\n",
      "Gradient Descent(48/49): loss=1.568349911593617e+93\n",
      "Gradient Descent(49/49): loss=1.4802012307975074e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.057269203498338\n",
      "Gradient Descent(2/49): loss=230.27743284076436\n",
      "Gradient Descent(3/49): loss=21081.02499703098\n",
      "Gradient Descent(4/49): loss=1959693.9485235587\n",
      "Gradient Descent(5/49): loss=182861911.6464496\n",
      "Gradient Descent(6/49): loss=17082145665.698172\n",
      "Gradient Descent(7/49): loss=1596288362361.2273\n",
      "Gradient Descent(8/49): loss=149185809797048.56\n",
      "Gradient Descent(9/49): loss=1.3943079914774234e+16\n",
      "Gradient Descent(10/49): loss=1.3031509290111409e+18\n",
      "Gradient Descent(11/49): loss=1.2179578396606213e+20\n",
      "Gradient Descent(12/49): loss=1.1383355108067935e+22\n",
      "Gradient Descent(13/49): loss=1.0639187685023006e+24\n",
      "Gradient Descent(14/49): loss=9.943670092154422e+25\n",
      "Gradient Descent(15/49): loss=9.293621045580223e+27\n",
      "Gradient Descent(16/49): loss=8.686067856912123e+29\n",
      "Gradient Descent(17/49): loss=8.118232358791738e+31\n",
      "Gradient Descent(18/49): loss=7.587518057365248e+33\n",
      "Gradient Descent(19/49): loss=7.091498216482405e+35\n",
      "Gradient Descent(20/49): loss=6.627904748359269e+37\n",
      "Gradient Descent(21/49): loss=6.194617838721777e+39\n",
      "Gradient Descent(22/49): loss=5.789656252649496e+41\n",
      "Gradient Descent(23/49): loss=5.411168274893587e+43\n",
      "Gradient Descent(24/49): loss=5.0574232426759186e+45\n",
      "Gradient Descent(25/49): loss=4.7268036320811625e+47\n",
      "Gradient Descent(26/49): loss=4.4177976618063296e+49\n",
      "Gradient Descent(27/49): loss=4.128992380432207e+51\n",
      "Gradient Descent(28/49): loss=3.859067205603223e+53\n",
      "Gradient Descent(29/49): loss=3.6067878855720947e+55\n",
      "Gradient Descent(30/49): loss=3.3710008554971924e+57\n",
      "Gradient Descent(31/49): loss=3.150627962686618e+59\n",
      "Gradient Descent(32/49): loss=2.944661536669576e+61\n",
      "Gradient Descent(33/49): loss=2.752159781552644e+63\n",
      "Gradient Descent(34/49): loss=2.5722424695921304e+65\n",
      "Gradient Descent(35/49): loss=2.4040869162911565e+67\n",
      "Gradient Descent(36/49): loss=2.2469242186172197e+69\n",
      "Gradient Descent(37/49): loss=2.1000357391392788e+71\n",
      "Gradient Descent(38/49): loss=1.9627498200078645e+73\n",
      "Gradient Descent(39/49): loss=1.8344387117525275e+75\n",
      "Gradient Descent(40/49): loss=1.714515702853448e+77\n",
      "Gradient Descent(41/49): loss=1.602432436962e+79\n",
      "Gradient Descent(42/49): loss=1.4976764055029903e+81\n",
      "Gradient Descent(43/49): loss=1.399768604193292e+83\n",
      "Gradient Descent(44/49): loss=1.3082613427612877e+85\n",
      "Gradient Descent(45/49): loss=1.2227361978517575e+87\n",
      "Gradient Descent(46/49): loss=1.1428020997558246e+89\n",
      "Gradient Descent(47/49): loss=1.0680935442173415e+91\n",
      "Gradient Descent(48/49): loss=9.982689211391158e+92\n",
      "Gradient Descent(49/49): loss=9.33008952546829e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.062514991480087\n",
      "Gradient Descent(2/49): loss=231.75001372321165\n",
      "Gradient Descent(3/49): loss=21289.132023199923\n",
      "Gradient Descent(4/49): loss=1985774.450352742\n",
      "Gradient Descent(5/49): loss=185946059.82491863\n",
      "Gradient Descent(6/49): loss=17432394482.0397\n",
      "Gradient Descent(7/49): loss=1634896169762.5632\n",
      "Gradient Descent(8/49): loss=153347282703364.62\n",
      "Gradient Descent(9/49): loss=1.4383984567844016e+16\n",
      "Gradient Descent(10/49): loss=1.3492361424739305e+18\n",
      "Gradient Descent(11/49): loss=1.2656061326140501e+20\n",
      "Gradient Descent(12/49): loss=1.187161435775557e+22\n",
      "Gradient Descent(13/49): loss=1.1135794010282204e+24\n",
      "Gradient Descent(14/49): loss=1.0445582471328362e+26\n",
      "Gradient Descent(15/49): loss=9.798151657080524e+27\n",
      "Gradient Descent(16/49): loss=9.190849596807544e+29\n",
      "Gradient Descent(17/49): loss=8.62118894612823e+31\n",
      "Gradient Descent(18/49): loss=8.086836607553175e+33\n",
      "Gradient Descent(19/49): loss=7.585604115996868e+35\n",
      "Gradient Descent(20/49): loss=7.115438657638279e+37\n",
      "Gradient Descent(21/49): loss=6.674414656961756e+39\n",
      "Gradient Descent(22/49): loss=6.26072588878851e+41\n",
      "Gradient Descent(23/49): loss=5.872678080304978e+43\n",
      "Gradient Descent(24/49): loss=5.508681971962667e+45\n",
      "Gradient Descent(25/49): loss=5.167246808575629e+47\n",
      "Gradient Descent(26/49): loss=4.846974233879822e+49\n",
      "Gradient Descent(27/49): loss=4.5465525635251346e+51\n",
      "Gradient Descent(28/49): loss=4.264751413037931e+53\n",
      "Gradient Descent(29/49): loss=4.00041665875018e+55\n",
      "Gradient Descent(30/49): loss=3.752465711056821e+57\n",
      "Gradient Descent(31/49): loss=3.5198830806429936e+59\n",
      "Gradient Descent(32/49): loss=3.301716219522127e+61\n",
      "Gradient Descent(33/49): loss=3.0970716198516597e+63\n",
      "Gradient Descent(34/49): loss=2.905111154549456e+65\n",
      "Gradient Descent(35/49): loss=2.725048644723266e+67\n",
      "Gradient Descent(36/49): loss=2.5561466398554192e+69\n",
      "Gradient Descent(37/49): loss=2.39771339755577e+71\n",
      "Gradient Descent(38/49): loss=2.2491000505133643e+73\n",
      "Gradient Descent(39/49): loss=2.1096979490442176e+75\n",
      "Gradient Descent(40/49): loss=1.9789361683511876e+77\n",
      "Gradient Descent(41/49): loss=1.8562791702872335e+79\n",
      "Gradient Descent(42/49): loss=1.7412246100455096e+81\n",
      "Gradient Descent(43/49): loss=1.633301278793649e+83\n",
      "Gradient Descent(44/49): loss=1.5320671738261458e+85\n",
      "Gradient Descent(45/49): loss=1.4371076883312663e+87\n",
      "Gradient Descent(46/49): loss=1.3480339133584136e+89\n",
      "Gradient Descent(47/49): loss=1.2644810450318326e+91\n",
      "Gradient Descent(48/49): loss=1.186106890487169e+93\n",
      "Gradient Descent(49/49): loss=1.1125904664120357e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.099252350847646\n",
      "Gradient Descent(2/49): loss=239.82315823602744\n",
      "Gradient Descent(3/49): loss=22483.94008698781\n",
      "Gradient Descent(4/49): loss=2138798.2507607983\n",
      "Gradient Descent(5/49): loss=204168636.80698717\n",
      "Gradient Descent(6/49): loss=19509628095.947235\n",
      "Gradient Descent(7/49): loss=1864843267475.6533\n",
      "Gradient Descent(8/49): loss=178269393218716.5\n",
      "Gradient Descent(9/49): loss=1.704213494047411e+16\n",
      "Gradient Descent(10/49): loss=1.6292029580205266e+18\n",
      "Gradient Descent(11/49): loss=1.557498441382942e+20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=1.4889511131844677e+22\n",
      "Gradient Descent(13/49): loss=1.4234210293558477e+24\n",
      "Gradient Descent(14/49): loss=1.3607751020163289e+26\n",
      "Gradient Descent(15/49): loss=1.3008863086480585e+28\n",
      "Gradient Descent(16/49): loss=1.243633279198482e+30\n",
      "Gradient Descent(17/49): loss=1.1889000037450736e+32\n",
      "Gradient Descent(18/49): loss=1.136575583587059e+34\n",
      "Gradient Descent(19/49): loss=1.0865540024143634e+36\n",
      "Gradient Descent(20/49): loss=1.0387339102709126e+38\n",
      "Gradient Descent(21/49): loss=9.930184178387299e+39\n",
      "Gradient Descent(22/49): loss=9.493149000185258e+41\n",
      "Gradient Descent(23/49): loss=9.075348082272498e+43\n",
      "Gradient Descent(24/49): loss=8.675934909782047e+45\n",
      "Gradient Descent(25/49): loss=8.294100223640593e+47\n",
      "Gradient Descent(26/49): loss=7.92907038090357e+49\n",
      "Gradient Descent(27/49): loss=7.58010578725894e+51\n",
      "Gradient Descent(28/49): loss=7.2464993985195684e+53\n",
      "Gradient Descent(29/49): loss=6.927575288066501e+55\n",
      "Gradient Descent(30/49): loss=6.622687277340288e+57\n",
      "Gradient Descent(31/49): loss=6.33121762660571e+59\n",
      "Gradient Descent(32/49): loss=6.052575783336863e+61\n",
      "Gradient Descent(33/49): loss=5.78619718568667e+63\n",
      "Gradient Descent(34/49): loss=5.531542118616872e+65\n",
      "Gradient Descent(35/49): loss=5.288094620370468e+67\n",
      "Gradient Descent(36/49): loss=5.055361437071232e+69\n",
      "Gradient Descent(37/49): loss=4.832871023332126e+71\n",
      "Gradient Descent(38/49): loss=4.620172586847661e+73\n",
      "Gradient Descent(39/49): loss=4.416835175034555e+75\n",
      "Gradient Descent(40/49): loss=4.2224468018700526e+77\n",
      "Gradient Descent(41/49): loss=4.036613613158704e+79\n",
      "Gradient Descent(42/49): loss=3.858959088536443e+81\n",
      "Gradient Descent(43/49): loss=3.6891232785951996e+83\n",
      "Gradient Descent(44/49): loss=3.526762075582037e+85\n",
      "Gradient Descent(45/49): loss=3.3715465161956015e+87\n",
      "Gradient Descent(46/49): loss=3.2231621150668793e+89\n",
      "Gradient Descent(47/49): loss=3.081308227574133e+91\n",
      "Gradient Descent(48/49): loss=2.9456974407007315e+93\n",
      "Gradient Descent(49/49): loss=2.816054990701859e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0882683257011436\n",
      "Gradient Descent(2/49): loss=238.1602197191177\n",
      "Gradient Descent(3/49): loss=22327.8756706738\n",
      "Gradient Descent(4/49): loss=2126671.8151699156\n",
      "Gradient Descent(5/49): loss=203367457.74109983\n",
      "Gradient Descent(6/49): loss=19470460663.00789\n",
      "Gradient Descent(7/49): loss=1864789772902.9136\n",
      "Gradient Descent(8/49): loss=178621391275925.2\n",
      "Gradient Descent(9/49): loss=1.7110112360556856e+16\n",
      "Gradient Descent(10/49): loss=1.6389936542286461e+18\n",
      "Gradient Descent(11/49): loss=1.570013103434882e+20\n",
      "Gradient Descent(12/49): loss=1.5039375022058616e+22\n",
      "Gradient Descent(13/49): loss=1.4406432948605184e+24\n",
      "Gradient Descent(14/49): loss=1.3800130291843604e+26\n",
      "Gradient Descent(15/49): loss=1.3219344712531506e+28\n",
      "Gradient Descent(16/49): loss=1.2663001944266909e+30\n",
      "Gradient Descent(17/49): loss=1.213007318720946e+32\n",
      "Gradient Descent(18/49): loss=1.1619573016930677e+34\n",
      "Gradient Descent(19/49): loss=1.1130557504803162e+36\n",
      "Gradient Descent(20/49): loss=1.0662122455133901e+38\n",
      "Gradient Descent(21/49): loss=1.0213401727958079e+40\n",
      "Gradient Descent(22/49): loss=9.783565635913383e+41\n",
      "Gradient Descent(23/49): loss=9.371819409647242e+43\n",
      "Gradient Descent(24/49): loss=8.977401728132957e+45\n",
      "Gradient Descent(25/49): loss=8.599583310936866e+47\n",
      "Gradient Descent(26/49): loss=8.237665569760953e+49\n",
      "Gradient Descent(27/49): loss=7.890979316744913e+51\n",
      "Gradient Descent(28/49): loss=7.558883527133038e+53\n",
      "Gradient Descent(29/49): loss=7.240764154015361e+55\n",
      "Gradient Descent(30/49): loss=6.93603299295174e+57\n",
      "Gradient Descent(31/49): loss=6.644126594378349e+59\n",
      "Gradient Descent(32/49): loss=6.364505221786608e+61\n",
      "Gradient Descent(33/49): loss=6.0966518537473584e+63\n",
      "Gradient Descent(34/49): loss=5.840071227935509e+65\n",
      "Gradient Descent(35/49): loss=5.594288925387202e+67\n",
      "Gradient Descent(36/49): loss=5.3588504932966864e+69\n",
      "Gradient Descent(37/49): loss=5.13332060473053e+71\n",
      "Gradient Descent(38/49): loss=4.9172822537059186e+73\n",
      "Gradient Descent(39/49): loss=4.710335984144219e+75\n",
      "Gradient Descent(40/49): loss=4.51209915127459e+77\n",
      "Gradient Descent(41/49): loss=4.322205214121598e+79\n",
      "Gradient Descent(42/49): loss=4.140303057769196e+81\n",
      "Gradient Descent(43/49): loss=3.9660563441471675e+83\n",
      "Gradient Descent(44/49): loss=3.7991428901403224e+85\n",
      "Gradient Descent(45/49): loss=3.6392540718700592e+87\n",
      "Gradient Descent(46/49): loss=3.486094254047311e+89\n",
      "Gradient Descent(47/49): loss=3.3393802433411305e+91\n",
      "Gradient Descent(48/49): loss=3.1988407647528054e+93\n",
      "Gradient Descent(49/49): loss=3.06421596002688e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0980103015320495\n",
      "Gradient Descent(2/49): loss=237.22724028874111\n",
      "Gradient Descent(3/49): loss=22042.400852915045\n",
      "Gradient Descent(4/49): loss=2079698.412724978\n",
      "Gradient Descent(5/49): loss=196962899.11713398\n",
      "Gradient Descent(6/49): loss=18674732499.542526\n",
      "Gradient Descent(7/49): loss=1771229297886.6409\n",
      "Gradient Descent(8/49): loss=168012922498159.34\n",
      "Gradient Descent(9/49): loss=1.5937702468428384e+16\n",
      "Gradient Descent(10/49): loss=1.5118669767053084e+18\n",
      "Gradient Descent(11/49): loss=1.4341778010359731e+20\n",
      "Gradient Descent(12/49): loss=1.3604823358268751e+22\n",
      "Gradient Descent(13/49): loss=1.29057419720543e+24\n",
      "Gradient Descent(14/49): loss=1.2242584194290993e+26\n",
      "Gradient Descent(15/49): loss=1.1613503022353052e+28\n",
      "Gradient Descent(16/49): loss=1.1016747113030524e+30\n",
      "Gradient Descent(17/49): loss=1.0450655343924749e+32\n",
      "Gradient Descent(18/49): loss=9.91365201837634e+33\n",
      "Gradient Descent(19/49): loss=9.404242427504171e+35\n",
      "Gradient Descent(20/49): loss=8.92100867395943e+37\n",
      "Gradient Descent(21/49): loss=8.462605720515854e+39\n",
      "Gradient Descent(22/49): loss=8.027757644815855e+41\n",
      "Gradient Descent(23/49): loss=7.615254087513159e+43\n",
      "Gradient Descent(24/49): loss=7.223946883213909e+45\n",
      "Gradient Descent(25/49): loss=6.852746864623643e+47\n",
      "Gradient Descent(26/49): loss=6.500620830938774e+49\n",
      "Gradient Descent(27/49): loss=6.166588672024175e+51\n",
      "Gradient Descent(28/49): loss=5.849720640366231e+53\n",
      "Gradient Descent(29/49): loss=5.549134763206882e+55\n",
      "Gradient Descent(30/49): loss=5.2639943876538786e+57\n",
      "Gradient Descent(31/49): loss=4.993505851935395e+59\n",
      "Gradient Descent(32/49): loss=4.736916276315851e+61\n",
      "Gradient Descent(33/49): loss=4.4935114675252466e+63\n",
      "Gradient Descent(34/49): loss=4.262613930868315e+65\n",
      "Gradient Descent(35/49): loss=4.043580984481065e+67\n",
      "Gradient Descent(36/49): loss=3.835802970485328e+69\n",
      "Gradient Descent(37/49): loss=3.63870155806273e+71\n",
      "Gradient Descent(38/49): loss=3.45172813372448e+73\n",
      "Gradient Descent(39/49): loss=3.27436227429666e+75\n",
      "Gradient Descent(40/49): loss=3.106110298370625e+77\n",
      "Gradient Descent(41/49): loss=2.9465038921865853e+79\n",
      "Gradient Descent(42/49): loss=2.795098806125785e+81\n",
      "Gradient Descent(43/49): loss=2.6514736181828255e+83\n",
      "Gradient Descent(44/49): loss=2.5152285609767164e+85\n",
      "Gradient Descent(45/49): loss=2.3859844090354534e+87\n",
      "Gradient Descent(46/49): loss=2.26338142325703e+89\n",
      "Gradient Descent(47/49): loss=2.1470783496091693e+91\n",
      "Gradient Descent(48/49): loss=2.0367514692802637e+93\n",
      "Gradient Descent(49/49): loss=1.9320936976382132e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1030056631376923\n",
      "Gradient Descent(2/49): loss=238.6954032603957\n",
      "Gradient Descent(3/49): loss=22253.2062061311\n",
      "Gradient Descent(4/49): loss=2106518.730448609\n",
      "Gradient Descent(5/49): loss=200182982.31329134\n",
      "Gradient Descent(6/49): loss=19045997821.234814\n",
      "Gradient Descent(7/49): loss=1812775706155.123\n",
      "Gradient Descent(8/49): loss=172558941670426.1\n",
      "Gradient Descent(9/49): loss=1.6426619119592294e+16\n",
      "Gradient Descent(10/49): loss=1.5637401287527066e+18\n",
      "Gradient Descent(11/49): loss=1.4886165284576746e+20\n",
      "Gradient Descent(12/49): loss=1.4171039332802293e+22\n",
      "Gradient Descent(13/49): loss=1.349027400388456e+24\n",
      "Gradient Descent(14/49): loss=1.2842214042080756e+26\n",
      "Gradient Descent(15/49): loss=1.2225286874964166e+28\n",
      "Gradient Descent(16/49): loss=1.1637996464684483e+30\n",
      "Gradient Descent(17/49): loss=1.1078918950940696e+32\n",
      "Gradient Descent(18/49): loss=1.054669897095643e+34\n",
      "Gradient Descent(19/49): loss=1.0040046302440018e+36\n",
      "Gradient Descent(20/49): loss=9.557732713516452e+37\n",
      "Gradient Descent(21/49): loss=9.098588978272913e+39\n",
      "Gradient Descent(22/49): loss=8.661502040141335e+41\n",
      "Gradient Descent(23/49): loss=8.24541231294507e+43\n",
      "Gradient Descent(24/49): loss=7.849311112037506e+45\n",
      "Gradient Descent(25/49): loss=7.472238208982123e+47\n",
      "Gradient Descent(26/49): loss=7.113279503746457e+49\n",
      "Gradient Descent(27/49): loss=6.771564808734364e+51\n",
      "Gradient Descent(28/49): loss=6.446265739275415e+53\n",
      "Gradient Descent(29/49): loss=6.136593705454461e+55\n",
      "Gradient Descent(30/49): loss=5.841798000412012e+57\n",
      "Gradient Descent(31/49): loss=5.561163980480668e+59\n",
      "Gradient Descent(32/49): loss=5.29401133274628e+61\n",
      "Gradient Descent(33/49): loss=5.039692425833422e+63\n",
      "Gradient Descent(34/49): loss=4.797590739917655e+65\n",
      "Gradient Descent(35/49): loss=4.567119372158358e+67\n",
      "Gradient Descent(36/49): loss=4.347719613929016e+69\n",
      "Gradient Descent(37/49): loss=4.1388595963958563e+71\n",
      "Gradient Descent(38/49): loss=3.940033001161654e+73\n",
      "Gradient Descent(39/49): loss=3.750757832848722e+75\n",
      "Gradient Descent(40/49): loss=3.5705752506459523e+77\n",
      "Gradient Descent(41/49): loss=3.399048455986936e+79\n",
      "Gradient Descent(42/49): loss=3.2357616336630977e+81\n",
      "Gradient Descent(43/49): loss=3.080318943804517e+83\n",
      "Gradient Descent(44/49): loss=2.9323435622850628e+85\n",
      "Gradient Descent(45/49): loss=2.7914767672254345e+87\n",
      "Gradient Descent(46/49): loss=2.657377069379639e+89\n",
      "Gradient Descent(47/49): loss=2.529719384296943e+91\n",
      "Gradient Descent(48/49): loss=2.4081942442521546e+93\n",
      "Gradient Descent(49/49): loss=2.292507048034086e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.140310494879901\n",
      "Gradient Descent(2/49): loss=247.00382858848874\n",
      "Gradient Descent(3/49): loss=23501.14503780182\n",
      "Gradient Descent(4/49): loss=2268725.7161950935\n",
      "Gradient Descent(5/49): loss=219786318.0125115\n",
      "Gradient Descent(6/49): loss=21313838108.935295\n",
      "Gradient Descent(7/49): loss=2067553072050.8103\n",
      "Gradient Descent(8/49): loss=200582482403129.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=1.9459970876683384e+16\n",
      "Gradient Descent(10/49): loss=1.88797125333038e+18\n",
      "Gradient Descent(11/49): loss=1.831680914286511e+20\n",
      "Gradient Descent(12/49): loss=1.7770704860505704e+22\n",
      "Gradient Descent(13/49): loss=1.7240887190178544e+24\n",
      "Gradient Descent(14/49): loss=1.672686703128944e+26\n",
      "Gradient Descent(15/49): loss=1.6228172326066455e+28\n",
      "Gradient Descent(16/49): loss=1.5744345836895724e+30\n",
      "Gradient Descent(17/49): loss=1.5274944184548476e+32\n",
      "Gradient Descent(18/49): loss=1.481953727733133e+34\n",
      "Gradient Descent(19/49): loss=1.4377707867112348e+36\n",
      "Gradient Descent(20/49): loss=1.3949051151900653e+38\n",
      "Gradient Descent(21/49): loss=1.3533174400379118e+40\n",
      "Gradient Descent(22/49): loss=1.3129696590695454e+42\n",
      "Gradient Descent(23/49): loss=1.2738248060951795e+44\n",
      "Gradient Descent(24/49): loss=1.2358470170396676e+46\n",
      "Gradient Descent(25/49): loss=1.1990014970803691e+48\n",
      "Gradient Descent(26/49): loss=1.163254488767298e+50\n",
      "Gradient Descent(27/49): loss=1.1285732410946268e+52\n",
      "Gradient Descent(28/49): loss=1.0949259794944454e+54\n",
      "Gradient Descent(29/49): loss=1.062281876725223e+56\n",
      "Gradient Descent(30/49): loss=1.0306110246282499e+58\n",
      "Gradient Descent(31/49): loss=9.998844067261064e+59\n",
      "Gradient Descent(32/49): loss=9.700738716380889e+61\n",
      "Gradient Descent(33/49): loss=9.41152107288217e+63\n",
      "Gradient Descent(34/49): loss=9.130926158822586e+65\n",
      "Gradient Descent(35/49): loss=8.858696896307153e+67\n",
      "Gradient Descent(36/49): loss=8.594583871956448e+69\n",
      "Gradient Descent(37/49): loss=8.338345108396971e+71\n",
      "Gradient Descent(38/49): loss=8.089745842563987e+73\n",
      "Gradient Descent(39/49): loss=7.848558310614618e+75\n",
      "Gradient Descent(40/49): loss=7.614561539252748e+77\n",
      "Gradient Descent(41/49): loss=7.387541143276112e+79\n",
      "Gradient Descent(42/49): loss=7.167289129158871e+81\n",
      "Gradient Descent(43/49): loss=6.953603704490254e+83\n",
      "Gradient Descent(44/49): loss=6.746289093094693e+85\n",
      "Gradient Descent(45/49): loss=6.545155355663829e+87\n",
      "Gradient Descent(46/49): loss=6.350018215736382e+89\n",
      "Gradient Descent(47/49): loss=6.16069889086602e+91\n",
      "Gradient Descent(48/49): loss=5.977023928822917e+93\n",
      "Gradient Descent(49/49): loss=5.7988250486788e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.129122604211476\n",
      "Gradient Descent(2/49): loss=245.29158103668402\n",
      "Gradient Descent(3/49): loss=23337.996515705945\n",
      "Gradient Descent(4/49): loss=2255853.610540266\n",
      "Gradient Descent(5/49): loss=218922496.0423905\n",
      "Gradient Descent(6/49): loss=21270879445.316875\n",
      "Gradient Descent(7/49): loss=2067474158314.9536\n",
      "Gradient Descent(8/49): loss=200976343898951.75\n",
      "Gradient Descent(9/49): loss=1.953735191922151e+16\n",
      "Gradient Descent(10/49): loss=1.899291005871948e+18\n",
      "Gradient Descent(11/49): loss=1.846370836882022e+20\n",
      "Gradient Descent(12/49): loss=1.794927304238188e+22\n",
      "Gradient Descent(13/49): loss=1.7449177446100457e+24\n",
      "Gradient Descent(14/49): loss=1.6963017343595577e+26\n",
      "Gradient Descent(15/49): loss=1.6490403013822091e+28\n",
      "Gradient Descent(16/49): loss=1.6030956600720593e+30\n",
      "Gradient Descent(17/49): loss=1.5584311087490908e+32\n",
      "Gradient Descent(18/49): loss=1.5150109779418326e+34\n",
      "Gradient Descent(19/49): loss=1.472800594967804e+36\n",
      "Gradient Descent(20/49): loss=1.4317662540996804e+38\n",
      "Gradient Descent(21/49): loss=1.3918751889862903e+40\n",
      "Gradient Descent(22/49): loss=1.3530955462826593e+42\n",
      "Gradient Descent(23/49): loss=1.315396360151199e+44\n",
      "Gradient Descent(24/49): loss=1.2787475275155367e+46\n",
      "Gradient Descent(25/49): loss=1.2431197840168923e+48\n",
      "Gradient Descent(26/49): loss=1.208484680644384e+50\n",
      "Gradient Descent(27/49): loss=1.1748145610176928e+52\n",
      "Gradient Descent(28/49): loss=1.1420825393030886e+54\n",
      "Gradient Descent(29/49): loss=1.1102624787448024e+56\n",
      "Gradient Descent(30/49): loss=1.0793289707946667e+58\n",
      "Gradient Descent(31/49): loss=1.0492573148231527e+60\n",
      "Gradient Descent(32/49): loss=1.0200234983957796e+62\n",
      "Gradient Descent(33/49): loss=9.916041780989908e+63\n",
      "Gradient Descent(34/49): loss=9.639766609002668e+65\n",
      "Gradient Descent(35/49): loss=9.371188860275935e+67\n",
      "Gradient Descent(36/49): loss=9.110094073537448e+69\n",
      "Gradient Descent(37/49): loss=8.85627376271429e+71\n",
      "Gradient Descent(38/49): loss=8.60952525045505e+73\n",
      "Gradient Descent(39/49): loss=8.36965150629054e+75\n",
      "Gradient Descent(40/49): loss=8.136460989303534e+77\n",
      "Gradient Descent(41/49): loss=7.909767495182065e+79\n",
      "Gradient Descent(42/49): loss=7.689390007533728e+81\n",
      "Gradient Descent(43/49): loss=7.475152553342963e+83\n",
      "Gradient Descent(44/49): loss=7.266884062455295e+85\n",
      "Gradient Descent(45/49): loss=7.064418230976544e+87\n",
      "Gradient Descent(46/49): loss=6.867593388477969e+89\n",
      "Gradient Descent(47/49): loss=6.676252368901216e+91\n",
      "Gradient Descent(48/49): loss=6.490242385060276e+93\n",
      "Gradient Descent(49/49): loss=6.309414906639534e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1390663291236685\n",
      "Gradient Descent(2/49): loss=244.33349082533863\n",
      "Gradient Descent(3/49): loss=23040.040327921575\n",
      "Gradient Descent(4/49): loss=2206084.45612314\n",
      "Gradient Descent(5/49): loss=212035083.65188637\n",
      "Gradient Descent(6/49): loss=20402384288.423576\n",
      "Gradient Descent(7/49): loss=1963836034261.6455\n",
      "Gradient Descent(8/49): loss=189050262071223.06\n",
      "Gradient Descent(9/49): loss=1.819971426734409e+16\n",
      "Gradient Descent(10/49): loss=1.7520912826787151e+18\n",
      "Gradient Descent(11/49): loss=1.6867489566208016e+20\n",
      "Gradient Descent(12/49): loss=1.6238453747837305e+22\n",
      "Gradient Descent(13/49): loss=1.5632882220977878e+24\n",
      "Gradient Descent(14/49): loss=1.5049895720892839e+26\n",
      "Gradient Descent(15/49): loss=1.448865069444443e+28\n",
      "Gradient Descent(16/49): loss=1.394833594916008e+30\n",
      "Gradient Descent(17/49): loss=1.3428170822660472e+32\n",
      "Gradient Descent(18/49): loss=1.2927403851405843e+34\n",
      "Gradient Descent(19/49): loss=1.2445311622261084e+36\n",
      "Gradient Descent(20/49): loss=1.1981197708034552e+38\n",
      "Gradient Descent(21/49): loss=1.1534391655426794e+40\n",
      "Gradient Descent(22/49): loss=1.1104248014648933e+42\n",
      "Gradient Descent(23/49): loss=1.0690145406442096e+44\n",
      "Gradient Descent(24/49): loss=1.0291485624264957e+46\n",
      "Gradient Descent(25/49): loss=9.907692770078949e+47\n",
      "Gradient Descent(26/49): loss=9.538212422397446e+49\n",
      "Gradient Descent(27/49): loss=9.182510835371388e+51\n",
      "Gradient Descent(28/49): loss=8.84007416774651e+53\n",
      "Gradient Descent(29/49): loss=8.510407740575115e+55\n",
      "Gradient Descent(30/49): loss=8.19303532261016e+57\n",
      "Gradient Descent(31/49): loss=7.887498442348574e+59\n",
      "Gradient Descent(32/49): loss=7.593355725730141e+61\n",
      "Gradient Descent(33/49): loss=7.310182258535032e+63\n",
      "Gradient Descent(34/49): loss=7.037568972558781e+65\n",
      "Gradient Descent(35/49): loss=6.775122054678782e+67\n",
      "Gradient Descent(36/49): loss=6.522462377957393e+69\n",
      "Gradient Descent(37/49): loss=6.279224953960848e+71\n",
      "Gradient Descent(38/49): loss=6.045058405502421e+73\n",
      "Gradient Descent(39/49): loss=5.819624459048034e+75\n",
      "Gradient Descent(40/49): loss=5.602597456051431e+77\n",
      "Gradient Descent(41/49): loss=5.393663882512545e+79\n",
      "Gradient Descent(42/49): loss=5.192521916079867e+81\n",
      "Gradient Descent(43/49): loss=4.998880990042307e+83\n",
      "Gradient Descent(44/49): loss=4.812461373580827e+85\n",
      "Gradient Descent(45/49): loss=4.6329937676735e+87\n",
      "Gradient Descent(46/49): loss=4.4602189160700283e+89\n",
      "Gradient Descent(47/49): loss=4.293887230774002e+91\n",
      "Gradient Descent(48/49): loss=4.133758431491885e+93\n",
      "Gradient Descent(49/49): loss=3.979601198527535e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1438069025881843\n",
      "Gradient Descent(2/49): loss=245.79605039868318\n",
      "Gradient Descent(3/49): loss=23253.39323035728\n",
      "Gradient Descent(4/49): loss=2233639.0046804254\n",
      "Gradient Descent(5/49): loss=215393548.81391582\n",
      "Gradient Descent(6/49): loss=20795475987.02529\n",
      "Gradient Descent(7/49): loss=2008489189783.2502\n",
      "Gradient Descent(8/49): loss=194009707106132.28\n",
      "Gradient Descent(9/49): loss=1.8741090185464548e+16\n",
      "Gradient Descent(10/49): loss=1.8103892269365448e+18\n",
      "Gradient Descent(11/49): loss=1.7488434691877126e+20\n",
      "Gradient Descent(12/49): loss=1.6893924121085563e+22\n",
      "Gradient Descent(13/49): loss=1.6319631256051824e+24\n",
      "Gradient Descent(14/49): loss=1.5764863348748913e+26\n",
      "Gradient Descent(15/49): loss=1.5228954934025425e+28\n",
      "Gradient Descent(16/49): loss=1.4711264353947788e+30\n",
      "Gradient Descent(17/49): loss=1.4211172139434827e+32\n",
      "Gradient Descent(18/49): loss=1.372807999906838e+34\n",
      "Gradient Descent(19/49): loss=1.3261410017604504e+36\n",
      "Gradient Descent(20/49): loss=1.281060393740417e+38\n",
      "Gradient Descent(21/49): loss=1.2375122481969946e+40\n",
      "Gradient Descent(22/49): loss=1.1954444708091795e+42\n",
      "Gradient Descent(23/49): loss=1.1548067381799158e+44\n",
      "Gradient Descent(24/49): loss=1.1155504376092577e+46\n",
      "Gradient Descent(25/49): loss=1.077628608932877e+48\n",
      "Gradient Descent(26/49): loss=1.0409958883433776e+50\n",
      "Gradient Descent(27/49): loss=1.0056084541231841e+52\n",
      "Gradient Descent(28/49): loss=9.714239742227287e+53\n",
      "Gradient Descent(29/49): loss=9.384015556208644e+55\n",
      "Gradient Descent(30/49): loss=9.065016954067422e+57\n",
      "Gradient Descent(31/49): loss=8.756862335246418e+59\n",
      "Gradient Descent(32/49): loss=8.459183071251777e+61\n",
      "Gradient Descent(33/49): loss=8.171623064682897e+63\n",
      "Gradient Descent(34/49): loss=7.89383832325267e+65\n",
      "Gradient Descent(35/49): loss=7.625496548288301e+67\n",
      "Gradient Descent(36/49): loss=7.366276737220582e+69\n",
      "Gradient Descent(37/49): loss=7.115868799586253e+71\n",
      "Gradient Descent(38/49): loss=6.873973186083543e+73\n",
      "Gradient Descent(39/49): loss=6.640300530237836e+75\n",
      "Gradient Descent(40/49): loss=6.414571302248499e+77\n",
      "Gradient Descent(41/49): loss=6.196515474602439e+79\n",
      "Gradient Descent(42/49): loss=5.985872199055364e+81\n",
      "Gradient Descent(43/49): loss=5.78238949459296e+83\n",
      "Gradient Descent(44/49): loss=5.58582394599997e+85\n",
      "Gradient Descent(45/49): loss=5.395940412675888e+87\n",
      "Gradient Descent(46/49): loss=5.212511747349147e+89\n",
      "Gradient Descent(47/49): loss=5.035318524353182e+91\n",
      "Gradient Descent(48/49): loss=4.864148777139629e+93\n",
      "Gradient Descent(49/49): loss=4.6987977447143134e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1816834855439193\n",
      "Gradient Descent(2/49): loss=254.34485809879328\n",
      "Gradient Descent(3/49): loss=24556.41664249133\n",
      "Gradient Descent(4/49): loss=2405507.73557159\n",
      "Gradient Descent(5/49): loss=236471090.93171996\n",
      "Gradient Descent(6/49): loss=23269833723.39489\n",
      "Gradient Descent(7/49): loss=2290567542808.074\n",
      "Gradient Descent(8/49): loss=225493736862480.34\n",
      "Gradient Descent(9/49): loss=2.2199275311619596e+16\n",
      "Gradient Descent(10/49): loss=2.185481569113726e+18\n",
      "Gradient Descent(11/49): loss=2.1515763483356142e+20\n",
      "Gradient Descent(12/49): loss=2.1181990536571473e+22\n",
      "Gradient Descent(13/49): loss=2.0853401324546732e+24\n",
      "Gradient Descent(14/49): loss=2.0529911235669137e+26\n",
      "Gradient Descent(15/49): loss=2.021143987614995e+28\n",
      "Gradient Descent(16/49): loss=1.9897908994275233e+30\n",
      "Gradient Descent(17/49): loss=1.958924182792208e+32\n",
      "Gradient Descent(18/49): loss=1.9285362890681887e+34\n",
      "Gradient Descent(19/49): loss=1.8986197893299853e+36\n",
      "Gradient Descent(20/49): loss=1.869167370699815e+38\n",
      "Gradient Descent(21/49): loss=1.8401718339895656e+40\n",
      "Gradient Descent(22/49): loss=1.8116260917653553e+42\n",
      "Gradient Descent(23/49): loss=1.783523166561006e+44\n",
      "Gradient Descent(24/49): loss=1.7558561891557943e+46\n",
      "Gradient Descent(25/49): loss=1.7286183968903424e+48\n",
      "Gradient Descent(26/49): loss=1.701803132012007e+50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=1.6754038400469931e+52\n",
      "Gradient Descent(28/49): loss=1.649414068198117e+54\n",
      "Gradient Descent(29/49): loss=1.623827463767521e+56\n",
      "Gradient Descent(30/49): loss=1.5986377726038257e+58\n",
      "Gradient Descent(31/49): loss=1.5738388375734527e+60\n",
      "Gradient Descent(32/49): loss=1.5494245970556148e+62\n",
      "Gradient Descent(33/49): loss=1.5253890834606508e+64\n",
      "Gradient Descent(34/49): loss=1.5017264217714018e+66\n",
      "Gradient Descent(35/49): loss=1.478430828107145e+68\n",
      "Gradient Descent(36/49): loss=1.455496608309864e+70\n",
      "Gradient Descent(37/49): loss=1.4329181565523928e+72\n",
      "Gradient Descent(38/49): loss=1.4106899539681947e+74\n",
      "Gradient Descent(39/49): loss=1.3888065673023828e+76\n",
      "Gradient Descent(40/49): loss=1.3672626475836666e+78\n",
      "Gradient Descent(41/49): loss=1.3460529288168836e+80\n",
      "Gradient Descent(42/49): loss=1.3251722266958536e+82\n",
      "Gradient Descent(43/49): loss=1.3046154373361518e+84\n",
      "Gradient Descent(44/49): loss=1.2843775360275727e+86\n",
      "Gradient Descent(45/49): loss=1.2644535760059525e+88\n",
      "Gradient Descent(46/49): loss=1.2448386872439915e+90\n",
      "Gradient Descent(47/49): loss=1.2255280752609019e+92\n",
      "Gradient Descent(48/49): loss=1.2065170199504822e+94\n",
      "Gradient Descent(49/49): loss=1.1878008744273708e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1702899940571405\n",
      "Gradient Descent(2/49): loss=252.5822147428648\n",
      "Gradient Descent(3/49): loss=24385.918201402557\n",
      "Gradient Descent(4/49): loss=2391850.1599305747\n",
      "Gradient Descent(5/49): loss=235540254.5105614\n",
      "Gradient Descent(6/49): loss=23222750791.9842\n",
      "Gradient Descent(7/49): loss=2290458716632.4688\n",
      "Gradient Descent(8/49): loss=225934082944947.94\n",
      "Gradient Descent(9/49): loss=2.228727851494903e+16\n",
      "Gradient Descent(10/49): loss=2.1985554396912996e+18\n",
      "Gradient Descent(11/49): loss=2.1687995981618958e+20\n",
      "Gradient Descent(12/49): loss=2.1394490255007015e+22\n",
      "Gradient Descent(13/49): loss=2.1104964571780953e+24\n",
      "Gradient Descent(14/49): loss=2.0819359475164112e+26\n",
      "Gradient Descent(15/49): loss=2.0537620149947193e+28\n",
      "Gradient Descent(16/49): loss=2.025969372909421e+30\n",
      "Gradient Descent(17/49): loss=1.9985528440318474e+32\n",
      "Gradient Descent(18/49): loss=1.9715073331217736e+34\n",
      "Gradient Descent(19/49): loss=1.9448278176409945e+36\n",
      "Gradient Descent(20/49): loss=1.918509344198359e+38\n",
      "Gradient Descent(21/49): loss=1.8925470268055368e+40\n",
      "Gradient Descent(22/49): loss=1.8669360457106384e+42\n",
      "Gradient Descent(23/49): loss=1.8416716464219896e+44\n",
      "Gradient Descent(24/49): loss=1.8167491387997944e+46\n",
      "Gradient Descent(25/49): loss=1.7921638961774607e+48\n",
      "Gradient Descent(26/49): loss=1.7679113545000778e+50\n",
      "Gradient Descent(27/49): loss=1.7439870114764006e+52\n",
      "Gradient Descent(28/49): loss=1.7203864257427556e+54\n",
      "Gradient Descent(29/49): loss=1.6971052160384707e+56\n",
      "Gradient Descent(30/49): loss=1.6741390603925066e+58\n",
      "Gradient Descent(31/49): loss=1.6514836953210854e+60\n",
      "Gradient Descent(32/49): loss=1.629134915036238e+62\n",
      "Gradient Descent(33/49): loss=1.6070885706650104e+64\n",
      "Gradient Descent(34/49): loss=1.5853405694792575e+66\n",
      "Gradient Descent(35/49): loss=1.563886874135899e+68\n",
      "Gradient Descent(36/49): loss=1.542723501927351e+70\n",
      "Gradient Descent(37/49): loss=1.521846524042238e+72\n",
      "Gradient Descent(38/49): loss=1.5012520648359876e+74\n",
      "Gradient Descent(39/49): loss=1.4809363011113742e+76\n",
      "Gradient Descent(40/49): loss=1.4608954614087713e+78\n",
      "Gradient Descent(41/49): loss=1.441125825305999e+80\n",
      "Gradient Descent(42/49): loss=1.4216237227276792e+82\n",
      "Gradient Descent(43/49): loss=1.4023855332638714e+84\n",
      "Gradient Descent(44/49): loss=1.3834076854980347e+86\n",
      "Gradient Descent(45/49): loss=1.3646866563439789e+88\n",
      "Gradient Descent(46/49): loss=1.3462189703918259e+90\n",
      "Gradient Descent(47/49): loss=1.3280011992628573e+92\n",
      "Gradient Descent(48/49): loss=1.3100299609730536e+94\n",
      "Gradient Descent(49/49): loss=1.292301919305251e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.180437286273195\n",
      "Gradient Descent(2/49): loss=251.59851568209314\n",
      "Gradient Descent(3/49): loss=24075.029635615207\n",
      "Gradient Descent(4/49): loss=2339140.781291966\n",
      "Gradient Descent(5/49): loss=228137475.40339082\n",
      "Gradient Descent(6/49): loss=22275435358.863037\n",
      "Gradient Descent(7/49): loss=2175742695632.3296\n",
      "Gradient Descent(8/49): loss=212538142007241.06\n",
      "Gradient Descent(9/49): loss=2.0762590960908556e+16\n",
      "Gradient Descent(10/49): loss=2.0282950251777503e+18\n",
      "Gradient Descent(11/49): loss=1.9814461686051488e+20\n",
      "Gradient Descent(12/49): loss=1.9356816652181243e+22\n",
      "Gradient Descent(13/49): loss=1.8909748698667738e+24\n",
      "Gradient Descent(14/49): loss=1.8473008514140936e+26\n",
      "Gradient Descent(15/49): loss=1.8046355992232654e+28\n",
      "Gradient Descent(16/49): loss=1.762955765375328e+30\n",
      "Gradient Descent(17/49): loss=1.7222385751420508e+32\n",
      "Gradient Descent(18/49): loss=1.6824617904543723e+34\n",
      "Gradient Descent(19/49): loss=1.643603690197181e+36\n",
      "Gradient Descent(20/49): loss=1.6056430559753022e+38\n",
      "Gradient Descent(21/49): loss=1.5685591597825513e+40\n",
      "Gradient Descent(22/49): loss=1.5323317524496838e+42\n",
      "Gradient Descent(23/49): loss=1.4969410525142821e+44\n",
      "Gradient Descent(24/49): loss=1.462367735396324e+46\n",
      "Gradient Descent(25/49): loss=1.4285929228390487e+48\n",
      "Gradient Descent(26/49): loss=1.3955981725985028e+50\n",
      "Gradient Descent(27/49): loss=1.3633654683726883e+52\n",
      "Gradient Descent(28/49): loss=1.3318772099638199e+54\n",
      "Gradient Descent(29/49): loss=1.301116203667937e+56\n",
      "Gradient Descent(30/49): loss=1.2710656528864587e+58\n",
      "Gradient Descent(31/49): loss=1.2417091489547013e+60\n",
      "Gradient Descent(32/49): loss=1.2130306621821117e+62\n",
      "Gradient Descent(33/49): loss=1.1850145330995354e+64\n",
      "Gradient Descent(34/49): loss=1.1576454639085577e+66\n",
      "Gradient Descent(35/49): loss=1.130908510128367e+68\n",
      "Gradient Descent(36/49): loss=1.1047890724355538e+70\n",
      "Gradient Descent(37/49): loss=1.0792728886923573e+72\n",
      "Gradient Descent(38/49): loss=1.0543460261590271e+74\n",
      "Gradient Descent(39/49): loss=1.029994873886059e+76\n",
      "Gradient Descent(40/49): loss=1.0062061352821451e+78\n",
      "Gradient Descent(41/49): loss=9.829668208537435e+79\n",
      "Gradient Descent(42/49): loss=9.602642411123646e+81\n",
      "Gradient Descent(43/49): loss=9.380859996456643e+83\n",
      "Gradient Descent(44/49): loss=9.164199863485644e+85\n",
      "Gradient Descent(45/49): loss=8.952543708106973e+87\n",
      "Gradient Descent(46/49): loss=8.745775958565912e+89\n",
      "Gradient Descent(47/49): loss=8.543783712350417e+91\n",
      "Gradient Descent(48/49): loss=8.346456674542312e+93\n",
      "Gradient Descent(49/49): loss=8.153687097592397e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1849187098315617\n",
      "Gradient Descent(2/49): loss=253.0542527214135\n",
      "Gradient Descent(3/49): loss=24290.767334794138\n",
      "Gradient Descent(4/49): loss=2367421.719676626\n",
      "Gradient Descent(5/49): loss=231636504.04761744\n",
      "Gradient Descent(6/49): loss=22691151943.985325\n",
      "Gradient Descent(7/49): loss=2223674577009.9395\n",
      "Gradient Descent(8/49): loss=217941290106687.6\n",
      "Gradient Descent(9/49): loss=2.1361184860901668e+16\n",
      "Gradient Descent(10/49): loss=2.0937118697113183e+18\n",
      "Gradient Descent(11/49): loss=2.05215605731774e+20\n",
      "Gradient Descent(12/49): loss=2.0114279232418664e+22\n",
      "Gradient Descent(13/49): loss=1.9715090298472834e+24\n",
      "Gradient Descent(14/49): loss=1.9323826684225797e+26\n",
      "Gradient Descent(15/49): loss=1.8940329013300546e+28\n",
      "Gradient Descent(16/49): loss=1.856444248951585e+30\n",
      "Gradient Descent(17/49): loss=1.819601584581959e+32\n",
      "Gradient Descent(18/49): loss=1.783490096459189e+34\n",
      "Gradient Descent(19/49): loss=1.7480952715272884e+36\n",
      "Gradient Descent(20/49): loss=1.7134028862871416e+38\n",
      "Gradient Descent(21/49): loss=1.6793990000112465e+40\n",
      "Gradient Descent(22/49): loss=1.6460699487967676e+42\n",
      "Gradient Descent(23/49): loss=1.6134023399636667e+44\n",
      "Gradient Descent(24/49): loss=1.5813830466372447e+46\n",
      "Gradient Descent(25/49): loss=1.5499992024617458e+48\n",
      "Gradient Descent(26/49): loss=1.5192381964265214e+50\n",
      "Gradient Descent(27/49): loss=1.4890876677973393e+52\n",
      "Gradient Descent(28/49): loss=1.459535501149063e+54\n",
      "Gradient Descent(29/49): loss=1.4305698214971662e+56\n",
      "Gradient Descent(30/49): loss=1.4021789895259414e+58\n",
      "Gradient Descent(31/49): loss=1.3743515969114747e+60\n",
      "Gradient Descent(32/49): loss=1.3470764617373908e+62\n",
      "Gradient Descent(33/49): loss=1.3203426240016325e+64\n",
      "Gradient Descent(34/49): loss=1.2941393412123761e+66\n",
      "Gradient Descent(35/49): loss=1.2684560840713529e+68\n",
      "Gradient Descent(36/49): loss=1.243282532242858e+70\n",
      "Gradient Descent(37/49): loss=1.218608570206725e+72\n",
      "Gradient Descent(38/49): loss=1.1944242831935824e+74\n",
      "Gradient Descent(39/49): loss=1.1707199532007924e+76\n",
      "Gradient Descent(40/49): loss=1.1474860550874573e+78\n",
      "Gradient Descent(41/49): loss=1.1247132527469095e+80\n",
      "Gradient Descent(42/49): loss=1.1023923953551795e+82\n",
      "Gradient Descent(43/49): loss=1.0805145136939234e+84\n",
      "Gradient Descent(44/49): loss=1.0590708165462767e+86\n",
      "Gradient Descent(45/49): loss=1.0380526871642915e+88\n",
      "Gradient Descent(46/49): loss=1.0174516798064599e+90\n",
      "Gradient Descent(47/49): loss=9.972595163439349e+91\n",
      "Gradient Descent(48/49): loss=9.774680829341441e+93\n",
      "Gradient Descent(49/49): loss=9.580694267603547e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.223371322839702\n",
      "Gradient Descent(2/49): loss=261.84861753648835\n",
      "Gradient Descent(3/49): loss=25650.886183004557\n",
      "Gradient Descent(4/49): loss=2549451.97115337\n",
      "Gradient Descent(5/49): loss=254287279.73461184\n",
      "Gradient Descent(6/49): loss=25389136203.59517\n",
      "Gradient Descent(7/49): loss=2535749853133.9316\n",
      "Gradient Descent(8/49): loss=253283369953799.97\n",
      "Gradient Descent(9/49): loss=2.529996718784871e+16\n",
      "Gradient Descent(10/49): loss=2.527186584890914e+18\n",
      "Gradient Descent(11/49): loss=2.524386971502088e+20\n",
      "Gradient Descent(12/49): loss=2.521592773908034e+22\n",
      "Gradient Descent(13/49): loss=2.5188023932448106e+24\n",
      "Gradient Descent(14/49): loss=2.516015326962615e+26\n",
      "Gradient Descent(15/49): loss=2.5132314154742336e+28\n",
      "Gradient Descent(16/49): loss=2.5104506065011198e+30\n",
      "Gradient Descent(17/49): loss=2.5076728813442067e+32\n",
      "Gradient Descent(18/49): loss=2.504898231814366e+34\n",
      "Gradient Descent(19/49): loss=2.502126653013747e+36\n",
      "Gradient Descent(20/49): loss=2.4993581410769777e+38\n",
      "Gradient Descent(21/49): loss=2.4965926924642953e+40\n",
      "Gradient Descent(22/49): loss=2.4938303037404528e+42\n",
      "Gradient Descent(23/49): loss=2.49107097150546e+44\n",
      "Gradient Descent(24/49): loss=2.488314692372928e+46\n",
      "Gradient Descent(25/49): loss=2.4855614629633273e+48\n",
      "Gradient Descent(26/49): loss=2.482811279901794e+50\n",
      "Gradient Descent(27/49): loss=2.480064139817532e+52\n",
      "Gradient Descent(28/49): loss=2.477320039343554e+54\n",
      "Gradient Descent(29/49): loss=2.4745789751166123e+56\n",
      "Gradient Descent(30/49): loss=2.4718409437772315e+58\n",
      "Gradient Descent(31/49): loss=2.469105941969611e+60\n",
      "Gradient Descent(32/49): loss=2.466373966341707e+62\n",
      "Gradient Descent(33/49): loss=2.4636450135451303e+64\n",
      "Gradient Descent(34/49): loss=2.4609190802352527e+66\n",
      "Gradient Descent(35/49): loss=2.45819616307112e+68\n",
      "Gradient Descent(36/49): loss=2.4554762587154722e+70\n",
      "Gradient Descent(37/49): loss=2.4527593638347537e+72\n",
      "Gradient Descent(38/49): loss=2.4500454750990624e+74\n",
      "Gradient Descent(39/49): loss=2.4473345891822207e+76\n",
      "Gradient Descent(40/49): loss=2.4446267027617563e+78\n",
      "Gradient Descent(41/49): loss=2.4419218125187907e+80\n",
      "Gradient Descent(42/49): loss=2.4392199151381854e+82\n",
      "Gradient Descent(43/49): loss=2.4365210073084297e+84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=2.4338250857217126e+86\n",
      "Gradient Descent(45/49): loss=2.4311321470738573e+88\n",
      "Gradient Descent(46/49): loss=2.4284421880643533e+90\n",
      "Gradient Descent(47/49): loss=2.42575520539633e+92\n",
      "Gradient Descent(48/49): loss=2.4230711957765858e+94\n",
      "Gradient Descent(49/49): loss=2.4203901559155413e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2117704952381363\n",
      "Gradient Descent(2/49): loss=260.0344757535616\n",
      "Gradient Descent(3/49): loss=25472.764094220067\n",
      "Gradient Descent(4/49): loss=2534967.3192090522\n",
      "Gradient Descent(5/49): loss=253284788.3701831\n",
      "Gradient Descent(6/49): loss=25337569629.56515\n",
      "Gradient Descent(7/49): loss=2535606031767.7783\n",
      "Gradient Descent(8/49): loss=253775293041817.4\n",
      "Gradient Descent(9/49): loss=2.5399958493756748e+16\n",
      "Gradient Descent(10/49): loss=2.542270771498503e+18\n",
      "Gradient Descent(11/49): loss=2.5445573102795627e+20\n",
      "Gradient Descent(12/49): loss=2.5468489637406876e+22\n",
      "Gradient Descent(13/49): loss=2.549143657616894e+24\n",
      "Gradient Descent(14/49): loss=2.55144073085106e+26\n",
      "Gradient Descent(15/49): loss=2.553739973608081e+28\n",
      "Gradient Descent(16/49): loss=2.5560413201455427e+30\n",
      "Gradient Descent(17/49): loss=2.5583447507390526e+32\n",
      "Gradient Descent(18/49): loss=2.560650260361896e+34\n",
      "Gradient Descent(19/49): loss=2.5629578486824084e+36\n",
      "Gradient Descent(20/49): loss=2.5652675168696333e+38\n",
      "Gradient Descent(21/49): loss=2.5675792665729424e+40\n",
      "Gradient Descent(22/49): loss=2.5698930995963024e+42\n",
      "Gradient Descent(23/49): loss=2.5722090177942098e+44\n",
      "Gradient Descent(24/49): loss=2.5745270230384495e+46\n",
      "Gradient Descent(25/49): loss=2.5768471172074464e+48\n",
      "Gradient Descent(26/49): loss=2.579169302182961e+50\n",
      "Gradient Descent(27/49): loss=2.581493579848904e+52\n",
      "Gradient Descent(28/49): loss=2.5838199520911092e+54\n",
      "Gradient Descent(29/49): loss=2.586148420797104e+56\n",
      "Gradient Descent(30/49): loss=2.588478987856175e+58\n",
      "Gradient Descent(31/49): loss=2.5908116551593035e+60\n",
      "Gradient Descent(32/49): loss=2.5931464245991543e+62\n",
      "Gradient Descent(33/49): loss=2.5954832980701212e+64\n",
      "Gradient Descent(34/49): loss=2.597822277468297e+66\n",
      "Gradient Descent(35/49): loss=2.6001633646914813e+68\n",
      "Gradient Descent(36/49): loss=2.6025065616392087e+70\n",
      "Gradient Descent(37/49): loss=2.60485187021269e+72\n",
      "Gradient Descent(38/49): loss=2.607199292314866e+74\n",
      "Gradient Descent(39/49): loss=2.609548829850384e+76\n",
      "Gradient Descent(40/49): loss=2.6119004847256417e+78\n",
      "Gradient Descent(41/49): loss=2.614254258848702e+80\n",
      "Gradient Descent(42/49): loss=2.6166101541293954e+82\n",
      "Gradient Descent(43/49): loss=2.618968172479237e+84\n",
      "Gradient Descent(44/49): loss=2.62132831581148e+86\n",
      "Gradient Descent(45/49): loss=2.6236905860411106e+88\n",
      "Gradient Descent(46/49): loss=2.6260549850848198e+90\n",
      "Gradient Descent(47/49): loss=2.628421514861042e+92\n",
      "Gradient Descent(48/49): loss=2.630790177289936e+94\n",
      "Gradient Descent(49/49): loss=2.6331609742933846e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.22212317298063\n",
      "Gradient Descent(2/49): loss=259.0246633434749\n",
      "Gradient Descent(3/49): loss=25148.479212295984\n",
      "Gradient Descent(4/49): loss=2479166.8940732605\n",
      "Gradient Descent(5/49): loss=245332198.33989272\n",
      "Gradient Descent(6/49): loss=24304926325.10134\n",
      "Gradient Descent(7/49): loss=2408720919609.781\n",
      "Gradient Descent(8/49): loss=238740953303595.78\n",
      "Gradient Descent(9/49): loss=2.366370709577102e+16\n",
      "Gradient Descent(10/49): loss=2.345543992188165e+18\n",
      "Gradient Descent(11/49): loss=2.3249090753223606e+20\n",
      "Gradient Descent(12/49): loss=2.30445840293335e+22\n",
      "Gradient Descent(13/49): loss=2.2841884847824153e+24\n",
      "Gradient Descent(14/49): loss=2.264097135155288e+26\n",
      "Gradient Descent(15/49): loss=2.244182593484573e+28\n",
      "Gradient Descent(16/49): loss=2.2244432440677346e+30\n",
      "Gradient Descent(17/49): loss=2.204877526654278e+32\n",
      "Gradient Descent(18/49): loss=2.1854839078635694e+34\n",
      "Gradient Descent(19/49): loss=2.166260871992987e+36\n",
      "Gradient Descent(20/49): loss=2.1472069180075884e+38\n",
      "Gradient Descent(21/49): loss=2.1283205585010262e+40\n",
      "Gradient Descent(22/49): loss=2.109600319285631e+42\n",
      "Gradient Descent(23/49): loss=2.091044739183627e+44\n",
      "Gradient Descent(24/49): loss=2.0726523698832417e+46\n",
      "Gradient Descent(25/49): loss=2.0544217758161414e+48\n",
      "Gradient Descent(26/49): loss=2.036351534042304e+50\n",
      "Gradient Descent(27/49): loss=2.018440234138052e+52\n",
      "Gradient Descent(28/49): loss=2.0006864780856215e+54\n",
      "Gradient Descent(29/49): loss=1.983088880163934e+56\n",
      "Gradient Descent(30/49): loss=1.9656460668404362e+58\n",
      "Gradient Descent(31/49): loss=1.9483566766638702e+60\n",
      "Gradient Descent(32/49): loss=1.9312193601580003e+62\n",
      "Gradient Descent(33/49): loss=1.9142327797163016e+64\n",
      "Gradient Descent(34/49): loss=1.897395609497521e+66\n",
      "Gradient Descent(35/49): loss=1.8807065353222165e+68\n",
      "Gradient Descent(36/49): loss=1.8641642545701836e+70\n",
      "Gradient Descent(37/49): loss=1.8477674760787995e+72\n",
      "Gradient Descent(38/49): loss=1.831514920042173e+74\n",
      "Gradient Descent(39/49): loss=1.8154053179113686e+76\n",
      "Gradient Descent(40/49): loss=1.7994374122952689e+78\n",
      "Gradient Descent(41/49): loss=1.783609956862516e+80\n",
      "Gradient Descent(42/49): loss=1.767921716244222e+82\n",
      "Gradient Descent(43/49): loss=1.7523714659375103e+84\n",
      "Gradient Descent(44/49): loss=1.736957992209972e+86\n",
      "Gradient Descent(45/49): loss=1.7216800920049414e+88\n",
      "Gradient Descent(46/49): loss=1.706536572847539e+90\n",
      "Gradient Descent(47/49): loss=1.6915262527516352e+92\n",
      "Gradient Descent(48/49): loss=1.6766479601276073e+94\n",
      "Gradient Descent(49/49): loss=1.66190053369087e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.226341084867826\n",
      "Gradient Descent(2/49): loss=260.4723246988708\n",
      "Gradient Descent(3/49): loss=25366.426550053202\n",
      "Gradient Descent(4/49): loss=2508163.9670222024\n",
      "Gradient Descent(5/49): loss=248973671.00689226\n",
      "Gradient Descent(6/49): loss=24744049228.683884\n",
      "Gradient Descent(7/49): loss=2460106688069.195\n",
      "Gradient Descent(8/49): loss=244619447275067.78\n",
      "Gradient Descent(9/49): loss=2.432459725880005e+16\n",
      "Gradient Descent(10/49): loss=2.4188343743244385e+18\n",
      "Gradient Descent(11/49): loss=2.4052959060491682e+20\n",
      "Gradient Descent(12/49): loss=2.391836671669716e+22\n",
      "Gradient Descent(13/49): loss=2.3784538829835166e+24\n",
      "Gradient Descent(14/49): loss=2.3651463444280317e+26\n",
      "Gradient Descent(15/49): loss=2.3519133835083516e+28\n",
      "Gradient Descent(16/49): loss=2.3387545006026528e+30\n",
      "Gradient Descent(17/49): loss=2.3256692542710977e+32\n",
      "Gradient Descent(18/49): loss=2.312657223684343e+34\n",
      "Gradient Descent(19/49): loss=2.2997179963092016e+36\n",
      "Gradient Descent(20/49): loss=2.2868511638666657e+38\n",
      "Gradient Descent(21/49): loss=2.274056320999478e+40\n",
      "Gradient Descent(22/49): loss=2.2613330648270955e+42\n",
      "Gradient Descent(23/49): loss=2.2486809947914455e+44\n",
      "Gradient Descent(24/49): loss=2.236099712597979e+46\n",
      "Gradient Descent(25/49): loss=2.223588822187907e+48\n",
      "Gradient Descent(26/49): loss=2.211147929720783e+50\n",
      "Gradient Descent(27/49): loss=2.1987766435604585e+52\n",
      "Gradient Descent(28/49): loss=2.1864745742622794e+54\n",
      "Gradient Descent(29/49): loss=2.1742413345605405e+56\n",
      "Gradient Descent(30/49): loss=2.1620765393563404e+58\n",
      "Gradient Descent(31/49): loss=2.149979805705396e+60\n",
      "Gradient Descent(32/49): loss=2.137950752805972e+62\n",
      "Gradient Descent(33/49): loss=2.125989001986903e+64\n",
      "Gradient Descent(34/49): loss=2.1140941766956964e+66\n",
      "Gradient Descent(35/49): loss=2.1022659024866263e+68\n",
      "Gradient Descent(36/49): loss=2.0905038070090005e+70\n",
      "Gradient Descent(37/49): loss=2.0788075199954034e+72\n",
      "Gradient Descent(38/49): loss=2.067176673250056e+74\n",
      "Gradient Descent(39/49): loss=2.0556109006372166e+76\n",
      "Gradient Descent(40/49): loss=2.0441098380696516e+78\n",
      "Gradient Descent(41/49): loss=2.0326731234972025e+80\n",
      "Gradient Descent(42/49): loss=2.0213003968953373e+82\n",
      "Gradient Descent(43/49): loss=2.00999130025389e+84\n",
      "Gradient Descent(44/49): loss=1.9987454775656933e+86\n",
      "Gradient Descent(45/49): loss=1.9875625748154722e+88\n",
      "Gradient Descent(46/49): loss=1.9764422399686314e+90\n",
      "Gradient Descent(47/49): loss=1.9653841229601956e+92\n",
      "Gradient Descent(48/49): loss=1.9543878756838035e+94\n",
      "Gradient Descent(49/49): loss=1.943453151980716e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2653740067672468\n",
      "Gradient Descent(2/49): loss=269.5174950792262\n",
      "Gradient Descent(3/49): loss=26785.70997263399\n",
      "Gradient Descent(4/49): loss=2700877.508091328\n",
      "Gradient Descent(5/49): loss=273302576.1773477\n",
      "Gradient Descent(6/49): loss=27684048001.53761\n",
      "Gradient Descent(7/49): loss=2805118717542.5205\n",
      "Gradient Descent(8/49): loss=284259445301447.75\n",
      "Gradient Descent(9/49): loss=2.880657728633007e+16\n",
      "Gradient Descent(10/49): loss=2.9192582257369165e+18\n",
      "Gradient Descent(11/49): loss=2.958384709024476e+20\n",
      "Gradient Descent(12/49): loss=2.9980383775986805e+22\n",
      "Gradient Descent(13/49): loss=3.03822443948475e+24\n",
      "Gradient Descent(14/49): loss=3.07894944053936e+26\n",
      "Gradient Descent(15/49): loss=3.1202204172102253e+28\n",
      "Gradient Descent(16/49): loss=3.1620446282687395e+30\n",
      "Gradient Descent(17/49): loss=3.2044294704612528e+32\n",
      "Gradient Descent(18/49): loss=3.247382452608108e+34\n",
      "Gradient Descent(19/49): loss=3.2909111882875746e+36\n",
      "Gradient Descent(20/49): loss=3.3350233944376124e+38\n",
      "Gradient Descent(21/49): loss=3.3797268918507876e+40\n",
      "Gradient Descent(22/49): loss=3.4250296062834176e+42\n",
      "Gradient Descent(23/49): loss=3.4709395697725588e+44\n",
      "Gradient Descent(24/49): loss=3.517464922032006e+46\n",
      "Gradient Descent(25/49): loss=3.564613911886581e+48\n",
      "Gradient Descent(26/49): loss=3.6123948987317645e+50\n",
      "Gradient Descent(27/49): loss=3.6608163540149833e+52\n",
      "Gradient Descent(28/49): loss=3.7098868627371264e+54\n",
      "Gradient Descent(29/49): loss=3.759615124974726e+56\n",
      "Gradient Descent(30/49): loss=3.8100099574223283e+58\n",
      "Gradient Descent(31/49): loss=3.8610802949556835e+60\n",
      "Gradient Descent(32/49): loss=3.912835192215878e+62\n",
      "Gradient Descent(33/49): loss=3.965283825214755e+64\n",
      "Gradient Descent(34/49): loss=4.018435492961665e+66\n",
      "Gradient Descent(35/49): loss=4.0722996191122627e+68\n",
      "Gradient Descent(36/49): loss=4.126885753639258e+70\n",
      "Gradient Descent(37/49): loss=4.182203574525624e+72\n",
      "Gradient Descent(38/49): loss=4.2382628894804584e+74\n",
      "Gradient Descent(39/49): loss=4.295073637677917e+76\n",
      "Gradient Descent(40/49): loss=4.352645891519299e+78\n",
      "Gradient Descent(41/49): loss=4.4109898584189313e+80\n",
      "Gradient Descent(42/49): loss=4.4701158826139225e+82\n",
      "Gradient Descent(43/49): loss=4.53003444699815e+84\n",
      "Gradient Descent(44/49): loss=4.5907561749808665e+86\n",
      "Gradient Descent(45/49): loss=4.652291832370134e+88\n",
      "Gradient Descent(46/49): loss=4.714652329281708e+90\n",
      "Gradient Descent(47/49): loss=4.77784872207325e+92\n",
      "Gradient Descent(48/49): loss=4.8418922153046144e+94\n",
      "Gradient Descent(49/49): loss=4.906794163724477e+96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2535641077544653\n",
      "Gradient Descent(2/49): loss=267.65073627784443\n",
      "Gradient Descent(3/49): loss=26599.682416578064\n",
      "Gradient Descent(4/49): loss=2685522.299347229\n",
      "Gradient Descent(5/49): loss=272223506.07189286\n",
      "Gradient Descent(6/49): loss=27627610215.492367\n",
      "Gradient Descent(7/49): loss=2804934164884.839\n",
      "Gradient Descent(8/49): loss=284808553249776.94\n",
      "Gradient Descent(9/49): loss=2.8920086621262656e+16\n",
      "Gradient Descent(10/49): loss=2.936644099114572e+18\n",
      "Gradient Descent(11/49): loss=2.9819797594580596e+20\n",
      "Gradient Descent(12/49): loss=3.028018977061251e+22\n",
      "Gradient Descent(13/49): loss=3.0747701906837394e+24\n",
      "Gradient Descent(14/49): loss=3.122243607329235e+26\n",
      "Gradient Descent(15/49): loss=3.170450122700597e+28\n",
      "Gradient Descent(16/49): loss=3.219400972964773e+30\n",
      "Gradient Descent(17/49): loss=3.269107623632184e+32\n",
      "Gradient Descent(18/49): loss=3.3195817353546095e+34\n",
      "Gradient Descent(19/49): loss=3.370835154696968e+36\n",
      "Gradient Descent(20/49): loss=3.4228799130376395e+38\n",
      "Gradient Descent(21/49): loss=3.4757282281342814e+40\n",
      "Gradient Descent(22/49): loss=3.5293925065837923e+42\n",
      "Gradient Descent(23/49): loss=3.5838853466025036e+44\n",
      "Gradient Descent(24/49): loss=3.639219540940775e+46\n",
      "Gradient Descent(25/49): loss=3.6954080798723295e+48\n",
      "Gradient Descent(26/49): loss=3.752464154239339e+50\n",
      "Gradient Descent(27/49): loss=3.810401158547639e+52\n",
      "Gradient Descent(28/49): loss=3.869232694110736e+54\n",
      "Gradient Descent(29/49): loss=3.928972572242705e+56\n",
      "Gradient Descent(30/49): loss=3.989634817500499e+58\n",
      "Gradient Descent(31/49): loss=4.051233670976361e+60\n",
      "Gradient Descent(32/49): loss=4.113783593641025e+62\n",
      "Gradient Descent(33/49): loss=4.177299269738628e+64\n",
      "Gradient Descent(34/49): loss=4.241795610233915e+66\n",
      "Gradient Descent(35/49): loss=4.3072877563128497e+68\n",
      "Gradient Descent(36/49): loss=4.373791082937031e+70\n",
      "Gradient Descent(37/49): loss=4.441321202453261e+72\n",
      "Gradient Descent(38/49): loss=4.509893968258549e+74\n",
      "Gradient Descent(39/49): loss=4.579525478522033e+76\n",
      "Gradient Descent(40/49): loss=4.650232079964089e+78\n",
      "Gradient Descent(41/49): loss=4.722030371693918e+80\n",
      "Gradient Descent(42/49): loss=4.794937209106341e+82\n",
      "Gradient Descent(43/49): loss=4.868969707838804e+84\n",
      "Gradient Descent(44/49): loss=4.944145247789478e+86\n",
      "Gradient Descent(45/49): loss=5.020481477197236e+88\n",
      "Gradient Descent(46/49): loss=5.097996316784965e+90\n",
      "Gradient Descent(47/49): loss=5.1767079639664146e+92\n",
      "Gradient Descent(48/49): loss=5.256634897118493e+94\n",
      "Gradient Descent(49/49): loss=5.337795879919054e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2641239892459732\n",
      "Gradient Descent(2/49): loss=266.6142995468631\n",
      "Gradient Descent(3/49): loss=26261.524074492456\n",
      "Gradient Descent(4/49): loss=2626473.424600353\n",
      "Gradient Descent(5/49): loss=263684630.0349191\n",
      "Gradient Descent(6/49): loss=26502646675.460796\n",
      "Gradient Descent(7/49): loss=2664690298086.446\n",
      "Gradient Descent(8/49): loss=267949374424326.25\n",
      "Gradient Descent(9/49): loss=2.694475671947504e+16\n",
      "Gradient Descent(10/49): loss=2.709572435543474e+18\n",
      "Gradient Descent(11/49): loss=2.7247638292025256e+20\n",
      "Gradient Descent(12/49): loss=2.740043644611382e+22\n",
      "Gradient Descent(13/49): loss=2.755410197494576e+24\n",
      "Gradient Descent(14/49): loss=2.7708632687102442e+26\n",
      "Gradient Descent(15/49): loss=2.786403115083991e+28\n",
      "Gradient Descent(16/49): loss=2.8020301493427657e+30\n",
      "Gradient Descent(17/49): loss=2.8177448365292594e+32\n",
      "Gradient Descent(18/49): loss=2.8335476604808633e+34\n",
      "Gradient Descent(19/49): loss=2.8494391129886606e+36\n",
      "Gradient Descent(20/49): loss=2.8654196902982098e+38\n",
      "Gradient Descent(21/49): loss=2.881489891987368e+40\n",
      "Gradient Descent(22/49): loss=2.8976502206135744e+42\n",
      "Gradient Descent(23/49): loss=2.913901181610314e+44\n",
      "Gradient Descent(24/49): loss=2.9302432832644037e+46\n",
      "Gradient Descent(25/49): loss=2.946677036719247e+48\n",
      "Gradient Descent(26/49): loss=2.9632029559868867e+50\n",
      "Gradient Descent(27/49): loss=2.979821557962767e+52\n",
      "Gradient Descent(28/49): loss=2.9965333624414437e+54\n",
      "Gradient Descent(29/49): loss=3.013338892132699e+56\n",
      "Gradient Descent(30/49): loss=3.0302386726778816e+58\n",
      "Gradient Descent(31/49): loss=3.0472332326662824e+60\n",
      "Gradient Descent(32/49): loss=3.0643231036517437e+62\n",
      "Gradient Descent(33/49): loss=3.0815088201692015e+64\n",
      "Gradient Descent(34/49): loss=3.0987909197514588e+66\n",
      "Gradient Descent(35/49): loss=3.116169942945939e+68\n",
      "Gradient Descent(36/49): loss=3.133646433331652e+70\n",
      "Gradient Descent(37/49): loss=3.1512209375361947e+72\n",
      "Gradient Descent(38/49): loss=3.1688940052528055e+74\n",
      "Gradient Descent(39/49): loss=3.1866661892575664e+76\n",
      "Gradient Descent(40/49): loss=3.2045380454267383e+78\n",
      "Gradient Descent(41/49): loss=3.222510132754113e+80\n",
      "Gradient Descent(42/49): loss=3.240583013368477e+82\n",
      "Gradient Descent(43/49): loss=3.2587572525512526e+84\n",
      "Gradient Descent(44/49): loss=3.277033418754122e+86\n",
      "Gradient Descent(45/49): loss=3.295412083616806e+88\n",
      "Gradient Descent(46/49): loss=3.31389382198499e+90\n",
      "Gradient Descent(47/49): loss=3.3324792119282867e+92\n",
      "Gradient Descent(48/49): loss=3.351168834758314e+94\n",
      "Gradient Descent(49/49): loss=3.369963275046834e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.268074027696975\n",
      "Gradient Descent(2/49): loss=268.05259768829757\n",
      "Gradient Descent(3/49): loss=26481.49304718985\n",
      "Gradient Descent(4/49): loss=2656173.797413716\n",
      "Gradient Descent(5/49): loss=267470088.1968489\n",
      "Gradient Descent(6/49): loss=26965933897.86653\n",
      "Gradient Descent(7/49): loss=2719707525818.7666\n",
      "Gradient Descent(8/49): loss=274336172183251.44\n",
      "Gradient Descent(9/49): loss=2.7673346996211616e+16\n",
      "Gradient Descent(10/49): loss=2.791554672376991e+18\n",
      "Gradient Descent(11/49): loss=2.815999081451294e+20\n",
      "Gradient Descent(12/49): loss=2.8406616818484234e+22\n",
      "Gradient Descent(13/49): loss=2.8655416554607004e+24\n",
      "Gradient Descent(14/49): loss=2.8906399987411683e+26\n",
      "Gradient Descent(15/49): loss=2.9159583225580067e+28\n",
      "Gradient Descent(16/49): loss=2.9414984533040627e+30\n",
      "Gradient Descent(17/49): loss=2.9672623003530413e+32\n",
      "Gradient Descent(18/49): loss=2.9932518120795606e+34\n",
      "Gradient Descent(19/49): loss=3.019468961332583e+36\n",
      "Gradient Descent(20/49): loss=3.045915740704834e+38\n",
      "Gradient Descent(21/49): loss=3.072594161061206e+40\n",
      "Gradient Descent(22/49): loss=3.099506251151781e+42\n",
      "Gradient Descent(23/49): loss=3.1266540575866275e+44\n",
      "Gradient Descent(24/49): loss=3.15403964493173e+46\n",
      "Gradient Descent(25/49): loss=3.181665095846106e+48\n",
      "Gradient Descent(26/49): loss=3.2095325112335947e+50\n",
      "Gradient Descent(27/49): loss=3.237644010400461e+52\n",
      "Gradient Descent(28/49): loss=3.266001731215778e+54\n",
      "Gradient Descent(29/49): loss=3.294607830273864e+56\n",
      "Gradient Descent(30/49): loss=3.3234644830580827e+58\n",
      "Gradient Descent(31/49): loss=3.352573884106363e+60\n",
      "Gradient Descent(32/49): loss=3.3819382471780768e+62\n",
      "Gradient Descent(33/49): loss=3.411559805422356e+64\n",
      "Gradient Descent(34/49): loss=3.441440811547892e+66\n",
      "Gradient Descent(35/49): loss=3.4715835379943306e+68\n",
      "Gradient Descent(36/49): loss=3.501990277105048e+70\n",
      "Gradient Descent(37/49): loss=3.532663341301506e+72\n",
      "Gradient Descent(38/49): loss=3.563605063259041e+74\n",
      "Gradient Descent(39/49): loss=3.5948177960843634e+76\n",
      "Gradient Descent(40/49): loss=3.6263039134944236e+78\n",
      "Gradient Descent(41/49): loss=3.658065809996998e+80\n",
      "Gradient Descent(42/49): loss=3.690105901072761e+82\n",
      "Gradient Descent(43/49): loss=3.722426623358971e+84\n",
      "Gradient Descent(44/49): loss=3.755030434834773e+86\n",
      "Gradient Descent(45/49): loss=3.787919815008169e+88\n",
      "Gradient Descent(46/49): loss=3.821097265104601e+90\n",
      "Gradient Descent(47/49): loss=3.854565308256991e+92\n",
      "Gradient Descent(48/49): loss=3.8883264896978896e+94\n",
      "Gradient Descent(49/49): loss=3.9223833769528344e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3076915373265567\n",
      "Gradient Descent(2/49): loss=277.35389631276456\n",
      "Gradient Descent(3/49): loss=27962.06972258911\n",
      "Gradient Descent(4/49): loss=2860115.1912300657\n",
      "Gradient Descent(5/49): loss=293588189.60680014\n",
      "Gradient Descent(6/49): loss=30167699490.15203\n",
      "Gradient Descent(7/49): loss=3100860108016.023\n",
      "Gradient Descent(8/49): loss=318760412717071.94\n",
      "Gradient Descent(9/49): loss=3.2768741756832412e+16\n",
      "Gradient Descent(10/49): loss=3.368676276550784e+18\n",
      "Gradient Descent(11/49): loss=3.4630605460881906e+20\n",
      "Gradient Descent(12/49): loss=3.5600926223675395e+22\n",
      "Gradient Descent(13/49): loss=3.659844530709492e+24\n",
      "Gradient Descent(14/49): loss=3.762391781199595e+26\n",
      "Gradient Descent(15/49): loss=3.867812472832373e+28\n",
      "Gradient Descent(16/49): loss=3.9761870455702264e+30\n",
      "Gradient Descent(17/49): loss=4.0875982425337985e+32\n",
      "Gradient Descent(18/49): loss=4.20213114116978e+34\n",
      "Gradient Descent(19/49): loss=4.319873207891315e+36\n",
      "Gradient Descent(20/49): loss=4.440914361541506e+38\n",
      "Gradient Descent(21/49): loss=4.565347040988243e+40\n",
      "Gradient Descent(22/49): loss=4.6932662753726806e+42\n",
      "Gradient Descent(23/49): loss=4.824769756570932e+44\n",
      "Gradient Descent(24/49): loss=4.959957913765062e+46\n",
      "Gradient Descent(25/49): loss=5.098933990129295e+48\n",
      "Gradient Descent(26/49): loss=5.2418041216729366e+50\n",
      "Gradient Descent(27/49): loss=5.388677418295319e+52\n",
      "Gradient Descent(28/49): loss=5.539666047112527e+54\n",
      "Gradient Descent(29/49): loss=5.694885318119388e+56\n",
      "Gradient Descent(30/49): loss=5.854453772251545e+58\n",
      "Gradient Descent(31/49): loss=6.0184932719151954e+60\n",
      "Gradient Descent(32/49): loss=6.187129094053479e+62\n",
      "Gradient Descent(33/49): loss=6.360490025820234e+64\n",
      "Gradient Descent(34/49): loss=6.538708462935009e+66\n",
      "Gradient Descent(35/49): loss=6.721920510793394e+68\n",
      "Gradient Descent(36/49): loss=6.91026608841087e+70\n",
      "Gradient Descent(37/49): loss=7.10388903527887e+72\n",
      "Gradient Descent(38/49): loss=7.302937221214962e+74\n",
      "Gradient Descent(39/49): loss=7.50756265929109e+76\n",
      "Gradient Descent(40/49): loss=7.717921621925845e+78\n",
      "Gradient Descent(41/49): loss=7.934174760229682e+80\n",
      "Gradient Descent(42/49): loss=8.15648722669429e+82\n",
      "Gradient Descent(43/49): loss=8.385028801319359e+84\n",
      "Gradient Descent(44/49): loss=8.619974021273628e+86\n",
      "Gradient Descent(45/49): loss=8.861502314188945e+88\n",
      "Gradient Descent(46/49): loss=9.109798135189043e+90\n",
      "Gradient Descent(47/49): loss=9.365051107757762e+92\n",
      "Gradient Descent(48/49): loss=9.627456168554844e+94\n",
      "Gradient Descent(49/49): loss=9.897213716288623e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.295670831606127\n",
      "Gradient Descent(2/49): loss=275.43338581795064\n",
      "Gradient Descent(3/49): loss=27767.84661086086\n",
      "Gradient Descent(4/49): loss=2843844.0012068357\n",
      "Gradient Descent(5/49): loss=292427318.64500606\n",
      "Gradient Descent(6/49): loss=30105972755.977665\n",
      "Gradient Descent(7/49): loss=3100628364832.301\n",
      "Gradient Descent(8/49): loss=319372876761130.9\n",
      "Gradient Descent(9/49): loss=3.2897481339904452e+16\n",
      "Gradient Descent(10/49): loss=3.388694905312e+18\n",
      "Gradient Descent(11/49): loss=3.4906310833419164e+20\n",
      "Gradient Descent(12/49): loss=3.5956380253416858e+22\n",
      "Gradient Descent(13/49): loss=3.703805287299961e+24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/49): loss=3.815227012205794e+26\n",
      "Gradient Descent(15/49): loss=3.9300007983907515e+28\n",
      "Gradient Descent(16/49): loss=4.0482273854771987e+30\n",
      "Gradient Descent(17/49): loss=4.170010611234818e+32\n",
      "Gradient Descent(18/49): loss=4.2954574594541325e+34\n",
      "Gradient Descent(19/49): loss=4.4246781396603146e+36\n",
      "Gradient Descent(20/49): loss=4.55778617923703e+38\n",
      "Gradient Descent(21/49): loss=4.694898521618851e+40\n",
      "Gradient Descent(22/49): loss=4.8361356285233733e+42\n",
      "Gradient Descent(23/49): loss=4.981621585616427e+44\n",
      "Gradient Descent(24/49): loss=5.131484211473924e+46\n",
      "Gradient Descent(25/49): loss=5.285855169860214e+48\n",
      "Gradient Descent(26/49): loss=5.44487008539711e+50\n",
      "Gradient Descent(27/49): loss=5.608668662716655e+52\n",
      "Gradient Descent(28/49): loss=5.777394809199769e+54\n",
      "Gradient Descent(29/49): loss=5.951196761407774e+56\n",
      "Gradient Descent(30/49): loss=6.130227215317474e+58\n",
      "Gradient Descent(31/49): loss=6.314643460474236e+60\n",
      "Gradient Descent(32/49): loss=6.504607518180701e+62\n",
      "Gradient Descent(33/49): loss=6.700286283842746e+64\n",
      "Gradient Descent(34/49): loss=6.901851673597661e+66\n",
      "Gradient Descent(35/49): loss=7.109480775353236e+68\n",
      "Gradient Descent(36/49): loss=7.323356004370667e+70\n",
      "Gradient Descent(37/49): loss=7.54366526352792e+72\n",
      "Gradient Descent(38/49): loss=7.770602108404256e+74\n",
      "Gradient Descent(39/49): loss=8.004365917331055e+76\n",
      "Gradient Descent(40/49): loss=8.24516206655803e+78\n",
      "Gradient Descent(41/49): loss=8.493202110689542e+80\n",
      "Gradient Descent(42/49): loss=8.748703968548445e+82\n",
      "Gradient Descent(43/49): loss=9.011892114631556e+84\n",
      "Gradient Descent(44/49): loss=9.282997776324715e+86\n",
      "Gradient Descent(45/49): loss=9.56225913705055e+88\n",
      "Gradient Descent(46/49): loss=9.849921545527736e+90\n",
      "Gradient Descent(47/49): loss=1.0146237731325272e+93\n",
      "Gradient Descent(48/49): loss=1.045146802690123e+95\n",
      "Gradient Descent(49/49): loss=1.0765880596321358e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.306439735069223\n",
      "Gradient Descent(2/49): loss=274.369807282576\n",
      "Gradient Descent(3/49): loss=27415.324179076637\n",
      "Gradient Descent(4/49): loss=2781382.4554242087\n",
      "Gradient Descent(5/49): loss=283263546.6481604\n",
      "Gradient Descent(6/49): loss=28881179588.854702\n",
      "Gradient Descent(7/49): loss=2945729526181.226\n",
      "Gradient Descent(8/49): loss=300482765802395.06\n",
      "Gradient Descent(9/49): loss=3.0652215798514236e+16\n",
      "Gradient Descent(10/49): loss=3.126865459477757e+18\n",
      "Gradient Descent(11/49): loss=3.1897608964227767e+20\n",
      "Gradient Descent(12/49): loss=3.2539253400906693e+22\n",
      "Gradient Descent(13/49): loss=3.3193817798028504e+24\n",
      "Gradient Descent(14/49): loss=3.386155371568925e+26\n",
      "Gradient Descent(15/49): loss=3.4542723374629163e+28\n",
      "Gradient Descent(16/49): loss=3.523759611115524e+30\n",
      "Gradient Descent(17/49): loss=3.594644728466726e+32\n",
      "Gradient Descent(18/49): loss=3.6669557992673646e+34\n",
      "Gradient Descent(19/49): loss=3.740721505271264e+36\n",
      "Gradient Descent(20/49): loss=3.8159711073532306e+38\n",
      "Gradient Descent(21/49): loss=3.892734455716032e+40\n",
      "Gradient Descent(22/49): loss=3.9710420012713886e+42\n",
      "Gradient Descent(23/49): loss=4.050924807567985e+44\n",
      "Gradient Descent(24/49): loss=4.132414563064438e+46\n",
      "Gradient Descent(25/49): loss=4.215543593683142e+48\n",
      "Gradient Descent(26/49): loss=4.300344875628234e+50\n",
      "Gradient Descent(27/49): loss=4.3868520484651347e+52\n",
      "Gradient Descent(28/49): loss=4.4750994284642366e+54\n",
      "Gradient Descent(29/49): loss=4.565122022213596e+56\n",
      "Gradient Descent(30/49): loss=4.6569555405055384e+58\n",
      "Gradient Descent(31/49): loss=4.7506364125025746e+60\n",
      "Gradient Descent(32/49): loss=4.846201800188397e+62\n",
      "Gradient Descent(33/49): loss=4.9436896131096213e+64\n",
      "Gradient Descent(34/49): loss=5.043138523413907e+66\n",
      "Gradient Descent(35/49): loss=5.144587981190766e+68\n",
      "Gradient Descent(36/49): loss=5.248078230120878e+70\n",
      "Gradient Descent(37/49): loss=5.353650323440207e+72\n",
      "Gradient Descent(38/49): loss=5.461346140225412e+74\n",
      "Gradient Descent(39/49): loss=5.5712084020064655e+76\n",
      "Gradient Descent(40/49): loss=5.683280689714062e+78\n",
      "Gradient Descent(41/49): loss=5.797607460967384e+80\n",
      "Gradient Descent(42/49): loss=5.9142340677099e+82\n",
      "Gradient Descent(43/49): loss=6.033206774199935e+84\n",
      "Gradient Descent(44/49): loss=6.154572775362982e+86\n",
      "Gradient Descent(45/49): loss=6.278380215513586e+88\n",
      "Gradient Descent(46/49): loss=6.404678207453236e+90\n",
      "Gradient Descent(47/49): loss=6.533516851953024e+92\n",
      "Gradient Descent(48/49): loss=6.664947257627848e+94\n",
      "Gradient Descent(49/49): loss=6.799021561210491e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3101175383190107\n",
      "Gradient Descent(2/49): loss=275.7974199338796\n",
      "Gradient Descent(3/49): loss=27637.113488984156\n",
      "Gradient Descent(4/49): loss=2811770.541716954\n",
      "Gradient Descent(5/49): loss=287194151.9377106\n",
      "Gradient Descent(6/49): loss=29369358658.206585\n",
      "Gradient Descent(7/49): loss=3004557289479.607\n",
      "Gradient Descent(8/49): loss=307412068551002.6\n",
      "Gradient Descent(9/49): loss=3.145423899177041e+16\n",
      "Gradient Descent(10/49): loss=3.2184244992178053e+18\n",
      "Gradient Descent(11/49): loss=3.2931340182798887e+20\n",
      "Gradient Descent(12/49): loss=3.3695827309012825e+22\n",
      "Gradient Descent(13/49): loss=3.4478078412629375e+24\n",
      "Gradient Descent(14/49): loss=3.5278495182588925e+26\n",
      "Gradient Descent(15/49): loss=3.609749572538956e+28\n",
      "Gradient Descent(16/49): loss=3.693551024852265e+30\n",
      "Gradient Descent(17/49): loss=3.7792979755397816e+32\n",
      "Gradient Descent(18/49): loss=3.8670355760465264e+34\n",
      "Gradient Descent(19/49): loss=3.9568100352411426e+36\n",
      "Gradient Descent(20/49): loss=4.04866863785873e+38\n",
      "Gradient Descent(21/49): loss=4.1426597674155792e+40\n",
      "Gradient Descent(22/49): loss=4.2388329310216915e+42\n",
      "Gradient Descent(23/49): loss=4.337238785230166e+44\n",
      "Gradient Descent(24/49): loss=4.4379291626426244e+46\n",
      "Gradient Descent(25/49): loss=4.5409570991846205e+48\n",
      "Gradient Descent(26/49): loss=4.646376862032224e+50\n",
      "Gradient Descent(27/49): loss=4.7542439781927744e+52\n",
      "Gradient Descent(28/49): loss=4.864615263751342e+54\n",
      "Gradient Descent(29/49): loss=4.977548853796568e+56\n",
      "Gradient Descent(30/49): loss=5.093104233041781e+58\n",
      "Gradient Descent(31/49): loss=5.211342267156856e+60\n",
      "Gradient Descent(32/49): loss=5.332325234827524e+62\n",
      "Gradient Descent(33/49): loss=5.456116860558995e+64\n",
      "Gradient Descent(34/49): loss=5.582782348241193e+66\n",
      "Gradient Descent(35/49): loss=5.712388415493041e+68\n",
      "Gradient Descent(36/49): loss=5.845003328804214e+70\n",
      "Gradient Descent(37/49): loss=5.980696939492648e+72\n",
      "Gradient Descent(38/49): loss=6.119540720496801e+74\n",
      "Gradient Descent(39/49): loss=6.261607804022123e+76\n",
      "Gradient Descent(40/49): loss=6.406973020061464e+78\n",
      "Gradient Descent(41/49): loss=6.555712935809861e+80\n",
      "Gradient Descent(42/49): loss=6.70790589599397e+82\n",
      "Gradient Descent(43/49): loss=6.8636320641382646e+84\n",
      "Gradient Descent(44/49): loss=7.022973464788962e+86\n",
      "Gradient Descent(45/49): loss=7.18601402671812e+88\n",
      "Gradient Descent(46/49): loss=7.352839627130943e+90\n",
      "Gradient Descent(47/49): loss=7.52353813689936e+92\n",
      "Gradient Descent(48/49): loss=7.69819946684534e+94\n",
      "Gradient Descent(49/49): loss=7.876915615099358e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3503239145176296\n",
      "Gradient Descent(2/49): loss=285.3602442309692\n",
      "Gradient Descent(3/49): loss=29181.172911231937\n",
      "Gradient Descent(4/49): loss=3027507.9693075526\n",
      "Gradient Descent(5/49): loss=315219002.4924786\n",
      "Gradient Descent(6/49): loss=32854098124.746506\n",
      "Gradient Descent(7/49): loss=3425339758755.558\n",
      "Gradient Descent(8/49): loss=357157854209405.56\n",
      "Gradient Descent(9/49): loss=3.7241744322855144e+16\n",
      "Gradient Descent(10/49): loss=3.8833271544684524e+18\n",
      "Gradient Descent(11/49): loss=4.0492934380857636e+20\n",
      "Gradient Descent(12/49): loss=4.222356800339006e+22\n",
      "Gradient Descent(13/49): loss=4.402818048786036e+24\n",
      "Gradient Descent(14/49): loss=4.5909925410666346e+26\n",
      "Gradient Descent(15/49): loss=4.7872096678444565e+28\n",
      "Gradient Descent(16/49): loss=4.991813079551945e+30\n",
      "Gradient Descent(17/49): loss=5.205161173648157e+32\n",
      "Gradient Descent(18/49): loss=5.427627684865495e+34\n",
      "Gradient Descent(19/49): loss=5.659602327601712e+36\n",
      "Gradient Descent(20/49): loss=5.901491474586958e+38\n",
      "Gradient Descent(21/49): loss=6.153718867445909e+40\n",
      "Gradient Descent(22/49): loss=6.416726358576157e+42\n",
      "Gradient Descent(23/49): loss=6.6909746850443884e+44\n",
      "Gradient Descent(24/49): loss=6.976944275659934e+46\n",
      "Gradient Descent(25/49): loss=7.275136092575744e+48\n",
      "Gradient Descent(26/49): loss=7.586072508870185e+50\n",
      "Gradient Descent(27/49): loss=7.910298223639454e+52\n",
      "Gradient Descent(28/49): loss=8.248381216202373e+54\n",
      "Gradient Descent(29/49): loss=8.600913741087501e+56\n",
      "Gradient Descent(30/49): loss=8.968513365545781e+58\n",
      "Gradient Descent(31/49): loss=9.351824051406303e+60\n",
      "Gradient Descent(32/49): loss=9.751517283170045e+62\n",
      "Gradient Descent(33/49): loss=1.0168293244317929e+65\n",
      "Gradient Descent(34/49): loss=1.0602882043893549e+67\n",
      "Gradient Descent(35/49): loss=1.1056044995510131e+69\n",
      "Gradient Descent(36/49): loss=1.1528575951021026e+71\n",
      "Gradient Descent(37/49): loss=1.2021302691191503e+73\n",
      "Gradient Descent(38/49): loss=1.253508837580674e+75\n",
      "Gradient Descent(39/49): loss=1.307083305575691e+77\n",
      "Gradient Descent(40/49): loss=1.3629475249748483e+79\n",
      "Gradient Descent(41/49): loss=1.4211993588403174e+81\n",
      "Gradient Descent(42/49): loss=1.481940852862551e+83\n",
      "Gradient Descent(43/49): loss=1.5452784141241302e+85\n",
      "Gradient Descent(44/49): loss=1.611322997503902e+87\n",
      "Gradient Descent(45/49): loss=1.68019030004803e+89\n",
      "Gradient Descent(46/49): loss=1.752000963648289e+91\n",
      "Gradient Descent(47/49): loss=1.8268807863828185e+93\n",
      "Gradient Descent(48/49): loss=1.904960942889472e+95\n",
      "Gradient Descent(49/49): loss=1.9863782141578312e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3380906667931196\n",
      "Gradient Descent(2/49): loss=283.3848311692809\n",
      "Gradient Descent(3/49): loss=28978.455706063472\n",
      "Gradient Descent(4/49): loss=3010273.3576756027\n",
      "Gradient Descent(5/49): loss=313970794.5530338\n",
      "Gradient Descent(6/49): loss=32786632445.96328\n",
      "Gradient Descent(7/49): loss=3425053566516.7534\n",
      "Gradient Descent(8/49): loss=357840460983194.3\n",
      "Gradient Descent(9/49): loss=3.7387628866317816e+16\n",
      "Gradient Descent(10/49): loss=3.906354373936363e+18\n",
      "Gradient Descent(11/49): loss=4.08147395217355e+20\n",
      "Gradient Descent(12/49): loss=4.264449298931176e+22\n",
      "Gradient Descent(13/49): loss=4.455629317817818e+24\n",
      "Gradient Descent(14/49): loss=4.65538073823655e+26\n",
      "Gradient Descent(15/49): loss=4.864087460629913e+28\n",
      "Gradient Descent(16/49): loss=5.082150840295898e+30\n",
      "Gradient Descent(17/49): loss=5.309990307432978e+32\n",
      "Gradient Descent(18/49): loss=5.54804412313531e+34\n",
      "Gradient Descent(19/49): loss=5.796770205441048e+36\n",
      "Gradient Descent(20/49): loss=6.0566470045049806e+38\n",
      "Gradient Descent(21/49): loss=6.328174421049228e+40\n",
      "Gradient Descent(22/49): loss=6.611874767341058e+42\n",
      "Gradient Descent(23/49): loss=6.908293771704777e+44\n",
      "Gradient Descent(24/49): loss=7.218001628218228e+46\n",
      "Gradient Descent(25/49): loss=7.541594093519316e+48\n",
      "Gradient Descent(26/49): loss=7.879693632800344e+50\n",
      "Gradient Descent(27/49): loss=8.232950617184762e+52\n",
      "Gradient Descent(28/49): loss=8.602044574785754e+54\n",
      "Gradient Descent(29/49): loss=8.98768549785179e+56\n",
      "Gradient Descent(30/49): loss=9.39061520851373e+58\n",
      "Gradient Descent(31/49): loss=9.811608785759862e+60\n",
      "Gradient Descent(32/49): loss=1.0251476056384645e+63\n",
      "Gradient Descent(33/49): loss=1.0711063152778223e+65\n",
      "Gradient Descent(34/49): loss=1.1191254140553918e+67\n",
      "Gradient Descent(35/49): loss=1.1692972719144006e+69\n",
      "Gradient Descent(36/49): loss=1.2217183998636242e+71\n",
      "Gradient Descent(37/49): loss=1.2764896356267156e+73\n",
      "Gradient Descent(38/49): loss=1.3337163376145591e+75\n",
      "Gradient Descent(39/49): loss=1.3935085875935485e+77\n",
      "Gradient Descent(40/49): loss=1.455981402439832e+79\n",
      "Gradient Descent(41/49): loss=1.5212549553867347e+81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=1.5894548071910157e+83\n",
      "Gradient Descent(43/49): loss=1.6607121476625693e+85\n",
      "Gradient Descent(44/49): loss=1.7351640480222628e+87\n",
      "Gradient Descent(45/49): loss=1.8129537245733196e+89\n",
      "Gradient Descent(46/49): loss=1.8942308141933608e+91\n",
      "Gradient Descent(47/49): loss=1.9791516621772197e+93\n",
      "Gradient Descent(48/49): loss=2.0678796229840107e+95\n",
      "Gradient Descent(49/49): loss=2.160585374467163e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.349070410450382\n",
      "Gradient Descent(2/49): loss=282.2935867938457\n",
      "Gradient Descent(3/49): loss=28611.064785995535\n",
      "Gradient Descent(4/49): loss=2944227.856845575\n",
      "Gradient Descent(5/49): loss=304141273.2505634\n",
      "Gradient Descent(6/49): loss=31453949080.80934\n",
      "Gradient Descent(7/49): loss=3254088301687.966\n",
      "Gradient Descent(8/49): loss=336691762788671.4\n",
      "Gradient Descent(9/49): loss=3.483785025925462e+16\n",
      "Gradient Descent(10/49): loss=3.6047508571552425e+18\n",
      "Gradient Descent(11/49): loss=3.729930897590847e+20\n",
      "Gradient Descent(12/49): loss=3.8594626508581704e+22\n",
      "Gradient Descent(13/49): loss=3.993494293126359e+24\n",
      "Gradient Descent(14/49): loss=4.1321811129614397e+26\n",
      "Gradient Descent(15/49): loss=4.275684447917624e+28\n",
      "Gradient Descent(16/49): loss=4.424171457461368e+30\n",
      "Gradient Descent(17/49): loss=4.577815179284192e+32\n",
      "Gradient Descent(18/49): loss=4.73679468476071e+34\n",
      "Gradient Descent(19/49): loss=4.901295272240389e+36\n",
      "Gradient Descent(20/49): loss=5.071508677876625e+38\n",
      "Gradient Descent(21/49): loss=5.247633297390501e+40\n",
      "Gradient Descent(22/49): loss=5.429874416741863e+42\n",
      "Gradient Descent(23/49): loss=5.618444451213673e+44\n",
      "Gradient Descent(24/49): loss=5.813563192933567e+46\n",
      "Gradient Descent(25/49): loss=6.01545806703631e+48\n",
      "Gradient Descent(26/49): loss=6.224364396737259e+50\n",
      "Gradient Descent(27/49): loss=6.44052567761644e+52\n",
      "Gradient Descent(28/49): loss=6.664193861429598e+54\n",
      "Gradient Descent(29/49): loss=6.8956296497760805e+56\n",
      "Gradient Descent(30/49): loss=7.135102797965547e+58\n",
      "Gradient Descent(31/49): loss=7.38289242943735e+60\n",
      "Gradient Descent(32/49): loss=7.639287361099489e+62\n",
      "Gradient Descent(33/49): loss=7.904586439965502e+64\n",
      "Gradient Descent(34/49): loss=8.179098891482696e+66\n",
      "Gradient Descent(35/49): loss=8.463144679956885e+68\n",
      "Gradient Descent(36/49): loss=8.757054881494162e+70\n",
      "Gradient Descent(37/49): loss=9.061172069894425e+72\n",
      "Gradient Descent(38/49): loss=9.375850715945929e+74\n",
      "Gradient Descent(39/49): loss=9.701457600586965e+76\n",
      "Gradient Descent(40/49): loss=1.0038372242415942e+79\n",
      "Gradient Descent(41/49): loss=1.0386987340047674e+81\n",
      "Gradient Descent(42/49): loss=1.0747709229832737e+83\n",
      "Gradient Descent(43/49): loss=1.112095835947194e+85\n",
      "Gradient Descent(44/49): loss=1.1507169778078743e+87\n",
      "Gradient Descent(45/49): loss=1.1906793643260765e+89\n",
      "Gradient Descent(46/49): loss=1.232029574581159e+91\n",
      "Gradient Descent(47/49): loss=1.274815805262368e+93\n",
      "Gradient Descent(48/49): loss=1.3190879268456107e+95\n",
      "Gradient Descent(49/49): loss=1.3648975417211334e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3524716167339337\n",
      "Gradient Descent(2/49): loss=283.70915656675396\n",
      "Gradient Descent(3/49): loss=28834.459383764606\n",
      "Gradient Descent(4/49): loss=2975285.139028358\n",
      "Gradient Descent(5/49): loss=308217763.8772191\n",
      "Gradient Descent(6/49): loss=31967709274.16838\n",
      "Gradient Descent(7/49): loss=3316906124664.0205\n",
      "Gradient Descent(8/49): loss=344198918738020.1\n",
      "Gradient Descent(9/49): loss=3.5719368310094484e+16\n",
      "Gradient Descent(10/49): loss=3.7068409420808294e+18\n",
      "Gradient Descent(11/49): loss=3.84685736395263e+20\n",
      "Gradient Descent(12/49): loss=3.9921684692593206e+22\n",
      "Gradient Descent(13/49): loss=4.142970582628568e+24\n",
      "Gradient Descent(14/49): loss=4.2994698640485196e+26\n",
      "Gradient Descent(15/49): loss=4.461881090186317e+28\n",
      "Gradient Descent(16/49): loss=4.6304274349946994e+30\n",
      "Gradient Descent(17/49): loss=4.8053406001652305e+32\n",
      "Gradient Descent(18/49): loss=4.986861073248163e+34\n",
      "Gradient Descent(19/49): loss=5.1752384375929383e+36\n",
      "Gradient Descent(20/49): loss=5.3707317084178465e+38\n",
      "Gradient Descent(21/49): loss=5.573609686517811e+40\n",
      "Gradient Descent(22/49): loss=5.784151327028222e+42\n",
      "Gradient Descent(23/49): loss=6.002646122699357e+44\n",
      "Gradient Descent(24/49): loss=6.229394501848207e+46\n",
      "Gradient Descent(25/49): loss=6.464708241411087e+48\n",
      "Gradient Descent(26/49): loss=6.708910895622435e+50\n",
      "Gradient Descent(27/49): loss=6.962338240895026e+52\n",
      "Gradient Descent(28/49): loss=7.225338737508027e+54\n",
      "Gradient Descent(29/49): loss=7.498274008736398e+56\n",
      "Gradient Descent(30/49): loss=7.781519338079279e+58\n",
      "Gradient Descent(31/49): loss=8.075464185271358e+60\n",
      "Gradient Descent(32/49): loss=8.380512721786433e+62\n",
      "Gradient Descent(33/49): loss=8.697084386569452e+64\n",
      "Gradient Descent(34/49): loss=9.025614462761178e+66\n",
      "Gradient Descent(35/49): loss=9.366554676208724e+68\n",
      "Gradient Descent(36/49): loss=9.720373816584243e+70\n",
      "Gradient Descent(37/49): loss=1.0087558381966397e+73\n",
      "Gradient Descent(38/49): loss=1.0468613247771088e+75\n",
      "Gradient Descent(39/49): loss=1.0864062360950124e+77\n",
      "Gradient Descent(40/49): loss=1.1274449460413754e+79\n",
      "Gradient Descent(41/49): loss=1.1700338824666602e+81\n",
      "Gradient Descent(42/49): loss=1.2142316047685482e+83\n",
      "Gradient Descent(43/49): loss=1.2600988844105719e+85\n",
      "Gradient Descent(44/49): loss=1.3076987884823112e+87\n",
      "Gradient Descent(45/49): loss=1.3570967664160862e+89\n",
      "Gradient Descent(46/49): loss=1.4083607399793097e+91\n",
      "Gradient Descent(47/49): loss=1.461561196666311e+93\n",
      "Gradient Descent(48/49): loss=1.5167712866179688e+95\n",
      "Gradient Descent(49/49): loss=1.5740669232025205e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3932711383404657\n",
      "Gradient Descent(2/49): loss=293.5389792358117\n",
      "Gradient Descent(3/49): loss=30444.253155977833\n",
      "Gradient Descent(4/49): loss=3203411.2466563215\n",
      "Gradient Descent(5/49): loss=338273731.6476015\n",
      "Gradient Descent(6/49): loss=35758180140.06958\n",
      "Gradient Descent(7/49): loss=3781116506154.5283\n",
      "Gradient Descent(8/49): loss=399859455789613.56\n",
      "Gradient Descent(9/49): loss=4.228711155934042e+16\n",
      "Gradient Descent(10/49): loss=4.4721139181146885e+18\n",
      "Gradient Descent(11/49): loss=4.72954115809939e+20\n",
      "Gradient Descent(12/49): loss=5.0017913833658885e+22\n",
      "Gradient Descent(13/49): loss=5.289714941923185e+24\n",
      "Gradient Descent(14/49): loss=5.594213084211839e+26\n",
      "Gradient Descent(15/49): loss=5.916239589826193e+28\n",
      "Gradient Descent(16/49): loss=6.256803357921723e+30\n",
      "Gradient Descent(17/49): loss=6.616971435994374e+32\n",
      "Gradient Descent(18/49): loss=6.997872319204594e+34\n",
      "Gradient Descent(19/49): loss=7.400699471618881e+36\n",
      "Gradient Descent(20/49): loss=7.826715060791143e+38\n",
      "Gradient Descent(21/49): loss=8.277253910861942e+40\n",
      "Gradient Descent(22/49): loss=8.7537276843922e+42\n",
      "Gradient Descent(23/49): loss=9.257629305312913e+44\n",
      "Gradient Descent(24/49): loss=9.790537636608729e+46\n",
      "Gradient Descent(25/49): loss=1.0354122427311686e+49\n",
      "Gradient Descent(26/49): loss=1.0950149544280117e+51\n",
      "Gradient Descent(27/49): loss=1.158048650514486e+53\n",
      "Gradient Descent(28/49): loss=1.2247108329757554e+55\n",
      "Gradient Descent(29/49): loss=1.2952103728472919e+57\n",
      "Gradient Descent(30/49): loss=1.3697681646655534e+59\n",
      "Gradient Descent(31/49): loss=1.448617818591595e+61\n",
      "Gradient Descent(32/49): loss=1.5320063923762111e+63\n",
      "Gradient Descent(33/49): loss=1.6201951654601926e+65\n",
      "Gradient Descent(34/49): loss=1.713460457634926e+67\n",
      "Gradient Descent(35/49): loss=1.812094494828695e+69\n",
      "Gradient Descent(36/49): loss=1.9164063247312366e+71\n",
      "Gradient Descent(37/49): loss=2.026722785125569e+73\n",
      "Gradient Descent(38/49): loss=2.143389527960976e+75\n",
      "Gradient Descent(39/49): loss=2.2667721023761816e+77\n",
      "Gradient Descent(40/49): loss=2.397257100065697e+79\n",
      "Gradient Descent(41/49): loss=2.535253366578478e+81\n",
      "Gradient Descent(42/49): loss=2.6811932823439734e+83\n",
      "Gradient Descent(43/49): loss=2.835534117439421e+85\n",
      "Gradient Descent(44/49): loss=2.9987594643433923e+87\n",
      "Gradient Descent(45/49): loss=3.171380753164663e+89\n",
      "Gradient Descent(46/49): loss=3.353938854093946e+91\n",
      "Gradient Descent(47/49): loss=3.547005772099717e+93\n",
      "Gradient Descent(48/49): loss=3.751186439177803e+95\n",
      "Gradient Descent(49/49): loss=3.967120609770425e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3808236133154446\n",
      "Gradient Descent(2/49): loss=291.50749642040364\n",
      "Gradient Descent(3/49): loss=30232.734687086173\n",
      "Gradient Descent(4/49): loss=3185163.683259479\n",
      "Gradient Descent(5/49): loss=336932320.2131957\n",
      "Gradient Descent(6/49): loss=35684491038.19944\n",
      "Gradient Descent(7/49): loss=3780767723996.056\n",
      "Gradient Descent(8/49): loss=400619663730550.2\n",
      "Gradient Descent(9/49): loss=4.24522805417036e+16\n",
      "Gradient Descent(10/49): loss=4.49857600364408e+18\n",
      "Gradient Descent(11/49): loss=4.767061850127242e+20\n",
      "Gradient Descent(12/49): loss=5.051577864259267e+22\n",
      "Gradient Descent(13/49): loss=5.35307698774591e+24\n",
      "Gradient Descent(14/49): loss=5.672571553875308e+26\n",
      "Gradient Descent(15/49): loss=6.011135170184324e+28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=6.36990581157058e+30\n",
      "Gradient Descent(17/49): loss=6.750089470590029e+32\n",
      "Gradient Descent(18/49): loss=7.152964152069134e+34\n",
      "Gradient Descent(19/49): loss=7.579884149053642e+36\n",
      "Gradient Descent(20/49): loss=8.032284588533939e+38\n",
      "Gradient Descent(21/49): loss=8.511686253424568e+40\n",
      "Gradient Descent(22/49): loss=9.01970069401962e+42\n",
      "Gradient Descent(23/49): loss=9.558035645091436e+44\n",
      "Gradient Descent(24/49): loss=1.0128500766499214e+47\n",
      "Gradient Descent(25/49): loss=1.0733013726487456e+49\n",
      "Gradient Descent(26/49): loss=1.1373606648084511e+51\n",
      "Gradient Descent(27/49): loss=1.205243294025798e+53\n",
      "Gradient Descent(28/49): loss=1.2771774536785353e+55\n",
      "Gradient Descent(29/49): loss=1.3534049567173042e+57\n",
      "Gradient Descent(30/49): loss=1.4341820485409223e+59\n",
      "Gradient Descent(31/49): loss=1.5197802683876775e+61\n",
      "Gradient Descent(32/49): loss=1.6104873621381175e+63\n",
      "Gradient Descent(33/49): loss=1.7066082495979582e+65\n",
      "Gradient Descent(34/49): loss=1.8084660495125388e+67\n",
      "Gradient Descent(35/49): loss=1.9164031657587345e+69\n",
      "Gradient Descent(36/49): loss=2.0307824383653873e+71\n",
      "Gradient Descent(37/49): loss=2.151988363231735e+73\n",
      "Gradient Descent(38/49): loss=2.2804283846439307e+75\n",
      "Gradient Descent(39/49): loss=2.4165342649344514e+77\n",
      "Gradient Descent(40/49): loss=2.5607635358889408e+79\n",
      "Gradient Descent(41/49): loss=2.7136010367791338e+81\n",
      "Gradient Descent(42/49): loss=2.875560544192365e+83\n",
      "Gradient Descent(43/49): loss=3.0471864991363935e+85\n",
      "Gradient Descent(44/49): loss=3.2290558372253067e+87\n",
      "Gradient Descent(45/49): loss=3.421779928098886e+89\n",
      "Gradient Descent(46/49): loss=3.626006630594965e+91\n",
      "Gradient Descent(47/49): loss=3.842422470583508e+93\n",
      "Gradient Descent(48/49): loss=4.0717549487829997e+95\n",
      "Gradient Descent(49/49): loss=4.314774986317696e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3920160153894474\n",
      "Gradient Descent(2/49): loss=290.3880555768335\n",
      "Gradient Descent(3/49): loss=29849.95682362262\n",
      "Gradient Descent(4/49): loss=3115355.629558062\n",
      "Gradient Descent(5/49): loss=326393839.6501494\n",
      "Gradient Descent(6/49): loss=34235269582.030697\n",
      "Gradient Descent(7/49): loss=3592200019605.2017\n",
      "Gradient Descent(8/49): loss=376961081888424.1\n",
      "Gradient Descent(9/49): loss=3.955927377578175e+16\n",
      "Gradient Descent(10/49): loss=4.151501398830367e+18\n",
      "Gradient Descent(11/49): loss=4.356760695751941e+20\n",
      "Gradient Descent(12/49): loss=4.57217402923284e+22\n",
      "Gradient Descent(13/49): loss=4.7982400301467173e+24\n",
      "Gradient Descent(14/49): loss=5.0354842492462246e+26\n",
      "Gradient Descent(15/49): loss=5.28445898998769e+28\n",
      "Gradient Descent(16/49): loss=5.545744123913699e+30\n",
      "Gradient Descent(17/49): loss=5.819948281851935e+32\n",
      "Gradient Descent(18/49): loss=6.1077102176581635e+34\n",
      "Gradient Descent(19/49): loss=6.40970027789086e+36\n",
      "Gradient Descent(20/49): loss=6.726621957197816e+38\n",
      "Gradient Descent(21/49): loss=7.059213535027413e+40\n",
      "Gradient Descent(22/49): loss=7.408249794763326e+42\n",
      "Gradient Descent(23/49): loss=7.77454382836762e+44\n",
      "Gradient Descent(24/49): loss=8.158948930424742e+46\n",
      "Gradient Descent(25/49): loss=8.562360585892976e+48\n",
      "Gradient Descent(26/49): loss=8.985718556158292e+50\n",
      "Gradient Descent(27/49): loss=9.43000906823771e+52\n",
      "Gradient Descent(28/49): loss=9.89626711222804e+54\n",
      "Gradient Descent(29/49): loss=1.0385578852350887e+57\n",
      "Gradient Descent(30/49): loss=1.0899084157209511e+59\n",
      "Gradient Descent(31/49): loss=1.1437979255152127e+61\n",
      "Gradient Descent(32/49): loss=1.2003519520926962e+63\n",
      "Gradient Descent(33/49): loss=1.259702240012136e+65\n",
      "Gradient Descent(34/49): loss=1.3219870478196632e+67\n",
      "Gradient Descent(35/49): loss=1.387351470126873e+69\n",
      "Gradient Descent(36/49): loss=1.4559477756137249e+71\n",
      "Gradient Descent(37/49): loss=1.5279357617437048e+73\n",
      "Gradient Descent(38/49): loss=1.6034831270175275e+75\n",
      "Gradient Descent(39/49): loss=1.6827658616326093e+77\n",
      "Gradient Descent(40/49): loss=1.7659686574582636e+79\n",
      "Gradient Descent(41/49): loss=1.8532853382818514e+81\n",
      "Gradient Descent(42/49): loss=1.9449193113279576e+83\n",
      "Gradient Descent(43/49): loss=2.0410840411025452e+85\n",
      "Gradient Descent(44/49): loss=2.1420035466659057e+87\n",
      "Gradient Descent(45/49): loss=2.2479129234927977e+89\n",
      "Gradient Descent(46/49): loss=2.3590588911354918e+91\n",
      "Gradient Descent(47/49): loss=2.4757003679654364e+93\n",
      "Gradient Descent(48/49): loss=2.5981090743326314e+95\n",
      "Gradient Descent(49/49): loss=2.7265701655475253e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.395136262941741\n",
      "Gradient Descent(2/49): loss=291.7901896050149\n",
      "Gradient Descent(3/49): loss=30074.727441766063\n",
      "Gradient Descent(4/49): loss=3147060.4718406443\n",
      "Gradient Descent(5/49): loss=330616483.86436486\n",
      "Gradient Descent(6/49): loss=34775253354.05017\n",
      "Gradient Descent(7/49): loss=3659186653125.7095\n",
      "Gradient Descent(8/49): loss=385082462055043.25\n",
      "Gradient Descent(9/49): loss=4.052667409687373e+16\n",
      "Gradient Descent(10/49): loss=4.265148334187242e+18\n",
      "Gradient Descent(11/49): loss=4.4887899277282175e+20\n",
      "Gradient Descent(12/49): loss=4.724165157901724e+22\n",
      "Gradient Descent(13/49): loss=4.971885036557129e+24\n",
      "Gradient Descent(14/49): loss=5.232595396479094e+26\n",
      "Gradient Descent(15/49): loss=5.506976903452832e+28\n",
      "Gradient Descent(16/49): loss=5.795746251933766e+30\n",
      "Gradient Descent(17/49): loss=6.099657834805478e+32\n",
      "Gradient Descent(18/49): loss=6.419505643808741e+34\n",
      "Gradient Descent(19/49): loss=6.756125319416137e+36\n",
      "Gradient Descent(20/49): loss=7.110396325521318e+38\n",
      "Gradient Descent(21/49): loss=7.483244244191347e+40\n",
      "Gradient Descent(22/49): loss=7.875643192846061e+42\n",
      "Gradient Descent(23/49): loss=8.288618368917549e+44\n",
      "Gradient Descent(24/49): loss=8.723248728197679e+46\n",
      "Gradient Descent(25/49): loss=9.180669803718329e+48\n",
      "Gradient Descent(26/49): loss=9.662076672476497e+50\n",
      "Gradient Descent(27/49): loss=1.0168727077735874e+53\n",
      "Gradient Descent(28/49): loss=1.0701944715056701e+55\n",
      "Gradient Descent(29/49): loss=1.1263122690635931e+57\n",
      "Gradient Descent(30/49): loss=1.1853727160993496e+59\n",
      "Gradient Descent(31/49): loss=1.247530116351252e+61\n",
      "Gradient Descent(32/49): loss=1.312946864784202e+63\n",
      "Gradient Descent(33/49): loss=1.3817938718693874e+65\n",
      "Gradient Descent(34/49): loss=1.454251010111988e+67\n",
      "Gradient Descent(35/49): loss=1.5305075839934306e+69\n",
      "Gradient Descent(36/49): loss=1.6107628245559934e+71\n",
      "Gradient Descent(37/49): loss=1.6952264099220267e+73\n",
      "Gradient Descent(38/49): loss=1.784119013107518e+75\n",
      "Gradient Descent(39/49): loss=1.87767287856148e+77\n",
      "Gradient Descent(40/49): loss=1.9761324289372856e+79\n",
      "Gradient Descent(41/49): loss=2.079754903681281e+81\n",
      "Gradient Descent(42/49): loss=2.1888110311070695e+83\n",
      "Gradient Descent(43/49): loss=2.3035857357113724e+85\n",
      "Gradient Descent(44/49): loss=2.4243788825794465e+87\n",
      "Gradient Descent(45/49): loss=2.5515060608248256e+89\n",
      "Gradient Descent(46/49): loss=2.685299408110338e+91\n",
      "Gradient Descent(47/49): loss=2.8261084784046e+93\n",
      "Gradient Descent(48/49): loss=2.9743011552409216e+95\n",
      "Gradient Descent(49/49): loss=3.1302646128649392e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4365332087950655\n",
      "Gradient Descent(2/49): loss=301.89255913736906\n",
      "Gradient Descent(3/49): loss=31752.57058786204\n",
      "Gradient Descent(4/49): loss=3388193.2425143244\n",
      "Gradient Descent(5/49): loss=362835095.3046116\n",
      "Gradient Descent(6/49): loss=38895864894.27503\n",
      "Gradient Descent(7/49): loss=4170956513221.666\n",
      "Gradient Descent(8/49): loss=447312221826959.94\n",
      "Gradient Descent(9/49): loss=4.797326600310371e+16\n",
      "Gradient Descent(10/49): loss=5.145078697966636e+18\n",
      "Gradient Descent(11/49): loss=5.5180557430724434e+20\n",
      "Gradient Descent(12/49): loss=5.918076323984115e+22\n",
      "Gradient Descent(13/49): loss=6.347097527715982e+24\n",
      "Gradient Descent(14/49): loss=6.807220565417634e+26\n",
      "Gradient Descent(15/49): loss=7.300699731217244e+28\n",
      "Gradient Descent(16/49): loss=7.829952997106297e+30\n",
      "Gradient Descent(17/49): loss=8.397573708833414e+32\n",
      "Gradient Descent(18/49): loss=9.006343242070847e+34\n",
      "Gradient Descent(19/49): loss=9.659244614018269e+36\n",
      "Gradient Descent(20/49): loss=1.0359477094572035e+39\n",
      "Gradient Descent(21/49): loss=1.1110471881035498e+41\n",
      "Gradient Descent(22/49): loss=1.1915908910604566e+43\n",
      "Gradient Descent(23/49): loss=1.2779734892142383e+45\n",
      "Gradient Descent(24/49): loss=1.3706182645307222e+47\n",
      "Gradient Descent(25/49): loss=1.4699791841697718e+49\n",
      "Gradient Descent(26/49): loss=1.576543124961547e+51\n",
      "Gradient Descent(27/49): loss=1.6908322591434106e+53\n",
      "Gradient Descent(28/49): loss=1.8134066130476403e+55\n",
      "Gradient Descent(29/49): loss=1.9448668112772246e+57\n",
      "Gradient Descent(30/49): loss=2.0858570198168133e+59\n",
      "Gradient Descent(31/49): loss=2.237068102499961e+61\n",
      "Gradient Descent(32/49): loss=2.3992410062997758e+63\n",
      "Gradient Descent(33/49): loss=2.5731703920312246e+65\n",
      "Gradient Descent(34/49): loss=2.759708528255624e+67\n",
      "Gradient Descent(35/49): loss=2.9597694674680657e+69\n",
      "Gradient Descent(36/49): loss=3.1743335250311683e+71\n",
      "Gradient Descent(37/49): loss=3.40445208280244e+73\n",
      "Gradient Descent(38/49): loss=3.651252740993594e+75\n",
      "Gradient Descent(39/49): loss=3.915944843505928e+77\n",
      "Gradient Descent(40/49): loss=4.199825403816797e+79\n",
      "Gradient Descent(41/49): loss=4.504285460454381e+81\n",
      "Gradient Descent(42/49): loss=4.8308168932028405e+83\n",
      "Gradient Descent(43/49): loss=5.181019733438449e+85\n",
      "Gradient Descent(44/49): loss=5.556610004417177e+87\n",
      "Gradient Descent(45/49): loss=5.959428129932662e+89\n",
      "Gradient Descent(46/49): loss=6.391447952546712e+91\n",
      "Gradient Descent(47/49): loss=6.854786405583355e+93\n",
      "Gradient Descent(48/49): loss=7.351713886279488e+95\n",
      "Gradient Descent(49/49): loss=7.884665380921544e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4238696711731027\n",
      "Gradient Descent(2/49): loss=299.8038229530545\n",
      "Gradient Descent(3/49): loss=31531.93486667465\n",
      "Gradient Descent(4/49): loss=3368881.0312382597\n",
      "Gradient Descent(5/49): loss=361394266.3444013\n",
      "Gradient Descent(6/49): loss=38815431052.43266\n",
      "Gradient Descent(7/49): loss=4170536027484.6777\n",
      "Gradient Descent(8/49): loss=448158222303015.25\n",
      "Gradient Descent(9/49): loss=4.81601082108178e+16\n",
      "Gradient Descent(10/49): loss=5.175458709819648e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=5.561756137777169e+20\n",
      "Gradient Descent(12/49): loss=5.9768943960872725e+22\n",
      "Gradient Descent(13/49): loss=6.423021813795996e+24\n",
      "Gradient Descent(14/49): loss=6.902449969635009e+26\n",
      "Gradient Descent(15/49): loss=7.417663980488873e+28\n",
      "Gradient Descent(16/49): loss=7.971334801303444e+30\n",
      "Gradient Descent(17/49): loss=8.56633287135131e+32\n",
      "Gradient Descent(18/49): loss=9.205742926624988e+34\n",
      "Gradient Descent(19/49): loss=9.892879968694467e+36\n",
      "Gradient Descent(20/49): loss=1.0631306443031231e+39\n",
      "Gradient Descent(21/49): loss=1.1424850705596275e+41\n",
      "Gradient Descent(22/49): loss=1.2277626869894748e+43\n",
      "Gradient Descent(23/49): loss=1.3194056136175208e+45\n",
      "Gradient Descent(24/49): loss=1.417888971293458e+47\n",
      "Gradient Descent(25/49): loss=1.5237233449420285e+49\n",
      "Gradient Descent(26/49): loss=1.6374574306785715e+51\n",
      "Gradient Descent(27/49): loss=1.759680880512216e+53\n",
      "Gradient Descent(28/49): loss=1.891027359384289e+55\n",
      "Gradient Descent(29/49): loss=2.0321778303910633e+57\n",
      "Gradient Descent(30/49): loss=2.1838640852227413e+59\n",
      "Gradient Descent(31/49): loss=2.3468725381223e+61\n",
      "Gradient Descent(32/49): loss=2.5220483030338592e+63\n",
      "Gradient Descent(33/49): loss=2.7102995750783684e+65\n",
      "Gradient Descent(34/49): loss=2.912602339072401e+67\n",
      "Gradient Descent(35/49): loss=3.1300054295011696e+69\n",
      "Gradient Descent(36/49): loss=3.3636359681792017e+71\n",
      "Gradient Descent(37/49): loss=3.6147052077899936e+73\n",
      "Gradient Descent(38/49): loss=3.884514811600439e+75\n",
      "Gradient Descent(39/49): loss=4.174463601907077e+77\n",
      "Gradient Descent(40/49): loss=4.486054812201206e+79\n",
      "Gradient Descent(41/49): loss=4.8209038806518035e+81\n",
      "Gradient Descent(42/49): loss=5.18074682531123e+83\n",
      "Gradient Descent(43/49): loss=5.567449244464844e+85\n",
      "Gradient Descent(44/49): loss=5.983015988786532e+87\n",
      "Gradient Descent(45/49): loss=6.429601555445531e+89\n",
      "Gradient Descent(46/49): loss=6.909521258052322e+91\n",
      "Gradient Descent(47/49): loss=7.425263230354052e+93\n",
      "Gradient Descent(48/49): loss=7.979501325912327e+95\n",
      "Gradient Descent(49/49): loss=8.575108980641611e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.435276549886421\n",
      "Gradient Descent(2/49): loss=298.65564838062227\n",
      "Gradient Descent(3/49): loss=31133.23725672902\n",
      "Gradient Descent(4/49): loss=3295124.2547077686\n",
      "Gradient Descent(5/49): loss=350101141.8783802\n",
      "Gradient Descent(6/49): loss=37240398056.31242\n",
      "Gradient Descent(7/49): loss=3962695308623.6904\n",
      "Gradient Descent(8/49): loss=421712556117496.1\n",
      "Gradient Descent(9/49): loss=4.488055976791137e+16\n",
      "Gradient Descent(10/49): loss=4.776448673557323e+18\n",
      "Gradient Descent(11/49): loss=5.083392189066706e+20\n",
      "Gradient Descent(12/49): loss=5.410067113970122e+22\n",
      "Gradient Descent(13/49): loss=5.757737492118831e+24\n",
      "Gradient Descent(14/49): loss=6.1277512055775366e+26\n",
      "Gradient Descent(15/49): loss=6.521543652366625e+28\n",
      "Gradient Descent(16/49): loss=6.940642782733382e+30\n",
      "Gradient Descent(17/49): loss=7.38667484477326e+32\n",
      "Gradient Descent(18/49): loss=7.861370631906309e+34\n",
      "Gradient Descent(19/49): loss=8.366572177448483e+36\n",
      "Gradient Descent(20/49): loss=8.904239895086542e+38\n",
      "Gradient Descent(21/49): loss=9.476460183622512e+40\n",
      "Gradient Descent(22/49): loss=1.0085453522282957e+43\n",
      "Gradient Descent(23/49): loss=1.0733583086901968e+45\n",
      "Gradient Descent(24/49): loss=1.1423363920032594e+47\n",
      "Gradient Descent(25/49): loss=1.2157472690439012e+49\n",
      "Gradient Descent(26/49): loss=1.2938758079798111e+51\n",
      "Gradient Descent(27/49): loss=1.3770251836897544e+53\n",
      "Gradient Descent(28/49): loss=1.4655180542222647e+55\n",
      "Gradient Descent(29/49): loss=1.559697812858095e+57\n",
      "Gradient Descent(30/49): loss=1.6599299206350015e+59\n",
      "Gradient Descent(31/49): loss=1.7666033245056684e+61\n",
      "Gradient Descent(32/49): loss=1.8801319666318118e+63\n",
      "Gradient Descent(33/49): loss=2.0009563906713055e+65\n",
      "Gradient Descent(34/49): loss=2.129545451291421e+67\n",
      "Gradient Descent(35/49): loss=2.2663981335417844e+69\n",
      "Gradient Descent(36/49): loss=2.412045489147333e+71\n",
      "Gradient Descent(37/49): loss=2.567052697234648e+73\n",
      "Gradient Descent(38/49): loss=2.7320212574884613e+75\n",
      "Gradient Descent(39/49): loss=2.9075913242487675e+77\n",
      "Gradient Descent(40/49): loss=3.0944441906057734e+79\n",
      "Gradient Descent(41/49): loss=3.2933049321323995e+81\n",
      "Gradient Descent(42/49): loss=3.5049452205128986e+83\n",
      "Gradient Descent(43/49): loss=3.730186317986027e+85\n",
      "Gradient Descent(44/49): loss=3.9699022642225737e+87\n",
      "Gradient Descent(45/49): loss=4.2250232680035304e+89\n",
      "Gradient Descent(46/49): loss=4.496539316860752e+91\n",
      "Gradient Descent(47/49): loss=4.785504018686408e+93\n",
      "Gradient Descent(48/49): loss=5.093038690219236e+95\n",
      "Gradient Descent(49/49): loss=5.420336708272229e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4381114769424364\n",
      "Gradient Descent(2/49): loss=300.04291795369795\n",
      "Gradient Descent(3/49): loss=31359.139934025527\n",
      "Gradient Descent(4/49): loss=3327451.7084148973\n",
      "Gradient Descent(5/49): loss=354469688.33921725\n",
      "Gradient Descent(6/49): loss=37807191617.91249\n",
      "Gradient Descent(7/49): loss=4034027327890.616\n",
      "Gradient Descent(8/49): loss=430485398362149.9\n",
      "Gradient Descent(9/49): loss=4.594054697866152e+16\n",
      "Gradient Descent(10/49): loss=4.902751578021513e+18\n",
      "Gradient Descent(11/49): loss=5.232215158136717e+20\n",
      "Gradient Descent(12/49): loss=5.583827022672792e+22\n",
      "Gradient Descent(13/49): loss=5.959070648722932e+24\n",
      "Gradient Descent(14/49): loss=6.359532398732635e+26\n",
      "Gradient Descent(15/49): loss=6.786906369188822e+28\n",
      "Gradient Descent(16/49): loss=7.243000901473539e+30\n",
      "Gradient Descent(17/49): loss=7.72974600191949e+32\n",
      "Gradient Descent(18/49): loss=8.249201427041161e+34\n",
      "Gradient Descent(19/49): loss=8.803565370917576e+36\n",
      "Gradient Descent(20/49): loss=9.395183757153532e+38\n",
      "Gradient Descent(21/49): loss=1.0026560162614668e+41\n",
      "Gradient Descent(22/49): loss=1.0700366410648729e+43\n",
      "Gradient Descent(23/49): loss=1.1419453877109255e+45\n",
      "Gradient Descent(24/49): loss=1.2186865556490656e+47\n",
      "Gradient Descent(25/49): loss=1.300584893904778e+49\n",
      "Gradient Descent(26/49): loss=1.3879869753325513e+51\n",
      "Gradient Descent(27/49): loss=1.481262663222984e+53\n",
      "Gradient Descent(28/49): loss=1.5808066764695833e+55\n",
      "Gradient Descent(29/49): loss=1.6870402599182134e+57\n",
      "Gradient Descent(30/49): loss=1.8004129669676712e+59\n",
      "Gradient Descent(31/49): loss=1.9214045619649138e+61\n",
      "Gradient Descent(32/49): loss=2.0505270504452563e+63\n",
      "Gradient Descent(33/49): loss=2.1883268458090062e+65\n",
      "Gradient Descent(34/49): loss=2.3353870816035024e+67\n",
      "Gradient Descent(35/49): loss=2.492330079195382e+69\n",
      "Gradient Descent(36/49): loss=2.659819981275663e+71\n",
      "Gradient Descent(37/49): loss=2.8385655623420484e+73\n",
      "Gradient Descent(38/49): loss=3.0293232280516597e+75\n",
      "Gradient Descent(39/49): loss=3.2329002161365086e+77\n",
      "Gradient Descent(40/49): loss=3.4501580124276016e+79\n",
      "Gradient Descent(41/49): loss=3.68201599644292e+81\n",
      "Gradient Descent(42/49): loss=3.9294553319667736e+83\n",
      "Gradient Descent(43/49): loss=4.193523119084431e+85\n",
      "Gradient Descent(44/49): loss=4.4753368252423054e+87\n",
      "Gradient Descent(45/49): loss=4.7760890140848174e+89\n",
      "Gradient Descent(46/49): loss=5.09705239207928e+91\n",
      "Gradient Descent(47/49): loss=5.4395851942845935e+93\n",
      "Gradient Descent(48/49): loss=5.805136932055289e+95\n",
      "Gradient Descent(49/49): loss=6.1952545270033925e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4801101258814295\n",
      "Gradient Descent(2/49): loss=310.42345915382646\n",
      "Gradient Descent(3/49): loss=33107.412228770685\n",
      "Gradient Descent(4/49): loss=3582235.3580545033\n",
      "Gradient Descent(5/49): loss=388989986.2139851\n",
      "Gradient Descent(6/49): loss=42284111975.10732\n",
      "Gradient Descent(7/49): loss=4597848430198.5\n",
      "Gradient Descent(8/49): loss=500005949820108.94\n",
      "Gradient Descent(9/49): loss=5.437624224433439e+16\n",
      "Gradient Descent(10/49): loss=5.913538847141019e+18\n",
      "Gradient Descent(11/49): loss=6.431126500601224e+20\n",
      "Gradient Descent(12/49): loss=6.994023241328338e+22\n",
      "Gradient Descent(13/49): loss=7.606190925791309e+24\n",
      "Gradient Descent(14/49): loss=8.271940765411855e+26\n",
      "Gradient Descent(15/49): loss=8.995962206046345e+28\n",
      "Gradient Descent(16/49): loss=9.783355451958342e+30\n",
      "Gradient Descent(17/49): loss=1.0639667219046017e+33\n",
      "Gradient Descent(18/49): loss=1.1570929748520438e+35\n",
      "Gradient Descent(19/49): loss=1.2583703280262762e+37\n",
      "Gradient Descent(20/49): loss=1.3685122259156346e+39\n",
      "Gradient Descent(21/49): loss=1.4882945591014046e+41\n",
      "Gradient Descent(22/49): loss=1.6185611299084667e+43\n",
      "Gradient Descent(23/49): loss=1.7602295965121395e+45\n",
      "Gradient Descent(24/49): loss=1.9142979373385847e+47\n",
      "Gradient Descent(25/49): loss=2.0818514812846188e+49\n",
      "Gradient Descent(26/49): loss=2.2640705532770946e+51\n",
      "Gradient Descent(27/49): loss=2.462238789029073e+53\n",
      "Gradient Descent(28/49): loss=2.677752177565368e+55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=2.9121288952171652e+57\n",
      "Gradient Descent(30/49): loss=3.167020000360658e+59\n",
      "Gradient Descent(31/49): loss=3.4442210642384215e+61\n",
      "Gradient Descent(32/49): loss=3.745684819796792e+63\n",
      "Gradient Descent(33/49): loss=4.0735349176428255e+65\n",
      "Gradient Descent(34/49): loss=4.430080886024871e+67\n",
      "Gradient Descent(35/49): loss=4.817834400221439e+69\n",
      "Gradient Descent(36/49): loss=5.239526975947572e+71\n",
      "Gradient Descent(37/49): loss=5.698129211419321e+73\n",
      "Gradient Descent(38/49): loss=6.19687171362602e+75\n",
      "Gradient Descent(39/49): loss=6.739267856225597e+77\n",
      "Gradient Descent(40/49): loss=7.329138529379005e+79\n",
      "Gradient Descent(41/49): loss=7.970639055874045e+81\n",
      "Gradient Descent(42/49): loss=8.668288463147328e+83\n",
      "Gradient Descent(43/49): loss=9.427001317411069e+85\n",
      "Gradient Descent(44/49): loss=1.0252122344138406e+88\n",
      "Gradient Descent(45/49): loss=1.1149464078790134e+90\n",
      "Gradient Descent(46/49): loss=1.2125347813012123e+92\n",
      "Gradient Descent(47/49): loss=1.3186648124747517e+94\n",
      "Gradient Descent(48/49): loss=1.4340841305954366e+96\n",
      "Gradient Descent(49/49): loss=1.559605802907588e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.467228840366093\n",
      "Gradient Descent(2/49): loss=308.2762694421318\n",
      "Gradient Descent(3/49): loss=32877.33426000393\n",
      "Gradient Descent(4/49): loss=3561804.558492945\n",
      "Gradient Descent(5/49): loss=387443160.3117885\n",
      "Gradient Descent(6/49): loss=42196372739.72399\n",
      "Gradient Descent(7/49): loss=4597346055980.627\n",
      "Gradient Descent(8/49): loss=500946733337758.56\n",
      "Gradient Descent(9/49): loss=5.458742279490141e+16\n",
      "Gradient Descent(10/49): loss=5.948383720878982e+18\n",
      "Gradient Descent(11/49): loss=6.481970879318756e+20\n",
      "Gradient Descent(12/49): loss=7.063431283847523e+22\n",
      "Gradient Descent(13/49): loss=7.697054290303544e+24\n",
      "Gradient Descent(14/49): loss=8.387517353757094e+26\n",
      "Gradient Descent(15/49): loss=9.13991868177425e+28\n",
      "Gradient Descent(16/49): loss=9.959814222454196e+30\n",
      "Gradient Descent(17/49): loss=1.0853258461812283e+33\n",
      "Gradient Descent(18/49): loss=1.1826849052690081e+35\n",
      "Gradient Descent(19/49): loss=1.2887775507176443e+37\n",
      "Gradient Descent(20/49): loss=1.4043872277989124e+39\n",
      "Gradient Descent(21/49): loss=1.5303676608865737e+41\n",
      "Gradient Descent(22/49): loss=1.6676491576987918e+43\n",
      "Gradient Descent(23/49): loss=1.8172454791520234e+45\n",
      "Gradient Descent(24/49): loss=1.9802613255059373e+47\n",
      "Gradient Descent(25/49): loss=2.1579004940632833e+49\n",
      "Gradient Descent(26/49): loss=2.3514747686594367e+51\n",
      "Gradient Descent(27/49): loss=2.562413606584069e+53\n",
      "Gradient Descent(28/49): loss=2.792274694467803e+55\n",
      "Gradient Descent(29/49): loss=3.0427554510839337e+57\n",
      "Gradient Descent(30/49): loss=3.3157055620079626e+59\n",
      "Gradient Descent(31/49): loss=3.613140638697787e+61\n",
      "Gradient Descent(32/49): loss=3.937257102860334e+63\n",
      "Gradient Descent(33/49): loss=4.29044840601922e+65\n",
      "Gradient Descent(34/49): loss=4.675322704057073e+67\n",
      "Gradient Descent(35/49): loss=5.094722117251262e+69\n",
      "Gradient Descent(36/49): loss=5.551743718029405e+71\n",
      "Gradient Descent(37/49): loss=6.049762401429696e+73\n",
      "Gradient Descent(38/49): loss=6.592455807153749e+75\n",
      "Gradient Descent(39/49): loss=7.183831477250149e+77\n",
      "Gradient Descent(40/49): loss=7.828256449975626e+79\n",
      "Gradient Descent(41/49): loss=8.530489508370572e+81\n",
      "Gradient Descent(42/49): loss=9.295716321690954e+83\n",
      "Gradient Descent(43/49): loss=1.0129587739198414e+86\n",
      "Gradient Descent(44/49): loss=1.1038261519092232e+88\n",
      "Gradient Descent(45/49): loss=1.2028447800730884e+90\n",
      "Gradient Descent(46/49): loss=1.310745865593577e+92\n",
      "Gradient Descent(47/49): loss=1.4283262085289697e+94\n",
      "Gradient Descent(48/49): loss=1.5564540858169028e+96\n",
      "Gradient Descent(49/49): loss=1.6960756631015733e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.478852013941302\n",
      "Gradient Descent(2/49): loss=307.0988172072211\n",
      "Gradient Descent(3/49): loss=32462.1694570724\n",
      "Gradient Descent(4/49): loss=3483905.0514754797\n",
      "Gradient Descent(5/49): loss=375347109.5003705\n",
      "Gradient Descent(6/49): loss=40485588768.457596\n",
      "Gradient Descent(7/49): loss=4368416458849.456\n",
      "Gradient Descent(8/49): loss=471408416360383.5\n",
      "Gradient Descent(9/49): loss=5.087291243699052e+16\n",
      "Gradient Descent(10/49): loss=5.490109693449735e+18\n",
      "Gradient Descent(11/49): loss=5.924846543033747e+20\n",
      "Gradient Descent(12/49): loss=6.39401614980548e+22\n",
      "Gradient Descent(13/49): loss=6.900340558295915e+24\n",
      "Gradient Descent(14/49): loss=7.446760357598077e+26\n",
      "Gradient Descent(15/49): loss=8.036450038843347e+28\n",
      "Gradient Descent(16/49): loss=8.672835837471068e+30\n",
      "Gradient Descent(17/49): loss=9.359615430946975e+32\n",
      "Gradient Descent(18/49): loss=1.0100779350740901e+35\n",
      "Gradient Descent(19/49): loss=1.0900634143773637e+37\n",
      "Gradient Descent(20/49): loss=1.1763827386777423e+39\n",
      "Gradient Descent(21/49): loss=1.2695374688048371e+41\n",
      "Gradient Descent(22/49): loss=1.3700688829691651e+43\n",
      "Gradient Descent(23/49): loss=1.4785611218367055e+45\n",
      "Gradient Descent(24/49): loss=1.5956445826817348e+47\n",
      "Gradient Descent(25/49): loss=1.7219995823237226e+49\n",
      "Gradient Descent(26/49): loss=1.8583603101261804e+51\n",
      "Gradient Descent(27/49): loss=2.005519094024508e+53\n",
      "Gradient Descent(28/49): loss=2.1643310043701144e+55\n",
      "Gradient Descent(29/49): loss=2.335718822341233e+57\n",
      "Gradient Descent(30/49): loss=2.5206784017894916e+59\n",
      "Gradient Descent(31/49): loss=2.720284455677438e+61\n",
      "Gradient Descent(32/49): loss=2.9356968007290703e+63\n",
      "Gradient Descent(33/49): loss=3.1681670965783937e+65\n",
      "Gradient Descent(34/49): loss=3.419046118573705e+67\n",
      "Gradient Descent(35/49): loss=3.689791606496685e+69\n",
      "Gradient Descent(36/49): loss=3.9819767348013533e+71\n",
      "Gradient Descent(37/49): loss=4.2972992535895494e+73\n",
      "Gradient Descent(38/49): loss=4.637591353436827e+75\n",
      "Gradient Descent(39/49): loss=5.004830311388473e+77\n",
      "Gradient Descent(40/49): loss=5.4011499799847494e+79\n",
      "Gradient Descent(41/49): loss=5.828853186072599e+81\n",
      "Gradient Descent(42/49): loss=6.290425111447149e+83\n",
      "Gradient Descent(43/49): loss=6.788547733072447e+85\n",
      "Gradient Descent(44/49): loss=7.326115406785425e+87\n",
      "Gradient Descent(45/49): loss=7.90625168503414e+89\n",
      "Gradient Descent(46/49): loss=8.53232746636914e+91\n",
      "Gradient Descent(47/49): loss=9.207980582146444e+93\n",
      "Gradient Descent(48/49): loss=9.93713693425161e+95\n",
      "Gradient Descent(49/49): loss=1.072403330666546e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.481397258736017\n",
      "Gradient Descent(2/49): loss=308.46975740479644\n",
      "Gradient Descent(3/49): loss=32688.945053818323\n",
      "Gradient Descent(4/49): loss=3516826.65246174\n",
      "Gradient Descent(5/49): loss=379860734.3970355\n",
      "Gradient Descent(6/49): loss=41079711754.84919\n",
      "Gradient Descent(7/49): loss=4444266661751.033\n",
      "Gradient Descent(8/49): loss=480870633441271.9\n",
      "Gradient Descent(9/49): loss=5.203249465602992e+16\n",
      "Gradient Descent(10/49): loss=5.630242085527272e+18\n",
      "Gradient Descent(11/49): loss=6.092302843461092e+20\n",
      "Gradient Descent(12/49): loss=6.592293866314651e+22\n",
      "Gradient Descent(13/49): loss=7.13332239333915e+24\n",
      "Gradient Descent(14/49): loss=7.718754334106227e+26\n",
      "Gradient Descent(15/49): loss=8.352233149657485e+28\n",
      "Gradient Descent(16/49): loss=9.037701790058009e+30\n",
      "Gradient Descent(17/49): loss=9.779426972087946e+32\n",
      "Gradient Descent(18/49): loss=1.0582025642796061e+35\n",
      "Gradient Descent(19/49): loss=1.1450493684224743e+37\n",
      "Gradient Descent(20/49): loss=1.239023699877327e+39\n",
      "Gradient Descent(21/49): loss=1.3407105154642036e+41\n",
      "Gradient Descent(22/49): loss=1.4507427795752975e+43\n",
      "Gradient Descent(23/49): loss=1.5698054040979864e+45\n",
      "Gradient Descent(24/49): loss=1.6986395117287376e+47\n",
      "Gradient Descent(25/49): loss=1.8380470491923938e+49\n",
      "Gradient Descent(26/49): loss=1.988895779074148e+51\n",
      "Gradient Descent(27/49): loss=2.1521246813335953e+53\n",
      "Gradient Descent(28/49): loss=2.3287497981223818e+55\n",
      "Gradient Descent(29/49): loss=2.5198705582868784e+57\n",
      "Gradient Descent(30/49): loss=2.7266766209236967e+59\n",
      "Gradient Descent(31/49): loss=2.95045528058645e+61\n",
      "Gradient Descent(32/49): loss=3.192599480238885e+63\n",
      "Gradient Descent(33/49): loss=3.454616481831787e+65\n",
      "Gradient Descent(34/49): loss=3.738137248475316e+67\n",
      "Gradient Descent(35/49): loss=4.044926596607063e+69\n",
      "Gradient Descent(36/49): loss=4.3768941813499615e+71\n",
      "Gradient Descent(37/49): loss=4.7361063834395566e+73\n",
      "Gradient Descent(38/49): loss=5.124799171712826e+75\n",
      "Gradient Descent(39/49): loss=5.545392021222894e+77\n",
      "Gradient Descent(40/49): loss=6.000502973615003e+79\n",
      "Gradient Descent(41/49): loss=6.492964933509289e+81\n",
      "Gradient Descent(42/49): loss=7.025843302329487e+83\n",
      "Gradient Descent(43/49): loss=7.60245505934207e+85\n",
      "Gradient Descent(44/49): loss=8.226389408678203e+87\n",
      "Gradient Descent(45/49): loss=8.901530120859303e+89\n",
      "Gradient Descent(46/49): loss=9.632079707894293e+91\n",
      "Gradient Descent(47/49): loss=1.0422585582429276e+94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=1.1277968363782504e+96\n",
      "Gradient Descent(49/49): loss=1.2203552507057788e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5240018895995573\n",
      "Gradient Descent(2/49): loss=319.1341719114738\n",
      "Gradient Descent(3/49): loss=34510.092371335915\n",
      "Gradient Descent(4/49): loss=3785932.551242479\n",
      "Gradient Descent(5/49): loss=416829650.9387665\n",
      "Gradient Descent(6/49): loss=45940981187.78633\n",
      "Gradient Descent(7/49): loss=5065019545748.431\n",
      "Gradient Descent(8/49): loss=558476984625218.56\n",
      "Gradient Descent(9/49): loss=6.158047156460479e+16\n",
      "Gradient Descent(10/49): loss=6.7902382372013e+18\n",
      "Gradient Descent(11/49): loss=7.487353870174527e+20\n",
      "Gradient Descent(12/49): loss=8.256046518012238e+22\n",
      "Gradient Descent(13/49): loss=9.103660157354485e+24\n",
      "Gradient Descent(14/49): loss=1.003829570663529e+27\n",
      "Gradient Descent(15/49): loss=1.1068886803904655e+29\n",
      "Gradient Descent(16/49): loss=1.2205284626292441e+31\n",
      "Gradient Descent(17/49): loss=1.34583518561795e+33\n",
      "Gradient Descent(18/49): loss=1.4840066447021104e+35\n",
      "Gradient Descent(19/49): loss=1.6363636095434054e+37\n",
      "Gradient Descent(20/49): loss=1.804362448411897e+39\n",
      "Gradient Descent(21/49): loss=1.9896090492049785e+41\n",
      "Gradient Descent(22/49): loss=2.1938741699042356e+43\n",
      "Gradient Descent(23/49): loss=2.419110365079368e+45\n",
      "Gradient Descent(24/49): loss=2.667470650194775e+47\n",
      "Gradient Descent(25/49): loss=2.941329082114542e+49\n",
      "Gradient Descent(26/49): loss=3.243303452527922e+51\n",
      "Gradient Descent(27/49): loss=3.5762803112182337e+53\n",
      "Gradient Descent(28/49): loss=3.943442558370108e+55\n",
      "Gradient Descent(29/49): loss=4.348299869667483e+57\n",
      "Gradient Descent(30/49): loss=4.794722245013536e+59\n",
      "Gradient Descent(31/49): loss=5.286977001562157e+61\n",
      "Gradient Descent(32/49): loss=5.82976956467438e+63\n",
      "Gradient Descent(33/49): loss=6.428288446717604e+65\n",
      "Gradient Descent(34/49): loss=7.088254843656259e+67\n",
      "Gradient Descent(35/49): loss=7.815977323524034e+69\n",
      "Gradient Descent(36/49): loss=8.618412129540617e+71\n",
      "Gradient Descent(37/49): loss=9.503229674305568e+73\n",
      "Gradient Descent(38/49): loss=1.0478887860682506e+76\n",
      "Gradient Descent(39/49): loss=1.1554712930242048e+78\n",
      "Gradient Descent(40/49): loss=1.274098861208802e+80\n",
      "Gradient Descent(41/49): loss=1.4049054424232797e+82\n",
      "Gradient Descent(42/49): loss=1.5491414067177896e+84\n",
      "Gradient Descent(43/49): loss=1.7081854945825894e+86\n",
      "Gradient Descent(44/49): loss=1.8835579962223032e+88\n",
      "Gradient Descent(45/49): loss=2.0769352838931035e+90\n",
      "Gradient Descent(46/49): loss=2.290165836216181e+92\n",
      "Gradient Descent(47/49): loss=2.525287907642722e+94\n",
      "Gradient Descent(48/49): loss=2.784549011971486e+96\n",
      "Gradient Descent(49/49): loss=3.070427406160271e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5109011208944163\n",
      "Gradient Descent(2/49): loss=316.92731185570346\n",
      "Gradient Descent(3/49): loss=34270.23796191317\n",
      "Gradient Descent(4/49): loss=3764326.8981144526\n",
      "Gradient Descent(5/49): loss=415169864.6396044\n",
      "Gradient Descent(6/49): loss=45845333921.06503\n",
      "Gradient Descent(7/49): loss=5064423919853.136\n",
      "Gradient Descent(8/49): loss=559522412501259.0\n",
      "Gradient Descent(9/49): loss=6.181896164228468e+16\n",
      "Gradient Descent(10/49): loss=6.830166701100478e+18\n",
      "Gradient Descent(11/49): loss=7.546448752453542e+20\n",
      "Gradient Descent(12/49): loss=8.337858203635608e+22\n",
      "Gradient Descent(13/49): loss=9.212267942719508e+24\n",
      "Gradient Descent(14/49): loss=1.0178380316763298e+27\n",
      "Gradient Descent(15/49): loss=1.1245811661054032e+29\n",
      "Gradient Descent(16/49): loss=1.2425187279170748e+31\n",
      "Gradient Descent(17/49): loss=1.3728246946464402e+33\n",
      "Gradient Descent(18/49): loss=1.516796167759351e+35\n",
      "Gradient Descent(19/49): loss=1.6758662810609822e+37\n",
      "Gradient Descent(20/49): loss=1.8516184653554446e+39\n",
      "Gradient Descent(21/49): loss=2.045802209967515e+41\n",
      "Gradient Descent(22/49): loss=2.260350477530516e+43\n",
      "Gradient Descent(23/49): loss=2.497398945217709e+45\n",
      "Gradient Descent(24/49): loss=2.7593072638886527e+47\n",
      "Gradient Descent(25/49): loss=3.048682546748141e+49\n",
      "Gradient Descent(26/49): loss=3.368405321323161e+51\n",
      "Gradient Descent(27/49): loss=3.7216582030887745e+53\n",
      "Gradient Descent(28/49): loss=4.1119575761676383e+55\n",
      "Gradient Descent(29/49): loss=4.54318859646212e+57\n",
      "Gradient Descent(30/49): loss=5.019643865650118e+59\n",
      "Gradient Descent(31/49): loss=5.546066161017422e+61\n",
      "Gradient Descent(32/49): loss=6.127695646471737e+63\n",
      "Gradient Descent(33/49): loss=6.770322034690689e+65\n",
      "Gradient Descent(34/49): loss=7.480342219641842e+67\n",
      "Gradient Descent(35/49): loss=8.264823953165567e+69\n",
      "Gradient Descent(36/49): loss=9.131576199476319e+71\n",
      "Gradient Descent(37/49): loss=1.008922686791222e+74\n",
      "Gradient Descent(38/49): loss=1.1147308697707547e+76\n",
      "Gradient Descent(39/49): loss=1.231635414971099e+78\n",
      "Gradient Descent(40/49): loss=1.3608000249629598e+80\n",
      "Gradient Descent(41/49): loss=1.5035104426439844e+82\n",
      "Gradient Descent(42/49): loss=1.6611872499054687e+84\n",
      "Gradient Descent(43/49): loss=1.8354000085265273e+86\n",
      "Gradient Descent(44/49): loss=2.0278828840583025e+88\n",
      "Gradient Descent(45/49): loss=2.2405519082229993e+90\n",
      "Gradient Descent(46/49): loss=2.475524051662825e+92\n",
      "Gradient Descent(47/49): loss=2.7351382968946657e+94\n",
      "Gradient Descent(48/49): loss=3.0219789212368196e+96\n",
      "Gradient Descent(49/49): loss=3.338901221473177e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.522742407554092\n",
      "Gradient Descent(2/49): loss=315.72003131155736\n",
      "Gradient Descent(3/49): loss=33838.04357660583\n",
      "Gradient Descent(4/49): loss=3682082.5422884575\n",
      "Gradient Descent(5/49): loss=402219878.9158589\n",
      "Gradient Descent(6/49): loss=43988150816.896454\n",
      "Gradient Descent(7/49): loss=4812432792546.708\n",
      "Gradient Descent(8/49): loss=526554836723968.1\n",
      "Gradient Descent(9/49): loss=5.7615402058343384e+16\n",
      "Gradient Descent(10/49): loss=6.304327586109484e+18\n",
      "Gradient Descent(11/49): loss=6.898276903694548e+20\n",
      "Gradient Descent(12/49): loss=7.548193368745786e+22\n",
      "Gradient Descent(13/49): loss=8.259344610828436e+24\n",
      "Gradient Descent(14/49): loss=9.03749798093217e+26\n",
      "Gradient Descent(15/49): loss=9.888965416166971e+28\n",
      "Gradient Descent(16/49): loss=1.082065396310461e+31\n",
      "Gradient Descent(17/49): loss=1.184012156802319e+33\n",
      "Gradient Descent(18/49): loss=1.295563830293495e+35\n",
      "Gradient Descent(19/49): loss=1.4176253423609412e+37\n",
      "Gradient Descent(20/49): loss=1.551186876799195e+39\n",
      "Gradient Descent(21/49): loss=1.6973319077833456e+41\n",
      "Gradient Descent(22/49): loss=1.8572459890646253e+43\n",
      "Gradient Descent(23/49): loss=2.0322263713400777e+45\n",
      "Gradient Descent(24/49): loss=2.2236925257579364e+47\n",
      "Gradient Descent(25/49): loss=2.4331976589072944e+49\n",
      "Gradient Descent(26/49): loss=2.6624413126967915e+51\n",
      "Gradient Descent(27/49): loss=2.9132831513319687e+53\n",
      "Gradient Descent(28/49): loss=3.1877580472330235e+55\n",
      "Gradient Descent(29/49): loss=3.488092588272087e+57\n",
      "Gradient Descent(30/49): loss=3.8167231402394025e+59\n",
      "Gradient Descent(31/49): loss=4.1763156110644584e+61\n",
      "Gradient Descent(32/49): loss=4.5697870771225614e+63\n",
      "Gradient Descent(33/49): loss=5.000329447063473e+65\n",
      "Gradient Descent(34/49): loss=5.471435355126808e+67\n",
      "Gradient Descent(35/49): loss=5.986926493995787e+69\n",
      "Gradient Descent(36/49): loss=6.55098461702981e+71\n",
      "Gradient Descent(37/49): loss=7.168185461371628e+73\n",
      "Gradient Descent(38/49): loss=7.843535867119223e+75\n",
      "Gradient Descent(39/49): loss=8.582514393679455e+77\n",
      "Gradient Descent(40/49): loss=9.391115762790347e+79\n",
      "Gradient Descent(41/49): loss=1.027589948874181e+82\n",
      "Gradient Descent(42/49): loss=1.1244043090290782e+84\n",
      "Gradient Descent(43/49): loss=1.2303400315936373e+86\n",
      "Gradient Descent(44/49): loss=1.346256485488686e+88\n",
      "Gradient Descent(45/49): loss=1.4730940050554746e+90\n",
      "Gradient Descent(46/49): loss=1.6118815182106067e+92\n",
      "Gradient Descent(47/49): loss=1.7637448932874336e+94\n",
      "Gradient Descent(48/49): loss=1.9299160722749047e+96\n",
      "Gradient Descent(49/49): loss=2.1117430645441912e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.524993608322484\n",
      "Gradient Descent(2/49): loss=317.073140637247\n",
      "Gradient Descent(3/49): loss=34065.41728062802\n",
      "Gradient Descent(4/49): loss=3715566.1002343954\n",
      "Gradient Descent(5/49): loss=406877129.687985\n",
      "Gradient Descent(6/49): loss=44610044980.88323\n",
      "Gradient Descent(7/49): loss=4892968379526.659\n",
      "Gradient Descent(8/49): loss=536744783707970.5\n",
      "Gradient Descent(9/49): loss=5.888187079934058e+16\n",
      "Gradient Descent(10/49): loss=6.459537636751678e+18\n",
      "Gradient Descent(11/49): loss=7.086361030058799e+20\n",
      "Gradient Descent(12/49): loss=7.774022295579688e+22\n",
      "Gradient Descent(13/49): loss=8.528418619205262e+24\n",
      "Gradient Descent(14/49): loss=9.356023642104097e+26\n",
      "Gradient Descent(15/49): loss=1.0263940735829634e+29\n",
      "Gradient Descent(16/49): loss=1.125996314331942e+31\n",
      "Gradient Descent(17/49): loss=1.2352640572458477e+33\n",
      "Gradient Descent(18/49): loss=1.3551352476069836e+35\n",
      "Gradient Descent(19/49): loss=1.486638852456713e+37\n",
      "Gradient Descent(20/49): loss=1.6309036917389583e+39\n",
      "Gradient Descent(21/49): loss=1.7891681275041227e+41\n",
      "Gradient Descent(22/49): loss=1.962790693754591e+43\n",
      "Gradient Descent(23/49): loss=2.1532617579630524e+45\n",
      "Gradient Descent(24/49): loss=2.3622163142874424e+47\n",
      "Gradient Descent(25/49): loss=2.5914480182703766e+49\n",
      "Gradient Descent(26/49): loss=2.842924583486018e+51\n",
      "Gradient Descent(27/49): loss=3.1188046722944805e+53\n",
      "Gradient Descent(28/49): loss=3.421456425692073e+55\n",
      "Gradient Descent(29/49): loss=3.753477791315943e+57\n",
      "Gradient Descent(30/49): loss=4.117718824097621e+59\n",
      "Gradient Descent(31/49): loss=4.5173061509932327e+61\n",
      "Gradient Descent(32/49): loss=4.955669809794069e+63\n",
      "Gradient Descent(33/49): loss=5.436572692400896e+65\n",
      "Gradient Descent(34/49): loss=5.964142845301282e+67\n",
      "Gradient Descent(35/49): loss=6.542908904516049e+69\n",
      "Gradient Descent(36/49): loss=7.1778389691860145e+71\n",
      "Gradient Descent(37/49): loss=7.874383247488658e+73\n",
      "Gradient Descent(38/49): loss=8.638520840954614e+75\n",
      "Gradient Descent(39/49): loss=9.47681106877883e+77\n",
      "Gradient Descent(40/49): loss=1.039644977269104e+80\n",
      "Gradient Descent(41/49): loss=1.1405331085704009e+82\n",
      "Gradient Descent(42/49): loss=1.2512115194960077e+84\n",
      "Gradient Descent(43/49): loss=1.3726302680347563e+86\n",
      "Gradient Descent(44/49): loss=1.5058316067007591e+88\n",
      "Gradient Descent(45/49): loss=1.6519589291772615e+90\n",
      "Gradient Descent(46/49): loss=1.8122665851513057e+92\n",
      "Gradient Descent(47/49): loss=1.9881306475892023e+94\n",
      "Gradient Descent(48/49): loss=2.181060724878644e+96\n",
      "Gradient Descent(49/49): loss=2.3927129192322106e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.568208499949448\n",
      "Gradient Descent(2/49): loss=328.02720744470565\n",
      "Gradient Descent(3/49): loss=35961.95296149644\n",
      "Gradient Descent(4/49): loss=3999693.719633412\n",
      "Gradient Descent(5/49): loss=446449875.5210765\n",
      "Gradient Descent(6/49): loss=49885695548.362206\n",
      "Gradient Descent(7/49): loss=5575952985837.674\n",
      "Gradient Descent(8/49): loss=623312272426154.2\n",
      "Gradient Descent(9/49): loss=6.9679641134389816e+16\n",
      "Gradient Descent(10/49): loss=7.789515259935317e+18\n",
      "Gradient Descent(11/49): loss=8.707957799040642e+20\n",
      "Gradient Descent(12/49): loss=9.73470122594149e+22\n",
      "Gradient Descent(13/49): loss=1.0882509954878086e+25\n",
      "Gradient Descent(14/49): loss=1.216565683924302e+27\n",
      "Gradient Descent(15/49): loss=1.360009883126464e+29\n",
      "Gradient Descent(16/49): loss=1.520367477684289e+31\n",
      "Gradient Descent(17/49): loss=1.6996327034250446e+33\n",
      "Gradient Descent(18/49): loss=1.9000349400857885e+35\n",
      "Gradient Descent(19/49): loss=2.1240664335204803e+37\n",
      "Gradient Descent(20/49): loss=2.3745132888153406e+39\n",
      "Gradient Descent(21/49): loss=2.654490118554679e+41\n",
      "Gradient Descent(22/49): loss=2.967478776708872e+43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=3.317371659699085e+45\n",
      "Gradient Descent(24/49): loss=3.7085201130875585e+47\n",
      "Gradient Descent(25/49): loss=4.145788545871466e+49\n",
      "Gradient Descent(26/49): loss=4.634614925351203e+51\n",
      "Gradient Descent(27/49): loss=5.181078404898144e+53\n",
      "Gradient Descent(28/49): loss=5.791974925655357e+55\n",
      "Gradient Descent(29/49): loss=6.4749017323701e+57\n",
      "Gradient Descent(30/49): loss=7.238351854416175e+59\n",
      "Gradient Descent(31/49): loss=8.09181972699253e+61\n",
      "Gradient Descent(32/49): loss=9.045919266026937e+63\n",
      "Gradient Descent(33/49): loss=1.0112515865191007e+66\n",
      "Gradient Descent(34/49): loss=1.130487395656969e+68\n",
      "Gradient Descent(35/49): loss=1.2637821970083355e+70\n",
      "Gradient Descent(36/49): loss=1.4127936743134212e+72\n",
      "Gradient Descent(37/49): loss=1.5793749673835898e+74\n",
      "Gradient Descent(38/49): loss=1.7655977181594787e+76\n",
      "Gradient Descent(39/49): loss=1.9737778341100317e+78\n",
      "Gradient Descent(40/49): loss=2.2065042893718582e+80\n",
      "Gradient Descent(41/49): loss=2.4666713218064312e+82\n",
      "Gradient Descent(42/49): loss=2.7575144263845425e+84\n",
      "Gradient Descent(43/49): loss=3.0826505925200753e+86\n",
      "Gradient Descent(44/49): loss=3.446123285753292e+88\n",
      "Gradient Descent(45/49): loss=3.8524527331858938e+90\n",
      "Gradient Descent(46/49): loss=4.306692138028701e+92\n",
      "Gradient Descent(47/49): loss=4.814490522358706e+94\n",
      "Gradient Descent(48/49): loss=5.382162979611441e+96\n",
      "Gradient Descent(49/49): loss=6.016769210485014e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.554886512758071\n",
      "Gradient Descent(2/49): loss=325.7594434550019\n",
      "Gradient Descent(3/49): loss=35711.978526781495\n",
      "Gradient Descent(4/49): loss=3976854.539902865\n",
      "Gradient Descent(5/49): loss=444669761.8676968\n",
      "Gradient Descent(6/49): loss=49781492823.72067\n",
      "Gradient Descent(7/49): loss=5575251450470.391\n",
      "Gradient Descent(8/49): loss=624473154332264.1\n",
      "Gradient Descent(9/49): loss=6.994875068938196e+16\n",
      "Gradient Descent(10/49): loss=7.835226669189394e+18\n",
      "Gradient Descent(11/49): loss=8.776571727846682e+20\n",
      "Gradient Descent(12/49): loss=9.831025147627626e+22\n",
      "Gradient Descent(13/49): loss=1.101216949339978e+25\n",
      "Gradient Descent(14/49): loss=1.233522356624852e+27\n",
      "Gradient Descent(15/49): loss=1.3817236197439027e+29\n",
      "Gradient Descent(16/49): loss=1.547730511211771e+31\n",
      "Gradient Descent(17/49): loss=1.733682273138467e+33\n",
      "Gradient Descent(18/49): loss=1.941975173774951e+35\n",
      "Gradient Descent(19/49): loss=2.1752933831727475e+37\n",
      "Gradient Descent(20/49): loss=2.436643561396484e+39\n",
      "Gradient Descent(21/49): loss=2.7293936034116995e+41\n",
      "Gradient Descent(22/49): loss=3.05731603935885e+43\n",
      "Gradient Descent(23/49): loss=3.4246366492845413e+45\n",
      "Gradient Descent(24/49): loss=3.836088918730427e+47\n",
      "Gradient Descent(25/49): loss=4.296975036895701e+49\n",
      "Gradient Descent(26/49): loss=4.813234223417989e+51\n",
      "Gradient Descent(27/49): loss=5.391519264263716e+53\n",
      "Gradient Descent(28/49): loss=6.039282243008116e+55\n",
      "Gradient Descent(29/49): loss=6.764870572282797e+57\n",
      "Gradient Descent(30/49): loss=7.577634562901837e+59\n",
      "Gradient Descent(31/49): loss=8.488047916858802e+61\n",
      "Gradient Descent(32/49): loss=9.507842696930081e+63\n",
      "Gradient Descent(33/49): loss=1.065016051217357e+66\n",
      "Gradient Descent(34/49): loss=1.1929721867578332e+68\n",
      "Gradient Descent(35/49): loss=1.3363015860192925e+70\n",
      "Gradient Descent(36/49): loss=1.4968512666256873e+72\n",
      "Gradient Descent(37/49): loss=1.676690155755439e+74\n",
      "Gradient Descent(38/49): loss=1.8781357514194405e+76\n",
      "Gradient Descent(39/49): loss=2.1037839869529218e+78\n",
      "Gradient Descent(40/49): loss=2.3565426835704274e+80\n",
      "Gradient Descent(41/49): loss=2.63966902207131e+82\n",
      "Gradient Descent(42/49): loss=2.956811516575568e+84\n",
      "Gradient Descent(43/49): loss=3.312057031185521e+86\n",
      "Gradient Descent(44/49): loss=3.709983445454773e+88\n",
      "Gradient Descent(45/49): loss=4.155718647339187e+90\n",
      "Gradient Descent(46/49): loss=4.655006613843681e+92\n",
      "Gradient Descent(47/49): loss=5.214281430915156e+94\n",
      "Gradient Descent(48/49): loss=5.840750206439897e+96\n",
      "Gradient Descent(49/49): loss=6.542485944806462e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.566947730724789\n",
      "Gradient Descent(2/49): loss=324.521777201494\n",
      "Gradient Descent(3/49): loss=35262.17692330636\n",
      "Gradient Descent(4/49): loss=3890054.8257701583\n",
      "Gradient Descent(5/49): loss=430811972.8212629\n",
      "Gradient Descent(6/49): loss=47766508549.7607\n",
      "Gradient Descent(7/49): loss=5298057032296.49\n",
      "Gradient Descent(8/49): loss=587705763056471.6\n",
      "Gradient Descent(9/49): loss=6.519577016903301e+16\n",
      "Gradient Descent(10/49): loss=7.232427827765204e+18\n",
      "Gradient Descent(11/49): loss=8.023252971656485e+20\n",
      "Gradient Descent(12/49): loss=8.900561588501273e+22\n",
      "Gradient Descent(13/49): loss=9.873804203050577e+24\n",
      "Gradient Descent(14/49): loss=1.095346865028526e+27\n",
      "Gradient Descent(15/49): loss=1.2151190984313516e+29\n",
      "Gradient Descent(16/49): loss=1.3479880127592839e+31\n",
      "Gradient Descent(17/49): loss=1.4953856729465794e+33\n",
      "Gradient Descent(18/49): loss=1.6589007416593243e+35\n",
      "Gradient Descent(19/49): loss=1.840295598496376e+37\n",
      "Gradient Descent(20/49): loss=2.0415253338039797e+39\n",
      "Gradient Descent(21/49): loss=2.2647588203631987e+41\n",
      "Gradient Descent(22/49): loss=2.512402089538738e+43\n",
      "Gradient Descent(23/49): loss=2.7871242636508186e+45\n",
      "Gradient Descent(24/49): loss=3.0918863240041396e+47\n",
      "Gradient Descent(25/49): loss=3.4299730246138665e+49\n",
      "Gradient Descent(26/49): loss=3.805028295589137e+51\n",
      "Gradient Descent(27/49): loss=4.221094517751981e+53\n",
      "Gradient Descent(28/49): loss=4.6826560917959036e+55\n",
      "Gradient Descent(29/49): loss=5.194687771576161e+57\n",
      "Gradient Descent(30/49): loss=5.762708282472589e+59\n",
      "Gradient Descent(31/49): loss=6.392839802728333e+61\n",
      "Gradient Descent(32/49): loss=7.091873948860067e+63\n",
      "Gradient Descent(33/49): loss=7.867344976336721e+65\n",
      "Gradient Descent(34/49): loss=8.727610984490133e+67\n",
      "Gradient Descent(35/49): loss=9.681944000892276e+69\n",
      "Gradient Descent(36/49): loss=1.0740629916136208e+72\n",
      "Gradient Descent(37/49): loss=1.191507934612798e+74\n",
      "Gradient Descent(38/49): loss=1.3217950616773373e+76\n",
      "Gradient Descent(39/49): loss=1.4663286196598843e+78\n",
      "Gradient Descent(40/49): loss=1.6266664047792648e+80\n",
      "Gradient Descent(41/49): loss=1.804536552693939e+82\n",
      "Gradient Descent(42/49): loss=2.0018561645098814e+84\n",
      "Gradient Descent(43/49): loss=2.220751969475851e+86\n",
      "Gradient Descent(44/49): loss=2.4635832470702693e+88\n",
      "Gradient Descent(45/49): loss=2.732967255536311e+90\n",
      "Gradient Descent(46/49): loss=3.0318074409363206e+92\n",
      "Gradient Descent(47/49): loss=3.363324730765212e+94\n",
      "Gradient Descent(48/49): loss=3.7310922494086233e+96\n",
      "Gradient Descent(49/49): loss=4.139073829611999e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5689005257018374\n",
      "Gradient Descent(2/49): loss=325.85551721694463\n",
      "Gradient Descent(3/49): loss=35489.857746658694\n",
      "Gradient Descent(4/49): loss=3924064.205138413\n",
      "Gradient Descent(5/49): loss=435610708.3175328\n",
      "Gradient Descent(6/49): loss=48416525412.76621\n",
      "Gradient Descent(7/49): loss=5383437547026.615\n",
      "Gradient Descent(8/49): loss=598661958968146.8\n",
      "Gradient Descent(9/49): loss=6.657667276466132e+16\n",
      "Gradient Descent(10/49): loss=7.404037582241765e+18\n",
      "Gradient Descent(11/49): loss=8.234119484250072e+20\n",
      "Gradient Descent(12/49): loss=9.15727772260565e+22\n",
      "Gradient Descent(13/49): loss=1.0183939936612873e+25\n",
      "Gradient Descent(14/49): loss=1.1325707640034003e+27\n",
      "Gradient Descent(15/49): loss=1.259548481055197e+29\n",
      "Gradient Descent(16/49): loss=1.4007622801543682e+31\n",
      "Gradient Descent(17/49): loss=1.5578082203273582e+33\n",
      "Gradient Descent(18/49): loss=1.7324613110317687e+35\n",
      "Gradient Descent(19/49): loss=1.926695569524797e+37\n",
      "Gradient Descent(20/49): loss=2.1427063304853195e+39\n",
      "Gradient Descent(21/49): loss=2.3829350581731046e+41\n",
      "Gradient Descent(22/49): loss=2.650096940874841e+43\n",
      "Gradient Descent(23/49): loss=2.9472115792683964e+45\n",
      "Gradient Descent(24/49): loss=3.2776371154672433e+47\n",
      "Gradient Descent(25/49): loss=3.6451081884541826e+49\n",
      "Gradient Descent(26/49): loss=4.05377814488341e+51\n",
      "Gradient Descent(27/49): loss=4.508265982334265e+53\n",
      "Gradient Descent(28/49): loss=5.013708555591795e+55\n",
      "Gradient Descent(29/49): loss=5.575818636015601e+57\n",
      "Gradient Descent(30/49): loss=6.200949480213548e+59\n",
      "Gradient Descent(31/49): loss=6.896166637808313e+61\n",
      "Gradient Descent(32/49): loss=7.669327809905423e+63\n",
      "Gradient Descent(33/49): loss=8.52917166086378e+65\n",
      "Gradient Descent(34/49): loss=9.485416587164822e+67\n",
      "Gradient Descent(35/49): loss=1.054887055971744e+70\n",
      "Gradient Descent(36/49): loss=1.1731553281091483e+72\n",
      "Gradient Descent(37/49): loss=1.3046832038365145e+74\n",
      "Gradient Descent(38/49): loss=1.450957278706357e+76\n",
      "Gradient Descent(39/49): loss=1.6136308173817435e+78\n",
      "Gradient Descent(40/49): loss=1.7945424396820152e+80\n",
      "Gradient Descent(41/49): loss=1.9957369016075263e+82\n",
      "Gradient Descent(42/49): loss=2.2194882062214106e+84\n",
      "Gradient Descent(43/49): loss=2.4683253055991565e+86\n",
      "Gradient Descent(44/49): loss=2.745060684342916e+88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=3.0528221477258303e+90\n",
      "Gradient Descent(46/49): loss=3.395088173752401e+92\n",
      "Gradient Descent(47/49): loss=3.775727228702177e+94\n",
      "Gradient Descent(48/49): loss=4.199041490520839e+96\n",
      "Gradient Descent(49/49): loss=4.669815474243381e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.612729956931102\n",
      "Gradient Descent(2/49): loss=337.10509319602755\n",
      "Gradient Descent(3/49): loss=37464.36398372561\n",
      "Gradient Descent(4/49): loss=4223942.091219371\n",
      "Gradient Descent(5/49): loss=477951177.70046675\n",
      "Gradient Descent(6/49): loss=54138707410.68866\n",
      "Gradient Descent(7/49): loss=6134406020289.863\n",
      "Gradient Descent(8/49): loss=695153736047718.5\n",
      "Gradient Descent(9/49): loss=7.877763426312893e+16\n",
      "Gradient Descent(10/49): loss=8.927489244364313e+18\n",
      "Gradient Descent(11/49): loss=1.0117124696986418e+21\n",
      "Gradient Descent(12/49): loss=1.1465296796787859e+23\n",
      "Gradient Descent(13/49): loss=1.2993125600682432e+25\n",
      "Gradient Descent(14/49): loss=1.4724549448127076e+27\n",
      "Gradient Descent(15/49): loss=1.668669802092921e+29\n",
      "Gradient Descent(16/49): loss=1.8910316718401355e+31\n",
      "Gradient Descent(17/49): loss=2.143024815701107e+33\n",
      "Gradient Descent(18/49): loss=2.428597803856277e+35\n",
      "Gradient Descent(19/49): loss=2.752225383264899e+37\n",
      "Gradient Descent(20/49): loss=3.118978592918215e+39\n",
      "Gradient Descent(21/49): loss=3.5346042233814385e+41\n",
      "Gradient Descent(22/49): loss=4.0056148652113525e+43\n",
      "Gradient Descent(23/49): loss=4.5393909570736834e+45\n",
      "Gradient Descent(24/49): loss=5.144296432526665e+47\n",
      "Gradient Descent(25/49): loss=5.829809777559841e+49\n",
      "Gradient Descent(26/49): loss=6.60667255246851e+51\n",
      "Gradient Descent(27/49): loss=7.4870577053050795e+53\n",
      "Gradient Descent(28/49): loss=8.48476031427106e+55\n",
      "Gradient Descent(29/49): loss=9.61541374786233e+57\n",
      "Gradient Descent(30/49): loss=1.0896734629860145e+60\n",
      "Gradient Descent(31/49): loss=1.234880044761375e+62\n",
      "Gradient Descent(32/49): loss=1.3994364153561315e+64\n",
      "Gradient Descent(33/49): loss=1.585921068959572e+66\n",
      "Gradient Descent(34/49): loss=1.7972561020786468e+68\n",
      "Gradient Descent(35/49): loss=2.0367530009409908e+70\n",
      "Gradient Descent(36/49): loss=2.308164530388429e+72\n",
      "Gradient Descent(37/49): loss=2.6157435373272444e+74\n",
      "Gradient Descent(38/49): loss=2.964309590147734e+76\n",
      "Gradient Descent(39/49): loss=3.3593244983108926e+78\n",
      "Gradient Descent(40/49): loss=3.806977895446232e+80\n",
      "Gradient Descent(41/49): loss=4.314284226993727e+82\n",
      "Gradient Descent(42/49): loss=4.889192662124751e+84\n",
      "Gradient Descent(43/49): loss=5.540711652192503e+86\n",
      "Gradient Descent(44/49): loss=6.279050087463777e+88\n",
      "Gradient Descent(45/49): loss=7.115777263968871e+90\n",
      "Gradient Descent(46/49): loss=8.064004167048919e+92\n",
      "Gradient Descent(47/49): loss=9.138588912198754e+94\n",
      "Gradient Descent(48/49): loss=1.035636956233427e+97\n",
      "Gradient Descent(49/49): loss=1.173642796958225e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.599185015957057\n",
      "Gradient Descent(2/49): loss=334.77517479442633\n",
      "Gradient Descent(3/49): loss=37203.91635105486\n",
      "Gradient Descent(4/49): loss=4199808.218868378\n",
      "Gradient Descent(5/49): loss=476042945.9307717\n",
      "Gradient Descent(6/49): loss=54025254043.11208\n",
      "Gradient Descent(7/49): loss=6133584496806.229\n",
      "Gradient Descent(8/49): loss=696441913864166.8\n",
      "Gradient Descent(9/49): loss=7.908104794636125e+16\n",
      "Gradient Descent(10/49): loss=8.97977343086159e+18\n",
      "Gradient Descent(11/49): loss=1.0196710612082109e+21\n",
      "Gradient Descent(12/49): loss=1.157858190965644e+23\n",
      "Gradient Descent(13/49): loss=1.314773166601912e+25\n",
      "Gradient Descent(14/49): loss=1.4929537358882525e+27\n",
      "Gradient Descent(15/49): loss=1.6952817493375705e+29\n",
      "Gradient Descent(16/49): loss=1.925029678103477e+31\n",
      "Gradient Descent(17/49): loss=2.1859135094319648e+33\n",
      "Gradient Descent(18/49): loss=2.4821528374094135e+35\n",
      "Gradient Descent(19/49): loss=2.818539107120256e+37\n",
      "Gradient Descent(20/49): loss=3.200513110966567e+39\n",
      "Gradient Descent(21/49): loss=3.634252988750078e+41\n",
      "Gradient Descent(22/49): loss=4.126774154177502e+43\n",
      "Gradient Descent(23/49): loss=4.6860427637783295e+45\n",
      "Gradient Descent(24/49): loss=5.321104563419249e+47\n",
      "Gradient Descent(25/49): loss=6.042231196376464e+49\n",
      "Gradient Descent(26/49): loss=6.861086339376764e+51\n",
      "Gradient Descent(27/49): loss=7.790914353726687e+53\n",
      "Gradient Descent(28/49): loss=8.846754502818219e+55\n",
      "Gradient Descent(29/49): loss=1.0045684200814962e+58\n",
      "Gradient Descent(30/49): loss=1.1407095226883037e+60\n",
      "Gradient Descent(31/49): loss=1.2953007372521375e+62\n",
      "Gradient Descent(32/49): loss=1.4708424594999938e+64\n",
      "Gradient Descent(33/49): loss=1.670173943741741e+66\n",
      "Gradient Descent(34/49): loss=1.8965192256566467e+68\n",
      "Gradient Descent(35/49): loss=2.15353926862689e+70\n",
      "Gradient Descent(36/49): loss=2.445391177045565e+72\n",
      "Gradient Descent(37/49): loss=2.7767954343294243e+74\n",
      "Gradient Descent(38/49): loss=3.1531122531604217e+76\n",
      "Gradient Descent(39/49): loss=3.5804282728631515e+78\n",
      "Gradient Descent(40/49): loss=4.065655006182739e+80\n",
      "Gradient Descent(41/49): loss=4.616640627765082e+82\n",
      "Gradient Descent(42/49): loss=5.242296912433406e+84\n",
      "Gradient Descent(43/49): loss=5.952743376391559e+86\n",
      "Gradient Descent(44/49): loss=6.759470952728831e+88\n",
      "Gradient Descent(45/49): loss=7.675527848553339e+90\n",
      "Gradient Descent(46/49): loss=8.71573058985244e+92\n",
      "Gradient Descent(47/49): loss=9.896903667570833e+94\n",
      "Gradient Descent(48/49): loss=1.1238151660999823e+97\n",
      "Gradient Descent(49/49): loss=1.276116823986747e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6114679834533936\n",
      "Gradient Descent(2/49): loss=333.50655863780264\n",
      "Gradient Descent(3/49): loss=36735.914339619405\n",
      "Gradient Descent(4/49): loss=4108233.9575351607\n",
      "Gradient Descent(5/49): loss=461220486.0068107\n",
      "Gradient Descent(6/49): loss=51840264987.41862\n",
      "Gradient Descent(7/49): loss=5828862723695.429\n",
      "Gradient Descent(8/49): loss=655467045045209.2\n",
      "Gradient Descent(9/49): loss=7.371131073995224e+16\n",
      "Gradient Descent(10/49): loss=8.289391607725348e+18\n",
      "Gradient Descent(11/49): loss=9.322081190062081e+20\n",
      "Gradient Descent(12/49): loss=1.0483436198052102e+23\n",
      "Gradient Descent(13/49): loss=1.178947892891592e+25\n",
      "Gradient Descent(14/49): loss=1.3258232257766882e+27\n",
      "Gradient Descent(15/49): loss=1.4909966019106324e+29\n",
      "Gradient Descent(16/49): loss=1.6767475896584084e+31\n",
      "Gradient Descent(17/49): loss=1.8856397718352746e+33\n",
      "Gradient Descent(18/49): loss=2.1205561148923304e+35\n",
      "Gradient Descent(19/49): loss=2.3847387532486763e+37\n",
      "Gradient Descent(20/49): loss=2.6818337332322357e+39\n",
      "Gradient Descent(21/49): loss=3.015941332469733e+41\n",
      "Gradient Descent(22/49): loss=3.3916726485818863e+43\n",
      "Gradient Descent(23/49): loss=3.814213237954279e+45\n",
      "Gradient Descent(24/49): loss=4.289394682795688e+47\n",
      "Gradient Descent(25/49): loss=4.823775074168207e+49\n",
      "Gradient Descent(26/49): loss=5.424729521743342e+51\n",
      "Gradient Descent(27/49): loss=6.100551939426778e+53\n",
      "Gradient Descent(28/49): loss=6.860569511617638e+55\n",
      "Gradient Descent(29/49): loss=7.715271419877549e+57\n",
      "Gradient Descent(30/49): loss=8.676453606596278e+59\n",
      "Gradient Descent(31/49): loss=9.757381573571674e+61\n",
      "Gradient Descent(32/49): loss=1.0972973462325099e+64\n",
      "Gradient Descent(33/49): loss=1.2340005942887086e+66\n",
      "Gradient Descent(34/49): loss=1.3877345752572491e+68\n",
      "Gradient Descent(35/49): loss=1.5606210080267348e+70\n",
      "Gradient Descent(36/49): loss=1.7550459389850447e+72\n",
      "Gradient Descent(37/49): loss=1.9736926724077064e+74\n",
      "Gradient Descent(38/49): loss=2.2195788033723248e+76\n",
      "Gradient Descent(39/49): loss=2.4960978642991317e+78\n",
      "Gradient Descent(40/49): loss=2.807066159891393e+80\n",
      "Gradient Descent(41/49): loss=3.156775436855683e+82\n",
      "Gradient Descent(42/49): loss=3.5500521153092118e+84\n",
      "Gradient Descent(43/49): loss=3.9923238993409685e+86\n",
      "Gradient Descent(44/49): loss=4.4896946860343954e+88\n",
      "Gradient Descent(45/49): loss=5.049028806789229e+90\n",
      "Gradient Descent(46/49): loss=5.67804576357605e+92\n",
      "Gradient Descent(47/49): loss=6.385426767601657e+94\n",
      "Gradient Descent(48/49): loss=7.18093455075011e+96\n",
      "Gradient Descent(49/49): loss=8.075548103345459e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6131180108740764\n",
      "Gradient Descent(2/49): loss=334.81935359672883\n",
      "Gradient Descent(3/49): loss=36963.59460587962\n",
      "Gradient Descent(4/49): loss=4142728.8499618317\n",
      "Gradient Descent(5/49): loss=466157812.91567415\n",
      "Gradient Descent(6/49): loss=52518652376.97966\n",
      "Gradient Descent(7/49): loss=5919237732277.223\n",
      "Gradient Descent(8/49): loss=667227843130387.6\n",
      "Gradient Descent(9/49): loss=7.521441408498838e+16\n",
      "Gradient Descent(10/49): loss=8.478794949048859e+18\n",
      "Gradient Descent(11/49): loss=9.558048388311751e+20\n",
      "Gradient Descent(12/49): loss=1.0774695196035043e+23\n",
      "Gradient Descent(13/49): loss=1.214621561379221e+25\n",
      "Gradient Descent(14/49): loss=1.3692320420503815e+27\n",
      "Gradient Descent(15/49): loss=1.5435231445279127e+29\n",
      "Gradient Descent(16/49): loss=1.7399999924186473e+31\n",
      "Gradient Descent(17/49): loss=1.9614866180533646e+33\n",
      "Gradient Descent(18/49): loss=2.211166538848943e+35\n",
      "Gradient Descent(19/49): loss=2.492628509883354e+37\n",
      "Gradient Descent(20/49): loss=2.809918104562125e+39\n",
      "Gradient Descent(21/49): loss=3.167595862617797e+41\n",
      "Gradient Descent(22/49): loss=3.570802840423586e+43\n",
      "Gradient Descent(23/49): loss=4.025334505503225e+45\n",
      "Gradient Descent(24/49): loss=4.5377240372414675e+47\n",
      "Gradient Descent(25/49): loss=5.115336231070363e+49\n",
      "Gradient Descent(26/49): loss=5.7664733558402965e+51\n",
      "Gradient Descent(27/49): loss=6.5004944859043156e+53\n",
      "Gradient Descent(28/49): loss=7.327950023123993e+55\n",
      "Gradient Descent(29/49): loss=8.26073334233947e+57\n",
      "Gradient Descent(30/49): loss=9.312251739968598e+59\n",
      "Gradient Descent(31/49): loss=1.0497619142853182e+62\n",
      "Gradient Descent(32/49): loss=1.1833873347239325e+64\n",
      "Gradient Descent(33/49): loss=1.3340220910361575e+66\n",
      "Gradient Descent(34/49): loss=1.503831321456249e+68\n",
      "Gradient Descent(35/49): loss=1.6952557671937063e+70\n",
      "Gradient Descent(36/49): loss=1.9110468542579254e+72\n",
      "Gradient Descent(37/49): loss=2.154306240889392e+74\n",
      "Gradient Descent(38/49): loss=2.4285303990294564e+76\n",
      "Gradient Descent(39/49): loss=2.737660870617582e+78\n",
      "Gradient Descent(40/49): loss=3.086140921071374e+80\n",
      "Gradient Descent(41/49): loss=3.4789794042542446e+82\n",
      "Gradient Descent(42/49): loss=3.921822756889367e+84\n",
      "Gradient Descent(43/49): loss=4.4210361572267766e+86\n",
      "Gradient Descent(44/49): loss=4.983795014491984e+88\n",
      "Gradient Descent(45/49): loss=5.61818810413345e+90\n",
      "Gradient Descent(46/49): loss=6.333333831275922e+92\n",
      "Gradient Descent(47/49): loss=7.139511293485074e+94\n",
      "Gradient Descent(48/49): loss=8.048308026664077e+96\n",
      "Gradient Descent(49/49): loss=9.072786557698066e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.65756626054452\n",
      "Gradient Descent(2/49): loss=346.3703740160491\n",
      "Gradient Descent(3/49): loss=39018.72384891843\n",
      "Gradient Descent(4/49): loss=4459115.623438266\n",
      "Gradient Descent(5/49): loss=511439005.8676715\n",
      "Gradient Descent(6/49): loss=58721767859.68379\n",
      "Gradient Descent(7/49): loss=6744429539978.395\n",
      "Gradient Descent(8/49): loss=774702994603818.0\n",
      "Gradient Descent(9/49): loss=8.898955870948466e+16\n",
      "Gradient Descent(10/49): loss=1.0222267159470901e+19\n",
      "Gradient Descent(11/49): loss=1.1742397479757287e+21\n",
      "Gradient Descent(12/49): loss=1.3488596468087791e+23\n",
      "Gradient Descent(13/49): loss=1.549447569219786e+25\n",
      "Gradient Descent(14/49): loss=1.7798649518397778e+27\n",
      "Gradient Descent(15/49): loss=2.044547624656593e+29\n",
      "Gradient Descent(16/49): loss=2.3485911260101857e+31\n",
      "Gradient Descent(17/49): loss=2.6978487701341553e+33\n",
      "Gradient Descent(18/49): loss=3.0990443220217783e+35\n",
      "Gradient Descent(19/49): loss=3.5599014367376133e+37\n",
      "Gradient Descent(20/49): loss=4.089292350710915e+39\n",
      "Gradient Descent(21/49): loss=4.697408685965827e+41\n",
      "Gradient Descent(22/49): loss=5.395957654965062e+43\n",
      "Gradient Descent(23/49): loss=6.198387443114696e+45\n",
      "Gradient Descent(24/49): loss=7.120146107827302e+47\n",
      "Gradient Descent(25/49): loss=8.178978978337193e+49\n",
      "Gradient Descent(26/49): loss=9.395270281679937e+51\n",
      "Gradient Descent(27/49): loss=1.0792435571678963e+54\n",
      "Gradient Descent(28/49): loss=1.2397372515824575e+56\n",
      "Gradient Descent(29/49): loss=1.4240978718412928e+58\n",
      "Gradient Descent(30/49): loss=1.635874654886921e+60\n",
      "Gradient Descent(31/49): loss=1.8791446426651405e+62\n",
      "Gradient Descent(32/49): loss=2.1585911717063555e+64\n",
      "Gradient Descent(33/49): loss=2.4795940348477862e+66\n",
      "Gradient Descent(34/49): loss=2.84833305085392e+68\n",
      "Gradient Descent(35/49): loss=3.2719070358163633e+70\n",
      "Gradient Descent(36/49): loss=3.758470466722731e+72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=4.3173904681868027e+74\n",
      "Gradient Descent(38/49): loss=4.959427144586177e+76\n",
      "Gradient Descent(39/49): loss=5.696940729289193e+78\n",
      "Gradient Descent(40/49): loss=6.544129538925202e+80\n",
      "Gradient Descent(41/49): loss=7.517303313699187e+82\n",
      "Gradient Descent(42/49): loss=8.635197205988351e+84\n",
      "Gradient Descent(43/49): loss=9.919332462004354e+86\n",
      "Gradient Descent(44/49): loss=1.1394430740219795e+89\n",
      "Gradient Descent(45/49): loss=1.3088890042851703e+91\n",
      "Gradient Descent(46/49): loss=1.503533142284548e+93\n",
      "Gradient Descent(47/49): loss=1.7271226991341902e+95\n",
      "Gradient Descent(48/49): loss=1.9839621315776907e+97\n",
      "Gradient Descent(49/49): loss=2.27899600966827e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6437966304913774\n",
      "Gradient Descent(2/49): loss=343.9770337215403\n",
      "Gradient Descent(3/49): loss=38747.440058414875\n",
      "Gradient Descent(4/49): loss=4433623.311844967\n",
      "Gradient Descent(5/49): loss=509394420.2432001\n",
      "Gradient Descent(6/49): loss=58598317762.564125\n",
      "Gradient Descent(7/49): loss=6743472391946.364\n",
      "Gradient Descent(8/49): loss=776131433048059.1\n",
      "Gradient Descent(9/49): loss=8.933137533805554e+16\n",
      "Gradient Descent(10/49): loss=1.0282015405798998e+19\n",
      "Gradient Descent(11/49): loss=1.183461799669483e+21\n",
      "Gradient Descent(12/49): loss=1.3621684151199415e+23\n",
      "Gradient Descent(13/49): loss=1.567860976296357e+25\n",
      "Gradient Descent(14/49): loss=1.804614133733401e+27\n",
      "Gradient Descent(15/49): loss=2.0771180373748564e+29\n",
      "Gradient Descent(16/49): loss=2.3907711467275545e+31\n",
      "Gradient Descent(17/49): loss=2.751787139241708e+33\n",
      "Gradient Descent(18/49): loss=3.167317993241407e+35\n",
      "Gradient Descent(19/49): loss=3.645595668345675e+37\n",
      "Gradient Descent(20/49): loss=4.1960951844537844e+39\n",
      "Gradient Descent(21/49): loss=4.82972232788648e+41\n",
      "Gradient Descent(22/49): loss=5.559029702489549e+43\n",
      "Gradient Descent(23/49): loss=6.39846540553411e+45\n",
      "Gradient Descent(24/49): loss=7.36465925475251e+47\n",
      "Gradient Descent(25/49): loss=8.476752236829989e+49\n",
      "Gradient Descent(26/49): loss=9.756775703946468e+51\n",
      "Gradient Descent(27/49): loss=1.123008783051621e+54\n",
      "Gradient Descent(28/49): loss=1.2925875976640448e+56\n",
      "Gradient Descent(29/49): loss=1.487773491045e+58\n",
      "Gradient Descent(30/49): loss=1.7124332344333284e+60\n",
      "Gradient Descent(31/49): loss=1.9710174969793757e+62\n",
      "Gradient Descent(32/49): loss=2.2686490166633608e+64\n",
      "Gradient Descent(33/49): loss=2.6112240853747775e+66\n",
      "Gradient Descent(34/49): loss=3.00552935864433e+68\n",
      "Gradient Descent(35/49): loss=3.4593763041123905e+70\n",
      "Gradient Descent(36/49): loss=3.981755952253344e+72\n",
      "Gradient Descent(37/49): loss=4.583017015078065e+74\n",
      "Gradient Descent(38/49): loss=5.275070901472122e+76\n",
      "Gradient Descent(39/49): loss=6.071627690669608e+78\n",
      "Gradient Descent(40/49): loss=6.988467738664545e+80\n",
      "Gradient Descent(41/49): loss=8.043754298275941e+82\n",
      "Gradient Descent(42/49): loss=9.258393346092386e+84\n",
      "Gradient Descent(43/49): loss=1.0656447744722877e+87\n",
      "Gradient Descent(44/49): loss=1.2265613945203382e+89\n",
      "Gradient Descent(45/49): loss=1.411777067337173e+91\n",
      "Gradient Descent(46/49): loss=1.6249610470078343e+93\n",
      "Gradient Descent(47/49): loss=1.8703366596492383e+95\n",
      "Gradient Descent(48/49): loss=2.1527649705014712e+97\n",
      "Gradient Descent(49/49): loss=2.4778410850842967e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6563031657399057\n",
      "Gradient Descent(2/49): loss=342.67689663419475\n",
      "Gradient Descent(3/49): loss=38260.62858352812\n",
      "Gradient Descent(4/49): loss=4337046.338940381\n",
      "Gradient Descent(5/49): loss=493547277.6665052\n",
      "Gradient Descent(6/49): loss=56230268378.81152\n",
      "Gradient Descent(7/49): loss=6408702772560.017\n",
      "Gradient Descent(8/49): loss=730500893621733.4\n",
      "Gradient Descent(9/49): loss=8.326983390402267e+16\n",
      "Gradient Descent(10/49): loss=9.492048064836973e+18\n",
      "Gradient Descent(11/49): loss=1.0820164711118684e+21\n",
      "Gradient Descent(12/49): loss=1.2334125722661988e+23\n",
      "Gradient Descent(13/49): loss=1.405992649737874e+25\n",
      "Gradient Descent(14/49): loss=1.602720486300713e+27\n",
      "Gradient Descent(15/49): loss=1.8269747499101823e+29\n",
      "Gradient Descent(16/49): loss=2.082606925389254e+31\n",
      "Gradient Descent(17/49): loss=2.3740074272497236e+33\n",
      "Gradient Descent(18/49): loss=2.706180990933984e+35\n",
      "Gradient Descent(19/49): loss=3.084832623180355e+37\n",
      "Gradient Descent(20/49): loss=3.5164655824646797e+39\n",
      "Gradient Descent(21/49): loss=4.00849307037728e+41\n",
      "Gradient Descent(22/49): loss=4.569365551494415e+43\n",
      "Gradient Descent(23/49): loss=5.2087158881707946e+45\n",
      "Gradient Descent(24/49): loss=5.937524782813066e+47\n",
      "Gradient Descent(25/49): loss=6.768309369033366e+49\n",
      "Gradient Descent(26/49): loss=7.715338190681162e+51\n",
      "Gradient Descent(27/49): loss=8.79487626096615e+53\n",
      "Gradient Descent(28/49): loss=1.0025464410508126e+56\n",
      "Gradient Descent(29/49): loss=1.1428237722053435e+58\n",
      "Gradient Descent(30/49): loss=1.3027288520906105e+60\n",
      "Gradient Descent(31/49): loss=1.48500801553538e+62\n",
      "Gradient Descent(32/49): loss=1.6927918673677745e+64\n",
      "Gradient Descent(33/49): loss=1.9296490498695086e+66\n",
      "Gradient Descent(34/49): loss=2.1996475334278813e+68\n",
      "Gradient Descent(35/49): loss=2.5074244830388162e+70\n",
      "Gradient Descent(36/49): loss=2.8582659006030184e+72\n",
      "Gradient Descent(37/49): loss=3.2581974108544086e+74\n",
      "Gradient Descent(38/49): loss=3.714087750149011e+76\n",
      "Gradient Descent(39/49): loss=4.233766735512075e+78\n",
      "Gradient Descent(40/49): loss=4.826159740035605e+80\n",
      "Gradient Descent(41/49): loss=5.501440984212214e+82\n",
      "Gradient Descent(42/49): loss=6.271208275950365e+84\n",
      "Gradient Descent(43/49): loss=7.148682200392638e+86\n",
      "Gradient Descent(44/49): loss=8.148933180578549e+88\n",
      "Gradient Descent(45/49): loss=9.289140308669352e+90\n",
      "Gradient Descent(46/49): loss=1.0588886393104422e+93\n",
      "Gradient Descent(47/49): loss=1.2070494289060194e+95\n",
      "Gradient Descent(48/49): loss=1.3759410288612957e+97\n",
      "Gradient Descent(49/49): loss=1.5684641155248659e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6576460638392017\n",
      "Gradient Descent(2/49): loss=343.9671331163926\n",
      "Gradient Descent(3/49): loss=38487.98340561273\n",
      "Gradient Descent(4/49): loss=4371982.026832189\n",
      "Gradient Descent(5/49): loss=498619483.0471018\n",
      "Gradient Descent(6/49): loss=56937155777.48157\n",
      "Gradient Descent(7/49): loss=6504209257333.298\n",
      "Gradient Descent(8/49): loss=743104094059756.4\n",
      "Gradient Descent(9/49): loss=8.490307816106506e+16\n",
      "Gradient Descent(10/49): loss=9.700707156409012e+18\n",
      "Gradient Descent(11/49): loss=1.1083716363026039e+21\n",
      "Gradient Descent(12/49): loss=1.2663918118523208e+23\n",
      "Gradient Descent(13/49): loss=1.4469416255165007e+25\n",
      "Gradient Descent(14/49): loss=1.6532327600055217e+27\n",
      "Gradient Descent(15/49): loss=1.8889350253872797e+29\n",
      "Gradient Descent(16/49): loss=2.1582415281965155e+31\n",
      "Gradient Descent(17/49): loss=2.4659432225338826e+33\n",
      "Gradient Descent(18/49): loss=2.8175141244261293e+35\n",
      "Gradient Descent(19/49): loss=3.219208688333815e+37\n",
      "Gradient Descent(20/49): loss=3.678173071652369e+39\n",
      "Gradient Descent(21/49): loss=4.202572264142668e+41\n",
      "Gradient Descent(22/49): loss=4.801735342992637e+43\n",
      "Gradient Descent(23/49): loss=5.486321437246244e+45\n",
      "Gradient Descent(24/49): loss=6.268509353977431e+47\n",
      "Gradient Descent(25/49): loss=7.162214239613166e+49\n",
      "Gradient Descent(26/49): loss=8.183335130797107e+51\n",
      "Gradient Descent(27/49): loss=9.350037798724335e+53\n",
      "Gradient Descent(28/49): loss=1.0683077918753363e+56\n",
      "Gradient Descent(29/49): loss=1.220617031449086e+58\n",
      "Gradient Descent(30/49): loss=1.394641084521315e+60\n",
      "Gradient Descent(31/49): loss=1.5934758442012783e+62\n",
      "Gradient Descent(32/49): loss=1.8206585868108877e+64\n",
      "Gradient Descent(33/49): loss=2.0802309001362644e+66\n",
      "Gradient Descent(34/49): loss=2.376810583395337e+68\n",
      "Gradient Descent(35/49): loss=2.7156737980240757e+70\n",
      "Gradient Descent(36/49): loss=3.102848930746219e+72\n",
      "Gradient Descent(37/49): loss=3.5452238387534053e+74\n",
      "Gradient Descent(38/49): loss=4.050668384890616e+76\n",
      "Gradient Descent(39/49): loss=4.628174442751629e+78\n",
      "Gradient Descent(40/49): loss=5.28801586237919e+80\n",
      "Gradient Descent(41/49): loss=6.041931242364511e+82\n",
      "Gradient Descent(42/49): loss=6.903332759867289e+84\n",
      "Gradient Descent(43/49): loss=7.887544773648708e+86\n",
      "Gradient Descent(44/49): loss=9.012076444871987e+88\n",
      "Gradient Descent(45/49): loss=1.029693322560322e+91\n",
      "Gradient Descent(46/49): loss=1.176497275640203e+93\n",
      "Gradient Descent(47/49): loss=1.3442311504430814e+95\n",
      "Gradient Descent(48/49): loss=1.5358789376186609e+97\n",
      "Gradient Descent(49/49): loss=1.7548500570330422e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7027174107897025\n",
      "Gradient Descent(2/49): loss=355.82561216348654\n",
      "Gradient Descent(3/49): loss=40626.4597849493\n",
      "Gradient Descent(4/49): loss=4705667.410458763\n",
      "Gradient Descent(5/49): loss=547023944.9414771\n",
      "Gradient Descent(6/49): loss=63657999508.22669\n",
      "Gradient Descent(7/49): loss=7410388770733.811\n",
      "Gradient Descent(8/49): loss=862726451943032.2\n",
      "Gradient Descent(9/49): loss=1.0044287061095134e+17\n",
      "Gradient Descent(10/49): loss=1.169417264830192e+19\n",
      "Gradient Descent(11/49): loss=1.3615113701540217e+21\n",
      "Gradient Descent(12/49): loss=1.585161378152086e+23\n",
      "Gradient Descent(13/49): loss=1.8455500652243636e+25\n",
      "Gradient Descent(14/49): loss=2.1487120711126887e+27\n",
      "Gradient Descent(15/49): loss=2.5016735115638787e+29\n",
      "Gradient Descent(16/49): loss=2.9126147267755644e+31\n",
      "Gradient Descent(17/49): loss=3.391059838403891e+33\n",
      "Gradient Descent(18/49): loss=3.9480974718728185e+35\n",
      "Gradient Descent(19/49): loss=4.596637746080076e+37\n",
      "Gradient Descent(20/49): loss=5.3517114816360734e+39\n",
      "Gradient Descent(21/49): loss=6.230818560375627e+41\n",
      "Gradient Descent(22/49): loss=7.254333509184067e+43\n",
      "Gradient Descent(23/49): loss=8.445977707858105e+45\n",
      "Gradient Descent(24/49): loss=9.833369164982364e+47\n",
      "Gradient Descent(25/49): loss=1.1448662603607426e+50\n",
      "Gradient Descent(26/49): loss=1.3329294691590816e+52\n",
      "Gradient Descent(27/49): loss=1.5518851688344328e+54\n",
      "Gradient Descent(28/49): loss=1.8068079616903249e+56\n",
      "Gradient Descent(29/49): loss=2.1036060373458348e+58\n",
      "Gradient Descent(30/49): loss=2.4491581032319466e+60\n",
      "Gradient Descent(31/49): loss=2.851472808185594e+62\n",
      "Gradient Descent(32/49): loss=3.319874354004421e+64\n",
      "Gradient Descent(33/49): loss=3.8652185967676345e+66\n",
      "Gradient Descent(34/49): loss=4.500144646371332e+68\n",
      "Gradient Descent(35/49): loss=5.2393677954463005e+70\n",
      "Gradient Descent(36/49): loss=6.100020566693272e+72\n",
      "Gradient Descent(37/49): loss=7.10204978288063e+74\n",
      "Gradient Descent(38/49): loss=8.268678862152918e+76\n",
      "Gradient Descent(39/49): loss=9.626946052986233e+78\n",
      "Gradient Descent(40/49): loss=1.1208331083132224e+81\n",
      "Gradient Descent(41/49): loss=1.3049484746010387e+83\n",
      "Gradient Descent(42/49): loss=1.5193078333725423e+85\n",
      "Gradient Descent(43/49): loss=1.7688792603500068e+87\n",
      "Gradient Descent(44/49): loss=2.059446919819298e+89\n",
      "Gradient Descent(45/49): loss=2.3977451206669455e+91\n",
      "Gradient Descent(46/49): loss=2.791614393337497e+93\n",
      "Gradient Descent(47/49): loss=3.250183205011137e+95\n",
      "Gradient Descent(48/49): loss=3.784079524503047e+97\n",
      "Gradient Descent(49/49): loss=4.40567714019498e+99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6887213563610293\n",
      "Gradient Descent(2/49): loss=353.36756537707544\n",
      "Gradient Descent(3/49): loss=40343.966887596944\n",
      "Gradient Descent(4/49): loss=4678750.242329217\n",
      "Gradient Descent(5/49): loss=544834302.6761001\n",
      "Gradient Descent(6/49): loss=63523752367.882256\n",
      "Gradient Descent(7/49): loss=7409278655521.91\n",
      "Gradient Descent(8/49): loss=864309336469583.5\n",
      "Gradient Descent(9/49): loss=1.0082764648388523e+17\n",
      "Gradient Descent(10/49): loss=1.1762389905400812e+19\n",
      "Gradient Descent(11/49): loss=1.3721869650080526e+21\n",
      "Gradient Descent(12/49): loss=1.6007797429089967e+23\n",
      "Gradient Descent(13/49): loss=1.867454610338467e+25\n",
      "Gradient Descent(14/49): loss=2.1785553039178327e+27\n",
      "Gradient Descent(15/49): loss=2.541482612487969e+29\n",
      "Gradient Descent(16/49): loss=2.96487032020884e+31\n",
      "Gradient Descent(17/49): loss=3.458790555087915e+33\n",
      "Gradient Descent(18/49): loss=4.0349933823820504e+35\n",
      "Gradient Descent(19/49): loss=4.707186325486565e+37\n",
      "Gradient Descent(20/49): loss=5.491360457491958e+39\n",
      "Gradient Descent(21/49): loss=6.406170818505681e+41\n",
      "Gradient Descent(22/49): loss=7.473380207729654e+43\n",
      "Gradient Descent(23/49): loss=8.718376907501974e+45\n",
      "Gradient Descent(24/49): loss=1.0170778655523657e+48\n",
      "Gradient Descent(25/49): loss=1.1865137233365013e+50\n",
      "Gradient Descent(26/49): loss=1.3841760432978346e+52\n",
      "Gradient Descent(27/49): loss=1.614767095531008e+54\n",
      "Gradient Descent(28/49): loss=1.8837725052640872e+56\n",
      "Gradient Descent(29/49): loss=2.1975917526496342e+58\n",
      "Gradient Descent(30/49): loss=2.5636904126258556e+60\n",
      "Gradient Descent(31/49): loss=2.990777756544306e+62\n",
      "Gradient Descent(32/49): loss=3.489013940602338e+64\n",
      "Gradient Descent(33/49): loss=4.070251709970913e+66\n",
      "Gradient Descent(34/49): loss=4.748318368616514e+68\n",
      "Gradient Descent(35/49): loss=5.539344722712998e+70\n",
      "Gradient Descent(36/49): loss=6.4621488230134254e+72\n",
      "Gradient Descent(37/49): loss=7.538683635186655e+74\n",
      "Gradient Descent(38/49): loss=8.794559287931919e+76\n",
      "Gradient Descent(39/49): loss=1.0259652322846782e+79\n",
      "Gradient Descent(40/49): loss=1.196881643974308e+81\n",
      "Gradient Descent(41/49): loss=1.3962711645623812e+83\n",
      "Gradient Descent(42/49): loss=1.62887715322856e+85\n",
      "Gradient Descent(43/49): loss=1.900233169351145e+87\n",
      "Gradient Descent(44/49): loss=2.2167946126233312e+89\n",
      "Gradient Descent(45/49): loss=2.586092293207281e+91\n",
      "Gradient Descent(46/49): loss=3.0169115852693745e+93\n",
      "Gradient Descent(47/49): loss=3.5195014258538167e+95\n",
      "Gradient Descent(48/49): loss=4.105818131054366e+97\n",
      "Gradient Descent(49/49): loss=4.789809829727564e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7014532775843256\n",
      "Gradient Descent(2/49): loss=352.03532945729387\n",
      "Gradient Descent(3/49): loss=39837.72071223455\n",
      "Gradient Descent(4/49): loss=4576933.113900554\n",
      "Gradient Descent(5/49): loss=527899170.40213984\n",
      "Gradient Descent(6/49): loss=60958682023.38246\n",
      "Gradient Descent(7/49): loss=7041729159564.708\n",
      "Gradient Descent(8/49): loss=813530686792737.5\n",
      "Gradient Descent(9/49): loss=9.399071932961365e+16\n",
      "Gradient Descent(10/49): loss=1.0859287299707595e+19\n",
      "Gradient Descent(11/49): loss=1.2546407759100982e+21\n",
      "Gradient Descent(12/49): loss=1.4495661302507738e+23\n",
      "Gradient Descent(13/49): loss=1.6747764672320309e+25\n",
      "Gradient Descent(14/49): loss=1.9349766378029504e+27\n",
      "Gradient Descent(15/49): loss=2.235602677542201e+29\n",
      "Gradient Descent(16/49): loss=2.582935269609296e+31\n",
      "Gradient Descent(17/49): loss=2.984230921334888e+33\n",
      "Gradient Descent(18/49): loss=3.4478735530636665e+35\n",
      "Gradient Descent(19/49): loss=3.983549650527026e+37\n",
      "Gradient Descent(20/49): loss=4.6024506341466964e+39\n",
      "Gradient Descent(21/49): loss=5.3175066709805625e+41\n",
      "Gradient Descent(22/49): loss=6.143656813310524e+43\n",
      "Gradient Descent(23/49): loss=7.098161107334221e+45\n",
      "Gradient Descent(24/49): loss=8.200961192461571e+47\n",
      "Gradient Descent(25/49): loss=9.475096924865158e+49\n",
      "Gradient Descent(26/49): loss=1.0947187729425002e+52\n",
      "Gradient Descent(27/49): loss=1.264798873653595e+54\n",
      "Gradient Descent(28/49): loss=1.4613033322663737e+56\n",
      "Gradient Descent(29/49): loss=1.688337547869823e+58\n",
      "Gradient Descent(30/49): loss=1.950644751576801e+60\n",
      "Gradient Descent(31/49): loss=2.253705102782859e+62\n",
      "Gradient Descent(32/49): loss=2.6038501814355215e+64\n",
      "Gradient Descent(33/49): loss=3.0083952683027718e+66\n",
      "Gradient Descent(34/49): loss=3.4757921768590287e+68\n",
      "Gradient Descent(35/49): loss=4.0158058297738755e+70\n",
      "Gradient Descent(36/49): loss=4.6397182690649526e+72\n",
      "Gradient Descent(37/49): loss=5.360564362123857e+74\n",
      "Gradient Descent(38/49): loss=6.193404128019071e+76\n",
      "Gradient Descent(39/49): loss=7.1556373735555686e+78\n",
      "Gradient Descent(40/49): loss=8.267367212512603e+80\n",
      "Gradient Descent(41/49): loss=9.551820062755122e+82\n",
      "Gradient Descent(42/49): loss=1.1035830895858223e+85\n",
      "Gradient Descent(43/49): loss=1.2750403877148598e+87\n",
      "Gradient Descent(44/49): loss=1.473136010913501e+89\n",
      "Gradient Descent(45/49): loss=1.7020086011075126e+91\n",
      "Gradient Descent(46/49): loss=1.9664397969930977e+93\n",
      "Gradient Descent(47/49): loss=2.2719541327123848e+95\n",
      "Gradient Descent(48/49): loss=2.6249344572062756e+97\n",
      "Gradient Descent(49/49): loss=3.032755285601996e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7024846845972132\n",
      "Gradient Descent(2/49): loss=353.30135600267465\n",
      "Gradient Descent(3/49): loss=40064.407460653274\n",
      "Gradient Descent(4/49): loss=4612260.225006036\n",
      "Gradient Descent(5/49): loss=533101650.13758016\n",
      "Gradient Descent(6/49): loss=61694064649.98177\n",
      "Gradient Descent(7/49): loss=7142488601847.068\n",
      "Gradient Descent(8/49): loss=827013085104207.9\n",
      "Gradient Descent(9/49): loss=9.576216007835443e+16\n",
      "Gradient Descent(10/49): loss=1.1088727205916504e+19\n",
      "Gradient Descent(11/49): loss=1.2840192351686193e+21\n",
      "Gradient Descent(12/49): loss=1.4868325024699432e+23\n",
      "Gradient Descent(13/49): loss=1.7216814140303e+25\n",
      "Gradient Descent(14/49): loss=1.9936256469820773e+27\n",
      "Gradient Descent(15/49): loss=2.308524340843393e+29\n",
      "Gradient Descent(16/49): loss=2.67316220747845e+31\n",
      "Gradient Descent(17/49): loss=3.095395663391365e+33\n",
      "Gradient Descent(18/49): loss=3.584322083654242e+35\n",
      "Gradient Descent(19/49): loss=4.1504758052304275e+37\n",
      "Gradient Descent(20/49): loss=4.8060550954510634e+39\n",
      "Gradient Descent(21/49): loss=5.565184973063369e+41\n",
      "Gradient Descent(22/49): loss=6.44422154347263e+43\n",
      "Gradient Descent(23/49): loss=7.462104404925048e+45\n",
      "Gradient Descent(24/49): loss=8.64076471836459e+47\n",
      "Gradient Descent(25/49): loss=1.0005597733117783e+50\n",
      "Gradient Descent(26/49): loss=1.1586009949354633e+52\n",
      "Gradient Descent(27/49): loss=1.3416052706401396e+54\n",
      "Gradient Descent(28/49): loss=1.5535155848107413e+56\n",
      "Gradient Descent(29/49): loss=1.7988977272713955e+58\n",
      "Gradient Descent(30/49): loss=2.08303866715084e+60\n",
      "Gradient Descent(31/49): loss=2.4120604651755733e+62\n",
      "Gradient Descent(32/49): loss=2.793052178729295e+64\n",
      "Gradient Descent(33/49): loss=3.2342226016861446e+66\n",
      "Gradient Descent(34/49): loss=3.7450771299290224e+68\n",
      "Gradient Descent(35/49): loss=4.336622563272319e+70\n",
      "Gradient Descent(36/49): loss=5.0216042564226005e+72\n",
      "Gradient Descent(37/49): loss=5.814780728598582e+74\n",
      "Gradient Descent(38/49): loss=6.733241648510339e+76\n",
      "Gradient Descent(39/49): loss=7.796776045957754e+78\n",
      "Gradient Descent(40/49): loss=9.028298683483251e+80\n",
      "Gradient Descent(41/49): loss=1.0454343774622694e+83\n",
      "Gradient Descent(42/49): loss=1.210563668633801e+85\n",
      "Gradient Descent(43/49): loss=1.4017755943452495e+87\n",
      "Gradient Descent(44/49): loss=1.6231899798542386e+89\n",
      "Gradient Descent(45/49): loss=1.8795773883692504e+91\n",
      "Gradient Descent(46/49): loss=2.17646190693356e+93\n",
      "Gradient Descent(47/49): loss=2.5202401676276404e+95\n",
      "Gradient Descent(48/49): loss=2.9183191685043916e+97\n",
      "Gradient Descent(49/49): loss=3.379275863727292e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7481834076666467\n",
      "Gradient Descent(2/49): loss=365.47338730516117\n",
      "Gradient Descent(3/49): loss=42289.028229892\n",
      "Gradient Descent(4/49): loss=4964066.098853553\n",
      "Gradient Descent(5/49): loss=584821929.3599405\n",
      "Gradient Descent(6/49): loss=68971972839.77644\n",
      "Gradient Descent(7/49): loss=8136985293280.289\n",
      "Gradient Descent(8/49): loss=960060779906669.4\n",
      "Gradient Descent(9/49): loss=1.1327860218254533e+17\n",
      "Gradient Descent(10/49): loss=1.33659996292995e+19\n",
      "Gradient Descent(11/49): loss=1.5770897317089682e+21\n",
      "Gradient Descent(12/49): loss=1.8608518800042127e+23\n",
      "Gradient Descent(13/49): loss=2.1956714107091537e+25\n",
      "Gradient Descent(14/49): loss=2.5907346458859264e+27\n",
      "Gradient Descent(15/49): loss=3.056881002296065e+29\n",
      "Gradient Descent(16/49): loss=3.606900294821478e+31\n",
      "Gradient Descent(17/49): loss=4.255883617744543e+33\n",
      "Gradient Descent(18/49): loss=5.021637391098241e+35\n",
      "Gradient Descent(19/49): loss=5.925171917475538e+37\n",
      "Gradient Descent(20/49): loss=6.991277848437491e+39\n",
      "Gradient Descent(21/49): loss=8.249206375176153e+41\n",
      "Gradient Descent(22/49): loss=9.733471805324027e+43\n",
      "Gradient Descent(23/49): loss=1.1484798546251235e+46\n",
      "Gradient Descent(24/49): loss=1.355123847751477e+48\n",
      "Gradient Descent(25/49): loss=1.5989489370231803e+50\n",
      "Gradient Descent(26/49): loss=1.8866450527381754e+52\n",
      "Gradient Descent(27/49): loss=2.2261058327780218e+54\n",
      "Gradient Descent(28/49): loss=2.626645203630741e+56\n",
      "Gradient Descent(29/49): loss=3.099252930462257e+58\n",
      "Gradient Descent(30/49): loss=3.656896147870161e+60\n",
      "Gradient Descent(31/49): loss=4.314875144544284e+62\n",
      "Gradient Descent(32/49): loss=5.091243163645631e+64\n",
      "Gradient Descent(33/49): loss=6.007301737141217e+66\n",
      "Gradient Descent(34/49): loss=7.08818514478867e+68\n",
      "Gradient Descent(35/49): loss=8.363550033814767e+70\n",
      "Gradient Descent(36/49): loss=9.868389120669472e+72\n",
      "Gradient Descent(37/49): loss=1.1643991300728549e+75\n",
      "Gradient Descent(38/49): loss=1.373907450887421e+77\n",
      "Gradient Descent(39/49): loss=1.6211122413719823e+79\n",
      "Gradient Descent(40/49): loss=1.9127961620912793e+81\n",
      "Gradient Descent(41/49): loss=2.256962265990069e+83\n",
      "Gradient Descent(42/49): loss=2.663053581482419e+85\n",
      "Gradient Descent(43/49): loss=3.1422122047465276e+87\n",
      "Gradient Descent(44/49): loss=3.7075850100476626e+89\n",
      "Gradient Descent(45/49): loss=4.374684365990793e+91\n",
      "Gradient Descent(46/49): loss=5.161813754824298e+93\n",
      "Gradient Descent(47/49): loss=6.090569972688469e+95\n",
      "Gradient Descent(48/49): loss=7.186435689886127e+97\n",
      "Gradient Descent(49/49): loss=8.479478629497134e+99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7339591935660126\n",
      "Gradient Descent(2/49): loss=362.949332194929\n",
      "Gradient Descent(3/49): loss=41994.94308285394\n",
      "Gradient Descent(4/49): loss=4935654.893657118\n",
      "Gradient Descent(5/49): loss=582478037.6171715\n",
      "Gradient Descent(6/49): loss=68826070598.48097\n",
      "Gradient Descent(7/49): loss=8135703001336.122\n",
      "Gradient Descent(8/49): loss=961813622407555.1\n",
      "Gradient Descent(9/49): loss=1.1371139859305277e+17\n",
      "Gradient Descent(10/49): loss=1.3443818108938258e+19\n",
      "Gradient Descent(11/49): loss=1.5894359933119874e+21\n",
      "Gradient Descent(12/49): loss=1.8791611969161085e+23\n",
      "Gradient Descent(13/49): loss=2.221698975108746e+25\n",
      "Gradient Descent(14/49): loss=2.626675681995121e+27\n",
      "Gradient Descent(15/49): loss=3.105472669108686e+29\n",
      "Gradient Descent(16/49): loss=3.6715460167039596e+31\n",
      "Gradient Descent(17/49): loss=4.34080465414423e+33\n",
      "Gradient Descent(18/49): loss=5.132057446087439e+35\n",
      "Gradient Descent(19/49): loss=6.067541790329588e+37\n",
      "Gradient Descent(20/49): loss=7.173548575768001e+39\n",
      "Gradient Descent(21/49): loss=8.481161061553718e+41\n",
      "Gradient Descent(22/49): loss=1.0027128441865032e+44\n",
      "Gradient Descent(23/49): loss=1.1854898646597113e+46\n",
      "Gradient Descent(24/49): loss=1.4015839403687496e+48\n",
      "Gradient Descent(25/49): loss=1.6570681879802892e+50\n",
      "Gradient Descent(26/49): loss=1.959122747150157e+52\n",
      "Gradient Descent(27/49): loss=2.3162365714590856e+54\n",
      "Gradient Descent(28/49): loss=2.7384460023083305e+56\n",
      "Gradient Descent(29/49): loss=3.237616830665354e+58\n",
      "Gradient Descent(30/49): loss=3.827777773734361e+60\n",
      "Gradient Descent(31/49): loss=4.525514738593606e+62\n",
      "Gradient Descent(32/49): loss=5.350436953200525e+64\n",
      "Gradient Descent(33/49): loss=6.325728064929551e+66\n",
      "Gradient Descent(34/49): loss=7.478797694738057e+68\n",
      "Gradient Descent(35/49): loss=8.842051758265324e+70\n",
      "Gradient Descent(36/49): loss=1.045380320300019e+73\n",
      "Gradient Descent(37/49): loss=1.2359348756910791e+75\n",
      "Gradient Descent(38/49): loss=1.4612241949524345e+77\n",
      "Gradient Descent(39/49): loss=1.7275798182493256e+79\n",
      "Gradient Descent(40/49): loss=2.042487414821064e+81\n",
      "Gradient Descent(41/49): loss=2.414797160532907e+83\n",
      "Gradient Descent(42/49): loss=2.854972463577523e+85\n",
      "Gradient Descent(43/49): loss=3.375384028523176e+87\n",
      "Gradient Descent(44/49): loss=3.990657523096631e+89\n",
      "Gradient Descent(45/49): loss=4.718084618542074e+91\n",
      "Gradient Descent(46/49): loss=5.578108955451014e+93\n",
      "Gradient Descent(47/49): loss=6.594900692666613e+95\n",
      "Gradient Descent(48/49): loss=7.79703578640801e+97\n",
      "Gradient Descent(49/49): loss=9.21829909010286e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.746918318986655\n",
      "Gradient Descent(2/49): loss=361.58441262664934\n",
      "Gradient Descent(3/49): loss=41468.62046846499\n",
      "Gradient Descent(4/49): loss=4828350.573880459\n",
      "Gradient Descent(5/49): loss=564388156.1066275\n",
      "Gradient Descent(6/49): loss=66049057494.9977\n",
      "Gradient Descent(7/49): loss=7732413898330.089\n",
      "Gradient Descent(8/49): loss=905346148483573.2\n",
      "Gradient Descent(9/49): loss=1.060060668824046e+17\n",
      "Gradient Descent(10/49): loss=1.241229624351918e+19\n",
      "Gradient Descent(11/49): loss=1.4533669506757582e+21\n",
      "Gradient Descent(12/49): loss=1.7017626693487467e+23\n",
      "Gradient Descent(13/49): loss=1.9926126744760873e+25\n",
      "Gradient Descent(14/49): loss=2.3331724732105733e+27\n",
      "Gradient Descent(15/49): loss=2.7319378736857696e+29\n",
      "Gradient Descent(16/49): loss=3.1988568096970093e+31\n",
      "Gradient Descent(17/49): loss=3.745577465673475e+33\n",
      "Gradient Descent(18/49): loss=4.3857388459497714e+35\n",
      "Gradient Descent(19/49): loss=5.135311019665758e+37\n",
      "Gradient Descent(20/49): loss=6.012993522708306e+39\n",
      "Gradient Descent(21/49): loss=7.040681853072267e+41\n",
      "Gradient Descent(22/49): loss=8.244013696310419e+43\n",
      "Gradient Descent(23/49): loss=9.653008507361679e+45\n",
      "Gradient Descent(24/49): loss=1.1302816404231508e+48\n",
      "Gradient Descent(25/49): loss=1.3234595056081333e+50\n",
      "Gradient Descent(26/49): loss=1.5496536441387543e+52\n",
      "Gradient Descent(27/49): loss=1.8145069090642153e+54\n",
      "Gradient Descent(28/49): loss=2.124626580587735e+56\n",
      "Gradient Descent(29/49): loss=2.4877491975315542e+58\n",
      "Gradient Descent(30/49): loss=2.9129335603562275e+60\n",
      "Gradient Descent(31/49): loss=3.4107867205701098e+62\n",
      "Gradient Descent(32/49): loss=3.9937285942748294e+64\n",
      "Gradient Descent(33/49): loss=4.6763018011464456e+66\n",
      "Gradient Descent(34/49): loss=5.475534458389056e+68\n",
      "Gradient Descent(35/49): loss=6.411364980261915e+70\n",
      "Gradient Descent(36/49): loss=7.50713948063114e+72\n",
      "Gradient Descent(37/49): loss=8.790194187221063e+74\n",
      "Gradient Descent(38/49): loss=1.0292537397021741e+77\n",
      "Gradient Descent(39/49): loss=1.2051647985558562e+79\n",
      "Gradient Descent(40/49): loss=1.411141039039079e+81\n",
      "Gradient Descent(41/49): loss=1.652320939382295e+83\n",
      "Gradient Descent(42/49): loss=1.9347212016314832e+85\n",
      "Gradient Descent(43/49): loss=2.265386849991577e+87\n",
      "Gradient Descent(44/49): loss=2.6525669826676504e+89\n",
      "Gradient Descent(45/49): loss=3.105920561675687e+91\n",
      "Gradient Descent(46/49): loss=3.6367573744502433e+93\n",
      "Gradient Descent(47/49): loss=4.258320178505338e+95\n",
      "Gradient Descent(48/49): loss=4.9861150677963415e+97\n",
      "Gradient Descent(49/49): loss=5.838298302414611e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7476338731481116\n",
      "Gradient Descent(2/49): loss=362.8245393692726\n",
      "Gradient Descent(3/49): loss=41694.278229933625\n",
      "Gradient Descent(4/49): loss=4864014.826598093\n",
      "Gradient Descent(5/49): loss=569715339.0957074\n",
      "Gradient Descent(6/49): loss=66812779034.99879\n",
      "Gradient Descent(7/49): loss=7838529022554.17\n",
      "Gradient Descent(8/49): loss=919743012247016.8\n",
      "Gradient Descent(9/49): loss=1.0792380401466046e+17\n",
      "Gradient Descent(10/49): loss=1.2664097383321477e+19\n",
      "Gradient Descent(11/49): loss=1.4860496387264055e+21\n",
      "Gradient Descent(12/49): loss=1.7437855839915924e+23\n",
      "Gradient Descent(13/49): loss=2.046223554958649e+25\n",
      "Gradient Descent(14/49): loss=2.401116062703333e+27\n",
      "Gradient Descent(15/49): loss=2.8175605110488966e+29\n",
      "Gradient Descent(16/49): loss=3.3062322608978395e+31\n",
      "Gradient Descent(17/49): loss=3.879658233179009e+33\n",
      "Gradient Descent(18/49): loss=4.552538010385322e+35\n",
      "Gradient Descent(19/49): loss=5.342120647049118e+37\n",
      "Gradient Descent(20/49): loss=6.268646840374149e+39\n",
      "Gradient Descent(21/49): loss=7.355867792724221e+41\n",
      "Gradient Descent(22/49): loss=8.631654065592461e+43\n",
      "Gradient Descent(23/49): loss=1.0128710032338186e+46\n",
      "Gradient Descent(24/49): loss=1.1885412244269496e+48\n",
      "Gradient Descent(25/49): loss=1.3946793201242442e+50\n",
      "Gradient Descent(26/49): loss=1.636569574539322e+52\n",
      "Gradient Descent(27/49): loss=1.9204127670506372e+54\n",
      "Gradient Descent(28/49): loss=2.2534851271992807e+56\n",
      "Gradient Descent(29/49): loss=2.6443248585081374e+58\n",
      "Gradient Descent(30/49): loss=3.1029510126009298e+60\n",
      "Gradient Descent(31/49): loss=3.6411203243890544e+62\n",
      "Gradient Descent(32/49): loss=4.2726285922142854e+64\n",
      "Gradient Descent(33/49): loss=5.0136643287310756e+66\n",
      "Gradient Descent(34/49): loss=5.88322374825549e+68\n",
      "Gradient Descent(35/49): loss=6.903597728649154e+70\n",
      "Gradient Descent(36/49): loss=8.100943230850605e+72\n",
      "Gradient Descent(37/49): loss=9.50595382421057e+74\n",
      "Gradient Descent(38/49): loss=1.1154646506334718e+77\n",
      "Gradient Descent(39/49): loss=1.3089284987308329e+79\n",
      "Gradient Descent(40/49): loss=1.535946310639948e+81\n",
      "Gradient Descent(41/49): loss=1.802337615428135e+83\n",
      "Gradient Descent(42/49): loss=2.114931269071339e+85\n",
      "Gradient Descent(43/49): loss=2.481740509994958e+87\n",
      "Gradient Descent(44/49): loss=2.912168375880339e+89\n",
      "Gradient Descent(45/49): loss=3.417248747531102e+91\n",
      "Gradient Descent(46/49): loss=4.0099291988818613e+93\n",
      "Gradient Descent(47/49): loss=4.7054029039186757e+95\n",
      "Gradient Descent(48/49): loss=5.521498108839457e+97\n",
      "Gradient Descent(49/49): loss=6.479135153448421e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7939642511753564\n",
      "Gradient Descent(2/49): loss=375.31629651599945\n",
      "Gradient Descent(3/49): loss=44007.91522790406\n",
      "Gradient Descent(4/49): loss=5234796.311775967\n",
      "Gradient Descent(5/49): loss=624954463.3813298\n",
      "Gradient Descent(6/49): loss=74689786243.73642\n",
      "Gradient Descent(7/49): loss=8929280441894.73\n",
      "Gradient Descent(8/49): loss=1067618824055050.8\n",
      "Gradient Descent(9/49): loss=1.2765270196663587e+17\n",
      "Gradient Descent(10/49): loss=1.5263292907126102e+19\n",
      "Gradient Descent(11/49): loss=1.8250210208324164e+21\n",
      "Gradient Descent(12/49): loss=2.1821668260806987e+23\n",
      "Gradient Descent(13/49): loss=2.6092047990740524e+25\n",
      "Gradient Descent(14/49): loss=3.11981206223464e+27\n",
      "Gradient Descent(15/49): loss=3.730342526400422e+29\n",
      "Gradient Descent(16/49): loss=4.460350569536189e+31\n",
      "Gradient Descent(17/49): loss=5.333217292464527e+33\n",
      "Gradient Descent(18/49): loss=6.3768993658442904e+35\n",
      "Gradient Descent(19/49): loss=7.624824436277323e+37\n",
      "Gradient Descent(20/49): loss=9.116961763823392e+39\n",
      "Gradient Descent(21/49): loss=1.0901102379527621e+42\n",
      "Gradient Descent(22/49): loss=1.3034389763675132e+44\n",
      "Gradient Descent(23/49): loss=1.558515006987121e+46\n",
      "Gradient Descent(24/49): loss=1.8635080514285462e+48\n",
      "Gradient Descent(25/49): loss=2.228186602099792e+50\n",
      "Gradient Descent(26/49): loss=2.664230793084877e+52\n",
      "Gradient Descent(27/49): loss=3.1856064981868324e+54\n",
      "Gradient Descent(28/49): loss=3.809012637955418e+56\n",
      "Gradient Descent(29/49): loss=4.554416022306002e+58\n",
      "Gradient Descent(30/49): loss=5.4456908589758995e+60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=6.5113834103627715e+62\n",
      "Gradient Descent(32/49): loss=7.785626289612945e+64\n",
      "Gradient Descent(33/49): loss=9.30923167956027e+66\n",
      "Gradient Descent(34/49): loss=1.1130998488759525e+69\n",
      "Gradient Descent(35/49): loss=1.3309275310958784e+71\n",
      "Gradient Descent(36/49): loss=1.5913829247374034e+73\n",
      "Gradient Descent(37/49): loss=1.902808044747959e+75\n",
      "Gradient Descent(38/49): loss=2.275177393747009e+77\n",
      "Gradient Descent(39/49): loss=2.720417431125101e+79\n",
      "Gradient Descent(40/49): loss=3.2527885605355297e+81\n",
      "Gradient Descent(41/49): loss=3.889341870293373e+83\n",
      "Gradient Descent(42/49): loss=4.65046525542586e+85\n",
      "Gradient Descent(43/49): loss=5.560536412884631e+87\n",
      "Gradient Descent(44/49): loss=6.648703624425752e+89\n",
      "Gradient Descent(45/49): loss=7.949819334519888e+91\n",
      "Gradient Descent(46/49): loss=9.505556424462344e+93\n",
      "Gradient Descent(47/49): loss=1.1365742935351103e+96\n",
      "Gradient Descent(48/49): loss=1.3589957989207428e+98\n",
      "Gradient Descent(49/49): loss=1.6249440023316561e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.779510142106329\n",
      "Gradient Descent(2/49): loss=372.724913902164\n",
      "Gradient Descent(3/49): loss=43701.8442870647\n",
      "Gradient Descent(4/49): loss=5204819.030632622\n",
      "Gradient Descent(5/49): loss=622446615.3077909\n",
      "Gradient Descent(6/49): loss=74531309381.69264\n",
      "Gradient Descent(7/49): loss=8927804722823.768\n",
      "Gradient Descent(8/49): loss=1069558576924738.2\n",
      "Gradient Descent(9/49): loss=1.28139137285594e+17\n",
      "Gradient Descent(10/49): loss=1.5351987193202088e+19\n",
      "Gradient Descent(11/49): loss=1.8392857417870077e+21\n",
      "Gradient Descent(12/49): loss=2.2036082534555304e+23\n",
      "Gradient Descent(13/49): loss=2.6400962738954392e+25\n",
      "Gradient Descent(14/49): loss=3.163043747598473e+27\n",
      "Gradient Descent(15/49): loss=3.789576273815845e+29\n",
      "Gradient Descent(16/49): loss=4.540211799153456e+31\n",
      "Gradient Descent(17/49): loss=5.439532493510956e+33\n",
      "Gradient Descent(18/49): loss=6.516989757700123e+35\n",
      "Gradient Descent(19/49): loss=7.807868704424801e+37\n",
      "Gradient Descent(20/49): loss=9.354443689765826e+39\n",
      "Gradient Descent(21/49): loss=1.1207362733940714e+42\n",
      "Gradient Descent(22/49): loss=1.3427306167828423e+44\n",
      "Gradient Descent(23/49): loss=1.608697382298168e+46\n",
      "Gradient Descent(24/49): loss=1.9273465842448842e+48\n",
      "Gradient Descent(25/49): loss=2.309113508033279e+50\n",
      "Gradient Descent(26/49): loss=2.766500450188483e+52\n",
      "Gradient Descent(27/49): loss=3.314486149887109e+54\n",
      "Gradient Descent(28/49): loss=3.971016320292009e+56\n",
      "Gradient Descent(29/49): loss=4.7575913438536127e+58\n",
      "Gradient Descent(30/49): loss=5.699970377720927e+60\n",
      "Gradient Descent(31/49): loss=6.829014927662957e+62\n",
      "Gradient Descent(32/49): loss=8.18169951628586e+64\n",
      "Gradient Descent(33/49): loss=9.802322543421523e+66\n",
      "Gradient Descent(34/49): loss=1.1743956992554976e+69\n",
      "Gradient Descent(35/49): loss=1.407018849176121e+71\n",
      "Gradient Descent(36/49): loss=1.6857197648049332e+73\n",
      "Gradient Descent(37/49): loss=2.019625484845444e+75\n",
      "Gradient Descent(38/49): loss=2.4196709228886507e+77\n",
      "Gradient Descent(39/49): loss=2.8989569695001483e+79\n",
      "Gradient Descent(40/49): loss=3.4731795268179127e+81\n",
      "Gradient Descent(41/49): loss=4.1611435259030747e+83\n",
      "Gradient Descent(42/49): loss=4.9853787601440414e+85\n",
      "Gradient Descent(43/49): loss=5.972877702338164e+87\n",
      "Gradient Descent(44/49): loss=7.155979467858439e+89\n",
      "Gradient Descent(45/49): loss=8.573428872378497e+91\n",
      "Gradient Descent(46/49): loss=1.0271645266714325e+94\n",
      "Gradient Descent(47/49): loss=1.2306242701229038e+96\n",
      "Gradient Descent(48/49): loss=1.47438512029141e+98\n",
      "Gradient Descent(49/49): loss=1.766429880925091e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7926982899468897\n",
      "Gradient Descent(2/49): loss=371.3267189147445\n",
      "Gradient Descent(3/49): loss=43154.786669394576\n",
      "Gradient Descent(4/49): loss=5091770.571175041\n",
      "Gradient Descent(5/49): loss=603131608.9156088\n",
      "Gradient Descent(6/49): loss=71526411408.96568\n",
      "Gradient Descent(7/49): loss=8485571306194.093\n",
      "Gradient Descent(8/49): loss=1006808926531774.6\n",
      "Gradient Descent(9/49): loss=1.1946195281209058e+17\n",
      "Gradient Descent(10/49): loss=1.4174819656425654e+19\n",
      "Gradient Descent(11/49): loss=1.681927313104853e+21\n",
      "Gradient Descent(12/49): loss=1.995710182003525e+23\n",
      "Gradient Descent(13/49): loss=2.3680338602688933e+25\n",
      "Gradient Descent(14/49): loss=2.8098193774227704e+27\n",
      "Gradient Descent(15/49): loss=3.334025506383119e+29\n",
      "Gradient Descent(16/49): loss=3.9560287504003946e+31\n",
      "Gradient Descent(17/49): loss=4.694074331124309e+33\n",
      "Gradient Descent(18/49): loss=5.5698113564471036e+35\n",
      "Gradient Descent(19/49): loss=6.608927846843582e+37\n",
      "Gradient Descent(20/49): loss=7.841904239975253e+39\n",
      "Gradient Descent(21/49): loss=9.30490747324653e+41\n",
      "Gradient Descent(22/49): loss=1.1040851869349593e+44\n",
      "Gradient Descent(23/49): loss=1.3100657943359576e+46\n",
      "Gradient Descent(24/49): loss=1.554474605584381e+48\n",
      "Gradient Descent(25/49): loss=1.8444808725297589e+50\n",
      "Gradient Descent(26/49): loss=2.1885913587181607e+52\n",
      "Gradient Descent(27/49): loss=2.5968998685720013e+54\n",
      "Gradient Descent(28/49): loss=3.0813833292932784e+56\n",
      "Gradient Descent(29/49): loss=3.6562531104703853e+58\n",
      "Gradient Descent(30/49): loss=4.338371886658616e+60\n",
      "Gradient Descent(31/49): loss=5.1477482707778025e+62\n",
      "Gradient Descent(32/49): loss=6.108123727425618e+64\n",
      "Gradient Descent(33/49): loss=7.247668982054305e+66\n",
      "Gradient Descent(34/49): loss=8.599810353804206e+68\n",
      "Gradient Descent(35/49): loss=1.0204210250843316e+71\n",
      "Gradient Descent(36/49): loss=1.2107930589115269e+73\n",
      "Gradient Descent(37/49): loss=1.4366813261096527e+75\n",
      "Gradient Descent(38/49): loss=1.7047118147899905e+77\n",
      "Gradient Descent(39/49): loss=2.0227466722587516e+79\n",
      "Gradient Descent(40/49): loss=2.4001148256474604e+81\n",
      "Gradient Descent(41/49): loss=2.847885627645145e+83\n",
      "Gradient Descent(42/49): loss=3.3791935541916805e+85\n",
      "Gradient Descent(43/49): loss=4.0096234785006575e+87\n",
      "Gradient Descent(44/49): loss=4.757667822667673e+89\n",
      "Gradient Descent(45/49): loss=5.6452689965072824e+91\n",
      "Gradient Descent(46/49): loss=6.698463034995363e+93\n",
      "Gradient Descent(47/49): loss=7.948143314155598e+95\n",
      "Gradient Descent(48/49): loss=9.43096674749358e+97\n",
      "Gradient Descent(49/49): loss=1.1190429044469085e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7930936294918967\n",
      "Gradient Descent(2/49): loss=372.5392172168243\n",
      "Gradient Descent(3/49): loss=43379.0356957173\n",
      "Gradient Descent(4/49): loss=5127712.510357253\n",
      "Gradient Descent(5/49): loss=608576876.8127233\n",
      "Gradient Descent(6/49): loss=72318145306.46156\n",
      "Gradient Descent(7/49): loss=8597122455942.672\n",
      "Gradient Descent(8/49): loss=1022153392339521.2\n",
      "Gradient Descent(9/49): loss=1.2153404427839086e+17\n",
      "Gradient Descent(10/49): loss=1.4450607695970882e+19\n",
      "Gradient Descent(11/49): loss=1.7182104799659945e+21\n",
      "Gradient Descent(12/49): loss=2.042995042904939e+23\n",
      "Gradient Descent(13/49): loss=2.429173273178792e+25\n",
      "Gradient Descent(14/49): loss=2.888349564113528e+27\n",
      "Gradient Descent(15/49): loss=3.434322199855292e+29\n",
      "Gradient Descent(16/49): loss=4.083497839759862e+31\n",
      "Gradient Descent(17/49): loss=4.8553844825150066e+33\n",
      "Gradient Descent(18/49): loss=5.773177665293565e+35\n",
      "Gradient Descent(19/49): loss=6.864457490325923e+37\n",
      "Gradient Descent(20/49): loss=8.162017415685188e+39\n",
      "Gradient Descent(21/49): loss=9.704849712193723e+41\n",
      "Gradient Descent(22/49): loss=1.1539317198443776e+44\n",
      "Gradient Descent(23/49): loss=1.3720546464536755e+46\n",
      "Gradient Descent(24/49): loss=1.63140844513284e+48\n",
      "Gradient Descent(25/49): loss=1.9397868166056808e+50\n",
      "Gradient Descent(26/49): loss=2.3064566725177468e+52\n",
      "Gradient Descent(27/49): loss=2.7424366103853272e+54\n",
      "Gradient Descent(28/49): loss=3.260828027509458e+56\n",
      "Gradient Descent(29/49): loss=3.8772088239798583e+58\n",
      "Gradient Descent(30/49): loss=4.6101015257247205e+60\n",
      "Gradient Descent(31/49): loss=5.481529894918996e+62\n",
      "Gradient Descent(32/49): loss=6.517680754149405e+64\n",
      "Gradient Descent(33/49): loss=7.749690912456004e+66\n",
      "Gradient Descent(34/49): loss=9.2145828407395e+68\n",
      "Gradient Descent(35/49): loss=1.0956377214009664e+71\n",
      "Gradient Descent(36/49): loss=1.3027415752880228e+73\n",
      "Gradient Descent(37/49): loss=1.5489934116304813e+75\n",
      "Gradient Descent(38/49): loss=1.8417932111701848e+77\n",
      "Gradient Descent(39/49): loss=2.1899397423142844e+79\n",
      "Gradient Descent(40/49): loss=2.6038949681655636e+81\n",
      "Gradient Descent(41/49): loss=3.096098433317032e+83\n",
      "Gradient Descent(42/49): loss=3.681341077878181e+85\n",
      "Gradient Descent(43/49): loss=4.377209712016141e+87\n",
      "Gradient Descent(44/49): loss=5.204615507675696e+89\n",
      "Gradient Descent(45/49): loss=6.188422388897119e+91\n",
      "Gradient Descent(46/49): loss=7.358194204148905e+93\n",
      "Gradient Descent(47/49): loss=8.749083133548017e+95\n",
      "Gradient Descent(48/49): loss=1.0402886028011282e+98\n",
      "Gradient Descent(49/49): loss=1.2369300423815425e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.840059941315829\n",
      "Gradient Descent(2/49): loss=385.35695427903903\n",
      "Gradient Descent(3/49): loss=45784.63682777837\n",
      "Gradient Descent(4/49): loss=5518359.081755268\n",
      "Gradient Descent(5/49): loss=667548848.8942358\n",
      "Gradient Descent(6/49): loss=80839149895.6292\n",
      "Gradient Descent(7/49): loss=9792720157995.719\n",
      "Gradient Descent(8/49): loss=1186395961248871.2\n",
      "Gradient Descent(9/49): loss=1.4373749709251472e+17\n",
      "Gradient Descent(10/49): loss=1.7414658459122665e+19\n",
      "Gradient Descent(11/49): loss=2.1098970263521106e+21\n",
      "Gradient Descent(12/49): loss=2.5562775608403765e+23\n",
      "Gradient Descent(13/49): loss=3.097097645345455e+25\n",
      "Gradient Descent(14/49): loss=3.752336980401406e+27\n",
      "Gradient Descent(15/49): loss=4.546202571633718e+29\n",
      "Gradient Descent(16/49): loss=5.508022906018449e+31\n",
      "Gradient Descent(17/49): loss=6.673331413681384e+33\n",
      "Gradient Descent(18/49): loss=8.085179194740019e+35\n",
      "Gradient Descent(19/49): loss=9.79572549011514e+37\n",
      "Gradient Descent(20/49): loss=1.1868164647523758e+40\n",
      "Gradient Descent(21/49): loss=1.4379060769682354e+42\n",
      "Gradient Descent(22/49): loss=1.7421176294817805e+44\n",
      "Gradient Descent(23/49): loss=2.11068990080401e+46\n",
      "Gradient Descent(24/49): loss=2.5572394090782912e+48\n",
      "Gradient Descent(25/49): loss=3.0982634601389254e+50\n",
      "Gradient Descent(26/49): loss=3.7537496232677297e+52\n",
      "Gradient Descent(27/49): loss=4.547914151093957e+54\n",
      "Gradient Descent(28/49): loss=5.510096623790137e+56\n",
      "Gradient Descent(29/49): loss=6.675843869260436e+58\n",
      "Gradient Descent(30/49): loss=8.088223203622642e+60\n",
      "Gradient Descent(31/49): loss=9.799413508283152e+62\n",
      "Gradient Descent(32/49): loss=1.1872632924288265e+65\n",
      "Gradient Descent(33/49): loss=1.4384474380609067e+67\n",
      "Gradient Descent(34/49): loss=1.742773524001641e+69\n",
      "Gradient Descent(35/49): loss=2.1114845600861727e+71\n",
      "Gradient Descent(36/49): loss=2.558202190979627e+73\n",
      "Gradient Descent(37/49): loss=3.0994299336320397e+75\n",
      "Gradient Descent(38/49): loss=3.7551628824990115e+77\n",
      "Gradient Descent(39/49): loss=4.5496264074516227e+79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=5.512171134799367e+81\n",
      "Gradient Descent(41/49): loss=6.678357275566774e+83\n",
      "Gradient Descent(42/49): loss=8.091268360400624e+85\n",
      "Gradient Descent(43/49): loss=9.803102915673856e+87\n",
      "Gradient Descent(44/49): loss=1.1877102883599622e+90\n",
      "Gradient Descent(45/49): loss=1.438989002982571e+92\n",
      "Gradient Descent(46/49): loss=1.7434296654650283e+94\n",
      "Gradient Descent(47/49): loss=2.1122795185532842e+96\n",
      "Gradient Descent(48/49): loss=2.559165335361904e+98\n",
      "Gradient Descent(49/49): loss=3.10059684629413e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8253742019819774\n",
      "Gradient Descent(2/49): loss=382.6969075190091\n",
      "Gradient Descent(3/49): loss=45466.175937489905\n",
      "Gradient Descent(4/49): loss=5486740.729722188\n",
      "Gradient Descent(5/49): loss=664866798.6557171\n",
      "Gradient Descent(6/49): loss=80667113501.89954\n",
      "Gradient Descent(7/49): loss=9791027532494.146\n",
      "Gradient Descent(8/49): loss=1188541140414049.5\n",
      "Gradient Descent(9/49): loss=1.442838038285052e+17\n",
      "Gradient Descent(10/49): loss=1.7515662295752133e+19\n",
      "Gradient Descent(11/49): loss=2.1263627546651087e+21\n",
      "Gradient Descent(12/49): loss=2.581360948084364e+23\n",
      "Gradient Descent(13/49): loss=3.133720826344457e+25\n",
      "Gradient Descent(14/49): loss=3.804275263426613e+27\n",
      "Gradient Descent(15/49): loss=4.618315332091038e+29\n",
      "Gradient Descent(16/49): loss=5.6065440441476435e+31\n",
      "Gradient Descent(17/49): loss=6.80623431016812e+33\n",
      "Gradient Descent(18/49): loss=8.26263473386412e+35\n",
      "Gradient Descent(19/49): loss=1.003067624004373e+38\n",
      "Gradient Descent(20/49): loss=1.2177043895740375e+40\n",
      "Gradient Descent(21/49): loss=1.4782692063624663e+42\n",
      "Gradient Descent(22/49): loss=1.7945897750114892e+44\n",
      "Gradient Descent(23/49): loss=2.1785967310500712e+46\n",
      "Gradient Descent(24/49): loss=2.6447736316339776e+48\n",
      "Gradient Descent(25/49): loss=3.2107032306152792e+50\n",
      "Gradient Descent(26/49): loss=3.8977306457477484e+52\n",
      "Gradient Descent(27/49): loss=4.7317684306471056e+54\n",
      "Gradient Descent(28/49): loss=5.7442739163351235e+56\n",
      "Gradient Descent(29/49): loss=6.973435684673974e+58\n",
      "Gradient Descent(30/49): loss=8.465613923806378e+60\n",
      "Gradient Descent(31/49): loss=1.0277088991363562e+63\n",
      "Gradient Descent(32/49): loss=1.2476184135847888e+65\n",
      "Gradient Descent(33/49): loss=1.514584243868949e+67\n",
      "Gradient Descent(34/49): loss=1.838675517127729e+69\n",
      "Gradient Descent(35/49): loss=2.232115955893578e+71\n",
      "Gradient Descent(36/49): loss=2.7097449191784765e+73\n",
      "Gradient Descent(37/49): loss=3.2895770972946907e+75\n",
      "Gradient Descent(38/49): loss=3.993481970371778e+77\n",
      "Gradient Descent(39/49): loss=4.848008657647761e+79\n",
      "Gradient Descent(40/49): loss=5.885387268304e+81\n",
      "Gradient Descent(41/49): loss=7.144744521706593e+83\n",
      "Gradient Descent(42/49): loss=8.673579486497711e+85\n",
      "Gradient Descent(43/49): loss=1.0529555098860872e+88\n",
      "Gradient Descent(44/49): loss=1.2782673030499315e+90\n",
      "Gradient Descent(45/49): loss=1.551791393563549e+92\n",
      "Gradient Descent(46/49): loss=1.883844265899861e+94\n",
      "Gradient Descent(47/49): loss=2.2869499295353935e+96\n",
      "Gradient Descent(48/49): loss=2.776312285932852e+98\n",
      "Gradient Descent(49/49): loss=3.3703885727780847e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.838793190465033\n",
      "Gradient Descent(2/49): loss=381.26483834696955\n",
      "Gradient Descent(3/49): loss=44897.70759818551\n",
      "Gradient Descent(4/49): loss=5367680.940588274\n",
      "Gradient Descent(5/49): loss=644252505.4199252\n",
      "Gradient Descent(6/49): loss=77417305878.07864\n",
      "Gradient Descent(7/49): loss=9306381660246.406\n",
      "Gradient Descent(8/49): loss=1118858597604744.0\n",
      "Gradient Descent(9/49): loss=1.3451980033506984e+17\n",
      "Gradient Descent(10/49): loss=1.6173448736953483e+19\n",
      "Gradient Descent(11/49): loss=1.9445576317714107e+21\n",
      "Gradient Descent(12/49): loss=2.3379734493632853e+23\n",
      "Gradient Descent(13/49): loss=2.810984930285723e+25\n",
      "Gradient Descent(14/49): loss=3.379695088691902e+27\n",
      "Gradient Descent(15/49): loss=4.063465189814135e+29\n",
      "Gradient Descent(16/49): loss=4.885573745779732e+31\n",
      "Gradient Descent(17/49): loss=5.874008961798093e+33\n",
      "Gradient Descent(18/49): loss=7.062421556416586e+35\n",
      "Gradient Descent(19/49): loss=8.491270372674583e+37\n",
      "Gradient Descent(20/49): loss=1.0209199773470027e+40\n",
      "Gradient Descent(21/49): loss=1.2274695710515693e+42\n",
      "Gradient Descent(22/49): loss=1.475807684554833e+44\n",
      "Gradient Descent(23/49): loss=1.7743888509902449e+46\n",
      "Gradient Descent(24/49): loss=2.133378100327566e+48\n",
      "Gradient Descent(25/49): loss=2.5649970221690953e+50\n",
      "Gradient Descent(26/49): loss=3.083939842978372e+52\n",
      "Gradient Descent(27/49): loss=3.707873682858049e+54\n",
      "Gradient Descent(28/49): loss=4.458040022841007e+56\n",
      "Gradient Descent(29/49): loss=5.35997785931404e+58\n",
      "Gradient Descent(30/49): loss=6.444393164964957e+60\n",
      "Gradient Descent(31/49): loss=7.748204256567983e+62\n",
      "Gradient Descent(32/49): loss=9.315798658573132e+64\n",
      "Gradient Descent(33/49): loss=1.120054425172222e+67\n",
      "Gradient Descent(34/49): loss=1.3466606152907486e+69\n",
      "Gradient Descent(35/49): loss=1.6191131180937076e+71\n",
      "Gradient Descent(36/49): loss=1.9466874277132786e+73\n",
      "Gradient Descent(37/49): loss=2.340535629578928e+75\n",
      "Gradient Descent(38/49): loss=2.814066066971736e+77\n",
      "Gradient Descent(39/49): loss=3.3833998206241646e+79\n",
      "Gradient Descent(40/49): loss=4.067919541959543e+81\n",
      "Gradient Descent(41/49): loss=4.890929324694342e+83\n",
      "Gradient Descent(42/49): loss=5.880448079765116e+85\n",
      "Gradient Descent(43/49): loss=7.070163423589127e+87\n",
      "Gradient Descent(44/49): loss=8.500578554254349e+89\n",
      "Gradient Descent(45/49): loss=1.0220391160402142e+92\n",
      "Gradient Descent(46/49): loss=1.228815130698942e+94\n",
      "Gradient Descent(47/49): loss=1.4774254739729906e+96\n",
      "Gradient Descent(48/49): loss=1.7763339469158136e+98\n",
      "Gradient Descent(49/49): loss=2.1357167224687947e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8388639536285654\n",
      "Gradient Descent(2/49): loss=382.44794043292814\n",
      "Gradient Descent(3/49): loss=45120.14874533886\n",
      "Gradient Descent(4/49): loss=5403835.663598449\n",
      "Gradient Descent(5/49): loss=649808107.7268301\n",
      "Gradient Descent(6/49): loss=78236535097.29323\n",
      "Gradient Descent(7/49): loss=9423422774601.004\n",
      "Gradient Descent(8/49): loss=1135180979452078.0\n",
      "Gradient Descent(9/49): loss=1.3675415863308379e+17\n",
      "Gradient Descent(10/49): loss=1.6474881472531163e+19\n",
      "Gradient Descent(11/49): loss=1.9847516008059825e+21\n",
      "Gradient Descent(12/49): loss=2.3910613791429605e+23\n",
      "Gradient Descent(13/49): loss=2.880550692667342e+25\n",
      "Gradient Descent(14/49): loss=3.4702470859772464e+27\n",
      "Gradient Descent(15/49): loss=4.180664339512553e+29\n",
      "Gradient Descent(16/49): loss=5.036515912473125e+31\n",
      "Gradient Descent(17/49): loss=6.067574587557251e+33\n",
      "Gradient Descent(18/49): loss=7.30970815880029e+35\n",
      "Gradient Descent(19/49): loss=8.806127165865392e+37\n",
      "Gradient Descent(20/49): loss=1.0608888069926915e+40\n",
      "Gradient Descent(21/49): loss=1.2780704158551086e+42\n",
      "Gradient Descent(22/49): loss=1.5397127174517937e+44\n",
      "Gradient Descent(23/49): loss=1.8549175561022174e+46\n",
      "Gradient Descent(24/49): loss=2.234650075269301e+48\n",
      "Gradient Descent(25/49): loss=2.692120165922339e+50\n",
      "Gradient Descent(26/49): loss=3.243242003737335e+52\n",
      "Gradient Descent(27/49): loss=3.907187661217783e+54\n",
      "Gradient Descent(28/49): loss=4.707054053438275e+56\n",
      "Gradient Descent(29/49): loss=5.670666418690586e+58\n",
      "Gradient Descent(30/49): loss=6.831546285001e+60\n",
      "Gradient Descent(31/49): loss=8.230077595516146e+62\n",
      "Gradient Descent(32/49): loss=9.914911559178163e+64\n",
      "Gradient Descent(33/49): loss=1.1944659097732329e+67\n",
      "Gradient Descent(34/49): loss=1.4389929764826434e+69\n",
      "Gradient Descent(35/49): loss=1.7335788065810088e+71\n",
      "Gradient Descent(36/49): loss=2.088471262710895e+73\n",
      "Gradient Descent(37/49): loss=2.516016115685854e+75\n",
      "Gradient Descent(38/49): loss=3.0310865212356143e+77\n",
      "Gradient Descent(39/49): loss=3.6516004177945174e+79\n",
      "Gradient Descent(40/49): loss=4.3991438442349374e+81\n",
      "Gradient Descent(41/49): loss=5.299721861122633e+83\n",
      "Gradient Descent(42/49): loss=6.384663198060467e+85\n",
      "Gradient Descent(43/49): loss=7.691710097411157e+87\n",
      "Gradient Descent(44/49): loss=9.266331267182488e+89\n",
      "Gradient Descent(45/49): loss=1.1163303617236411e+92\n",
      "Gradient Descent(46/49): loss=1.3448617803245744e+94\n",
      "Gradient Descent(47/49): loss=1.6201773867237558e+96\n",
      "Gradient Descent(48/49): loss=1.9518546833991338e+98\n",
      "Gradient Descent(49/49): loss=2.3514318471084116e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.886470478088064\n",
      "Gradient Descent(2/49): loss=395.5979924854218\n",
      "Gradient Descent(3/49): loss=47620.73948415728\n",
      "Gradient Descent(4/49): loss=5815272.292225754\n",
      "Gradient Descent(5/49): loss=712738420.9399889\n",
      "Gradient Descent(6/49): loss=87449473639.30835\n",
      "Gradient Descent(7/49): loss=10733161378529.451\n",
      "Gradient Descent(8/49): loss=1317476940299703.8\n",
      "Gradient Descent(9/49): loss=1.6172328772796362e+17\n",
      "Gradient Descent(10/49): loss=1.985210630623048e+19\n",
      "Gradient Descent(11/49): loss=2.436924351503339e+21\n",
      "Gradient Descent(12/49): loss=2.9914238776690278e+23\n",
      "Gradient Descent(13/49): loss=3.672095841504841e+25\n",
      "Gradient Descent(14/49): loss=4.5076491457006864e+27\n",
      "Gradient Descent(15/49): loss=5.533325483919772e+29\n",
      "Gradient Descent(16/49): loss=6.792385619665484e+31\n",
      "Gradient Descent(17/49): loss=8.337933977635744e+33\n",
      "Gradient Descent(18/49): loss=1.0235158452804168e+36\n",
      "Gradient Descent(19/49): loss=1.256407988724964e+38\n",
      "Gradient Descent(20/49): loss=1.5422927173833033e+40\n",
      "Gradient Descent(21/49): loss=1.893228033824529e+42\n",
      "Gradient Descent(22/49): loss=2.3240156344506523e+44\n",
      "Gradient Descent(23/49): loss=2.852825213177857e+46\n",
      "Gradient Descent(24/49): loss=3.5019608200166224e+48\n",
      "Gradient Descent(25/49): loss=4.2988015978990926e+50\n",
      "Gradient Descent(26/49): loss=5.27695657600589e+52\n",
      "Gradient Descent(27/49): loss=6.477682226288801e+54\n",
      "Gradient Descent(28/49): loss=7.951622572672014e+56\n",
      "Gradient Descent(29/49): loss=9.760945246993373e+58\n",
      "Gradient Descent(30/49): loss=1.1981963585928472e+61\n",
      "Gradient Descent(31/49): loss=1.4708355363302342e+63\n",
      "Gradient Descent(32/49): loss=1.8055113916991756e+65\n",
      "Gradient Descent(33/49): loss=2.2163398320446648e+67\n",
      "Gradient Descent(34/49): loss=2.720648716862932e+69\n",
      "Gradient Descent(35/49): loss=3.3397087096248093e+71\n",
      "Gradient Descent(36/49): loss=4.099630428585654e+73\n",
      "Gradient Descent(37/49): loss=5.032465736472416e+75\n",
      "Gradient Descent(38/49): loss=6.177559619076742e+77\n",
      "Gradient Descent(39/49): loss=7.583209672083685e+79\n",
      "Gradient Descent(40/49): loss=9.308703189719751e+81\n",
      "Gradient Descent(41/49): loss=1.1426817775234827e+84\n",
      "Gradient Descent(42/49): loss=1.4026890943587458e+86\n",
      "Gradient Descent(43/49): loss=1.7218588185567857e+88\n",
      "Gradient Descent(44/49): loss=2.1136528422195694e+90\n",
      "Gradient Descent(45/49): loss=2.5945961941104034e+92\n",
      "Gradient Descent(46/49): loss=3.184974029804688e+94\n",
      "Gradient Descent(47/49): loss=3.9096872158977177e+96\n",
      "Gradient Descent(48/49): loss=4.7993025949701303e+98\n",
      "Gradient Descent(49/49): loss=5.891342229226967e+100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.871551373192957\n",
      "Gradient Descent(2/49): loss=392.8679273588603\n",
      "Gradient Descent(3/49): loss=47289.47366417413\n",
      "Gradient Descent(4/49): loss=5781934.817930956\n",
      "Gradient Descent(5/49): loss=709871357.7259717\n",
      "Gradient Descent(6/49): loss=87262823261.32137\n",
      "Gradient Descent(7/49): loss=10731225935168.748\n",
      "Gradient Descent(8/49): loss=1319847757853462.2\n",
      "Gradient Descent(9/49): loss=1.623363750021916e+17\n",
      "Gradient Descent(10/49): loss=1.996703123583582e+19\n",
      "Gradient Descent(11/49): loss=2.455912989180462e+21\n",
      "Gradient Descent(12/49): loss=3.0207379031549087e+23\n",
      "Gradient Descent(13/49): loss=3.715466113843929e+25\n",
      "Gradient Descent(14/49): loss=4.5699729165733593e+27\n",
      "Gradient Descent(15/49): loss=5.6210049502768776e+29\n",
      "Gradient Descent(16/49): loss=6.913760255861213e+31\n",
      "Gradient Descent(17/49): loss=8.503831882079998e+33\n",
      "Gradient Descent(18/49): loss=1.0459598556359167e+36\n",
      "Gradient Descent(19/49): loss=1.2865165202266963e+38\n",
      "Gradient Descent(20/49): loss=1.5823979746139006e+40\n",
      "Gradient Descent(21/49): loss=1.946328174435604e+42\n",
      "Gradient Descent(22/49): loss=2.3939574136466238e+44\n",
      "Gradient Descent(23/49): loss=2.9445353428370865e+46\n",
      "Gradient Descent(24/49): loss=3.621738772714869e+48\n",
      "Gradient Descent(25/49): loss=4.4546898612381404e+50\n",
      "Gradient Descent(26/49): loss=5.479208470064843e+52\n",
      "Gradient Descent(27/49): loss=6.739352546102495e+54\n",
      "Gradient Descent(28/49): loss=8.289312770047165e+56\n",
      "Gradient Descent(29/49): loss=1.0195742948542625e+59\n",
      "Gradient Descent(30/49): loss=1.2540626365117296e+61\n",
      "Gradient Descent(31/49): loss=1.5424801353191672e+63\n",
      "Gradient Descent(32/49): loss=1.8972297703345167e+65\n",
      "Gradient Descent(33/49): loss=2.3335670385789136e+67\n",
      "Gradient Descent(34/49): loss=2.8702559957098924e+69\n",
      "Gradient Descent(35/49): loss=3.530376177204488e+71\n",
      "Gradient Descent(36/49): loss=4.34231510053527e+73\n",
      "Gradient Descent(37/49): loss=5.34098902946581e+75\n",
      "Gradient Descent(38/49): loss=6.569344497675409e+77\n",
      "Gradient Descent(39/49): loss=8.080205162573501e+79\n",
      "Gradient Descent(40/49): loss=9.938543410591764e+81\n",
      "Gradient Descent(41/49): loss=1.2224274401067136e+84\n",
      "Gradient Descent(42/49): loss=1.5035692702547368e+86\n",
      "Gradient Descent(43/49): loss=1.849369930911414e+88\n",
      "Gradient Descent(44/49): loss=2.27470008134699e+90\n",
      "Gradient Descent(45/49): loss=2.797850431973877e+92\n",
      "Gradient Descent(46/49): loss=3.4413183099993374e+94\n",
      "Gradient Descent(47/49): loss=4.232775124573666e+96\n",
      "Gradient Descent(48/49): loss=5.206256335878784e+98\n",
      "Gradient Descent(49/49): loss=6.403625101063652e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8852030205410832\n",
      "Gradient Descent(2/49): loss=391.4013782016582\n",
      "Gradient Descent(3/49): loss=46698.90139815219\n",
      "Gradient Descent(4/49): loss=5656585.929626919\n",
      "Gradient Descent(5/49): loss=687879652.3360611\n",
      "Gradient Descent(6/49): loss=83749932808.637\n",
      "Gradient Descent(7/49): loss=10200416314698.426\n",
      "Gradient Descent(8/49): loss=1242519128543900.5\n",
      "Gradient Descent(9/49): loss=1.5135787416324202e+17\n",
      "Gradient Descent(10/49): loss=1.8437940049788168e+19\n",
      "Gradient Descent(11/49): loss=2.2460610147371107e+21\n",
      "Gradient Descent(12/49): loss=2.736095667454995e+23\n",
      "Gradient Descent(13/49): loss=3.333045138198035e+25\n",
      "Gradient Descent(14/49): loss=4.0602350256006337e+27\n",
      "Gradient Descent(15/49): loss=4.946080397553074e+29\n",
      "Gradient Descent(16/49): loss=6.025195958371423e+31\n",
      "Gradient Descent(17/49): loss=7.339748567854589e+33\n",
      "Gradient Descent(18/49): loss=8.941104903056695e+35\n",
      "Gradient Descent(19/49): loss=1.0891838622261271e+38\n",
      "Gradient Descent(20/49): loss=1.326817545255969e+40\n",
      "Gradient Descent(21/49): loss=1.61629717401277e+42\n",
      "Gradient Descent(22/49): loss=1.9689342849809452e+44\n",
      "Gradient Descent(23/49): loss=2.39850831946426e+46\n",
      "Gradient Descent(24/49): loss=2.9218050609576137e+48\n",
      "Gradient Descent(25/49): loss=3.5592725465925033e+50\n",
      "Gradient Descent(26/49): loss=4.335820082664099e+52\n",
      "Gradient Descent(27/49): loss=5.281791586101582e+54\n",
      "Gradient Descent(28/49): loss=6.434151285602391e+56\n",
      "Gradient Descent(29/49): loss=7.837928114194114e+58\n",
      "Gradient Descent(30/49): loss=9.547975233461264e+60\n",
      "Gradient Descent(31/49): loss=1.1631113443576528e+63\n",
      "Gradient Descent(32/49): loss=1.4168742233771544e+65\n",
      "Gradient Descent(33/49): loss=1.7260020501126717e+67\n",
      "Gradient Descent(34/49): loss=2.1025741225586176e+69\n",
      "Gradient Descent(35/49): loss=2.5613051505729953e+71\n",
      "Gradient Descent(36/49): loss=3.120120239265832e+73\n",
      "Gradient Descent(37/49): loss=3.800855319913129e+75\n",
      "Gradient Descent(38/49): loss=4.63011039802469e+77\n",
      "Gradient Descent(39/49): loss=5.6402889595877065e+79\n",
      "Gradient Descent(40/49): loss=6.870864150716386e+81\n",
      "Gradient Descent(41/49): loss=8.369921207201874e+83\n",
      "Gradient Descent(42/49): loss=1.019603640503693e+86\n",
      "Gradient Descent(43/49): loss=1.2420565952686233e+88\n",
      "Gradient Descent(44/49): loss=1.5130434264516577e+90\n",
      "Gradient Descent(45/49): loss=1.8431530568326887e+92\n",
      "Gradient Descent(46/49): loss=2.245284657082669e+94\n",
      "Gradient Descent(47/49): loss=2.735151686205557e+96\n",
      "Gradient Descent(48/49): loss=3.3318959014636955e+98\n",
      "Gradient Descent(49/49): loss=4.058835330479103e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8849448455581204\n",
      "Gradient Descent(2/49): loss=392.5532767921238\n",
      "Gradient Descent(3/49): loss=46919.11555547455\n",
      "Gradient Descent(4/49): loss=5692882.802399146\n",
      "Gradient Descent(5/49): loss=693536616.6423267\n",
      "Gradient Descent(6/49): loss=84595927968.22972\n",
      "Gradient Descent(7/49): loss=10322970471112.863\n",
      "Gradient Descent(8/49): loss=1259846128052720.8\n",
      "Gradient Descent(9/49): loss=1.5376214322201344e+17\n",
      "Gradient Descent(10/49): loss=1.8766690771166347e+19\n",
      "Gradient Descent(11/49): loss=2.2904883689200676e+21\n",
      "Gradient Descent(12/49): loss=2.7955623979349815e+23\n",
      "Gradient Descent(13/49): loss=3.4120118093926735e+25\n",
      "Gradient Descent(14/49): loss=4.164395223428853e+27\n",
      "Gradient Descent(15/49): loss=5.082687163558484e+29\n",
      "Gradient Descent(16/49): loss=6.203472038041341e+31\n",
      "Gradient Descent(17/49): loss=7.571401573134075e+33\n",
      "Gradient Descent(18/49): loss=9.240973693272819e+35\n",
      "Gradient Descent(19/49): loss=1.127870369032103e+38\n",
      "Gradient Descent(20/49): loss=1.376577416928571e+40\n",
      "Gradient Descent(21/49): loss=1.680126933897756e+42\n",
      "Gradient Descent(22/49): loss=2.0506122498985002e+44\n",
      "Gradient Descent(23/49): loss=2.5027933988996997e+46\n",
      "Gradient Descent(24/49): loss=3.0546851545948944e+48\n",
      "Gradient Descent(25/49): loss=3.7282747340688803e+50\n",
      "Gradient Descent(26/49): loss=4.550397762529658e+52\n",
      "Gradient Descent(27/49): loss=5.553807397299709e+54\n",
      "Gradient Descent(28/49): loss=6.778479204673898e+56\n",
      "Gradient Descent(29/49): loss=8.273203775582356e+58\n",
      "Gradient Descent(30/49): loss=1.0097530529431417e+61\n",
      "Gradient Descent(31/49): loss=1.232414014673808e+63\n",
      "Gradient Descent(32/49): loss=1.5041740147627276e+65\n",
      "Gradient Descent(33/49): loss=1.8358598975250095e+67\n",
      "Gradient Descent(34/49): loss=2.2406859381041572e+69\n",
      "Gradient Descent(35/49): loss=2.73478029559134e+71\n",
      "Gradient Descent(36/49): loss=3.3378275544865704e+73\n",
      "Gradient Descent(37/49): loss=4.0738529531787465e+75\n",
      "Gradient Descent(38/49): loss=4.972179542893166e+77\n",
      "Gradient Descent(39/49): loss=6.068596410058072e+79\n",
      "Gradient Descent(40/49): loss=7.406784503751221e+81\n",
      "Gradient Descent(41/49): loss=9.040056872802381e+83\n",
      "Gradient Descent(42/49): loss=1.1033482643124338e+86\n",
      "Gradient Descent(43/49): loss=1.3466479353949873e+88\n",
      "Gradient Descent(44/49): loss=1.6435976930943548e+90\n",
      "Gradient Descent(45/49): loss=2.0060279348014878e+92\n",
      "Gradient Descent(46/49): loss=2.4483777825385993e+94\n",
      "Gradient Descent(47/49): loss=2.988270333644116e+96\n",
      "Gradient Descent(48/49): loss=3.6472147601660763e+98\n",
      "Gradient Descent(49/49): loss=4.4514632284126876e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.933195861492064\n",
      "Gradient Descent(2/49): loss=406.0420604343948\n",
      "Gradient Descent(3/49): loss=49517.8004614126\n",
      "Gradient Descent(4/49): loss=6126071.1279075965\n",
      "Gradient Descent(5/49): loss=760662791.1551893\n",
      "Gradient Descent(6/49): loss=94551959033.7986\n",
      "Gradient Descent(7/49): loss=11756900042835.156\n",
      "Gradient Descent(8/49): loss=1462043238831541.5\n",
      "Gradient Descent(9/49): loss=1.8182008467870154e+17\n",
      "Gradient Descent(10/49): loss=2.2611429140152685e+19\n",
      "Gradient Descent(11/49): loss=2.8120018629599733e+21\n",
      "Gradient Descent(12/49): loss=3.497064572825405e+23\n",
      "Gradient Descent(13/49): loss=4.349024397410831e+25\n",
      "Gradient Descent(14/49): loss=5.408540454981244e+27\n",
      "Gradient Descent(15/49): loss=6.726177681868586e+29\n",
      "Gradient Descent(16/49): loss=8.364819875113048e+31\n",
      "Gradient Descent(17/49): loss=1.0402670742736293e+34\n",
      "Gradient Descent(18/49): loss=1.2936986129376558e+36\n",
      "Gradient Descent(19/49): loss=1.60887155143517e+38\n",
      "Gradient Descent(20/49): loss=2.0008274287496096e+40\n",
      "Gradient Descent(21/49): loss=2.4882722279507822e+42\n",
      "Gradient Descent(22/49): loss=3.0944691138750506e+44\n",
      "Gradient Descent(23/49): loss=3.8483486610492255e+46\n",
      "Gradient Descent(24/49): loss=4.785889557149404e+48\n",
      "Gradient Descent(25/49): loss=5.9518356756674505e+50\n",
      "Gradient Descent(26/49): loss=7.401831464588093e+52\n",
      "Gradient Descent(27/49): loss=9.205077561894243e+54\n",
      "Gradient Descent(28/49): loss=1.1447633376397741e+57\n",
      "Gradient Descent(29/49): loss=1.4236524248628777e+59\n",
      "Gradient Descent(30/49): loss=1.7704849204873213e+61\n",
      "Gradient Descent(31/49): loss=2.201813307047118e+63\n",
      "Gradient Descent(32/49): loss=2.738222609518404e+65\n",
      "Gradient Descent(33/49): loss=3.4053128098009753e+67\n",
      "Gradient Descent(34/49): loss=4.2349205985973885e+69\n",
      "Gradient Descent(35/49): loss=5.266638772451773e+71\n",
      "Gradient Descent(36/49): loss=6.549705788741118e+73\n",
      "Gradient Descent(37/49): loss=8.145355657095573e+75\n",
      "Gradient Descent(38/49): loss=1.0129740315149459e+78\n",
      "Gradient Descent(39/49): loss=1.2597563958177546e+80\n",
      "Gradient Descent(40/49): loss=1.5666602769967606e+82\n",
      "Gradient Descent(41/49): loss=1.9483325757805126e+84\n",
      "Gradient Descent(42/49): loss=2.422988494432453e+86\n",
      "Gradient Descent(43/49): loss=3.01328085211538e+88\n",
      "Gradient Descent(44/49): loss=3.7473811842643517e+90\n",
      "Gradient Descent(45/49): loss=4.660324221129316e+92\n",
      "Gradient Descent(46/49): loss=5.7956799103446614e+94\n",
      "Gradient Descent(47/49): loss=7.207632780328992e+96\n",
      "Gradient Descent(48/49): loss=8.96356788154366e+98\n",
      "Gradient Descent(49/49): loss=1.1147286718923764e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.91804165573927\n",
      "Gradient Descent(2/49): loss=403.2406050282783\n",
      "Gradient Descent(3/49): loss=49173.303690994566\n",
      "Gradient Descent(4/49): loss=6090933.320476496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=757599312.1164079\n",
      "Gradient Descent(6/49): loss=94349566294.59245\n",
      "Gradient Descent(7/49): loss=11754693218631.627\n",
      "Gradient Descent(8/49): loss=1464661745952134.8\n",
      "Gradient Descent(9/49): loss=1.8250760658844387e+17\n",
      "Gradient Descent(10/49): loss=2.2742085181321654e+19\n",
      "Gradient Descent(11/49): loss=2.83387983701632e+21\n",
      "Gradient Descent(12/49): loss=3.5312882967969516e+23\n",
      "Gradient Descent(13/49): loss=4.400328622837272e+25\n",
      "Gradient Descent(14/49): loss=5.483238166600598e+27\n",
      "Gradient Descent(15/49): loss=6.832649277984019e+29\n",
      "Gradient Descent(16/49): loss=8.514147195368228e+31\n",
      "Gradient Descent(17/49): loss=1.0609457600982925e+34\n",
      "Gradient Descent(18/49): loss=1.3220418696263028e+36\n",
      "Gradient Descent(19/49): loss=1.6473930823518063e+38\n",
      "Gradient Descent(20/49): loss=2.0528124188652129e+40\n",
      "Gradient Descent(21/49): loss=2.5580044449722675e+42\n",
      "Gradient Descent(22/49): loss=3.187522971157918e+44\n",
      "Gradient Descent(23/49): loss=3.9719644395795745e+46\n",
      "Gradient Descent(24/49): loss=4.949455000659095e+48\n",
      "Gradient Descent(25/49): loss=6.16750355553452e+50\n",
      "Gradient Descent(26/49): loss=7.685310827651257e+52\n",
      "Gradient Descent(27/49): loss=9.576646691125988e+54\n",
      "Gradient Descent(28/49): loss=1.1933435602458848e+57\n",
      "Gradient Descent(29/49): loss=1.487022439806537e+59\n",
      "Gradient Descent(30/49): loss=1.8529749605658962e+61\n",
      "Gradient Descent(31/49): loss=2.3089874857106403e+63\n",
      "Gradient Descent(32/49): loss=2.877223558131685e+65\n",
      "Gradient Descent(33/49): loss=3.5853011134532573e+67\n",
      "Gradient Descent(34/49): loss=4.467634792506718e+69\n",
      "Gradient Descent(35/49): loss=5.567108593562973e+71\n",
      "Gradient Descent(36/49): loss=6.937160160115352e+73\n",
      "Gradient Descent(37/49): loss=8.644378006697367e+75\n",
      "Gradient Descent(38/49): loss=1.0771737915508995e+78\n",
      "Gradient Descent(39/49): loss=1.3422635802196264e+80\n",
      "Gradient Descent(40/49): loss=1.6725913059860073e+82\n",
      "Gradient Descent(41/49): loss=2.0842118627715588e+84\n",
      "Gradient Descent(42/49): loss=2.5971312139261e+86\n",
      "Gradient Descent(43/49): loss=3.2362787405785735e+88\n",
      "Gradient Descent(44/49): loss=4.0327188824965123e+90\n",
      "Gradient Descent(45/49): loss=5.0251609607448063e+92\n",
      "Gradient Descent(46/49): loss=6.261840563942584e+94\n",
      "Gradient Descent(47/49): loss=7.8028639389941275e+96\n",
      "Gradient Descent(48/49): loss=9.723129330543109e+98\n",
      "Gradient Descent(49/49): loss=1.2115967254794275e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.9319277801750423\n",
      "Gradient Descent(2/49): loss=401.73896301005124\n",
      "Gradient Descent(3/49): loss=48559.91646953733\n",
      "Gradient Descent(4/49): loss=5959006.63731891\n",
      "Gradient Descent(5/49): loss=734147921.8348585\n",
      "Gradient Descent(6/49): loss=90554202192.4533\n",
      "Gradient Descent(7/49): loss=11173664359286.514\n",
      "Gradient Descent(8/49): loss=1378905825458893.8\n",
      "Gradient Descent(9/49): loss=1.701729092547618e+17\n",
      "Gradient Descent(10/49): loss=2.100156772412556e+19\n",
      "Gradient Descent(11/49): loss=2.591879402400981e+21\n",
      "Gradient Descent(12/49): loss=3.198736361710382e+23\n",
      "Gradient Descent(13/49): loss=3.94768320313959e+25\n",
      "Gradient Descent(14/49): loss=4.8719879136840133e+27\n",
      "Gradient Descent(15/49): loss=6.0127082426120694e+29\n",
      "Gradient Descent(16/49): loss=7.420515321411897e+31\n",
      "Gradient Descent(17/49): loss=9.157944421387446e+33\n",
      "Gradient Descent(18/49): loss=1.1302172764021467e+36\n",
      "Gradient Descent(19/49): loss=1.3948447749560018e+38\n",
      "Gradient Descent(20/49): loss=1.7214317876559084e+40\n",
      "Gradient Descent(21/49): loss=2.1244854287187115e+42\n",
      "Gradient Descent(22/49): loss=2.621909487954045e+44\n",
      "Gradient Descent(23/49): loss=3.235799723611402e+46\n",
      "Gradient Descent(24/49): loss=3.99342536401379e+48\n",
      "Gradient Descent(25/49): loss=4.92844041662758e+50\n",
      "Gradient Descent(26/49): loss=6.082378591355438e+52\n",
      "Gradient Descent(27/49): loss=7.506498243088537e+54\n",
      "Gradient Descent(28/49): loss=9.26405928653902e+56\n",
      "Gradient Descent(29/49): loss=1.1433133224740362e+59\n",
      "Gradient Descent(30/49): loss=1.4110071113706854e+61\n",
      "Gradient Descent(31/49): loss=1.7413783511508676e+63\n",
      "Gradient Descent(32/49): loss=2.149102252866151e+65\n",
      "Gradient Descent(33/49): loss=2.6522900610438533e+67\n",
      "Gradient Descent(34/49): loss=3.2732935617792248e+69\n",
      "Gradient Descent(35/49): loss=4.0396979572319065e+71\n",
      "Gradient Descent(36/49): loss=4.985547210374018e+73\n",
      "Gradient Descent(37/49): loss=6.152856290250926e+75\n",
      "Gradient Descent(38/49): loss=7.593477492241079e+77\n",
      "Gradient Descent(39/49): loss=9.371403735941999e+79\n",
      "Gradient Descent(40/49): loss=1.1565611154015301e+82\n",
      "Gradient Descent(41/49): loss=1.4273567240824612e+84\n",
      "Gradient Descent(42/49): loss=1.7615560394109344e+86\n",
      "Gradient Descent(43/49): loss=2.174004316951584e+88\n",
      "Gradient Descent(44/49): loss=2.683022659730197e+90\n",
      "Gradient Descent(45/49): loss=3.3112218483170663e+92\n",
      "Gradient Descent(46/49): loss=4.086506719952495e+94\n",
      "Gradient Descent(47/49): loss=5.043315711601903e+96\n",
      "Gradient Descent(48/49): loss=6.224150627895289e+98\n",
      "Gradient Descent(49/49): loss=7.6814645868014e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.931336305280564\n",
      "Gradient Descent(2/49): loss=402.85781095590795\n",
      "Gradient Descent(3/49): loss=48777.46397895525\n",
      "Gradient Descent(4/49): loss=5995369.0001700185\n",
      "Gradient Descent(5/49): loss=739895958.997374\n",
      "Gradient Descent(6/49): loss=91425997970.98647\n",
      "Gradient Descent(7/49): loss=11301718846862.037\n",
      "Gradient Descent(8/49): loss=1397259633484206.8\n",
      "Gradient Descent(9/49): loss=1.7275431910998355e+17\n",
      "Gradient Descent(10/49): loss=2.1359304480083526e+19\n",
      "Gradient Descent(11/49): loss=2.6408724821033275e+21\n",
      "Gradient Descent(12/49): loss=3.265190073191302e+23\n",
      "Gradient Descent(13/49): loss=4.037102177589298e+25\n",
      "Gradient Descent(14/49): loss=4.991500220652629e+27\n",
      "Gradient Descent(15/49): loss=6.171524741627842e+29\n",
      "Gradient Descent(16/49): loss=7.630515221726931e+31\n",
      "Gradient Descent(17/49): loss=9.434421052011608e+33\n",
      "Gradient Descent(18/49): loss=1.1664782547789364e+36\n",
      "Gradient Descent(19/49): loss=1.442241672704865e+38\n",
      "Gradient Descent(20/49): loss=1.7831974445236094e+40\n",
      "Gradient Descent(21/49): loss=2.2047574873227488e+42\n",
      "Gradient Descent(22/49): loss=2.7259772006581546e+44\n",
      "Gradient Descent(23/49): loss=3.3704168106043046e+46\n",
      "Gradient Descent(24/49): loss=4.167206341453377e+48\n",
      "Gradient Descent(25/49): loss=5.152362354006316e+50\n",
      "Gradient Descent(26/49): loss=6.370415969786617e+52\n",
      "Gradient Descent(27/49): loss=7.87642577128953e+54\n",
      "Gradient Descent(28/49): loss=9.738466565585055e+56\n",
      "Gradient Descent(29/49): loss=1.2040706508618866e+59\n",
      "Gradient Descent(30/49): loss=1.4887211682693465e+61\n",
      "Gradient Descent(31/49): loss=1.8406650102025307e+63\n",
      "Gradient Descent(32/49): loss=2.2758107777311415e+65\n",
      "Gradient Descent(33/49): loss=2.8138279737644084e+67\n",
      "Gradient Descent(34/49): loss=3.479036105907067e+69\n",
      "Gradient Descent(35/49): loss=4.3015039792970697e+71\n",
      "Gradient Descent(36/49): loss=5.318408869770661e+73\n",
      "Gradient Descent(37/49): loss=6.5757170148375695e+75\n",
      "Gradient Descent(38/49): loss=8.130261384188892e+77\n",
      "Gradient Descent(39/49): loss=1.0052310649330191e+80\n",
      "Gradient Descent(40/49): loss=1.242874547516386e+82\n",
      "Gradient Descent(41/49): loss=1.536698570857427e+84\n",
      "Gradient Descent(42/49): loss=1.8999845981189973e+86\n",
      "Gradient Descent(43/49): loss=2.3491539209769653e+88\n",
      "Gradient Descent(44/49): loss=2.904509936504141e+90\n",
      "Gradient Descent(45/49): loss=3.59115590337427e+92\n",
      "Gradient Descent(46/49): loss=4.4401296618947055e+94\n",
      "Gradient Descent(47/49): loss=5.489806609596952e+96\n",
      "Gradient Descent(48/49): loss=6.787634349829677e+98\n",
      "Gradient Descent(49/49): loss=8.392277423115007e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.9802360915278268\n",
      "Gradient Descent(2/49): loss=416.69182483330917\n",
      "Gradient Descent(3/49): loss=51477.428240190726\n",
      "Gradient Descent(4/49): loss=6451308.534155915\n",
      "Gradient Descent(5/49): loss=811468099.3457389\n",
      "Gradient Descent(6/49): loss=102179695732.7754\n",
      "Gradient Descent(7/49): loss=12870700805630.617\n",
      "Gradient Descent(8/49): loss=1621380971527196.8\n",
      "Gradient Descent(9/49): loss=2.0425950191824995e+17\n",
      "Gradient Descent(10/49): loss=2.5732620163252437e+19\n",
      "Gradient Descent(11/49): loss=3.2418072910155607e+21\n",
      "Gradient Descent(12/49): loss=4.0840480140942665e+23\n",
      "Gradient Descent(13/49): loss=5.145109593614046e+25\n",
      "Gradient Descent(14/49): loss=6.481842395806549e+27\n",
      "Gradient Descent(15/49): loss=8.165867317450668e+29\n",
      "Gradient Descent(16/49): loss=1.0287413006533727e+32\n",
      "Gradient Descent(17/49): loss=1.2960150173614617e+34\n",
      "Gradient Descent(18/49): loss=1.6327281951281394e+36\n",
      "Gradient Descent(19/49): loss=2.0569216593835734e+38\n",
      "Gradient Descent(20/49): loss=2.5913233604563793e+40\n",
      "Gradient Descent(21/49): loss=3.2645661190114165e+42\n",
      "Gradient Descent(22/49): loss=4.1127217498784576e+44\n",
      "Gradient Descent(23/49): loss=5.181233761350825e+46\n",
      "Gradient Descent(24/49): loss=6.527352182428946e+48\n",
      "Gradient Descent(25/49): loss=8.22320097411841e+50\n",
      "Gradient Descent(26/49): loss=1.0359642374250825e+53\n",
      "Gradient Descent(27/49): loss=1.3051145224367196e+55\n",
      "Gradient Descent(28/49): loss=1.6441918119769415e+57\n",
      "Gradient Descent(29/49): loss=2.0713635992071334e+59\n",
      "Gradient Descent(30/49): loss=2.6095174108436264e+61\n",
      "Gradient Descent(31/49): loss=3.2874871027484083e+63\n",
      "Gradient Descent(32/49): loss=4.1415977551355695e+65\n",
      "Gradient Descent(33/49): loss=5.2176119416571574e+67\n",
      "Gradient Descent(34/49): loss=6.5731816519280145e+69\n",
      "Gradient Descent(35/49): loss=8.280937239560249e+71\n",
      "Gradient Descent(36/49): loss=1.0432378899101649e+74\n",
      "Gradient Descent(37/49): loss=1.3142779174136197e+76\n",
      "Gradient Descent(38/49): loss=1.6557359169056093e+78\n",
      "Gradient Descent(39/49): loss=2.0859069381050099e+80\n",
      "Gradient Descent(40/49): loss=2.627839204313557e+82\n",
      "Gradient Descent(41/49): loss=3.3105690180026648e+84\n",
      "Gradient Descent(42/49): loss=4.1706765029491563e+86\n",
      "Gradient Descent(43/49): loss=5.254245538353612e+88\n",
      "Gradient Descent(44/49): loss=6.619332896662554e+90\n",
      "Gradient Descent(45/49): loss=8.339078879547115e+92\n",
      "Gradient Descent(46/49): loss=1.0505626117455178e+95\n",
      "Gradient Descent(47/49): loss=1.3235056498919957e+97\n",
      "Gradient Descent(48/49): loss=1.6673610746394508e+99\n",
      "Gradient Descent(49/49): loss=2.100552387856969e+101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.9648450496209158\n",
      "Gradient Descent(2/49): loss=413.8175894269912\n",
      "Gradient Descent(3/49): loss=51119.26323935475\n",
      "Gradient Descent(4/49): loss=6414285.917376489\n",
      "Gradient Descent(5/49): loss=808196181.4286059\n",
      "Gradient Descent(6/49): loss=101960353704.69638\n",
      "Gradient Descent(7/49): loss=12868191149268.855\n",
      "Gradient Descent(8/49): loss=1624271212405102.0\n",
      "Gradient Descent(9/49): loss=2.0502993230047546e+17\n",
      "Gradient Descent(10/49): loss=2.5881038736196727e+19\n",
      "Gradient Descent(11/49): loss=3.266991363731058e+21\n",
      "Gradient Descent(12/49): loss=4.123964032943717e+23\n",
      "Gradient Descent(13/49): loss=5.205733670000006e+25\n",
      "Gradient Descent(14/49): loss=6.571266564915709e+27\n",
      "Gradient Descent(15/49): loss=8.294997214428706e+29\n",
      "Gradient Descent(16/49): loss=1.0470885506292123e+32\n",
      "Gradient Descent(17/49): loss=1.321753835369006e+34\n",
      "Gradient Descent(18/49): loss=1.6684674883022442e+36\n",
      "Gradient Descent(19/49): loss=2.106128755951423e+38\n",
      "Gradient Descent(20/49): loss=2.6585944098327045e+40\n",
      "Gradient Descent(21/49): loss=3.3559791709809476e+42\n",
      "Gradient Descent(22/49): loss=4.236297253391366e+44\n",
      "Gradient Descent(23/49): loss=5.347534506284696e+46\n",
      "Gradient Descent(24/49): loss=6.750264106954639e+48\n",
      "Gradient Descent(25/49): loss=8.520948384739897e+50\n",
      "Gradient Descent(26/49): loss=1.0756106757454194e+53\n",
      "Gradient Descent(27/49): loss=1.3577576973116598e+55\n",
      "Gradient Descent(28/49): loss=1.713915644553748e+57\n",
      "Gradient Descent(29/49): loss=2.1634985700779463e+59\n",
      "Gradient Descent(30/49): loss=2.731013091340348e+61\n",
      "Gradient Descent(31/49): loss=3.447394238307103e+63\n",
      "Gradient Descent(32/49): loss=4.351691711767035e+65\n",
      "Gradient Descent(33/49): loss=5.49319846968277e+67\n",
      "Gradient Descent(34/49): loss=6.934137670122848e+69\n",
      "Gradient Descent(35/49): loss=8.753054435150168e+71\n",
      "Gradient Descent(36/49): loss=1.1049097319601488e+74\n",
      "Gradient Descent(37/49): loss=1.3947422866213488e+76\n",
      "Gradient Descent(38/49): loss=1.7606017847618158e+78\n",
      "Gradient Descent(39/49): loss=2.2224311073376368e+80\n",
      "Gradient Descent(40/49): loss=2.805404418881792e+82\n",
      "Gradient Descent(41/49): loss=3.5412994029361475e+84\n",
      "Gradient Descent(42/49): loss=4.470229453133406e+86\n",
      "Gradient Descent(43/49): loss=5.642830241095432e+88\n",
      "Gradient Descent(44/49): loss=7.123019850245472e+90\n",
      "Gradient Descent(45/49): loss=8.991482929520327e+92\n",
      "Gradient Descent(46/49): loss=1.1350068787056588e+95\n",
      "Gradient Descent(47/49): loss=1.4327343162490818e+97\n",
      "Gradient Descent(48/49): loss=1.808559630315736e+99\n",
      "Gradient Descent(49/49): loss=2.2829689352112736e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.978967469366909\n",
      "Gradient Descent(2/49): loss=412.28023455632297\n",
      "Gradient Descent(3/49): loss=50482.331868913956\n",
      "Gradient Descent(4/49): loss=6275481.461775293\n",
      "Gradient Descent(5/49): loss=783198494.7335875\n",
      "Gradient Descent(6/49): loss=97861834556.1446\n",
      "Gradient Descent(7/49): loss=12232560902188.377\n",
      "Gradient Descent(8/49): loss=1529232803818322.5\n",
      "Gradient Descent(9/49): loss=1.9118188483836464e+17\n",
      "Gradient Descent(10/49): loss=2.390151213989375e+19\n",
      "Gradient Descent(11/49): loss=2.9881735105877503e+21\n",
      "Gradient Descent(12/49): loss=3.7358276421408666e+23\n",
      "Gradient Descent(13/49): loss=4.670550176590361e+25\n",
      "Gradient Descent(14/49): loss=5.839145747065793e+27\n",
      "Gradient Descent(15/49): loss=7.300130255963313e+29\n",
      "Gradient Descent(16/49): loss=9.126660795842188e+31\n",
      "Gradient Descent(17/49): loss=1.1410198829159132e+34\n",
      "Gradient Descent(18/49): loss=1.4265089987024965e+36\n",
      "Gradient Descent(19/49): loss=1.7834289787001224e+38\n",
      "Gradient Descent(20/49): loss=2.229652196736693e+40\n",
      "Gradient Descent(21/49): loss=2.7875227878819715e+42\n",
      "Gradient Descent(22/49): loss=3.4849755062597877e+44\n",
      "Gradient Descent(23/49): loss=4.3569345269935445e+46\n",
      "Gradient Descent(24/49): loss=5.447062235716907e+48\n",
      "Gradient Descent(25/49): loss=6.809945574340846e+50\n",
      "Gradient Descent(26/49): loss=8.513829421923349e+52\n",
      "Gradient Descent(27/49): loss=1.0644033881670003e+55\n",
      "Gradient Descent(28/49): loss=1.3307226590939573e+57\n",
      "Gradient Descent(29/49): loss=1.6636763985462511e+59\n",
      "Gradient Descent(30/49): loss=2.0799368975683814e+61\n",
      "Gradient Descent(31/49): loss=2.600347941250255e+63\n",
      "Gradient Descent(32/49): loss=3.250968538261686e+65\n",
      "Gradient Descent(33/49): loss=4.0643777969519954e+67\n",
      "Gradient Descent(34/49): loss=5.081306288245167e+69\n",
      "Gradient Descent(35/49): loss=6.352675584027374e+71\n",
      "Gradient Descent(36/49): loss=7.942148098660364e+73\n",
      "Gradient Descent(37/49): loss=9.92931491412086e+75\n",
      "Gradient Descent(38/49): loss=1.2413681215591186e+78\n",
      "Gradient Descent(39/49): loss=1.5519648903789956e+80\n",
      "Gradient Descent(40/49): loss=1.9402745882856932e+82\n",
      "Gradient Descent(41/49): loss=2.4257413948506723e+84\n",
      "Gradient Descent(42/49): loss=3.032674524635727e+86\n",
      "Gradient Descent(43/49): loss=3.791465484283693e+88\n",
      "Gradient Descent(44/49): loss=4.740109893672576e+90\n",
      "Gradient Descent(45/49): loss=5.92611007464773e+92\n",
      "Gradient Descent(46/49): loss=7.408853677363109e+94\n",
      "Gradient Descent(47/49): loss=9.262587451320588e+96\n",
      "Gradient Descent(48/49): loss=1.1580135069410282e+99\n",
      "Gradient Descent(49/49): loss=1.447754517088707e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.978038332795893\n",
      "Gradient Descent(2/49): loss=413.3641444727269\n",
      "Gradient Descent(3/49): loss=50696.75193411638\n",
      "Gradient Descent(4/49): loss=6311826.32471109\n",
      "Gradient Descent(5/49): loss=789025898.7785614\n",
      "Gradient Descent(6/49): loss=98758204262.02817\n",
      "Gradient Descent(7/49): loss=12366061786764.8\n",
      "Gradient Descent(8/49): loss=1548630082069023.5\n",
      "Gradient Descent(9/49): loss=1.9394708121087117e+17\n",
      "Gradient Descent(10/49): loss=2.4289872251698733e+19\n",
      "Gradient Descent(11/49): loss=3.04207108785327e+21\n",
      "Gradient Descent(12/49): loss=3.809905494334594e+23\n",
      "Gradient Descent(13/49): loss=4.771547889985213e+25\n",
      "Gradient Descent(14/49): loss=5.97591554346311e+27\n",
      "Gradient Descent(15/49): loss=7.484273364940123e+29\n",
      "Gradient Descent(16/49): loss=9.37335016653556e+31\n",
      "Gradient Descent(17/49): loss=1.1739241695894385e+34\n",
      "Gradient Descent(18/49): loss=1.4702298907348297e+36\n",
      "Gradient Descent(19/49): loss=1.8413250099691005e+38\n",
      "Gradient Descent(20/49): loss=2.3060868334778124e+40\n",
      "Gradient Descent(21/49): loss=2.88815741664521e+42\n",
      "Gradient Descent(22/49): loss=3.617146216034925e+44\n",
      "Gradient Descent(23/49): loss=4.530136298283566e+46\n",
      "Gradient Descent(24/49): loss=5.673570725484323e+48\n",
      "Gradient Descent(25/49): loss=7.1056150759312935e+50\n",
      "Gradient Descent(26/49): loss=8.899116279723507e+52\n",
      "Gradient Descent(27/49): loss=1.1145308282783648e+55\n",
      "Gradient Descent(28/49): loss=1.3958453043401568e+57\n",
      "Gradient Descent(29/49): loss=1.7481652945017194e+59\n",
      "Gradient Descent(30/49): loss=2.1894130297948418e+61\n",
      "Gradient Descent(31/49): loss=2.7420344232389807e+63\n",
      "Gradient Descent(32/49): loss=3.4341408751605364e+65\n",
      "Gradient Descent(33/49): loss=4.300939277238466e+67\n",
      "Gradient Descent(34/49): loss=5.386522958417592e+69\n",
      "Gradient Descent(35/49): loss=6.746114676650223e+71\n",
      "Gradient Descent(36/49): loss=8.448875755629363e+73\n",
      "Gradient Descent(37/49): loss=1.0581424265012262e+76\n",
      "Gradient Descent(38/49): loss=1.325224121109702e+78\n",
      "Gradient Descent(39/49): loss=1.6597188877285262e+80\n",
      "Gradient Descent(40/49): loss=2.0786422027816304e+82\n",
      "Gradient Descent(41/49): loss=2.6033043542079767e+84\n",
      "Gradient Descent(42/49): loss=3.260394478457592e+86\n",
      "Gradient Descent(43/49): loss=4.083338215131933e+88\n",
      "Gradient Descent(44/49): loss=5.113998042054304e+90\n",
      "Gradient Descent(45/49): loss=6.404802785431347e+92\n",
      "Gradient Descent(46/49): loss=8.021414631553273e+94\n",
      "Gradient Descent(47/49): loss=1.0046069308746654e+97\n",
      "Gradient Descent(48/49): loss=1.2581759351916082e+99\n",
      "Gradient Descent(49/49): loss=1.575747324893553e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.027591168195354\n",
      "Gradient Descent(2/49): loss=427.5499697976286\n",
      "Gradient Descent(3/49): loss=53501.26292662272\n",
      "Gradient Descent(4/49): loss=6791555.685396853\n",
      "Gradient Descent(5/49): loss=865307273.4083251\n",
      "Gradient Descent(6/49): loss=110367762370.34169\n",
      "Gradient Descent(7/49): loss=14081828547890.346\n",
      "Gradient Descent(8/49): loss=1796889387076704.2\n",
      "Gradient Descent(9/49): loss=2.2929681671383357e+17\n",
      "Gradient Descent(10/49): loss=2.9260333905865986e+19\n",
      "Gradient Descent(11/49): loss=3.733893991652466e+21\n",
      "Gradient Descent(12/49): loss=4.7648052276077806e+23\n",
      "Gradient Descent(13/49): loss=6.080348453766612e+25\n",
      "Gradient Descent(14/49): loss=7.759108619697868e+27\n",
      "Gradient Descent(15/49): loss=9.901368170076549e+29\n",
      "Gradient Descent(16/49): loss=1.2635097344091016e+32\n",
      "Gradient Descent(17/49): loss=1.612359854812734e+34\n",
      "Gradient Descent(18/49): loss=2.057526138107083e+36\n",
      "Gradient Descent(19/49): loss=2.625601101301925e+38\n",
      "Gradient Descent(20/49): loss=3.350519352466993e+40\n",
      "Gradient Descent(21/49): loss=4.2755847131889286e+42\n",
      "Gradient Descent(22/49): loss=5.456057021847562e+44\n",
      "Gradient Descent(23/49): loss=6.962453143293593e+46\n",
      "Gradient Descent(24/49): loss=8.884759374490557e+48\n",
      "Gradient Descent(25/49): loss=1.1337806878982416e+51\n",
      "Gradient Descent(26/49): loss=1.446813125791455e+53\n",
      "Gradient Descent(27/49): loss=1.8462726021934153e+55\n",
      "Gradient Descent(28/49): loss=2.3560212862634866e+57\n",
      "Gradient Descent(29/49): loss=3.0065095992499487e+59\n",
      "Gradient Descent(30/49): loss=3.8365952052655623e+61\n",
      "Gradient Descent(31/49): loss=4.895864218341034e+63\n",
      "Gradient Descent(32/49): loss=6.24759328571725e+65\n",
      "Gradient Descent(33/49): loss=7.972529490812862e+67\n",
      "Gradient Descent(34/49): loss=1.0173713872698651e+70\n",
      "Gradient Descent(35/49): loss=1.298263670053707e+72\n",
      "Gradient Descent(36/49): loss=1.656709219535209e+74\n",
      "Gradient Descent(37/49): loss=2.1141201909928155e+76\n",
      "Gradient Descent(38/49): loss=2.697820552491032e+78\n",
      "Gradient Descent(39/49): loss=3.442678313395759e+80\n",
      "Gradient Descent(40/49): loss=4.393188404833652e+82\n",
      "Gradient Descent(41/49): loss=5.606130635344699e+84\n",
      "Gradient Descent(42/49): loss=7.153961497751971e+86\n",
      "Gradient Descent(43/49): loss=9.129142440714945e+88\n",
      "Gradient Descent(44/49): loss=1.1649663159223313e+91\n",
      "Gradient Descent(45/49): loss=1.4866089844111987e+93\n",
      "Gradient Descent(46/49): loss=1.8970559425852433e+95\n",
      "Gradient Descent(47/49): loss=2.420825709406953e+97\n",
      "Gradient Descent(48/49): loss=3.0892062715553365e+99\n",
      "Gradient Descent(49/49): loss=3.9421241071315016e+101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.011961554837893\n",
      "Gradient Descent(2/49): loss=424.60154674789266\n",
      "Gradient Descent(3/49): loss=53128.98093452473\n",
      "Gradient Descent(4/49): loss=6752560.409068148\n",
      "Gradient Descent(5/49): loss=861814244.0489888\n",
      "Gradient Descent(6/49): loss=110130180693.4451\n",
      "Gradient Descent(7/49): loss=14078981464400.375\n",
      "Gradient Descent(8/49): loss=1800077564619237.2\n",
      "Gradient Descent(9/49): loss=2.3015953086133622e+17\n",
      "Gradient Descent(10/49): loss=2.9428793232922296e+19\n",
      "Gradient Descent(11/49): loss=3.762857784192027e+21\n",
      "Gradient Descent(12/49): loss=4.8113146389471856e+23\n",
      "Gradient Descent(13/49): loss=6.151909082213232e+25\n",
      "Gradient Descent(14/49): loss=7.86603947584259e+27\n",
      "Gradient Descent(15/49): loss=1.0057785174871936e+30\n",
      "Gradient Descent(16/49): loss=1.2860225846450504e+32\n",
      "Gradient Descent(17/49): loss=1.6443521786282074e+34\n",
      "Gradient Descent(18/49): loss=2.102524577722478e+36\n",
      "Gradient Descent(19/49): loss=2.688359379271116e+38\n",
      "Gradient Descent(20/49): loss=3.4374276666815292e+40\n",
      "Gradient Descent(21/49): loss=4.395211836417955e+42\n",
      "Gradient Descent(22/49): loss=5.619867226466589e+44\n",
      "Gradient Descent(23/49): loss=7.185753228457741e+46\n",
      "Gradient Descent(24/49): loss=9.187948287677327e+48\n",
      "Gradient Descent(25/49): loss=1.174802293554445e+51\n",
      "Gradient Descent(26/49): loss=1.502142138514318e+53\n",
      "Gradient Descent(27/49): loss=1.9206899890138263e+55\n",
      "Gradient Descent(28/49): loss=2.4558594951251655e+57\n",
      "Gradient Descent(29/49): loss=3.1401454135204907e+59\n",
      "Gradient Descent(30/49): loss=4.0150966444239717e+61\n",
      "Gradient Descent(31/49): loss=5.1338390237129204e+63\n",
      "Gradient Descent(32/49): loss=6.564301050636047e+65\n",
      "Gradient Descent(33/49): loss=8.39333841290131e+67\n",
      "Gradient Descent(34/49): loss=1.0732007744626359e+70\n",
      "Gradient Descent(35/49): loss=1.3722309832483862e+72\n",
      "Gradient Descent(36/49): loss=1.7545811708248907e+74\n",
      "Gradient Descent(37/49): loss=2.2434671149354176e+76\n",
      "Gradient Descent(38/49): loss=2.868573297997011e+78\n",
      "Gradient Descent(39/49): loss=3.667855308063362e+80\n",
      "Gradient Descent(40/49): loss=4.689844449950887e+82\n",
      "Gradient Descent(41/49): loss=5.996594499347491e+84\n",
      "Gradient Descent(42/49): loss=7.667449522762107e+86\n",
      "Gradient Descent(43/49): loss=9.803861540162818e+88\n",
      "Gradient Descent(44/49): loss=1.2535550552155367e+91\n",
      "Gradient Descent(45/49): loss=1.6028380960083745e+93\n",
      "Gradient Descent(46/49): loss=2.0494432624453185e+95\n",
      "Gradient Descent(47/49): loss=2.6204878062497447e+97\n",
      "Gradient Descent(48/49): loss=3.3506447670623494e+99\n",
      "Gradient Descent(49/49): loss=4.2842482717404236e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.026322088116683\n",
      "Gradient Descent(2/49): loss=423.0278518775686\n",
      "Gradient Descent(3/49): loss=52467.75771120238\n",
      "Gradient Descent(4/49): loss=6606566.556607294\n",
      "Gradient Descent(5/49): loss=835179111.7598962\n",
      "Gradient Descent(6/49): loss=105706457734.37552\n",
      "Gradient Descent(7/49): loss=13384017064860.648\n",
      "Gradient Descent(8/49): loss=1694821014808352.5\n",
      "Gradient Descent(9/49): loss=2.146239555912953e+17\n",
      "Gradient Descent(10/49): loss=2.717928860607545e+19\n",
      "Gradient Descent(11/49): loss=3.441912157510047e+21\n",
      "Gradient Descent(12/49): loss=4.358751094003591e+23\n",
      "Gradient Descent(13/49): loss=5.519815341785865e+25\n",
      "Gradient Descent(14/49): loss=6.99015960517045e+27\n",
      "Gradient Descent(15/49): loss=8.852168160700211e+29\n",
      "Gradient Descent(16/49): loss=1.1210170694953635e+32\n",
      "Gradient Descent(17/49): loss=1.4196287888422466e+34\n",
      "Gradient Descent(18/49): loss=1.79778342032762e+36\n",
      "Gradient Descent(19/49): loss=2.2766692628061094e+38\n",
      "Gradient Descent(20/49): loss=2.8831186640808496e+40\n",
      "Gradient Descent(21/49): loss=3.6511114580629465e+42\n",
      "Gradient Descent(22/49): loss=4.623678881319933e+44\n",
      "Gradient Descent(23/49): loss=5.855314646854929e+46\n",
      "Gradient Descent(24/49): loss=7.415028269427647e+48\n",
      "Gradient Descent(25/49): loss=9.390211722606615e+50\n",
      "Gradient Descent(26/49): loss=1.1891536079363294e+53\n",
      "Gradient Descent(27/49): loss=1.5059152498805987e+55\n",
      "Gradient Descent(28/49): loss=1.9070545005186743e+57\n",
      "Gradient Descent(29/49): loss=2.4150475056527484e+59\n",
      "Gradient Descent(30/49): loss=3.0583575104818844e+61\n",
      "Gradient Descent(31/49): loss=3.873029677481588e+63\n",
      "Gradient Descent(32/49): loss=4.904710725035431e+65\n",
      "Gradient Descent(33/49): loss=6.211206548750195e+67\n",
      "Gradient Descent(34/49): loss=7.865721130976291e+69\n",
      "Gradient Descent(35/49): loss=9.960958217165757e+71\n",
      "Gradient Descent(36/49): loss=1.261431557920069e+74\n",
      "Gradient Descent(37/49): loss=1.5974462904326912e+76\n",
      "Gradient Descent(38/49): loss=2.022967187395251e+78\n",
      "Gradient Descent(39/49): loss=2.561836517313752e+80\n",
      "Gradient Descent(40/49): loss=3.244247550002388e+82\n",
      "Gradient Descent(41/49): loss=4.108436309094672e+84\n",
      "Gradient Descent(42/49): loss=5.202823966338546e+86\n",
      "Gradient Descent(43/49): loss=6.588729917702352e+88\n",
      "Gradient Descent(44/49): loss=8.3438075570672e+90\n",
      "Gradient Descent(45/49): loss=1.0566395256591367e+93\n",
      "Gradient Descent(46/49): loss=1.3381026342577876e+95\n",
      "Gradient Descent(47/49): loss=1.6945406795100644e+97\n",
      "Gradient Descent(48/49): loss=2.1459251637354064e+99\n",
      "Gradient Descent(49/49): loss=2.717547512452916e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.025050928104108\n",
      "Gradient Descent(2/49): loss=424.0748957779719\n",
      "Gradient Descent(3/49): loss=52678.56779668077\n",
      "Gradient Descent(4/49): loss=6642804.283863346\n",
      "Gradient Descent(5/49): loss=841072654.2837894\n",
      "Gradient Descent(6/49): loss=106625885928.27888\n",
      "Gradient Descent(7/49): loss=13522863204725.68\n",
      "Gradient Descent(8/49): loss=1715271745128138.8\n",
      "Gradient Descent(9/49): loss=2.17578801171521e+17\n",
      "Gradient Descent(10/49): loss=2.7599847690037748e+19\n",
      "Gradient Descent(11/49): loss=3.501055131030769e+21\n",
      "Gradient Descent(12/49): loss=4.441114144737653e+23\n",
      "Gradient Descent(13/49): loss=5.633589037687658e+25\n",
      "Gradient Descent(14/49): loss=7.146254365444191e+27\n",
      "Gradient Descent(15/49): loss=9.06508342885716e+29\n",
      "Gradient Descent(16/49): loss=1.149913438267624e+32\n",
      "Gradient Descent(17/49): loss=1.458674853997848e+34\n",
      "Gradient Descent(18/49): loss=1.8503413072918698e+36\n",
      "Gradient Descent(19/49): loss=2.3471734956915518e+38\n",
      "Gradient Descent(20/49): loss=2.977409302057102e+40\n",
      "Gradient Descent(21/49): loss=3.776868718490849e+42\n",
      "Gradient Descent(22/49): loss=4.7909897061431084e+44\n",
      "Gradient Descent(23/49): loss=6.077410700619893e+46\n",
      "Gradient Descent(24/49): loss=7.709246541851463e+48\n",
      "Gradient Descent(25/49): loss=9.779244018682429e+50\n",
      "Gradient Descent(26/49): loss=1.2405053212113667e+53\n",
      "Gradient Descent(27/49): loss=1.5735914238504084e+55\n",
      "Gradient Descent(28/49): loss=1.9961139439511794e+57\n",
      "Gradient Descent(29/49): loss=2.5320873111311428e+59\n",
      "Gradient Descent(30/49): loss=3.211974031151888e+61\n",
      "Gradient Descent(31/49): loss=4.074416048546664e+63\n",
      "Gradient Descent(32/49): loss=5.168430994661938e+65\n",
      "Gradient Descent(33/49): loss=6.556198146752954e+67\n",
      "Gradient Descent(34/49): loss=8.316592440506876e+69\n",
      "Gradient Descent(35/49): loss=1.0549667394624325e+72\n",
      "Gradient Descent(36/49): loss=1.3382341738320984e+74\n",
      "Gradient Descent(37/49): loss=1.697561294609743e+76\n",
      "Gradient Descent(38/49): loss=2.1533707667210256e+78\n",
      "Gradient Descent(39/49): loss=2.731568912234606e+80\n",
      "Gradient Descent(40/49): loss=3.4650181183838987e+82\n",
      "Gradient Descent(41/49): loss=4.395404599515192e+84\n",
      "Gradient Descent(42/49): loss=5.575607668813577e+86\n",
      "Gradient Descent(43/49): loss=7.072705179396175e+88\n",
      "Gradient Descent(44/49): loss=8.971785951593247e+90\n",
      "Gradient Descent(45/49): loss=1.1380785868990208e+93\n",
      "Gradient Descent(46/49): loss=1.4436622506894095e+95\n",
      "Gradient Descent(47/49): loss=1.831297696009237e+97\n",
      "Gradient Descent(48/49): loss=2.3230165156754803e+99\n",
      "Gradient Descent(49/49): loss=2.946765970306669e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.075261091494645\n",
      "Gradient Descent(2/49): loss=438.6191968509186\n",
      "Gradient Descent(3/49): loss=55590.97666419887\n",
      "Gradient Descent(4/49): loss=7147402.462769785\n",
      "Gradient Descent(5/49): loss=922340297.8195387\n",
      "Gradient Descent(6/49): loss=119153332132.49316\n",
      "Gradient Descent(7/49): loss=15398081781672.22\n",
      "Gradient Descent(8/49): loss=1990089993401617.8\n",
      "Gradient Descent(9/49): loss=2.572132109565914e+17\n",
      "Gradient Descent(10/49): loss=3.324439411904414e+19\n",
      "Gradient Descent(11/49): loss=4.296798985183602e+21\n",
      "Gradient Descent(12/49): loss=5.553568300021615e+23\n",
      "Gradient Descent(13/49): loss=7.177932110941586e+25\n",
      "Gradient Descent(14/49): loss=9.277407284947529e+27\n",
      "Gradient Descent(15/49): loss=1.199095889870958e+30\n",
      "Gradient Descent(16/49): loss=1.5498198201914249e+32\n",
      "Gradient Descent(17/49): loss=2.0031271092002624e+34\n",
      "Gradient Descent(18/49): loss=2.5890223933440206e+36\n",
      "Gradient Descent(19/49): loss=3.3462863763621196e+38\n",
      "Gradient Descent(20/49): loss=4.3250427430418696e+40\n",
      "Gradient Descent(21/49): loss=5.590075870958747e+42\n",
      "Gradient Descent(22/49): loss=7.225118941011866e+44\n",
      "Gradient Descent(23/49): loss=9.338396278869854e+46\n",
      "Gradient Descent(24/49): loss=1.206978677766935e+49\n",
      "Gradient Descent(25/49): loss=1.5600082552513705e+51\n",
      "Gradient Descent(26/49): loss=2.0162955661778756e+53\n",
      "Gradient Descent(27/49): loss=2.6060424978543883e+55\n",
      "Gradient Descent(28/49): loss=3.36828469721688e+57\n",
      "Gradient Descent(29/49): loss=4.353475359993672e+59\n",
      "Gradient Descent(30/49): loss=5.626824753184347e+61\n",
      "Gradient Descent(31/49): loss=7.272616515531222e+63\n",
      "Gradient Descent(32/49): loss=9.399786434088865e+65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=1.214913295892751e+68\n",
      "Gradient Descent(34/49): loss=1.5702636723576358e+70\n",
      "Gradient Descent(35/49): loss=2.0295505934966318e+72\n",
      "Gradient Descent(36/49): loss=2.6231744923309863e+74\n",
      "Gradient Descent(37/49): loss=3.3904276342088404e+76\n",
      "Gradient Descent(38/49): loss=4.382094891671643e+78\n",
      "Gradient Descent(39/49): loss=5.663815220788721e+80\n",
      "Gradient Descent(40/49): loss=7.320426336774477e+82\n",
      "Gradient Descent(41/49): loss=9.46158016515922e+84\n",
      "Gradient Descent(42/49): loss=1.222900075805951e+87\n",
      "Gradient Descent(43/49): loss=1.580586508068807e+89\n",
      "Gradient Descent(44/49): loss=2.0428927587093837e+91\n",
      "Gradient Descent(45/49): loss=2.640419111691906e+93\n",
      "Gradient Descent(46/49): loss=3.4127161377733666e+95\n",
      "Gradient Descent(47/49): loss=4.4109025667352906e+97\n",
      "Gradient Descent(48/49): loss=5.701048861897459e+99\n",
      "Gradient Descent(49/49): loss=7.368550457417744e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.059391171390203\n",
      "Gradient Descent(2/49): loss=435.595160477042\n",
      "Gradient Descent(3/49): loss=55204.11721462939\n",
      "Gradient Descent(4/49): loss=7106343.191178611\n",
      "Gradient Descent(5/49): loss=918612804.4594818\n",
      "Gradient Descent(6/49): loss=118896131865.44264\n",
      "Gradient Descent(7/49): loss=15394859257293.545\n",
      "Gradient Descent(8/49): loss=1993604647533617.2\n",
      "Gradient Descent(9/49): loss=2.5817857488278186e+17\n",
      "Gradient Descent(10/49): loss=3.3435447349015118e+19\n",
      "Gradient Descent(11/49): loss=4.3300802967384217e+21\n",
      "Gradient Descent(12/49): loss=5.60770771374124e+23\n",
      "Gradient Descent(13/49): loss=7.2623133794303254e+25\n",
      "Gradient Descent(14/49): loss=9.405127028609767e+27\n",
      "Gradient Descent(15/49): loss=1.2180198516542966e+30\n",
      "Gradient Descent(16/49): loss=1.5774081280493842e+32\n",
      "Gradient Descent(17/49): loss=2.0428373245115694e+34\n",
      "Gradient Descent(18/49): loss=2.6455958144400566e+36\n",
      "Gradient Descent(19/49): loss=3.426203904299884e+38\n",
      "Gradient Descent(20/49): loss=4.437137801543794e+40\n",
      "Gradient Descent(21/49): loss=5.746357315826605e+42\n",
      "Gradient Descent(22/49): loss=7.441874442182783e+44\n",
      "Gradient Descent(23/49): loss=9.637669948056834e+46\n",
      "Gradient Descent(24/49): loss=1.2481355705406277e+49\n",
      "Gradient Descent(25/49): loss=1.616409786645406e+51\n",
      "Gradient Descent(26/49): loss=2.0933467966399227e+53\n",
      "Gradient Descent(27/49): loss=2.7110085865647044e+55\n",
      "Gradient Descent(28/49): loss=3.51091733497033e+57\n",
      "Gradient Descent(29/49): loss=4.5468467322765e+59\n",
      "Gradient Descent(30/49): loss=5.888436905333237e+61\n",
      "Gradient Descent(31/49): loss=7.625875959696179e+63\n",
      "Gradient Descent(32/49): loss=9.87596285526999e+65\n",
      "Gradient Descent(33/49): loss=1.2789959190807365e+68\n",
      "Gradient Descent(34/49): loss=1.6563757731756436e+70\n",
      "Gradient Descent(35/49): loss=2.145105125851464e+72\n",
      "Gradient Descent(36/49): loss=2.7780387008028857e+74\n",
      "Gradient Descent(37/49): loss=3.597725318984186e+76\n",
      "Gradient Descent(38/49): loss=4.659268233779127e+78\n",
      "Gradient Descent(39/49): loss=6.034029435141164e+80\n",
      "Gradient Descent(40/49): loss=7.814426943738854e+82\n",
      "Gradient Descent(41/49): loss=1.0120147592154243e+85\n",
      "Gradient Descent(42/49): loss=1.310619294599011e+87\n",
      "Gradient Descent(43/49): loss=1.6973299250169963e+89\n",
      "Gradient Descent(44/49): loss=2.1981431879038792e+91\n",
      "Gradient Descent(45/49): loss=2.846726145171715e+93\n",
      "Gradient Descent(46/49): loss=3.686679644073587e+95\n",
      "Gradient Descent(47/49): loss=4.7744693746109015e+97\n",
      "Gradient Descent(48/49): loss=6.183221763176976e+99\n",
      "Gradient Descent(49/49): loss=8.007639880555534e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.073991636424367\n",
      "Gradient Descent(2/49): loss=433.9844912638113\n",
      "Gradient Descent(3/49): loss=54517.83557430711\n",
      "Gradient Descent(4/49): loss=6952836.296317048\n",
      "Gradient Descent(5/49): loss=890244333.1007608\n",
      "Gradient Descent(6/49): loss=114123708139.24887\n",
      "Gradient Descent(7/49): loss=14635451780293.377\n",
      "Gradient Descent(8/49): loss=1877106865365882.2\n",
      "Gradient Descent(9/49): loss=2.407625527355028e+17\n",
      "Gradient Descent(10/49): loss=3.0881219845758763e+19\n",
      "Gradient Descent(11/49): loss=3.960972004289864e+21\n",
      "Gradient Descent(12/49): loss=5.0805378681396686e+23\n",
      "Gradient Descent(13/49): loss=6.516551126363314e+25\n",
      "Gradient Descent(14/49): loss=8.358454537017028e+27\n",
      "Gradient Descent(15/49): loss=1.0720972525935015e+30\n",
      "Gradient Descent(16/49): loss=1.3751256669649705e+32\n",
      "Gradient Descent(17/49): loss=1.7638051072764603e+34\n",
      "Gradient Descent(18/49): loss=2.2623448433820898e+36\n",
      "Gradient Descent(19/49): loss=2.901796900385339e+38\n",
      "Gradient Descent(20/49): loss=3.721990163093375e+40\n",
      "Gradient Descent(21/49): loss=4.774011156030647e+42\n",
      "Gradient Descent(22/49): loss=6.123386016528197e+44\n",
      "Gradient Descent(23/49): loss=7.854161853032566e+46\n",
      "Gradient Descent(24/49): loss=1.0074141699910948e+49\n",
      "Gradient Descent(25/49): loss=1.2921599132919151e+51\n",
      "Gradient Descent(26/49): loss=1.6573890771595014e+53\n",
      "Gradient Descent(27/49): loss=2.1258503106552055e+55\n",
      "Gradient Descent(28/49): loss=2.7267221713915068e+57\n",
      "Gradient Descent(29/49): loss=3.4974305400019245e+59\n",
      "Gradient Descent(30/49): loss=4.485979727041977e+61\n",
      "Gradient Descent(31/49): loss=5.753942467552341e+63\n",
      "Gradient Descent(32/49): loss=7.380295037965612e+65\n",
      "Gradient Descent(33/49): loss=9.46633636929472e+67\n",
      "Gradient Descent(34/49): loss=1.2141997548289502e+70\n",
      "Gradient Descent(35/49): loss=1.5573934699898353e+72\n",
      "Gradient Descent(36/49): loss=1.9975909323986479e+74\n",
      "Gradient Descent(37/49): loss=2.5622102635548812e+76\n",
      "Gradient Descent(38/49): loss=3.286419320487716e+78\n",
      "Gradient Descent(39/49): loss=4.215326159489331e+80\n",
      "Gradient Descent(40/49): loss=5.4067886347010735e+82\n",
      "Gradient Descent(41/49): loss=6.935018130097467e+84\n",
      "Gradient Descent(42/49): loss=8.895201886773895e+86\n",
      "Gradient Descent(43/49): loss=1.1409431831630052e+89\n",
      "Gradient Descent(44/49): loss=1.4634309190235359e+91\n",
      "Gradient Descent(45/49): loss=1.8770698544487446e+93\n",
      "Gradient Descent(46/49): loss=2.4076238875909335e+95\n",
      "Gradient Descent(47/49): loss=3.0881390856925913e+97\n",
      "Gradient Descent(48/49): loss=3.9610019911061104e+99\n",
      "Gradient Descent(49/49): loss=5.0805797077717486e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.072374091205209\n",
      "Gradient Descent(2/49): loss=434.9927001939927\n",
      "Gradient Descent(3/49): loss=54724.53079418851\n",
      "Gradient Descent(4/49): loss=6988870.27986956\n",
      "Gradient Descent(5/49): loss=896189151.9395418\n",
      "Gradient Descent(6/49): loss=115064361191.54018\n",
      "Gradient Descent(7/49): loss=14779488248567.607\n",
      "Gradient Descent(8/49): loss=1898613053262935.5\n",
      "Gradient Descent(9/49): loss=2.4391189663894122e+17\n",
      "Gradient Descent(10/49): loss=3.133545451200349e+19\n",
      "Gradient Descent(11/49): loss=4.0256979357893777e+21\n",
      "Gradient Descent(12/49): loss=5.1718640221122006e+23\n",
      "Gradient Descent(13/49): loss=6.6443615157971015e+25\n",
      "Gradient Descent(14/49): loss=8.536099951056385e+27\n",
      "Gradient Descent(15/49): loss=1.0966442264193444e+30\n",
      "Gradient Descent(16/49): loss=1.4088735974149175e+32\n",
      "Gradient Descent(17/49): loss=1.809998885351766e+34\n",
      "Gradient Descent(18/49): loss=2.3253299506040543e+36\n",
      "Gradient Descent(19/49): loss=2.9873827145396293e+38\n",
      "Gradient Descent(20/49): loss=3.83793082059831e+40\n",
      "Gradient Descent(21/49): loss=4.93064143181556e+42\n",
      "Gradient Descent(22/49): loss=6.3344614757971644e+44\n",
      "Gradient Descent(23/49): loss=8.137967999425301e+46\n",
      "Gradient Descent(24/49): loss=1.0454957128230895e+49\n",
      "Gradient Descent(25/49): loss=1.3431624277827955e+51\n",
      "Gradient Descent(26/49): loss=1.7255788668291781e+53\n",
      "Gradient Descent(27/49): loss=2.216874418206514e+55\n",
      "Gradient Descent(28/49): loss=2.84804843207736e+57\n",
      "Gradient Descent(29/49): loss=3.658926191236694e+59\n",
      "Gradient Descent(30/49): loss=4.7006717730403786e+61\n",
      "Gradient Descent(31/49): loss=6.039016356979361e+63\n",
      "Gradient Descent(32/49): loss=7.758405674913926e+65\n",
      "Gradient Descent(33/49): loss=9.967328296266589e+67\n",
      "Gradient Descent(34/49): loss=1.2805160947794647e+70\n",
      "Gradient Descent(35/49): loss=1.6450962788126782e+72\n",
      "Gradient Descent(36/49): loss=2.1134773530741237e+74\n",
      "Gradient Descent(37/49): loss=2.7152128294771054e+76\n",
      "Gradient Descent(38/49): loss=3.4882705029385227e+78\n",
      "Gradient Descent(39/49): loss=4.4814281111121336e+80\n",
      "Gradient Descent(40/49): loss=5.757351070723401e+82\n",
      "Gradient Descent(41/49): loss=7.396546486904191e+84\n",
      "Gradient Descent(42/49): loss=9.502442922255302e+86\n",
      "Gradient Descent(43/49): loss=1.2207916444599191e+89\n",
      "Gradient Descent(44/49): loss=1.568367472845012e+91\n",
      "Gradient Descent(45/49): loss=2.014902822312856e+93\n",
      "Gradient Descent(46/49): loss=2.5885728017553125e+95\n",
      "Gradient Descent(47/49): loss=3.3255743531570213e+97\n",
      "Gradient Descent(48/49): loss=4.2724101755516957e+99\n",
      "Gradient Descent(49/49): loss=5.488822912899055e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.123245861425699\n",
      "Gradient Descent(2/49): loss=449.90222492485566\n",
      "Gradient Descent(3/49): loss=57748.27404830774\n",
      "Gradient Descent(4/49): loss=7519457.941095082\n",
      "Gradient Descent(5/49): loss=982734490.9170144\n",
      "Gradient Descent(6/49): loss=128575783199.62038\n",
      "Gradient Descent(7/49): loss=16827828049420.127\n",
      "Gradient Descent(8/49): loss=2202636353109403.8\n",
      "Gradient Descent(9/49): loss=2.8831820831073075e+17\n",
      "Gradient Descent(10/49): loss=3.7740353201187906e+19\n",
      "Gradient Descent(11/49): loss=4.940163500015594e+21\n",
      "Gradient Descent(12/49): loss=6.466617219178388e+23\n",
      "Gradient Descent(13/49): loss=8.464730504017256e+25\n",
      "Gradient Descent(14/49): loss=1.108023993904143e+28\n",
      "Gradient Descent(15/49): loss=1.4503913781199222e+30\n",
      "Gradient Descent(16/49): loss=1.89854658818123e+32\n",
      "Gradient Descent(17/49): loss=2.4851769079209644e+34\n",
      "Gradient Descent(18/49): loss=3.2530696423976106e+36\n",
      "Gradient Descent(19/49): loss=4.2582329122726336e+38\n",
      "Gradient Descent(20/49): loss=5.573980741431747e+40\n",
      "Gradient Descent(21/49): loss=7.296280393083758e+42\n",
      "Gradient Descent(22/49): loss=9.55075197505098e+44\n",
      "Gradient Descent(23/49): loss=1.2501830847365493e+47\n",
      "Gradient Descent(24/49): loss=1.6364761114581517e+49\n",
      "Gradient Descent(25/49): loss=2.1421294977275908e+51\n",
      "Gradient Descent(26/49): loss=2.8040243013058743e+53\n",
      "Gradient Descent(27/49): loss=3.670437427174667e+55\n",
      "Gradient Descent(28/49): loss=4.804562820846644e+57\n",
      "Gradient Descent(29/49): loss=6.289120672254827e+59\n",
      "Gradient Descent(30/49): loss=8.232390813700132e+61\n",
      "Gradient Descent(31/49): loss=1.0776110372389483e+64\n",
      "Gradient Descent(32/49): loss=1.4105811712032465e+66\n",
      "Gradient Descent(33/49): loss=1.8464354686374013e+68\n",
      "Gradient Descent(34/49): loss=2.4169640212438124e+70\n",
      "Gradient Descent(35/49): loss=3.1637797145968106e+72\n",
      "Gradient Descent(36/49): loss=4.1413533650133533e+74\n",
      "Gradient Descent(37/49): loss=5.4209866808293986e+76\n",
      "Gradient Descent(38/49): loss=7.096012825661074e+78\n",
      "Gradient Descent(39/49): loss=9.288603899362932e+80\n",
      "Gradient Descent(40/49): loss=1.2158681856838866e+83\n",
      "Gradient Descent(41/49): loss=1.5915582804210277e+85\n",
      "Gradient Descent(42/49): loss=2.0833325436112023e+87\n",
      "Gradient Descent(43/49): loss=2.7270597254668856e+89\n",
      "Gradient Descent(44/49): loss=3.569691631357434e+91\n",
      "Gradient Descent(45/49): loss=4.672687665761224e+93\n",
      "Gradient Descent(46/49): loss=6.116497523192046e+95\n",
      "Gradient Descent(47/49): loss=8.006428982049234e+97\n",
      "Gradient Descent(48/49): loss=1.0480328783186298e+100\n",
      "Gradient Descent(49/49): loss=1.3718636816731122e+102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.107133899277844\n",
      "Gradient Descent(2/49): loss=446.8011313936645\n",
      "Gradient Descent(3/49): loss=57346.36474227926\n",
      "Gradient Descent(4/49): loss=7476239.738563309\n",
      "Gradient Descent(5/49): loss=978758469.300745\n",
      "Gradient Descent(6/49): loss=128297491390.31192\n",
      "Gradient Descent(7/49): loss=16824188355297.65\n",
      "Gradient Descent(8/49): loss=2206508552537998.8\n",
      "Gradient Descent(9/49): loss=2.893976762058451e+17\n",
      "Gradient Descent(10/49): loss=3.795685952946705e+19\n",
      "Gradient Descent(11/49): loss=4.978372513461776e+21\n",
      "Gradient Descent(12/49): loss=6.529578078423161e+23\n",
      "Gradient Descent(13/49): loss=8.564125979957125e+25\n",
      "Gradient Descent(14/49): loss=1.123261929205529e+28\n",
      "Gradient Descent(15/49): loss=1.473258830385887e+30\n",
      "Gradient Descent(16/49): loss=1.9323112079820887e+32\n",
      "Gradient Descent(17/49): loss=2.5343996224384337e+34\n",
      "Gradient Descent(18/49): loss=3.3240926359545045e+36\n",
      "Gradient Descent(19/49): loss=4.35984592189251e+38\n",
      "Gradient Descent(20/49): loss=5.718329345083351e+40\n",
      "Gradient Descent(21/49): loss=7.500102317032642e+42\n",
      "Gradient Descent(22/49): loss=9.837057534182966e+44\n",
      "Gradient Descent(23/49): loss=1.2902184109094766e+47\n",
      "Gradient Descent(24/49): loss=1.6922372793575998e+49\n",
      "Gradient Descent(25/49): loss=2.219521117847645e+51\n",
      "Gradient Descent(26/49): loss=2.9111012106072686e+53\n",
      "Gradient Descent(27/49): loss=3.818170591058745e+55\n",
      "Gradient Descent(28/49): loss=5.007873518552465e+57\n",
      "Gradient Descent(29/49): loss=6.56827571731545e+59\n",
      "Gradient Descent(30/49): loss=8.614883291051334e+61\n",
      "Gradient Descent(31/49): loss=1.1299192864694297e+64\n",
      "Gradient Descent(32/49): loss=1.4819905862935688e+66\n",
      "Gradient Descent(33/49): loss=1.943763704330901e+68\n",
      "Gradient Descent(34/49): loss=2.549420605783642e+70\n",
      "Gradient Descent(35/49): loss=3.3437940068088356e+72\n",
      "Gradient Descent(36/49): loss=4.3856860396458396e+74\n",
      "Gradient Descent(37/49): loss=5.7522209798744896e+76\n",
      "Gradient Descent(38/49): loss=7.544554238994313e+78\n",
      "Gradient Descent(39/49): loss=9.895360220734923e+80\n",
      "Gradient Descent(40/49): loss=1.2978653316853326e+83\n",
      "Gradient Descent(41/49): loss=1.7022669024832956e+85\n",
      "Gradient Descent(42/49): loss=2.2326758690189002e+87\n",
      "Gradient Descent(43/49): loss=2.9283548477781784e+89\n",
      "Gradient Descent(44/49): loss=3.840800285208499e+91\n",
      "Gradient Descent(45/49): loss=5.037554394082524e+93\n",
      "Gradient Descent(46/49): loss=6.607204850267949e+95\n",
      "Gradient Descent(47/49): loss=8.66594234390498e+97\n",
      "Gradient Descent(48/49): loss=1.136616139650031e+100\n",
      "Gradient Descent(49/49): loss=1.4907741104711732e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.121976114289956\n",
      "Gradient Descent(2/49): loss=445.1528462579879\n",
      "Gradient Descent(3/49): loss=56634.238906372724\n",
      "Gradient Descent(4/49): loss=7314883.750777458\n",
      "Gradient Descent(5/49): loss=948555806.4534311\n",
      "Gradient Descent(6/49): loss=123151336703.72977\n",
      "Gradient Descent(7/49): loss=15994825490428.947\n",
      "Gradient Descent(8/49): loss=2077651471541524.5\n",
      "Gradient Descent(9/49): loss=2.6988766876082512e+17\n",
      "Gradient Descent(10/49): loss=3.5058956438157152e+19\n",
      "Gradient Descent(11/49): loss=4.554248843939885e+21\n",
      "Gradient Descent(12/49): loss=5.916094834060722e+23\n",
      "Gradient Descent(13/49): loss=7.68517379324171e+25\n",
      "Gradient Descent(14/49): loss=9.983258653239363e+27\n",
      "Gradient Descent(15/49): loss=1.2968536669563624e+30\n",
      "Gradient Descent(16/49): loss=1.684649790707243e+32\n",
      "Gradient Descent(17/49): loss=2.188407994149056e+34\n",
      "Gradient Descent(18/49): loss=2.8428042334853663e+36\n",
      "Gradient Descent(19/49): loss=3.692883564676649e+38\n",
      "Gradient Descent(20/49): loss=4.797160797812701e+40\n",
      "Gradient Descent(21/49): loss=6.231648336156503e+42\n",
      "Gradient Descent(22/49): loss=8.095088453983654e+44\n",
      "Gradient Descent(23/49): loss=1.0515750174480059e+47\n",
      "Gradient Descent(24/49): loss=1.3660258607537468e+49\n",
      "Gradient Descent(25/49): loss=1.774506451073572e+51\n",
      "Gradient Descent(26/49): loss=2.305134357533267e+53\n",
      "Gradient Descent(27/49): loss=2.9944351022595274e+55\n",
      "Gradient Descent(28/49): loss=3.889856377499618e+57\n",
      "Gradient Descent(29/49): loss=5.0530340851792545e+59\n",
      "Gradient Descent(30/49): loss=6.564035015194045e+61\n",
      "Gradient Descent(31/49): loss=8.52686820519734e+63\n",
      "Gradient Descent(32/49): loss=1.1076644353740727e+66\n",
      "Gradient Descent(33/49): loss=1.4388876101599916e+68\n",
      "Gradient Descent(34/49): loss=1.8691559361773e+70\n",
      "Gradient Descent(35/49): loss=2.4280867310813533e+72\n",
      "Gradient Descent(36/49): loss=3.1541537330003275e+74\n",
      "Gradient Descent(37/49): loss=4.097335422186179e+76\n",
      "Gradient Descent(38/49): loss=5.322555266173494e+78\n",
      "Gradient Descent(39/49): loss=6.914150696101797e+80\n",
      "Gradient Descent(40/49): loss=8.981678434082931e+82\n",
      "Gradient Descent(41/49): loss=1.1667455778589391e+85\n",
      "Gradient Descent(42/49): loss=1.5156356948692982e+87\n",
      "Gradient Descent(43/49): loss=1.9688538813897876e+89\n",
      "Gradient Descent(44/49): loss=2.557597197918913e+91\n",
      "Gradient Descent(45/49): loss=3.3223915134755095e+93\n",
      "Gradient Descent(46/49): loss=4.315881084713328e+95\n",
      "Gradient Descent(47/49): loss=5.606452298543511e+97\n",
      "Gradient Descent(48/49): loss=7.282941016882003e+99\n",
      "Gradient Descent(49/49): loss=9.460747551381411e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.120007822099196\n",
      "Gradient Descent(2/49): loss=446.1202099300804\n",
      "Gradient Descent(3/49): loss=56836.2914029543\n",
      "Gradient Descent(4/49): loss=7350610.072555231\n",
      "Gradient Descent(5/49): loss=954535288.382082\n",
      "Gradient Descent(6/49): loss=124111031163.75665\n",
      "Gradient Descent(7/49): loss=16143836357272.805\n",
      "Gradient Descent(8/49): loss=2100205689420426.5\n",
      "Gradient Descent(9/49): loss=2.7323508025100576e+17\n",
      "Gradient Descent(10/49): loss=3.5548199728421188e+19\n",
      "Gradient Descent(11/49): loss=4.6248851301260035e+21\n",
      "Gradient Descent(12/49): loss=6.01706940195693e+23\n",
      "Gradient Descent(13/49): loss=7.828333805168217e+25\n",
      "Gradient Descent(14/49): loss=1.0184828801203333e+28\n",
      "Gradient Descent(15/49): loss=1.3250679755992042e+30\n",
      "Gradient Descent(16/49): loss=1.7239417681522783e+32\n",
      "Gradient Descent(17/49): loss=2.242885116702485e+34\n",
      "Gradient Descent(18/49): loss=2.918041520676148e+36\n",
      "Gradient Descent(19/49): loss=3.796434449436615e+38\n",
      "Gradient Descent(20/49): loss=4.939242444150883e+40\n",
      "Gradient Descent(21/49): loss=6.426060096442515e+42\n",
      "Gradient Descent(22/49): loss=8.360441673242774e+44\n",
      "Gradient Descent(23/49): loss=1.0877113491529717e+47\n",
      "Gradient Descent(24/49): loss=1.415135737226268e+49\n",
      "Gradient Descent(25/49): loss=1.8411218714750603e+51\n",
      "Gradient Descent(26/49): loss=2.3953389462619014e+53\n",
      "Gradient Descent(27/49): loss=3.116387218236084e+55\n",
      "Gradient Descent(28/49): loss=4.054486447165112e+57\n",
      "Gradient Descent(29/49): loss=5.274973614976654e+59\n",
      "Gradient Descent(30/49): loss=6.862853533067139e+61\n",
      "Gradient Descent(31/49): loss=8.928719279772298e+63\n",
      "Gradient Descent(32/49): loss=1.161645481618615e+66\n",
      "Gradient Descent(33/49): loss=1.5113256254141634e+68\n",
      "Gradient Descent(34/49): loss=1.9662669740262733e+70\n",
      "Gradient Descent(35/49): loss=2.558155402206559e+72\n",
      "Gradient Descent(36/49): loss=3.328214910937683e+74\n",
      "Gradient Descent(37/49): loss=4.330078807500682e+76\n",
      "Gradient Descent(38/49): loss=5.633525172172254e+78\n",
      "Gradient Descent(39/49): loss=7.329336780319842e+80\n",
      "Gradient Descent(40/49): loss=9.535623965027135e+82\n",
      "Gradient Descent(41/49): loss=1.2406050796649534e+85\n",
      "Gradient Descent(42/49): loss=1.6140537518418309e+87\n",
      "Gradient Descent(43/49): loss=2.0999184644143644e+89\n",
      "Gradient Descent(44/49): loss=2.732038850723801e+91\n",
      "Gradient Descent(45/49): loss=3.5544409977583795e+93\n",
      "Gradient Descent(46/49): loss=4.6244037866439424e+95\n",
      "Gradient Descent(47/49): loss=6.016448267227793e+97\n",
      "Gradient Descent(48/49): loss=7.827527919766259e+99\n",
      "Gradient Descent(49/49): loss=1.0183781296427885e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.171545477988517\n",
      "Gradient Descent(2/49): loss=461.40179035921363\n",
      "Gradient Descent(3/49): loss=59974.89254344193\n",
      "Gradient Descent(4/49): loss=7908350.885288319\n",
      "Gradient Descent(5/49): loss=1046664791.2014031\n",
      "Gradient Descent(6/49): loss=138676814251.39734\n",
      "Gradient Descent(7/49): loss=18380041422894.26\n",
      "Gradient Descent(8/49): loss=2436324593634313.0\n",
      "Gradient Descent(9/49): loss=3.229523228692822e+17\n",
      "Gradient Descent(10/49): loss=4.281010800649605e+19\n",
      "Gradient Descent(11/49): loss=5.674867374721649e+21\n",
      "Gradient Descent(12/49): loss=7.522558637838286e+23\n",
      "Gradient Descent(13/49): loss=9.971846808421916e+25\n",
      "Gradient Descent(14/49): loss=1.3218606139718035e+28\n",
      "Gradient Descent(15/49): loss=1.752248683901172e+30\n",
      "Gradient Descent(16/49): loss=2.3227679624213818e+32\n",
      "Gradient Descent(17/49): loss=3.0790441312788782e+34\n",
      "Gradient Descent(18/49): loss=4.081558264387592e+36\n",
      "Gradient Descent(19/49): loss=5.410483631097678e+38\n",
      "Gradient Descent(20/49): loss=7.172097328911117e+40\n",
      "Gradient Descent(21/49): loss=9.507279497464051e+42\n",
      "Gradient Descent(22/49): loss=1.2602779814437189e+45\n",
      "Gradient Descent(23/49): loss=1.67061522798565e+47\n",
      "Gradient Descent(24/49): loss=2.214555265643664e+49\n",
      "Gradient Descent(25/49): loss=2.9355981810996012e+51\n",
      "Gradient Descent(26/49): loss=3.89140736949348e+53\n",
      "Gradient Descent(27/49): loss=5.158421003543653e+55\n",
      "Gradient Descent(28/49): loss=6.837964963114155e+57\n",
      "Gradient Descent(29/49): loss=9.064356089713517e+59\n",
      "Gradient Descent(30/49): loss=1.2015643801092025e+62\n",
      "Gradient Descent(31/49): loss=1.5927849096590768e+64\n",
      "Gradient Descent(32/49): loss=2.1113839677963125e+66\n",
      "Gradient Descent(33/49): loss=2.7988350670785026e+68\n",
      "Gradient Descent(34/49): loss=3.710115190883182e+70\n",
      "Gradient Descent(35/49): loss=4.918101424243774e+72\n",
      "Gradient Descent(36/49): loss=6.519399095366299e+74\n",
      "Gradient Descent(37/49): loss=8.642067517181867e+76\n",
      "Gradient Descent(38/49): loss=1.1455861173557066e+79\n",
      "Gradient Descent(39/49): loss=1.5185805360452433e+81\n",
      "Gradient Descent(40/49): loss=2.0130191956048473e+83\n",
      "Gradient Descent(41/49): loss=2.668443448133894e+85\n",
      "Gradient Descent(42/49): loss=3.537269019309664e+87\n",
      "Gradient Descent(43/49): loss=4.6889778097857393e+89\n",
      "Gradient Descent(44/49): loss=6.2156745163120104e+91\n",
      "Gradient Descent(45/49): loss=8.239452447845113e+93\n",
      "Gradient Descent(46/49): loss=1.0922157597238804e+96\n",
      "Gradient Descent(47/49): loss=1.4478331822901633e+98\n",
      "Gradient Descent(48/49): loss=1.9192370235258373e+100\n",
      "Gradient Descent(49/49): loss=2.544126490211982e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.15518973850082\n",
      "Gradient Descent(2/49): loss=458.22217757015443\n",
      "Gradient Descent(3/49): loss=59557.44881885226\n",
      "Gradient Descent(4/49): loss=7862875.098734679\n",
      "Gradient Descent(5/49): loss=1042425432.4162612\n",
      "Gradient Descent(6/49): loss=138375858214.1112\n",
      "Gradient Descent(7/49): loss=18375938796187.598\n",
      "Gradient Descent(8/49): loss=2440588142001716.5\n",
      "Gradient Descent(9/49): loss=3.241585434412489e+17\n",
      "Gradient Descent(10/49): loss=4.3055267088136086e+19\n",
      "Gradient Descent(11/49): loss=5.718695849528601e+21\n",
      "Gradient Descent(12/49): loss=7.595709145878224e+23\n",
      "Gradient Descent(13/49): loss=1.0088807926592453e+26\n",
      "Gradient Descent(14/49): loss=1.3400205124768045e+28\n",
      "Gradient Descent(15/49): loss=1.7798485949180311e+30\n",
      "Gradient Descent(16/49): loss=2.3640392377913984e+32\n",
      "Gradient Descent(17/49): loss=3.139975817327959e+34\n",
      "Gradient Descent(18/49): loss=4.1705941222399015e+36\n",
      "Gradient Descent(19/49): loss=5.539487038695956e+38\n",
      "Gradient Descent(20/49): loss=7.357684722582252e+40\n",
      "Gradient Descent(21/49): loss=9.77266019432182e+42\n",
      "Gradient Descent(22/49): loss=1.298029079478467e+45\n",
      "Gradient Descent(23/49): loss=1.7240745689294417e+47\n",
      "Gradient Descent(24/49): loss=2.2899588046435509e+49\n",
      "Gradient Descent(25/49): loss=3.0415803477827824e+51\n",
      "Gradient Descent(26/49): loss=4.039902811029125e+53\n",
      "Gradient Descent(27/49): loss=5.365899583898717e+55\n",
      "Gradient Descent(28/49): loss=7.127121540122026e+57\n",
      "Gradient Descent(29/49): loss=9.46642043024679e+59\n",
      "Gradient Descent(30/49): loss=1.2573535509071614e+62\n",
      "Gradient Descent(31/49): loss=1.6700483182930387e+64\n",
      "Gradient Descent(32/49): loss=2.2181997922709675e+66\n",
      "Gradient Descent(33/49): loss=2.946268239388481e+68\n",
      "Gradient Descent(34/49): loss=3.913306893578936e+70\n",
      "Gradient Descent(35/49): loss=5.197751731699397e+72\n",
      "Gradient Descent(36/49): loss=6.903783372756612e+74\n",
      "Gradient Descent(37/49): loss=9.169777110990943e+76\n",
      "Gradient Descent(38/49): loss=1.2179526460384713e+79\n",
      "Gradient Descent(39/49): loss=1.6177150546157708e+81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=2.1486894473628447e+83\n",
      "Gradient Descent(41/49): loss=2.853942867154051e+85\n",
      "Gradient Descent(42/49): loss=3.7906780335223103e+87\n",
      "Gradient Descent(43/49): loss=5.034873023985086e+89\n",
      "Gradient Descent(44/49): loss=6.68744381439785e+91\n",
      "Gradient Descent(45/49): loss=8.882429518615934e+93\n",
      "Gradient Descent(46/49): loss=1.1797864227780976e+96\n",
      "Gradient Descent(47/49): loss=1.5670217258176818e+98\n",
      "Gradient Descent(48/49): loss=2.0813573048267553e+100\n",
      "Gradient Descent(49/49): loss=2.764510637588778e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.170275521713454\n",
      "Gradient Descent(2/49): loss=456.53562765597565\n",
      "Gradient Descent(3/49): loss=58818.67343566171\n",
      "Gradient Descent(4/49): loss=7693321.168920245\n",
      "Gradient Descent(5/49): loss=1010282543.7999319\n",
      "Gradient Descent(6/49): loss=132829319682.82608\n",
      "Gradient Descent(7/49): loss=17470675842913.164\n",
      "Gradient Descent(8/49): loss=2298150587216541.5\n",
      "Gradient Descent(9/49): loss=3.0231834049106694e+17\n",
      "Gradient Descent(10/49): loss=3.977004973051974e+19\n",
      "Gradient Descent(11/49): loss=5.231781688884203e+21\n",
      "Gradient Descent(12/49): loss=6.882459990319136e+23\n",
      "Gradient Descent(13/49): loss=9.053947555785907e+25\n",
      "Gradient Descent(14/49): loss=1.19105637596271e+28\n",
      "Gradient Descent(15/49): loss=1.5668473349364917e+30\n",
      "Gradient Descent(16/49): loss=2.0612043726259947e+32\n",
      "Gradient Descent(17/49): loss=2.711536340018709e+34\n",
      "Gradient Descent(18/49): loss=3.567054986626165e+36\n",
      "Gradient Descent(19/49): loss=4.692498897013818e+38\n",
      "Gradient Descent(20/49): loss=6.173032371263707e+40\n",
      "Gradient Descent(21/49): loss=8.120689955454781e+42\n",
      "Gradient Descent(22/49): loss=1.0682854290690108e+45\n",
      "Gradient Descent(23/49): loss=1.4053408814135845e+47\n",
      "Gradient Descent(24/49): loss=1.8487409256322535e+49\n",
      "Gradient Descent(25/49): loss=2.432038415243471e+51\n",
      "Gradient Descent(26/49): loss=3.1993724870879532e+53\n",
      "Gradient Descent(27/49): loss=4.2088086466805855e+55\n",
      "Gradient Descent(28/49): loss=5.536732686132792e+57\n",
      "Gradient Descent(29/49): loss=7.283630930065895e+59\n",
      "Gradient Descent(30/49): loss=9.581694210790371e+61\n",
      "Gradient Descent(31/49): loss=1.2604820978794853e+64\n",
      "Gradient Descent(32/49): loss=1.65817765013356e+66\n",
      "Gradient Descent(33/49): loss=2.1813503928600353e+68\n",
      "Gradient Descent(34/49): loss=2.869589718597029e+70\n",
      "Gradient Descent(35/49): loss=3.774975895679559e+72\n",
      "Gradient Descent(36/49): loss=4.966021072841331e+74\n",
      "Gradient Descent(37/49): loss=6.532853712822098e+76\n",
      "Gradient Descent(38/49): loss=8.594038770100283e+78\n",
      "Gradient Descent(39/49): loss=1.1305549707477172e+81\n",
      "Gradient Descent(40/49): loss=1.4872571279631895e+83\n",
      "Gradient Descent(41/49): loss=1.9565026220833954e+85\n",
      "Gradient Descent(42/49): loss=2.5738000768310584e+87\n",
      "Gradient Descent(43/49): loss=3.385861465619437e+89\n",
      "Gradient Descent(44/49): loss=4.4541368879285735e+91\n",
      "Gradient Descent(45/49): loss=5.859464605347199e+93\n",
      "Gradient Descent(46/49): loss=7.708188213605503e+95\n",
      "Gradient Descent(47/49): loss=1.01402038476596e+98\n",
      "Gradient Descent(48/49): loss=1.3339546365850103e+100\n",
      "Gradient Descent(49/49): loss=1.7548315588126522e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.16795212078607\n",
      "Gradient Descent(2/49): loss=457.460094082487\n",
      "Gradient Descent(3/49): loss=59015.53174757131\n",
      "Gradient Descent(4/49): loss=7728628.251444472\n",
      "Gradient Descent(5/49): loss=1016278201.0166556\n",
      "Gradient Descent(6/49): loss=133805488330.93634\n",
      "Gradient Descent(7/49): loss=17624376267637.773\n",
      "Gradient Descent(8/49): loss=2321734341549202.0\n",
      "Gradient Descent(9/49): loss=3.0586580266497734e+17\n",
      "Gradient Descent(10/49): loss=4.029543824215406e+19\n",
      "Gradient Descent(11/49): loss=5.308637133190947e+21\n",
      "Gradient Descent(12/49): loss=6.993763367428046e+23\n",
      "Gradient Descent(13/49): loss=9.213806218815107e+25\n",
      "Gradient Descent(14/49): loss=1.2138563566916537e+28\n",
      "Gradient Descent(15/49): loss=1.5991733641571104e+30\n",
      "Gradient Descent(16/49): loss=2.1068024146965674e+32\n",
      "Gradient Descent(17/49): loss=2.7755692696670767e+34\n",
      "Gradient Descent(18/49): loss=3.6566242451190204e+36\n",
      "Gradient Descent(19/49): loss=4.817354416948432e+38\n",
      "Gradient Descent(20/49): loss=6.346537688619553e+40\n",
      "Gradient Descent(21/49): loss=8.361132927029702e+42\n",
      "Gradient Descent(22/49): loss=1.1015225506490482e+45\n",
      "Gradient Descent(23/49): loss=1.4511812456398652e+47\n",
      "Gradient Descent(24/49): loss=1.9118328594028355e+49\n",
      "Gradient Descent(25/49): loss=2.5187101151413735e+51\n",
      "Gradient Descent(26/49): loss=3.3182297358878174e+53\n",
      "Gradient Descent(27/49): loss=4.3715426058522566e+55\n",
      "Gradient Descent(28/49): loss=5.759210867197328e+57\n",
      "Gradient Descent(29/49): loss=7.5873696777977e+59\n",
      "Gradient Descent(30/49): loss=9.995844909144519e+61\n",
      "Gradient Descent(31/49): loss=1.3168847662721485e+64\n",
      "Gradient Descent(32/49): loss=1.734906356993572e+66\n",
      "Gradient Descent(33/49): loss=2.285621448911698e+68\n",
      "Gradient Descent(34/49): loss=3.011151228229993e+70\n",
      "Gradient Descent(35/49): loss=3.96698750074659e+72\n",
      "Gradient Descent(36/49): loss=5.226236956663955e+74\n",
      "Gradient Descent(37/49): loss=6.885212701592743e+76\n",
      "Gradient Descent(38/49): loss=9.07080071938315e+78\n",
      "Gradient Descent(39/49): loss=1.1950164687247511e+81\n",
      "Gradient Descent(40/49): loss=1.5743531411419743e+83\n",
      "Gradient Descent(41/49): loss=2.0741034771416997e+85\n",
      "Gradient Descent(42/49): loss=2.7324906474101714e+87\n",
      "Gradient Descent(43/49): loss=3.599871086698941e+89\n",
      "Gradient Descent(44/49): loss=4.742585982182038e+91\n",
      "Gradient Descent(45/49): loss=6.248035348125368e+93\n",
      "Gradient Descent(46/49): loss=8.231362775095759e+95\n",
      "Gradient Descent(47/49): loss=1.084426213362585e+98\n",
      "Gradient Descent(48/49): loss=1.4286579808945916e+100\n",
      "Gradient Descent(49/49): loss=1.882159985827791e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.2201599411830975\n",
      "Gradient Descent(2/49): loss=473.12064690187714\n",
      "Gradient Descent(3/49): loss=62272.6029030649\n",
      "Gradient Descent(4/49): loss=8314730.256342322\n",
      "Gradient Descent(5/49): loss=1114314052.8925822\n",
      "Gradient Descent(6/49): loss=149500565231.6993\n",
      "Gradient Descent(7/49): loss=20064342211714.09\n",
      "Gradient Descent(8/49): loss=2693104679160621.5\n",
      "Gradient Descent(9/49): loss=3.6148993614896506e+17\n",
      "Gradient Descent(10/49): loss=4.852257730386683e+19\n",
      "Gradient Descent(11/49): loss=6.513178807982649e+21\n",
      "Gradient Descent(12/49): loss=8.742640445616903e+23\n",
      "Gradient Descent(13/49): loss=1.1735251092293935e+26\n",
      "Gradient Descent(14/49): loss=1.5752235775500443e+28\n",
      "Gradient Descent(15/49): loss=2.114423788969806e+30\n",
      "Gradient Descent(16/49): loss=2.8381926693428877e+32\n",
      "Gradient Descent(17/49): loss=3.809708204995363e+34\n",
      "Gradient Descent(18/49): loss=5.113774262361991e+36\n",
      "Gradient Descent(19/49): loss=6.864223139670981e+38\n",
      "Gradient Descent(20/49): loss=9.213852019942433e+40\n",
      "Gradient Descent(21/49): loss=1.2367760680444493e+43\n",
      "Gradient Descent(22/49): loss=1.6601254710791396e+45\n",
      "Gradient Descent(23/49): loss=2.228387701658987e+47\n",
      "Gradient Descent(24/49): loss=2.9911665325412237e+49\n",
      "Gradient Descent(25/49): loss=4.015045146200247e+51\n",
      "Gradient Descent(26/49): loss=5.389398199883081e+53\n",
      "Gradient Descent(27/49): loss=7.234193365021617e+55\n",
      "Gradient Descent(28/49): loss=9.71046333960977e+57\n",
      "Gradient Descent(29/49): loss=1.3034362438503052e+60\n",
      "Gradient Descent(30/49): loss=1.7496034765431413e+62\n",
      "Gradient Descent(31/49): loss=2.3484940974860486e+64\n",
      "Gradient Descent(32/49): loss=3.1523854403994382e+66\n",
      "Gradient Descent(33/49): loss=4.231449410701129e+68\n",
      "Gradient Descent(34/49): loss=5.67987781121527e+70\n",
      "Gradient Descent(35/49): loss=7.62410437160113e+72\n",
      "Gradient Descent(36/49): loss=1.0233841184803683e+75\n",
      "Gradient Descent(37/49): loss=1.373689292422288e+77\n",
      "Gradient Descent(38/49): loss=1.8439041978858264e+79\n",
      "Gradient Descent(39/49): loss=2.4750740285568146e+81\n",
      "Gradient Descent(40/49): loss=3.3222937796119426e+83\n",
      "Gradient Descent(41/49): loss=4.459517505617468e+85\n",
      "Gradient Descent(42/49): loss=5.986013791119813e+87\n",
      "Gradient Descent(43/49): loss=8.035030933803937e+89\n",
      "Gradient Descent(44/49): loss=1.0785428226537406e+92\n",
      "Gradient Descent(45/49): loss=1.4477288636239243e+94\n",
      "Gradient Descent(46/49): loss=1.9432875714780036e+96\n",
      "Gradient Descent(47/49): loss=2.6084764076665188e+98\n",
      "Gradient Descent(48/49): loss=3.501359896094948e+100\n",
      "Gradient Descent(49/49): loss=4.699878092035015e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.203558689059127\n",
      "Gradient Descent(2/49): loss=469.8610343720669\n",
      "Gradient Descent(3/49): loss=61839.12780141692\n",
      "Gradient Descent(4/49): loss=8266894.39479971\n",
      "Gradient Descent(5/49): loss=1109795769.1091232\n",
      "Gradient Descent(6/49): loss=149175266516.97626\n",
      "Gradient Descent(7/49): loss=20059726512604.434\n",
      "Gradient Descent(8/49): loss=2697796336563835.0\n",
      "Gradient Descent(9/49): loss=3.628368685956033e+17\n",
      "Gradient Descent(10/49): loss=4.879996728402985e+19\n",
      "Gradient Descent(11/49): loss=6.563410371799257e+21\n",
      "Gradient Descent(12/49): loss=8.827550430269302e+23\n",
      "Gradient Descent(13/49): loss=1.1872742730899658e+26\n",
      "Gradient Descent(14/49): loss=1.5968420951421522e+28\n",
      "Gradient Descent(15/49): loss=2.1476965010074136e+30\n",
      "Gradient Descent(16/49): loss=2.8885763646585387e+32\n",
      "Gradient Descent(17/49): loss=3.8850337810297546e+34\n",
      "Gradient Descent(18/49): loss=5.2252340252846425e+36\n",
      "Gradient Descent(19/49): loss=7.027756300940337e+38\n",
      "Gradient Descent(20/49): loss=9.45208547507012e+40\n",
      "Gradient Descent(21/49): loss=1.2712723095032658e+43\n",
      "Gradient Descent(22/49): loss=1.7098166211074563e+45\n",
      "Gradient Descent(23/49): loss=2.2996433226612122e+47\n",
      "Gradient Descent(24/49): loss=3.092939527073715e+49\n",
      "Gradient Descent(25/49): loss=4.159895068888734e+51\n",
      "Gradient Descent(26/49): loss=5.594912811160185e+53\n",
      "Gradient Descent(27/49): loss=7.524961290153098e+55\n",
      "Gradient Descent(28/49): loss=1.0120808729200165e+58\n",
      "Gradient Descent(29/49): loss=1.36121323929058e+60\n",
      "Gradient Descent(30/49): loss=1.8307840138051793e+62\n",
      "Gradient Descent(31/49): loss=2.462340218606346e+64\n",
      "Gradient Descent(32/49): loss=3.3117611397340326e+66\n",
      "Gradient Descent(33/49): loss=4.454202454955702e+68\n",
      "Gradient Descent(34/49): loss=5.990745912106045e+70\n",
      "Gradient Descent(35/49): loss=8.05734291297994e+72\n",
      "Gradient Descent(36/49): loss=1.0836843319653549e+75\n",
      "Gradient Descent(37/49): loss=1.457517377664729e+77\n",
      "Gradient Descent(38/49): loss=1.9603096986204118e+79\n",
      "Gradient Descent(39/49): loss=2.6365477169557383e+81\n",
      "Gradient Descent(40/49): loss=3.546064108480718e+83\n",
      "Gradient Descent(41/49): loss=4.769331721397501e+85\n",
      "Gradient Descent(42/49): loss=6.414583711086402e+87\n",
      "Gradient Descent(43/49): loss=8.627389871400662e+89\n",
      "Gradient Descent(44/49): loss=1.1603536464027307e+92\n",
      "Gradient Descent(45/49): loss=1.5606349136758385e+94\n",
      "Gradient Descent(46/49): loss=2.0989991640347404e+96\n",
      "Gradient Descent(47/49): loss=2.823080178464896e+98\n",
      "Gradient Descent(48/49): loss=3.796943719940164e+100\n",
      "Gradient Descent(49/49): loss=5.106755990271755e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.218889858694859\n",
      "Gradient Descent(2/49): loss=468.1355635065553\n",
      "Gradient Descent(3/49): loss=61072.87758304339\n",
      "Gradient Descent(4/49): loss=8088780.471747794\n",
      "Gradient Descent(5/49): loss=1075601207.130343\n",
      "Gradient Descent(6/49): loss=143199974502.15628\n",
      "Gradient Descent(7/49): loss=19072155491932.06\n",
      "Gradient Descent(8/49): loss=2540445252689699.5\n",
      "Gradient Descent(9/49): loss=3.384053462826893e+17\n",
      "Gradient Descent(10/49): loss=4.507858212458866e+19\n",
      "Gradient Descent(11/49): loss=6.004891033114681e+21\n",
      "Gradient Descent(12/49): loss=7.999090694036536e+23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=1.0655560752341693e+26\n",
      "Gradient Descent(14/49): loss=1.4194237360300732e+28\n",
      "Gradient Descent(15/49): loss=1.8908097736856677e+30\n",
      "Gradient Descent(16/49): loss=2.5187416323481364e+32\n",
      "Gradient Descent(17/49): loss=3.3552076641062114e+34\n",
      "Gradient Descent(18/49): loss=4.4694613946074284e+36\n",
      "Gradient Descent(19/49): loss=5.9537552274118625e+38\n",
      "Gradient Descent(20/49): loss=7.930978295795925e+40\n",
      "Gradient Descent(21/49): loss=1.0564830821835036e+43\n",
      "Gradient Descent(22/49): loss=1.4073377348087387e+45\n",
      "Gradient Descent(23/49): loss=1.8747100954386624e+47\n",
      "Gradient Descent(24/49): loss=2.497295322238232e+49\n",
      "Gradient Descent(25/49): loss=3.326639111642507e+51\n",
      "Gradient Descent(26/49): loss=4.431405321014935e+53\n",
      "Gradient Descent(27/49): loss=5.903060855141936e+55\n",
      "Gradient Descent(28/49): loss=7.863448485350666e+57\n",
      "Gradient Descent(29/49): loss=1.0474874577635939e+60\n",
      "Gradient Descent(30/49): loss=1.395354692303446e+62\n",
      "Gradient Descent(31/49): loss=1.858747522848774e+64\n",
      "Gradient Descent(32/49): loss=2.476031630347007e+66\n",
      "Gradient Descent(33/49): loss=3.2983138156831234e+68\n",
      "Gradient Descent(34/49): loss=4.393673284860834e+70\n",
      "Gradient Descent(35/49): loss=5.85279813045979e+72\n",
      "Gradient Descent(36/49): loss=7.796493670557194e+74\n",
      "Gradient Descent(37/49): loss=1.0385684282991487e+77\n",
      "Gradient Descent(38/49): loss=1.3834736816796244e+79\n",
      "Gradient Descent(39/49): loss=1.8429208665958687e+81\n",
      "Gradient Descent(40/49): loss=2.4549489921709694e+83\n",
      "Gradient Descent(41/49): loss=3.270229700797492e+85\n",
      "Gradient Descent(42/49): loss=4.356262525243246e+87\n",
      "Gradient Descent(43/49): loss=5.802963377224208e+89\n",
      "Gradient Descent(44/49): loss=7.73010895515874e+91\n",
      "Gradient Descent(45/49): loss=1.029725341592774e+94\n",
      "Gradient Descent(46/49): loss=1.371693834160945e+96\n",
      "Gradient Descent(47/49): loss=1.8272289693917623e+98\n",
      "Gradient Descent(48/49): loss=2.4340458660928458e+100\n",
      "Gradient Descent(49/49): loss=3.242384713403378e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.21620698726583\n",
      "Gradient Descent(2/49): loss=469.01503863440433\n",
      "Gradient Descent(3/49): loss=61263.96600294503\n",
      "Gradient Descent(4/49): loss=8123548.716924158\n",
      "Gradient Descent(5/49): loss=1081592547.272558\n",
      "Gradient Descent(6/49): loss=144189629949.24835\n",
      "Gradient Descent(7/49): loss=19230183071867.734\n",
      "Gradient Descent(8/49): loss=2565027158059295.5\n",
      "Gradient Descent(9/49): loss=3.4215290497637165e+17\n",
      "Gradient Descent(10/49): loss=4.564099363993431e+19\n",
      "Gradient Descent(11/49): loss=6.088245547730944e+21\n",
      "Gradient Descent(12/49): loss=8.121382585906423e+23\n",
      "Gradient Descent(13/49): loss=1.0833481064031236e+26\n",
      "Gradient Descent(14/49): loss=1.4451275168466882e+28\n",
      "Gradient Descent(15/49): loss=1.9277217194678355e+30\n",
      "Gradient Descent(16/49): loss=2.5714762639877773e+32\n",
      "Gradient Descent(17/49): loss=3.4302099506430405e+34\n",
      "Gradient Descent(18/49): loss=4.5757141525626344e+36\n",
      "Gradient Descent(19/49): loss=6.1037546751682015e+38\n",
      "Gradient Descent(20/49): loss=8.142077914541818e+40\n",
      "Gradient Descent(21/49): loss=1.0861090640201951e+43\n",
      "Gradient Descent(22/49): loss=1.4488106247132972e+45\n",
      "Gradient Descent(23/49): loss=1.9326348484140915e+47\n",
      "Gradient Descent(24/49): loss=2.5780301397614566e+49\n",
      "Gradient Descent(25/49): loss=3.438952478260495e+51\n",
      "Gradient Descent(26/49): loss=4.5873762161806765e+53\n",
      "Gradient Descent(27/49): loss=6.119311238469737e+55\n",
      "Gradient Descent(28/49): loss=8.162829527951831e+57\n",
      "Gradient Descent(29/49): loss=1.0888772168265478e+60\n",
      "Gradient Descent(30/49): loss=1.452503190546754e+62\n",
      "Gradient Descent(31/49): loss=1.9375605311104456e+64\n",
      "Gradient Descent(32/49): loss=2.584600733512913e+66\n",
      "Gradient Descent(33/49): loss=3.4477172942034544e+68\n",
      "Gradient Descent(34/49): loss=4.599068005600039e+70\n",
      "Gradient Descent(35/49): loss=6.134907451865365e+72\n",
      "Gradient Descent(36/49): loss=8.183634031313435e+74\n",
      "Gradient Descent(37/49): loss=1.0916524248154338e+77\n",
      "Gradient Descent(38/49): loss=1.4562051675888015e+79\n",
      "Gradient Descent(39/49): loss=1.9424987678388918e+81\n",
      "Gradient Descent(40/49): loss=2.59118807365826e+83\n",
      "Gradient Descent(41/49): loss=3.4565044489262145e+85\n",
      "Gradient Descent(42/49): loss=4.610789593739989e+87\n",
      "Gradient Descent(43/49): loss=6.150543415138762e+89\n",
      "Gradient Descent(44/49): loss=8.204491558857301e+91\n",
      "Gradient Descent(45/49): loss=1.0944347059428386e+94\n",
      "Gradient Descent(46/49): loss=1.459916579814258e+96\n",
      "Gradient Descent(47/49): loss=1.947449590590633e+98\n",
      "Gradient Descent(48/49): loss=2.597792202876523e+100\n",
      "Gradient Descent(49/49): loss=3.4653139993622467e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.269089251009442\n",
      "Gradient Descent(2/49): loss=485.06156570884446\n",
      "Gradient Descent(3/49): loss=64643.20959214998\n",
      "Gradient Descent(4/49): loss=8739265.726999713\n",
      "Gradient Descent(5/49): loss=1185873350.9777296\n",
      "Gradient Descent(6/49): loss=161093743577.54617\n",
      "Gradient Descent(7/49): loss=21891038996497.945\n",
      "Gradient Descent(8/49): loss=2975092494196546.0\n",
      "Gradient Descent(9/49): loss=4.043424204763292e+17\n",
      "Gradient Descent(10/49): loss=5.495444660903846e+19\n",
      "Gradient Descent(11/49): loss=7.468921095063455e+21\n",
      "Gradient Descent(12/49): loss=1.0151106477878014e+24\n",
      "Gradient Descent(13/49): loss=1.3796503908116293e+26\n",
      "Gradient Descent(14/49): loss=1.8751014184180844e+28\n",
      "Gradient Descent(15/49): loss=2.5484756714623004e+30\n",
      "Gradient Descent(16/49): loss=3.4636677563632756e+32\n",
      "Gradient Descent(17/49): loss=4.707517715398851e+34\n",
      "Gradient Descent(18/49): loss=6.398051033094505e+36\n",
      "Gradient Descent(19/49): loss=8.69567774617016e+38\n",
      "Gradient Descent(20/49): loss=1.1818413307930293e+41\n",
      "Gradient Descent(21/49): loss=1.6062565472134355e+43\n",
      "Gradient Descent(22/49): loss=2.1830850117303936e+45\n",
      "Gradient Descent(23/49): loss=2.9670603844252256e+47\n",
      "Gradient Descent(24/49): loss=4.0325719234629017e+49\n",
      "Gradient Descent(25/49): loss=5.48072307637173e+51\n",
      "Gradient Descent(26/49): loss=7.448924906981621e+53\n",
      "Gradient Descent(27/49): loss=1.0123934651809805e+56\n",
      "Gradient Descent(28/49): loss=1.3759576598503832e+58\n",
      "Gradient Descent(29/49): loss=1.870082677156073e+60\n",
      "Gradient Descent(30/49): loss=2.54165467546398e+62\n",
      "Gradient Descent(31/49): loss=3.4543972671475644e+64\n",
      "Gradient Descent(32/49): loss=4.694918076193094e+66\n",
      "Gradient Descent(33/49): loss=6.380926696472808e+68\n",
      "Gradient Descent(34/49): loss=8.67240383005244e+70\n",
      "Gradient Descent(35/49): loss=1.1786781414223537e+73\n",
      "Gradient Descent(36/49): loss=1.6019574137594796e+75\n",
      "Gradient Descent(37/49): loss=2.177242001283003e+77\n",
      "Gradient Descent(38/49): loss=2.9591190698547193e+79\n",
      "Gradient Descent(39/49): loss=4.021778775357964e+81\n",
      "Gradient Descent(40/49): loss=5.466053962713274e+83\n",
      "Gradient Descent(41/49): loss=7.428987916083013e+85\n",
      "Gradient Descent(42/49): loss=1.0096838017660546e+88\n",
      "Gradient Descent(43/49): loss=1.372274919631684e+90\n",
      "Gradient Descent(44/49): loss=1.865077415084125e+92\n",
      "Gradient Descent(45/49): loss=2.534851956042819e+94\n",
      "Gradient Descent(46/49): loss=3.445151599117014e+96\n",
      "Gradient Descent(47/49): loss=4.682352163645671e+98\n",
      "Gradient Descent(48/49): loss=6.3638481946676484e+100\n",
      "Gradient Descent(49/49): loss=8.649192207115564e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.252240750952765\n",
      "Gradient Descent(2/49): loss=481.720454458127\n",
      "Gradient Descent(3/49): loss=64193.1935223061\n",
      "Gradient Descent(4/49): loss=8688963.338029165\n",
      "Gradient Descent(5/49): loss=1181059739.848383\n",
      "Gradient Descent(6/49): loss=160742311620.46835\n",
      "Gradient Descent(7/49): loss=21885855339495.945\n",
      "Gradient Descent(8/49): loss=2980252215114565.0\n",
      "Gradient Descent(9/49): loss=4.0584546089335744e+17\n",
      "Gradient Descent(10/49): loss=5.526806612493353e+19\n",
      "Gradient Descent(11/49): loss=7.526442757171322e+21\n",
      "Gradient Descent(12/49): loss=1.0249575564865826e+24\n",
      "Gradient Descent(13/49): loss=1.3957968165430779e+26\n",
      "Gradient Descent(14/49): loss=1.9008094894867746e+28\n",
      "Gradient Descent(15/49): loss=2.588540718492371e+30\n",
      "Gradient Descent(16/49): loss=3.525099804280983e+32\n",
      "Gradient Descent(17/49): loss=4.800515057230383e+34\n",
      "Gradient Descent(18/49): loss=6.537387913410742e+36\n",
      "Gradient Descent(19/49): loss=8.90267820112516e+38\n",
      "Gradient Descent(20/49): loss=1.2123753434387513e+41\n",
      "Gradient Descent(21/49): loss=1.6510244897755318e+43\n",
      "Gradient Descent(22/49): loss=2.2483811475076778e+45\n",
      "Gradient Descent(23/49): loss=3.0618672320159264e+47\n",
      "Gradient Descent(24/49): loss=4.169680464054808e+49\n",
      "Gradient Descent(25/49): loss=5.67831125743645e+51\n",
      "Gradient Descent(26/49): loss=7.732779289514581e+53\n",
      "Gradient Descent(27/49): loss=1.0530573762056407e+56\n",
      "Gradient Descent(28/49): loss=1.4340637383570568e+58\n",
      "Gradient Descent(29/49): loss=1.9529218940383996e+60\n",
      "Gradient Descent(30/49): loss=2.6595079578429176e+62\n",
      "Gradient Descent(31/49): loss=3.62174370589073e+64\n",
      "Gradient Descent(32/49): loss=4.932125671020033e+66\n",
      "Gradient Descent(33/49): loss=6.716616527881061e+68\n",
      "Gradient Descent(34/49): loss=9.146753467308727e+70\n",
      "Gradient Descent(35/49): loss=1.245613749786578e+73\n",
      "Gradient Descent(36/49): loss=1.6962888736454527e+75\n",
      "Gradient Descent(37/49): loss=2.310022624064952e+77\n",
      "Gradient Descent(38/49): loss=3.1458111920665925e+79\n",
      "Gradient Descent(39/49): loss=4.2839961622181926e+81\n",
      "Gradient Descent(40/49): loss=5.833987482841832e+83\n",
      "Gradient Descent(41/49): loss=7.944780681673634e+85\n",
      "Gradient Descent(42/49): loss=1.081927931205438e+88\n",
      "Gradient Descent(43/49): loss=1.4733799398926768e+90\n",
      "Gradient Descent(44/49): loss=2.0064630782380075e+92\n",
      "Gradient Descent(45/49): loss=2.732420861265151e+94\n",
      "Gradient Descent(46/49): loss=3.7210372042497087e+96\n",
      "Gradient Descent(47/49): loss=5.067344519174604e+98\n",
      "Gradient Descent(48/49): loss=6.900758865480529e+100\n",
      "Gradient Descent(49/49): loss=9.397520286871041e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.267819125234173\n",
      "Gradient Descent(2/49): loss=479.9553991114539\n",
      "Gradient Descent(3/49): loss=63398.6228771144\n",
      "Gradient Descent(4/49): loss=8501913.754792208\n",
      "Gradient Descent(5/49): loss=1144696403.3455722\n",
      "Gradient Descent(6/49): loss=154308080849.81226\n",
      "Gradient Descent(7/49): loss=20809072112674.258\n",
      "Gradient Descent(8/49): loss=2806533210276329.5\n",
      "Gradient Descent(9/49): loss=3.785341342908741e+17\n",
      "Gradient Descent(10/49): loss=5.105586006521653e+19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=6.886332802879696e+21\n",
      "Gradient Descent(12/49): loss=9.288188678298546e+23\n",
      "Gradient Descent(13/49): loss=1.2527783824662744e+26\n",
      "Gradient Descent(14/49): loss=1.6897308692507974e+28\n",
      "Gradient Descent(15/49): loss=2.279086702077698e+30\n",
      "Gradient Descent(16/49): loss=3.074002123432486e+32\n",
      "Gradient Descent(17/49): loss=4.1461735951910054e+34\n",
      "Gradient Descent(18/49): loss=5.592304371072939e+36\n",
      "Gradient Descent(19/49): loss=7.542826531179908e+38\n",
      "Gradient Descent(20/49): loss=1.017366515036005e+41\n",
      "Gradient Descent(21/49): loss=1.3722105654981427e+43\n",
      "Gradient Descent(22/49): loss=1.850819550549362e+45\n",
      "Gradient Descent(23/49): loss=2.496361050444901e+47\n",
      "Gradient Descent(24/49): loss=3.3670589292955186e+49\n",
      "Gradient Descent(25/49): loss=4.541444768710951e+51\n",
      "Gradient Descent(26/49): loss=6.125440932384331e+53\n",
      "Gradient Descent(27/49): loss=8.261914110381784e+55\n",
      "Gradient Descent(28/49): loss=1.1143561013942803e+58\n",
      "Gradient Descent(29/49): loss=1.5030288431033864e+60\n",
      "Gradient Descent(30/49): loss=2.027265521653378e+62\n",
      "Gradient Descent(31/49): loss=2.734349054006725e+64\n",
      "Gradient Descent(32/49): loss=3.688054016254214e+66\n",
      "Gradient Descent(33/49): loss=4.974398717266108e+68\n",
      "Gradient Descent(34/49): loss=6.709403519927455e+70\n",
      "Gradient Descent(35/49): loss=9.04955516271026e+72\n",
      "Gradient Descent(36/49): loss=1.2205920898885218e+75\n",
      "Gradient Descent(37/49): loss=1.6463185461728651e+77\n",
      "Gradient Descent(38/49): loss=2.2205327872641455e+79\n",
      "Gradient Descent(39/49): loss=2.9950253981998035e+81\n",
      "Gradient Descent(40/49): loss=4.0396508384430344e+83\n",
      "Gradient Descent(41/49): loss=5.448627883537158e+85\n",
      "Gradient Descent(42/49): loss=7.349037577886551e+87\n",
      "Gradient Descent(43/49): loss=9.912285161622553e+89\n",
      "Gradient Descent(44/49): loss=1.3369559766706055e+92\n",
      "Gradient Descent(45/49): loss=1.803268625155924e+94\n",
      "Gradient Descent(46/49): loss=2.4322249881178384e+96\n",
      "Gradient Descent(47/49): loss=3.2805530525510576e+98\n",
      "Gradient Descent(48/49): loss=4.424766780695798e+100\n",
      "Gradient Descent(49/49): loss=5.968067197792844e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.264772421538475\n",
      "Gradient Descent(2/49): loss=480.787746455985\n",
      "Gradient Descent(3/49): loss=63583.34079887131\n",
      "Gradient Descent(4/49): loss=8536015.170571474\n",
      "Gradient Descent(5/49): loss=1150660792.7762778\n",
      "Gradient Descent(6/49): loss=155307776542.7366\n",
      "Gradient Descent(7/49): loss=20970977432231.2\n",
      "Gradient Descent(8/49): loss=2832066951830849.5\n",
      "Gradient Descent(9/49): loss=3.8247949698696544e+17\n",
      "Gradient Descent(10/49): loss=5.165584036392156e+19\n",
      "Gradient Descent(11/49): loss=6.976424933631927e+21\n",
      "Gradient Descent(12/49): loss=9.422088207585055e+23\n",
      "Gradient Descent(13/49): loss=1.2725113250315783e+26\n",
      "Gradient Descent(14/49): loss=1.7186056317796816e+28\n",
      "Gradient Descent(15/49): loss=2.3210838688488018e+30\n",
      "Gradient Descent(16/49): loss=3.1347682936134932e+32\n",
      "Gradient Descent(17/49): loss=4.233699806578755e+34\n",
      "Gradient Descent(18/49): loss=5.717875267784676e+36\n",
      "Gradient Descent(19/49): loss=7.72234667042862e+38\n",
      "Gradient Descent(20/49): loss=1.0429510145279651e+41\n",
      "Gradient Descent(21/49): loss=1.4085703028218683e+43\n",
      "Gradient Descent(22/49): loss=1.902361923442439e+45\n",
      "Gradient Descent(23/49): loss=2.569258261764751e+47\n",
      "Gradient Descent(24/49): loss=3.4699433027573346e+49\n",
      "Gradient Descent(25/49): loss=4.6863745476854e+51\n",
      "Gradient Descent(26/49): loss=6.329240706541295e+53\n",
      "Gradient Descent(27/49): loss=8.548033776158635e+55\n",
      "Gradient Descent(28/49): loss=1.1544652009021369e+58\n",
      "Gradient Descent(29/49): loss=1.5591771569870483e+60\n",
      "Gradient Descent(30/49): loss=2.1057658602186958e+62\n",
      "Gradient Descent(31/49): loss=2.8439679469338366e+64\n",
      "Gradient Descent(32/49): loss=3.8409558422354875e+66\n",
      "Gradient Descent(33/49): loss=5.1874500899029435e+68\n",
      "Gradient Descent(34/49): loss=7.005974434627267e+70\n",
      "Gradient Descent(35/49): loss=9.462004824718987e+72\n",
      "Gradient Descent(36/49): loss=1.277902683465456e+75\n",
      "Gradient Descent(37/49): loss=1.7258871652041358e+77\n",
      "Gradient Descent(38/49): loss=2.330918109459385e+79\n",
      "Gradient Descent(39/49): loss=3.1480500826153893e+81\n",
      "Gradient Descent(40/49): loss=4.25163770551905e+83\n",
      "Gradient Descent(41/49): loss=5.742101524628097e+85\n",
      "Gradient Descent(42/49): loss=7.755065742392785e+87\n",
      "Gradient Descent(43/49): loss=1.047369929125895e+90\n",
      "Gradient Descent(44/49): loss=1.4145383222743705e+92\n",
      "Gradient Descent(45/49): loss=1.9104221054472222e+94\n",
      "Gradient Descent(46/49): loss=2.580144039585426e+96\n",
      "Gradient Descent(47/49): loss=3.4846452237055926e+98\n",
      "Gradient Descent(48/49): loss=4.7062304076036244e+100\n",
      "Gradient Descent(49/49): loss=6.356057282038008e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.318333407467551\n",
      "Gradient Descent(2/49): loss=497.2273353442134\n",
      "Gradient Descent(3/49): loss=67088.55121237479\n",
      "Gradient Descent(4/49): loss=9182648.207237218\n",
      "Gradient Descent(5/49): loss=1261542295.9936502\n",
      "Gradient Descent(6/49): loss=173505756122.59335\n",
      "Gradient Descent(7/49): loss=23871173106787.445\n",
      "Gradient Descent(8/49): loss=3284582791587884.5\n",
      "Gradient Descent(9/49): loss=4.5196152811725184e+17\n",
      "Gradient Descent(10/49): loss=6.219098660282782e+19\n",
      "Gradient Descent(11/49): loss=8.557658152439556e+21\n",
      "Gradient Descent(12/49): loss=1.1775596181294197e+24\n",
      "Gradient Descent(13/49): loss=1.620358189622913e+26\n",
      "Gradient Descent(14/49): loss=2.2296628729481495e+28\n",
      "Gradient Descent(15/49): loss=3.068085031869862e+30\n",
      "Gradient Descent(16/49): loss=4.221779887238352e+32\n",
      "Gradient Descent(17/49): loss=5.809299708942705e+34\n",
      "Gradient Descent(18/49): loss=7.993776096791878e+36\n",
      "Gradient Descent(19/49): loss=1.0999683182402672e+39\n",
      "Gradient Descent(20/49): loss=1.5135904316427177e+41\n",
      "Gradient Descent(21/49): loss=2.0827472544980577e+43\n",
      "Gradient Descent(22/49): loss=2.865924648760756e+45\n",
      "Gradient Descent(23/49): loss=3.9436009696658006e+47\n",
      "Gradient Descent(24/49): loss=5.426516923497482e+49\n",
      "Gradient Descent(25/49): loss=7.467055147699759e+51\n",
      "Gradient Descent(26/49): loss=1.0274898865119555e+54\n",
      "Gradient Descent(27/49): loss=1.4138578676623586e+56\n",
      "Gradient Descent(28/49): loss=1.9455121614255537e+58\n",
      "Gradient Descent(29/49): loss=2.677084915552928e+60\n",
      "Gradient Descent(30/49): loss=3.6837516553120247e+62\n",
      "Gradient Descent(31/49): loss=5.068956228910431e+64\n",
      "Gradient Descent(32/49): loss=6.975040571358282e+66\n",
      "Gradient Descent(33/49): loss=9.597871588358942e+68\n",
      "Gradient Descent(34/49): loss=1.3206968201002063e+71\n",
      "Gradient Descent(35/49): loss=1.8173196781860817e+73\n",
      "Gradient Descent(36/49): loss=2.500688093177803e+75\n",
      "Gradient Descent(37/49): loss=3.4410241711590325e+77\n",
      "Gradient Descent(38/49): loss=4.734955702313858e+79\n",
      "Gradient Descent(39/49): loss=6.515445514968025e+81\n",
      "Gradient Descent(40/49): loss=8.965454573898606e+83\n",
      "Gradient Descent(41/49): loss=1.2336742826255298e+86\n",
      "Gradient Descent(42/49): loss=1.6975739747123606e+88\n",
      "Gradient Descent(43/49): loss=2.3359143010485017e+90\n",
      "Gradient Descent(44/49): loss=3.214290336164859e+92\n",
      "Gradient Descent(45/49): loss=4.42296293169886e+94\n",
      "Gradient Descent(46/49): loss=6.08613381158447e+96\n",
      "Gradient Descent(47/49): loss=8.374708389944456e+98\n",
      "Gradient Descent(48/49): loss=1.1523857803308295e+101\n",
      "Gradient Descent(49/49): loss=1.58571848101985e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.301235924181737\n",
      "Gradient Descent(2/49): loss=493.8032077802247\n",
      "Gradient Descent(3/49): loss=66621.47171133252\n",
      "Gradient Descent(4/49): loss=9129768.750179974\n",
      "Gradient Descent(5/49): loss=1256416103.6661737\n",
      "Gradient Descent(6/49): loss=173126281554.58447\n",
      "Gradient Descent(7/49): loss=23865361464652.78\n",
      "Gradient Descent(8/49): loss=3290253980319323.0\n",
      "Gradient Descent(9/49): loss=4.536376472080186e+17\n",
      "Gradient Descent(10/49): loss=6.254530114336635e+19\n",
      "Gradient Descent(11/49): loss=8.623473175175324e+21\n",
      "Gradient Descent(12/49): loss=1.1889685692112181e+24\n",
      "Gradient Descent(13/49): loss=1.6393011215229383e+26\n",
      "Gradient Descent(14/49): loss=2.26020153019715e+28\n",
      "Gradient Descent(15/49): loss=3.116273847080754e+30\n",
      "Gradient Descent(16/49): loss=4.296591573090979e+32\n",
      "Gradient Descent(17/49): loss=5.923965655702927e+34\n",
      "Gradient Descent(18/49): loss=8.167722845792929e+36\n",
      "Gradient Descent(19/49): loss=1.126132398472077e+39\n",
      "Gradient Descent(20/49): loss=1.5526655385645727e+41\n",
      "Gradient Descent(21/49): loss=2.140752080360058e+43\n",
      "Gradient Descent(22/49): loss=2.951581880210857e+45\n",
      "Gradient Descent(23/49): loss=4.069521022807535e+47\n",
      "Gradient Descent(24/49): loss=5.610890033626199e+49\n",
      "Gradient Descent(25/49): loss=7.736066921150272e+51\n",
      "Gradient Descent(26/49): loss=1.0666174359124018e+54\n",
      "Gradient Descent(27/49): loss=1.4706087294592137e+56\n",
      "Gradient Descent(28/49): loss=2.027615490189038e+58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=2.7955937522323666e+60\n",
      "Gradient Descent(30/49): loss=3.8544509377327964e+62\n",
      "Gradient Descent(31/49): loss=5.314360149619592e+64\n",
      "Gradient Descent(32/49): loss=7.327223580248056e+66\n",
      "Gradient Descent(33/49): loss=1.0102477792888362e+69\n",
      "Gradient Descent(34/49): loss=1.3928885400866636e+71\n",
      "Gradient Descent(35/49): loss=1.9204580548254325e+73\n",
      "Gradient Descent(36/49): loss=2.6478494396360195e+75\n",
      "Gradient Descent(37/49): loss=3.6507470899269735e+77\n",
      "Gradient Descent(38/49): loss=5.0335015711627364e+79\n",
      "Gradient Descent(39/49): loss=6.9399871979092636e+81\n",
      "Gradient Descent(40/49): loss=9.568572022123877e+83\n",
      "Gradient Descent(41/49): loss=1.3192757844013585e+86\n",
      "Gradient Descent(42/49): loss=1.818963781934835e+88\n",
      "Gradient Descent(43/49): loss=2.5079132650737023e+90\n",
      "Gradient Descent(44/49): loss=3.4578087851987627e+92\n",
      "Gradient Descent(45/49): loss=4.767486085547077e+94\n",
      "Gradient Descent(46/49): loss=6.5732158681465085e+96\n",
      "Gradient Descent(47/49): loss=9.062882633310287e+98\n",
      "Gradient Descent(48/49): loss=1.2495533886721058e+101\n",
      "Gradient Descent(49/49): loss=1.7228333790874976e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.3170633213313945\n",
      "Gradient Descent(2/49): loss=491.9978970253044\n",
      "Gradient Descent(3/49): loss=65797.71437192462\n",
      "Gradient Descent(4/49): loss=8933393.800136324\n",
      "Gradient Descent(5/49): loss=1217760988.5732026\n",
      "Gradient Descent(6/49): loss=166201007213.5268\n",
      "Gradient Descent(7/49): loss=22691930743894.965\n",
      "Gradient Descent(8/49): loss=3098581136819016.0\n",
      "Gradient Descent(9/49): loss=4.231279999569683e+17\n",
      "Gradient Descent(10/49): loss=5.778117551508502e+19\n",
      "Gradient Descent(11/49): loss=7.89046966038533e+21\n",
      "Gradient Descent(12/49): loss=1.077506627639238e+24\n",
      "Gradient Descent(13/49): loss=1.4714220088314556e+26\n",
      "Gradient Descent(14/49): loss=2.0093454672054387e+28\n",
      "Gradient Descent(15/49): loss=2.743923486523244e+30\n",
      "Gradient Descent(16/49): loss=3.747049149316262e+32\n",
      "Gradient Descent(17/49): loss=5.116898292559541e+34\n",
      "Gradient Descent(18/49): loss=6.98753796371356e+36\n",
      "Gradient Descent(19/49): loss=9.54204755128887e+38\n",
      "Gradient Descent(20/49): loss=1.3030436752008486e+41\n",
      "Gradient Descent(21/49): loss=1.77941140039479e+43\n",
      "Gradient Descent(22/49): loss=2.4299300109276714e+45\n",
      "Gradient Descent(23/49): loss=3.3182657235627436e+47\n",
      "Gradient Descent(24/49): loss=4.5313599003586903e+49\n",
      "Gradient Descent(25/49): loss=6.187937994473704e+51\n",
      "Gradient Descent(26/49): loss=8.450129203030053e+53\n",
      "Gradient Descent(27/49): loss=1.1539334041756164e+56\n",
      "Gradient Descent(28/49): loss=1.5757892799969143e+58\n",
      "Gradient Descent(29/49): loss=2.1518675566266222e+60\n",
      "Gradient Descent(30/49): loss=2.938548979893617e+62\n",
      "Gradient Descent(31/49): loss=4.0128260127545335e+64\n",
      "Gradient Descent(32/49): loss=5.479838082951542e+66\n",
      "Gradient Descent(33/49): loss=7.483161572398508e+68\n",
      "Gradient Descent(34/49): loss=1.021886162892985e+71\n",
      "Gradient Descent(35/49): loss=1.3954681050371143e+73\n",
      "Gradient Descent(36/49): loss=1.905624425584676e+75\n",
      "Gradient Descent(37/49): loss=2.602284092539931e+77\n",
      "Gradient Descent(38/49): loss=3.5536291450549846e+79\n",
      "Gradient Descent(39/49): loss=4.852767665446746e+81\n",
      "Gradient Descent(40/49): loss=6.626846261539542e+83\n",
      "Gradient Descent(41/49): loss=9.04949389742471e+85\n",
      "Gradient Descent(42/49): loss=1.2357814949595926e+88\n",
      "Gradient Descent(43/49): loss=1.6875594597827884e+90\n",
      "Gradient Descent(44/49): loss=2.3044987660990036e+92\n",
      "Gradient Descent(45/49): loss=3.146979226222604e+94\n",
      "Gradient Descent(46/49): loss=4.297454351447057e+96\n",
      "Gradient Descent(47/49): loss=5.868521072170854e+98\n",
      "Gradient Descent(48/49): loss=8.013939592614126e+100\n",
      "Gradient Descent(49/49): loss=1.0943681892635865e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.313648423604008\n",
      "Gradient Descent(2/49): loss=492.78093730431976\n",
      "Gradient Descent(3/49): loss=65975.4356271479\n",
      "Gradient Descent(4/49): loss=8966691.614760777\n",
      "Gradient Descent(5/49): loss=1223673508.6692066\n",
      "Gradient Descent(6/49): loss=167206795698.1355\n",
      "Gradient Descent(7/49): loss=22857167063674.184\n",
      "Gradient Descent(8/49): loss=3125003201179261.5\n",
      "Gradient Descent(9/49): loss=4.272660789590354e+17\n",
      "Gradient Descent(10/49): loss=5.84188528905522e+19\n",
      "Gradient Descent(11/49): loss=7.987481584446815e+21\n",
      "Gradient Descent(12/49): loss=1.092112719837385e+24\n",
      "Gradient Descent(13/49): loss=1.4932252081873798e+26\n",
      "Gradient Descent(14/49): loss=2.0416591773697865e+28\n",
      "Gradient Descent(15/49): loss=2.7915229710629034e+30\n",
      "Gradient Descent(16/49): loss=3.8167980022647246e+32\n",
      "Gradient Descent(17/49): loss=5.218637728878254e+34\n",
      "Gradient Descent(18/49): loss=7.135347429120875e+36\n",
      "Gradient Descent(19/49): loss=9.75602937429858e+38\n",
      "Gradient Descent(20/49): loss=1.3339239626989736e+41\n",
      "Gradient Descent(21/49): loss=1.823849714021701e+43\n",
      "Gradient Descent(22/49): loss=2.4937161880740845e+45\n",
      "Gradient Descent(23/49): loss=3.409612304632477e+47\n",
      "Gradient Descent(24/49): loss=4.661900228879386e+49\n",
      "Gradient Descent(25/49): loss=6.374130488244544e+51\n",
      "Gradient Descent(26/49): loss=8.715231447789166e+53\n",
      "Gradient Descent(27/49): loss=1.1916175755834282e+56\n",
      "Gradient Descent(28/49): loss=1.6292768068707948e+58\n",
      "Gradient Descent(29/49): loss=2.2276802287910507e+60\n",
      "Gradient Descent(30/49): loss=3.045866227776067e+62\n",
      "Gradient Descent(31/49): loss=4.1645569043549464e+64\n",
      "Gradient Descent(32/49): loss=5.694122102753642e+66\n",
      "Gradient Descent(33/49): loss=7.785468482172076e+68\n",
      "Gradient Descent(34/49): loss=1.0644927943779505e+71\n",
      "Gradient Descent(35/49): loss=1.4554614303267207e+73\n",
      "Gradient Descent(36/49): loss=1.9900256595035073e+75\n",
      "Gradient Descent(37/49): loss=2.720925503737577e+77\n",
      "Gradient Descent(38/49): loss=3.7202714254130176e+79\n",
      "Gradient Descent(39/49): loss=5.0866587342185125e+81\n",
      "Gradient Descent(40/49): loss=6.954894984719831e+83\n",
      "Gradient Descent(41/49): loss=9.509300068252386e+85\n",
      "Gradient Descent(42/49): loss=1.3001891184084918e+88\n",
      "Gradient Descent(43/49): loss=1.7777246816216345e+90\n",
      "Gradient Descent(44/49): loss=2.4306502791802583e+92\n",
      "Gradient Descent(45/49): loss=3.3233834466930857e+94\n",
      "Gradient Descent(46/49): loss=4.544001096479628e+96\n",
      "Gradient Descent(47/49): loss=6.212929171731193e+98\n",
      "Gradient Descent(48/49): loss=8.494823850912661e+100\n",
      "Gradient Descent(49/49): loss=1.1614816500141633e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.367892410557424\n",
      "Gradient Descent(2/49): loss=509.6207617801885\n",
      "Gradient Descent(3/49): loss=69610.50092998876\n",
      "Gradient Descent(4/49): loss=9645590.379687557\n",
      "Gradient Descent(5/49): loss=1341529358.7903466\n",
      "Gradient Descent(6/49): loss=186788846892.5063\n",
      "Gradient Descent(7/49): loss=26016565669355.48\n",
      "Gradient Descent(8/49): loss=3624063060830859.0\n",
      "Gradient Descent(9/49): loss=5.048430668862958e+17\n",
      "Gradient Descent(10/49): loss=7.0326951876906074e+19\n",
      "Gradient Descent(11/49): loss=9.796900810167725e+21\n",
      "Gradient Descent(12/49): loss=1.3647594598825005e+24\n",
      "Gradient Descent(13/49): loss=1.9011819007029016e+26\n",
      "Gradient Descent(14/49): loss=2.648446953607094e+28\n",
      "Gradient Descent(15/49): loss=3.689426835445394e+30\n",
      "Gradient Descent(16/49): loss=5.1395669886905365e+32\n",
      "Gradient Descent(17/49): loss=7.159689054364664e+34\n",
      "Gradient Descent(18/49): loss=9.973826108256602e+36\n",
      "Gradient Descent(19/49): loss=1.389406809719778e+39\n",
      "Gradient Descent(20/49): loss=1.9355172851228933e+41\n",
      "Gradient Descent(21/49): loss=2.6962781058437153e+43\n",
      "Gradient Descent(22/49): loss=3.756058227959765e+45\n",
      "Gradient Descent(23/49): loss=5.23238807647654e+47\n",
      "Gradient Descent(24/49): loss=7.288993759226273e+49\n",
      "Gradient Descent(25/49): loss=1.0153954417279933e+52\n",
      "Gradient Descent(26/49): loss=1.414499637589987e+54\n",
      "Gradient Descent(27/49): loss=1.9704729236693415e+56\n",
      "Gradient Descent(28/49): loss=2.744973162050088e+58\n",
      "Gradient Descent(29/49): loss=3.8238930207394634e+60\n",
      "Gradient Descent(30/49): loss=5.326885536155634e+62\n",
      "Gradient Descent(31/49): loss=7.420633726258572e+64\n",
      "Gradient Descent(32/49): loss=1.0337335864555984e+67\n",
      "Gradient Descent(33/49): loss=1.4400456445990548e+69\n",
      "Gradient Descent(34/49): loss=2.0060598646494625e+71\n",
      "Gradient Descent(35/49): loss=2.794547655937582e+73\n",
      "Gradient Descent(36/49): loss=3.892952916771904e+75\n",
      "Gradient Descent(37/49): loss=5.423089629551628e+77\n",
      "Gradient Descent(38/49): loss=7.554651124457332e+79\n",
      "Gradient Descent(39/49): loss=1.0524029199381602e+82\n",
      "Gradient Descent(40/49): loss=1.4660530150873388e+84\n",
      "Gradient Descent(41/49): loss=2.042289509395294e+86\n",
      "Gradient Descent(42/49): loss=2.845017470215818e+88\n",
      "Gradient Descent(43/49): loss=3.9632600415353607e+90\n",
      "Gradient Descent(44/49): loss=5.521031178637835e+92\n",
      "Gradient Descent(45/49): loss=7.691088890468696e+94\n",
      "Gradient Descent(46/49): loss=1.0714094234781237e+97\n",
      "Gradient Descent(47/49): loss=1.4925300813260623e+99\n",
      "Gradient Descent(48/49): loss=2.0791734652020986e+101\n",
      "Gradient Descent(49/49): loss=2.896398774461997e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.35054420874604\n",
      "Gradient Descent(2/49): loss=506.11208158341867\n",
      "Gradient Descent(3/49): loss=69125.82242065607\n",
      "Gradient Descent(4/49): loss=9590019.095694253\n",
      "Gradient Descent(5/49): loss=1336072441.4914596\n",
      "Gradient Descent(6/49): loss=186379294501.1594\n",
      "Gradient Descent(7/49): loss=26010060447839.133\n",
      "Gradient Descent(8/49): loss=3630292845611938.5\n",
      "Gradient Descent(9/49): loss=5.0671095990505344e+17\n",
      "Gradient Descent(10/49): loss=7.072694492112947e+19\n",
      "Gradient Descent(11/49): loss=9.872143088591812e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=1.3779663637805222e+24\n",
      "Gradient Descent(13/49): loss=1.9233839947309857e+26\n",
      "Gradient Descent(14/49): loss=2.68468567940269e+28\n",
      "Gradient Descent(15/49): loss=3.747321169567005e+30\n",
      "Gradient Descent(16/49): loss=5.2305624775547444e+32\n",
      "Gradient Descent(17/49): loss=7.300891154028289e+34\n",
      "Gradient Descent(18/49): loss=1.0190684455982842e+37\n",
      "Gradient Descent(19/49): loss=1.422429776716127e+39\n",
      "Gradient Descent(20/49): loss=1.9854470804228525e+41\n",
      "Gradient Descent(21/49): loss=2.771314390295393e+43\n",
      "Gradient Descent(22/49): loss=3.8682388091522726e+45\n",
      "Gradient Descent(23/49): loss=5.3993410264553396e+47\n",
      "Gradient Descent(24/49): loss=7.536474596927416e+49\n",
      "Gradient Descent(25/49): loss=1.0519515080057618e+52\n",
      "Gradient Descent(26/49): loss=1.4683284086793177e+54\n",
      "Gradient Descent(27/49): loss=2.0495130234872724e+56\n",
      "Gradient Descent(28/49): loss=2.860738516407352e+58\n",
      "Gradient Descent(29/49): loss=3.993058236503282e+60\n",
      "Gradient Descent(30/49): loss=5.573565702932775e+62\n",
      "Gradient Descent(31/49): loss=7.779659800832668e+64\n",
      "Gradient Descent(32/49): loss=1.0858956338281862e+67\n",
      "Gradient Descent(33/49): loss=1.5157080871851268e+69\n",
      "Gradient Descent(34/49): loss=2.115646231543736e+71\n",
      "Gradient Descent(35/49): loss=2.9530481593970074e+73\n",
      "Gradient Descent(36/49): loss=4.121905308032038e+75\n",
      "Gradient Descent(37/49): loss=5.753412220629636e+77\n",
      "Gradient Descent(38/49): loss=8.030692048162187e+79\n",
      "Gradient Descent(39/49): loss=1.1209350607830699e+82\n",
      "Gradient Descent(40/49): loss=1.56461660210255e+84\n",
      "Gradient Descent(41/49): loss=2.1839134105277937e+86\n",
      "Gradient Descent(42/49): loss=3.048336428409264e+88\n",
      "Gradient Descent(43/49): loss=4.254909986802657e+90\n",
      "Gradient Descent(44/49): loss=5.939061983798285e+92\n",
      "Gradient Descent(45/49): loss=8.289824545478504e+94\n",
      "Gradient Descent(46/49): loss=1.15710513179166e+97\n",
      "Gradient Descent(47/49): loss=1.615103285568171e+99\n",
      "Gradient Descent(48/49): loss=2.2543834189154552e+101\n",
      "Gradient Descent(49/49): loss=3.146699437053699e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.366622446986523\n",
      "Gradient Descent(2/49): loss=504.2658370556726\n",
      "Gradient Descent(3/49): loss=68271.99106733143\n",
      "Gradient Descent(4/49): loss=9383914.598121835\n",
      "Gradient Descent(5/49): loss=1294996382.1359725\n",
      "Gradient Descent(6/49): loss=178928843071.77466\n",
      "Gradient Descent(7/49): loss=24731978578226.566\n",
      "Gradient Descent(8/49): loss=3418937745915248.0\n",
      "Gradient Descent(9/49): loss=4.726515321708183e+17\n",
      "Gradient Descent(10/49): loss=6.534264219173629e+19\n",
      "Gradient Descent(11/49): loss=9.033461489476596e+21\n",
      "Gradient Descent(12/49): loss=1.2488558769914475e+24\n",
      "Gradient Descent(13/49): loss=1.7265161676080707e+26\n",
      "Gradient Descent(14/49): loss=2.3868715277487713e+28\n",
      "Gradient Descent(15/49): loss=3.2997987997088275e+30\n",
      "Gradient Descent(16/49): loss=4.561901288999583e+32\n",
      "Gradient Descent(17/49): loss=6.306731030320735e+34\n",
      "Gradient Descent(18/49): loss=8.718920870975115e+36\n",
      "Gradient Descent(19/49): loss=1.2053721782891529e+39\n",
      "Gradient Descent(20/49): loss=1.6664012782944692e+41\n",
      "Gradient Descent(21/49): loss=2.303764157240636e+43\n",
      "Gradient Descent(22/49): loss=3.184904717385489e+45\n",
      "Gradient Descent(23/49): loss=4.403062712393026e+47\n",
      "Gradient Descent(24/49): loss=6.087140109241416e+49\n",
      "Gradient Descent(25/49): loss=8.415341122730679e+51\n",
      "Gradient Descent(26/49): loss=1.163402927171403e+54\n",
      "Gradient Descent(27/49): loss=1.6083796856376245e+56\n",
      "Gradient Descent(28/49): loss=2.2235505453483592e+58\n",
      "Gradient Descent(29/49): loss=3.0740111130904984e+60\n",
      "Gradient Descent(30/49): loss=4.249754674195383e+62\n",
      "Gradient Descent(31/49): loss=5.875195022534644e+64\n",
      "Gradient Descent(32/49): loss=8.122331569492549e+66\n",
      "Gradient Descent(33/49): loss=1.1228949825790557e+69\n",
      "Gradient Descent(34/49): loss=1.5523783178676406e+71\n",
      "Gradient Descent(35/49): loss=2.146129851119822e+73\n",
      "Gradient Descent(36/49): loss=2.9669786577503007e+75\n",
      "Gradient Descent(37/49): loss=4.1017845919027004e+77\n",
      "Gradient Descent(38/49): loss=5.6706295457910604e+79\n",
      "Gradient Descent(39/49): loss=7.839524169328018e+81\n",
      "Gradient Descent(40/49): loss=1.0837974638476333e+84\n",
      "Gradient Descent(41/49): loss=1.498326833710437e+86\n",
      "Gradient Descent(42/49): loss=2.071404829318139e+88\n",
      "Gradient Descent(43/49): loss=2.8636729119354016e+90\n",
      "Gradient Descent(44/49): loss=3.9589666058913543e+92\n",
      "Gradient Descent(45/49): loss=5.473186732059419e+94\n",
      "Gradient Descent(46/49): loss=7.566563698570771e+96\n",
      "Gradient Descent(47/49): loss=1.0460612620645224e+99\n",
      "Gradient Descent(48/49): loss=1.4461573411437936e+101\n",
      "Gradient Descent(49/49): loss=1.999281620673462e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.3628349934624255\n",
      "Gradient Descent(2/49): loss=504.9973478234635\n",
      "Gradient Descent(3/49): loss=68442.06325122614\n",
      "Gradient Descent(4/49): loss=9416262.86166583\n",
      "Gradient Descent(5/49): loss=1300829678.3004515\n",
      "Gradient Descent(6/49): loss=179936231358.46536\n",
      "Gradient Descent(7/49): loss=24899890600244.96\n",
      "Gradient Descent(8/49): loss=3446164898983628.0\n",
      "Gradient Descent(9/49): loss=4.76973925747344e+17\n",
      "Gradient Descent(10/49): loss=6.601762802025448e+19\n",
      "Gradient Descent(11/49): loss=9.137501088090431e+21\n",
      "Gradient Descent(12/49): loss=1.2647238902467805e+24\n",
      "Gradient Descent(13/49): loss=1.750508632641174e+26\n",
      "Gradient Descent(14/49): loss=2.422885420865307e+28\n",
      "Gradient Descent(15/49): loss=3.353524821868908e+30\n",
      "Gradient Descent(16/49): loss=4.6416264194019476e+32\n",
      "Gradient Descent(17/49): loss=6.424492771806013e+34\n",
      "Gradient Descent(18/49): loss=8.892164889893184e+36\n",
      "Gradient Descent(19/49): loss=1.230767926812686e+39\n",
      "Gradient Descent(20/49): loss=1.7035105723493281e+41\n",
      "Gradient Descent(21/49): loss=2.357835467729452e+43\n",
      "Gradient Descent(22/49): loss=3.263489046266559e+45\n",
      "Gradient Descent(23/49): loss=4.5170076118469604e+47\n",
      "Gradient Descent(24/49): loss=6.252007430176242e+49\n",
      "Gradient Descent(25/49): loss=8.653427283254939e+51\n",
      "Gradient Descent(26/49): loss=1.1977241643249455e+54\n",
      "Gradient Descent(27/49): loss=1.6577745751490013e+56\n",
      "Gradient Descent(28/49): loss=2.2945321000178022e+58\n",
      "Gradient Descent(29/49): loss=3.175870614096595e+60\n",
      "Gradient Descent(30/49): loss=4.39573460637315e+62\n",
      "Gradient Descent(31/49): loss=6.084153001668509e+64\n",
      "Gradient Descent(32/49): loss=8.421099329800955e+66\n",
      "Gradient Descent(33/49): loss=1.165567563848691e+69\n",
      "Gradient Descent(34/49): loss=1.6132665020212725e+71\n",
      "Gradient Descent(35/49): loss=2.23292830657547e+73\n",
      "Gradient Descent(36/49): loss=3.0906045690895327e+75\n",
      "Gradient Descent(37/49): loss=4.2777175489016104e+77\n",
      "Gradient Descent(38/49): loss=5.920805143173464e+79\n",
      "Gradient Descent(39/49): loss=8.195008937051238e+81\n",
      "Gradient Descent(40/49): loss=1.134274306523688e+84\n",
      "Gradient Descent(41/49): loss=1.5699533854355127e+86\n",
      "Gradient Descent(42/49): loss=2.1729784570316075e+88\n",
      "Gradient Descent(43/49): loss=3.0076277541282464e+90\n",
      "Gradient Descent(44/49): loss=4.162869023450681e+92\n",
      "Gradient Descent(45/49): loss=5.7618428619096284e+94\n",
      "Gradient Descent(46/49): loss=7.974988638441422e+96\n",
      "Gradient Descent(47/49): loss=1.1038212132392563e+99\n",
      "Gradient Descent(48/49): loss=1.5278031430965093e+101\n",
      "Gradient Descent(49/49): loss=2.1146381461593114e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.41776626027906\n",
      "Gradient Descent(2/49): loss=522.244668397077\n",
      "Gradient Descent(3/49): loss=72210.96690633912\n",
      "Gradient Descent(4/49): loss=10128827.245121775\n",
      "Gradient Descent(5/49): loss=1426052205.5272086\n",
      "Gradient Descent(6/49): loss=200998241016.27698\n",
      "Gradient Descent(7/49): loss=28339867358086.836\n",
      "Gradient Descent(8/49): loss=3996228375769070.5\n",
      "Gradient Descent(9/49): loss=5.635308844458587e+17\n",
      "Gradient Descent(10/49): loss=7.94675673183007e+19\n",
      "Gradient Descent(11/49): loss=1.120633604586454e+22\n",
      "Gradient Descent(12/49): loss=1.5802938637098147e+24\n",
      "Gradient Descent(13/49): loss=2.2284978661593198e+26\n",
      "Gradient Descent(14/49): loss=3.142582166184754e+28\n",
      "Gradient Descent(15/49): loss=4.4316053368815155e+30\n",
      "Gradient Descent(16/49): loss=6.24935962094765e+32\n",
      "Gradient Descent(17/49): loss=8.81271973732688e+34\n",
      "Gradient Descent(18/49): loss=1.2427518013674134e+37\n",
      "Gradient Descent(19/49): loss=1.7525032980035056e+39\n",
      "Gradient Descent(20/49): loss=2.4713444844882112e+41\n",
      "Gradient Descent(21/49): loss=3.485039696195034e+43\n",
      "Gradient Descent(22/49): loss=4.9145320534810923e+45\n",
      "Gradient Descent(23/49): loss=6.930373083314449e+47\n",
      "Gradient Descent(24/49): loss=9.773071078043288e+49\n",
      "Gradient Descent(25/49): loss=1.3781785936818268e+52\n",
      "Gradient Descent(26/49): loss=1.943479404698395e+54\n",
      "Gradient Descent(27/49): loss=2.7406551036294805e+56\n",
      "Gradient Descent(28/49): loss=3.864816050477278e+58\n",
      "Gradient Descent(29/49): loss=5.4500849392708985e+60\n",
      "Gradient Descent(30/49): loss=7.685598863521379e+62\n",
      "Gradient Descent(31/49): loss=1.0838075103259497e+65\n",
      "Gradient Descent(32/49): loss=1.5283632938666415e+67\n",
      "Gradient Descent(33/49): loss=2.155266812403246e+69\n",
      "Gradient Descent(34/49): loss=3.039313395766608e+71\n",
      "Gradient Descent(35/49): loss=4.2859778958810497e+73\n",
      "Gradient Descent(36/49): loss=6.043998802350394e+75\n",
      "Gradient Descent(37/49): loss=8.52312410615072e+77\n",
      "Gradient Descent(38/49): loss=1.2019136155453546e+80\n",
      "Gradient Descent(39/49): loss=1.6949141198012312e+82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=2.390133397563765e+84\n",
      "Gradient Descent(41/49): loss=3.37051747425391e+86\n",
      "Gradient Descent(42/49): loss=4.7530351468376234e+88\n",
      "Gradient Descent(43/49): loss=6.702633432296468e+90\n",
      "Gradient Descent(44/49): loss=9.451917257045466e+92\n",
      "Gradient Descent(45/49): loss=1.3328901354437427e+95\n",
      "Gradient Descent(46/49): loss=1.8796145425828494e+97\n",
      "Gradient Descent(47/49): loss=2.6505941748250245e+99\n",
      "Gradient Descent(48/49): loss=3.7378139615594487e+101\n",
      "Gradient Descent(49/49): loss=5.270989178172084e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.400165604645677\n",
      "Gradient Descent(2/49): loss=518.6498804059281\n",
      "Gradient Descent(3/49): loss=71708.14045229\n",
      "Gradient Descent(4/49): loss=10070445.023897473\n",
      "Gradient Descent(5/49): loss=1420245489.6706188\n",
      "Gradient Descent(6/49): loss=200556442337.1379\n",
      "Gradient Descent(7/49): loss=28332596939609.184\n",
      "Gradient Descent(8/49): loss=4003067902812460.5\n",
      "Gradient Descent(9/49): loss=5.656111343768062e+17\n",
      "Gradient Descent(10/49): loss=7.991879671135217e+19\n",
      "Gradient Descent(11/49): loss=1.1292286161502677e+22\n",
      "Gradient Descent(12/49): loss=1.5955684886935194e+24\n",
      "Gradient Descent(13/49): loss=2.2544947788717275e+26\n",
      "Gradient Descent(14/49): loss=3.185540161124278e+28\n",
      "Gradient Descent(15/49): loss=4.5010823397746356e+30\n",
      "Gradient Descent(16/49): loss=6.359908066085103e+32\n",
      "Gradient Descent(17/49): loss=8.986378780617747e+34\n",
      "Gradient Descent(18/49): loss=1.2697511173461073e+37\n",
      "Gradient Descent(19/49): loss=1.794124129741279e+39\n",
      "Gradient Descent(20/49): loss=2.535049073437548e+41\n",
      "Gradient Descent(21/49): loss=3.581956063462065e+43\n",
      "Gradient Descent(22/49): loss=5.061207443779728e+45\n",
      "Gradient Descent(23/49): loss=7.151349803096103e+47\n",
      "Gradient Descent(24/49): loss=1.0104664662422716e+50\n",
      "Gradient Descent(25/49): loss=1.4277619016186332e+52\n",
      "Gradient Descent(26/49): loss=2.0173891126688397e+54\n",
      "Gradient Descent(27/49): loss=2.8505164812852247e+56\n",
      "Gradient Descent(28/49): loss=4.027703014283458e+58\n",
      "Gradient Descent(29/49): loss=5.6910358799097024e+60\n",
      "Gradient Descent(30/49): loss=8.041280420021602e+62\n",
      "Gradient Descent(31/49): loss=1.1362112655394003e+65\n",
      "Gradient Descent(32/49): loss=1.605435916305442e+67\n",
      "Gradient Descent(33/49): loss=2.268437710076662e+69\n",
      "Gradient Descent(34/49): loss=3.205241387859174e+71\n",
      "Gradient Descent(35/49): loss=4.528919753365522e+73\n",
      "Gradient Descent(36/49): loss=6.39924163281943e+75\n",
      "Gradient Descent(37/49): loss=9.041956074575787e+77\n",
      "Gradient Descent(38/49): loss=1.277604040379653e+80\n",
      "Gradient Descent(39/49): loss=1.8052200989828357e+82\n",
      "Gradient Descent(40/49): loss=2.550727379355512e+84\n",
      "Gradient Descent(41/49): loss=3.60410908756212e+86\n",
      "Gradient Descent(42/49): loss=5.092509070228384e+88\n",
      "Gradient Descent(43/49): loss=7.1955781582350465e+90\n",
      "Gradient Descent(44/49): loss=1.0167158137029519e+93\n",
      "Gradient Descent(45/49): loss=1.4365920612655935e+95\n",
      "Gradient Descent(46/49): loss=2.0298658904250014e+97\n",
      "Gradient Descent(47/49): loss=2.868145832214166e+99\n",
      "Gradient Descent(48/49): loss=4.052612812329755e+101\n",
      "Gradient Descent(49/49): loss=5.726232753646479e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.416496502199559\n",
      "Gradient Descent(2/49): loss=516.7620162630471\n",
      "Gradient Descent(3/49): loss=70823.32633197075\n",
      "Gradient Descent(4/49): loss=9854191.878863268\n",
      "Gradient Descent(5/49): loss=1376612890.41594\n",
      "Gradient Descent(6/49): loss=192544536953.95764\n",
      "Gradient Descent(7/49): loss=26941252325212.227\n",
      "Gradient Descent(8/49): loss=3770147815718527.5\n",
      "Gradient Descent(9/49): loss=5.2761434894423366e+17\n",
      "Gradient Descent(10/49): loss=7.383811337375012e+19\n",
      "Gradient Descent(11/49): loss=1.0333477071814274e+22\n",
      "Gradient Descent(12/49): loss=1.446148832830334e+24\n",
      "Gradient Descent(13/49): loss=2.0238564594522337e+26\n",
      "Gradient Descent(14/49): loss=2.8323472037082557e+28\n",
      "Gradient Descent(15/49): loss=3.963814258317169e+30\n",
      "Gradient Descent(16/49): loss=5.547280261693158e+32\n",
      "Gradient Descent(17/49): loss=7.763309899424905e+34\n",
      "Gradient Descent(18/49): loss=1.0864599922903772e+37\n",
      "Gradient Descent(19/49): loss=1.5204794486394554e+39\n",
      "Gradient Descent(20/49): loss=2.1278811650608447e+41\n",
      "Gradient Descent(21/49): loss=2.977927953556535e+43\n",
      "Gradient Descent(22/49): loss=4.167551761052942e+45\n",
      "Gradient Descent(23/49): loss=5.8324069460555745e+47\n",
      "Gradient Descent(24/49): loss=8.162339122545277e+49\n",
      "Gradient Descent(25/49): loss=1.1423033503610467e+52\n",
      "Gradient Descent(26/49): loss=1.598631133374784e+54\n",
      "Gradient Descent(27/49): loss=2.2372529151626907e+56\n",
      "Gradient Descent(28/49): loss=3.1309915726698595e+58\n",
      "Gradient Descent(29/49): loss=4.381761293812812e+60\n",
      "Gradient Descent(30/49): loss=6.132188985607531e+62\n",
      "Gradient Descent(31/49): loss=8.58187820689916e+64\n",
      "Gradient Descent(32/49): loss=1.2010170223211761e+67\n",
      "Gradient Descent(33/49): loss=1.6807997656569101e+69\n",
      "Gradient Descent(34/49): loss=2.352246304363235e+71\n",
      "Gradient Descent(35/49): loss=3.291922565343752e+73\n",
      "Gradient Descent(36/49): loss=4.606981061514712e+75\n",
      "Gradient Descent(37/49): loss=6.447379632983253e+77\n",
      "Gradient Descent(38/49): loss=9.022981335664535e+79\n",
      "Gradient Descent(39/49): loss=1.2627485399999568e+82\n",
      "Gradient Descent(40/49): loss=1.7671918138292126e+84\n",
      "Gradient Descent(41/49): loss=2.4731502812627385e+86\n",
      "Gradient Descent(42/49): loss=3.461125309570414e+88\n",
      "Gradient Descent(43/49): loss=4.843776983270291e+90\n",
      "Gradient Descent(44/49): loss=6.778770880899194e+92\n",
      "Gradient Descent(45/49): loss=9.486756887122908e+94\n",
      "Gradient Descent(46/49): loss=1.3276530187643045e+97\n",
      "Gradient Descent(47/49): loss=1.858024358805456e+99\n",
      "Gradient Descent(48/49): loss=2.6002686463422054e+101\n",
      "Gradient Descent(49/49): loss=3.639024968164212e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.412332131113731\n",
      "Gradient Descent(2/49): loss=517.439731544407\n",
      "Gradient Descent(3/49): loss=70985.07011839634\n",
      "Gradient Descent(4/49): loss=9885435.051773658\n",
      "Gradient Descent(5/49): loss=1382337013.5298011\n",
      "Gradient Descent(6/49): loss=193548438823.44302\n",
      "Gradient Descent(7/49): loss=27111063966328.63\n",
      "Gradient Descent(8/49): loss=3798074304127775.0\n",
      "Gradient Descent(9/49): loss=5.321087535354481e+17\n",
      "Gradient Descent(10/49): loss=7.454938689456259e+19\n",
      "Gradient Descent(11/49): loss=1.0444556626688972e+22\n",
      "Gradient Descent(12/49): loss=1.463311216356725e+24\n",
      "Gradient Descent(13/49): loss=2.0501406165335632e+26\n",
      "Gradient Descent(14/49): loss=2.872305858638551e+28\n",
      "Gradient Descent(15/49): loss=4.0241832333737297e+30\n",
      "Gradient Descent(16/49): loss=5.6379967369999e+32\n",
      "Gradient Descent(17/49): loss=7.898996045323438e+34\n",
      "Gradient Descent(18/49): loss=1.1066721318377384e+37\n",
      "Gradient Descent(19/49): loss=1.5504795817140993e+39\n",
      "Gradient Descent(20/49): loss=2.172266622464291e+41\n",
      "Gradient Descent(21/49): loss=3.043408204261241e+43\n",
      "Gradient Descent(22/49): loss=4.263902691446816e+45\n",
      "Gradient Descent(23/49): loss=5.9738506772999304e+47\n",
      "Gradient Descent(24/49): loss=8.369537134699261e+49\n",
      "Gradient Descent(25/49): loss=1.1725962973158366e+52\n",
      "Gradient Descent(26/49): loss=1.6428412400237175e+54\n",
      "Gradient Descent(27/49): loss=2.3016679705548365e+56\n",
      "Gradient Descent(28/49): loss=3.2247032261022367e+58\n",
      "Gradient Descent(29/49): loss=4.517902247181038e+60\n",
      "Gradient Descent(30/49): loss=6.329711382388289e+62\n",
      "Gradient Descent(31/49): loss=8.868108248542744e+64\n",
      "Gradient Descent(32/49): loss=1.2424475486621378e+67\n",
      "Gradient Descent(33/49): loss=1.7407048582545456e+69\n",
      "Gradient Descent(34/49): loss=2.4387777229016393e+71\n",
      "Gradient Descent(35/49): loss=3.4167979445321786e+73\n",
      "Gradient Descent(36/49): loss=4.787032489319733e+75\n",
      "Gradient Descent(37/49): loss=6.706770615591734e+77\n",
      "Gradient Descent(38/49): loss=9.396379111802684e+79\n",
      "Gradient Descent(39/49): loss=1.3164598205798678e+82\n",
      "Gradient Descent(40/49): loss=1.8443981863442312e+84\n",
      "Gradient Descent(41/49): loss=2.584055066938152e+86\n",
      "Gradient Descent(42/49): loss=3.620335694541005e+88\n",
      "Gradient Descent(43/49): loss=5.072194748813146e+90\n",
      "Gradient Descent(44/49): loss=7.10629116760661e+92\n",
      "Gradient Descent(45/49): loss=9.95611893068977e+94\n",
      "Gradient Descent(46/49): loss=1.3948809839637333e+97\n",
      "Gradient Descent(47/49): loss=1.954268498567274e+99\n",
      "Gradient Descent(48/49): loss=2.7379865439413684e+101\n",
      "Gradient Descent(49/49): loss=3.835998134493773e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.467954956632458\n",
      "Gradient Descent(2/49): loss=535.1018959832954\n",
      "Gradient Descent(3/49): loss=74891.89273106787\n",
      "Gradient Descent(4/49): loss=10633116.678118518\n",
      "Gradient Descent(5/49): loss=1515338043.1585698\n",
      "Gradient Descent(6/49): loss=216192294984.8348\n",
      "Gradient Descent(7/49): loss=30854610982475.15\n",
      "Gradient Descent(8/49): loss=4403997284156711.0\n",
      "Gradient Descent(9/49): loss=6.286211850739291e+17\n",
      "Gradient Descent(10/49): loss=8.972961005785617e+19\n",
      "Gradient Descent(11/49): loss=1.2808081546126098e+22\n",
      "Gradient Descent(12/49): loss=1.82823862422595e+24\n",
      "Gradient Descent(13/49): loss=2.609647408637904e+26\n",
      "Gradient Descent(14/49): loss=3.72503911717837e+28\n",
      "Gradient Descent(15/49): loss=5.317161581620981e+30\n",
      "Gradient Descent(16/49): loss=7.589774693442842e+32\n",
      "Gradient Descent(17/49): loss=1.0833727588329675e+35\n",
      "Gradient Descent(18/49): loss=1.546418152623631e+37\n",
      "Gradient Descent(19/49): loss=2.207374224808774e+39\n",
      "Gradient Descent(20/49): loss=3.150830168864884e+41\n",
      "Gradient Descent(21/49): loss=4.49752952709776e+43\n",
      "Gradient Descent(22/49): loss=6.419822955632913e+45\n",
      "Gradient Descent(23/49): loss=9.163725670621345e+47\n",
      "Gradient Descent(24/49): loss=1.30804024888002e+50\n",
      "Gradient Descent(25/49): loss=1.8671109919583704e+52\n",
      "Gradient Descent(26/49): loss=2.665134700004208e+54\n",
      "Gradient Descent(27/49): loss=3.804242490006813e+56\n",
      "Gradient Descent(28/49): loss=5.430217438071944e+58\n",
      "Gradient Descent(29/49): loss=7.751151905326584e+60\n",
      "Gradient Descent(30/49): loss=1.1064079209465337e+63\n",
      "Gradient Descent(31/49): loss=1.5792987964692167e+65\n",
      "Gradient Descent(32/49): loss=2.2543084167323557e+67\n",
      "Gradient Descent(33/49): loss=3.2178245491681386e+69\n",
      "Gradient Descent(34/49): loss=4.593158040122072e+71\n",
      "Gradient Descent(35/49): loss=6.556324143587044e+73\n",
      "Gradient Descent(36/49): loss=9.358568962856666e+75\n",
      "Gradient Descent(37/49): loss=1.3358523940310698e+78\n",
      "Gradient Descent(38/49): loss=1.9068103528659955e+80\n",
      "Gradient Descent(39/49): loss=2.7218020030081026e+82\n",
      "Gradient Descent(40/49): loss=3.8851300195869787e+84\n",
      "Gradient Descent(41/49): loss=5.545677184605617e+86\n",
      "Gradient Descent(42/49): loss=7.915960413372461e+88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=1.1299328680729155e+91\n",
      "Gradient Descent(44/49): loss=1.6128785639133207e+93\n",
      "Gradient Descent(45/49): loss=2.3022405449340537e+95\n",
      "Gradient Descent(46/49): loss=3.2862433944674226e+97\n",
      "Gradient Descent(47/49): loss=4.690819850012713e+99\n",
      "Gradient Descent(48/49): loss=6.695727681740789e+101\n",
      "Gradient Descent(49/49): loss=9.557555101568981e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.450100111880644\n",
      "Gradient Descent(2/49): loss=531.4194260791429\n",
      "Gradient Descent(3/49): loss=74370.35578826239\n",
      "Gradient Descent(4/49): loss=10571799.92132266\n",
      "Gradient Descent(5/49): loss=1509161483.9305637\n",
      "Gradient Descent(6/49): loss=215715940508.40884\n",
      "Gradient Descent(7/49): loss=30846497236741.24\n",
      "Gradient Descent(8/49): loss=4411502032925424.5\n",
      "Gradient Descent(9/49): loss=6.309364401236371e+17\n",
      "Gradient Descent(10/49): loss=9.023827012413057e+19\n",
      "Gradient Descent(11/49): loss=1.290618467766211e+22\n",
      "Gradient Descent(12/49): loss=1.8458892047839419e+24\n",
      "Gradient Descent(13/49): loss=2.6400587640897955e+26\n",
      "Gradient Descent(14/49): loss=3.775909927396604e+28\n",
      "Gradient Descent(15/49): loss=5.400446651085006e+30\n",
      "Gradient Descent(16/49): loss=7.723919553261893e+32\n",
      "Gradient Descent(17/49): loss=1.1047036929688885e+35\n",
      "Gradient Descent(18/49): loss=1.579988298714369e+37\n",
      "Gradient Descent(19/49): loss=2.259758016033491e+39\n",
      "Gradient Descent(20/49): loss=3.2319899433138524e+41\n",
      "Gradient Descent(21/49): loss=4.622512198304044e+43\n",
      "Gradient Descent(22/49): loss=6.611288834022104e+45\n",
      "Gradient Descent(23/49): loss=9.45571113110114e+47\n",
      "Gradient Descent(24/49): loss=1.3523909670215997e+50\n",
      "Gradient Descent(25/49): loss=1.934239849678996e+52\n",
      "Gradient Descent(26/49): loss=2.7664217577011775e+54\n",
      "Gradient Descent(27/49): loss=3.956639267231073e+56\n",
      "Gradient Descent(28/49): loss=5.658932607587697e+58\n",
      "Gradient Descent(29/49): loss=8.093615842727589e+60\n",
      "Gradient Descent(30/49): loss=1.157579033929785e+63\n",
      "Gradient Descent(31/49): loss=1.6556125788918482e+65\n",
      "Gradient Descent(32/49): loss=2.36791867426928e+67\n",
      "Gradient Descent(33/49): loss=3.38668533897354e+69\n",
      "Gradient Descent(34/49): loss=4.843763305662357e+71\n",
      "Gradient Descent(35/49): loss=6.927730395051183e+73\n",
      "Gradient Descent(36/49): loss=9.908297618591638e+75\n",
      "Gradient Descent(37/49): loss=1.4171215694063367e+78\n",
      "Gradient Descent(38/49): loss=2.0268199642171483e+80\n",
      "Gradient Descent(39/49): loss=2.898833280104651e+82\n",
      "Gradient Descent(40/49): loss=4.146019150293894e+84\n",
      "Gradient Descent(41/49): loss=5.929790758433365e+86\n",
      "Gradient Descent(42/49): loss=8.48100724192492e+88\n",
      "Gradient Descent(43/49): loss=1.2129851923575418e+91\n",
      "Gradient Descent(44/49): loss=1.7348565269527112e+93\n",
      "Gradient Descent(45/49): loss=2.4812563154713814e+95\n",
      "Gradient Descent(46/49): loss=3.5487850478799034e+97\n",
      "Gradient Descent(47/49): loss=5.075604337016431e+99\n",
      "Gradient Descent(48/49): loss=7.259318059100329e+101\n",
      "Gradient Descent(49/49): loss=1.0382546625798983e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.466685486970505\n",
      "Gradient Descent(2/49): loss=529.4892489608397\n",
      "Gradient Descent(3/49): loss=73453.62832884546\n",
      "Gradient Descent(4/49): loss=10344963.65369051\n",
      "Gradient Descent(5/49): loss=1462830040.8623352\n",
      "Gradient Descent(6/49): loss=207104040591.61743\n",
      "Gradient Descent(7/49): loss=29332628277593.766\n",
      "Gradient Descent(8/49): loss=4154967201377535.5\n",
      "Gradient Descent(9/49): loss=5.885751449037492e+17\n",
      "Gradient Descent(10/49): loss=8.337618865469899e+19\n",
      "Gradient Descent(11/49): loss=1.1810929157963019e+22\n",
      "Gradient Descent(12/49): loss=1.6731185615521215e+24\n",
      "Gradient Descent(13/49): loss=2.3701158577764094e+26\n",
      "Gradient Descent(14/49): loss=3.3574727966715105e+28\n",
      "Gradient Descent(15/49): loss=4.7561489967500393e+30\n",
      "Gradient Descent(16/49): loss=6.737494249774919e+32\n",
      "Gradient Descent(17/49): loss=9.544240319003883e+34\n",
      "Gradient Descent(18/49): loss=1.3520237651623782e+37\n",
      "Gradient Descent(19/49): loss=1.9152580001818827e+39\n",
      "Gradient Descent(20/49): loss=2.7131277589235498e+41\n",
      "Gradient Descent(21/49): loss=3.8433789265057205e+43\n",
      "Gradient Descent(22/49): loss=5.444476960010651e+45\n",
      "Gradient Descent(23/49): loss=7.712570093952549e+47\n",
      "Gradient Descent(24/49): loss=1.0925519180478388e+50\n",
      "Gradient Descent(25/49): loss=1.547693802571581e+52\n",
      "Gradient Descent(26/49): loss=2.1924414455278864e+54\n",
      "Gradient Descent(27/49): loss=3.1057819602831283e+56\n",
      "Gradient Descent(28/49): loss=4.399607389513606e+58\n",
      "Gradient Descent(29/49): loss=6.2324224396287926e+60\n",
      "Gradient Descent(30/49): loss=8.828762666089371e+62\n",
      "Gradient Descent(31/49): loss=1.2506702003783984e+65\n",
      "Gradient Descent(32/49): loss=1.7716819550744422e+67\n",
      "Gradient Descent(33/49): loss=2.509739937024727e+69\n",
      "Gradient Descent(34/49): loss=3.555262576026081e+71\n",
      "Gradient Descent(35/49): loss=5.036335358107288e+73\n",
      "Gradient Descent(36/49): loss=7.134402395581507e+75\n",
      "Gradient Descent(37/49): loss=1.0106494886235677e+78\n",
      "Gradient Descent(38/49): loss=1.4316719638461373e+80\n",
      "Gradient Descent(39/49): loss=2.0280865276591387e+82\n",
      "Gradient Descent(40/49): loss=2.8729590768982317e+84\n",
      "Gradient Descent(41/49): loss=4.069793741521865e+86\n",
      "Gradient Descent(42/49): loss=5.765213027821073e+88\n",
      "Gradient Descent(43/49): loss=8.166920332362805e+90\n",
      "Gradient Descent(44/49): loss=1.1569145388608367e+93\n",
      "Gradient Descent(45/49): loss=1.6388689931549217e+95\n",
      "Gradient Descent(46/49): loss=2.3215989483279397e+97\n",
      "Gradient Descent(47/49): loss=3.28874467659655e+99\n",
      "Gradient Descent(48/49): loss=4.658789820538097e+101\n",
      "Gradient Descent(49/49): loss=6.599576655008259e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.46213983655792\n",
      "Gradient Descent(2/49): loss=530.1108588851082\n",
      "Gradient Descent(3/49): loss=73606.33677451979\n",
      "Gradient Descent(4/49): loss=10374936.182030661\n",
      "Gradient Descent(5/49): loss=1468412280.8806355\n",
      "Gradient Descent(6/49): loss=208098725671.32364\n",
      "Gradient Descent(7/49): loss=29503429379037.62\n",
      "Gradient Descent(8/49): loss=4183461652496117.5\n",
      "Gradient Descent(9/49): loss=5.932246908233595e+17\n",
      "Gradient Descent(10/49): loss=8.412196390967948e+19\n",
      "Gradient Descent(11/49): loss=1.1928940160109236e+22\n",
      "Gradient Descent(12/49): loss=1.6915898922927098e+24\n",
      "Gradient Descent(13/49): loss=2.3987697322150042e+26\n",
      "Gradient Descent(14/49): loss=3.4015912498825165e+28\n",
      "Gradient Descent(15/49): loss=4.8236492452437886e+30\n",
      "Gradient Descent(16/49): loss=6.840208260761366e+32\n",
      "Gradient Descent(17/49): loss=9.69980341187652e+34\n",
      "Gradient Descent(18/49): loss=1.3754871618436754e+37\n",
      "Gradient Descent(19/49): loss=1.9505188442401076e+39\n",
      "Gradient Descent(20/49): loss=2.7659463994860025e+41\n",
      "Gradient Descent(21/49): loss=3.9222689430401834e+43\n",
      "Gradient Descent(22/49): loss=5.56199992339074e+45\n",
      "Gradient Descent(23/49): loss=7.887231497226931e+47\n",
      "Gradient Descent(24/49): loss=1.1184541810125925e+50\n",
      "Gradient Descent(25/49): loss=1.5860314933902124e+52\n",
      "Gradient Descent(26/49): loss=2.249082654194413e+54\n",
      "Gradient Descent(27/49): loss=3.1893268238869806e+56\n",
      "Gradient Descent(28/49): loss=4.5226464090128563e+58\n",
      "Gradient Descent(29/49): loss=6.413369237596246e+60\n",
      "Gradient Descent(30/49): loss=9.094521494268988e+62\n",
      "Gradient Descent(31/49): loss=1.2896547531500173e+65\n",
      "Gradient Descent(32/49): loss=1.8288036191574536e+67\n",
      "Gradient Descent(33/49): loss=2.5933473042101587e+69\n",
      "Gradient Descent(34/49): loss=3.6775136323016226e+71\n",
      "Gradient Descent(35/49): loss=5.2149230046467574e+73\n",
      "Gradient Descent(36/49): loss=7.39505673222301e+75\n",
      "Gradient Descent(37/49): loss=1.0486610065780053e+78\n",
      "Gradient Descent(38/49): loss=1.4870608117520519e+80\n",
      "Gradient Descent(39/49): loss=2.108736611714742e+82\n",
      "Gradient Descent(40/49): loss=2.9903081719617034e+84\n",
      "Gradient Descent(41/49): loss=4.2404266676195674e+86\n",
      "Gradient Descent(42/49): loss=6.0131656302377505e+88\n",
      "Gradient Descent(43/49): loss=8.527010070184925e+90\n",
      "Gradient Descent(44/49): loss=1.209178412971131e+93\n",
      "Gradient Descent(45/49): loss=1.7146836022954073e+95\n",
      "Gradient Descent(46/49): loss=2.4315186447601255e+97\n",
      "Gradient Descent(47/49): loss=3.4480314105187987e+99\n",
      "Gradient Descent(48/49): loss=4.88950419259365e+101\n",
      "Gradient Descent(49/49): loss=6.933594391413534e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.518458499617622\n",
      "Gradient Descent(2/49): loss=548.1953027353746\n",
      "Gradient Descent(3/49): loss=77655.25785796993\n",
      "Gradient Descent(4/49): loss=11159239.993045403\n",
      "Gradient Descent(5/49): loss=1609623975.6693122\n",
      "Gradient Descent(6/49): loss=232432653495.27103\n",
      "Gradient Descent(7/49): loss=33575267057782.23\n",
      "Gradient Descent(8/49): loss=4850528805129024.0\n",
      "Gradient Descent(9/49): loss=7.007672043471607e+17\n",
      "Gradient Descent(10/49): loss=1.0124259556905973e+20\n",
      "Gradient Descent(11/49): loss=1.4626968212335755e+22\n",
      "Gradient Descent(12/49): loss=2.1132255842359932e+24\n",
      "Gradient Descent(13/49): loss=3.0530756199065705e+26\n",
      "Gradient Descent(14/49): loss=4.410921344542296e+28\n",
      "Gradient Descent(15/49): loss=6.37266490022451e+30\n",
      "Gradient Descent(16/49): loss=9.206887915080664e+32\n",
      "Gradient Descent(17/49): loss=1.330162290608274e+35\n",
      "Gradient Descent(18/49): loss=1.9217478673330154e+37\n",
      "Gradient Descent(19/49): loss=2.7764393060524643e+39\n",
      "Gradient Descent(20/49): loss=4.011252127407634e+41\n",
      "Gradient Descent(21/49): loss=5.795244144259924e+43\n",
      "Gradient Descent(22/49): loss=8.372661110616109e+45\n",
      "Gradient Descent(23/49): loss=1.209637632660463e+48\n",
      "Gradient Descent(24/49): loss=1.747620240469436e+50\n",
      "Gradient Descent(25/49): loss=2.524868954500413e+52\n",
      "Gradient Descent(26/49): loss=3.647796637837359e+54\n",
      "Gradient Descent(27/49): loss=5.270142946349847e+56\n",
      "Gradient Descent(28/49): loss=7.614022773875875e+58\n",
      "Gradient Descent(29/49): loss=1.1000335928507091e+61\n",
      "Gradient Descent(30/49): loss=1.58927014186492e+63\n",
      "Gradient Descent(31/49): loss=2.296093137735772e+65\n",
      "Gradient Descent(32/49): loss=3.317273481883234e+67\n",
      "Gradient Descent(33/49): loss=4.7926206357889e+69\n",
      "Gradient Descent(34/49): loss=6.924123887895383e+71\n",
      "Gradient Descent(35/49): loss=1.0003606639946685e+74\n",
      "Gradient Descent(36/49): loss=1.4452679851920305e+76\n",
      "Gradient Descent(37/49): loss=2.088046465841615e+78\n",
      "Gradient Descent(38/49): loss=3.016698694072547e+80\n",
      "Gradient Descent(39/49): loss=4.358366137772204e+82\n",
      "Gradient Descent(40/49): loss=6.296736040693391e+84\n",
      "Gradient Descent(41/49): loss=9.09718998193985e+86\n",
      "Gradient Descent(42/49): loss=1.3143137179749664e+89\n",
      "Gradient Descent(43/49): loss=1.8988506920120766e+91\n",
      "Gradient Descent(44/49): loss=2.743358683123325e+93\n",
      "Gradient Descent(45/49): loss=3.963458999661185e+95\n",
      "Gradient Descent(46/49): loss=5.7261951704071534e+97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=8.272902818572637e+99\n",
      "Gradient Descent(48/49): loss=1.1952250841754064e+102\n",
      "Gradient Descent(49/49): loss=1.7267977554806863e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.500347730450945\n",
      "Gradient Descent(2/49): loss=544.4235577276181\n",
      "Gradient Descent(3/49): loss=77114.43402341405\n",
      "Gradient Descent(4/49): loss=11094860.474282645\n",
      "Gradient Descent(5/49): loss=1603056514.043595\n",
      "Gradient Descent(6/49): loss=231919284471.91263\n",
      "Gradient Descent(7/49): loss=33566224817230.41\n",
      "Gradient Descent(8/49): loss=4858758926238191.0\n",
      "Gradient Descent(9/49): loss=7.033423709081984e+17\n",
      "Gradient Descent(10/49): loss=1.0181558550632307e+20\n",
      "Gradient Descent(11/49): loss=1.4738854104808265e+22\n",
      "Gradient Descent(12/49): loss=2.1336040229942513e+24\n",
      "Gradient Descent(13/49): loss=3.088617542319877e+26\n",
      "Gradient Descent(14/49): loss=4.471101352777133e+28\n",
      "Gradient Descent(15/49): loss=6.472393576418848e+30\n",
      "Gradient Descent(16/49): loss=9.369476558592244e+32\n",
      "Gradient Descent(17/49): loss=1.3563311691426927e+35\n",
      "Gradient Descent(18/49): loss=1.9634333168994136e+37\n",
      "Gradient Descent(19/49): loss=2.8422781101593234e+39\n",
      "Gradient Descent(20/49): loss=4.114499223127347e+41\n",
      "Gradient Descent(21/49): loss=5.956174308794375e+43\n",
      "Gradient Descent(22/49): loss=8.622194457605423e+45\n",
      "Gradient Descent(23/49): loss=1.2481541575351874e+48\n",
      "Gradient Descent(24/49): loss=1.806835613179209e+50\n",
      "Gradient Descent(25/49): loss=2.6155863146760354e+52\n",
      "Gradient Descent(26/49): loss=3.7863387901041935e+54\n",
      "Gradient Descent(27/49): loss=5.4811272535750854e+56\n",
      "Gradient Descent(28/49): loss=7.9345134271668e+58\n",
      "Gradient Descent(29/49): loss=1.1486050298289864e+61\n",
      "Gradient Descent(30/49): loss=1.662727685394485e+63\n",
      "Gradient Descent(31/49): loss=2.4069747946244883e+65\n",
      "Gradient Descent(32/49): loss=3.4843514742963584e+67\n",
      "Gradient Descent(33/49): loss=5.0439685631711265e+69\n",
      "Gradient Descent(34/49): loss=7.301679825912627e+71\n",
      "Gradient Descent(35/49): loss=1.0569956496045375e+74\n",
      "Gradient Descent(36/49): loss=1.530113384755647e+76\n",
      "Gradient Descent(37/49): loss=2.2150015197170647e+78\n",
      "Gradient Descent(38/49): loss=3.2064497841984496e+80\n",
      "Gradient Descent(39/49): loss=4.641676372258017e+82\n",
      "Gradient Descent(40/49): loss=6.719319183152026e+84\n",
      "Gradient Descent(41/49): loss=9.726927658058885e+86\n",
      "Gradient Descent(42/49): loss=1.4080760131523896e+89\n",
      "Gradient Descent(43/49): loss=2.0383394721481747e+91\n",
      "Gradient Descent(44/49): loss=2.95071272069717e+93\n",
      "Gradient Descent(45/49): loss=4.2714698307383664e+95\n",
      "Gradient Descent(46/49): loss=6.183405923229672e+97\n",
      "Gradient Descent(47/49): loss=8.951136336323514e+99\n",
      "Gradient Descent(48/49): loss=1.2957719856373553e+102\n",
      "Gradient Descent(49/49): loss=1.8757674731744858e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.5171894012993565\n",
      "Gradient Descent(2/49): loss=542.450366715385\n",
      "Gradient Descent(3/49): loss=76164.84044353344\n",
      "Gradient Descent(4/49): loss=10856990.766642837\n",
      "Gradient Descent(5/49): loss=1553876926.395676\n",
      "Gradient Descent(6/49): loss=222666459389.59583\n",
      "Gradient Descent(7/49): loss=31919875217138.79\n",
      "Gradient Descent(8/49): loss=4576378894548867.0\n",
      "Gradient Descent(9/49): loss=6.561460744765352e+17\n",
      "Gradient Descent(10/49): loss=9.407731763936297e+19\n",
      "Gradient Descent(11/49): loss=1.3488735351424032e+22\n",
      "Gradient Descent(12/49): loss=1.9340075811042127e+24\n",
      "Gradient Descent(13/49): loss=2.7729709371861162e+26\n",
      "Gradient Descent(14/49): loss=3.975873260746522e+28\n",
      "Gradient Descent(15/49): loss=5.70058949840332e+30\n",
      "Gradient Descent(16/49): loss=8.173480151891746e+32\n",
      "Gradient Descent(17/49): loss=1.1719099960960064e+35\n",
      "Gradient Descent(18/49): loss=1.6802794109294275e+37\n",
      "Gradient Descent(19/49): loss=2.409177249034024e+39\n",
      "Gradient Descent(20/49): loss=3.454267772771509e+41\n",
      "Gradient Descent(21/49): loss=4.952713982160642e+43\n",
      "Gradient Descent(22/49): loss=7.101179585142793e+45\n",
      "Gradient Descent(23/49): loss=1.0181640143655113e+48\n",
      "Gradient Descent(24/49): loss=1.4598390981693332e+50\n",
      "Gradient Descent(25/49): loss=2.0931108961590504e+52\n",
      "Gradient Descent(26/49): loss=3.0010932226125876e+54\n",
      "Gradient Descent(27/49): loss=4.302954299908034e+56\n",
      "Gradient Descent(28/49): loss=6.169557002624237e+58\n",
      "Gradient Descent(29/49): loss=8.845883770934715e+60\n",
      "Gradient Descent(30/49): loss=1.2683189353077098e+63\n",
      "Gradient Descent(31/49): loss=1.8185101266485629e+65\n",
      "Gradient Descent(32/49): loss=2.6073718436767117e+67\n",
      "Gradient Descent(33/49): loss=3.738438313636023e+69\n",
      "Gradient Descent(34/49): loss=5.360156457451828e+71\n",
      "Gradient Descent(35/49): loss=7.685368819264635e+73\n",
      "Gradient Descent(36/49): loss=1.1019248105344357e+76\n",
      "Gradient Descent(37/49): loss=1.5799349603465382e+78\n",
      "Gradient Descent(38/49): loss=2.2653038166139272e+80\n",
      "Gradient Descent(39/49): loss=3.247982676729973e+82\n",
      "Gradient Descent(40/49): loss=4.656943316374567e+84\n",
      "Gradient Descent(41/49): loss=6.677104901852491e+86\n",
      "Gradient Descent(42/49): loss=9.573603722763608e+88\n",
      "Gradient Descent(43/49): loss=1.3726591028259041e+91\n",
      "Gradient Descent(44/49): loss=1.9681126012043773e+93\n",
      "Gradient Descent(45/49): loss=2.8218712155444424e+95\n",
      "Gradient Descent(46/49): loss=4.045986572234404e+97\n",
      "Gradient Descent(47/49): loss=5.801117801735938e+99\n",
      "Gradient Descent(48/49): loss=8.317617260660567e+101\n",
      "Gradient Descent(49/49): loss=1.1925763147601675e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.512258109794997\n",
      "Gradient Descent(2/49): loss=543.0135171504593\n",
      "Gradient Descent(3/49): loss=76307.77828128666\n",
      "Gradient Descent(4/49): loss=10885516.643734861\n",
      "Gradient Descent(5/49): loss=1559281637.7858377\n",
      "Gradient Descent(6/49): loss=223645498823.32364\n",
      "Gradient Descent(7/49): loss=32090607113626.477\n",
      "Gradient Descent(8/49): loss=4605280888011238.0\n",
      "Gradient Descent(9/49): loss=6.609285768235324e+17\n",
      "Gradient Descent(10/49): loss=9.485489029027963e+19\n",
      "Gradient Descent(11/49): loss=1.3613418844256905e+22\n",
      "Gradient Descent(12/49): loss=1.9537790862492988e+24\n",
      "Gradient Descent(13/49): loss=2.8040384555508533e+26\n",
      "Gradient Descent(14/49): loss=4.024320505617924e+28\n",
      "Gradient Descent(15/49): loss=5.775654251901528e+30\n",
      "Gradient Descent(16/49): loss=8.28914664021121e+32\n",
      "Gradient Descent(17/49): loss=1.18964795249314e+35\n",
      "Gradient Descent(18/49): loss=1.7073678581927701e+37\n",
      "Gradient Descent(19/49): loss=2.4503929919961148e+39\n",
      "Gradient Descent(20/49): loss=3.516773370293594e+41\n",
      "Gradient Descent(21/49): loss=5.047229150437372e+43\n",
      "Gradient Descent(22/49): loss=7.243720142178912e+45\n",
      "Gradient Descent(23/49): loss=1.0396096538285909e+48\n",
      "Gradient Descent(24/49): loss=1.492034770979705e+50\n",
      "Gradient Descent(25/49): loss=2.1413496398546574e+52\n",
      "Gradient Descent(26/49): loss=3.073238217562982e+54\n",
      "Gradient Descent(27/49): loss=4.41067304755151e+56\n",
      "Gradient Descent(28/49): loss=6.330142785945474e+58\n",
      "Gradient Descent(29/49): loss=9.08494174436775e+60\n",
      "Gradient Descent(30/49): loss=1.303859475047026e+63\n",
      "Gradient Descent(31/49): loss=1.871282808966678e+65\n",
      "Gradient Descent(32/49): loss=2.6856416800651982e+67\n",
      "Gradient Descent(33/49): loss=3.854399345273873e+69\n",
      "Gradient Descent(34/49): loss=5.531785726712064e+71\n",
      "Gradient Descent(35/49): loss=7.939149679385619e+73\n",
      "Gradient Descent(36/49): loss=1.1394168311206856e+76\n",
      "Gradient Descent(37/49): loss=1.635276783371554e+78\n",
      "Gradient Descent(38/49): loss=2.3469287842657595e+80\n",
      "Gradient Descent(39/49): loss=3.368282834089294e+82\n",
      "Gradient Descent(40/49): loss=4.834117390558139e+84\n",
      "Gradient Descent(41/49): loss=6.937864810279528e+86\n",
      "Gradient Descent(42/49): loss=9.95713679186373e+88\n",
      "Gradient Descent(43/49): loss=1.4290358172587621e+91\n",
      "Gradient Descent(44/49): loss=2.0509343295124017e+93\n",
      "Gradient Descent(45/49): loss=2.9434752951407675e+95\n",
      "Gradient Descent(46/49): loss=4.224438924460295e+97\n",
      "Gradient Descent(47/49): loss=6.062861902036744e+99\n",
      "Gradient Descent(48/49): loss=8.701343563125314e+101\n",
      "Gradient Descent(49/49): loss=1.2488059439075655e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.5692768892345486\n",
      "Gradient Descent(2/49): loss=561.5277642579453\n",
      "Gradient Descent(3/49): loss=80503.07804351598\n",
      "Gradient Descent(4/49): loss=11708002.52047929\n",
      "Gradient Descent(5/49): loss=1709157371.3266916\n",
      "Gradient Descent(6/49): loss=249784413126.6699\n",
      "Gradient Descent(7/49): loss=36517302506215.1\n",
      "Gradient Descent(8/49): loss=5339240604370637.0\n",
      "Gradient Descent(9/49): loss=7.806842689605535e+17\n",
      "Gradient Descent(10/49): loss=1.1415007721559858e+20\n",
      "Gradient Descent(11/49): loss=1.6690853479705299e+22\n",
      "Gradient Descent(12/49): loss=2.440514422200399e+24\n",
      "Gradient Descent(13/49): loss=3.568489081449272e+26\n",
      "Gradient Descent(14/49): loss=5.217799868523807e+28\n",
      "Gradient Descent(15/49): loss=7.629401654579334e+30\n",
      "Gradient Descent(16/49): loss=1.11556157353044e+33\n",
      "Gradient Descent(17/49): loss=1.6311601951972774e+35\n",
      "Gradient Descent(18/49): loss=2.385062061004778e+37\n",
      "Gradient Descent(19/49): loss=3.4874079528733755e+39\n",
      "Gradient Descent(20/49): loss=5.099244346798211e+41\n",
      "Gradient Descent(21/49): loss=7.456051388542584e+43\n",
      "Gradient Descent(22/49): loss=1.090214520593373e+46\n",
      "Gradient Descent(23/49): loss=1.594098054023822e+48\n",
      "Gradient Descent(24/49): loss=2.3308702625451823e+50\n",
      "Gradient Descent(25/49): loss=3.4081693827465594e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=4.983382699648444e+54\n",
      "Gradient Descent(27/49): loss=7.286639935466848e+56\n",
      "Gradient Descent(28/49): loss=1.0654433895451545e+59\n",
      "Gradient Descent(29/49): loss=1.5578780156271575e+61\n",
      "Gradient Descent(30/49): loss=2.277909774831415e+63\n",
      "Gradient Descent(31/49): loss=3.3307312191472304e+65\n",
      "Gradient Descent(32/49): loss=4.870153584122082e+67\n",
      "Gradient Descent(33/49): loss=7.121077737101218e+69\n",
      "Gradient Descent(34/49): loss=1.0412350917056292e+72\n",
      "Gradient Descent(35/49): loss=1.5224809449146186e+74\n",
      "Gradient Descent(36/49): loss=2.226152620184077e+76\n",
      "Gradient Descent(37/49): loss=3.255052554125963e+78\n",
      "Gradient Descent(38/49): loss=4.759497185438186e+80\n",
      "Gradient Descent(39/49): loss=6.959277333166344e+82\n",
      "Gradient Descent(40/49): loss=1.0175768387488588e+85\n",
      "Gradient Descent(41/49): loss=1.4878881429589628e+87\n",
      "Gradient Descent(42/49): loss=2.175571457266863e+89\n",
      "Gradient Descent(43/49): loss=3.181093409523049e+91\n",
      "Gradient Descent(44/49): loss=4.651355048031291e+93\n",
      "Gradient Descent(45/49): loss=6.801153250664802e+95\n",
      "Gradient Descent(46/49): loss=9.944561329199482e+97\n",
      "Gradient Descent(47/49): loss=1.454081335699102e+100\n",
      "Gradient Descent(48/49): loss=2.1261395659758927e+102\n",
      "Gradient Descent(49/49): loss=3.108814715536377e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.550908460356578\n",
      "Gradient Descent(2/49): loss=557.6651317690734\n",
      "Gradient Descent(3/49): loss=79942.37680085172\n",
      "Gradient Descent(4/49): loss=11640427.241819592\n",
      "Gradient Descent(5/49): loss=1702176889.4595013\n",
      "Gradient Descent(6/49): loss=249231412951.33444\n",
      "Gradient Descent(7/49): loss=36507239004075.81\n",
      "Gradient Descent(8/49): loss=5348261281592078.0\n",
      "Gradient Descent(9/49): loss=7.83546721290365e+17\n",
      "Gradient Descent(10/49): loss=1.1479507636170621e+20\n",
      "Gradient Descent(11/49): loss=1.681835869422901e+22\n",
      "Gradient Descent(12/49): loss=2.4640221575345835e+24\n",
      "Gradient Descent(13/49): loss=3.6099885056662474e+26\n",
      "Gradient Descent(14/49): loss=5.288921226129636e+28\n",
      "Gradient Descent(15/49): loss=7.748692039143918e+30\n",
      "Gradient Descent(16/49): loss=1.1352452943438824e+33\n",
      "Gradient Descent(17/49): loss=1.6632250694723003e+35\n",
      "Gradient Descent(18/49): loss=2.436757633388319e+37\n",
      "Gradient Descent(19/49): loss=3.570044653845936e+39\n",
      "Gradient Descent(20/49): loss=5.2304007006279484e+41\n",
      "Gradient Descent(21/49): loss=7.662954988923915e+43\n",
      "Gradient Descent(22/49): loss=1.1226841407474811e+46\n",
      "Gradient Descent(23/49): loss=1.6448219801842736e+48\n",
      "Gradient Descent(24/49): loss=2.4097956391385904e+50\n",
      "Gradient Descent(25/49): loss=3.5305431787608776e+52\n",
      "Gradient Descent(26/49): loss=5.172527883548509e+54\n",
      "Gradient Descent(27/49): loss=7.578166687506483e+56\n",
      "Gradient Descent(28/49): loss=1.1102619770555382e+59\n",
      "Gradient Descent(29/49): loss=1.6266225177225303e+61\n",
      "Gradient Descent(30/49): loss=2.3831319723107496e+63\n",
      "Gradient Descent(31/49): loss=3.491478776158498e+65\n",
      "Gradient Descent(32/49): loss=5.115295412089606e+67\n",
      "Gradient Descent(33/49): loss=7.494316543357149e+69\n",
      "Gradient Descent(34/49): loss=1.097977260888897e+72\n",
      "Gradient Descent(35/49): loss=1.6086244268634007e+74\n",
      "Gradient Descent(36/49): loss=2.3567633309697965e+76\n",
      "Gradient Descent(37/49): loss=3.452846609468717e+78\n",
      "Gradient Descent(38/49): loss=5.058696200782156e+80\n",
      "Gradient Descent(39/49): loss=7.411394175933393e+82\n",
      "Gradient Descent(40/49): loss=1.0858284714264148e+85\n",
      "Gradient Descent(41/49): loss=1.5908254794877994e+87\n",
      "Gradient Descent(42/49): loss=2.3306864507458338e+89\n",
      "Gradient Descent(43/49): loss=3.414641896130007e+91\n",
      "Gradient Descent(44/49): loss=5.0027232427919225e+93\n",
      "Gradient Descent(45/49): loss=7.329389319663401e+95\n",
      "Gradient Descent(46/49): loss=1.0738141046798352e+98\n",
      "Gradient Descent(47/49): loss=1.5732234721327459e+100\n",
      "Gradient Descent(48/49): loss=2.3048981033894663e+102\n",
      "Gradient Descent(49/49): loss=3.37685990649911e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.568008245186116\n",
      "Gradient Descent(2/49): loss=555.6482183459482\n",
      "Gradient Descent(3/49): loss=78958.9417150176\n",
      "Gradient Descent(4/49): loss=11391057.456139056\n",
      "Gradient Descent(5/49): loss=1649992560.4656584\n",
      "Gradient Descent(6/49): loss=239294209453.18234\n",
      "Gradient Descent(7/49): loss=34717710302276.605\n",
      "Gradient Descent(8/49): loss=5037610195961111.0\n",
      "Gradient Descent(9/49): loss=7.309974963085056e+17\n",
      "Gradient Descent(10/49): loss=1.0607500923968953e+20\n",
      "Gradient Descent(11/49): loss=1.5392607456459963e+22\n",
      "Gradient Descent(12/49): loss=2.233633653312181e+24\n",
      "Gradient Descent(13/49): loss=3.2412453260416144e+26\n",
      "Gradient Descent(14/49): loss=4.703400180582815e+28\n",
      "Gradient Descent(15/49): loss=6.825146568118632e+30\n",
      "Gradient Descent(16/49): loss=9.904032123320317e+32\n",
      "Gradient Descent(17/49): loss=1.4371830974825804e+35\n",
      "Gradient Descent(18/49): loss=2.085509450924171e+37\n",
      "Gradient Descent(19/49): loss=3.026301714921569e+39\n",
      "Gradient Descent(20/49): loss=4.3914939189239995e+41\n",
      "Gradient Descent(21/49): loss=6.372536732257986e+43\n",
      "Gradient Descent(22/49): loss=9.247245961078285e+45\n",
      "Gradient Descent(23/49): loss=1.3418762646347075e+48\n",
      "Gradient Descent(24/49): loss=1.9472088416088068e+50\n",
      "Gradient Descent(25/49): loss=2.8256124448811683e+52\n",
      "Gradient Descent(26/49): loss=4.100271895886029e+54\n",
      "Gradient Descent(27/49): loss=5.949941808421383e+56\n",
      "Gradient Descent(28/49): loss=8.634014626962134e+58\n",
      "Gradient Descent(29/49): loss=1.2528897084856571e+61\n",
      "Gradient Descent(30/49): loss=1.8180796413378223e+63\n",
      "Gradient Descent(31/49): loss=2.638231888936367e+65\n",
      "Gradient Descent(32/49): loss=3.8283622683762675e+67\n",
      "Gradient Descent(33/49): loss=5.555371277024468e+69\n",
      "Gradient Descent(34/49): loss=8.061449743280972e+71\n",
      "Gradient Descent(35/49): loss=1.16980429790919e+74\n",
      "Gradient Descent(36/49): loss=1.69751364703026e+76\n",
      "Gradient Descent(37/49): loss=2.4632774789802223e+78\n",
      "Gradient Descent(38/49): loss=3.5744843342299304e+80\n",
      "Gradient Descent(39/49): loss=5.1869666997258944e+82\n",
      "Gradient Descent(40/49): loss=7.5268545133690114e+84\n",
      "Gradient Descent(41/49): loss=1.0922286982952342e+87\n",
      "Gradient Descent(42/49): loss=1.5849429894795914e+89\n",
      "Gradient Descent(43/49): loss=2.2999251748478398e+91\n",
      "Gradient Descent(44/49): loss=3.337442321275995e+93\n",
      "Gradient Descent(45/49): loss=4.8429928806623364e+95\n",
      "Gradient Descent(46/49): loss=7.027710978740973e+97\n",
      "Gradient Descent(47/49): loss=1.0197975263998647e+100\n",
      "Gradient Descent(48/49): loss=1.479837457740192e+102\n",
      "Gradient Descent(49/49): loss=2.1474055826178597e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.562686950824961\n",
      "Gradient Descent(2/49): loss=556.1505105323157\n",
      "Gradient Descent(3/49): loss=79091.34463601964\n",
      "Gradient Descent(4/49): loss=11417949.770298006\n",
      "Gradient Descent(5/49): loss=1655180979.1759095\n",
      "Gradient Descent(6/49): loss=240250417978.80292\n",
      "Gradient Descent(7/49): loss=34887150169574.04\n",
      "Gradient Descent(8/49): loss=5066726477615136.0\n",
      "Gradient Descent(9/49): loss=7.358846120283942e+17\n",
      "Gradient Descent(10/49): loss=1.0688058072753121e+20\n",
      "Gradient Descent(11/49): loss=1.552351926027939e+22\n",
      "Gradient Descent(12/49): loss=2.2546666378285494e+24\n",
      "Gradient Descent(13/49): loss=3.274724381281209e+26\n",
      "Gradient Descent(14/49): loss=4.756278273772271e+28\n",
      "Gradient Descent(15/49): loss=6.908118646726548e+30\n",
      "Gradient Descent(16/49): loss=1.003349711436423e+33\n",
      "Gradient Descent(17/49): loss=1.45728627825933e+35\n",
      "Gradient Descent(18/49): loss=2.116593325485995e+37\n",
      "Gradient Descent(19/49): loss=3.074184788723256e+39\n",
      "Gradient Descent(20/49): loss=4.465010828594182e+41\n",
      "Gradient Descent(21/49): loss=6.485075905192968e+43\n",
      "Gradient Descent(22/49): loss=9.419061030760367e+45\n",
      "Gradient Descent(23/49): loss=1.3680442912187976e+48\n",
      "Gradient Descent(24/49): loss=1.986976383985097e+50\n",
      "Gradient Descent(25/49): loss=2.8859264103227676e+52\n",
      "Gradient Descent(26/49): loss=4.191580389646233e+54\n",
      "Gradient Descent(27/49): loss=6.087939768673569e+56\n",
      "Gradient Descent(28/49): loss=8.842252129662043e+58\n",
      "Gradient Descent(29/49): loss=1.2842673497991767e+61\n",
      "Gradient Descent(30/49): loss=1.8652969872091297e+63\n",
      "Gradient Descent(31/49): loss=2.7091966879291477e+65\n",
      "Gradient Descent(32/49): loss=3.9348944131776163e+67\n",
      "Gradient Descent(33/49): loss=5.715123642311729e+69\n",
      "Gradient Descent(34/49): loss=8.300766124124225e+71\n",
      "Gradient Descent(35/49): loss=1.205620780227562e+74\n",
      "Gradient Descent(36/49): loss=1.751069050713518e+76\n",
      "Gradient Descent(37/49): loss=2.543289623614466e+78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/49): loss=3.6939274935784415e+80\n",
      "Gradient Descent(39/49): loss=5.3651382057001535e+82\n",
      "Gradient Descent(40/49): loss=7.792439893934923e+84\n",
      "Gradient Descent(41/49): loss=1.1317904063696707e+87\n",
      "Gradient Descent(42/49): loss=1.643836258457937e+89\n",
      "Gradient Descent(43/49): loss=2.3875424543388247e+91\n",
      "Gradient Descent(44/49): loss=3.4677170198311856e+93\n",
      "Gradient Descent(45/49): loss=5.03658534229374e+95\n",
      "Gradient Descent(46/49): loss=7.315242785134466e+97\n",
      "Gradient Descent(47/49): loss=1.0624812917612821e+100\n",
      "Gradient Descent(48/49): loss=1.543170238500806e+102\n",
      "Gradient Descent(49/49): loss=2.2413330036588468e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.620410125483239\n",
      "Gradient Descent(2/49): loss=575.1021735637369\n",
      "Gradient Descent(3/49): loss=83437.40578804108\n",
      "Gradient Descent(4/49): loss=12280234.194193669\n",
      "Gradient Descent(5/49): loss=1814196241.2192378\n",
      "Gradient Descent(6/49): loss=268316293100.97046\n",
      "Gradient Descent(7/49): loss=39697242644957.805\n",
      "Gradient Descent(8/49): loss=5873828420702061.0\n",
      "Gradient Descent(9/49): loss=8.69155270790908e+17\n",
      "Gradient Descent(10/49): loss=1.286110693119608e+20\n",
      "Gradient Descent(11/49): loss=1.9030968592766364e+22\n",
      "Gradient Descent(12/49): loss=2.816073188553239e+24\n",
      "Gradient Descent(13/49): loss=4.167034960945456e+26\n",
      "Gradient Descent(14/49): loss=6.166097709006837e+28\n",
      "Gradient Descent(15/49): loss=9.124176487609981e+30\n",
      "Gradient Descent(16/49): loss=1.3501342588242378e+33\n",
      "Gradient Descent(17/49): loss=1.9978378613524213e+35\n",
      "Gradient Descent(18/49): loss=2.9562660892714386e+37\n",
      "Gradient Descent(19/49): loss=4.374483717226598e+39\n",
      "Gradient Descent(20/49): loss=6.4730667730615e+41\n",
      "Gradient Descent(21/49): loss=9.57840882691505e+43\n",
      "Gradient Descent(22/49): loss=1.4173485130507106e+46\n",
      "Gradient Descent(23/49): loss=2.0972969976087355e+48\n",
      "Gradient Descent(24/49): loss=3.1034390311787704e+50\n",
      "Gradient Descent(25/49): loss=4.59226033853551e+52\n",
      "Gradient Descent(26/49): loss=6.795317969845604e+54\n",
      "Gradient Descent(27/49): loss=1.005525447323277e+57\n",
      "Gradient Descent(28/49): loss=1.4879089244997673e+59\n",
      "Gradient Descent(29/49): loss=2.2017075485254243e+61\n",
      "Gradient Descent(30/49): loss=3.2579387416898303e+63\n",
      "Gradient Descent(31/49): loss=4.820878618376108e+65\n",
      "Gradient Descent(32/49): loss=7.133611923305048e+67\n",
      "Gradient Descent(33/49): loss=1.0555839111639203e+70\n",
      "Gradient Descent(34/49): loss=1.5619820723186693e+72\n",
      "Gradient Descent(35/49): loss=2.3113160104484257e+74\n",
      "Gradient Descent(36/49): loss=3.4201299712902915e+76\n",
      "Gradient Descent(37/49): loss=5.060878290826498e+78\n",
      "Gradient Descent(38/49): loss=7.488747296026352e+80\n",
      "Gradient Descent(39/49): loss=1.1081344549501824e+83\n",
      "Gradient Descent(40/49): loss=1.6397428324218062e+85\n",
      "Gradient Descent(41/49): loss=2.426381152997855e+87\n",
      "Gradient Descent(42/49): loss=3.59039562986104e+89\n",
      "Gradient Descent(43/49): loss=5.312825960174561e+91\n",
      "Gradient Descent(44/49): loss=7.861562510925046e+93\n",
      "Gradient Descent(45/49): loss=1.163301142865816e+96\n",
      "Gradient Descent(46/49): loss=1.721374786643625e+98\n",
      "Gradient Descent(47/49): loss=2.547174628224508e+100\n",
      "Gradient Descent(48/49): loss=3.7691376898352585e+102\n",
      "Gradient Descent(49/49): loss=5.5773164381898774e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.601782301597543\n",
      "Gradient Descent(2/49): loss=571.147021914396\n",
      "Gradient Descent(3/49): loss=82856.2222500407\n",
      "Gradient Descent(4/49): loss=12209325.239156077\n",
      "Gradient Descent(5/49): loss=1806779516.173883\n",
      "Gradient Descent(6/49): loss=267720878259.08765\n",
      "Gradient Descent(7/49): loss=39686056913567.52\n",
      "Gradient Descent(8/49): loss=5883710258631726.0\n",
      "Gradient Descent(9/49): loss=8.723350787412169e+17\n",
      "Gradient Descent(10/49): loss=1.2933661992744221e+20\n",
      "Gradient Descent(11/49): loss=1.917616128118243e+22\n",
      "Gradient Descent(12/49): loss=2.8431678083101487e+24\n",
      "Gradient Descent(13/49): loss=4.215445959853301e+26\n",
      "Gradient Descent(14/49): loss=6.250066368049736e+28\n",
      "Gradient Descent(15/49): loss=9.26671389725288e+30\n",
      "Gradient Descent(16/49): loss=1.3739372172050233e+33\n",
      "Gradient Descent(17/49): loss=2.037079711721615e+35\n",
      "Gradient Descent(18/49): loss=3.0202935822545697e+37\n",
      "Gradient Descent(19/49): loss=4.478064003096977e+39\n",
      "Gradient Descent(20/49): loss=6.639439734468665e+41\n",
      "Gradient Descent(21/49): loss=9.844021871950413e+43\n",
      "Gradient Descent(22/49): loss=1.4595322872560885e+46\n",
      "Gradient Descent(23/49): loss=2.163987976934076e+48\n",
      "Gradient Descent(24/49): loss=3.208455205283262e+50\n",
      "Gradient Descent(25/49): loss=4.7570434374139055e+52\n",
      "Gradient Descent(26/49): loss=7.053070969538565e+54\n",
      "Gradient Descent(27/49): loss=1.0457295745946544e+57\n",
      "Gradient Descent(28/49): loss=1.5504598605414073e+59\n",
      "Gradient Descent(29/49): loss=2.2988025179281405e+61\n",
      "Gradient Descent(30/49): loss=3.4083391327444354e+63\n",
      "Gradient Descent(31/49): loss=5.053403044932715e+65\n",
      "Gradient Descent(32/49): loss=7.492471065804048e+67\n",
      "Gradient Descent(33/49): loss=1.1108776041167388e+70\n",
      "Gradient Descent(34/49): loss=1.6470521413961835e+72\n",
      "Gradient Descent(35/49): loss=2.442015885840699e+74\n",
      "Gradient Descent(36/49): loss=3.6206756524679355e+76\n",
      "Gradient Descent(37/49): loss=5.368225594429777e+78\n",
      "Gradient Descent(38/49): loss=7.959245400246821e+80\n",
      "Gradient Descent(39/49): loss=1.180084298377543e+83\n",
      "Gradient Descent(40/49): loss=1.7496620360945684e+85\n",
      "Gradient Descent(41/49): loss=2.594151320172185e+87\n",
      "Gradient Descent(42/49): loss=3.846240549959191e+89\n",
      "Gradient Descent(43/49): loss=5.70266131089398e+91\n",
      "Gradient Descent(44/49): loss=8.455099363744102e+93\n",
      "Gradient Descent(45/49): loss=1.2536025086080261e+96\n",
      "Gradient Descent(46/49): loss=1.8586644366677625e+98\n",
      "Gradient Descent(47/49): loss=2.7557646577856853e+100\n",
      "Gradient Descent(48/49): loss=4.0858579414774514e+102\n",
      "Gradient Descent(49/49): loss=6.057932077316256e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.619142018630784\n",
      "Gradient Descent(2/49): loss=569.0856699247073\n",
      "Gradient Descent(3/49): loss=81837.94726912778\n",
      "Gradient Descent(4/49): loss=11947971.926945666\n",
      "Gradient Descent(5/49): loss=1751426243.0244133\n",
      "Gradient Descent(6/49): loss=257053181414.54007\n",
      "Gradient Descent(7/49): loss=37741858085993.14\n",
      "Gradient Descent(8/49): loss=5542151070722921.0\n",
      "Gradient Descent(9/49): loss=8.138631062232536e+17\n",
      "Gradient Descent(10/49): loss=1.1951715594140457e+20\n",
      "Gradient Descent(11/49): loss=1.7551372194166233e+22\n",
      "Gradient Descent(12/49): loss=2.577463596921469e+24\n",
      "Gradient Descent(13/49): loss=3.7850726072514665e+26\n",
      "Gradient Descent(14/49): loss=5.558478865241311e+28\n",
      "Gradient Descent(15/49): loss=8.162773116189305e+30\n",
      "Gradient Descent(16/49): loss=1.1987248259401794e+33\n",
      "Gradient Descent(17/49): loss=1.760359128067496e+35\n",
      "Gradient Descent(18/49): loss=2.585133967824508e+37\n",
      "Gradient Descent(19/49): loss=3.796337650198511e+39\n",
      "Gradient Descent(20/49): loss=5.575022314760607e+41\n",
      "Gradient Descent(21/49): loss=8.18706782062777e+43\n",
      "Gradient Descent(22/49): loss=1.2022925777998287e+46\n",
      "Gradient Descent(23/49): loss=1.7655984710437312e+48\n",
      "Gradient Descent(24/49): loss=2.592828084046028e+50\n",
      "Gradient Descent(25/49): loss=3.807636664665318e+52\n",
      "Gradient Descent(26/49): loss=5.591615217112138e+54\n",
      "Gradient Descent(27/49): loss=8.211434937159676e+56\n",
      "Gradient Descent(28/49): loss=1.2058709533670697e+59\n",
      "Gradient Descent(29/49): loss=1.7708534102778805e+61\n",
      "Gradient Descent(30/49): loss=2.600545101394633e+63\n",
      "Gradient Descent(31/49): loss=3.8189693088861683e+65\n",
      "Gradient Descent(32/49): loss=5.60825750508734e+67\n",
      "Gradient Descent(33/49): loss=8.235874577515691e+69\n",
      "Gradient Descent(34/49): loss=1.2094599792366923e+72\n",
      "Gradient Descent(35/49): loss=1.7761239897566103e+74\n",
      "Gradient Descent(36/49): loss=2.6082850868532864e+76\n",
      "Gradient Descent(37/49): loss=3.830335682383001e+78\n",
      "Gradient Descent(38/49): loss=5.624949325396252e+80\n",
      "Gradient Descent(39/49): loss=8.26038695741444e+82\n",
      "Gradient Descent(40/49): loss=1.2130596871006685e+85\n",
      "Gradient Descent(41/49): loss=1.7814102560267505e+87\n",
      "Gradient Descent(42/49): loss=2.616048108780262e+89\n",
      "Gradient Descent(43/49): loss=3.8417358855432846e+91\n",
      "Gradient Descent(44/49): loss=5.6416908254612875e+93\n",
      "Gradient Descent(45/49): loss=8.284972293349873e+95\n",
      "Gradient Descent(46/49): loss=1.2166701087517103e+98\n",
      "Gradient Descent(47/49): loss=1.78671225577674e+100\n",
      "Gradient Descent(48/49): loss=2.6238342357388343e+102\n",
      "Gradient Descent(49/49): loss=3.853170019053927e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.61342635964781\n",
      "Gradient Descent(2/49): loss=569.5246601094676\n",
      "Gradient Descent(3/49): loss=81959.0211940129\n",
      "Gradient Descent(4/49): loss=11973032.394994976\n",
      "Gradient Descent(5/49): loss=1756356294.6617548\n",
      "Gradient Descent(6/49): loss=257978555656.2656\n",
      "Gradient Descent(7/49): loss=37908601980912.9\n",
      "Gradient Descent(8/49): loss=5571251377671996.0\n",
      "Gradient Descent(9/49): loss=8.188193874181512e+17\n",
      "Gradient Descent(10/49): loss=1.2034563217368846e+20\n",
      "Gradient Descent(11/49): loss=1.768784227579833e+22\n",
      "Gradient Descent(12/49): loss=2.5996815959235557e+24\n",
      "Gradient Descent(13/49): loss=3.820900469338346e+26\n",
      "Gradient Descent(14/49): loss=5.615796717580496e+28\n",
      "Gradient Descent(15/49): loss=8.253859318032736e+30\n",
      "Gradient Descent(16/49): loss=1.2131171879689508e+33\n",
      "Gradient Descent(17/49): loss=1.7829881350745932e+35\n",
      "Gradient Descent(18/49): loss=2.6205602638701376e+37\n",
      "Gradient Descent(19/49): loss=3.8515882229376075e+39\n",
      "Gradient Descent(20/49): loss=5.660900856986856e+41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=8.320151755849131e+43\n",
      "Gradient Descent(22/49): loss=1.222860583351971e+46\n",
      "Gradient Descent(23/49): loss=1.7973085710628853e+48\n",
      "Gradient Descent(24/49): loss=2.6416078362539943e+50\n",
      "Gradient Descent(25/49): loss=3.8825230530342206e+52\n",
      "Gradient Descent(26/49): loss=5.7063675578449796e+54\n",
      "Gradient Descent(27/49): loss=8.386976782992321e+56\n",
      "Gradient Descent(28/49): loss=1.2326822421691386e+59\n",
      "Gradient Descent(29/49): loss=1.811744028242121e+61\n",
      "Gradient Descent(30/49): loss=2.662824458389988e+63\n",
      "Gradient Descent(31/49): loss=3.913706343538929e+65\n",
      "Gradient Descent(32/49): loss=5.752199434399811e+67\n",
      "Gradient Descent(33/49): loss=8.454338529443749e+69\n",
      "Gradient Descent(34/49): loss=1.2425827856904774e+72\n",
      "Gradient Descent(35/49): loss=1.8262954268001135e+74\n",
      "Gradient Descent(36/49): loss=2.6842114862371866e+76\n",
      "Gradient Descent(37/49): loss=3.945140089121105e+78\n",
      "Gradient Descent(38/49): loss=5.798399419193615e+80\n",
      "Gradient Descent(39/49): loss=8.522241305756736e+82\n",
      "Gradient Descent(40/49): loss=1.252562847483983e+85\n",
      "Gradient Descent(41/49): loss=1.84096369793871e+87\n",
      "Gradient Descent(42/49): loss=2.7057702884417433e+89\n",
      "Gradient Descent(43/49): loss=3.9768263013613634e+91\n",
      "Gradient Descent(44/49): loss=5.8449704687634205e+93\n",
      "Gradient Descent(45/49): loss=8.590689457324649e+95\n",
      "Gradient Descent(46/49): loss=1.2626230662171742e+98\n",
      "Gradient Descent(47/49): loss=1.855749780344326e+100\n",
      "Gradient Descent(48/49): loss=2.7275022446451104e+102\n",
      "Gradient Descent(49/49): loss=4.008767007998128e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.671858208363691\n",
      "Gradient Descent(2/49): loss=588.9214410735962\n",
      "Gradient Descent(3/49): loss=86460.33077959887\n",
      "Gradient Descent(4/49): loss=12876790.148840997\n",
      "Gradient Descent(5/49): loss=1925009629.3585868\n",
      "Gradient Descent(6/49): loss=288100813390.14575\n",
      "Gradient Descent(7/49): loss=43132736623640.93\n",
      "Gradient Descent(8/49): loss=6458286821930442.0\n",
      "Gradient Descent(9/49): loss=9.670365863149213e+17\n",
      "Gradient Descent(10/49): loss=1.4480160458531219e+20\n",
      "Gradient Descent(11/49): loss=2.16823032792433e+22\n",
      "Gradient Descent(12/49): loss=3.246668594639521e+24\n",
      "Gradient Descent(13/49): loss=4.8615042263142036e+26\n",
      "Gradient Descent(14/49): loss=7.279531462253876e+28\n",
      "Gradient Descent(15/49): loss=1.0900243605721238e+31\n",
      "Gradient Descent(16/49): loss=1.6321835101055395e+33\n",
      "Gradient Descent(17/49): loss=2.4440031963151013e+35\n",
      "Gradient Descent(18/49): loss=3.659607877598362e+37\n",
      "Gradient Descent(19/49): loss=5.479833186594045e+39\n",
      "Gradient Descent(20/49): loss=8.205406907307824e+41\n",
      "Gradient Descent(21/49): loss=1.2286633593533965e+44\n",
      "Gradient Descent(22/49): loss=1.8397791452533743e+46\n",
      "Gradient Descent(23/49): loss=2.7548532944820997e+48\n",
      "Gradient Descent(24/49): loss=4.125069410483229e+50\n",
      "Gradient Descent(25/49): loss=6.176807191655269e+52\n",
      "Gradient Descent(26/49): loss=9.249043661163676e+54\n",
      "Gradient Descent(27/49): loss=1.3849357117975856e+57\n",
      "Gradient Descent(28/49): loss=2.073778647911569e+59\n",
      "Gradient Descent(29/49): loss=3.1052400800265553e+61\n",
      "Gradient Descent(30/49): loss=4.649732489199861e+63\n",
      "Gradient Descent(31/49): loss=6.96242856073651e+65\n",
      "Gradient Descent(32/49): loss=1.0425419435624598e+68\n",
      "Gradient Descent(33/49): loss=1.5610841743014265e+70\n",
      "Gradient Descent(34/49): loss=2.3375402920739694e+72\n",
      "Gradient Descent(35/49): loss=3.500192178627639e+74\n",
      "Gradient Descent(36/49): loss=5.241126892600508e+76\n",
      "Gradient Descent(37/49): loss=7.847972254800763e+78\n",
      "Gradient Descent(38/49): loss=1.1751417161656032e+81\n",
      "Gradient Descent(39/49): loss=1.7596367676094533e+83\n",
      "Gradient Descent(40/49): loss=2.6348494920477504e+85\n",
      "Gradient Descent(41/49): loss=3.945377803838399e+87\n",
      "Gradient Descent(42/49): loss=5.907740105080191e+89\n",
      "Gradient Descent(43/49): loss=8.846147285367183e+91\n",
      "Gradient Descent(44/49): loss=1.324606709207071e+94\n",
      "Gradient Descent(45/49): loss=1.9834430486803287e+96\n",
      "Gradient Descent(46/49): loss=2.96997312486308e+98\n",
      "Gradient Descent(47/49): loss=4.4471861031138844e+100\n",
      "Gradient Descent(48/49): loss=6.659139124917437e+102\n",
      "Gradient Descent(49/49): loss=9.971279109268032e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.65296925417384\n",
      "Gradient Descent(2/49): loss=584.8721191676398\n",
      "Gradient Descent(3/49): loss=85858.04542754599\n",
      "Gradient Descent(4/49): loss=12802404.53177715\n",
      "Gradient Descent(5/49): loss=1917132285.1077895\n",
      "Gradient Descent(6/49): loss=287460023945.23444\n",
      "Gradient Descent(7/49): loss=43120318850547.62\n",
      "Gradient Descent(8/49): loss=6469106260972851.0\n",
      "Gradient Descent(9/49): loss=9.705667625452869e+17\n",
      "Gradient Descent(10/49): loss=1.4561720285096262e+20\n",
      "Gradient Descent(11/49): loss=2.184751075260929e+22\n",
      "Gradient Descent(12/49): loss=3.277871285529274e+24\n",
      "Gradient Descent(13/49): loss=4.9179266248839544e+26\n",
      "Gradient Descent(14/49): loss=7.378571059808495e+28\n",
      "Gradient Descent(15/49): loss=1.1070379434994002e+31\n",
      "Gradient Descent(16/49): loss=1.6609354571224218e+33\n",
      "Gradient Descent(17/49): loss=2.491971142086442e+35\n",
      "Gradient Descent(18/49): loss=3.7388088488971993e+37\n",
      "Gradient Descent(19/49): loss=5.609491771800539e+39\n",
      "Gradient Descent(20/49): loss=8.416155844246156e+41\n",
      "Gradient Descent(21/49): loss=1.2627111702941443e+44\n",
      "Gradient Descent(22/49): loss=1.894498544379622e+46\n",
      "Gradient Descent(23/49): loss=2.842395647648779e+48\n",
      "Gradient Descent(24/49): loss=4.264565439630294e+50\n",
      "Gradient Descent(25/49): loss=6.398306444054142e+52\n",
      "Gradient Descent(26/49): loss=9.599647591662649e+54\n",
      "Gradient Descent(27/49): loss=1.4402754024036943e+57\n",
      "Gradient Descent(28/49): loss=2.160905611338056e+59\n",
      "Gradient Descent(29/49): loss=3.2420973470208136e+61\n",
      "Gradient Descent(30/49): loss=4.864254668231804e+63\n",
      "Gradient Descent(31/49): loss=7.298045353004951e+65\n",
      "Gradient Descent(32/49): loss=1.0949563624282513e+68\n",
      "Gradient Descent(33/49): loss=1.6428089681964575e+70\n",
      "Gradient Descent(34/49): loss=2.4647752171617293e+72\n",
      "Gradient Descent(35/49): loss=3.698005665140828e+74\n",
      "Gradient Descent(36/49): loss=5.548273045020781e+76\n",
      "Gradient Descent(37/49): loss=8.32430682091228e+78\n",
      "Gradient Descent(38/49): loss=1.2489306760213211e+81\n",
      "Gradient Descent(39/49): loss=1.8738230906968495e+83\n",
      "Gradient Descent(40/49): loss=2.8113753970830954e+85\n",
      "Gradient Descent(41/49): loss=4.218024456292097e+87\n",
      "Gradient Descent(42/49): loss=6.328479054180337e+89\n",
      "Gradient Descent(43/49): loss=9.494882629107661e+91\n",
      "Gradient Descent(44/49): loss=1.4245570755421752e+94\n",
      "Gradient Descent(45/49): loss=2.137322746103289e+96\n",
      "Gradient Descent(46/49): loss=3.2067149849169114e+98\n",
      "Gradient Descent(47/49): loss=4.811169025940683e+100\n",
      "Gradient Descent(48/49): loss=7.218398736727985e+102\n",
      "Gradient Descent(49/49): loss=1.0830066464399175e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.670590721633358\n",
      "Gradient Descent(2/49): loss=582.7656047767713\n",
      "Gradient Descent(3/49): loss=84803.90875461207\n",
      "Gradient Descent(4/49): loss=12528566.932571664\n",
      "Gradient Descent(5/49): loss=1858437937.6825418\n",
      "Gradient Descent(6/49): loss=276012911309.3019\n",
      "Gradient Descent(7/49): loss=41009112818889.59\n",
      "Gradient Descent(8/49): loss=6093773759976801.0\n",
      "Gradient Descent(9/49): loss=9.055454879117321e+17\n",
      "Gradient Descent(10/49): loss=1.3456748317958467e+20\n",
      "Gradient Descent(11/49): loss=1.999732646735453e+22\n",
      "Gradient Descent(12/49): loss=2.971696041865566e+24\n",
      "Gradient Descent(13/49): loss=4.41608116251437e+26\n",
      "Gradient Descent(14/49): loss=6.562506964245264e+28\n",
      "Gradient Descent(15/49): loss=9.752198466373992e+30\n",
      "Gradient Descent(16/49): loss=1.449223248219092e+33\n",
      "Gradient Descent(17/49): loss=2.153614943896234e+35\n",
      "Gradient Descent(18/49): loss=3.200374642833379e+37\n",
      "Gradient Descent(19/49): loss=4.755909541746793e+39\n",
      "Gradient Descent(20/49): loss=7.067508682027291e+41\n",
      "Gradient Descent(21/49): loss=1.0502655388100567e+44\n",
      "Gradient Descent(22/49): loss=1.560744742828439e+46\n",
      "Gradient Descent(23/49): loss=2.319341216360022e+48\n",
      "Gradient Descent(24/49): loss=3.446651800454526e+50\n",
      "Gradient Descent(25/49): loss=5.121889159642341e+52\n",
      "Gradient Descent(26/49): loss=7.611371871161739e+54\n",
      "Gradient Descent(27/49): loss=1.1310862058007345e+57\n",
      "Gradient Descent(28/49): loss=1.6808481133342284e+59\n",
      "Gradient Descent(29/49): loss=2.4978205601041276e+61\n",
      "Gradient Descent(30/49): loss=3.711880627990024e+63\n",
      "Gradient Descent(31/49): loss=5.516031862542346e+65\n",
      "Gradient Descent(32/49): loss=8.197086748734752e+67\n",
      "Gradient Descent(33/49): loss=1.2181262335079083e+70\n",
      "Gradient Descent(34/49): loss=1.8101937508337176e+72\n",
      "Gradient Descent(35/49): loss=2.690034353928204e+74\n",
      "Gradient Descent(36/49): loss=3.997519504186301e+76\n",
      "Gradient Descent(37/49): loss=5.940504872368785e+78\n",
      "Gradient Descent(38/49): loss=8.827873910729169e+80\n",
      "Gradient Descent(39/49): loss=1.311864217908763e+83\n",
      "Gradient Descent(40/49): loss=1.9494928718201627e+85\n",
      "Gradient Descent(41/49): loss=2.897039499511618e+87\n",
      "Gradient Descent(42/49): loss=4.3051390354120863e+89\n",
      "Gradient Descent(43/49): loss=6.397642185187076e+91\n",
      "Gradient Descent(44/49): loss=9.507201786751948e+93\n",
      "Gradient Descent(45/49): loss=1.412815584205368e+96\n",
      "Gradient Descent(46/49): loss=2.099511422756374e+98\n",
      "Gradient Descent(47/49): loss=3.1199742298735502e+100\n",
      "Gradient Descent(48/49): loss=4.63643069028665e+102\n",
      "Gradient Descent(49/49): loss=6.889957404136393e+104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.664476336263545\n",
      "Gradient Descent(2/49): loss=583.1388038476796\n",
      "Gradient Descent(3/49): loss=84912.82909341171\n",
      "Gradient Descent(4/49): loss=12551585.418822346\n",
      "Gradient Descent(5/49): loss=1863064036.5698762\n",
      "Gradient Descent(6/49): loss=276898564082.5083\n",
      "Gradient Descent(7/49): loss=41171557320591.26\n",
      "Gradient Descent(8/49): loss=6122586223031848.0\n",
      "Gradient Descent(9/49): loss=9.105273205905695e+17\n",
      "Gradient Descent(10/49): loss=1.354122446259457e+20\n",
      "Gradient Descent(11/49): loss=2.0138411625932602e+22\n",
      "Gradient Descent(12/49): loss=2.9949754916884254e+24\n",
      "Gradient Descent(13/49): loss=4.4541167493558264e+26\n",
      "Gradient Descent(14/49): loss=6.624147725300718e+28\n",
      "Gradient Descent(15/49): loss=9.851411311610353e+30\n",
      "Gradient Descent(16/49): loss=1.465098773844321e+33\n",
      "Gradient Descent(17/49): loss=2.1788902789404387e+35\n",
      "Gradient Descent(18/49): loss=3.240438764315661e+37\n",
      "Gradient Descent(19/49): loss=4.8191703346504106e+39\n",
      "Gradient Descent(20/49): loss=7.167054960945419e+41\n",
      "Gradient Descent(21/49): loss=1.0658821591996074e+44\n",
      "Gradient Descent(22/49): loss=1.5851765941940947e+46\n",
      "Gradient Descent(23/49): loss=2.3574696443901947e+48\n",
      "Gradient Descent(24/49): loss=3.5060214392508634e+50\n",
      "Gradient Descent(25/49): loss=5.214144055578107e+52\n",
      "Gradient Descent(26/49): loss=7.754458637349869e+54\n",
      "Gradient Descent(27/49): loss=1.1532406492308233e+57\n",
      "Gradient Descent(28/49): loss=1.715095865793307e+59\n",
      "Gradient Descent(29/49): loss=2.5506851764402253e+61\n",
      "Gradient Descent(30/49): loss=3.793370970725648e+63\n",
      "Gradient Descent(31/49): loss=5.641489374877147e+65\n",
      "Gradient Descent(32/49): loss=8.390005251915378e+67\n",
      "Gradient Descent(33/49): loss=1.2477589418255487e+70\n",
      "Gradient Descent(34/49): loss=1.855663173214563e+72\n",
      "Gradient Descent(35/49): loss=2.7597364338553367e+74\n",
      "Gradient Descent(36/49): loss=4.1042713431420465e+76\n",
      "Gradient Descent(37/49): loss=6.103859430737208e+78\n",
      "Gradient Descent(38/49): loss=9.077640544515584e+80\n",
      "Gradient Descent(39/49): loss=1.3500238462319925e+83\n",
      "Gradient Descent(40/49): loss=2.007751217353678e+85\n",
      "Gradient Descent(41/49): loss=2.985921294676506e+87\n",
      "Gradient Descent(42/49): loss=4.4406527566468306e+89\n",
      "Gradient Descent(43/49): loss=6.604124810748347e+91\n",
      "Gradient Descent(44/49): loss=9.821633643986134e+93\n",
      "Gradient Descent(45/49): loss=1.4606702659477582e+96\n",
      "Gradient Descent(46/49): loss=2.172304224695153e+98\n",
      "Gradient Descent(47/49): loss=3.2306440095612563e+100\n",
      "Gradient Descent(48/49): loss=4.80460360840053e+102\n",
      "Gradient Descent(49/49): loss=7.145391372598335e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.723621137875909\n",
      "Gradient Descent(2/49): loss=602.988494616469\n",
      "Gradient Descent(3/49): loss=89573.98034048078\n",
      "Gradient Descent(4/49): loss=13498551.328460021\n",
      "Gradient Descent(5/49): loss=2041878014.6250446\n",
      "Gradient Descent(6/49): loss=309214480438.9699\n",
      "Gradient Descent(7/49): loss=46842626480841.87\n",
      "Gradient Descent(8/49): loss=7096931372147121.0\n",
      "Gradient Descent(9/49): loss=1.0752644746218966e+18\n",
      "Gradient Descent(10/49): loss=1.629164378126537e+20\n",
      "Gradient Descent(11/49): loss=2.4684031589287234e+22\n",
      "Gradient Descent(12/49): loss=3.7399671661232374e+24\n",
      "Gradient Descent(13/49): loss=5.66656205097227e+26\n",
      "Gradient Descent(14/49): loss=8.585617984960915e+28\n",
      "Gradient Descent(15/49): loss=1.3008388224038636e+31\n",
      "Gradient Descent(16/49): loss=1.970949168763737e+33\n",
      "Gradient Descent(17/49): loss=2.9862582316956288e+35\n",
      "Gradient Descent(18/49): loss=4.524590679084798e+37\n",
      "Gradient Descent(19/49): loss=6.855375266787104e+39\n",
      "Gradient Descent(20/49): loss=1.038683350342173e+42\n",
      "Gradient Descent(21/49): loss=1.5737476948220872e+44\n",
      "Gradient Descent(22/49): loss=2.384443542082734e+46\n",
      "Gradient Descent(23/49): loss=3.6127589092661054e+48\n",
      "Gradient Descent(24/49): loss=5.473825111036154e+50\n",
      "Gradient Descent(25/49): loss=8.293595586844879e+52\n",
      "Gradient Descent(26/49): loss=1.2565934490574707e+55\n",
      "Gradient Descent(27/49): loss=1.9039113731550265e+57\n",
      "Gradient Descent(28/49): loss=2.8846867851714245e+59\n",
      "Gradient Descent(29/49): loss=4.370696013414244e+61\n",
      "Gradient Descent(30/49): loss=6.622203748383745e+63\n",
      "Gradient Descent(31/49): loss=1.0033546682385445e+66\n",
      "Gradient Descent(32/49): loss=1.5202198973744695e+68\n",
      "Gradient Descent(33/49): loss=2.3033415895004212e+70\n",
      "Gradient Descent(34/49): loss=3.4898783308159202e+72\n",
      "Gradient Descent(35/49): loss=5.287644185915137e+74\n",
      "Gradient Descent(36/49): loss=8.011505957087489e+76\n",
      "Gradient Descent(37/49): loss=1.2138530022768461e+79\n",
      "Gradient Descent(38/49): loss=1.8391537359253005e+81\n",
      "Gradient Descent(39/49): loss=2.786570085523861e+83\n",
      "Gradient Descent(40/49): loss=4.2220357601752e+85\n",
      "Gradient Descent(41/49): loss=6.396963081173325e+87\n",
      "Gradient Descent(42/49): loss=9.692276187683578e+89\n",
      "Gradient Descent(43/49): loss=1.4685127380961466e+92\n",
      "Gradient Descent(44/49): loss=2.2249981533657062e+94\n",
      "Gradient Descent(45/49): loss=3.371177283010176e+96\n",
      "Gradient Descent(46/49): loss=5.107795822793203e+98\n",
      "Gradient Descent(47/49): loss=7.739011027046281e+100\n",
      "Gradient Descent(48/49): loss=1.172566284060888e+103\n",
      "Gradient Descent(49/49): loss=1.7765986967990272e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.70446931808547\n",
      "Gradient Descent(2/49): loss=598.8433318260184\n",
      "Gradient Descent(3/49): loss=88949.9587604227\n",
      "Gradient Descent(4/49): loss=13420540.840270596\n",
      "Gradient Descent(5/49): loss=2033514472.277957\n",
      "Gradient Descent(6/49): loss=308525170041.81604\n",
      "Gradient Descent(7/49): loss=46828857320083.35\n",
      "Gradient Descent(8/49): loss=7108771132563481.0\n",
      "Gradient Descent(9/49): loss=1.079181242835093e+18\n",
      "Gradient Descent(10/49): loss=1.6383263380158782e+20\n",
      "Gradient Descent(11/49): loss=2.4871870975898206e+22\n",
      "Gradient Descent(12/49): loss=3.77587109846381e+24\n",
      "Gradient Descent(13/49): loss=5.732262630667018e+26\n",
      "Gradient Descent(14/49): loss=8.702320437149475e+28\n",
      "Gradient Descent(15/49): loss=1.3211255296056673e+31\n",
      "Gradient Descent(16/49): loss=2.005640573833877e+33\n",
      "Gradient Descent(17/49): loss=3.0448235571302776e+35\n",
      "Gradient Descent(18/49): loss=4.622438652475911e+37\n",
      "Gradient Descent(19/49): loss=7.017463806402819e+39\n",
      "Gradient Descent(20/49): loss=1.0653423871340362e+42\n",
      "Gradient Descent(21/49): loss=1.6173284725227792e+44\n",
      "Gradient Descent(22/49): loss=2.4553152298072177e+46\n",
      "Gradient Descent(23/49): loss=3.7274882500289775e+48\n",
      "Gradient Descent(24/49): loss=5.6588125571262e+50\n",
      "Gradient Descent(25/49): loss=8.59081435238987e+52\n",
      "Gradient Descent(26/49): loss=1.3041974882929533e+55\n",
      "Gradient Descent(27/49): loss=1.9799416198495846e+57\n",
      "Gradient Descent(28/49): loss=3.0058092069658607e+59\n",
      "Gradient Descent(29/49): loss=4.563209792704483e+61\n",
      "Gradient Descent(30/49): loss=6.927546686588676e+63\n",
      "Gradient Descent(31/49): loss=1.0516917975498786e+66\n",
      "Gradient Descent(32/49): loss=1.596605100005978e+68\n",
      "Gradient Descent(33/49): loss=2.4238544517546203e+70\n",
      "Gradient Descent(34/49): loss=3.6797266921349104e+72\n",
      "Gradient Descent(35/49): loss=5.586304292738459e+74\n",
      "Gradient Descent(36/49): loss=8.480737365024916e+76\n",
      "Gradient Descent(37/49): loss=1.2874863681883843e+79\n",
      "Gradient Descent(38/49): loss=1.954571963408556e+81\n",
      "Gradient Descent(39/49): loss=2.9672947648512715e+83\n",
      "Gradient Descent(40/49): loss=4.5047398542231785e+85\n",
      "Gradient Descent(41/49): loss=6.838781706017618e+87\n",
      "Gradient Descent(42/49): loss=1.0382161176014507e+90\n",
      "Gradient Descent(43/49): loss=1.576147263040964e+92\n",
      "Gradient Descent(44/49): loss=2.3927967912218144e+94\n",
      "Gradient Descent(45/49): loss=3.632577119116963e+96\n",
      "Gradient Descent(46/49): loss=5.514725101078937e+98\n",
      "Gradient Descent(47/49): loss=8.372070831042108e+100\n",
      "Gradient Descent(48/49): loss=1.2709893732739864e+103\n",
      "Gradient Descent(49/49): loss=1.929527376889523e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.722354354193842\n",
      "Gradient Descent(2/49): loss=596.690923480177\n",
      "Gradient Descent(3/49): loss=87858.91478181706\n",
      "Gradient Descent(4/49): loss=13133700.368212407\n",
      "Gradient Descent(5/49): loss=1971298660.319077\n",
      "Gradient Descent(6/49): loss=296246758761.7055\n",
      "Gradient Descent(7/49): loss=44537404198942.11\n",
      "Gradient Descent(8/49): loss=6696553726589826.0\n",
      "Gradient Descent(9/49): loss=1.0069221125398015e+18\n",
      "Gradient Descent(10/49): loss=1.514071347864568e+20\n",
      "Gradient Descent(11/49): loss=2.2766630655262747e+22\n",
      "Gradient Descent(12/49): loss=3.4233541444213824e+24\n",
      "Gradient Descent(13/49): loss=5.147603757443222e+26\n",
      "Gradient Descent(14/49): loss=7.740311874994759e+28\n",
      "Gradient Descent(15/49): loss=1.163889723237616e+31\n",
      "Gradient Descent(16/49): loss=1.7501094705814203e+33\n",
      "Gradient Descent(17/49): loss=2.6315922511488334e+35\n",
      "Gradient Descent(18/49): loss=3.957054062066438e+37\n",
      "Gradient Descent(19/49): loss=5.950115126392037e+39\n",
      "Gradient Descent(20/49): loss=8.947027124074626e+41\n",
      "Gradient Descent(21/49): loss=1.345340260886754e+44\n",
      "Gradient Descent(22/49): loss=2.022951749787344e+46\n",
      "Gradient Descent(23/49): loss=3.0418578117190454e+48\n",
      "Gradient Descent(24/49): loss=4.57395928880254e+50\n",
      "Gradient Descent(25/49): loss=6.877738826265915e+52\n",
      "Gradient Descent(26/49): loss=1.0341869784049335e+55\n",
      "Gradient Descent(27/49): loss=1.5550789777271804e+57\n",
      "Gradient Descent(28/49): loss=2.338330183482757e+59\n",
      "Gradient Descent(29/49): loss=3.516083829374367e+61\n",
      "Gradient Descent(30/49): loss=5.287040120559224e+63\n",
      "Gradient Descent(31/49): loss=7.949979179357817e+65\n",
      "Gradient Descent(32/49): loss=1.1954168591695431e+68\n",
      "Gradient Descent(33/49): loss=1.7975159870823818e+70\n",
      "Gradient Descent(34/49): loss=2.702876154901632e+72\n",
      "Gradient Descent(35/49): loss=4.064241743181235e+74\n",
      "Gradient Descent(36/49): loss=6.111290344199258e+76\n",
      "Gradient Descent(37/49): loss=9.189381939143663e+78\n",
      "Gradient Descent(38/49): loss=1.381782498742067e+81\n",
      "Gradient Descent(39/49): loss=2.0777489568659725e+83\n",
      "Gradient Descent(40/49): loss=3.124254889382175e+85\n",
      "Gradient Descent(41/49): loss=4.697857545096135e+87\n",
      "Gradient Descent(42/49): loss=7.064041281977818e+89\n",
      "Gradient Descent(43/49): loss=1.0622007745972506e+92\n",
      "Gradient Descent(44/49): loss=1.5972025651003828e+94\n",
      "Gradient Descent(45/49): loss=2.4016702820900646e+96\n",
      "Gradient Descent(46/49): loss=3.6113266218753155e+98\n",
      "Gradient Descent(47/49): loss=5.430254130686025e+100\n",
      "Gradient Descent(48/49): loss=8.165326211485136e+102\n",
      "Gradient Descent(49/49): loss=1.2277980097322458e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.7158368806721676\n",
      "Gradient Descent(2/49): loss=596.9957965996452\n",
      "Gradient Descent(3/49): loss=87954.82568262145\n",
      "Gradient Descent(4/49): loss=13154454.388585383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=1975571499.0917172\n",
      "Gradient Descent(6/49): loss=297082849179.47943\n",
      "Gradient Descent(7/49): loss=44693726555028.21\n",
      "Gradient Descent(8/49): loss=6724759813932541.0\n",
      "Gradient Descent(9/49): loss=1.0118765290182193e+18\n",
      "Gradient Descent(10/49): loss=1.5225977452763162e+20\n",
      "Gradient Descent(11/49): loss=2.2911059591843702e+22\n",
      "Gradient Descent(12/49): loss=3.447513336924349e+24\n",
      "Gradient Descent(13/49): loss=5.187606202857722e+26\n",
      "Gradient Descent(14/49): loss=7.805992624444772e+28\n",
      "Gradient Descent(15/49): loss=1.1745981234082768e+31\n",
      "Gradient Descent(16/49): loss=1.7674636019257764e+33\n",
      "Gradient Descent(17/49): loss=2.659571428531574e+35\n",
      "Gradient Descent(18/49): loss=4.0019608860917317e+37\n",
      "Gradient Descent(19/49): loss=6.021906678855261e+39\n",
      "Gradient Descent(20/49): loss=9.061397922704499e+41\n",
      "Gradient Descent(21/49): loss=1.3635038983652529e+44\n",
      "Gradient Descent(22/49): loss=2.0517175128840112e+46\n",
      "Gradient Descent(23/49): loss=3.0872993892952575e+48\n",
      "Gradient Descent(24/49): loss=4.64557984192623e+50\n",
      "Gradient Descent(25/49): loss=6.990385235254716e+52\n",
      "Gradient Descent(26/49): loss=1.0518705393087832e+55\n",
      "Gradient Descent(27/49): loss=1.5827906391851422e+57\n",
      "Gradient Descent(28/49): loss=2.381686827295778e+59\n",
      "Gradient Descent(29/49): loss=3.5838170904489457e+61\n",
      "Gradient Descent(30/49): loss=5.3927093984800276e+63\n",
      "Gradient Descent(31/49): loss=8.114620228235992e+65\n",
      "Gradient Descent(32/49): loss=1.2210385649012794e+68\n",
      "Gradient Descent(33/49): loss=1.8373443673781036e+70\n",
      "Gradient Descent(34/49): loss=2.764723753511405e+72\n",
      "Gradient Descent(35/49): loss=4.1601876974961026e+74\n",
      "Gradient Descent(36/49): loss=6.259996737980273e+76\n",
      "Gradient Descent(37/49): loss=9.419661325163148e+78\n",
      "Gradient Descent(38/49): loss=1.4174131903685674e+81\n",
      "Gradient Descent(39/49): loss=2.1328369278669548e+83\n",
      "Gradient Descent(40/49): loss=3.2093629379094864e+85\n",
      "Gradient Descent(41/49): loss=4.829253625840024e+87\n",
      "Gradient Descent(42/49): loss=7.266766343940048e+89\n",
      "Gradient Descent(43/49): loss=1.0934586830327041e+92\n",
      "Gradient Descent(44/49): loss=1.6453699416064293e+94\n",
      "Gradient Descent(45/49): loss=2.47585234517816e+96\n",
      "Gradient Descent(46/49): loss=3.725511619070556e+98\n",
      "Gradient Descent(47/49): loss=5.605922683903393e+100\n",
      "Gradient Descent(48/49): loss=8.435450577320485e+102\n",
      "Gradient Descent(49/49): loss=1.269315159246359e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.77569891401989\n",
      "Gradient Descent(2/49): loss=617.3062794294202\n",
      "Gradient Descent(3/49): loss=92780.51987639944\n",
      "Gradient Descent(4/49): loss=14146425.105935743\n",
      "Gradient Descent(5/49): loss=2165093724.8427253\n",
      "Gradient Descent(6/49): loss=331737980780.7585\n",
      "Gradient Descent(7/49): loss=50847019996436.266\n",
      "Gradient Descent(8/49): loss=7794422297193322.0\n",
      "Gradient Descent(9/49): loss=1.1948619895207636e+18\n",
      "Gradient Descent(10/49): loss=1.8317090835912422e+20\n",
      "Gradient Descent(11/49): loss=2.8079983020563946e+22\n",
      "Gradient Descent(12/49): loss=4.3046484916884484e+24\n",
      "Gradient Descent(13/49): loss=6.599008853763196e+26\n",
      "Gradient Descent(14/49): loss=1.0116255310258497e+29\n",
      "Gradient Descent(15/49): loss=1.550818127212495e+31\n",
      "Gradient Descent(16/49): loss=2.3773983755180568e+33\n",
      "Gradient Descent(17/49): loss=3.644542812430692e+35\n",
      "Gradient Descent(18/49): loss=5.58707049934414e+37\n",
      "Gradient Descent(19/49): loss=8.564958181131179e+39\n",
      "Gradient Descent(20/49): loss=1.3130048862476641e+42\n",
      "Gradient Descent(21/49): loss=2.0128315806417084e+44\n",
      "Gradient Descent(22/49): loss=3.085663286192409e+46\n",
      "Gradient Descent(23/49): loss=4.730310278996807e+48\n",
      "Gradient Descent(24/49): loss=7.2515479688733425e+50\n",
      "Gradient Descent(25/49): loss=1.1116595919379655e+53\n",
      "Gradient Descent(26/49): loss=1.704169997430048e+55\n",
      "Gradient Descent(27/49): loss=2.612486233378289e+57\n",
      "Gradient Descent(28/49): loss=4.0049316264713333e+59\n",
      "Gradient Descent(29/49): loss=6.139545207083953e+61\n",
      "Gradient Descent(30/49): loss=9.411899843853015e+63\n",
      "Gradient Descent(31/49): loss=1.4428407265168522e+66\n",
      "Gradient Descent(32/49): loss=2.21186943829975e+68\n",
      "Gradient Descent(33/49): loss=3.390787577707949e+70\n",
      "Gradient Descent(34/49): loss=5.1980646768990865e+72\n",
      "Gradient Descent(35/49): loss=7.968613711711942e+74\n",
      "Gradient Descent(36/49): loss=1.2215855021712438e+77\n",
      "Gradient Descent(37/49): loss=1.872686006753816e+79\n",
      "Gradient Descent(38/49): loss=2.87082064551218e+81\n",
      "Gradient Descent(39/49): loss=4.400957314240455e+83\n",
      "Gradient Descent(40/49): loss=6.746651105510314e+85\n",
      "Gradient Descent(41/49): loss=1.034259091589018e+88\n",
      "Gradient Descent(42/49): loss=1.5855153198315974e+90\n",
      "Gradient Descent(43/49): loss=2.430589056324794e+92\n",
      "Gradient Descent(44/49): loss=3.72608393424628e+94\n",
      "Gradient Descent(45/49): loss=5.712072737643809e+96\n",
      "Gradient Descent(46/49): loss=8.756586146719111e+98\n",
      "Gradient Descent(47/49): loss=1.3423813817984043e+101\n",
      "Gradient Descent(48/49): loss=2.0578656385104502e+103\n",
      "Gradient Descent(49/49): loss=3.154700328522571e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.756282493332432\n",
      "Gradient Descent(2/49): loss=613.0635854799276\n",
      "Gradient Descent(3/49): loss=92134.11249224909\n",
      "Gradient Descent(4/49): loss=14064636.156054966\n",
      "Gradient Descent(5/49): loss=2156217151.042328\n",
      "Gradient Descent(6/49): loss=330996806179.25543\n",
      "Gradient Descent(7/49): loss=50831769832242.58\n",
      "Gradient Descent(8/49): loss=7807371854062975.0\n",
      "Gradient Descent(9/49): loss=1.1992050753690163e+18\n",
      "Gradient Descent(10/49): loss=1.8419941580780723e+20\n",
      "Gradient Descent(11/49): loss=2.8293395338462197e+22\n",
      "Gradient Descent(12/49): loss=4.345928250378055e+24\n",
      "Gradient Descent(13/49): loss=6.675445489261657e+26\n",
      "Gradient Descent(14/49): loss=1.0253639093015061e+29\n",
      "Gradient Descent(15/49): loss=1.5749828304614337e+31\n",
      "Gradient Descent(16/49): loss=2.4192103286799642e+33\n",
      "Gradient Descent(17/49): loss=3.71596346008344e+35\n",
      "Gradient Descent(18/49): loss=5.707806509901355e+37\n",
      "Gradient Descent(19/49): loss=8.76732387836477e+39\n",
      "Gradient Descent(20/49): loss=1.3466813892598332e+42\n",
      "Gradient Descent(21/49): loss=2.0685340127278734e+44\n",
      "Gradient Descent(22/49): loss=3.1773164729464553e+46\n",
      "Gradient Descent(23/49): loss=4.8804321839582184e+48\n",
      "Gradient Descent(24/49): loss=7.496457625507866e+50\n",
      "Gradient Descent(25/49): loss=1.1514733698338253e+53\n",
      "Gradient Descent(26/49): loss=1.7686899435344633e+55\n",
      "Gradient Descent(27/49): loss=2.716748991608591e+57\n",
      "Gradient Descent(28/49): loss=4.172989794162146e+59\n",
      "Gradient Descent(29/49): loss=6.409809620236838e+61\n",
      "Gradient Descent(30/49): loss=9.84561702622862e+63\n",
      "Gradient Descent(31/49): loss=1.5123097310272465e+66\n",
      "Gradient Descent(32/49): loss=2.322943007499643e+68\n",
      "Gradient Descent(33/49): loss=3.568094620687351e+70\n",
      "Gradient Descent(34/49): loss=5.480676530192467e+72\n",
      "Gradient Descent(35/49): loss=8.418446936481858e+74\n",
      "Gradient Descent(36/49): loss=1.2930930776874643e+77\n",
      "Gradient Descent(37/49): loss=1.986221116768152e+79\n",
      "Gradient Descent(38/49): loss=3.050881945598976e+81\n",
      "Gradient Descent(39/49): loss=4.6862258020531984e+83\n",
      "Gradient Descent(40/49): loss=7.198152094842104e+85\n",
      "Gradient Descent(41/49): loss=1.1056529448021529e+88\n",
      "Gradient Descent(42/49): loss=1.6983087023482588e+90\n",
      "Gradient Descent(43/49): loss=2.608641764154983e+92\n",
      "Gradient Descent(44/49): loss=4.006934572191897e+94\n",
      "Gradient Descent(45/49): loss=6.154744927587864e+96\n",
      "Gradient Descent(46/49): loss=9.453831711294095e+98\n",
      "Gradient Descent(47/49): loss=1.452130593176301e+101\n",
      "Gradient Descent(48/49): loss=2.2305064486386018e+103\n",
      "Gradient Descent(49/49): loss=3.426109911048725e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.774432916312234\n",
      "Gradient Descent(2/49): loss=610.8645438658704\n",
      "Gradient Descent(3/49): loss=91005.0913639939\n",
      "Gradient Descent(4/49): loss=13764255.874371659\n",
      "Gradient Descent(5/49): loss=2090290879.4221678\n",
      "Gradient Descent(6/49): loss=317832092744.65826\n",
      "Gradient Descent(7/49): loss=48345866736433.91\n",
      "Gradient Descent(8/49): loss=7354892016875924.0\n",
      "Gradient Descent(9/49): loss=1.1189518205822871e+18\n",
      "Gradient Descent(10/49): loss=1.70236406361304e+20\n",
      "Gradient Descent(11/49): loss=2.5899743745885447e+22\n",
      "Gradient Descent(12/49): loss=3.9403893920538234e+24\n",
      "Gradient Descent(13/49): loss=5.9949150016857276e+26\n",
      "Gradient Descent(14/49): loss=9.120675183560345e+28\n",
      "Gradient Descent(15/49): loss=1.3876213468224936e+31\n",
      "Gradient Descent(16/49): loss=2.1111299302752133e+33\n",
      "Gradient Descent(17/49): loss=3.2118773748435447e+35\n",
      "Gradient Descent(18/49): loss=4.886556787967191e+37\n",
      "Gradient Descent(19/49): loss=7.434417466983494e+39\n",
      "Gradient Descent(20/49): loss=1.1310737904694133e+42\n",
      "Gradient Descent(21/49): loss=1.7208179729722167e+44\n",
      "Gradient Descent(22/49): loss=2.6180559757677076e+46\n",
      "Gradient Descent(23/49): loss=3.9831157042523595e+48\n",
      "Gradient Descent(24/49): loss=6.05992036088893e+50\n",
      "Gradient Descent(25/49): loss=9.219575203688391e+52\n",
      "Gradient Descent(26/49): loss=1.4026680529514603e+55\n",
      "Gradient Descent(27/49): loss=2.1340220382211264e+57\n",
      "Gradient Descent(28/49): loss=3.246705483903404e+59\n",
      "Gradient Descent(29/49): loss=4.9395443488462527e+61\n",
      "Gradient Descent(30/49): loss=7.515032852590242e+63\n",
      "Gradient Descent(31/49): loss=1.143338631805213e+66\n",
      "Gradient Descent(32/49): loss=1.7394777276690844e+68\n",
      "Gradient Descent(33/49): loss=2.6464449646728108e+70\n",
      "Gradient Descent(34/49): loss=4.026306769921739e+72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=6.12563133634698e+74\n",
      "Gradient Descent(36/49): loss=9.319548015852202e+76\n",
      "Gradient Descent(37/49): loss=1.4178779369959525e+79\n",
      "Gradient Descent(38/49): loss=2.1571623868457343e+81\n",
      "Gradient Descent(39/49): loss=3.2819112575240354e+83\n",
      "Gradient Descent(40/49): loss=4.993106484678011e+85\n",
      "Gradient Descent(41/49): loss=7.5965223953503345e+87\n",
      "Gradient Descent(42/49): loss=1.1557364674705256e+90\n",
      "Gradient Descent(43/49): loss=1.758339820150873e+92\n",
      "Gradient Descent(44/49): loss=2.6751417906669624e+94\n",
      "Gradient Descent(45/49): loss=4.0699661795515665e+96\n",
      "Gradient Descent(46/49): loss=6.192054851254726e+98\n",
      "Gradient Descent(47/49): loss=9.420604886984985e+100\n",
      "Gradient Descent(48/49): loss=1.433252749993034e+103\n",
      "Gradient Descent(49/49): loss=2.1805536587152538e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.767507992873674\n",
      "Gradient Descent(2/49): loss=611.0985101050126\n",
      "Gradient Descent(3/49): loss=91087.10495026351\n",
      "Gradient Descent(4/49): loss=13782510.085339587\n",
      "Gradient Descent(5/49): loss=2094157208.8149312\n",
      "Gradient Descent(6/49): loss=318607751906.18964\n",
      "Gradient Descent(7/49): loss=48494003411711.945\n",
      "Gradient Descent(8/49): loss=7382120980058783.0\n",
      "Gradient Descent(9/49): loss=1.1238151726824869e+18\n",
      "Gradient Descent(10/49): loss=1.710864322695575e+20\n",
      "Gradient Descent(11/49): loss=2.6045853469818936e+22\n",
      "Gradient Descent(12/49): loss=3.9651754435999156e+24\n",
      "Gradient Descent(13/49): loss=6.036517868009189e+26\n",
      "Gradient Descent(14/49): loss=9.189897414776264e+28\n",
      "Gradient Descent(15/49): loss=1.3990552547665648e+31\n",
      "Gradient Descent(16/49): loss=2.1298993493620582e+33\n",
      "Gradient Descent(17/49): loss=3.2425247390505713e+35\n",
      "Gradient Descent(18/49): loss=4.936367868777003e+37\n",
      "Gradient Descent(19/49): loss=7.515047600039193e+39\n",
      "Gradient Descent(20/49): loss=1.1440788439937375e+42\n",
      "Gradient Descent(21/49): loss=1.7417273596570688e+44\n",
      "Gradient Descent(22/49): loss=2.6515779147666393e+46\n",
      "Gradient Descent(23/49): loss=4.036719868456765e+48\n",
      "Gradient Descent(24/49): loss=6.145437856346628e+50\n",
      "Gradient Descent(25/49): loss=9.355716442297501e+52\n",
      "Gradient Descent(26/49): loss=1.4242993289456445e+55\n",
      "Gradient Descent(27/49): loss=2.168330550575295e+57\n",
      "Gradient Descent(28/49): loss=3.3010317992910104e+59\n",
      "Gradient Descent(29/49): loss=5.0254380897042775e+61\n",
      "Gradient Descent(30/49): loss=7.650646685340994e+63\n",
      "Gradient Descent(31/49): loss=1.1647222323529474e+66\n",
      "Gradient Descent(32/49): loss=1.7731545244881168e+68\n",
      "Gradient Descent(33/49): loss=2.6994221286229418e+70\n",
      "Gradient Descent(34/49): loss=4.10955713552538e+72\n",
      "Gradient Descent(35/49): loss=6.256324148443883e+74\n",
      "Gradient Descent(36/49): loss=9.524527962402512e+76\n",
      "Gradient Descent(37/49): loss=1.449998925154011e+79\n",
      "Gradient Descent(38/49): loss=2.207455205388931e+81\n",
      "Gradient Descent(39/49): loss=3.360594548910531e+83\n",
      "Gradient Descent(40/49): loss=5.11611546843477e+85\n",
      "Gradient Descent(41/49): loss=7.788692478491037e+87\n",
      "Gradient Descent(42/49): loss=1.1857381034260092e+90\n",
      "Gradient Descent(43/49): loss=1.805148750960444e+92\n",
      "Gradient Descent(44/49): loss=2.7481296280172927e+94\n",
      "Gradient Descent(45/49): loss=4.183708654684681e+96\n",
      "Gradient Descent(46/49): loss=6.369211236921081e+98\n",
      "Gradient Descent(47/49): loss=9.696385462954565e+100\n",
      "Gradient Descent(48/49): loss=1.4761622365605017e+103\n",
      "Gradient Descent(49/49): loss=2.2472858128139231e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.828091536795637\n",
      "Gradient Descent(2/49): loss=631.877758157604\n",
      "Gradient Descent(3/49): loss=96082.1533283348\n",
      "Gradient Descent(4/49): loss=14821345.913545806\n",
      "Gradient Descent(5/49): loss=2294961363.2756724\n",
      "Gradient Descent(6/49): loss=355756382831.9471\n",
      "Gradient Descent(7/49): loss=55167367524164.31\n",
      "Gradient Descent(8/49): loss=8555789739788811.0\n",
      "Gradient Descent(9/49): loss=1.3269464436408166e+18\n",
      "Gradient Descent(10/49): loss=2.0580297536582477e+20\n",
      "Gradient Descent(11/49): loss=3.1919163433987735e+22\n",
      "Gradient Descent(12/49): loss=4.9505319287354e+24\n",
      "Gradient Descent(13/49): loss=7.678075828718432e+26\n",
      "Gradient Descent(14/49): loss=1.1908388133355655e+29\n",
      "Gradient Descent(15/49): loss=1.8469433841323523e+31\n",
      "Gradient Descent(16/49): loss=2.8645353758088323e+33\n",
      "Gradient Descent(17/49): loss=4.442779905147689e+35\n",
      "Gradient Descent(18/49): loss=6.890574114880385e+37\n",
      "Gradient Descent(19/49): loss=1.0687005133405625e+40\n",
      "Gradient Descent(20/49): loss=1.6575117955771607e+42\n",
      "Gradient Descent(21/49): loss=2.570734568107698e+44\n",
      "Gradient Descent(22/49): loss=3.987106600020823e+46\n",
      "Gradient Descent(23/49): loss=6.18384302962264e+48\n",
      "Gradient Descent(24/49): loss=9.590893460152106e+50\n",
      "Gradient Descent(25/49): loss=1.48750925473679e+53\n",
      "Gradient Descent(26/49): loss=2.3070674198614542e+55\n",
      "Gradient Descent(27/49): loss=3.5781693880810835e+57\n",
      "Gradient Descent(28/49): loss=5.5495977532245405e+59\n",
      "Gradient Descent(29/49): loss=8.607204378077716e+61\n",
      "Gradient Descent(30/49): loss=1.3349430084902007e+64\n",
      "Gradient Descent(31/49): loss=2.0704432677998884e+66\n",
      "Gradient Descent(32/49): loss=3.211174782679383e+68\n",
      "Gradient Descent(33/49): loss=4.980403783714089e+70\n",
      "Gradient Descent(34/49): loss=7.724407273819261e+72\n",
      "Gradient Descent(35/49): loss=1.1980247048831941e+75\n",
      "Gradient Descent(36/49): loss=1.858088449550134e+77\n",
      "Gradient Descent(37/49): loss=2.881820944325375e+79\n",
      "Gradient Descent(38/49): loss=4.469589139937422e+81\n",
      "Gradient Descent(39/49): loss=6.932154171196465e+83\n",
      "Gradient Descent(40/49): loss=1.0751494141564296e+86\n",
      "Gradient Descent(41/49): loss=1.6675137831814826e+88\n",
      "Gradient Descent(42/49): loss=2.586247251301265e+90\n",
      "Gradient Descent(43/49): loss=4.0111661518634703e+92\n",
      "Gradient Descent(44/49): loss=6.221158433231703e+94\n",
      "Gradient Descent(45/49): loss=9.648768160199402e+96\n",
      "Gradient Descent(46/49): loss=1.496485389472966e+99\n",
      "Gradient Descent(47/49): loss=2.320989046191121e+101\n",
      "Gradient Descent(48/49): loss=3.5997612742723755e+103\n",
      "Gradient Descent(49/49): loss=5.583085905991844e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.808408779914727\n",
      "Gradient Descent(2/49): loss=627.5358230129111\n",
      "Gradient Descent(3/49): loss=95412.6951318029\n",
      "Gradient Descent(4/49): loss=14735619.368124569\n",
      "Gradient Descent(5/49): loss=2285543616.710645\n",
      "Gradient Descent(6/49): loss=354959792859.9067\n",
      "Gradient Descent(7/49): loss=55150495684186.19\n",
      "Gradient Descent(8/49): loss=8569945830837748.0\n",
      "Gradient Descent(9/49): loss=1.331759390070566e+18\n",
      "Gradient Descent(10/49): loss=2.0695679213894468e+20\n",
      "Gradient Descent(11/49): loss=3.21614514349e+22\n",
      "Gradient Descent(12/49): loss=4.997954113704126e+24\n",
      "Gradient Descent(13/49): loss=7.766924941622497e+26\n",
      "Gradient Descent(14/49): loss=1.2069965362120386e+29\n",
      "Gradient Descent(15/49): loss=1.8756982054791853e+31\n",
      "Gradient Descent(16/49): loss=2.914874827820863e+33\n",
      "Gradient Descent(17/49): loss=4.5297773845768125e+35\n",
      "Gradient Descent(18/49): loss=7.039370266356632e+37\n",
      "Gradient Descent(19/49): loss=1.0939330914297093e+40\n",
      "Gradient Descent(20/49): loss=1.699995260216367e+42\n",
      "Gradient Descent(21/49): loss=2.641828744001405e+44\n",
      "Gradient Descent(22/49): loss=4.1054579833936307e+46\n",
      "Gradient Descent(23/49): loss=6.37996891047681e+48\n",
      "Gradient Descent(24/49): loss=9.914607204212912e+50\n",
      "Gradient Descent(25/49): loss=1.5407510192171317e+53\n",
      "Gradient Descent(26/49): loss=2.3943598110578677e+55\n",
      "Gradient Descent(27/49): loss=3.7208860051395786e+57\n",
      "Gradient Descent(28/49): loss=5.7823358875736125e+59\n",
      "Gradient Descent(29/49): loss=8.985872792270077e+61\n",
      "Gradient Descent(30/49): loss=1.3964237188708644e+64\n",
      "Gradient Descent(31/49): loss=2.170072120654311e+66\n",
      "Gradient Descent(32/49): loss=3.3723381701428795e+68\n",
      "Gradient Descent(33/49): loss=5.24068514846112e+70\n",
      "Gradient Descent(34/49): loss=8.144136038449958e+72\n",
      "Gradient Descent(35/49): loss=1.2656160393885827e+75\n",
      "Gradient Descent(36/49): loss=1.9667942082442277e+77\n",
      "Gradient Descent(37/49): loss=3.0564399764179565e+79\n",
      "Gradient Descent(38/49): loss=4.749772645398079e+81\n",
      "Gradient Descent(39/49): loss=7.381247581184869e+83\n",
      "Gradient Descent(40/49): loss=1.1470615526731494e+86\n",
      "Gradient Descent(41/49): loss=1.7825580176647154e+88\n",
      "Gradient Descent(42/49): loss=2.7701330228842315e+90\n",
      "Gradient Descent(43/49): loss=4.3048455581416724e+92\n",
      "Gradient Descent(44/49): loss=6.689821436862629e+94\n",
      "Gradient Descent(45/49): loss=1.0396124611826057e+97\n",
      "Gradient Descent(46/49): loss=1.6155798471551217e+99\n",
      "Gradient Descent(47/49): loss=2.510645399117902e+101\n",
      "Gradient Descent(48/49): loss=3.9015962790149425e+103\n",
      "Gradient Descent(49/49): loss=6.063163491654966e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.8268264079885315\n",
      "Gradient Descent(2/49): loss=625.2894010177461\n",
      "Gradient Descent(3/49): loss=94244.60236122228\n",
      "Gradient Descent(4/49): loss=14421143.451288862\n",
      "Gradient Descent(5/49): loss=2215708928.4426184\n",
      "Gradient Descent(6/49): loss=340850485189.15796\n",
      "Gradient Descent(7/49): loss=52454912909697.14\n",
      "Gradient Descent(8/49): loss=8073539124852439.0\n",
      "Gradient Descent(9/49): loss=1.2426818214445425e+18\n",
      "Gradient Descent(10/49): loss=1.9127663936485733e+20\n",
      "Gradient Descent(11/49): loss=2.9441904468976915e+22\n",
      "Gradient Descent(12/49): loss=4.5317977468205735e+24\n",
      "Gradient Descent(13/49): loss=6.975500192991868e+26\n",
      "Gradient Descent(14/49): loss=1.0736933476939779e+29\n",
      "Gradient Descent(15/49): loss=1.6526663831981664e+31\n",
      "Gradient Descent(16/49): loss=2.5438419908712143e+33\n",
      "Gradient Descent(17/49): loss=3.9155707278264145e+35\n",
      "Gradient Descent(18/49): loss=6.02698367653956e+37\n",
      "Gradient Descent(19/49): loss=9.276944482875477e+39\n",
      "Gradient Descent(20/49): loss=1.4279398049772698e+42\n",
      "Gradient Descent(21/49): loss=2.197934988774317e+44\n",
      "Gradient Descent(22/49): loss=3.3831385595895855e+46\n",
      "Gradient Descent(23/49): loss=5.207445430336898e+48\n",
      "Gradient Descent(24/49): loss=8.015482497203926e+50\n",
      "Gradient Descent(25/49): loss=1.2337711555986553e+53\n",
      "Gradient Descent(26/49): loss=1.8990637992393615e+55\n",
      "Gradient Descent(27/49): loss=2.923105550989991e+57\n",
      "Gradient Descent(28/49): loss=4.4993465020243765e+59\n",
      "Gradient Descent(29/49): loss=6.92555181198399e+61\n",
      "Gradient Descent(30/49): loss=1.0660052049535375e+64\n",
      "Gradient Descent(31/49): loss=1.6408325687805325e+66\n",
      "Gradient Descent(32/49): loss=2.525626991557011e+68\n",
      "Gradient Descent(33/49): loss=3.887533574020926e+70\n",
      "Gradient Descent(34/49): loss=5.983827912697035e+72\n",
      "Gradient Descent(35/49): loss=9.210517621777753e+74\n",
      "Gradient Descent(36/49): loss=1.4177151498804088e+77\n",
      "Gradient Descent(37/49): loss=2.182196841411047e+79\n",
      "Gradient Descent(38/49): loss=3.3589138516760933e+81\n",
      "Gradient Descent(39/49): loss=5.170157911000438e+83\n",
      "Gradient Descent(40/49): loss=7.958088240739482e+85\n",
      "Gradient Descent(41/49): loss=1.2249368305104897e+88\n",
      "Gradient Descent(42/49): loss=1.8854656964719126e+90\n",
      "Gradient Descent(43/49): loss=2.9021748746755986e+92\n",
      "Gradient Descent(44/49): loss=4.46712927154221e+94\n",
      "Gradient Descent(45/49): loss=6.875961921798275e+96\n",
      "Gradient Descent(46/49): loss=1.0583721552722722e+99\n",
      "Gradient Descent(47/49): loss=1.6290835112168942e+101\n",
      "Gradient Descent(48/49): loss=2.5075424304185535e+103\n",
      "Gradient Descent(49/49): loss=3.8596971837573637e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.819489672868068\n",
      "Gradient Descent(2/49): loss=625.4498329903919\n",
      "Gradient Descent(3/49): loss=94311.79795766657\n",
      "Gradient Descent(4/49): loss=14436649.123305725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=2219111326.9073625\n",
      "Gradient Descent(6/49): loss=341553737220.4261\n",
      "Gradient Descent(7/49): loss=52592536429522.734\n",
      "Gradient Descent(8/49): loss=8099361905406267.0\n",
      "Gradient Descent(9/49): loss=1.247378300500216e+18\n",
      "Gradient Descent(10/49): loss=1.9211113619173504e+20\n",
      "Gradient Descent(11/49): loss=2.958756686642296e+22\n",
      "Gradient Descent(12/49): loss=4.556871276093917e+24\n",
      "Gradient Descent(13/49): loss=7.018180573845651e+26\n",
      "Gradient Descent(14/49): loss=1.0808922593861075e+29\n",
      "Gradient Descent(15/49): loss=1.6647165842223905e+31\n",
      "Gradient Descent(16/49): loss=2.563883077307673e+33\n",
      "Gradient Descent(17/49): loss=3.9487180879374795e+35\n",
      "Gradient Descent(18/49): loss=6.081546665980632e+37\n",
      "Gradient Descent(19/49): loss=9.366383991246216e+39\n",
      "Gradient Descent(20/49): loss=1.4425466730319682e+42\n",
      "Gradient Descent(21/49): loss=2.2217121421043304e+44\n",
      "Gradient Descent(22/49): loss=3.4217297331366976e+46\n",
      "Gradient Descent(23/49): loss=5.269915100560329e+48\n",
      "Gradient Descent(24/49): loss=8.116364334178155e+50\n",
      "Gradient Descent(25/49): loss=1.2500271588475155e+53\n",
      "Gradient Descent(26/49): loss=1.92520669787718e+55\n",
      "Gradient Descent(27/49): loss=2.965072241285487e+57\n",
      "Gradient Descent(28/49): loss=4.566602331965882e+59\n",
      "Gradient Descent(29/49): loss=7.033169906604226e+61\n",
      "Gradient Descent(30/49): loss=1.0832009301293599e+64\n",
      "Gradient Descent(31/49): loss=1.6682723019834114e+66\n",
      "Gradient Descent(32/49): loss=2.569359383058024e+68\n",
      "Gradient Descent(33/49): loss=3.957152337456943e+70\n",
      "Gradient Descent(34/49): loss=6.094536531204767e+72\n",
      "Gradient Descent(35/49): loss=9.386390101438361e+74\n",
      "Gradient Descent(36/49): loss=1.4456278781048474e+77\n",
      "Gradient Descent(37/49): loss=2.2264576044348086e+79\n",
      "Gradient Descent(38/49): loss=3.429038371094599e+81\n",
      "Gradient Descent(39/49): loss=5.281171367026343e+83\n",
      "Gradient Descent(40/49): loss=8.133700469206425e+85\n",
      "Gradient Descent(41/49): loss=1.2526971522989852e+88\n",
      "Gradient Descent(42/49): loss=1.9293188399536604e+90\n",
      "Gradient Descent(43/49): loss=2.9714054824575244e+92\n",
      "Gradient Descent(44/49): loss=4.576356358698435e+94\n",
      "Gradient Descent(45/49): loss=7.048192394286936e+96\n",
      "Gradient Descent(46/49): loss=1.0855145913727038e+99\n",
      "Gradient Descent(47/49): loss=1.671835645460221e+101\n",
      "Gradient Descent(48/49): loss=2.57484740200211e+103\n",
      "Gradient Descent(49/49): loss=3.9656046104771223e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.880799006203144\n",
      "Gradient Descent(2/49): loss=646.7059108542902\n",
      "Gradient Descent(3/49): loss=99481.12362704845\n",
      "Gradient Descent(4/49): loss=15524275.884722134\n",
      "Gradient Descent(5/49): loss=2431798247.84079\n",
      "Gradient Descent(6/49): loss=381359347159.7936\n",
      "Gradient Descent(7/49): loss=59826542996526.52\n",
      "Gradient Descent(8/49): loss=9386460700808744.0\n",
      "Gradient Descent(9/49): loss=1.472737464970403e+18\n",
      "Gradient Descent(10/49): loss=2.310754404333215e+20\n",
      "Gradient Descent(11/49): loss=3.6256330682423564e+22\n",
      "Gradient Descent(12/49): loss=5.688718271669204e+24\n",
      "Gradient Descent(13/49): loss=8.925759277854069e+26\n",
      "Gradient Descent(14/49): loss=1.4004769570212312e+29\n",
      "Gradient Descent(15/49): loss=2.197388170946225e+31\n",
      "Gradient Descent(16/49): loss=3.447764571706855e+33\n",
      "Gradient Descent(17/49): loss=5.4096407491266596e+35\n",
      "Gradient Descent(18/49): loss=8.48788032508784e+37\n",
      "Gradient Descent(19/49): loss=1.3317725850036806e+40\n",
      "Gradient Descent(20/49): loss=2.0895890970227732e+42\n",
      "Gradient Descent(21/49): loss=3.278624777052267e+44\n",
      "Gradient Descent(22/49): loss=5.144255607081698e+46\n",
      "Gradient Descent(23/49): loss=8.071483487935838e+48\n",
      "Gradient Descent(24/49): loss=1.2664387361785321e+51\n",
      "Gradient Descent(25/49): loss=1.987078428508837e+53\n",
      "Gradient Descent(26/49): loss=3.1177826200738835e+55\n",
      "Gradient Descent(27/49): loss=4.891889684158006e+57\n",
      "Gradient Descent(28/49): loss=7.675514170838748e+59\n",
      "Gradient Descent(29/49): loss=1.2043100231293728e+62\n",
      "Gradient Descent(30/49): loss=1.889596709130153e+64\n",
      "Gradient Descent(31/49): loss=2.964831027377346e+66\n",
      "Gradient Descent(32/49): loss=4.651904281176429e+68\n",
      "Gradient Descent(33/49): loss=7.298970241946745e+70\n",
      "Gradient Descent(34/49): loss=1.145229208786539e+73\n",
      "Gradient Descent(35/49): loss=1.7968972295851329e+75\n",
      "Gradient Descent(36/49): loss=2.819382905114628e+77\n",
      "Gradient Descent(37/49): loss=4.4236920369050946e+79\n",
      "Gradient Descent(38/49): loss=6.9408987342150185e+81\n",
      "Gradient Descent(39/49): loss=1.0890467699087997e+84\n",
      "Gradient Descent(40/49): loss=1.7087453836522735e+86\n",
      "Gradient Descent(41/49): loss=2.681070149445911e+88\n",
      "Gradient Descent(42/49): loss=4.2066753859407515e+90\n",
      "Gradient Descent(43/49): loss=6.600393431084615e+92\n",
      "Gradient Descent(44/49): loss=1.035620518538355e+95\n",
      "Gradient Descent(45/49): loss=1.624918074378186e+97\n",
      "Gradient Descent(46/49): loss=2.549542714900498e+99\n",
      "Gradient Descent(47/49): loss=4.000305096975183e+101\n",
      "Gradient Descent(48/49): loss=6.276592572998001e+103\n",
      "Gradient Descent(49/49): loss=9.848152421474698e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.860848177832352\n",
      "Gradient Descent(2/49): loss=642.2630046016822\n",
      "Gradient Descent(3/49): loss=98787.93390439227\n",
      "Gradient Descent(4/49): loss=15434446.900943594\n",
      "Gradient Descent(5/49): loss=2421809823.815172\n",
      "Gradient Descent(6/49): loss=380503571182.2733\n",
      "Gradient Descent(7/49): loss=59807896911540.48\n",
      "Gradient Descent(8/49): loss=9401927869172452.0\n",
      "Gradient Descent(9/49): loss=1.4780679738997883e+18\n",
      "Gradient Descent(10/49): loss=2.3236898065309178e+20\n",
      "Gradient Descent(11/49): loss=3.6531200858953894e+22\n",
      "Gradient Descent(12/49): loss=5.743153404729987e+24\n",
      "Gradient Descent(13/49): loss=9.028947039811419e+26\n",
      "Gradient Descent(14/49): loss=1.4194623143738479e+29\n",
      "Gradient Descent(15/49): loss=2.2315707061532436e+31\n",
      "Gradient Descent(16/49): loss=3.50830582505088e+33\n",
      "Gradient Descent(17/49): loss=5.5154917568408194e+35\n",
      "Gradient Descent(18/49): loss=8.67103693223519e+37\n",
      "Gradient Descent(19/49): loss=1.3631945226758234e+40\n",
      "Gradient Descent(20/49): loss=2.143110820035745e+42\n",
      "Gradient Descent(21/49): loss=3.3692359460575376e+44\n",
      "Gradient Descent(22/49): loss=5.296856678774775e+46\n",
      "Gradient Descent(23/49): loss=8.327315487808896e+48\n",
      "Gradient Descent(24/49): loss=1.3091572500260614e+51\n",
      "Gradient Descent(25/49): loss=2.0581575272442605e+53\n",
      "Gradient Descent(26/49): loss=3.235678836036672e+55\n",
      "Gradient Descent(27/49): loss=5.086888341338308e+57\n",
      "Gradient Descent(28/49): loss=7.997219226163989e+59\n",
      "Gradient Descent(29/49): loss=1.2572620246368836e+62\n",
      "Gradient Descent(30/49): loss=1.9765717981352052e+64\n",
      "Gradient Descent(31/49): loss=3.1074159535772095e+66\n",
      "Gradient Descent(32/49): loss=4.885243186033609e+68\n",
      "Gradient Descent(33/49): loss=7.680208038841385e+70\n",
      "Gradient Descent(34/49): loss=1.2074239351792664e+73\n",
      "Gradient Descent(35/49): loss=1.898220141786308e+75\n",
      "Gradient Descent(36/49): loss=2.9842374345082515e+77\n",
      "Gradient Descent(37/49): loss=4.691591280419036e+79\n",
      "Gradient Descent(38/49): loss=7.375763231162248e+81\n",
      "Gradient Descent(39/49): loss=1.1595614364196207e+84\n",
      "Gradient Descent(40/49): loss=1.8229743589798768e+86\n",
      "Gradient Descent(41/49): loss=2.8659417337638124e+88\n",
      "Gradient Descent(42/49): loss=4.505615770660338e+90\n",
      "Gradient Descent(43/49): loss=7.083386669610518e+92\n",
      "Gradient Descent(44/49): loss=1.1135962155925771e+95\n",
      "Gradient Descent(45/49): loss=1.7507113323383e+97\n",
      "Gradient Descent(46/49): loss=2.752335295560209e+99\n",
      "Gradient Descent(47/49): loss=4.327012363065404e+101\n",
      "Gradient Descent(48/49): loss=6.802599966771065e+103\n",
      "Gradient Descent(49/49): loss=1.0694530642646747e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.879534829222738\n",
      "Gradient Descent(2/49): loss=639.9684472725918\n",
      "Gradient Descent(3/49): loss=97579.64992694564\n",
      "Gradient Descent(4/49): loss=15105300.08429694\n",
      "Gradient Descent(5/49): loss=2347859430.4455843\n",
      "Gradient Descent(6/49): loss=365387912725.424\n",
      "Gradient Descent(7/49): loss=56886310294673.9\n",
      "Gradient Descent(8/49): loss=8857620450253004.0\n",
      "Gradient Descent(9/49): loss=1.379255248820025e+18\n",
      "Gradient Descent(10/49): loss=2.1477228976287603e+20\n",
      "Gradient Descent(11/49): loss=3.3443662978709407e+22\n",
      "Gradient Descent(12/49): loss=5.207749508214805e+24\n",
      "Gradient Descent(13/49): loss=8.109359471204237e+26\n",
      "Gradient Descent(14/49): loss=1.2627666098857363e+29\n",
      "Gradient Descent(15/49): loss=1.9663446958737768e+31\n",
      "Gradient Descent(16/49): loss=3.061936784443745e+33\n",
      "Gradient Descent(17/49): loss=4.767962070804653e+35\n",
      "Gradient Descent(18/49): loss=7.4245368059649375e+37\n",
      "Gradient Descent(19/49): loss=1.1561280480136007e+40\n",
      "Gradient Descent(20/49): loss=1.800290171124686e+42\n",
      "Gradient Descent(21/49): loss=2.8033613629985583e+44\n",
      "Gradient Descent(22/49): loss=4.365315690653182e+46\n",
      "Gradient Descent(23/49): loss=6.797547162749405e+48\n",
      "Gradient Descent(24/49): loss=1.0584949796152845e+51\n",
      "Gradient Descent(25/49): loss=1.6482586954476533e+53\n",
      "Gradient Descent(26/49): loss=2.566622213085014e+55\n",
      "Gradient Descent(27/49): loss=3.9966721261027045e+57\n",
      "Gradient Descent(28/49): loss=6.223505743124994e+59\n",
      "Gradient Descent(29/49): loss=9.691068597233025e+61\n",
      "Gradient Descent(30/49): loss=1.5090660221535832e+64\n",
      "Gradient Descent(31/49): loss=2.3498752860635412e+66\n",
      "Gradient Descent(32/49): loss=3.6591598902822882e+68\n",
      "Gradient Descent(33/49): loss=5.697941155456121e+70\n",
      "Gradient Descent(34/49): loss=8.872674161427789e+72\n",
      "Gradient Descent(35/49): loss=1.381627935899006e+75\n",
      "Gradient Descent(36/49): loss=2.1514322723075936e+77\n",
      "Gradient Descent(37/49): loss=3.350149994842722e+79\n",
      "Gradient Descent(38/49): loss=5.2167596128445824e+81\n",
      "Gradient Descent(39/49): loss=8.123391758608104e+83\n",
      "Gradient Descent(40/49): loss=1.26495178158764e+86\n",
      "Gradient Descent(41/49): loss=1.9697474371418307e+88\n",
      "Gradient Descent(42/49): loss=3.067235465099833e+90\n",
      "Gradient Descent(43/49): loss=4.776213041818929e+92\n",
      "Gradient Descent(44/49): loss=7.437384993883633e+94\n",
      "Gradient Descent(45/49): loss=1.158128732176062e+97\n",
      "Gradient Descent(46/49): loss=1.8034055805834434e+99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=2.8082125913314194e+101\n",
      "Gradient Descent(48/49): loss=4.372869887405477e+103\n",
      "Gradient Descent(49/49): loss=6.809310346091553e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.871781920655348\n",
      "Gradient Descent(2/49): loss=640.0526707693272\n",
      "Gradient Descent(3/49): loss=97631.07327389003\n",
      "Gradient Descent(4/49): loss=15117794.55938315\n",
      "Gradient Descent(5/49): loss=2350736063.2308908\n",
      "Gradient Descent(6/49): loss=366005590933.2009\n",
      "Gradient Descent(7/49): loss=57010804268629.57\n",
      "Gradient Descent(8/49): loss=8881543002150025.0\n",
      "Gradient Descent(9/49): loss=1.383695237258996e+18\n",
      "Gradient Descent(10/49): loss=2.1557553647074332e+20\n",
      "Gradient Descent(11/49): loss=3.3586200227300984e+22\n",
      "Gradient Descent(12/49): loss=5.23266667509097e+24\n",
      "Gradient Descent(13/49): loss=8.152401113480073e+26\n",
      "Gradient Descent(14/49): loss=1.2701297841097842e+29\n",
      "Gradient Descent(15/49): loss=1.9788400512339147e+31\n",
      "Gradient Descent(16/49): loss=3.0829983612711036e+33\n",
      "Gradient Descent(17/49): loss=4.803257831482954e+35\n",
      "Gradient Descent(18/49): loss=7.483392190337372e+37\n",
      "Gradient Descent(19/49): loss=1.1658994934603527e+40\n",
      "Gradient Descent(20/49): loss=1.8164511423537608e+42\n",
      "Gradient Descent(21/49): loss=2.829999302186333e+44\n",
      "Gradient Descent(22/49): loss=4.4090897156131847e+46\n",
      "Gradient Descent(23/49): loss=6.869285128638072e+48\n",
      "Gradient Descent(24/49): loss=1.0702226813763607e+51\n",
      "Gradient Descent(25/49): loss=1.6673883326795202e+53\n",
      "Gradient Descent(26/49): loss=2.597762036197622e+55\n",
      "Gradient Descent(27/49): loss=4.047268092530055e+57\n",
      "Gradient Descent(28/49): loss=6.30557332987624e+59\n",
      "Gradient Descent(29/49): loss=9.823973630961551e+61\n",
      "Gradient Descent(30/49): loss=1.530558013568011e+64\n",
      "Gradient Descent(31/49): loss=2.384582777700284e+66\n",
      "Gradient Descent(32/49): loss=3.715138513730133e+68\n",
      "Gradient Descent(33/49): loss=5.788121219894062e+70\n",
      "Gradient Descent(34/49): loss=9.01779223907058e+72\n",
      "Gradient Descent(35/49): loss=1.404956354188634e+75\n",
      "Gradient Descent(36/49): loss=2.1888975758643705e+77\n",
      "Gradient Descent(37/49): loss=3.410264371089231e+79\n",
      "Gradient Descent(38/49): loss=5.313132605635129e+81\n",
      "Gradient Descent(39/49): loss=8.277768235324977e+83\n",
      "Gradient Descent(40/49): loss=1.2896619008733384e+86\n",
      "Gradient Descent(41/49): loss=2.009270821894346e+88\n",
      "Gradient Descent(42/49): loss=3.1304090110610243e+90\n",
      "Gradient Descent(43/49): loss=4.8771228197566203e+92\n",
      "Gradient Descent(44/49): loss=7.598472568582546e+94\n",
      "Gradient Descent(45/49): loss=1.1838288168921442e+97\n",
      "Gradient Descent(46/49): loss=1.844384716869072e+99\n",
      "Gradient Descent(47/49): loss=2.8735193258351883e+101\n",
      "Gradient Descent(48/49): loss=4.4768931559816554e+103\n",
      "Gradient Descent(49/49): loss=6.974921710070653e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.933821322242416\n",
      "Gradient Descent(2/49): loss=661.79373498085\n",
      "Gradient Descent(3/49): loss=102979.71315026368\n",
      "Gradient Descent(4/49): loss=16256205.50716175\n",
      "Gradient Descent(5/49): loss=2575934863.3396797\n",
      "Gradient Descent(6/49): loss=408641345526.4406\n",
      "Gradient Descent(7/49): loss=64848929302234.516\n",
      "Gradient Descent(8/49): loss=1.0292287768441816e+16\n",
      "Gradient Descent(9/49): loss=1.6335656889787584e+18\n",
      "Gradient Descent(10/49): loss=2.5927837382535745e+20\n",
      "Gradient Descent(11/49): loss=4.115263032812956e+22\n",
      "Gradient Descent(12/49): loss=6.531748046500948e+24\n",
      "Gradient Descent(13/49): loss=1.0367198569497824e+27\n",
      "Gradient Descent(14/49): loss=1.645483242909995e+29\n",
      "Gradient Descent(15/49): loss=2.6117134666614383e+31\n",
      "Gradient Descent(16/49): loss=4.145315578139789e+33\n",
      "Gradient Descent(17/49): loss=6.579451204586146e+35\n",
      "Gradient Descent(18/49): loss=1.0442914995480494e+38\n",
      "Gradient Descent(19/49): loss=1.657501062919923e+40\n",
      "Gradient Descent(20/49): loss=2.6307882187641416e+42\n",
      "Gradient Descent(21/49): loss=4.1755910793259706e+44\n",
      "Gradient Descent(22/49): loss=6.627504539377984e+46\n",
      "Gradient Descent(23/49): loss=1.0519185328553142e+49\n",
      "Gradient Descent(24/49): loss=1.6696067021781435e+51\n",
      "Gradient Descent(25/49): loss=2.6500023080617935e+53\n",
      "Gradient Descent(26/49): loss=4.2060877112989917e+55\n",
      "Gradient Descent(27/49): loss=6.675908840275905e+57\n",
      "Gradient Descent(28/49): loss=1.0596012708900623e+60\n",
      "Gradient Descent(29/49): loss=1.6818007557236797e+62\n",
      "Gradient Descent(30/49): loss=2.669356728476594e+64\n",
      "Gradient Descent(31/49): loss=4.236807076946042e+66\n",
      "Gradient Descent(32/49): loss=6.724666664355648e+68\n",
      "Gradient Descent(33/49): loss=1.0673401201758773e+71\n",
      "Gradient Descent(34/49): loss=1.6940838691314046e+73\n",
      "Gradient Descent(35/49): loss=2.68885250484011e+75\n",
      "Gradient Descent(36/49): loss=4.2677508029704883e+77\n",
      "Gradient Descent(37/49): loss=6.773780593568961e+79\n",
      "Gradient Descent(38/49): loss=1.0751354905228985e+82\n",
      "Gradient Descent(39/49): loss=1.7064566928538208e+84\n",
      "Gradient Descent(40/49): loss=2.7084906695520976e+86\n",
      "Gradient Descent(41/49): loss=4.29892052799911e+88\n",
      "Gradient Descent(42/49): loss=6.8232532287469115e+90\n",
      "Gradient Descent(43/49): loss=1.0829877947354e+93\n",
      "Gradient Descent(44/49): loss=1.718919882094497e+95\n",
      "Gradient Descent(45/49): loss=2.728272262552734e+97\n",
      "Gradient Descent(46/49): loss=4.3303179026266166e+99\n",
      "Gradient Descent(47/49): loss=6.873087189715949e+101\n",
      "Gradient Descent(48/49): loss=1.0908974486326682e+104\n",
      "Gradient Descent(49/49): loss=1.7314740968424172e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.913600687085312\n",
      "Gradient Descent(2/49): loss=657.2481077161359\n",
      "Gradient Descent(3/49): loss=102262.09520582913\n",
      "Gradient Descent(4/49): loss=16162103.363619935\n",
      "Gradient Descent(5/49): loss=2565344836.3417015\n",
      "Gradient Descent(6/49): loss=407722381318.2647\n",
      "Gradient Descent(7/49): loss=64828343609107.35\n",
      "Gradient Descent(8/49): loss=1.0309178942545402e+16\n",
      "Gradient Descent(9/49): loss=1.6394659913365944e+18\n",
      "Gradient Descent(10/49): loss=2.607276127176173e+20\n",
      "Gradient Descent(11/49): loss=4.146423951413041e+22\n",
      "Gradient Descent(12/49): loss=6.594183936616683e+24\n",
      "Gradient Descent(13/49): loss=1.048693634053301e+27\n",
      "Gradient Descent(14/49): loss=1.667770467549331e+29\n",
      "Gradient Descent(15/49): loss=2.6523079719589056e+31\n",
      "Gradient Descent(16/49): loss=4.2180490914806786e+33\n",
      "Gradient Descent(17/49): loss=6.708096658454494e+35\n",
      "Gradient Descent(18/49): loss=1.0668097950277646e+38\n",
      "Gradient Descent(19/49): loss=1.6965813068036696e+40\n",
      "Gradient Descent(20/49): loss=2.6981268306845173e+42\n",
      "Gradient Descent(21/49): loss=4.290916306674399e+44\n",
      "Gradient Descent(22/49): loss=6.823979711372926e+46\n",
      "Gradient Descent(23/49): loss=1.0852390439110402e+49\n",
      "Gradient Descent(24/49): loss=1.72588992383578e+51\n",
      "Gradient Descent(25/49): loss=2.7447372502057074e+53\n",
      "Gradient Descent(26/49): loss=4.365042328960046e+55\n",
      "Gradient Descent(27/49): loss=6.941864665620224e+57\n",
      "Gradient Descent(28/49): loss=1.1039866604745736e+60\n",
      "Gradient Descent(29/49): loss=1.7557048505164497e+62\n",
      "Gradient Descent(30/49): loss=2.7921528696750007e+64\n",
      "Gradient Descent(31/49): loss=4.440448886007863e+66\n",
      "Gradient Descent(32/49): loss=7.061786094664431e+68\n",
      "Gradient Descent(33/49): loss=1.1230581440524208e+71\n",
      "Gradient Descent(34/49): loss=1.7860348331357958e+73\n",
      "Gradient Descent(35/49): loss=2.8403875988681835e+75\n",
      "Gradient Descent(36/49): loss=4.5171580991168476e+77\n",
      "Gradient Descent(37/49): loss=7.183779178780972e+79\n",
      "Gradient Descent(38/49): loss=1.1424590894787751e+82\n",
      "Gradient Descent(39/49): loss=1.8168887693373572e+84\n",
      "Gradient Descent(40/49): loss=2.889455587989823e+86\n",
      "Gradient Descent(41/49): loss=4.59519247180477e+88\n",
      "Gradient Descent(42/49): loss=7.307879706024911e+90\n",
      "Gradient Descent(43/49): loss=1.1621951882410659e+93\n",
      "Gradient Descent(44/49): loss=1.8482757104733435e+95\n",
      "Gradient Descent(45/49): loss=2.9393712316911986e+97\n",
      "Gradient Descent(46/49): loss=4.674574896340091e+99\n",
      "Gradient Descent(47/49): loss=7.434124082693864e+101\n",
      "Gradient Descent(48/49): loss=1.1822722301477921e+104\n",
      "Gradient Descent(49/49): loss=1.8802048642590493e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.932558180014853\n",
      "Gradient Descent(2/49): loss=654.9046522201515\n",
      "Gradient Descent(3/49): loss=101012.47495714345\n",
      "Gradient Descent(4/49): loss=15817690.380246315\n",
      "Gradient Descent(5/49): loss=2487061735.3540926\n",
      "Gradient Descent(6/49): loss=391534966847.1391\n",
      "Gradient Descent(7/49): loss=61663262859083.8\n",
      "Gradient Descent(8/49): loss=9712663446488240.0\n",
      "Gradient Descent(9/49): loss=1.529919312269163e+18\n",
      "Gradient Descent(10/49): loss=2.4099318614156073e+20\n",
      "Gradient Descent(11/49): loss=3.796146805421626e+22\n",
      "Gradient Descent(12/49): loss=5.979734421400447e+24\n",
      "Gradient Descent(13/49): loss=9.4193516718713e+26\n",
      "Gradient Descent(14/49): loss=1.4837481809579521e+29\n",
      "Gradient Descent(15/49): loss=2.337219017390987e+31\n",
      "Gradient Descent(16/49): loss=3.6816172077230656e+33\n",
      "Gradient Descent(17/49): loss=5.7993304201299056e+35\n",
      "Gradient Descent(18/49): loss=9.135179322421472e+37\n",
      "Gradient Descent(19/49): loss=1.438985111424723e+40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/49): loss=2.2667077221713614e+42\n",
      "Gradient Descent(21/49): loss=3.5705469482143775e+44\n",
      "Gradient Descent(22/49): loss=5.62437114631437e+46\n",
      "Gradient Descent(23/49): loss=8.859581249187729e+48\n",
      "Gradient Descent(24/49): loss=1.3955725514771775e+51\n",
      "Gradient Descent(25/49): loss=2.198323703636404e+53\n",
      "Gradient Descent(26/49): loss=3.4628275691262535e+55\n",
      "Gradient Descent(27/49): loss=5.454690204935078e+57\n",
      "Gradient Descent(28/49): loss=8.592297663646865e+59\n",
      "Gradient Descent(29/49): loss=1.3534696997808883e+62\n",
      "Gradient Descent(30/49): loss=2.1320027540194108e+64\n",
      "Gradient Descent(31/49): loss=3.3583579624148255e+66\n",
      "Gradient Descent(32/49): loss=5.290128346434783e+68\n",
      "Gradient Descent(33/49): loss=8.333077722789814e+70\n",
      "Gradient Descent(34/49): loss=1.3126370436901484e+73\n",
      "Gradient Descent(35/49): loss=2.067682632738921e+75\n",
      "Gradient Descent(36/49): loss=3.257040086047849e+77\n",
      "Gradient Descent(37/49): loss=5.130531133818408e+79\n",
      "Gradient Descent(38/49): loss=8.081678155524315e+81\n",
      "Gradient Descent(39/49): loss=1.273036262833648e+84\n",
      "Gradient Descent(40/49): loss=2.0053029770576347e+86\n",
      "Gradient Descent(41/49): loss=3.15877885587121e+88\n",
      "Gradient Descent(42/49): loss=4.975748789312352e+90\n",
      "Gradient Descent(43/49): loss=7.837863029988862e+92\n",
      "Gradient Descent(44/49): loss=1.2346301929233107e+95\n",
      "Gradient Descent(45/49): loss=1.944805245314437e+97\n",
      "Gradient Descent(46/49): loss=3.063482056312761e+99\n",
      "Gradient Descent(47/49): loss=4.825636053769974e+101\n",
      "Gradient Descent(48/49): loss=7.601403532120884e+103\n",
      "Gradient Descent(49/49): loss=1.1973827908757984e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.924384736235514\n",
      "Gradient Descent(2/49): loss=654.9099458423279\n",
      "Gradient Descent(3/49): loss=101047.137413293\n",
      "Gradient Descent(4/49): loss=15826896.513387833\n",
      "Gradient Descent(5/49): loss=2489346102.6662827\n",
      "Gradient Descent(6/49): loss=392052624736.7998\n",
      "Gradient Descent(7/49): loss=61771695064184.18\n",
      "Gradient Descent(8/49): loss=9734119426478048.0\n",
      "Gradient Descent(9/49): loss=1.5339975502149724e+18\n",
      "Gradient Descent(10/49): loss=2.4174622333455716e+20\n",
      "Gradient Descent(11/49): loss=3.809755541104484e+22\n",
      "Gradient Descent(12/49): loss=6.003925931915325e+24\n",
      "Gradient Descent(13/49): loss=9.461801113152058e+26\n",
      "Gradient Descent(14/49): loss=1.4911193166534956e+29\n",
      "Gradient Descent(15/49): loss=2.349908806874838e+31\n",
      "Gradient Descent(16/49): loss=3.7033062837307405e+33\n",
      "Gradient Descent(17/49): loss=5.836174367331905e+35\n",
      "Gradient Descent(18/49): loss=9.197438378423e+37\n",
      "Gradient Descent(19/49): loss=1.4494575980869509e+40\n",
      "Gradient Descent(20/49): loss=2.2842526830661247e+42\n",
      "Gradient Descent(21/49): loss=3.5998364681707685e+44\n",
      "Gradient Descent(22/49): loss=5.67311256504843e+46\n",
      "Gradient Descent(23/49): loss=8.940463396234442e+48\n",
      "Gradient Descent(24/49): loss=1.4089599813724513e+51\n",
      "Gradient Descent(25/49): loss=2.2204310236849708e+53\n",
      "Gradient Descent(26/49): loss=3.499257605700433e+55\n",
      "Gradient Descent(27/49): loss=5.514606695925691e+57\n",
      "Gradient Descent(28/49): loss=8.690668260950426e+59\n",
      "Gradient Descent(29/49): loss=1.3695938620190954e+62\n",
      "Gradient Descent(30/49): loss=2.158392531571862e+64\n",
      "Gradient Descent(31/49): loss=3.401488900860916e+66\n",
      "Gradient Descent(32/49): loss=5.360529455804782e+68\n",
      "Gradient Descent(33/49): loss=8.447852362322214e+70\n",
      "Gradient Descent(34/49): loss=1.3313276258245775e+73\n",
      "Gradient Descent(35/49): loss=2.098087385130973e+75\n",
      "Gradient Descent(36/49): loss=3.306451838193687e+77\n",
      "Gradient Descent(37/49): loss=5.2107571094384986e+79\n",
      "Gradient Descent(38/49): loss=8.211820701552052e+81\n",
      "Gradient Descent(39/49): loss=1.2941305422256653e+84\n",
      "Gradient Descent(40/49): loss=2.0394671549572008e+86\n",
      "Gradient Descent(41/49): loss=3.214070096047476e+88\n",
      "Gradient Descent(42/49): loss=5.065169378775033e+90\n",
      "Gradient Descent(43/49): loss=7.982383728105584e+92\n",
      "Gradient Descent(44/49): loss=1.2579727392676969e+95\n",
      "Gradient Descent(45/49): loss=1.9824847647561538e+97\n",
      "Gradient Descent(46/49): loss=3.124269485186283e+99\n",
      "Gradient Descent(47/49): loss=4.923649346312521e+101\n",
      "Gradient Descent(48/49): loss=7.759357187460513e+103\n",
      "Gradient Descent(49/49): loss=1.2228251796137003e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.9871584849134525\n",
      "Gradient Descent(2/49): loss=677.1442454067741\n",
      "Gradient Descent(3/49): loss=106580.24418250819\n",
      "Gradient Descent(4/49): loss=17018154.287419446\n",
      "Gradient Descent(5/49): loss=2727715327.0159693\n",
      "Gradient Descent(6/49): loss=437701889021.45557\n",
      "Gradient Descent(7/49): loss=70260508244761.41\n",
      "Gradient Descent(8/49): loss=1.1279579742447838e+16\n",
      "Gradient Descent(9/49): loss=1.810882132328655e+18\n",
      "Gradient Descent(10/49): loss=2.9073176148133636e+20\n",
      "Gradient Descent(11/49): loss=4.667629731690548e+22\n",
      "Gradient Descent(12/49): loss=7.49377826913717e+24\n",
      "Gradient Descent(13/49): loss=1.2031103110551964e+27\n",
      "Gradient Descent(14/49): loss=1.9315684962307576e+29\n",
      "Gradient Descent(15/49): loss=3.101093035693291e+31\n",
      "Gradient Descent(16/49): loss=4.9787404177250965e+33\n",
      "Gradient Descent(17/49): loss=7.993264299530208e+35\n",
      "Gradient Descent(18/49): loss=1.2833019777801982e+38\n",
      "Gradient Descent(19/49): loss=2.0603146664320464e+40\n",
      "Gradient Descent(20/49): loss=3.307792397125708e+42\n",
      "Gradient Descent(21/49): loss=5.310591979589244e+44\n",
      "Gradient Descent(22/49): loss=8.52604510446344e+46\n",
      "Gradient Descent(23/49): loss=1.368838830079258e+49\n",
      "Gradient Descent(24/49): loss=2.19764230634286e+51\n",
      "Gradient Descent(25/49): loss=3.5282690704724765e+53\n",
      "Gradient Descent(26/49): loss=5.664562698727286e+55\n",
      "Gradient Descent(27/49): loss=9.094337740947118e+57\n",
      "Gradient Descent(28/49): loss=1.4600770323364695e+60\n",
      "Gradient Descent(29/49): loss=2.3441233447466805e+62\n",
      "Gradient Descent(30/49): loss=3.763441334730945e+64\n",
      "Gradient Descent(31/49): loss=6.042126883682403e+66\n",
      "Gradient Descent(32/49): loss=9.70050919662544e+68\n",
      "Gradient Descent(33/49): loss=1.557396600325359e+71\n",
      "Gradient Descent(34/49): loss=2.5003678895008338e+73\n",
      "Gradient Descent(35/49): loss=4.0142887056134345e+75\n",
      "Gradient Descent(36/49): loss=6.444857126697757e+77\n",
      "Gradient Descent(37/49): loss=1.0347084235736237e+80\n",
      "Gradient Descent(38/49): loss=1.6612028797025985e+82\n",
      "Gradient Descent(39/49): loss=2.6670267146383667e+84\n",
      "Gradient Descent(40/49): loss=4.28185598731215e+86\n",
      "Gradient Descent(41/49): loss=6.874430839200253e+88\n",
      "Gradient Descent(42/49): loss=1.1036755907480253e+91\n",
      "Gradient Descent(43/49): loss=1.7719282339230225e+93\n",
      "Gradient Descent(44/49): loss=2.844793970704374e+95\n",
      "Gradient Descent(45/49): loss=4.5672576241073485e+97\n",
      "Gradient Descent(46/49): loss=7.332637238331033e+99\n",
      "Gradient Descent(47/49): loss=1.177239676280965e+102\n",
      "Gradient Descent(48/49): loss=1.8900338450747544e+104\n",
      "Gradient Descent(49/49): loss=3.034410075961025e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.966666307673603\n",
      "Gradient Descent(2/49): loss=672.4941271193139\n",
      "Gradient Descent(3/49): loss=105837.48505904141\n",
      "Gradient Descent(4/49): loss=16919602.210488863\n",
      "Gradient Descent(5/49): loss=2716491291.225845\n",
      "Gradient Descent(6/49): loss=436715490054.66974\n",
      "Gradient Descent(7/49): loss=70237803829279.97\n",
      "Gradient Descent(8/49): loss=1.1298016855306036e+16\n",
      "Gradient Descent(9/49): loss=1.8174093886265324e+18\n",
      "Gradient Descent(10/49): loss=2.9235439407875677e+20\n",
      "Gradient Descent(11/49): loss=4.702930435244929e+22\n",
      "Gradient Descent(12/49): loss=7.565335004770936e+24\n",
      "Gradient Descent(13/49): loss=1.216992765797271e+27\n",
      "Gradient Descent(14/49): loss=1.9577079420773352e+29\n",
      "Gradient Descent(15/49): loss=3.1492550383457794e+31\n",
      "Gradient Descent(16/49): loss=5.066030159228096e+33\n",
      "Gradient Descent(17/49): loss=8.149438967500433e+35\n",
      "Gradient Descent(18/49): loss=1.3109546040677305e+38\n",
      "Gradient Descent(19/49): loss=2.108859250179882e+40\n",
      "Gradient Descent(20/49): loss=3.3924037676177805e+42\n",
      "Gradient Descent(21/49): loss=5.457169947387498e+44\n",
      "Gradient Descent(22/49): loss=8.778643662533725e+46\n",
      "Gradient Descent(23/49): loss=1.412171240721908e+49\n",
      "Gradient Descent(24/49): loss=2.2716807855388215e+51\n",
      "Gradient Descent(25/49): loss=3.6543256529939954e+53\n",
      "Gradient Descent(26/49): loss=5.878509015502435e+55\n",
      "Gradient Descent(27/49): loss=9.456428224187963e+57\n",
      "Gradient Descent(28/49): loss=1.5212026471916219e+60\n",
      "Gradient Descent(29/49): loss=2.447073502766986e+62\n",
      "Gradient Descent(30/49): loss=3.936470094237232e+64\n",
      "Gradient Descent(31/49): loss=6.3323789764804236e+66\n",
      "Gradient Descent(32/49): loss=1.018654341118303e+69\n",
      "Gradient Descent(33/49): loss=1.638652188274281e+71\n",
      "Gradient Descent(34/49): loss=2.6360080016821253e+73\n",
      "Gradient Descent(35/49): loss=4.240398441264046e+75\n",
      "Gradient Descent(36/49): loss=6.821291486672389e+77\n",
      "Gradient Descent(37/49): loss=1.0973029584521517e+80\n",
      "Gradient Descent(38/49): loss=1.765169814221234e+82\n",
      "Gradient Descent(39/49): loss=2.839529820855487e+84\n",
      "Gradient Descent(40/49): loss=4.5677925934196096e+86\n",
      "Gradient Descent(41/49): loss=7.34795212335996e+88\n",
      "Gradient Descent(42/49): loss=1.1820239054849366e+91\n",
      "Gradient Descent(43/49): loss=1.9014556568708038e+93\n",
      "Gradient Descent(44/49): loss=3.058765223164134e+95\n",
      "Gradient Descent(45/49): loss=4.920464306717237e+97\n",
      "Gradient Descent(46/49): loss=7.91527535697338e+99\n",
      "Gradient Descent(47/49): loss=1.2732860167521307e+102\n",
      "Gradient Descent(48/49): loss=2.048263904082838e+104\n",
      "Gradient Descent(49/49): loss=3.2949274283795093e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.9858964603648745\n",
      "Gradient Descent(2/49): loss=670.1010027030667\n",
      "Gradient Descent(3/49): loss=104545.3575420981\n",
      "Gradient Descent(4/49): loss=16559307.21511522\n",
      "Gradient Descent(5/49): loss=2633648370.0792027\n",
      "Gradient Descent(6/49): loss=419387072798.05005\n",
      "Gradient Descent(7/49): loss=66810496619836.87\n",
      "Gradient Descent(8/49): loss=1.064462655992217e+16\n",
      "Gradient Descent(9/49): loss=1.696034088189357e+18\n",
      "Gradient Descent(10/49): loss=2.7023699339455087e+20\n",
      "Gradient Descent(11/49): loss=4.3058315241180545e+22\n",
      "Gradient Descent(12/49): loss=6.860723716105895e+24\n",
      "Gradient Descent(13/49): loss=1.0931582784439347e+27\n",
      "Gradient Descent(14/49): loss=1.7417917871137198e+29\n",
      "Gradient Descent(15/49): loss=2.7752969154040846e+31\n",
      "Gradient Descent(16/49): loss=4.422040089601157e+33\n",
      "Gradient Descent(17/49): loss=7.045890680969704e+35\n",
      "Gradient Descent(18/49): loss=1.1226622686002386e+38\n",
      "Gradient Descent(19/49): loss=1.7888023349337226e+40\n",
      "Gradient Descent(20/49): loss=2.850201599953827e+42\n",
      "Gradient Descent(21/49): loss=4.541390069951755e+44\n",
      "Gradient Descent(22/49): loss=7.236057887375659e+46\n",
      "Gradient Descent(23/49): loss=1.1529627040037148e+49\n",
      "Gradient Descent(24/49): loss=1.8370817612533113e+51\n",
      "Gradient Descent(25/49): loss=2.927127985851721e+53\n",
      "Gradient Descent(26/49): loss=4.6639613033409114e+55\n",
      "Gradient Descent(27/49): loss=7.431357680362641e+57\n",
      "Gradient Descent(28/49): loss=1.1840809428227392e+60\n",
      "Gradient Descent(29/49): loss=1.8866642401843984e+62\n",
      "Gradient Descent(30/49): loss=3.006130600079655e+64\n",
      "Gradient Descent(31/49): loss=4.7898407105294e+66\n",
      "Gradient Descent(32/49): loss=7.631928576768085e+68\n",
      "Gradient Descent(33/49): loss=1.2160390568489588e+71\n",
      "Gradient Descent(34/49): loss=1.9375849405659797e+73\n",
      "Gradient Descent(35/49): loss=3.0872654794790707e+75\n",
      "Gradient Descent(36/49): loss=4.91911757840101e+77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=7.837912842602937e+79\n",
      "Gradient Descent(38/49): loss=1.2488597141483521e+82\n",
      "Gradient Descent(39/49): loss=1.9898799807331803e+84\n",
      "Gradient Descent(40/49): loss=3.170590173471066e+86\n",
      "Gradient Descent(41/49): loss=5.051883603757565e+88\n",
      "Gradient Descent(42/49): loss=8.049456583653744e+90\n",
      "Gradient Descent(43/49): loss=1.2825661945958731e+93\n",
      "Gradient Descent(44/49): loss=2.0435864538491234e+95\n",
      "Gradient Descent(45/49): loss=3.2561637847249864e+97\n",
      "Gradient Descent(46/49): loss=5.188232958279993e+99\n",
      "Gradient Descent(47/49): loss=8.266709849073588e+101\n",
      "Gradient Descent(48/49): loss=1.3171824063857332e+104\n",
      "Gradient Descent(49/49): loss=2.09874245421416e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.977298119608566\n",
      "Gradient Descent(2/49): loss=670.0245974968406\n",
      "Gradient Descent(3/49): loss=104562.2352756342\n",
      "Gradient Descent(4/49): loss=16564932.799136076\n",
      "Gradient Descent(5/49): loss=2635269043.93554\n",
      "Gradient Descent(6/49): loss=419788889695.5853\n",
      "Gradient Descent(7/49): loss=66899590015670.79\n",
      "Gradient Descent(8/49): loss=1.0662969334341308e+16\n",
      "Gradient Descent(9/49): loss=1.6996276370913812e+18\n",
      "Gradient Descent(10/49): loss=2.7091713517988813e+20\n",
      "Gradient Descent(11/49): loss=4.318386955769471e+22\n",
      "Gradient Descent(12/49): loss=6.883470345611403e+24\n",
      "Gradient Descent(13/49): loss=1.097219735081802e+27\n",
      "Gradient Descent(14/49): loss=1.7489599636824032e+29\n",
      "Gradient Descent(15/49): loss=2.7878291635067544e+31\n",
      "Gradient Descent(16/49): loss=4.4437790441808214e+33\n",
      "Gradient Descent(17/49): loss=7.083350951437917e+35\n",
      "Gradient Descent(18/49): loss=1.1290809093749348e+38\n",
      "Gradient Descent(19/49): loss=1.7997466310398113e+40\n",
      "Gradient Descent(20/49): loss=2.868782839292113e+42\n",
      "Gradient Descent(21/49): loss=4.5728186612379965e+44\n",
      "Gradient Descent(22/49): loss=7.289039178198289e+46\n",
      "Gradient Descent(23/49): loss=1.1618674624546312e+49\n",
      "Gradient Descent(24/49): loss=1.8520081554153485e+51\n",
      "Gradient Descent(25/49): loss=2.9520873236983853e+53\n",
      "Gradient Descent(26/49): loss=4.705605394480862e+55\n",
      "Gradient Descent(27/49): loss=7.500700250570654e+57\n",
      "Gradient Descent(28/49): loss=1.1956060810985273e+60\n",
      "Gradient Descent(29/49): loss=1.9057872643971584e+62\n",
      "Gradient Descent(30/49): loss=3.0378108262892943e+64\n",
      "Gradient Descent(31/49): loss=4.8422480245923914e+66\n",
      "Gradient Descent(32/49): loss=7.71850759394064e+68\n",
      "Gradient Descent(33/49): loss=1.2303244107933572e+71\n",
      "Gradient Descent(34/49): loss=1.9611280255555372e+73\n",
      "Gradient Descent(35/49): loss=3.126023590915593e+75\n",
      "Gradient Descent(36/49): loss=4.982858520005402e+77\n",
      "Gradient Descent(37/49): loss=7.94263968530005e+79\n",
      "Gradient Descent(38/49): loss=1.266050900647192e+82\n",
      "Gradient Descent(39/49): loss=2.0180757865626557e+84\n",
      "Gradient Descent(40/49): loss=3.2167979014339557e+86\n",
      "Gradient Descent(41/49): loss=5.1275521006548175e+88\n",
      "Gradient Descent(42/49): loss=8.173280184375126e+90\n",
      "Gradient Descent(43/49): loss=1.3028148258848068e+93\n",
      "Gradient Descent(44/49): loss=2.0766772119106264e+95\n",
      "Gradient Descent(45/49): loss=3.3102081407002946e+97\n",
      "Gradient Descent(46/49): loss=5.276447332263601e+99\n",
      "Gradient Descent(47/49): loss=8.410618084052516e+101\n",
      "Gradient Descent(48/49): loss=1.3406463118327765e+104\n",
      "Gradient Descent(49/49): loss=2.1369803211475947e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.0408104942162515\n",
      "Gradient Descent(2/49): loss=692.7604744096479\n",
      "Gradient Descent(3/49): loss=110285.07937761911\n",
      "Gradient Descent(4/49): loss=17811171.42711422\n",
      "Gradient Descent(5/49): loss=2887497867.750092\n",
      "Gradient Descent(6/49): loss=468645765604.0409\n",
      "Gradient Descent(7/49): loss=76088955299190.16\n",
      "Gradient Descent(8/49): loss=1.2355134266485086e+16\n",
      "Gradient Descent(9/49): loss=2.0062682972219832e+18\n",
      "Gradient Descent(10/49): loss=3.2578839148166473e+20\n",
      "Gradient Descent(11/49): loss=5.290343001088694e+22\n",
      "Gradient Descent(12/49): loss=8.590779695342531e+24\n",
      "Gradient Descent(13/49): loss=1.3950234349534118e+27\n",
      "Gradient Descent(14/49): loss=2.2653248005971702e+29\n",
      "Gradient Descent(15/49): loss=3.6785738024966662e+31\n",
      "Gradient Descent(16/49): loss=5.973494571407012e+33\n",
      "Gradient Descent(17/49): loss=9.700128218817509e+35\n",
      "Gradient Descent(18/49): loss=1.5751665372113507e+38\n",
      "Gradient Descent(19/49): loss=2.557852396465915e+40\n",
      "Gradient Descent(20/49): loss=4.153598193260478e+42\n",
      "Gradient Descent(21/49): loss=6.744868459061047e+44\n",
      "Gradient Descent(22/49): loss=1.0952732646247283e+47\n",
      "Gradient Descent(23/49): loss=1.7785721567274454e+49\n",
      "Gradient Descent(24/49): loss=2.8881549644826637e+51\n",
      "Gradient Descent(25/49): loss=4.6899638383062524e+53\n",
      "Gradient Descent(26/49): loss=7.615852014562504e+55\n",
      "Gradient Descent(27/49): loss=1.2367089365163444e+58\n",
      "Gradient Descent(28/49): loss=2.00824410812463e+60\n",
      "Gradient Descent(29/49): loss=3.2611104187359506e+62\n",
      "Gradient Descent(30/49): loss=5.295591865631996e+64\n",
      "Gradient Descent(31/49): loss=8.599308090346021e+66\n",
      "Gradient Descent(32/49): loss=1.3964085886717988e+69\n",
      "Gradient Descent(33/49): loss=2.26757423507768e+71\n",
      "Gradient Descent(34/49): loss=3.68222664433686e+73\n",
      "Gradient Descent(35/49): loss=5.979426318450741e+75\n",
      "Gradient Descent(36/49): loss=9.709760574561383e+77\n",
      "Gradient Descent(37/49): loss=1.5767306994717489e+80\n",
      "Gradient Descent(38/49): loss=2.560392379983053e+82\n",
      "Gradient Descent(39/49): loss=4.1577227751521345e+84\n",
      "Gradient Descent(40/49): loss=6.751566209212531e+86\n",
      "Gradient Descent(41/49): loss=1.0963608865363181e+89\n",
      "Gradient Descent(42/49): loss=1.7803383041501767e+91\n",
      "Gradient Descent(43/49): loss=2.891022943401338e+93\n",
      "Gradient Descent(44/49): loss=4.69462103904039e+95\n",
      "Gradient Descent(45/49): loss=7.623414663832018e+97\n",
      "Gradient Descent(46/49): loss=1.2379370060636253e+100\n",
      "Gradient Descent(47/49): loss=2.0102383230606522e+102\n",
      "Gradient Descent(48/49): loss=3.264348747721322e+104\n",
      "Gradient Descent(49/49): loss=5.300850463603617e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.020045039597226\n",
      "Gradient Descent(2/49): loss=688.0040748674281\n",
      "Gradient Descent(3/49): loss=109516.44957334382\n",
      "Gradient Descent(4/49): loss=17707986.413244087\n",
      "Gradient Descent(5/49): loss=2875605875.426544\n",
      "Gradient Descent(6/49): loss=467587427719.4688\n",
      "Gradient Descent(7/49): loss=76063938275227.97\n",
      "Gradient Descent(8/49): loss=1.2375248916863168e+16\n",
      "Gradient Descent(9/49): loss=2.0134850309849032e+18\n",
      "Gradient Descent(10/49): loss=3.276040064194469e+20\n",
      "Gradient Descent(11/49): loss=5.330305298857509e+22\n",
      "Gradient Descent(12/49): loss=8.672726450935299e+24\n",
      "Gradient Descent(13/49): loss=1.4111053459269543e+27\n",
      "Gradient Descent(14/49): loss=2.2959546198306527e+29\n",
      "Gradient Descent(15/49): loss=3.7356586639448567e+31\n",
      "Gradient Descent(16/49): loss=6.07814535604443e+33\n",
      "Gradient Descent(17/49): loss=9.889514678953814e+35\n",
      "Gradient Descent(18/49): loss=1.609084598190249e+38\n",
      "Gradient Descent(19/49): loss=2.618079177672857e+40\n",
      "Gradient Descent(20/49): loss=4.25977514777456e+42\n",
      "Gradient Descent(21/49): loss=6.930915025652316e+44\n",
      "Gradient Descent(22/49): loss=1.127702318255598e+47\n",
      "Gradient Descent(23/49): loss=1.8348407301228927e+49\n",
      "Gradient Descent(24/49): loss=2.9853982300347657e+51\n",
      "Gradient Descent(25/49): loss=4.857425740328512e+53\n",
      "Gradient Descent(26/49): loss=7.903329138952962e+55\n",
      "Gradient Descent(27/49): loss=1.2859200493799856e+58\n",
      "Gradient Descent(28/49): loss=2.0922706676196162e+60\n",
      "Gradient Descent(29/49): loss=3.404252502861403e+62\n",
      "Gradient Descent(30/49): loss=5.538927292052024e+64\n",
      "Gradient Descent(31/49): loss=9.012173897456592e+66\n",
      "Gradient Descent(32/49): loss=1.4663358819412846e+69\n",
      "Gradient Descent(33/49): loss=2.38581827551657e+71\n",
      "Gradient Descent(34/49): loss=3.881872437202497e+73\n",
      "Gradient Descent(35/49): loss=6.31604417375405e+75\n",
      "Gradient Descent(36/49): loss=1.0276590653133733e+78\n",
      "Gradient Descent(37/49): loss=1.6720642311357484e+80\n",
      "Gradient Descent(38/49): loss=2.7205508980656336e+82\n",
      "Gradient Descent(39/49): loss=4.426502912473837e+84\n",
      "Gradient Descent(40/49): loss=7.202191309146642e+86\n",
      "Gradient Descent(41/49): loss=1.1718406308369074e+89\n",
      "Gradient Descent(42/49): loss=1.9066564676451877e+91\n",
      "Gradient Descent(43/49): loss=3.1022468328452903e+93\n",
      "Gradient Descent(44/49): loss=5.0475455726876024e+95\n",
      "Gradient Descent(45/49): loss=8.212665748776247e+97\n",
      "Gradient Descent(46/49): loss=1.3362510101163868e+100\n",
      "Gradient Descent(47/49): loss=2.1741622229093118e+102\n",
      "Gradient Descent(48/49): loss=3.537495078199598e+104\n",
      "Gradient Descent(49/49): loss=5.75572112164711e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.0395496702728035\n",
      "Gradient Descent(2/49): loss=685.5605028169164\n",
      "Gradient Descent(3/49): loss=108180.61742080488\n",
      "Gradient Descent(4/49): loss=17331172.392946735\n",
      "Gradient Descent(5/49): loss=2787965501.8407207\n",
      "Gradient Descent(6/49): loss=449044717489.7367\n",
      "Gradient Descent(7/49): loss=72354349870683.9\n",
      "Gradient Descent(8/49): loss=1.1659930067288064e+16\n",
      "Gradient Descent(9/49): loss=1.8790819961966976e+18\n",
      "Gradient Descent(10/49): loss=3.0283189934432865e+20\n",
      "Gradient Descent(11/49): loss=4.8804461851371486e+22\n",
      "Gradient Descent(12/49): loss=7.865350936337775e+24\n",
      "Gradient Descent(13/49): loss=1.2675844492229776e+27\n",
      "Gradient Descent(14/49): loss=2.042846683315488e+29\n",
      "Gradient Descent(15/49): loss=3.2922641230491407e+31\n",
      "Gradient Descent(16/49): loss=5.305832956264024e+33\n",
      "Gradient Descent(17/49): loss=8.55091283098374e+35\n",
      "Gradient Descent(18/49): loss=1.378070342482904e+38\n",
      "Gradient Descent(19/49): loss=2.220906595177733e+40\n",
      "Gradient Descent(20/49): loss=3.579226657414118e+42\n",
      "Gradient Descent(21/49): loss=5.768303581028176e+44\n",
      "Gradient Descent(22/49): loss=9.296233345489775e+46\n",
      "Gradient Descent(23/49): loss=1.4981866540230673e+49\n",
      "Gradient Descent(24/49): loss=2.414486778548733e+51\n",
      "Gradient Descent(25/49): loss=3.891201665785265e+53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=6.271084413605901e+55\n",
      "Gradient Descent(27/49): loss=1.0106518011740808e+58\n",
      "Gradient Descent(28/49): loss=1.6287726266295507e+60\n",
      "Gradient Descent(29/49): loss=2.624939931018647e+62\n",
      "Gradient Descent(30/49): loss=4.2303692540035206e+64\n",
      "Gradient Descent(31/49): loss=6.817688974038138e+66\n",
      "Gradient Descent(32/49): loss=1.0987429265833644e+69\n",
      "Gradient Descent(33/49): loss=1.770740823340798e+71\n",
      "Gradient Descent(34/49): loss=2.8537367454967996e+73\n",
      "Gradient Descent(35/49): loss=4.599099600151563e+75\n",
      "Gradient Descent(36/49): loss=7.411937056034211e+77\n",
      "Gradient Descent(37/49): loss=1.194512311079384e+80\n",
      "Gradient Descent(38/49): loss=1.9250833493770376e+82\n",
      "Gradient Descent(39/49): loss=3.10247610482971e+84\n",
      "Gradient Descent(40/49): loss=4.999969473609639e+86\n",
      "Gradient Descent(41/49): loss=8.057981396894698e+88\n",
      "Gradient Descent(42/49): loss=1.2986292123464746e+91\n",
      "Gradient Descent(43/49): loss=2.0928787845173273e+93\n",
      "Gradient Descent(44/49): loss=3.3728962547887943e+95\n",
      "Gradient Descent(45/49): loss=5.4357802418987084e+97\n",
      "Gradient Descent(46/49): loss=8.760336697657052e+99\n",
      "Gradient Descent(47/49): loss=1.4118212223662399e+102\n",
      "Gradient Descent(48/49): loss=2.2752997204511525e+104\n",
      "Gradient Descent(49/49): loss=3.6668869513155212e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.030522070774506\n",
      "Gradient Descent(2/49): loss=685.3995819072763\n",
      "Gradient Descent(3/49): loss=108178.65058871855\n",
      "Gradient Descent(4/49): loss=17332909.56650471\n",
      "Gradient Descent(5/49): loss=2788845851.2131944\n",
      "Gradient Descent(6/49): loss=449313398497.3292\n",
      "Gradient Descent(7/49): loss=72420451411725.67\n",
      "Gradient Descent(8/49): loss=1.1674423980303534e+16\n",
      "Gradient Descent(9/49): loss=1.882047979955088e+18\n",
      "Gradient Descent(10/49): loss=3.034121833819081e+20\n",
      "Gradient Descent(11/49): loss=4.891451397665942e+22\n",
      "Gradient Descent(12/49): loss=7.885755063517096e+24\n",
      "Gradient Descent(13/49): loss=1.2713030829428666e+27\n",
      "Gradient Descent(14/49): loss=2.0495334481702e+29\n",
      "Gradient Descent(15/49): loss=3.3041591089623924e+31\n",
      "Gradient Descent(16/49): loss=5.326806301804344e+33\n",
      "Gradient Descent(17/49): loss=8.58762083762576e+35\n",
      "Gradient Descent(18/49): loss=1.384454918804894e+38\n",
      "Gradient Descent(19/49): loss=2.231951620002827e+40\n",
      "Gradient Descent(20/49): loss=3.598245033476182e+42\n",
      "Gradient Descent(21/49): loss=5.800917549617044e+44\n",
      "Gradient Descent(22/49): loss=9.351960221066346e+46\n",
      "Gradient Descent(23/49): loss=1.5076780393697386e+49\n",
      "Gradient Descent(24/49): loss=2.4306060084464212e+51\n",
      "Gradient Descent(25/49): loss=3.918506082886365e+53\n",
      "Gradient Descent(26/49): loss=6.317227007695658e+55\n",
      "Gradient Descent(27/49): loss=1.0184329492572954e+58\n",
      "Gradient Descent(28/49): loss=1.6418686092322574e+60\n",
      "Gradient Descent(29/49): loss=2.6469415899674254e+62\n",
      "Gradient Descent(30/49): loss=4.267271900627601e+64\n",
      "Gradient Descent(31/49): loss=6.879490481733709e+66\n",
      "Gradient Descent(32/49): loss=1.1090783617820136e+69\n",
      "Gradient Descent(33/49): loss=1.788002782821031e+71\n",
      "Gradient Descent(34/49): loss=2.88253207486535e+73\n",
      "Gradient Descent(35/49): loss=4.64707954733602e+75\n",
      "Gradient Descent(36/49): loss=7.491798099168617e+77\n",
      "Gradient Descent(37/49): loss=1.2077916503684895e+80\n",
      "Gradient Descent(38/49): loss=1.9471435980926015e+82\n",
      "Gradient Descent(39/49): loss=3.139091241801762e+84\n",
      "Gradient Descent(40/49): loss=5.06069189453169e+86\n",
      "Gradient Descent(41/49): loss=8.158604028559174e+88\n",
      "Gradient Descent(42/49): loss=1.3152908946451757e+91\n",
      "Gradient Descent(43/49): loss=2.12044870848086e+93\n",
      "Gradient Descent(44/49): loss=3.418485403954036e+95\n",
      "Gradient Descent(45/49): loss=5.511117722540371e+97\n",
      "Gradient Descent(46/49): loss=8.884758880809604e+99\n",
      "Gradient Descent(47/49): loss=1.432358086768247e+102\n",
      "Gradient Descent(48/49): loss=2.3091788041224203e+104\n",
      "Gradient Descent(49/49): loss=3.7227469853151365e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.094777350150812\n",
      "Gradient Descent(2/49): loss=708.6454716751608\n",
      "Gradient Descent(3/49): loss=114096.62222391763\n",
      "Gradient Descent(4/49): loss=18636336.51088732\n",
      "Gradient Descent(5/49): loss=3055655319.2102747\n",
      "Gradient Descent(6/49): loss=501583287385.9277\n",
      "Gradient Descent(7/49): loss=82363739393566.84\n",
      "Gradient Descent(8/49): loss=1.3526272587511776e+16\n",
      "Gradient Descent(9/49): loss=2.2214470586370248e+18\n",
      "Gradient Descent(10/49): loss=3.648370000633334e+20\n",
      "Gradient Descent(11/49): loss=5.991884356105592e+22\n",
      "Gradient Descent(12/49): loss=9.840756799479775e+24\n",
      "Gradient Descent(13/49): loss=1.6161949524713724e+27\n",
      "Gradient Descent(14/49): loss=2.654355259498143e+29\n",
      "Gradient Descent(15/49): loss=4.359376399882381e+31\n",
      "Gradient Descent(16/49): loss=7.159615420911297e+33\n",
      "Gradient Descent(17/49): loss=1.175858391052694e+36\n",
      "Gradient Descent(18/49): loss=1.9311693107016617e+38\n",
      "Gradient Descent(19/49): loss=3.1716530975562295e+40\n",
      "Gradient Descent(20/49): loss=5.208959834310074e+42\n",
      "Gradient Descent(21/49): loss=8.554927579448715e+44\n",
      "Gradient Descent(22/49): loss=1.4050172821195273e+47\n",
      "Gradient Descent(23/49): loss=2.3075280821856225e+49\n",
      "Gradient Descent(24/49): loss=3.789765377156908e+51\n",
      "Gradient Descent(25/49): loss=6.22411563472762e+53\n",
      "Gradient Descent(26/49): loss=1.0222167226491968e+56\n",
      "Gradient Descent(27/49): loss=1.6788361421717778e+58\n",
      "Gradient Descent(28/49): loss=2.7572340872665677e+60\n",
      "Gradient Descent(29/49): loss=4.528339378106425e+62\n",
      "Gradient Descent(30/49): loss=7.437111566989976e+64\n",
      "Gradient Descent(31/49): loss=1.2214329325066011e+67\n",
      "Gradient Descent(32/49): loss=2.006018593607688e+69\n",
      "Gradient Descent(33/49): loss=3.294581708748886e+71\n",
      "Gradient Descent(34/49): loss=5.4108514598072907e+73\n",
      "Gradient Descent(35/49): loss=8.88650399604647e+75\n",
      "Gradient Descent(36/49): loss=1.4594736865048348e+78\n",
      "Gradient Descent(37/49): loss=2.3969644784356867e+80\n",
      "Gradient Descent(38/49): loss=3.9366511119784e+82\n",
      "Gradient Descent(39/49): loss=6.465353206883794e+84\n",
      "Gradient Descent(40/49): loss=1.0618363400955595e+87\n",
      "Gradient Descent(41/49): loss=1.7439053630466164e+89\n",
      "Gradient Descent(42/49): loss=2.864100427179839e+91\n",
      "Gradient Descent(43/49): loss=4.703851155455399e+93\n",
      "Gradient Descent(44/49): loss=7.725363078300246e+95\n",
      "Gradient Descent(45/49): loss=1.2687738773865845e+98\n",
      "Gradient Descent(46/49): loss=2.083768925321203e+100\n",
      "Gradient Descent(47/49): loss=3.422274852535671e+102\n",
      "Gradient Descent(48/49): loss=5.6205681080941626e+104\n",
      "Gradient Descent(49/49): loss=9.230931827208082e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.073736882856181\n",
      "Gradient Descent(2/49): loss=703.780980309872\n",
      "Gradient Descent(3/49): loss=113301.37540634928\n",
      "Gradient Descent(4/49): loss=18528329.14474611\n",
      "Gradient Descent(5/49): loss=3043059816.8921924\n",
      "Gradient Descent(6/49): loss=500448234822.7039\n",
      "Gradient Descent(7/49): loss=82336200014815.5\n",
      "Gradient Descent(8/49): loss=1.35482067455123e+16\n",
      "Gradient Descent(9/49): loss=2.2294216251790748e+18\n",
      "Gradient Descent(10/49): loss=3.668672698047408e+20\n",
      "Gradient Descent(11/49): loss=6.037092321967394e+22\n",
      "Gradient Descent(12/49): loss=9.934530663339493e+24\n",
      "Gradient Descent(13/49): loss=1.6348093691059374e+27\n",
      "Gradient Descent(14/49): loss=2.6902147889617937e+29\n",
      "Gradient Descent(15/49): loss=4.426972438141056e+31\n",
      "Gradient Descent(16/49): loss=7.284951897468762e+33\n",
      "Gradient Descent(17/49): loss=1.1987995230088202e+36\n",
      "Gradient Descent(18/49): loss=1.9727244866909935e+38\n",
      "Gradient Descent(19/49): loss=3.2462824919984787e+40\n",
      "Gradient Descent(20/49): loss=5.3420282919596307e+42\n",
      "Gradient Descent(21/49): loss=8.790752605318817e+44\n",
      "Gradient Descent(22/49): loss=1.4465915780724635e+47\n",
      "Gradient Descent(23/49): loss=2.3804869591031786e+49\n",
      "Gradient Descent(24/49): loss=3.9172896126116097e+51\n",
      "Gradient Descent(25/49): loss=6.446226411955989e+53\n",
      "Gradient Descent(26/49): loss=1.060780260423659e+56\n",
      "Gradient Descent(27/49): loss=1.745602293486843e+58\n",
      "Gradient Descent(28/49): loss=2.8725340022915364e+60\n",
      "Gradient Descent(29/49): loss=4.726994015251207e+62\n",
      "Gradient Descent(30/49): loss=7.778662464011116e+64\n",
      "Gradient Descent(31/49): loss=1.2800437134845804e+67\n",
      "Gradient Descent(32/49): loss=2.1064185726172874e+69\n",
      "Gradient Descent(33/49): loss=3.466287249666288e+71\n",
      "Gradient Descent(34/49): loss=5.704064450148583e+73\n",
      "Gradient Descent(35/49): loss=9.386513265622003e+75\n",
      "Gradient Descent(36/49): loss=1.544628958100267e+78\n",
      "Gradient Descent(37/49): loss=2.541815635567436e+80\n",
      "Gradient Descent(38/49): loss=4.182769390236757e+82\n",
      "Gradient Descent(39/49): loss=6.88309550350052e+84\n",
      "Gradient Descent(40/49): loss=1.1326707090497156e+87\n",
      "Gradient Descent(41/49): loss=1.8639040159862878e+89\n",
      "Gradient Descent(42/49): loss=3.0672093425321822e+91\n",
      "Gradient Descent(43/49): loss=5.047348506268729e+93\n",
      "Gradient Descent(44/49): loss=8.305832468122771e+95\n",
      "Gradient Descent(45/49): loss=1.3667939295818754e+98\n",
      "Gradient Descent(46/49): loss=2.2491732804768168e+100\n",
      "Gradient Descent(47/49): loss=3.7012020145263594e+102\n",
      "Gradient Descent(48/49): loss=6.090636266775251e+104\n",
      "Gradient Descent(49/49): loss=1.0022649395673498e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.093517809738642\n",
      "Gradient Descent(2/49): loss=701.2861739102044\n",
      "Gradient Descent(3/49): loss=111920.61443798292\n",
      "Gradient Descent(4/49): loss=18134337.316236295\n",
      "Gradient Descent(5/49): loss=2950373414.9847503\n",
      "Gradient Descent(6/49): loss=480613686767.88306\n",
      "Gradient Descent(7/49): loss=78322868195550.25\n",
      "Gradient Descent(8/49): loss=1.276548892375602e+16\n",
      "Gradient Descent(9/49): loss=2.0806780099566958e+18\n",
      "Gradient Descent(10/49): loss=3.391395430142894e+20\n",
      "Gradient Descent(11/49): loss=5.527821527234709e+22\n",
      "Gradient Descent(12/49): loss=9.010113612932773e+24\n",
      "Gradient Descent(13/49): loss=1.4686108906490253e+27\n",
      "Gradient Descent(14/49): loss=2.393774826358455e+29\n",
      "Gradient Descent(15/49): loss=3.901753881074785e+31\n",
      "Gradient Descent(16/49): loss=6.35969743913263e+33\n",
      "Gradient Descent(17/49): loss=1.0366043835863343e+36\n",
      "Gradient Descent(18/49): loss=1.689622282447603e+38\n",
      "Gradient Descent(19/49): loss=2.7540144576771852e+40\n",
      "Gradient Descent(20/49): loss=4.488929693695173e+42\n",
      "Gradient Descent(21/49): loss=7.316769794100097e+44\n",
      "Gradient Descent(22/49): loss=1.1926032233558092e+47\n",
      "Gradient Descent(23/49): loss=1.9438939428123755e+49\n",
      "Gradient Descent(24/49): loss=3.16846675148183e+51\n",
      "Gradient Descent(25/49): loss=5.16446979650057e+53\n",
      "Gradient Descent(26/49): loss=8.417872229999192e+55\n",
      "Gradient Descent(27/49): loss=1.3720783676305717e+58\n",
      "Gradient Descent(28/49): loss=2.2364310071263833e+60\n",
      "Gradient Descent(29/49): loss=3.64529007062013e+62\n",
      "Gradient Descent(30/49): loss=5.941672091210984e+64\n",
      "Gradient Descent(31/49): loss=9.684679834949281e+66\n",
      "Gradient Descent(32/49): loss=1.5785627692954235e+69\n",
      "Gradient Descent(33/49): loss=2.5729920442111156e+71\n",
      "Gradient Descent(34/49): loss=4.193870645085995e+73\n",
      "Gradient Descent(35/49): loss=6.83583574511466e+75\n",
      "Gradient Descent(36/49): loss=1.1142129619314725e+78\n",
      "Gradient Descent(37/49): loss=1.816121057945752e+80\n",
      "Gradient Descent(38/49): loss=2.9602022322523133e+82\n",
      "Gradient Descent(39/49): loss=4.82500724139133e+84\n",
      "Gradient Descent(40/49): loss=7.864562301125415e+86\n",
      "Gradient Descent(41/49): loss=1.28189113702651e+89\n",
      "Gradient Descent(42/49): loss=2.089429550264968e+91\n",
      "Gradient Descent(43/49): loss=3.4056837740895972e+93\n",
      "Gradient Descent(44/49): loss=5.551123735005225e+95\n",
      "Gradient Descent(45/49): loss=9.048102162560835e+97\n",
      "Gradient Descent(46/49): loss=1.4748032407903315e+100\n",
      "Gradient Descent(47/49): loss=2.4038683029525603e+102\n",
      "Gradient Descent(48/49): loss=3.91820593969088e+104\n",
      "Gradient Descent(49/49): loss=6.38651367338729e+106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.084056589733328\n",
      "Gradient Descent(2/49): loss=701.0378721349796\n",
      "Gradient Descent(3/49): loss=111898.70635356635\n",
      "Gradient Descent(4/49): loss=18131861.954587847\n",
      "Gradient Descent(5/49): loss=2950431318.8224607\n",
      "Gradient Descent(6/49): loss=480730356771.22626\n",
      "Gradient Descent(7/49): loss=78361915298372.77\n",
      "Gradient Descent(8/49): loss=1.2775299768140514e+16\n",
      "Gradient Descent(9/49): loss=2.0828511123482145e+18\n",
      "Gradient Descent(10/49): loss=3.395881118877563e+20\n",
      "Gradient Descent(11/49): loss=5.536676429389712e+22\n",
      "Gradient Descent(12/49): loss=9.02706619076562e+24\n",
      "Gradient Descent(13/49): loss=1.4717850521712597e+27\n",
      "Gradient Descent(14/49): loss=2.399618728187635e+29\n",
      "Gradient Descent(15/49): loss=3.9123718884207387e+31\n",
      "Gradient Descent(16/49): loss=6.378785927588711e+33\n",
      "Gradient Descent(17/49): loss=1.0400062012139682e+36\n",
      "Gradient Descent(18/49): loss=1.6956406970618955e+38\n",
      "Gradient Descent(19/49): loss=2.7645963773816155e+40\n",
      "Gradient Descent(20/49): loss=4.5074367143923174e+42\n",
      "Gradient Descent(21/49): loss=7.348988048580607e+44\n",
      "Gradient Descent(22/49): loss=1.1981893205019333e+47\n",
      "Gradient Descent(23/49): loss=1.953544676209761e+49\n",
      "Gradient Descent(24/49): loss=3.185086644208708e+51\n",
      "Gradient Descent(25/49): loss=5.19300994478122e+53\n",
      "Gradient Descent(26/49): loss=8.466756260977606e+55\n",
      "Gradient Descent(27/49): loss=1.3804318178681244e+58\n",
      "Gradient Descent(28/49): loss=2.250675400407457e+60\n",
      "Gradient Descent(29/49): loss=3.669532745066899e+62\n",
      "Gradient Descent(30/49): loss=5.982857663384305e+64\n",
      "Gradient Descent(31/49): loss=9.75453506129268e+66\n",
      "Gradient Descent(32/49): loss=1.590393079954444e+69\n",
      "Gradient Descent(33/49): loss=2.5929991874279806e+71\n",
      "Gradient Descent(34/49): loss=4.227662249508037e+73\n",
      "Gradient Descent(35/49): loss=6.892839836808392e+75\n",
      "Gradient Descent(36/49): loss=1.1238182761979496e+78\n",
      "Gradient Descent(37/49): loss=1.832289082320123e+80\n",
      "Gradient Descent(38/49): loss=2.9873898229772048e+82\n",
      "Gradient Descent(39/49): loss=4.870682274178701e+84\n",
      "Gradient Descent(40/49): loss=7.941228705250098e+86\n",
      "Gradient Descent(41/49): loss=1.2947490679777812e+89\n",
      "Gradient Descent(42/49): loss=2.1109770430375058e+91\n",
      "Gradient Descent(43/49): loss=3.4417665835368156e+93\n",
      "Gradient Descent(44/49): loss=5.61150451854547e+95\n",
      "Gradient Descent(45/49): loss=9.149075684643748e+97\n",
      "Gradient Descent(46/49): loss=1.4916781338534343e+100\n",
      "Gradient Descent(47/49): loss=2.432052954541819e+102\n",
      "Gradient Descent(48/49): loss=3.9652532536732774e+104\n",
      "Gradient Descent(49/49): loss=6.465004528953003e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.1490590527171385\n",
      "Gradient Descent(2/49): loss=724.802304297111\n",
      "Gradient Descent(3/49): loss=118017.31751204611\n",
      "Gradient Descent(4/49): loss=19494760.20624454\n",
      "Gradient Descent(5/49): loss=3232575627.281766\n",
      "Gradient Descent(6/49): loss=536630547995.03986\n",
      "Gradient Descent(7/49): loss=89116227950162.16\n",
      "Gradient Descent(8/49): loss=1.4800876567563084e+16\n",
      "Gradient Descent(9/49): loss=2.458294390121045e+18\n",
      "Gradient Descent(10/49): loss=4.083056988399707e+20\n",
      "Gradient Descent(11/49): loss=6.781701022817218e+22\n",
      "Gradient Descent(12/49): loss=1.1263992948409976e+25\n",
      "Gradient Descent(13/49): loss=1.8708814645246316e+27\n",
      "Gradient Descent(14/49): loss=3.107421958465614e+29\n",
      "Gradient Descent(15/49): loss=5.161241805623802e+31\n",
      "Gradient Descent(16/49): loss=8.572513707229248e+33\n",
      "Gradient Descent(17/49): loss=1.4238432212000423e+36\n",
      "Gradient Descent(18/49): loss=2.3649183782454922e+38\n",
      "Gradient Descent(19/49): loss=3.9279878957352463e+40\n",
      "Gradient Descent(20/49): loss=6.524152822031442e+42\n",
      "Gradient Descent(21/49): loss=1.0836227396080445e+45\n",
      "Gradient Descent(22/49): loss=1.7998325205509494e+47\n",
      "Gradient Descent(23/49): loss=2.9894141047839005e+49\n",
      "Gradient Descent(24/49): loss=4.965237925115189e+51\n",
      "Gradient Descent(25/49): loss=8.246963046559005e+53\n",
      "Gradient Descent(26/49): loss=1.3697712076857453e+56\n",
      "Gradient Descent(27/49): loss=2.275108001348246e+58\n",
      "Gradient Descent(28/49): loss=3.778818235305179e+60\n",
      "Gradient Descent(29/49): loss=6.276390943644383e+62\n",
      "Gradient Descent(30/49): loss=1.0424709743754097e+65\n",
      "Gradient Descent(31/49): loss=1.731481901259952e+67\n",
      "Gradient Descent(32/49): loss=2.875887816624376e+69\n",
      "Gradient Descent(33/49): loss=4.776677554521455e+71\n",
      "Gradient Descent(34/49): loss=7.933775555491056e+73\n",
      "Gradient Descent(35/49): loss=1.3177526397050873e+76\n",
      "Gradient Descent(36/49): loss=2.1887082730086794e+78\n",
      "Gradient Descent(37/49): loss=3.635313457166568e+80\n",
      "Gradient Descent(38/49): loss=6.03803809526873e+82\n",
      "Gradient Descent(39/49): loss=1.0028819926943125e+85\n",
      "Gradient Descent(40/49): loss=1.665726971909331e+87\n",
      "Gradient Descent(41/49): loss=2.7666728141084096e+89\n",
      "Gradient Descent(42/49): loss=4.5952779713668335e+91\n",
      "Gradient Descent(43/49): loss=7.632481703816655e+93\n",
      "Gradient Descent(44/49): loss=1.267709534049547e+96\n",
      "Gradient Descent(45/49): loss=2.1055896693685874e+98\n",
      "Gradient Descent(46/49): loss=3.4972584307932416e+100\n",
      "Gradient Descent(47/49): loss=5.808736958432223e+102\n",
      "Gradient Descent(48/49): loss=9.647964461294676e+104\n",
      "Gradient Descent(49/49): loss=1.6024691583130022e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.127741837450468\n",
      "Gradient Descent(2/49): loss=719.8278900891839\n",
      "Gradient Descent(3/49): loss=117194.69022852229\n",
      "Gradient Descent(4/49): loss=19381734.474643126\n",
      "Gradient Descent(5/49): loss=3219239389.7412314\n",
      "Gradient Descent(6/49): loss=535413718751.3033\n",
      "Gradient Descent(7/49): loss=89085939450498.55\n",
      "Gradient Descent(8/49): loss=1.482478332735212e+16\n",
      "Gradient Descent(9/49): loss=2.4671014833256596e+18\n",
      "Gradient Descent(10/49): loss=4.105745877790342e+20\n",
      "Gradient Descent(11/49): loss=6.832808011342219e+22\n",
      "Gradient Descent(12/49): loss=1.137122000160248e+25\n",
      "Gradient Descent(13/49): loss=1.8924095607607212e+27\n",
      "Gradient Descent(14/49): loss=3.1493670536551123e+29\n",
      "Gradient Descent(15/49): loss=5.241208672474629e+31\n",
      "Gradient Descent(16/49): loss=8.722472926713527e+33\n",
      "Gradient Descent(17/49): loss=1.4516028492598227e+36\n",
      "Gradient Descent(18/49): loss=2.4157722860831036e+38\n",
      "Gradient Descent(19/49): loss=4.020352912005824e+40\n",
      "Gradient Descent(20/49): loss=6.690712379529614e+42\n",
      "Gradient Descent(21/49): loss=1.1134751880896553e+45\n",
      "Gradient Descent(22/49): loss=1.8530567810896418e+47\n",
      "Gradient Descent(23/49): loss=3.083876022294194e+49\n",
      "Gradient Descent(24/49): loss=5.132217975162803e+51\n",
      "Gradient Descent(25/49): loss=8.541089574999084e+53\n",
      "Gradient Descent(26/49): loss=1.4214168509836873e+56\n",
      "Gradient Descent(27/49): loss=2.365536441831307e+58\n",
      "Gradient Descent(28/49): loss=3.936749908205705e+60\n",
      "Gradient Descent(29/49): loss=6.551579407400622e+62\n",
      "Gradient Descent(30/49): loss=1.0903205367962923e+65\n",
      "Gradient Descent(31/49): loss=1.8145225739260684e+67\n",
      "Gradient Descent(32/49): loss=3.0197470011540572e+69\n",
      "Gradient Descent(33/49): loss=5.025493803170757e+71\n",
      "Gradient Descent(34/49): loss=8.363478117887286e+73\n",
      "Gradient Descent(35/49): loss=1.3918585708780944e+76\n",
      "Gradient Descent(36/49): loss=2.3163452501699017e+78\n",
      "Gradient Descent(37/49): loss=3.854885424601515e+80\n",
      "Gradient Descent(38/49): loss=6.415339697618576e+82\n",
      "Gradient Descent(39/49): loss=1.0676473851384352e+85\n",
      "Gradient Descent(40/49): loss=1.7767896833523285e+87\n",
      "Gradient Descent(41/49): loss=2.956951539255566e+89\n",
      "Gradient Descent(42/49): loss=4.92098895408324e+91\n",
      "Gradient Descent(43/49): loss=8.1895600806179675e+93\n",
      "Gradient Descent(44/49): loss=1.3629149534749633e+96\n",
      "Gradient Descent(45/49): loss=2.2681769864560147e+98\n",
      "Gradient Descent(46/49): loss=3.774723308135743e+100\n",
      "Gradient Descent(47/49): loss=6.281933084616277e+102\n",
      "Gradient Descent(48/49): loss=1.0454457203403952e+105\n",
      "Gradient Descent(49/49): loss=1.7398414460264984e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.147800878762387\n",
      "Gradient Descent(2/49): loss=717.2810545843482\n",
      "Gradient Descent(3/49): loss=115767.74900371488\n",
      "Gradient Descent(4/49): loss=18969883.667904105\n",
      "Gradient Descent(5/49): loss=3121247001.610853\n",
      "Gradient Descent(6/49): loss=514205312353.82086\n",
      "Gradient Descent(7/49): loss=84745904491881.5\n",
      "Gradient Descent(8/49): loss=1.3968747740135068e+16\n",
      "Gradient Descent(9/49): loss=2.302580654698695e+18\n",
      "Gradient Descent(10/49): loss=3.7955820471821795e+20\n",
      "Gradient Descent(11/49): loss=6.256680161985079e+22\n",
      "Gradient Descent(12/49): loss=1.0313598041344122e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=1.7001086311254616e+27\n",
      "Gradient Descent(14/49): loss=2.8024844878051702e+29\n",
      "Gradient Descent(15/49): loss=4.619657595177207e+31\n",
      "Gradient Descent(16/49): loss=7.615113229563275e+33\n",
      "Gradient Descent(17/49): loss=1.2552867537636649e+36\n",
      "Gradient Descent(18/49): loss=2.069233627839291e+38\n",
      "Gradient Descent(19/49): loss=3.410955940296835e+40\n",
      "Gradient Descent(20/49): loss=5.622671250249334e+42\n",
      "Gradient Descent(21/49): loss=9.268496147159958e+44\n",
      "Gradient Descent(22/49): loss=1.527832893098054e+47\n",
      "Gradient Descent(23/49): loss=2.518502799364158e+49\n",
      "Gradient Descent(24/49): loss=4.1515380242670994e+51\n",
      "Gradient Descent(25/49): loss=6.84345793512832e+53\n",
      "Gradient Descent(26/49): loss=1.1280859343243627e+56\n",
      "Gradient Descent(27/49): loss=1.859553879462456e+58\n",
      "Gradient Descent(28/49): loss=3.0653166797040393e+60\n",
      "Gradient Descent(29/49): loss=5.052914277260987e+62\n",
      "Gradient Descent(30/49): loss=8.329300154336143e+64\n",
      "Gradient Descent(31/49): loss=1.3730144082046697e+67\n",
      "Gradient Descent(32/49): loss=2.2632976723215097e+69\n",
      "Gradient Descent(33/49): loss=3.7308540412435218e+71\n",
      "Gradient Descent(34/49): loss=6.14999610845965e+73\n",
      "Gradient Descent(35/49): loss=1.0137746402285538e+76\n",
      "Gradient Descent(36/49): loss=1.671121417063693e+78\n",
      "Gradient Descent(37/49): loss=2.75470176482158e+80\n",
      "Gradient Descent(38/49): loss=4.5408919636998186e+82\n",
      "Gradient Descent(39/49): loss=7.485274845107997e+84\n",
      "Gradient Descent(40/49): loss=1.2338840024098512e+87\n",
      "Gradient Descent(41/49): loss=2.0339530116225013e+89\n",
      "Gradient Descent(42/49): loss=3.3527988412269635e+91\n",
      "Gradient Descent(43/49): loss=5.526804211059708e+93\n",
      "Gradient Descent(44/49): loss=9.110467473261624e+95\n",
      "Gradient Descent(45/49): loss=1.5017832080113337e+98\n",
      "Gradient Descent(46/49): loss=2.4755621053300032e+100\n",
      "Gradient Descent(47/49): loss=4.0807539361564383e+102\n",
      "Gradient Descent(48/49): loss=6.726776376000696e+104\n",
      "Gradient Descent(49/49): loss=1.1088519700195524e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.13790167648504\n",
      "Gradient Descent(2/49): loss=716.9424581282645\n",
      "Gradient Descent(3/49): loss=115724.76529213907\n",
      "Gradient Descent(4/49): loss=18962854.756085243\n",
      "Gradient Descent(5/49): loss=3120394549.3190722\n",
      "Gradient Descent(6/49): loss=514149403788.0859\n",
      "Gradient Descent(7/49): loss=84753389007211.2\n",
      "Gradient Descent(8/49): loss=1.3972932367584226e+16\n",
      "Gradient Descent(9/49): loss=2.3037703501265047e+18\n",
      "Gradient Descent(10/49): loss=3.7983761107997943e+20\n",
      "Gradient Descent(11/49): loss=6.2626648656358115e+22\n",
      "Gradient Descent(12/49): loss=1.0325740356579827e+25\n",
      "Gradient Descent(13/49): loss=1.7024858381398498e+27\n",
      "Gradient Descent(14/49): loss=2.8070226066943934e+29\n",
      "Gradient Descent(15/49): loss=4.628159789163694e+31\n",
      "Gradient Descent(16/49): loss=7.63081263437226e+33\n",
      "Gradient Descent(17/49): loss=1.2581523674640872e+36\n",
      "Gradient Descent(18/49): loss=2.0744152165474863e+38\n",
      "Gradient Descent(19/49): loss=3.4202522732155127e+40\n",
      "Gradient Descent(20/49): loss=5.639240168998232e+42\n",
      "Gradient Descent(21/49): loss=9.297860843856537e+44\n",
      "Gradient Descent(22/49): loss=1.5330117832239441e+47\n",
      "Gradient Descent(23/49): loss=2.5275976560639765e+49\n",
      "Gradient Descent(24/49): loss=4.167449970627598e+51\n",
      "Gradient Descent(25/49): loss=6.871204052608312e+53\n",
      "Gradient Descent(26/49): loss=1.1329097041444444e+56\n",
      "Gradient Descent(27/49): loss=1.8679177447184995e+58\n",
      "Gradient Descent(28/49): loss=3.0797835769880554e+60\n",
      "Gradient Descent(29/49): loss=5.077882528770022e+62\n",
      "Gradient Descent(30/49): loss=8.372306147955041e+64\n",
      "Gradient Descent(31/49): loss=1.3804082673819642e+67\n",
      "Gradient Descent(32/49): loss=2.275988181729241e+69\n",
      "Gradient Descent(33/49): loss=3.7526015496818326e+71\n",
      "Gradient Descent(34/49): loss=6.187210682251998e+73\n",
      "Gradient Descent(35/49): loss=1.0201343126828706e+76\n",
      "Gradient Descent(36/49): loss=1.6819760460042474e+78\n",
      "Gradient Descent(37/49): loss=2.7732068063586e+80\n",
      "Gradient Descent(38/49): loss=4.5724051832390034e+82\n",
      "Gradient Descent(39/49): loss=7.538885708694377e+84\n",
      "Gradient Descent(40/49): loss=1.2429956543898344e+87\n",
      "Gradient Descent(41/49): loss=2.0494251491969953e+89\n",
      "Gradient Descent(42/49): loss=3.3790491763407935e+91\n",
      "Gradient Descent(43/49): loss=5.571305368533798e+93\n",
      "Gradient Descent(44/49): loss=9.185851371084912e+95\n",
      "Gradient Descent(45/49): loss=1.514543896448966e+98\n",
      "Gradient Descent(46/49): loss=2.4971481919371654e+100\n",
      "Gradient Descent(47/49): loss=4.117245533203477e+102\n",
      "Gradient Descent(48/49): loss=6.788428029789299e+104\n",
      "Gradient Descent(49/49): loss=1.1192617672177989e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.203655601915229\n",
      "Gradient Descent(2/49): loss=741.2340567774072\n",
      "Gradient Descent(3/49): loss=122049.65180547349\n",
      "Gradient Descent(4/49): loss=20387584.97542083\n",
      "Gradient Descent(5/49): loss=3418662372.104304\n",
      "Gradient Descent(6/49): loss=573909690370.3812\n",
      "Gradient Descent(7/49): loss=96379797431709.92\n",
      "Gradient Descent(8/49): loss=1.618742807982141e+16\n",
      "Gradient Descent(9/49): loss=2.71885198740948e+18\n",
      "Gradient Descent(10/49): loss=4.5666570654844296e+20\n",
      "Gradient Descent(11/49): loss=7.670309494091015e+22\n",
      "Gradient Descent(12/49): loss=1.2883323488211037e+25\n",
      "Gradient Descent(13/49): loss=2.1639295050749257e+27\n",
      "Gradient Descent(14/49): loss=3.634614555212394e+29\n",
      "Gradient Descent(15/49): loss=6.104830811452701e+31\n",
      "Gradient Descent(16/49): loss=1.0253896024938939e+34\n",
      "Gradient Descent(17/49): loss=1.7222817043937887e+36\n",
      "Gradient Descent(18/49): loss=2.8928070523349797e+38\n",
      "Gradient Descent(19/49): loss=4.858864043275094e+40\n",
      "Gradient Descent(20/49): loss=8.161124944531778e+42\n",
      "Gradient Descent(21/49): loss=1.3707722580884876e+45\n",
      "Gradient Descent(22/49): loss=2.30239899075821e+47\n",
      "Gradient Descent(23/49): loss=3.8671931689536757e+49\n",
      "Gradient Descent(24/49): loss=6.49547844055534e+51\n",
      "Gradient Descent(25/49): loss=1.0910042071458396e+54\n",
      "Gradient Descent(26/49): loss=1.8324903868180025e+56\n",
      "Gradient Descent(27/49): loss=3.077917569690649e+58\n",
      "Gradient Descent(28/49): loss=5.169782408660132e+60\n",
      "Gradient Descent(29/49): loss=8.683354751302869e+62\n",
      "Gradient Descent(30/49): loss=1.4584878777618843e+65\n",
      "Gradient Descent(31/49): loss=2.4497293390658718e+67\n",
      "Gradient Descent(32/49): loss=4.114654585877811e+69\n",
      "Gradient Descent(33/49): loss=6.911123645823309e+71\n",
      "Gradient Descent(34/49): loss=1.1608174890740788e+74\n",
      "Gradient Descent(35/49): loss=1.949751316856558e+76\n",
      "Gradient Descent(36/49): loss=3.274873296935033e+78\n",
      "Gradient Descent(37/49): loss=5.500596418762227e+80\n",
      "Gradient Descent(38/49): loss=9.23900200670882e+82\n",
      "Gradient Descent(39/49): loss=1.5518164137404057e+85\n",
      "Gradient Descent(40/49): loss=2.606487345933587e+87\n",
      "Gradient Descent(41/49): loss=4.377951041345516e+89\n",
      "Gradient Descent(42/49): loss=7.353365958334742e+91\n",
      "Gradient Descent(43/49): loss=1.2350981179674824e+94\n",
      "Gradient Descent(44/49): loss=2.0745157655015846e+96\n",
      "Gradient Descent(45/49): loss=3.4844322072134806e+98\n",
      "Gradient Descent(46/49): loss=5.852579193935921e+100\n",
      "Gradient Descent(47/49): loss=9.830205090626138e+102\n",
      "Gradient Descent(48/49): loss=1.651117036124804e+105\n",
      "Gradient Descent(49/49): loss=2.773276286556008e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.182059903380089\n",
      "Gradient Descent(2/49): loss=736.1478681410773\n",
      "Gradient Descent(3/49): loss=121198.86319038522\n",
      "Gradient Descent(4/49): loss=20269338.07694125\n",
      "Gradient Descent(5/49): loss=3404546433.985304\n",
      "Gradient Descent(6/49): loss=572605720867.3423\n",
      "Gradient Descent(7/49): loss=96346514790074.6\n",
      "Gradient Descent(8/49): loss=1.6213472462358192e+16\n",
      "Gradient Descent(9/49): loss=2.7285731873579494e+18\n",
      "Gradient Descent(10/49): loss=4.591996985555485e+20\n",
      "Gradient Descent(11/49): loss=7.728045901141071e+22\n",
      "Gradient Descent(12/49): loss=1.3005842388603585e+25\n",
      "Gradient Descent(13/49): loss=2.1888072037983843e+27\n",
      "Gradient Descent(14/49): loss=3.68363514177484e+29\n",
      "Gradient Descent(15/49): loss=6.199343921223161e+31\n",
      "Gradient Descent(16/49): loss=1.043313582867652e+34\n",
      "Gradient Descent(17/49): loss=1.7558361865071838e+36\n",
      "Gradient Descent(18/49): loss=2.9549703663148077e+38\n",
      "Gradient Descent(19/49): loss=4.973043578197085e+40\n",
      "Gradient Descent(20/49): loss=8.369343638113278e+42\n",
      "Gradient Descent(21/49): loss=1.4085119471914283e+45\n",
      "Gradient Descent(22/49): loss=2.3704438379064273e+47\n",
      "Gradient Descent(23/49): loss=3.9893193663833587e+49\n",
      "Gradient Descent(24/49): loss=6.713792899265072e+51\n",
      "Gradient Descent(25/49): loss=1.1298923689612974e+54\n",
      "Gradient Descent(26/49): loss=1.9015432626423264e+56\n",
      "Gradient Descent(27/49): loss=3.200186919595529e+58\n",
      "Gradient Descent(28/49): loss=5.385728803308841e+60\n",
      "Gradient Descent(29/49): loss=9.063868915024798e+62\n",
      "Gradient Descent(30/49): loss=1.525396519376907e+65\n",
      "Gradient Descent(31/49): loss=2.5671537873524316e+67\n",
      "Gradient Descent(32/49): loss=4.320370791595828e+69\n",
      "Gradient Descent(33/49): loss=7.270933229179353e+71\n",
      "Gradient Descent(34/49): loss=1.2236558520861856e+74\n",
      "Gradient Descent(35/49): loss=2.0593417614340533e+76\n",
      "Gradient Descent(36/49): loss=3.465752632291295e+78\n",
      "Gradient Descent(37/49): loss=5.832660480730317e+80\n",
      "Gradient Descent(38/49): loss=9.816028982132474e+82\n",
      "Gradient Descent(39/49): loss=1.651980692111193e+85\n",
      "Gradient Descent(40/49): loss=2.7801876013973353e+87\n",
      "Gradient Descent(41/49): loss=4.678894333253616e+89\n",
      "Gradient Descent(42/49): loss=7.874307536207215e+91\n",
      "Gradient Descent(43/49): loss=1.3252002451539248e+94\n",
      "Gradient Descent(44/49): loss=2.230235079949526e+96\n",
      "Gradient Descent(45/49): loss=3.7533561663805273e+98\n",
      "Gradient Descent(46/49): loss=6.316680532182023e+100\n",
      "Gradient Descent(47/49): loss=1.0630606629619378e+103\n",
      "Gradient Descent(48/49): loss=1.7890693812667697e+105\n",
      "Gradient Descent(49/49): loss=3.010899906754301e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.202398877344039\n",
      "Gradient Descent(2/49): loss=733.5482006937013\n",
      "Gradient Descent(3/49): loss=119724.46255570422\n",
      "Gradient Descent(4/49): loss=19838924.104988318\n",
      "Gradient Descent(5/49): loss=3300976266.3281674\n",
      "Gradient Descent(6/49): loss=549936728797.785\n",
      "Gradient Descent(7/49): loss=91655224237489.64\n",
      "Gradient Descent(8/49): loss=1.5277718013947502e+16\n",
      "Gradient Descent(9/49): loss=2.546703847035782e+18\n",
      "Gradient Descent(10/49): loss=4.245262796877963e+20\n",
      "Gradient Descent(11/49): loss=7.0767322394208665e+22\n",
      "Gradient Descent(12/49): loss=1.179672965762293e+25\n",
      "Gradient Descent(13/49): loss=1.9664853516653886e+27\n",
      "Gradient Descent(14/49): loss=3.2780824998937564e+29\n",
      "Gradient Descent(15/49): loss=5.464482855985251e+31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=9.109158616733021e+33\n",
      "Gradient Descent(17/49): loss=1.51847436279976e+36\n",
      "Gradient Descent(18/49): loss=2.531259463489929e+38\n",
      "Gradient Descent(19/49): loss=4.2195473515554607e+40\n",
      "Gradient Descent(20/49): loss=7.033881793996342e+42\n",
      "Gradient Descent(21/49): loss=1.1725308184410002e+45\n",
      "Gradient Descent(22/49): loss=1.954580074692627e+47\n",
      "Gradient Descent(23/49): loss=3.258236976233348e+49\n",
      "Gradient Descent(24/49): loss=5.431401010773869e+51\n",
      "Gradient Descent(25/49): loss=9.05401207924602e+53\n",
      "Gradient Descent(26/49): loss=1.5092815752059266e+56\n",
      "Gradient Descent(27/49): loss=2.515935314994624e+58\n",
      "Gradient Descent(28/49): loss=4.194002373860347e+60\n",
      "Gradient Descent(29/49): loss=6.991298944418195e+62\n",
      "Gradient Descent(30/49): loss=1.1654323620526079e+65\n",
      "Gradient Descent(31/49): loss=1.942747122269651e+67\n",
      "Gradient Descent(32/49): loss=3.2385117351980948e+69\n",
      "Gradient Descent(33/49): loss=5.398519518465697e+71\n",
      "Gradient Descent(34/49): loss=8.999199439205513e+73\n",
      "Gradient Descent(35/49): loss=1.5001444427418363e+76\n",
      "Gradient Descent(36/49): loss=2.5007039396027738e+78\n",
      "Gradient Descent(37/49): loss=4.168612045194224e+80\n",
      "Gradient Descent(38/49): loss=6.948973890167359e+82\n",
      "Gradient Descent(39/49): loss=1.158376879467523e+85\n",
      "Gradient Descent(40/49): loss=1.9309858060966105e+87\n",
      "Gradient Descent(41/49): loss=3.2189059100182954e+89\n",
      "Gradient Descent(42/49): loss=5.365837089447959e+91\n",
      "Gradient Descent(43/49): loss=8.944718632776583e+93\n",
      "Gradient Descent(44/49): loss=1.4910626261255378e+96\n",
      "Gradient Descent(45/49): loss=2.4855647743703816e+98\n",
      "Gradient Descent(46/49): loss=4.143375428599013e+100\n",
      "Gradient Descent(47/49): loss=6.906905070163258e+102\n",
      "Gradient Descent(48/49): loss=1.1513641105019905e+105\n",
      "Gradient Descent(49/49): loss=1.9192956924782254e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.192057331029637\n",
      "Gradient Descent(2/49): loss=733.1163467223781\n",
      "Gradient Descent(3/49): loss=119659.23029758726\n",
      "Gradient Descent(4/49): loss=19826983.0930426\n",
      "Gradient Descent(5/49): loss=3299119445.267937\n",
      "Gradient Descent(6/49): loss=549685862866.68604\n",
      "Gradient Descent(7/49): loss=91626153768738.44\n",
      "Gradient Descent(8/49): loss=1.5275213017588378e+16\n",
      "Gradient Descent(9/49): loss=2.546691339704691e+18\n",
      "Gradient Descent(10/49): loss=4.2459270688845215e+20\n",
      "Gradient Descent(11/49): loss=7.078988139602731e+22\n",
      "Gradient Descent(12/49): loss=1.180240914693766e+25\n",
      "Gradient Descent(13/49): loss=1.9677522967072586e+27\n",
      "Gradient Descent(14/49): loss=3.280728453632979e+29\n",
      "Gradient Descent(15/49): loss=5.4697839797448965e+31\n",
      "Gradient Descent(16/49): loss=9.119480007304365e+33\n",
      "Gradient Descent(17/49): loss=1.5204424274598574e+36\n",
      "Gradient Descent(18/49): loss=2.5349528483162654e+38\n",
      "Gradient Descent(19/49): loss=4.2263921563228537e+40\n",
      "Gradient Descent(20/49): loss=7.046439020317514e+42\n",
      "Gradient Descent(21/49): loss=1.1748153279730138e+45\n",
      "Gradient Descent(22/49): loss=1.9587071582033443e+47\n",
      "Gradient Descent(23/49): loss=3.2656483451463107e+49\n",
      "Gradient Descent(24/49): loss=5.444641925946068e+51\n",
      "Gradient Descent(25/49): loss=9.077562115919298e+53\n",
      "Gradient Descent(26/49): loss=1.5134536869308948e+56\n",
      "Gradient Descent(27/49): loss=2.5233008964688906e+58\n",
      "Gradient Descent(28/49): loss=4.206965478430152e+60\n",
      "Gradient Descent(29/49): loss=7.014049953959396e+62\n",
      "Gradient Descent(30/49): loss=1.1694152711468519e+65\n",
      "Gradient Descent(31/49): loss=1.9497039304938133e+67\n",
      "Gradient Descent(32/49): loss=3.250637742104247e+69\n",
      "Gradient Descent(33/49): loss=5.419615545277342e+71\n",
      "Gradient Descent(34/49): loss=9.035836961518269e+73\n",
      "Gradient Descent(35/49): loss=1.5064970736953128e+76\n",
      "Gradient Descent(36/49): loss=2.5117025049455984e+78\n",
      "Gradient Descent(37/49): loss=4.1876280966649425e+80\n",
      "Gradient Descent(38/49): loss=6.98180976506908e+82\n",
      "Gradient Descent(39/49): loss=1.1640400358005843e+85\n",
      "Gradient Descent(40/49): loss=1.9407420862794275e+87\n",
      "Gradient Descent(41/49): loss=3.235696135542087e+89\n",
      "Gradient Descent(42/49): loss=5.39470419876008e+91\n",
      "Gradient Descent(43/49): loss=8.994303597437107e+93\n",
      "Gradient Descent(44/49): loss=1.4995724366400636e+96\n",
      "Gradient Descent(45/49): loss=2.5001574256081047e+98\n",
      "Gradient Descent(46/49): loss=4.168379599473621e+100\n",
      "Gradient Descent(47/49): loss=6.949717768704765e+102\n",
      "Gradient Descent(48/49): loss=1.1586895078065732e+105\n",
      "Gradient Descent(49/49): loss=1.9318214353203272e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.258566997745083\n",
      "Gradient Descent(2/49): loss=757.9438310260705\n",
      "Gradient Descent(3/49): loss=126196.15391366185\n",
      "Gradient Descent(4/49): loss=21315985.799402207\n",
      "Gradient Descent(5/49): loss=3614335305.0514035\n",
      "Gradient Descent(6/49): loss=613549185348.3318\n",
      "Gradient Descent(7/49): loss=104189949647582.5\n",
      "Gradient Descent(8/49): loss=1.7695050927803632e+16\n",
      "Gradient Descent(9/49): loss=3.0053408529606093e+18\n",
      "Gradient Descent(10/49): loss=5.1043541042968415e+20\n",
      "Gradient Descent(11/49): loss=8.669409511641614e+22\n",
      "Gradient Descent(12/49): loss=1.4724439736598314e+25\n",
      "Gradient Descent(13/49): loss=2.5008532844791724e+27\n",
      "Gradient Descent(14/49): loss=4.247542232195229e+29\n",
      "Gradient Descent(15/49): loss=7.214184008647102e+31\n",
      "Gradient Descent(16/49): loss=1.2252839118274255e+34\n",
      "Gradient Descent(17/49): loss=2.0810678926428304e+36\n",
      "Gradient Descent(18/49): loss=3.534563327050815e+38\n",
      "Gradient Descent(19/49): loss=6.003234187060124e+40\n",
      "Gradient Descent(20/49): loss=1.0196116853888948e+43\n",
      "Gradient Descent(21/49): loss=1.7317465163575105e+45\n",
      "Gradient Descent(22/49): loss=2.9412628748164665e+47\n",
      "Gradient Descent(23/49): loss=4.995550571115818e+49\n",
      "Gradient Descent(24/49): loss=8.484629416258467e+51\n",
      "Gradient Descent(25/49): loss=1.4410611064076836e+54\n",
      "Gradient Descent(26/49): loss=2.44755193246501e+56\n",
      "Gradient Descent(27/49): loss=4.157013491986372e+58\n",
      "Gradient Descent(28/49): loss=7.06042676494018e+60\n",
      "Gradient Descent(29/49): loss=1.1991692160533275e+63\n",
      "Gradient Descent(30/49): loss=2.0367137236953249e+65\n",
      "Gradient Descent(31/49): loss=3.459230554584561e+67\n",
      "Gradient Descent(32/49): loss=5.875286197836554e+69\n",
      "Gradient Descent(33/49): loss=9.978805217460995e+71\n",
      "Gradient Descent(34/49): loss=1.6948374975280834e+74\n",
      "Gradient Descent(35/49): loss=2.8785752206095567e+76\n",
      "Gradient Descent(36/49): loss=4.889079521070754e+78\n",
      "Gradient Descent(37/49): loss=8.30379501366367e+80\n",
      "Gradient Descent(38/49): loss=1.4103475169870911e+83\n",
      "Gradient Descent(39/49): loss=2.395386826624067e+85\n",
      "Gradient Descent(40/49): loss=4.068414330548737e+87\n",
      "Gradient Descent(41/49): loss=6.909946644543385e+89\n",
      "Gradient Descent(42/49): loss=1.1736111111376425e+92\n",
      "Gradient Descent(43/49): loss=1.9933048850288936e+94\n",
      "Gradient Descent(44/49): loss=3.3855033639112076e+96\n",
      "Gradient Descent(45/49): loss=5.750065187287163e+98\n",
      "Gradient Descent(46/49): loss=9.766125182594569e+100\n",
      "Gradient Descent(47/49): loss=1.658715127142853e+103\n",
      "Gradient Descent(48/49): loss=2.8172236394389545e+105\n",
      "Gradient Descent(49/49): loss=4.784877707291876e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.236691080645041\n",
      "Gradient Descent(2/49): loss=752.7439956944412\n",
      "Gradient Descent(3/49): loss=125316.40539237112\n",
      "Gradient Descent(4/49): loss=21192307.94965846\n",
      "Gradient Descent(5/49): loss=3599398890.126962\n",
      "Gradient Descent(6/49): loss=612152394368.8\n",
      "Gradient Descent(7/49): loss=104153408273007.19\n",
      "Gradient Descent(8/49): loss=1.772341073658827e+16\n",
      "Gradient Descent(9/49): loss=3.0160652174470364e+18\n",
      "Gradient Descent(10/49): loss=5.132637575328831e+20\n",
      "Gradient Descent(11/49): loss=8.734591353673753e+22\n",
      "Gradient Descent(12/49): loss=1.486432808922683e+25\n",
      "Gradient Descent(13/49): loss=2.5295787490156066e+27\n",
      "Gradient Descent(14/49): loss=4.304782380434173e+29\n",
      "Gradient Descent(15/49): loss=7.325785930659587e+31\n",
      "Gradient Descent(16/49): loss=1.2466864937079596e+34\n",
      "Gradient Descent(17/49): loss=2.12158427508897e+36\n",
      "Gradient Descent(18/49): loss=3.610466519290253e+38\n",
      "Gradient Descent(19/49): loss=6.144214325507281e+40\n",
      "Gradient Descent(20/49): loss=1.0456091888547258e+43\n",
      "Gradient Descent(21/49): loss=1.7793952456368245e+45\n",
      "Gradient Descent(22/49): loss=3.0281365868063962e+47\n",
      "Gradient Descent(23/49): loss=5.153217763683114e+49\n",
      "Gradient Descent(24/49): loss=8.769635238949421e+51\n",
      "Gradient Descent(25/49): loss=1.4923976775497984e+54\n",
      "Gradient Descent(26/49): loss=2.5397302935298497e+56\n",
      "Gradient Descent(27/49): loss=4.3220584304739147e+58\n",
      "Gradient Descent(28/49): loss=7.355186148710568e+60\n",
      "Gradient Descent(29/49): loss=1.2516897712614272e+63\n",
      "Gradient Descent(30/49): loss=2.130098751824456e+65\n",
      "Gradient Descent(31/49): loss=3.624956276467347e+67\n",
      "Gradient Descent(32/49): loss=6.168872684914391e+69\n",
      "Gradient Descent(33/49): loss=1.049805495578792e+72\n",
      "Gradient Descent(34/49): loss=1.7865364302987475e+74\n",
      "Gradient Descent(35/49): loss=3.040289301424233e+76\n",
      "Gradient Descent(36/49): loss=5.173898992257902e+78\n",
      "Gradient Descent(37/49): loss=8.804830109275207e+80\n",
      "Gradient Descent(38/49): loss=1.4983870649428147e+83\n",
      "Gradient Descent(39/49): loss=2.549922904273679e+85\n",
      "Gradient Descent(40/49): loss=4.3394039963817306e+87\n",
      "Gradient Descent(41/49): loss=7.384704459987298e+89\n",
      "Gradient Descent(42/49): loss=1.2567131340347187e+92\n",
      "Gradient Descent(43/49): loss=2.1386474026315643e+94\n",
      "Gradient Descent(44/49): loss=3.639504186686084e+96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=6.193630005865676e+98\n",
      "Gradient Descent(46/49): loss=1.0540186432506566e+101\n",
      "Gradient Descent(47/49): loss=1.793706274459125e+103\n",
      "Gradient Descent(48/49): loss=3.0524907881244195e+105\n",
      "Gradient Descent(49/49): loss=5.194663220093902e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.2573118054836\n",
      "Gradient Descent(2/49): loss=750.0906853455378\n",
      "Gradient Descent(3/49): loss=123793.23802414857\n",
      "Gradient Descent(4/49): loss=20742602.9641858\n",
      "Gradient Descent(5/49): loss=3489966845.462176\n",
      "Gradient Descent(6/49): loss=587931140789.4662\n",
      "Gradient Descent(7/49): loss=99084616243781.03\n",
      "Gradient Descent(8/49): loss=1.6701017745633306e+16\n",
      "Gradient Descent(9/49): loss=2.81512963629267e+18\n",
      "Gradient Descent(10/49): loss=4.745260586217123e+20\n",
      "Gradient Descent(11/49): loss=7.998780748283067e+22\n",
      "Gradient Descent(12/49): loss=1.3483051758084293e+25\n",
      "Gradient Descent(13/49): loss=2.2727561283452836e+27\n",
      "Gradient Descent(14/49): loss=3.8310476051370844e+29\n",
      "Gradient Descent(15/49): loss=6.457765712374799e+31\n",
      "Gradient Descent(16/49): loss=1.0885466089696236e+34\n",
      "Gradient Descent(17/49): loss=1.83489736265557e+36\n",
      "Gradient Descent(18/49): loss=3.092975818254438e+38\n",
      "Gradient Descent(19/49): loss=5.213642797528069e+40\n",
      "Gradient Descent(20/49): loss=8.788323227785611e+42\n",
      "Gradient Descent(21/49): loss=1.4813946440409708e+45\n",
      "Gradient Descent(22/49): loss=2.497097608462228e+47\n",
      "Gradient Descent(23/49): loss=4.2092068385178673e+49\n",
      "Gradient Descent(24/49): loss=7.095206110259517e+51\n",
      "Gradient Descent(25/49): loss=1.19599610279161e+54\n",
      "Gradient Descent(26/49): loss=2.0160184999063255e+56\n",
      "Gradient Descent(27/49): loss=3.3982807991416723e+58\n",
      "Gradient Descent(28/49): loss=5.728276992672414e+60\n",
      "Gradient Descent(29/49): loss=9.655811053950654e+62\n",
      "Gradient Descent(30/49): loss=1.6276218351322947e+65\n",
      "Gradient Descent(31/49): loss=2.743583965549451e+67\n",
      "Gradient Descent(32/49): loss=4.624694025076325e+69\n",
      "Gradient Descent(33/49): loss=7.795567802603554e+71\n",
      "Gradient Descent(34/49): loss=1.3140518493866493e+74\n",
      "Gradient Descent(35/49): loss=2.215017951995461e+76\n",
      "Gradient Descent(36/49): loss=3.733722173864179e+78\n",
      "Gradient Descent(37/49): loss=6.293710287560521e+80\n",
      "Gradient Descent(38/49): loss=1.060892785784067e+83\n",
      "Gradient Descent(39/49): loss=1.7882829865130655e+85\n",
      "Gradient Descent(40/49): loss=3.014400778952047e+87\n",
      "Gradient Descent(41/49): loss=5.081193594457034e+89\n",
      "Gradient Descent(42/49): loss=8.565061595202666e+91\n",
      "Gradient Descent(43/49): loss=1.4437607771851768e+94\n",
      "Gradient Descent(44/49): loss=2.433660468835212e+96\n",
      "Gradient Descent(45/49): loss=4.102274678162692e+98\n",
      "Gradient Descent(46/49): loss=6.914957016641333e+100\n",
      "Gradient Descent(47/49): loss=1.1656126001639051e+103\n",
      "Gradient Descent(48/49): loss=1.9648028619572944e+105\n",
      "Gradient Descent(49/49): loss=3.311949687068174e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.246523553367121\n",
      "Gradient Descent(2/49): loss=749.5625616395329\n",
      "Gradient Descent(3/49): loss=123704.5448870439\n",
      "Gradient Descent(4/49): loss=20725373.10407727\n",
      "Gradient Descent(5/49): loss=3487005215.025886\n",
      "Gradient Descent(6/49): loss=587461001820.0825\n",
      "Gradient Descent(7/49): loss=99013472645186.86\n",
      "Gradient Descent(8/49): loss=1.669062714278217e+16\n",
      "Gradient Descent(9/49): loss=2.813664480853596e+18\n",
      "Gradient Descent(10/49): loss=4.743284477277255e+20\n",
      "Gradient Descent(11/49): loss=7.996289021025207e+22\n",
      "Gradient Descent(12/49): loss=1.3480271055951781e+25\n",
      "Gradient Descent(13/49): loss=2.272526985678212e+27\n",
      "Gradient Descent(14/49): loss=3.8310654286503794e+29\n",
      "Gradient Descent(15/49): loss=6.458477066463219e+31\n",
      "Gradient Descent(16/49): loss=1.0887813810126188e+34\n",
      "Gradient Descent(17/49): loss=1.835486746660077e+36\n",
      "Gradient Descent(18/49): loss=3.0942957565544945e+38\n",
      "Gradient Descent(19/49): loss=5.216418073315678e+40\n",
      "Gradient Descent(20/49): loss=8.793929108820417e+42\n",
      "Gradient Descent(21/49): loss=1.4824959980774027e+45\n",
      "Gradient Descent(22/49): loss=2.4992177639882844e+47\n",
      "Gradient Descent(23/49): loss=4.2132251554258607e+49\n",
      "Gradient Descent(24/49): loss=7.102728888284615e+51\n",
      "Gradient Descent(25/49): loss=1.1973904977658754e+54\n",
      "Gradient Descent(26/49): loss=2.018581909420204e+56\n",
      "Gradient Descent(27/49): loss=3.402960799039135e+58\n",
      "Gradient Descent(28/49): loss=5.7367710201682965e+60\n",
      "Gradient Descent(29/49): loss=9.67114923778615e+62\n",
      "Gradient Descent(30/49): loss=1.6303793065944558e+65\n",
      "Gradient Descent(31/49): loss=2.748522040158183e+67\n",
      "Gradient Descent(32/49): loss=4.633506678280267e+69\n",
      "Gradient Descent(33/49): loss=7.811246853393355e+71\n",
      "Gradient Descent(34/49): loss=1.3168336994238106e+74\n",
      "Gradient Descent(35/49): loss=2.2199413544137195e+76\n",
      "Gradient Descent(36/49): loss=3.742416084272888e+78\n",
      "Gradient Descent(37/49): loss=6.309030695778528e+80\n",
      "Gradient Descent(38/49): loss=1.0635874639259791e+83\n",
      "Gradient Descent(39/49): loss=1.7930144073913235e+85\n",
      "Gradient Descent(40/49): loss=3.022695146524027e+87\n",
      "Gradient Descent(41/49): loss=5.095712511375155e+89\n",
      "Gradient Descent(42/49): loss=8.590441556253341e+91\n",
      "Gradient Descent(43/49): loss=1.4481917095336587e+94\n",
      "Gradient Descent(44/49): loss=2.4413869925409537e+96\n",
      "Gradient Descent(45/49): loss=4.115733026304571e+98\n",
      "Gradient Descent(46/49): loss=6.9383749465233695e+100\n",
      "Gradient Descent(47/49): loss=1.1696834219047519e+103\n",
      "Gradient Descent(48/49): loss=1.971872834811786e+105\n",
      "Gradient Descent(49/49): loss=3.324217821551108e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.3137932402066985\n",
      "Gradient Descent(2/49): loss=774.9347463612116\n",
      "Gradient Descent(3/49): loss=130459.39536789674\n",
      "Gradient Descent(4/49): loss=22281170.91424373\n",
      "Gradient Descent(5/49): loss=3820030900.992224\n",
      "Gradient Descent(6/49): loss=655684121410.9711\n",
      "Gradient Descent(7/49): loss=112584434085153.05\n",
      "Gradient Descent(8/49): loss=1.9333555433725468e+16\n",
      "Gradient Descent(9/49): loss=3.3201759085351173e+18\n",
      "Gradient Descent(10/49): loss=5.701847842624815e+20\n",
      "Gradient Descent(11/49): loss=9.792009456565372e+22\n",
      "Gradient Descent(12/49): loss=1.6816227175229717e+25\n",
      "Gradient Descent(13/49): loss=2.887922154963612e+27\n",
      "Gradient Descent(14/49): loss=4.959552090017584e+29\n",
      "Gradient Descent(15/49): loss=8.51725103911926e+31\n",
      "Gradient Descent(16/49): loss=1.462703989029921e+34\n",
      "Gradient Descent(17/49): loss=2.5119641997260307e+36\n",
      "Gradient Descent(18/49): loss=4.313903699666393e+38\n",
      "Gradient Descent(19/49): loss=7.408451577736966e+40\n",
      "Gradient Descent(20/49): loss=1.272285118284287e+43\n",
      "Gradient Descent(21/49): loss=2.1849497230316076e+45\n",
      "Gradient Descent(22/49): loss=3.752307736405447e+47\n",
      "Gradient Descent(23/49): loss=6.443998779626349e+49\n",
      "Gradient Descent(24/49): loss=1.1066555087956211e+52\n",
      "Gradient Descent(25/49): loss=1.900506901119188e+54\n",
      "Gradient Descent(26/49): loss=3.263821896240474e+56\n",
      "Gradient Descent(27/49): loss=5.60510112544512e+58\n",
      "Gradient Descent(28/49): loss=9.625880218113462e+60\n",
      "Gradient Descent(29/49): loss=1.6530936355962822e+63\n",
      "Gradient Descent(30/49): loss=2.838928499137823e+65\n",
      "Gradient Descent(31/49): loss=4.875413497257714e+67\n",
      "Gradient Descent(32/49): loss=8.372756403150547e+69\n",
      "Gradient Descent(33/49): loss=1.437889315971442e+72\n",
      "Gradient Descent(34/49): loss=2.4693489042710467e+74\n",
      "Gradient Descent(35/49): loss=4.24071863063048e+76\n",
      "Gradient Descent(36/49): loss=7.2827677259667365e+78\n",
      "Gradient Descent(37/49): loss=1.2507008922329142e+81\n",
      "Gradient Descent(38/49): loss=2.147882207275193e+83\n",
      "Gradient Descent(39/49): loss=3.688650104097187e+85\n",
      "Gradient Descent(40/49): loss=6.3346768013488435e+87\n",
      "Gradient Descent(41/49): loss=1.0878811772625155e+90\n",
      "Gradient Descent(42/49): loss=1.8682649375104236e+92\n",
      "Gradient Descent(43/49): loss=3.2084513912759603e+94\n",
      "Gradient Descent(44/49): loss=5.510010985860587e+96\n",
      "Gradient Descent(45/49): loss=9.462577848882555e+98\n",
      "Gradient Descent(46/49): loss=1.6250490203365305e+101\n",
      "Gradient Descent(47/49): loss=2.79076628025689e+103\n",
      "Gradient Descent(48/49): loss=4.792702456080993e+105\n",
      "Gradient Descent(49/49): loss=8.230713189787562e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.291635369245325\n",
      "Gradient Descent(2/49): loss=769.6193712713138\n",
      "Gradient Descent(3/49): loss=129549.8703573109\n",
      "Gradient Descent(4/49): loss=22151845.146699626\n",
      "Gradient Descent(5/49): loss=3804231348.971414\n",
      "Gradient Descent(6/49): loss=654188493282.6182\n",
      "Gradient Descent(7/49): loss=112544348417378.6\n",
      "Gradient Descent(8/49): loss=1.9364422166749556e+16\n",
      "Gradient Descent(9/49): loss=3.332000611713378e+18\n",
      "Gradient Descent(10/49): loss=5.7333977829412635e+20\n",
      "Gradient Descent(11/49): loss=9.86554784973135e+22\n",
      "Gradient Descent(12/49): loss=1.6975830999884589e+25\n",
      "Gradient Descent(13/49): loss=2.9210642553258856e+27\n",
      "Gradient Descent(14/49): loss=5.0263329579212204e+29\n",
      "Gradient Descent(15/49): loss=8.648910897083955e+31\n",
      "Gradient Descent(16/49): loss=1.4882352952427254e+34\n",
      "Gradient Descent(17/49): loss=2.560836080205778e+36\n",
      "Gradient Descent(18/49): loss=4.406481598900794e+38\n",
      "Gradient Descent(19/49): loss=7.582320572899517e+40\n",
      "Gradient Descent(20/49): loss=1.3047049895217775e+43\n",
      "Gradient Descent(21/49): loss=2.245031838793685e+45\n",
      "Gradient Descent(22/49): loss=3.863070960714826e+47\n",
      "Gradient Descent(23/49): loss=6.647263076526288e+49\n",
      "Gradient Descent(24/49): loss=1.1438077855161121e+52\n",
      "Gradient Descent(25/49): loss=1.9681728181158568e+54\n",
      "Gradient Descent(26/49): loss=3.386674134433637e+56\n",
      "Gradient Descent(27/49): loss=5.82751757735522e+58\n",
      "Gradient Descent(28/49): loss=1.0027525461957194e+61\n",
      "Gradient Descent(29/49): loss=1.7254562608429864e+63\n",
      "Gradient Descent(30/49): loss=2.969026924316752e+65\n",
      "Gradient Descent(31/49): loss=5.1088637117993565e+67\n",
      "Gradient Descent(32/49): loss=8.790923454406437e+69\n",
      "Gradient Descent(33/49): loss=1.512671692586902e+72\n",
      "Gradient Descent(34/49): loss=2.6028842833420207e+74\n",
      "Gradient Descent(35/49): loss=4.4788347832982754e+76\n",
      "Gradient Descent(36/49): loss=7.70682013966681e+78\n",
      "Gradient Descent(37/49): loss=1.3261278778726194e+81\n",
      "Gradient Descent(38/49): loss=2.281894629173178e+83\n",
      "Gradient Descent(39/49): loss=3.926501497730786e+85\n",
      "Gradient Descent(40/49): loss=6.756409263853024e+87\n",
      "Gradient Descent(41/49): loss=1.1625887871699626e+90\n",
      "Gradient Descent(42/49): loss=2.0004896614011923e+92\n",
      "Gradient Descent(43/49): loss=3.4422823697748435e+94\n",
      "Gradient Descent(44/49): loss=5.923203774501463e+96\n",
      "Gradient Descent(45/49): loss=1.019217460552579e+99\n",
      "Gradient Descent(46/49): loss=1.7537877666258192e+101\n",
      "Gradient Descent(47/49): loss=3.0177775101093735e+103\n",
      "Gradient Descent(48/49): loss=5.192749814901075e+105\n",
      "Gradient Descent(49/49): loss=8.935267941332627e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.312539663181068\n",
      "Gradient Descent(2/49): loss=766.9115989000429\n",
      "Gradient Descent(3/49): loss=127976.60029923117\n",
      "Gradient Descent(4/49): loss=21682096.979380857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=3688640541.042892\n",
      "Gradient Descent(6/49): loss=628318101181.9608\n",
      "Gradient Descent(7/49): loss=107070009148132.3\n",
      "Gradient Descent(8/49): loss=1.824791357800597e+16\n",
      "Gradient Descent(9/49): loss=3.110121910340478e+18\n",
      "Gradient Descent(10/49): loss=5.300878403209696e+20\n",
      "Gradient Descent(11/49): loss=9.034837358806368e+22\n",
      "Gradient Descent(12/49): loss=1.5399035584638599e+25\n",
      "Gradient Descent(13/49): loss=2.6246230239533586e+27\n",
      "Gradient Descent(14/49): loss=4.4734276912116205e+29\n",
      "Gradient Descent(15/49): loss=7.624545043837468e+31\n",
      "Gradient Descent(16/49): loss=1.2995334284778474e+34\n",
      "Gradient Descent(17/49): loss=2.2149349670569577e+36\n",
      "Gradient Descent(18/49): loss=3.7751525364624025e+38\n",
      "Gradient Descent(19/49): loss=6.43439960793903e+40\n",
      "Gradient Descent(20/49): loss=1.0966841187078725e+43\n",
      "Gradient Descent(21/49): loss=1.869197018531584e+45\n",
      "Gradient Descent(22/49): loss=3.1858740676345976e+47\n",
      "Gradient Descent(23/49): loss=5.430028763306978e+49\n",
      "Gradient Descent(24/49): loss=9.254983638545913e+51\n",
      "Gradient Descent(25/49): loss=1.5774266745811647e+54\n",
      "Gradient Descent(26/49): loss=2.688578403658776e+56\n",
      "Gradient Descent(27/49): loss=4.5824341309179074e+58\n",
      "Gradient Descent(28/49): loss=7.810336695268385e+60\n",
      "Gradient Descent(29/49): loss=1.3311999158237191e+63\n",
      "Gradient Descent(30/49): loss=2.2689075836674753e+65\n",
      "Gradient Descent(31/49): loss=3.867143891786061e+67\n",
      "Gradient Descent(32/49): loss=6.5911903981585714e+69\n",
      "Gradient Descent(33/49): loss=1.1234076641692448e+72\n",
      "Gradient Descent(34/49): loss=1.9147448392126383e+74\n",
      "Gradient Descent(35/49): loss=3.263506130699784e+76\n",
      "Gradient Descent(36/49): loss=5.562345460868098e+78\n",
      "Gradient Descent(37/49): loss=9.480505256291873e+80\n",
      "Gradient Descent(38/49): loss=1.6158647560978796e+83\n",
      "Gradient Descent(39/49): loss=2.754092571454899e+85\n",
      "Gradient Descent(40/49): loss=4.694096992659211e+87\n",
      "Gradient Descent(41/49): loss=8.000655753140524e+89\n",
      "Gradient Descent(42/49): loss=1.3636380454081462e+92\n",
      "Gradient Descent(43/49): loss=2.3241953863027093e+94\n",
      "Gradient Descent(44/49): loss=3.961376856491248e+96\n",
      "Gradient Descent(45/49): loss=6.751801802733939e+98\n",
      "Gradient Descent(46/49): loss=1.1507823980114696e+101\n",
      "Gradient Descent(47/49): loss=1.9614025504077823e+103\n",
      "Gradient Descent(48/49): loss=3.3430298998263015e+105\n",
      "Gradient Descent(49/49): loss=5.697886397062718e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.3013003434974895\n",
      "Gradient Descent(2/49): loss=766.2841434888717\n",
      "Gradient Descent(3/49): loss=127863.19365695008\n",
      "Gradient Descent(4/49): loss=21659182.643217653\n",
      "Gradient Descent(5/49): loss=3684466892.8472695\n",
      "Gradient Descent(6/49): loss=627602303784.9326\n",
      "Gradient Descent(7/49): loss=106950704026599.33\n",
      "Gradient Descent(8/49): loss=1.822829541634071e+16\n",
      "Gradient Descent(9/49): loss=3.1069182848334966e+18\n",
      "Gradient Descent(10/49): loss=5.295669135461922e+20\n",
      "Gradient Descent(11/49): loss=9.02639456229557e+22\n",
      "Gradient Descent(12/49): loss=1.5385393873937267e+25\n",
      "Gradient Descent(13/49): loss=2.6224257952094556e+27\n",
      "Gradient Descent(14/49): loss=4.469900891621263e+29\n",
      "Gradient Descent(15/49): loss=7.6189059483681885e+31\n",
      "Gradient Descent(16/49): loss=1.2986357135961307e+34\n",
      "Gradient Descent(17/49): loss=2.213512993574518e+36\n",
      "Gradient Descent(18/49): loss=3.772913170315758e+38\n",
      "Gradient Descent(19/49): loss=6.430896880403143e+40\n",
      "Gradient Descent(20/49): loss=1.096140643414543e+43\n",
      "Gradient Descent(21/49): loss=1.8683619604395057e+45\n",
      "Gradient Descent(22/49): loss=3.1846063151913206e+47\n",
      "Gradient Descent(23/49): loss=5.428133090714144e+49\n",
      "Gradient Descent(24/49): loss=9.252204490736506e+51\n",
      "Gradient Descent(25/49): loss=1.5770300121229285e+54\n",
      "Gradient Descent(26/49): loss=2.6880336049941604e+56\n",
      "Gradient Descent(27/49): loss=4.581729330472625e+58\n",
      "Gradient Descent(28/49): loss=7.809516822525035e+60\n",
      "Gradient Descent(29/49): loss=1.3311251844510722e+63\n",
      "Gradient Descent(30/49): loss=2.2688910171359432e+65\n",
      "Gradient Descent(31/49): loss=3.8673045238514153e+67\n",
      "Gradient Descent(32/49): loss=6.5917861048614385e+69\n",
      "Gradient Descent(33/49): loss=1.123564068571748e+72\n",
      "Gradient Descent(34/49): loss=1.9151049444011718e+74\n",
      "Gradient Descent(35/49): loss=3.264279314958886e+76\n",
      "Gradient Descent(36/49): loss=5.563935009003024e+78\n",
      "Gradient Descent(37/49): loss=9.483677650544185e+80\n",
      "Gradient Descent(38/49): loss=1.6164844059806568e+83\n",
      "Gradient Descent(39/49): loss=2.75528326780347e+85\n",
      "Gradient Descent(40/49): loss=4.696355781565545e+87\n",
      "Gradient Descent(41/49): loss=8.004896587140078e+89\n",
      "Gradient Descent(42/49): loss=1.3644274912546305e+92\n",
      "Gradient Descent(43/49): loss=2.3256545023731885e+94\n",
      "Gradient Descent(44/49): loss=3.9640573786997473e+96\n",
      "Gradient Descent(45/49): loss=6.756700483923527e+98\n",
      "Gradient Descent(46/49): loss=1.1516735775511647e+101\n",
      "Gradient Descent(47/49): loss=1.9630173520127786e+103\n",
      "Gradient Descent(48/49): loss=3.345945586853621e+105\n",
      "Gradient Descent(49/49): loss=5.703134441835744e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.369334329300081\n",
      "Gradient Descent(2/49): loss=792.2099395090565\n",
      "Gradient Descent(3/49): loss=134841.990899783\n",
      "Gradient Descent(4/49): loss=23284382.559821334\n",
      "Gradient Descent(5/49): loss=4036202926.1816707\n",
      "Gradient Descent(6/49): loss=700456505977.703\n",
      "Gradient Descent(7/49): loss=121603376542215.95\n",
      "Gradient Descent(8/49): loss=2.1113485849666764e+16\n",
      "Gradient Descent(9/49): loss=3.6659817072238787e+18\n",
      "Gradient Descent(10/49): loss=6.365401921135635e+20\n",
      "Gradient Descent(11/49): loss=1.1052564216841661e+23\n",
      "Gradient Descent(12/49): loss=1.9191141465769183e+25\n",
      "Gradient Descent(13/49): loss=3.332258949357013e+27\n",
      "Gradient Descent(14/49): loss=5.785977445014836e+29\n",
      "Gradient Descent(15/49): loss=1.0046499065898427e+32\n",
      "Gradient Descent(16/49): loss=1.7444268636679163e+34\n",
      "Gradient Descent(17/49): loss=3.028940804518462e+36\n",
      "Gradient Descent(18/49): loss=5.259310436963289e+38\n",
      "Gradient Descent(19/49): loss=9.132019432046204e+40\n",
      "Gradient Descent(20/49): loss=1.5856409299192292e+43\n",
      "Gradient Descent(21/49): loss=2.7532323791784008e+45\n",
      "Gradient Descent(22/49): loss=4.780583290230863e+47\n",
      "Gradient Descent(23/49): loss=8.300780118594076e+49\n",
      "Gradient Descent(24/49): loss=1.4413084428037535e+52\n",
      "Gradient Descent(25/49): loss=2.5026202328202e+54\n",
      "Gradient Descent(26/49): loss=4.3454321391045135e+56\n",
      "Gradient Descent(27/49): loss=7.545204113643949e+58\n",
      "Gradient Descent(28/49): loss=1.3101137768148986e+61\n",
      "Gradient Descent(29/49): loss=2.2748199814720228e+63\n",
      "Gradient Descent(30/49): loss=3.9498904901871477e+65\n",
      "Gradient Descent(31/49): loss=6.858404186504092e+67\n",
      "Gradient Descent(32/49): loss=1.1908610657007977e+70\n",
      "Gradient Descent(33/49): loss=2.0677551792480544e+72\n",
      "Gradient Descent(34/49): loss=3.590352900479655e+74\n",
      "Gradient Descent(35/49): loss=6.234120015441283e+76\n",
      "Gradient Descent(36/49): loss=1.0824632966228345e+79\n",
      "Gradient Descent(37/49): loss=1.8795383881499388e+81\n",
      "Gradient Descent(38/49): loss=3.2635421113591417e+83\n",
      "Gradient Descent(39/49): loss=5.666661122627125e+85\n",
      "Gradient Descent(40/49): loss=9.839324017584244e+87\n",
      "Gradient Descent(41/49): loss=1.7084539736536665e+90\n",
      "Gradient Descent(42/49): loss=2.9664791756798107e+92\n",
      "Gradient Descent(43/49): loss=5.150855004260054e+94\n",
      "Gradient Descent(44/49): loss=8.943702518603015e+96\n",
      "Gradient Descent(45/49): loss=1.552942466349946e+99\n",
      "Gradient Descent(46/49): loss=2.6964563040606787e+101\n",
      "Gradient Descent(47/49): loss=4.681999982135976e+103\n",
      "Gradient Descent(48/49): loss=8.129604696248698e+105\n",
      "Gradient Descent(49/49): loss=1.411586347061841e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.346892769180941\n",
      "Gradient Descent(2/49): loss=786.777110686904\n",
      "Gradient Descent(3/49): loss=133901.85450558434\n",
      "Gradient Descent(4/49): loss=23149184.52209008\n",
      "Gradient Descent(5/49): loss=4019495616.9961133\n",
      "Gradient Descent(6/49): loss=698855672970.139\n",
      "Gradient Descent(7/49): loss=121559438563116.17\n",
      "Gradient Descent(8/49): loss=2.114706567092081e+16\n",
      "Gradient Descent(9/49): loss=3.679012728852928e+18\n",
      "Gradient Descent(10/49): loss=6.40057461298903e+20\n",
      "Gradient Descent(11/49): loss=1.1135475844490713e+23\n",
      "Gradient Descent(12/49): loss=1.93731081053666e+25\n",
      "Gradient Descent(13/49): loss=3.3704668236018304e+27\n",
      "Gradient Descent(14/49): loss=5.863823480316697e+29\n",
      "Gradient Descent(15/49): loss=1.0201681136684064e+32\n",
      "Gradient Descent(16/49): loss=1.7748539049507918e+34\n",
      "Gradient Descent(17/49): loss=3.0878306837276326e+36\n",
      "Gradient Descent(18/49): loss=5.372103205100334e+38\n",
      "Gradient Descent(19/49): loss=9.346203151069646e+40\n",
      "Gradient Descent(20/49): loss=1.6260207599526808e+43\n",
      "Gradient Descent(21/49): loss=2.828895829953954e+45\n",
      "Gradient Descent(22/49): loss=4.921617124471858e+47\n",
      "Gradient Descent(23/49): loss=8.562462733202717e+49\n",
      "Gradient Descent(24/49): loss=1.4896682574744484e+52\n",
      "Gradient Descent(25/49): loss=2.591674365745127e+54\n",
      "Gradient Descent(26/49): loss=4.5089072579487265e+56\n",
      "Gradient Descent(27/49): loss=7.844444089695491e+58\n",
      "Gradient Descent(28/49): loss=1.364749806460983e+61\n",
      "Gradient Descent(29/49): loss=2.3743454767967055e+63\n",
      "Gradient Descent(30/49): loss=4.1308058198623263e+65\n",
      "Gradient Descent(31/49): loss=7.186636017446501e+67\n",
      "Gradient Descent(32/49): loss=1.2503065866451396e+70\n",
      "Gradient Descent(33/49): loss=2.175241040193479e+72\n",
      "Gradient Descent(34/49): loss=3.784410666537529e+74\n",
      "Gradient Descent(35/49): loss=6.583989465245303e+76\n",
      "Gradient Descent(36/49): loss=1.145460181204969e+79\n",
      "Gradient Descent(37/49): loss=1.992832816109669e+81\n",
      "Gradient Descent(38/49): loss=3.467063018101507e+83\n",
      "Gradient Descent(39/49): loss=6.031878777946387e+85\n",
      "Gradient Descent(40/49): loss=1.0494058343295786e+88\n",
      "Gradient Descent(41/49): loss=1.8257207176499117e+90\n",
      "Gradient Descent(42/49): loss=3.1763270508073604e+92\n",
      "Gradient Descent(43/49): loss=5.52606619191859e+94\n",
      "Gradient Descent(44/49): loss=9.61406274259566e+96\n",
      "Gradient Descent(45/49): loss=1.6726220643852868e+99\n",
      "Gradient Descent(46/49): loss=2.909971200700911e+101\n",
      "Gradient Descent(47/49): loss=5.062669307797748e+103\n",
      "Gradient Descent(48/49): loss=8.807860543067844e+105\n",
      "Gradient Descent(49/49): loss=1.532361737051277e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.368082450436445\n",
      "Gradient Descent(2/49): loss=784.0140489703524\n",
      "Gradient Descent(3/49): loss=132277.1167012425\n",
      "Gradient Descent(4/49): loss=22658616.011294726\n",
      "Gradient Descent(5/49): loss=3897435869.9086356\n",
      "Gradient Descent(6/49): loss=671233800095.0055\n",
      "Gradient Descent(7/49): loss=115649593908259.28\n",
      "Gradient Descent(8/49): loss=1.992836560422307e+16\n",
      "Gradient Descent(9/49): loss=3.4341411329466424e+18\n",
      "Gradient Descent(10/49): loss=5.917944034792314e+20\n",
      "Gradient Descent(11/49): loss=1.019824979570094e+23\n",
      "Gradient Descent(12/49): loss=1.757442510028712e+25\n",
      "Gradient Descent(13/49): loss=3.0285645769083117e+27\n",
      "Gradient Descent(14/49): loss=5.219064040642264e+29\n",
      "Gradient Descent(15/49): loss=8.99390795852573e+31\n",
      "Gradient Descent(16/49): loss=1.5499020764532511e+34\n",
      "Gradient Descent(17/49): loss=2.670915105184536e+36\n",
      "Gradient Descent(18/49): loss=4.602734342253493e+38\n",
      "Gradient Descent(19/49): loss=7.931799629034961e+40\n",
      "Gradient Descent(20/49): loss=1.3668710964381884e+43\n",
      "Gradient Descent(21/49): loss=2.3555015025675222e+45\n",
      "Gradient Descent(22/49): loss=4.059188421791636e+47\n",
      "Gradient Descent(23/49): loss=6.995117865894381e+49\n",
      "Gradient Descent(24/49): loss=1.2054546099710132e+52\n",
      "Gradient Descent(25/49): loss=2.0773357140794465e+54\n",
      "Gradient Descent(26/49): loss=3.579830906362486e+56\n",
      "Gradient Descent(27/49): loss=6.169050688963059e+58\n",
      "Gradient Descent(28/49): loss=1.063100112783468e+61\n",
      "Gradient Descent(29/49): loss=1.832019068707322e+63\n",
      "Gradient Descent(30/49): loss=3.157081659336503e+65\n",
      "Gradient Descent(31/49): loss=5.440535403789112e+67\n",
      "Gradient Descent(32/49): loss=9.375565371376362e+69\n",
      "Gradient Descent(33/49): loss=1.6156723467277166e+72\n",
      "Gradient Descent(34/49): loss=2.78425569934181e+74\n",
      "Gradient Descent(35/49): loss=4.798051916292258e+76\n",
      "Gradient Descent(36/49): loss=8.268386483640166e+78\n",
      "Gradient Descent(37/49): loss=1.424874433115223e+81\n",
      "Gradient Descent(38/49): loss=2.4554574875793706e+83\n",
      "Gradient Descent(39/49): loss=4.231440562890645e+85\n",
      "Gradient Descent(40/49): loss=7.291956520463917e+87\n",
      "Gradient Descent(41/49): loss=1.2566082190225118e+90\n",
      "Gradient Descent(42/49): loss=2.1654877009805196e+92\n",
      "Gradient Descent(43/49): loss=3.7317414545844926e+94\n",
      "Gradient Descent(44/49): loss=6.430835085121365e+96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=1.1082128918985547e+99\n",
      "Gradient Descent(46/49): loss=1.9097610147267267e+101\n",
      "Gradient Descent(47/49): loss=3.291052793224417e+103\n",
      "Gradient Descent(48/49): loss=5.671405167593763e+105\n",
      "Gradient Descent(49/49): loss=9.773418597607994e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.356387701420747\n",
      "Gradient Descent(2/49): loss=783.2841497665177\n",
      "Gradient Descent(3/49): loss=132137.7027409321\n",
      "Gradient Descent(4/49): loss=22629601.990486193\n",
      "Gradient Descent(5/49): loss=3891935873.6352496\n",
      "Gradient Descent(6/49): loss=670243748786.5946\n",
      "Gradient Descent(7/49): loss=115475420943586.12\n",
      "Gradient Descent(8/49): loss=1.9898017409365412e+16\n",
      "Gradient Descent(9/49): loss=3.428873732498592e+18\n",
      "Gradient Descent(10/49): loss=5.908815731019077e+20\n",
      "Gradient Descent(11/49): loss=1.0182440225699719e+23\n",
      "Gradient Descent(12/49): loss=1.7547050720671574e+25\n",
      "Gradient Descent(13/49): loss=3.023825178855589e+27\n",
      "Gradient Descent(14/49): loss=5.210859022401884e+29\n",
      "Gradient Descent(15/49): loss=8.97970359071771e+31\n",
      "Gradient Descent(16/49): loss=1.5474431051859058e+34\n",
      "Gradient Descent(17/49): loss=2.6666583820596973e+36\n",
      "Gradient Descent(18/49): loss=4.595365686832509e+38\n",
      "Gradient Descent(19/49): loss=7.919044283772047e+40\n",
      "Gradient Descent(20/49): loss=1.364663155529905e+43\n",
      "Gradient Descent(21/49): loss=2.3516796491422576e+45\n",
      "Gradient Descent(22/49): loss=4.052573083695578e+47\n",
      "Gradient Descent(23/49): loss=6.983667441655185e+49\n",
      "Gradient Descent(24/49): loss=1.2034727055776085e+52\n",
      "Gradient Descent(25/49): loss=2.0739053873523223e+54\n",
      "Gradient Descent(26/49): loss=3.573893729170191e+56\n",
      "Gradient Descent(27/49): loss=6.158774872421132e+58\n",
      "Gradient Descent(28/49): loss=1.0613216509371504e+61\n",
      "Gradient Descent(29/49): loss=1.8289410963729025e+63\n",
      "Gradient Descent(30/49): loss=3.1517547305739787e+65\n",
      "Gradient Descent(31/49): loss=5.431316460325282e+67\n",
      "Gradient Descent(32/49): loss=9.359611078247866e+69\n",
      "Gradient Descent(33/49): loss=1.6129113480309611e+72\n",
      "Gradient Descent(34/49): loss=2.779477688611453e+74\n",
      "Gradient Descent(35/49): loss=4.789783537030802e+76\n",
      "Gradient Descent(36/49): loss=8.254078248446935e+78\n",
      "Gradient Descent(37/49): loss=1.4223984696752815e+81\n",
      "Gradient Descent(38/49): loss=2.451173039115869e+83\n",
      "Gradient Descent(39/49): loss=4.224026808085755e+85\n",
      "Gradient Descent(40/49): loss=7.279128070804313e+87\n",
      "Gradient Descent(41/49): loss=1.254388475228064e+90\n",
      "Gradient Descent(42/49): loss=2.1616468778672296e+92\n",
      "Gradient Descent(43/49): loss=3.725095787207057e+94\n",
      "Gradient Descent(44/49): loss=6.41933646329817e+96\n",
      "Gradient Descent(45/49): loss=1.106223382779791e+99\n",
      "Gradient Descent(46/49): loss=1.9063187910546538e+101\n",
      "Gradient Descent(47/49): loss=3.2850971961885475e+103\n",
      "Gradient Descent(48/49): loss=5.661101196214559e+105\n",
      "Gradient Descent(49/49): loss=9.75559164306155e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.425190265025224\n",
      "Gradient Descent(2/49): loss=809.7725646039403\n",
      "Gradient Descent(3/49): loss=139346.59892241404\n",
      "Gradient Descent(4/49): loss=24326897.74115885\n",
      "Gradient Descent(5/49): loss=4263323022.1311707\n",
      "Gradient Descent(6/49): loss=748015578632.038\n",
      "Gradient Descent(7/49): loss=131289414347345.7\n",
      "Gradient Descent(8/49): loss=2.3046170753079388e+16\n",
      "Gradient Descent(9/49): loss=4.045609320847892e+18\n",
      "Gradient Descent(10/49): loss=7.101896090539139e+20\n",
      "Gradient Descent(11/49): loss=1.2467126693563167e+23\n",
      "Gradient Descent(12/49): loss=2.1885626274141332e+25\n",
      "Gradient Descent(13/49): loss=3.8419504729467775e+27\n",
      "Gradient Descent(14/49): loss=6.744419924860303e+29\n",
      "Gradient Descent(15/49): loss=1.1839611779705867e+32\n",
      "Gradient Descent(16/49): loss=2.078405713296911e+34\n",
      "Gradient Descent(17/49): loss=3.6485743020650793e+36\n",
      "Gradient Descent(18/49): loss=6.4049547073685806e+38\n",
      "Gradient Descent(19/49): loss=1.1243691761938075e+41\n",
      "Gradient Descent(20/49): loss=1.9737938866535816e+43\n",
      "Gradient Descent(21/49): loss=3.464931616476841e+45\n",
      "Gradient Descent(22/49): loss=6.082575890135389e+47\n",
      "Gradient Descent(23/49): loss=1.0677766130654675e+50\n",
      "Gradient Descent(24/49): loss=1.874447464372098e+52\n",
      "Gradient Descent(25/49): loss=3.2905321709621465e+54\n",
      "Gradient Descent(26/49): loss=5.776423278827844e+56\n",
      "Gradient Descent(27/49): loss=1.0140325078915667e+59\n",
      "Gradient Descent(28/49): loss=1.7801014181729504e+61\n",
      "Gradient Descent(29/49): loss=3.1249107245783203e+63\n",
      "Gradient Descent(30/49): loss=5.485680162317481e+65\n",
      "Gradient Descent(31/49): loss=9.629934899117587e+67\n",
      "Gradient Descent(32/49): loss=1.6905040654441972e+70\n",
      "Gradient Descent(33/49): loss=2.9676254566842593e+72\n",
      "Gradient Descent(34/49): loss=5.209570938740357e+74\n",
      "Gradient Descent(35/49): loss=9.145234047187056e+76\n",
      "Gradient Descent(36/49): loss=1.6054163915090403e+79\n",
      "Gradient Descent(37/49): loss=2.8182567847114507e+81\n",
      "Gradient Descent(38/49): loss=4.947359044407424e+83\n",
      "Gradient Descent(39/49): loss=8.68492950928384e+85\n",
      "Gradient Descent(40/49): loss=1.5246114119510767e+88\n",
      "Gradient Descent(41/49): loss=2.676406244825264e+90\n",
      "Gradient Descent(42/49): loss=4.6983449888865916e+92\n",
      "Gradient Descent(43/49): loss=8.247793352476316e+94\n",
      "Gradient Descent(44/49): loss=1.4478735670977892e+97\n",
      "Gradient Descent(45/49): loss=2.5416954289610995e+99\n",
      "Gradient Descent(46/49): loss=4.4618644890043655e+101\n",
      "Gradient Descent(47/49): loss=7.832659449041684e+103\n",
      "Gradient Descent(48/49): loss=1.3749981469820935e+106\n",
      "Gradient Descent(49/49): loss=2.413764975362874e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.402463280451892\n",
      "Gradient Descent(2/49): loss=804.220347049604\n",
      "Gradient Descent(3/49): loss=138374.997632908\n",
      "Gradient Descent(4/49): loss=24185595.486708727\n",
      "Gradient Descent(5/49): loss=4245661297.6291265\n",
      "Gradient Descent(6/49): loss=746302802535.7289\n",
      "Gradient Descent(7/49): loss=131241291998102.94\n",
      "Gradient Descent(8/49): loss=2.308268552715081e+16\n",
      "Gradient Descent(9/49): loss=4.0599621898347643e+18\n",
      "Gradient Descent(10/49): loss=7.14108441676003e+20\n",
      "Gradient Descent(11/49): loss=1.2560545358921028e+23\n",
      "Gradient Descent(12/49): loss=2.209294112334039e+25\n",
      "Gradient Descent(13/49): loss=3.8859643180685854e+27\n",
      "Gradient Descent(14/49): loss=6.835088767198565e+29\n",
      "Gradient Descent(15/49): loss=1.2022354653407471e+32\n",
      "Gradient Descent(16/49): loss=2.114632584020471e+34\n",
      "Gradient Descent(17/49): loss=3.719463551834482e+36\n",
      "Gradient Descent(18/49): loss=6.542228303506244e+38\n",
      "Gradient Descent(19/49): loss=1.1507237699368143e+41\n",
      "Gradient Descent(20/49): loss=2.0240277984387788e+43\n",
      "Gradient Descent(21/49): loss=3.5600972502634327e+45\n",
      "Gradient Descent(22/49): loss=6.261916185848885e+47\n",
      "Gradient Descent(23/49): loss=1.1014191906139671e+50\n",
      "Gradient Descent(24/49): loss=1.9373051274585295e+52\n",
      "Gradient Descent(25/49): loss=3.407559255244461e+54\n",
      "Gradient Descent(26/49): loss=5.993614487170774e+56\n",
      "Gradient Descent(27/49): loss=1.0542271441219525e+59\n",
      "Gradient Descent(28/49): loss=1.8542982265250515e+61\n",
      "Gradient Descent(29/49): loss=3.2615569918357484e+63\n",
      "Gradient Descent(30/49): loss=5.736808598974827e+65\n",
      "Gradient Descent(31/49): loss=1.0090571154713404e+68\n",
      "Gradient Descent(32/49): loss=1.7748478874914774e+70\n",
      "Gradient Descent(33/49): loss=3.1218104262230345e+72\n",
      "Gradient Descent(34/49): loss=5.491005965051526e+74\n",
      "Gradient Descent(35/49): loss=9.658224681090018e+76\n",
      "Gradient Descent(36/49): loss=1.6988017238393462e+79\n",
      "Gradient Descent(37/49): loss=2.9880515231437193e+81\n",
      "Gradient Descent(38/49): loss=5.255735133575752e+83\n",
      "Gradient Descent(39/49): loss=9.24440277563913e+85\n",
      "Gradient Descent(40/49): loss=1.6260138782926584e+88\n",
      "Gradient Descent(41/49): loss=2.8600237317304976e+90\n",
      "Gradient Descent(42/49): loss=5.030544852821624e+92\n",
      "Gradient Descent(43/49): loss=8.848311723951356e+94\n",
      "Gradient Descent(44/49): loss=1.5563447430609953e+97\n",
      "Gradient Descent(45/49): loss=2.737481493443505e+99\n",
      "Gradient Descent(46/49): loss=4.815003205656714e+101\n",
      "Gradient Descent(47/49): loss=8.469191819565804e+103\n",
      "Gradient Descent(48/49): loss=1.4896606920704417e+106\n",
      "Gradient Descent(49/49): loss=2.620189771086759e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.42394016724973\n",
      "Gradient Descent(2/49): loss=801.4011604224978\n",
      "Gradient Descent(3/49): loss=136697.39745330336\n",
      "Gradient Descent(4/49): loss=23673403.789388705\n",
      "Gradient Descent(5/49): loss=4116808628.265298\n",
      "Gradient Descent(6/49): loss=716821365473.6228\n",
      "Gradient Descent(7/49): loss=124863952571861.06\n",
      "Gradient Descent(8/49): loss=2.1753074997017376e+16\n",
      "Gradient Descent(9/49): loss=3.789860183892036e+18\n",
      "Gradient Descent(10/49): loss=6.602858667343843e+20\n",
      "Gradient Descent(11/49): loss=1.1503841815243552e+23\n",
      "Gradient Descent(12/49): loss=2.0042620110157285e+25\n",
      "Gradient Descent(13/49): loss=3.491936352765116e+27\n",
      "Gradient Descent(14/49): loss=6.083846113907151e+29\n",
      "Gradient Descent(15/49): loss=1.0599616366111812e+32\n",
      "Gradient Descent(16/49): loss=1.8467243780208901e+34\n",
      "Gradient Descent(17/49): loss=3.217466399518734e+36\n",
      "Gradient Descent(18/49): loss=5.605649753508682e+38\n",
      "Gradient Descent(19/49): loss=9.766476251734664e+40\n",
      "Gradient Descent(20/49): loss=1.701570069354795e+43\n",
      "Gradient Descent(21/49): loss=2.964570461778138e+45\n",
      "Gradient Descent(22/49): loss=5.165040324441741e+47\n",
      "Gradient Descent(23/49): loss=8.998821885766101e+49\n",
      "Gradient Descent(24/49): loss=1.5678250361139918e+52\n",
      "Gradient Descent(25/49): loss=2.7315523910483366e+54\n",
      "Gradient Descent(26/49): loss=4.759063220177982e+56\n",
      "Gradient Descent(27/49): loss=8.291505887961723e+58\n",
      "Gradient Descent(28/49): loss=1.444592490358533e+61\n",
      "Gradient Descent(29/49): loss=2.5168497633586168e+63\n",
      "Gradient Descent(30/49): loss=4.384996304214619e+65\n",
      "Gradient Descent(31/49): loss=7.639785603379357e+67\n",
      "Gradient Descent(32/49): loss=1.3310461404381044e+70\n",
      "Gradient Descent(33/49): loss=2.319022967334978e+72\n",
      "Gradient Descent(34/49): loss=4.0403314052336533e+74\n",
      "Gradient Descent(35/49): loss=7.03929115582553e+76\n",
      "Gradient Descent(36/49): loss=1.2264246421047618e+79\n",
      "Gradient Descent(37/49): loss=2.1367455464845096e+81\n",
      "Gradient Descent(38/49): loss=3.722757496608902e+83\n",
      "Gradient Descent(39/49): loss=6.485996145567832e+85\n",
      "Gradient Descent(40/49): loss=1.1300264934968532e+88\n",
      "Gradient Descent(41/49): loss=1.9687953050625768e+90\n",
      "Gradient Descent(42/49): loss=3.4301452006153615e+92\n",
      "Gradient Descent(43/49): loss=5.976190651740026e+94\n",
      "Gradient Descent(44/49): loss=1.0412053314692795e+97\n",
      "Gradient Descent(45/49): loss=1.8140461130777444e+99\n",
      "Gradient Descent(46/49): loss=3.160532510651657e+101\n",
      "Gradient Descent(47/49): loss=5.5064563568004085e+103\n",
      "Gradient Descent(48/49): loss=9.593655976377183e+105\n",
      "Gradient Descent(49/49): loss=1.6714603554318755e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.4117856271368865\n",
      "Gradient Descent(2/49): loss=800.5656548555149\n",
      "Gradient Descent(3/49): loss=136530.64027019497\n",
      "Gradient Descent(4/49): loss=23637854.57435492\n",
      "Gradient Descent(5/49): loss=4109860462.6665745\n",
      "Gradient Descent(6/49): loss=715526106402.6753\n",
      "Gradient Descent(7/49): loss=124627536460232.33\n",
      "Gradient Descent(8/49): loss=2.1710317974045452e+16\n",
      "Gradient Descent(9/49): loss=3.782159701078283e+18\n",
      "Gradient Descent(10/49): loss=6.58902017534562e+20\n",
      "Gradient Descent(11/49): loss=1.1479006227566323e+23\n",
      "Gradient Descent(12/49): loss=1.999809324994594e+25\n",
      "Gradient Descent(13/49): loss=3.4839601095026285e+27\n",
      "Gradient Descent(14/49): loss=6.069569044864439e+29\n",
      "Gradient Descent(15/49): loss=1.0574079510827744e+32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=1.8421597907197102e+34\n",
      "Gradient Descent(17/49): loss=3.209312660419456e+36\n",
      "Gradient Descent(18/49): loss=5.5910935822740275e+38\n",
      "Gradient Descent(19/49): loss=9.740505455699375e+40\n",
      "Gradient Descent(20/49): loss=1.6969389828580818e+43\n",
      "Gradient Descent(21/49): loss=2.95631671783646e+45\n",
      "Gradient Descent(22/49): loss=5.1503375341129466e+47\n",
      "Gradient Descent(23/49): loss=8.972643748086318e+49\n",
      "Gradient Descent(24/49): loss=1.5631662060439328e+52\n",
      "Gradient Descent(25/49): loss=2.723264910905251e+54\n",
      "Gradient Descent(26/49): loss=4.7443270883783335e+56\n",
      "Gradient Descent(27/49): loss=8.265313973454923e+58\n",
      "Gradient Descent(28/49): loss=1.4399389799058754e+61\n",
      "Gradient Descent(29/49): loss=2.508585000535352e+63\n",
      "Gradient Descent(30/49): loss=4.370323182252052e+65\n",
      "Gradient Descent(31/49): loss=7.613744287418474e+67\n",
      "Gradient Descent(32/49): loss=1.3264259794243808e+70\n",
      "Gradient Descent(33/49): loss=2.310828696728486e+72\n",
      "Gradient Descent(34/49): loss=4.025802682137771e+74\n",
      "Gradient Descent(35/49): loss=7.013539021067455e+76\n",
      "Gradient Descent(36/49): loss=1.2218614145767173e+79\n",
      "Gradient Descent(37/49): loss=2.1286618808947476e+81\n",
      "Gradient Descent(38/49): loss=3.70844135768382e+83\n",
      "Gradient Descent(39/49): loss=6.460649024070985e+85\n",
      "Gradient Descent(40/49): loss=1.1255398639578066e+88\n",
      "Gradient Descent(41/49): loss=1.9608556054324905e+90\n",
      "Gradient Descent(42/49): loss=3.4160982018315925e+92\n",
      "Gradient Descent(43/49): loss=5.951344347960338e+94\n",
      "Gradient Descent(44/49): loss=1.0368115158109141e+97\n",
      "Gradient Descent(45/49): loss=1.8062778029077556e+99\n",
      "Gradient Descent(46/49): loss=3.146800986990846e+101\n",
      "Gradient Descent(47/49): loss=5.482189082867379e+103\n",
      "Gradient Descent(48/49): loss=9.550777842182495e+105\n",
      "Gradient Descent(49/49): loss=1.6638856488148406e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.481361047382133\n",
      "Gradient Descent(2/49): loss=827.6257931882974\n",
      "Gradient Descent(3/49): loss=143975.92201419311\n",
      "Gradient Descent(4/49): loss=25410029.002467107\n",
      "Gradient Descent(5/49): loss=4501881305.816977\n",
      "Gradient Descent(6/49): loss=798518136686.5668\n",
      "Gradient Descent(7/49): loss=141687838466390.62\n",
      "Gradient Descent(8/49): loss=2.5143776596444184e+16\n",
      "Gradient Descent(9/49): loss=4.462154483432653e+18\n",
      "Gradient Descent(10/49): loss=7.91888292428778e+20\n",
      "Gradient Descent(11/49): loss=1.405351420849509e+23\n",
      "Gradient Descent(12/49): loss=2.494057728154261e+25\n",
      "Gradient Descent(13/49): loss=4.426171570613819e+27\n",
      "Gradient Descent(14/49): loss=7.855069737197974e+29\n",
      "Gradient Descent(15/49): loss=1.3940291798552792e+32\n",
      "Gradient Descent(16/49): loss=2.4739657909883786e+34\n",
      "Gradient Descent(17/49): loss=4.390515529595381e+36\n",
      "Gradient Descent(18/49): loss=7.791791913346871e+38\n",
      "Gradient Descent(19/49): loss=1.382799373804456e+41\n",
      "Gradient Descent(20/49): loss=2.4540364139042695e+43\n",
      "Gradient Descent(21/49): loss=4.355147127745377e+45\n",
      "Gradient Descent(22/49): loss=7.729024067105755e+47\n",
      "Gradient Descent(23/49): loss=1.3716600444941395e+50\n",
      "Gradient Descent(24/49): loss=2.434267588416844e+52\n",
      "Gradient Descent(25/49): loss=4.320063645365877e+54\n",
      "Gradient Descent(26/49): loss=7.6667618584007365e+56\n",
      "Gradient Descent(27/49): loss=1.360610449720661e+59\n",
      "Gradient Descent(28/49): loss=2.4146580134879322e+61\n",
      "Gradient Descent(29/49): loss=4.2852627828182194e+63\n",
      "Gradient Descent(30/49): loss=7.605001211447412e+65\n",
      "Gradient Descent(31/49): loss=1.3496498664681823e+68\n",
      "Gradient Descent(32/49): loss=2.395206406168215e+70\n",
      "Gradient Descent(33/49): loss=4.250742263370938e+72\n",
      "Gradient Descent(34/49): loss=7.543738085818667e+74\n",
      "Gradient Descent(35/49): loss=1.3387775776906595e+77\n",
      "Gradient Descent(36/49): loss=2.375911493927184e+79\n",
      "Gradient Descent(37/49): loss=4.216499828681495e+81\n",
      "Gradient Descent(38/49): loss=7.482968473663155e+83\n",
      "Gradient Descent(39/49): loss=1.3279928721198692e+86\n",
      "Gradient Descent(40/49): loss=2.3567720144861932e+88\n",
      "Gradient Descent(41/49): loss=4.182533238600042e+90\n",
      "Gradient Descent(42/49): loss=7.422688399415699e+92\n",
      "Gradient Descent(43/49): loss=1.3172950442173135e+95\n",
      "Gradient Descent(44/49): loss=2.3377867157350864e+97\n",
      "Gradient Descent(45/49): loss=4.148840271022718e+99\n",
      "Gradient Descent(46/49): loss=7.362893919536843e+101\n",
      "Gradient Descent(47/49): loss=1.3066833941280783e+104\n",
      "Gradient Descent(48/49): loss=2.3189543556502772e+106\n",
      "Gradient Descent(49/49): loss=4.1154187217460407e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.458346903058175\n",
      "Gradient Descent(2/49): loss=821.9522307609461\n",
      "Gradient Descent(3/49): loss=142971.98339076378\n",
      "Gradient Descent(4/49): loss=25262382.777652524\n",
      "Gradient Descent(5/49): loss=4483216388.791353\n",
      "Gradient Descent(6/49): loss=796686289540.5475\n",
      "Gradient Descent(7/49): loss=141635173965162.7\n",
      "Gradient Descent(8/49): loss=2.5183464989928556e+16\n",
      "Gradient Descent(9/49): loss=4.4779550796211963e+18\n",
      "Gradient Descent(10/49): loss=7.962519898735892e+20\n",
      "Gradient Descent(11/49): loss=1.4158703578155316e+23\n",
      "Gradient Descent(12/49): loss=2.517660475448584e+25\n",
      "Gradient Descent(13/49): loss=4.476834813027366e+27\n",
      "Gradient Descent(14/49): loss=7.960586318201943e+29\n",
      "Gradient Descent(15/49): loss=1.4155299654203055e+32\n",
      "Gradient Descent(16/49): loss=2.5170572142428175e+34\n",
      "Gradient Descent(17/49): loss=4.4757632939362775e+36\n",
      "Gradient Descent(18/49): loss=7.958681667121884e+38\n",
      "Gradient Descent(19/49): loss=1.415191326338474e+41\n",
      "Gradient Descent(20/49): loss=2.516455079342594e+43\n",
      "Gradient Descent(21/49): loss=4.474692607974353e+45\n",
      "Gradient Descent(22/49): loss=7.956777810474544e+47\n",
      "Gradient Descent(23/49): loss=1.4148527881661799e+50\n",
      "Gradient Descent(24/49): loss=2.51585310017319e+52\n",
      "Gradient Descent(25/49): loss=4.4736221850053424e+54\n",
      "Gradient Descent(26/49): loss=7.95487441329559e+56\n",
      "Gradient Descent(27/49): loss=1.414514331214927e+59\n",
      "Gradient Descent(28/49): loss=2.5152512651466877e+61\n",
      "Gradient Descent(29/49): loss=4.472552018181592e+63\n",
      "Gradient Descent(30/49): loss=7.952971471489794e+65\n",
      "Gradient Descent(31/49): loss=1.4141759552311725e+68\n",
      "Gradient Descent(32/49): loss=2.514649574091049e+70\n",
      "Gradient Descent(33/49): loss=4.471482107360953e+72\n",
      "Gradient Descent(34/49): loss=7.951068984900732e+74\n",
      "Gradient Descent(35/49): loss=1.4138376601927703e+77\n",
      "Gradient Descent(36/49): loss=2.5140480269701936e+79\n",
      "Gradient Descent(37/49): loss=4.470412452481284e+81\n",
      "Gradient Descent(38/49): loss=7.949166953418954e+83\n",
      "Gradient Descent(39/49): loss=1.4134994460803073e+86\n",
      "Gradient Descent(40/49): loss=2.5134466237496857e+88\n",
      "Gradient Descent(41/49): loss=4.469343053481274e+90\n",
      "Gradient Descent(42/49): loss=7.947265376935573e+92\n",
      "Gradient Descent(43/49): loss=1.4131613128744478e+95\n",
      "Gradient Descent(44/49): loss=2.5128453643951044e+97\n",
      "Gradient Descent(45/49): loss=4.468273910299842e+99\n",
      "Gradient Descent(46/49): loss=7.94536425534178e+101\n",
      "Gradient Descent(47/49): loss=1.4128232605558193e+104\n",
      "Gradient Descent(48/49): loss=2.512244248872022e+106\n",
      "Gradient Descent(49/49): loss=4.467205022875696e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.480112813620922\n",
      "Gradient Descent(2/49): loss=819.0760753754524\n",
      "Gradient Descent(3/49): loss=141240.09615671937\n",
      "Gradient Descent(4/49): loss=24727738.66616354\n",
      "Gradient Descent(5/49): loss=4347232472.048042\n",
      "Gradient Descent(6/49): loss=765231175489.3428\n",
      "Gradient Descent(7/49): loss=134756193605750.56\n",
      "Gradient Descent(8/49): loss=2.3733534619774416e+16\n",
      "Gradient Descent(9/49): loss=4.180181377594926e+18\n",
      "Gradient Descent(10/49): loss=7.362649682654454e+20\n",
      "Gradient Descent(11/49): loss=1.2968066953625435e+23\n",
      "Gradient Descent(12/49): loss=2.2841101745136504e+25\n",
      "Gradient Descent(13/49): loss=4.0230838532168665e+27\n",
      "Gradient Descent(14/49): loss=7.086000819507219e+29\n",
      "Gradient Descent(15/49): loss=1.2480826256316785e+32\n",
      "Gradient Descent(16/49): loss=2.1982925359162564e+34\n",
      "Gradient Descent(17/49): loss=3.8719312375034505e+36\n",
      "Gradient Descent(18/49): loss=6.819770933920903e+38\n",
      "Gradient Descent(19/49): loss=1.2011906403315981e+41\n",
      "Gradient Descent(20/49): loss=2.1157000268041524e+43\n",
      "Gradient Descent(21/49): loss=3.7264581103829106e+45\n",
      "Gradient Descent(22/49): loss=6.56354392063189e+47\n",
      "Gradient Descent(23/49): loss=1.1560604606936438e+50\n",
      "Gradient Descent(24/49): loss=2.0362106278927833e+52\n",
      "Gradient Descent(25/49): loss=3.586450589841926e+54\n",
      "Gradient Descent(26/49): loss=6.3169436683929135e+56\n",
      "Gradient Descent(27/49): loss=1.1126258764772935e+59\n",
      "Gradient Descent(28/49): loss=1.9597077415791637e+61\n",
      "Gradient Descent(29/49): loss=3.451703320584912e+63\n",
      "Gradient Descent(30/49): loss=6.079608484751022e+65\n",
      "Gradient Descent(31/49): loss=1.0708231819179947e+68\n",
      "Gradient Descent(32/49): loss=1.886079160868769e+70\n",
      "Gradient Descent(33/49): loss=3.322018668564699e+72\n",
      "Gradient Descent(34/49): loss=5.851190269876588e+74\n",
      "Gradient Descent(35/49): loss=1.0305910649530081e+77\n",
      "Gradient Descent(36/49): loss=1.8152168946359915e+79\n",
      "Gradient Descent(37/49): loss=3.1972064251519384e+81\n",
      "Gradient Descent(38/49): loss=5.631354002510358e+83\n",
      "Gradient Descent(39/49): loss=9.918705170900091e+85\n",
      "Gradient Descent(40/49): loss=1.7470170091133252e+88\n",
      "Gradient Descent(41/49): loss=3.0770835280854455e+90\n",
      "Gradient Descent(42/49): loss=5.419777248545705e+92\n",
      "Gradient Descent(43/49): loss=9.546047468568455e+94\n",
      "Gradient Descent(44/49): loss=1.6813794754501197e+97\n",
      "Gradient Descent(45/49): loss=2.9614737929737763e+99\n",
      "Gradient Descent(46/49): loss=5.216149688114008e+101\n",
      "Gradient Descent(47/49): loss=9.187390965054134e+103\n",
      "Gradient Descent(48/49): loss=1.6182080230001576e+106\n",
      "Gradient Descent(49/49): loss=2.850207654885223e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.467494120645915\n",
      "Gradient Descent(2/49): loss=818.1317500258768\n",
      "Gradient Descent(3/49): loss=141044.6168364701\n",
      "Gradient Descent(4/49): loss=24685197.70620946\n",
      "Gradient Descent(5/49): loss=4338706440.623788\n",
      "Gradient Descent(6/49): loss=763597239898.055\n",
      "Gradient Descent(7/49): loss=134449435421081.06\n",
      "Gradient Descent(8/49): loss=2.367649651538651e+16\n",
      "Gradient Descent(9/49): loss=4.1696295326457216e+18\n",
      "Gradient Descent(10/49): loss=7.343191003926827e+20\n",
      "Gradient Descent(11/49): loss=1.2932267224580547e+23\n",
      "Gradient Descent(12/49): loss=2.2775365898540533e+25\n",
      "Gradient Descent(13/49): loss=4.011034008790293e+27\n",
      "Gradient Descent(14/49): loss=7.063946867636458e+29\n",
      "Gradient Descent(15/49): loss=1.2440520096345978e+32\n",
      "Gradient Descent(16/49): loss=2.1909358497229785e+34\n",
      "Gradient Descent(17/49): loss=3.8585203056769874e+36\n",
      "Gradient Descent(18/49): loss=6.795351400411053e+38\n",
      "Gradient Descent(19/49): loss=1.1967489359841221e+41\n",
      "Gradient Descent(20/49): loss=2.1076290715413783e+43\n",
      "Gradient Descent(21/49): loss=3.7118063531740966e+45\n",
      "Gradient Descent(22/49): loss=6.536969237168284e+47\n",
      "Gradient Descent(23/49): loss=1.1512445085325744e+50\n",
      "Gradient Descent(24/49): loss=2.0274899121398191e+52\n",
      "Gradient Descent(25/49): loss=3.570670968129148e+54\n",
      "Gradient Descent(26/49): loss=6.288411639588038e+56\n",
      "Gradient Descent(27/49): loss=1.1074703130552743e+59\n",
      "Gradient Descent(28/49): loss=1.9503979138032058e+61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=3.4349020261079556e+63\n",
      "Gradient Descent(30/49): loss=6.049305039479819e+65\n",
      "Gradient Descent(31/49): loss=1.0653605599965278e+68\n",
      "Gradient Descent(32/49): loss=1.8762372130166494e+70\n",
      "Gradient Descent(33/49): loss=3.3042954767538726e+72\n",
      "Gradient Descent(34/49): loss=5.819290078007389e+74\n",
      "Gradient Descent(35/49): loss=1.0248519616430713e+77\n",
      "Gradient Descent(36/49): loss=1.8048963519675468e+79\n",
      "Gradient Descent(37/49): loss=3.1786550285009207e+81\n",
      "Gradient Descent(38/49): loss=5.59802106043365e+83\n",
      "Gradient Descent(39/49): loss=9.858836366976764e+85\n",
      "Gradient Descent(40/49): loss=1.736268110847272e+88\n",
      "Gradient Descent(41/49): loss=3.0577918534513743e+90\n",
      "Gradient Descent(42/49): loss=5.385165436500959e+92\n",
      "Gradient Descent(43/49): loss=9.483970187752231e+94\n",
      "Gradient Descent(44/49): loss=1.6702493467056135e+97\n",
      "Gradient Descent(45/49): loss=2.9415243035804114e+99\n",
      "Gradient Descent(46/49): loss=5.180403300633213e+101\n",
      "Gradient Descent(47/49): loss=9.12335768381927e+103\n",
      "Gradient Descent(48/49): loss=1.606740838434916e+106\n",
      "Gradient Descent(49/49): loss=2.8296776377332674e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.537846676370805\n",
      "Gradient Descent(2/49): loss=845.7728142126772\n",
      "Gradient Descent(3/49): loss=148732.70740533015\n",
      "Gradient Descent(4/49): loss=26535125.214037262\n",
      "Gradient Descent(5/49): loss=4752386986.5903635\n",
      "Gradient Descent(6/49): loss=852128873500.3628\n",
      "Gradient Descent(7/49): loss=152846742805110.44\n",
      "Gradient Descent(8/49): loss=2.741936458953641e+16\n",
      "Gradient Descent(9/49): loss=4.918977076509903e+18\n",
      "Gradient Descent(10/49): loss=8.824649397668006e+20\n",
      "Gradient Descent(11/49): loss=1.5831491184511686e+23\n",
      "Gradient Descent(12/49): loss=2.8401857191384234e+25\n",
      "Gradient Descent(13/49): loss=5.095324348590875e+27\n",
      "Gradient Descent(14/49): loss=9.141069023767296e+29\n",
      "Gradient Descent(15/49): loss=1.6399181056869097e+32\n",
      "Gradient Descent(16/49): loss=2.942031642467421e+34\n",
      "Gradient Descent(17/49): loss=5.2780380892265825e+36\n",
      "Gradient Descent(18/49): loss=9.468860127386883e+38\n",
      "Gradient Descent(19/49): loss=1.6987242350230274e+41\n",
      "Gradient Descent(20/49): loss=3.047530524568795e+43\n",
      "Gradient Descent(21/49): loss=5.467304290579526e+45\n",
      "Gradient Descent(22/49): loss=9.808405843782188e+47\n",
      "Gradient Descent(23/49): loss=1.7596391216531684e+50\n",
      "Gradient Descent(24/49): loss=3.1568125215990915e+52\n",
      "Gradient Descent(25/49): loss=5.663357431589723e+54\n",
      "Gradient Descent(26/49): loss=1.0160127400185715e+57\n",
      "Gradient Descent(27/49): loss=1.822738367389869e+59\n",
      "Gradient Descent(28/49): loss=3.2700132833909267e+61\n",
      "Gradient Descent(29/49): loss=5.8664408808519195e+63\n",
      "Gradient Descent(30/49): loss=1.0524461409172918e+66\n",
      "Gradient Descent(31/49): loss=1.8881003014060091e+68\n",
      "Gradient Descent(32/49): loss=3.387273333590594e+70\n",
      "Gradient Descent(33/49): loss=6.076806739509443e+72\n",
      "Gradient Descent(34/49): loss=1.0901860143126815e+75\n",
      "Gradient Descent(35/49): loss=1.9558060618839302e+77\n",
      "Gradient Descent(36/49): loss=3.508738235019041e+79\n",
      "Gradient Descent(37/49): loss=6.294716148913931e+81\n",
      "Gradient Descent(38/49): loss=1.1292792092591866e+84\n",
      "Gradient Descent(39/49): loss=2.0259396965581721e+86\n",
      "Gradient Descent(40/49): loss=3.634558770264398e+88\n",
      "Gradient Descent(41/49): loss=6.520439614736859e+90\n",
      "Gradient Descent(42/49): loss=1.1697742547807845e+93\n",
      "Gradient Descent(43/49): loss=2.0985882670476206e+95\n",
      "Gradient Descent(44/49): loss=3.764891128857387e+97\n",
      "Gradient Descent(45/49): loss=6.754257342766047e+99\n",
      "Gradient Descent(46/49): loss=1.2117214201132665e+102\n",
      "Gradient Descent(47/49): loss=2.1738419569308542e+104\n",
      "Gradient Descent(48/49): loss=3.899897101159779e+106\n",
      "Gradient Descent(49/49): loss=6.99645958674366e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.514543636999788\n",
      "Gradient Descent(2/49): loss=839.97592951564\n",
      "Gradient Descent(3/49): loss=147695.5397694848\n",
      "Gradient Descent(4/49): loss=26380887.240381453\n",
      "Gradient Descent(5/49): loss=4732667897.065705\n",
      "Gradient Descent(6/49): loss=850170417434.5737\n",
      "Gradient Descent(7/49): loss=152789150859633.4\n",
      "Gradient Descent(8/49): loss=2.746248324318706e+16\n",
      "Gradient Descent(9/49): loss=4.936362494927057e+18\n",
      "Gradient Descent(10/49): loss=8.873212014345227e+20\n",
      "Gradient Descent(11/49): loss=1.594985883800529e+23\n",
      "Gradient Descent(12/49): loss=2.8670386390708456e+25\n",
      "Gradient Descent(13/49): loss=5.153597361596637e+27\n",
      "Gradient Descent(14/49): loss=9.263764426603997e+29\n",
      "Gradient Descent(15/49): loss=1.6651928826486835e+32\n",
      "Gradient Descent(16/49): loss=2.993240397041463e+34\n",
      "Gradient Descent(17/49): loss=5.380450652993045e+36\n",
      "Gradient Descent(18/49): loss=9.671541691137133e+38\n",
      "Gradient Descent(19/49): loss=1.7384922710496634e+41\n",
      "Gradient Descent(20/49): loss=3.124998551140495e+43\n",
      "Gradient Descent(21/49): loss=5.61729040042988e+45\n",
      "Gradient Descent(22/49): loss=1.0097269143390878e+48\n",
      "Gradient Descent(23/49): loss=1.8150182185193134e+50\n",
      "Gradient Descent(24/49): loss=3.262556525714201e+52\n",
      "Gradient Descent(25/49): loss=5.8645555041171065e+54\n",
      "Gradient Descent(26/49): loss=1.0541736515462116e+57\n",
      "Gradient Descent(27/49): loss=1.8949127292498728e+59\n",
      "Gradient Descent(28/49): loss=3.406169606123875e+61\n",
      "Gradient Descent(29/49): loss=6.122704864764407e+63\n",
      "Gradient Descent(30/49): loss=1.1005768706764215e+66\n",
      "Gradient Descent(31/49): loss=1.978324082283734e+68\n",
      "Gradient Descent(32/49): loss=3.5561043293035415e+70\n",
      "Gradient Descent(33/49): loss=6.392217591716949e+72\n",
      "Gradient Descent(34/49): loss=1.149022693264401e+75\n",
      "Gradient Descent(35/49): loss=2.0654070839944066e+77\n",
      "Gradient Descent(36/49): loss=3.7126389649404796e+79\n",
      "Gradient Descent(37/49): loss=6.673593884135051e+81\n",
      "Gradient Descent(38/49): loss=1.1996010317981136e+84\n",
      "Gradient Descent(39/49): loss=2.1563233551147e+86\n",
      "Gradient Descent(40/49): loss=3.876064031758531e+88\n",
      "Gradient Descent(41/49): loss=6.967355959233231e+90\n",
      "Gradient Descent(42/49): loss=1.2524057565849604e+93\n",
      "Gradient Descent(43/49): loss=2.251241630691371e+95\n",
      "Gradient Descent(44/49): loss=4.046682836701028e+97\n",
      "Gradient Descent(45/49): loss=7.274049021482285e+99\n",
      "Gradient Descent(46/49): loss=1.3075348699692554e+102\n",
      "Gradient Descent(47/49): loss=2.3503380732470566e+104\n",
      "Gradient Descent(48/49): loss=4.224812037849964e+106\n",
      "Gradient Descent(49/49): loss=7.594242274475419e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.53660038955002\n",
      "Gradient Descent(2/49): loss=837.0419532011097\n",
      "Gradient Descent(3/49): loss=145907.91026895464\n",
      "Gradient Descent(4/49): loss=25822934.383986104\n",
      "Gradient Descent(5/49): loss=4589199513.436763\n",
      "Gradient Descent(6/49): loss=816621183181.5309\n",
      "Gradient Descent(7/49): loss=145372094079786.56\n",
      "Gradient Descent(8/49): loss=2.5882082788204468e+16\n",
      "Gradient Descent(9/49): loss=4.6082547407205417e+18\n",
      "Gradient Descent(10/49): loss=8.205027985460265e+20\n",
      "Gradient Descent(11/49): loss=1.4609177314448523e+23\n",
      "Gradient Descent(12/49): loss=2.6011904720332944e+25\n",
      "Gradient Descent(13/49): loss=4.6314692206687086e+27\n",
      "Gradient Descent(14/49): loss=8.24642071037322e+29\n",
      "Gradient Descent(15/49): loss=1.4682912774575267e+32\n",
      "Gradient Descent(16/49): loss=2.6143212843120775e+34\n",
      "Gradient Descent(17/49): loss=4.654850115728114e+36\n",
      "Gradient Descent(18/49): loss=8.288051578676879e+38\n",
      "Gradient Descent(19/49): loss=1.4757037780678601e+41\n",
      "Gradient Descent(20/49): loss=2.6275194120877854e+43\n",
      "Gradient Descent(21/49): loss=4.678349655307674e+45\n",
      "Gradient Descent(22/49): loss=8.329892977087291e+47\n",
      "Gradient Descent(23/49): loss=1.4831537213477396e+50\n",
      "Gradient Descent(24/49): loss=2.6407841819877143e+52\n",
      "Gradient Descent(25/49): loss=4.701967837498637e+54\n",
      "Gradient Descent(26/49): loss=8.371945612093902e+56\n",
      "Gradient Descent(27/49): loss=1.4906412751889972e+59\n",
      "Gradient Descent(28/49): loss=2.6541159179143445e+61\n",
      "Gradient Descent(29/49): loss=4.725705253823234e+63\n",
      "Gradient Descent(30/49): loss=8.414210545695233e+65\n",
      "Gradient Descent(31/49): loss=1.4981666292033535e+68\n",
      "Gradient Descent(32/49): loss=2.6675149577839367e+70\n",
      "Gradient Descent(33/49): loss=4.749562506131082e+72\n",
      "Gradient Descent(34/49): loss=8.456688849605037e+74\n",
      "Gradient Descent(35/49): loss=1.505729974217977e+77\n",
      "Gradient Descent(36/49): loss=2.6809816413718023e+79\n",
      "Gradient Descent(37/49): loss=4.7735401994010735e+81\n",
      "Gradient Descent(38/49): loss=8.499381601001403e+83\n",
      "Gradient Descent(39/49): loss=1.513331502026634e+86\n",
      "Gradient Descent(40/49): loss=2.6945163101705597e+88\n",
      "Gradient Descent(41/49): loss=4.797638941667502e+90\n",
      "Gradient Descent(42/49): loss=8.542289882501225e+92\n",
      "Gradient Descent(43/49): loss=1.520971405391345e+95\n",
      "Gradient Descent(44/49): loss=2.7081193073967338e+97\n",
      "Gradient Descent(45/49): loss=4.821859344034108e+99\n",
      "Gradient Descent(46/49): loss=8.585414782186646e+101\n",
      "Gradient Descent(47/49): loss=1.5286498780472908e+104\n",
      "Gradient Descent(48/49): loss=2.7217909779995775e+106\n",
      "Gradient Descent(49/49): loss=4.846202020689784e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.52351318194783\n",
      "Gradient Descent(2/49): loss=835.9855434345596\n",
      "Gradient Descent(3/49): loss=145682.28595749545\n",
      "Gradient Descent(4/49): loss=25772923.326949608\n",
      "Gradient Descent(5/49): loss=4578957644.273879\n",
      "Gradient Descent(6/49): loss=814612422214.5872\n",
      "Gradient Descent(7/49): loss=144986112836763.25\n",
      "Gradient Descent(8/49): loss=2.5808679314120496e+16\n",
      "Gradient Descent(9/49): loss=4.594378821827663e+18\n",
      "Gradient Descent(10/49): loss=8.178905165062612e+20\n",
      "Gradient Descent(11/49): loss=1.4560156564116534e+23\n",
      "Gradient Descent(12/49): loss=2.5920165431712423e+25\n",
      "Gradient Descent(13/49): loss=4.614342035926132e+27\n",
      "Gradient Descent(14/49): loss=8.214514419920451e+29\n",
      "Gradient Descent(15/49): loss=1.462359137280847e+32\n",
      "Gradient Descent(16/49): loss=2.60331188622199e+34\n",
      "Gradient Descent(17/49): loss=4.6344517329446576e+36\n",
      "Gradient Descent(18/49): loss=8.250314933278529e+38\n",
      "Gradient Descent(19/49): loss=1.4687324519005163e+41\n",
      "Gradient Descent(20/49): loss=2.6146577839504937e+43\n",
      "Gradient Descent(21/49): loss=4.65464987796535e+45\n",
      "Gradient Descent(22/49): loss=8.28627196275137e+47\n",
      "Gradient Descent(23/49): loss=1.4751335727030205e+50\n",
      "Gradient Descent(24/49): loss=2.626053148035354e+52\n",
      "Gradient Descent(25/49): loss=4.674936062692405e+54\n",
      "Gradient Descent(26/49): loss=8.322385708996278e+56\n",
      "Gradient Descent(27/49): loss=1.4815625916695142e+59\n",
      "Gradient Descent(28/49): loss=2.6374981763484833e+61\n",
      "Gradient Descent(29/49): loss=4.695310660080062e+63\n",
      "Gradient Descent(30/49): loss=8.358656848507669e+65\n",
      "Gradient Descent(31/49): loss=1.4880196299921224e+68\n",
      "Gradient Descent(32/49): loss=2.6489930850998168e+70\n",
      "Gradient Descent(33/49): loss=4.715774055308523e+72\n",
      "Gradient Descent(34/49): loss=8.395086067158651e+74\n",
      "Gradient Descent(35/49): loss=1.4945048097812271e+77\n",
      "Gradient Descent(36/49): loss=2.6605380916781597e+79\n",
      "Gradient Descent(37/49): loss=4.736326635380065e+81\n",
      "Gradient Descent(38/49): loss=8.431674053898216e+83\n",
      "Gradient Descent(39/49): loss=1.5010182536846034e+86\n",
      "Gradient Descent(40/49): loss=2.6721334144228657e+88\n",
      "Gradient Descent(41/49): loss=4.75696878898555e+90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=8.468421500679426e+92\n",
      "Gradient Descent(43/49): loss=1.507560084884713e+95\n",
      "Gradient Descent(44/49): loss=2.6837792726250827e+97\n",
      "Gradient Descent(45/49): loss=4.77770090650999e+99\n",
      "Gradient Descent(46/49): loss=8.505329102470933e+101\n",
      "Gradient Descent(47/49): loss=1.514130427100816e+104\n",
      "Gradient Descent(48/49): loss=2.6954758865315043e+106\n",
      "Gradient Descent(49/49): loss=4.798523380039711e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.594647151991239\n",
      "Gradient Descent(2/49): loss=864.2168340357284\n",
      "Gradient Descent(3/49): loss=153619.74746699102\n",
      "Gradient Descent(4/49): loss=27703572.37213061\n",
      "Gradient Descent(5/49): loss=5015369000.15936\n",
      "Gradient Descent(6/49): loss=909020729974.6233\n",
      "Gradient Descent(7/49): loss=164817181030045.28\n",
      "Gradient Descent(8/49): loss=2.9886951101796576e+16\n",
      "Gradient Descent(9/49): loss=5.419722047337344e+18\n",
      "Gradient Descent(10/49): loss=9.828283720826471e+20\n",
      "Gradient Descent(11/49): loss=1.7822969588151374e+23\n",
      "Gradient Descent(12/49): loss=3.232086701160342e+25\n",
      "Gradient Descent(13/49): loss=5.861194303751429e+27\n",
      "Gradient Descent(14/49): loss=1.0628923812483263e+30\n",
      "Gradient Descent(15/49): loss=1.9274915744116934e+32\n",
      "Gradient Descent(16/49): loss=3.495390396899662e+34\n",
      "Gradient Descent(17/49): loss=6.338680928874858e+36\n",
      "Gradient Descent(18/49): loss=1.149481786473627e+39\n",
      "Gradient Descent(19/49): loss=2.08451631010687e+41\n",
      "Gradient Descent(20/49): loss=3.7801453654360887e+43\n",
      "Gradient Descent(21/49): loss=6.85506700776677e+45\n",
      "Gradient Descent(22/49): loss=1.243125307069736e+48\n",
      "Gradient Descent(23/49): loss=2.2543332214473702e+50\n",
      "Gradient Descent(24/49): loss=4.0880981542476107e+52\n",
      "Gradient Descent(25/49): loss=7.413520929280498e+54\n",
      "Gradient Descent(26/49): loss=1.3443975779247298e+57\n",
      "Gradient Descent(27/49): loss=2.437984413575261e+59\n",
      "Gradient Descent(28/49): loss=4.421138581647159e+61\n",
      "Gradient Descent(29/49): loss=8.017469779252881e+63\n",
      "Gradient Descent(30/49): loss=1.4539200813127407e+66\n",
      "Gradient Descent(31/49): loss=2.6365969078107726e+68\n",
      "Gradient Descent(32/49): loss=4.7813104335148296e+70\n",
      "Gradient Descent(33/49): loss=8.670619841020643e+72\n",
      "Gradient Descent(34/49): loss=1.5723649295081454e+75\n",
      "Gradient Descent(35/49): loss=2.851389539477386e+77\n",
      "Gradient Descent(36/49): loss=5.1708239946463315e+79\n",
      "Gradient Descent(37/49): loss=9.376979333560613e+81\n",
      "Gradient Descent(38/49): loss=1.7004589889940035e+84\n",
      "Gradient Descent(39/49): loss=3.083680437368022e+86\n",
      "Gradient Descent(40/49): loss=5.5920696125884686e+88\n",
      "Gradient Descent(41/49): loss=1.014088300885223e+91\n",
      "Gradient Descent(42/49): loss=1.8389883410558239e+93\n",
      "Gradient Descent(43/49): loss=3.3348951127698904e+95\n",
      "Gradient Descent(44/49): loss=6.047632366603896e+97\n",
      "Gradient Descent(45/49): loss=1.096701875316787e+100\n",
      "Gradient Descent(46/49): loss=1.9888031057661345e+102\n",
      "Gradient Descent(47/49): loss=3.606575207471475e+104\n",
      "Gradient Descent(48/49): loss=6.540307931657804e+106\n",
      "Gradient Descent(49/49): loss=1.1860456355461735e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.571053482276734\n",
      "Gradient Descent(2/49): loss=858.2946283015739\n",
      "Gradient Descent(3/49): loss=152548.43958398522\n",
      "Gradient Descent(4/49): loss=27542486.623779602\n",
      "Gradient Descent(5/49): loss=4994542468.86091\n",
      "Gradient Descent(6/49): loss=906927696131.5222\n",
      "Gradient Descent(7/49): loss=164754246939373.78\n",
      "Gradient Descent(8/49): loss=2.9933775877538036e+16\n",
      "Gradient Descent(9/49): loss=5.438841529386888e+18\n",
      "Gradient Descent(10/49): loss=9.882297148459141e+20\n",
      "Gradient Descent(11/49): loss=1.7956082499289937e+23\n",
      "Gradient Descent(12/49): loss=3.2626162608309655e+25\n",
      "Gradient Descent(13/49): loss=5.928169858606959e+27\n",
      "Gradient Descent(14/49): loss=1.07714795238237e+30\n",
      "Gradient Descent(15/49): loss=1.9571770185335407e+32\n",
      "Gradient Descent(16/49): loss=3.5561892397360036e+34\n",
      "Gradient Descent(17/49): loss=6.461593341851056e+36\n",
      "Gradient Descent(18/49): loss=1.1740710572853633e+39\n",
      "Gradient Descent(19/49): loss=2.1332862896464488e+41\n",
      "Gradient Descent(20/49): loss=3.8761796966494784e+43\n",
      "Gradient Descent(21/49): loss=7.043015799059311e+45\n",
      "Gradient Descent(22/49): loss=1.2797154783603918e+48\n",
      "Gradient Descent(23/49): loss=2.3252421296399033e+50\n",
      "Gradient Descent(24/49): loss=4.224963324187485e+52\n",
      "Gradient Descent(25/49): loss=7.676755406762918e+54\n",
      "Gradient Descent(26/49): loss=1.3948659207974384e+57\n",
      "Gradient Descent(27/49): loss=2.5344704030667125e+59\n",
      "Gradient Descent(28/49): loss=4.605130950757631e+61\n",
      "Gradient Descent(29/49): loss=8.367519718504231e+63\n",
      "Gradient Descent(30/49): loss=1.5203777479560879e+66\n",
      "Gradient Descent(31/49): loss=2.762525305280368e+68\n",
      "Gradient Descent(32/49): loss=5.019506548667752e+70\n",
      "Gradient Descent(33/49): loss=9.120439890255257e+72\n",
      "Gradient Descent(34/49): loss=1.6571832905335445e+75\n",
      "Gradient Descent(35/49): loss=3.011100880515466e+77\n",
      "Gradient Descent(36/49): loss=5.4711681951137e+79\n",
      "Gradient Descent(37/49): loss=9.941108786132492e+81\n",
      "Gradient Descent(38/49): loss=1.8062987715490547e+84\n",
      "Gradient Descent(39/49): loss=3.2820436052877595e+86\n",
      "Gradient Descent(40/49): loss=5.9634709366339e+88\n",
      "Gradient Descent(41/49): loss=1.083562252335128e+91\n",
      "Gradient Descent(42/49): loss=1.968831855074495e+93\n",
      "Gradient Descent(43/49): loss=3.5773661044415876e+95\n",
      "Gradient Descent(44/49): loss=6.50007171116367e+97\n",
      "Gradient Descent(45/49): loss=1.1810625755583727e+100\n",
      "Gradient Descent(46/49): loss=2.1459898742176388e+102\n",
      "Gradient Descent(47/49): loss=3.8992621013898315e+104\n",
      "Gradient Descent(48/49): loss=7.084956512610853e+106\n",
      "Gradient Descent(49/49): loss=1.2873361030974225e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.593402895037027\n",
      "Gradient Descent(2/49): loss=855.3019705242833\n",
      "Gradient Descent(3/49): loss=150703.58158421674\n",
      "Gradient Descent(4/49): loss=26960340.854585655\n",
      "Gradient Descent(5/49): loss=4843220933.883087\n",
      "Gradient Descent(6/49): loss=871157253747.7131\n",
      "Gradient Descent(7/49): loss=156760249012533.03\n",
      "Gradient Descent(8/49): loss=2.821196035991501e+16\n",
      "Gradient Descent(9/49): loss=5.077497634266046e+18\n",
      "Gradient Descent(10/49): loss=9.138450223520274e+20\n",
      "Gradient Descent(11/49): loss=1.6447408772078284e+23\n",
      "Gradient Descent(12/49): loss=2.96021412121449e+25\n",
      "Gradient Descent(13/49): loss=5.327813334536801e+27\n",
      "Gradient Descent(14/49): loss=9.589036088341634e+29\n",
      "Gradient Descent(15/49): loss=1.7258415947779675e+32\n",
      "Gradient Descent(16/49): loss=3.106182146330886e+34\n",
      "Gradient Descent(17/49): loss=5.590529061104181e+36\n",
      "Gradient Descent(18/49): loss=1.0061874603640438e+39\n",
      "Gradient Descent(19/49): loss=1.8109434636282307e+41\n",
      "Gradient Descent(20/49): loss=3.259349135677027e+43\n",
      "Gradient Descent(21/49): loss=5.866200134060395e+45\n",
      "Gradient Descent(22/49): loss=1.0558029404433486e+48\n",
      "Gradient Descent(23/49): loss=1.9002417639771565e+50\n",
      "Gradient Descent(24/49): loss=3.420068862526307e+52\n",
      "Gradient Descent(25/49): loss=6.155464660429599e+54\n",
      "Gradient Descent(26/49): loss=1.1078649790057897e+57\n",
      "Gradient Descent(27/49): loss=1.9939433973161291e+59\n",
      "Gradient Descent(28/49): loss=3.588713739528679e+61\n",
      "Gradient Descent(29/49): loss=6.458992929095771e+63\n",
      "Gradient Descent(30/49): loss=1.1624942162031736e+66\n",
      "Gradient Descent(31/49): loss=2.0922654933065746e+68\n",
      "Gradient Descent(32/49): loss=3.765674558604722e+70\n",
      "Gradient Descent(33/49): loss=6.777488290414126e+72\n",
      "Gradient Descent(34/49): loss=1.2198172415547405e+75\n",
      "Gradient Descent(35/49): loss=2.1954358886886224e+77\n",
      "Gradient Descent(36/49): loss=3.951361381971174e+79\n",
      "Gradient Descent(37/49): loss=7.11168877732946e+81\n",
      "Gradient Descent(38/49): loss=1.2799668867635478e+84\n",
      "Gradient Descent(39/49): loss=2.3036936549216854e+86\n",
      "Gradient Descent(40/49): loss=4.1462044921688976e+88\n",
      "Gradient Descent(41/49): loss=7.4623688154690825e+90\n",
      "Gradient Descent(42/49): loss=1.3430825335138058e+93\n",
      "Gradient Descent(43/49): loss=2.4172896521685397e+95\n",
      "Gradient Descent(44/49): loss=4.350655389132141e+97\n",
      "Gradient Descent(45/49): loss=7.830341017678282e+99\n",
      "Gradient Descent(46/49): loss=1.4093104364527982e+102\n",
      "Gradient Descent(47/49): loss=2.5364871106005277e+104\n",
      "Gradient Descent(48/49): loss=4.565187836426103e+106\n",
      "Gradient Descent(49/49): loss=8.216458067046327e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.579842811042631\n",
      "Gradient Descent(2/49): loss=854.1301601254644\n",
      "Gradient Descent(3/49): loss=150446.34454502774\n",
      "Gradient Descent(4/49): loss=26902358.765859976\n",
      "Gradient Descent(5/49): loss=4831116563.137505\n",
      "Gradient Descent(6/49): loss=868734664209.2964\n",
      "Gradient Descent(7/49): loss=156285319203998.2\n",
      "Gradient Descent(8/49): loss=2.8119875071616344e+16\n",
      "Gradient Descent(9/49): loss=5.059764505114464e+18\n",
      "Gradient Descent(10/49): loss=9.104468544789892e+20\n",
      "Gradient Descent(11/49): loss=1.638254642476455e+23\n",
      "Gradient Descent(12/49): loss=2.947875045098531e+25\n",
      "Gradient Descent(13/49): loss=5.304409273326237e+27\n",
      "Gradient Descent(14/49): loss=9.544761480655874e+29\n",
      "Gradient Descent(15/49): loss=1.7174858525803144e+32\n",
      "Gradient Descent(16/49): loss=3.0904467736247188e+34\n",
      "Gradient Descent(17/49): loss=5.560954888148564e+36\n",
      "Gradient Descent(18/49): loss=1.0006391188343387e+39\n",
      "Gradient Descent(19/49): loss=1.8005516450617784e+41\n",
      "Gradient Descent(20/49): loss=3.2399155366586104e+43\n",
      "Gradient Descent(21/49): loss=5.829909247400482e+45\n",
      "Gradient Descent(22/49): loss=1.0490348112407203e+48\n",
      "Gradient Descent(23/49): loss=1.8876349330844512e+50\n",
      "Gradient Descent(24/49): loss=3.396613346323374e+52\n",
      "Gradient Descent(25/49): loss=6.111871539476843e+54\n",
      "Gradient Descent(26/49): loss=1.0997711516243854e+57\n",
      "Gradient Descent(27/49): loss=1.9789299859024633e+59\n",
      "Gradient Descent(28/49): loss=3.560889811775765e+61\n",
      "Gradient Descent(29/49): loss=6.407470876654701e+63\n",
      "Gradient Descent(30/49): loss=1.152961344083388e+66\n",
      "Gradient Descent(31/49): loss=2.0746405040932639e+68\n",
      "Gradient Descent(32/49): loss=3.7331114727407917e+70\n",
      "Gradient Descent(33/49): loss=6.717366811461061e+72\n",
      "Gradient Descent(34/49): loss=1.2087240686276606e+75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=2.1749800406716607e+77\n",
      "Gradient Descent(36/49): loss=3.913662596866295e+79\n",
      "Gradient Descent(37/49): loss=7.042250795726909e+81\n",
      "Gradient Descent(38/49): loss=1.2671837452116144e+84\n",
      "Gradient Descent(39/49): loss=2.28017247710473e+86\n",
      "Gradient Descent(40/49): loss=4.102946036825684e+88\n",
      "Gradient Descent(41/49): loss=7.382847723203337e+90\n",
      "Gradient Descent(42/49): loss=1.328470811333862e+93\n",
      "Gradient Descent(43/49): loss=2.3904525228380364e+95\n",
      "Gradient Descent(44/49): loss=4.301384129174278e+97\n",
      "Gradient Descent(45/49): loss=7.739917546969812e+99\n",
      "Gradient Descent(46/49): loss=1.3927220130742259e+102\n",
      "Gradient Descent(47/49): loss=2.50606623898843e+104\n",
      "Gradient Descent(48/49): loss=4.509419636683007e+106\n",
      "Gradient Descent(49/49): loss=8.114256975071243e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.651762474243439\n",
      "Gradient Descent(2/49): loss=882.9610764242083\n",
      "Gradient Descent(3/49): loss=158639.88020312702\n",
      "Gradient Descent(4/49): loss=28916794.4120059\n",
      "Gradient Descent(5/49): loss=5291376660.0175705\n",
      "Gradient Descent(6/49): loss=969375259664.1172\n",
      "Gradient Descent(7/49): loss=177653331242295.78\n",
      "Gradient Descent(8/49): loss=3.2561571781737796e+16\n",
      "Gradient Descent(9/49): loss=5.968341856761348e+18\n",
      "Gradient Descent(10/49): loss=1.0939747841766453e+21\n",
      "Gradient Descent(11/49): loss=2.0052228750568577e+23\n",
      "Gradient Descent(12/49): loss=3.675517940503453e+25\n",
      "Gradient Descent(13/49): loss=6.737125304853282e+27\n",
      "Gradient Descent(14/49): loss=1.2348970749300395e+30\n",
      "Gradient Descent(15/49): loss=2.2635334567423704e+32\n",
      "Gradient Descent(16/49): loss=4.148996614638663e+34\n",
      "Gradient Descent(17/49): loss=7.605000463903838e+36\n",
      "Gradient Descent(18/49): loss=1.39397636381851e+39\n",
      "Gradient Descent(19/49): loss=2.5551216111896416e+41\n",
      "Gradient Descent(20/49): loss=4.683469977303616e+43\n",
      "Gradient Descent(21/49): loss=8.584675943950352e+45\n",
      "Gradient Descent(22/49): loss=1.573548275552689e+48\n",
      "Gradient Descent(23/49): loss=2.8842721515345636e+50\n",
      "Gradient Descent(24/49): loss=5.286794166649098e+52\n",
      "Gradient Descent(25/49): loss=9.690553142032884e+54\n",
      "Gradient Descent(26/49): loss=1.776252625664492e+57\n",
      "Gradient Descent(27/49): loss=3.2558238357880943e+59\n",
      "Gradient Descent(28/49): loss=5.96783852506371e+61\n",
      "Gradient Descent(29/49): loss=1.0938889343382972e+64\n",
      "Gradient Descent(30/49): loss=2.0050693322923107e+66\n",
      "Gradient Descent(31/49): loss=3.6752387752519534e+68\n",
      "Gradient Descent(32/49): loss=6.736614957684772e+70\n",
      "Gradient Descent(33/49): loss=1.2348036104127973e+73\n",
      "Gradient Descent(34/49): loss=2.263362186893493e+75\n",
      "Gradient Descent(35/49): loss=4.148682710238214e+77\n",
      "Gradient Descent(36/49): loss=7.604425102573979e+79\n",
      "Gradient Descent(37/49): loss=1.393870902634944e+82\n",
      "Gradient Descent(38/49): loss=2.554928304251058e+84\n",
      "Gradient Descent(39/49): loss=4.683115651186477e+86\n",
      "Gradient Descent(40/49): loss=8.584026473814033e+88\n",
      "Gradient Descent(41/49): loss=1.5734292294163651e+91\n",
      "Gradient Descent(42/49): loss=2.8840539431395547e+93\n",
      "Gradient Descent(43/49): loss=5.286394196467366e+95\n",
      "Gradient Descent(44/49): loss=9.689820007326968e+97\n",
      "Gradient Descent(45/49): loss=1.7761182440223126e+100\n",
      "Gradient Descent(46/49): loss=3.2555775178110137e+102\n",
      "Gradient Descent(47/49): loss=5.96738703076088e+104\n",
      "Gradient Descent(48/49): loss=1.0938061766330292e+107\n",
      "Gradient Descent(49/49): loss=2.004917639618925e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.627876438889015\n",
      "Gradient Descent(2/49): loss=876.9115293997795\n",
      "Gradient Descent(3/49): loss=157533.50096212243\n",
      "Gradient Descent(4/49): loss=28748596.388271105\n",
      "Gradient Descent(5/49): loss=5269387038.943507\n",
      "Gradient Descent(6/49): loss=967139226162.8955\n",
      "Gradient Descent(7/49): loss=177584608881577.4\n",
      "Gradient Descent(8/49): loss=3.261239908889201e+16\n",
      "Gradient Descent(9/49): loss=5.989357793153211e+18\n",
      "Gradient Descent(10/49): loss=1.0999789992762807e+21\n",
      "Gradient Descent(11/49): loss=2.0201830337592117e+23\n",
      "Gradient Descent(12/49): loss=3.71020382908622e+25\n",
      "Gradient Descent(13/49): loss=6.81404596339846e+27\n",
      "Gradient Descent(14/49): loss=1.2514469016733926e+30\n",
      "Gradient Descent(15/49): loss=2.2983693545457174e+32\n",
      "Gradient Descent(16/49): loss=4.221115405209166e+34\n",
      "Gradient Descent(17/49): loss=7.752372501244234e+36\n",
      "Gradient Descent(18/49): loss=1.4237772190577834e+39\n",
      "Gradient Descent(19/49): loss=2.6148660563585157e+41\n",
      "Gradient Descent(20/49): loss=4.802383689534179e+43\n",
      "Gradient Descent(21/49): loss=8.819912227339447e+45\n",
      "Gradient Descent(22/49): loss=1.6198383288186824e+48\n",
      "Gradient Descent(23/49): loss=2.974945944926216e+50\n",
      "Gradient Descent(24/49): loss=5.463695492199327e+52\n",
      "Gradient Descent(25/49): loss=1.003445742683846e+55\n",
      "Gradient Descent(26/49): loss=1.8428980164583264e+57\n",
      "Gradient Descent(27/49): loss=3.3846106018474083e+59\n",
      "Gradient Descent(28/49): loss=6.216073175960969e+61\n",
      "Gradient Descent(29/49): loss=1.1416251461190698e+64\n",
      "Gradient Descent(30/49): loss=2.0966741178202352e+66\n",
      "Gradient Descent(31/49): loss=3.850688092568279e+68\n",
      "Gradient Descent(32/49): loss=7.072056959267748e+70\n",
      "Gradient Descent(33/49): loss=1.2988325315585387e+73\n",
      "Gradient Descent(34/49): loss=2.3853964338112817e+75\n",
      "Gradient Descent(35/49): loss=4.3809467411566064e+77\n",
      "Gradient Descent(36/49): loss=8.04591391049644e+79\n",
      "Gradient Descent(37/49): loss=1.477688145508678e+82\n",
      "Gradient Descent(38/49): loss=2.713877229693282e+84\n",
      "Gradient Descent(39/49): loss=4.984224608036195e+86\n",
      "Gradient Descent(40/49): loss=9.15387574336264e+88\n",
      "Gradient Descent(41/49): loss=1.6811730552796573e+91\n",
      "Gradient Descent(42/49): loss=3.087591443272185e+93\n",
      "Gradient Descent(43/49): loss=5.670576797926269e+95\n",
      "Gradient Descent(44/49): loss=1.0414409358221949e+98\n",
      "Gradient Descent(45/49): loss=1.912678835780599e+100\n",
      "Gradient Descent(46/49): loss=3.5127679381594696e+102\n",
      "Gradient Descent(47/49): loss=6.45144305281403e+104\n",
      "Gradient Descent(48/49): loss=1.1848524638240134e+107\n",
      "Gradient Descent(49/49): loss=2.1760640984306326e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.650520330081945\n",
      "Gradient Descent(2/49): loss=873.8593212227194\n",
      "Gradient Descent(3/49): loss=155629.89671666798\n",
      "Gradient Descent(4/49): loss=28141344.9513559\n",
      "Gradient Descent(5/49): loss=5109827614.012873\n",
      "Gradient Descent(6/49): loss=929013514902.9789\n",
      "Gradient Descent(7/49): loss=168972228197534.25\n",
      "Gradient Descent(8/49): loss=3.0737371338117144e+16\n",
      "Gradient Descent(9/49): loss=5.591615810902824e+18\n",
      "Gradient Descent(10/49): loss=1.0172186287766479e+21\n",
      "Gradient Descent(11/49): loss=1.8505184085057378e+23\n",
      "Gradient Descent(12/49): loss=3.3664581668790493e+25\n",
      "Gradient Descent(13/49): loss=6.124255069874374e+27\n",
      "Gradient Descent(14/49): loss=1.1141236603537896e+30\n",
      "Gradient Descent(15/49): loss=2.026812389993972e+32\n",
      "Gradient Descent(16/49): loss=3.687174676496223e+34\n",
      "Gradient Descent(17/49): loss=6.707703807902533e+36\n",
      "Gradient Descent(18/49): loss=1.2202646856434544e+39\n",
      "Gradient Descent(19/49): loss=2.2199040786293227e+41\n",
      "Gradient Descent(20/49): loss=4.0384468857616595e+43\n",
      "Gradient Descent(21/49): loss=7.346737819654641e+45\n",
      "Gradient Descent(22/49): loss=1.3365176791735236e+48\n",
      "Gradient Descent(23/49): loss=2.431391388407872e+50\n",
      "Gradient Descent(24/49): loss=4.4231843512147476e+52\n",
      "Gradient Descent(25/49): loss=8.046651764141695e+54\n",
      "Gradient Descent(26/49): loss=1.4638459415693195e+57\n",
      "Gradient Descent(27/49): loss=2.6630268134609395e+59\n",
      "Gradient Descent(28/49): loss=4.8445752437647516e+61\n",
      "Gradient Descent(29/49): loss=8.813245579753069e+63\n",
      "Gradient Descent(30/49): loss=1.6033045982515624e+66\n",
      "Gradient Descent(31/49): loss=2.916729837507429e+68\n",
      "Gradient Descent(32/49): loss=5.306111486415935e+70\n",
      "Gradient Descent(33/49): loss=9.652871768999939e+72\n",
      "Gradient Descent(34/49): loss=1.7560492957469597e+75\n",
      "Gradient Descent(35/49): loss=3.194602811358863e+77\n",
      "Gradient Descent(36/49): loss=5.811617673295982e+79\n",
      "Gradient Descent(37/49): loss=1.0572488029020312e+82\n",
      "Gradient Descent(38/49): loss=1.9233457774999198e+84\n",
      "Gradient Descent(39/49): loss=3.498948373999308e+86\n",
      "Gradient Descent(40/49): loss=6.365282762533802e+88\n",
      "Gradient Descent(41/49): loss=1.1579714907510646e+91\n",
      "Gradient Descent(42/49): loss=2.1065803726502182e+93\n",
      "Gradient Descent(43/49): loss=3.8322885337676393e+95\n",
      "Gradient Descent(44/49): loss=6.971694788730231e+97\n",
      "Gradient Descent(45/49): loss=1.2682898951615188e+100\n",
      "Gradient Descent(46/49): loss=2.3072714840716413e+102\n",
      "Gradient Descent(47/49): loss=4.197385567384181e+104\n",
      "Gradient Descent(48/49): loss=7.635878882442793e+106\n",
      "Gradient Descent(49/49): loss=1.3891181872927728e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.636483007930317\n",
      "Gradient Descent(2/49): loss=872.5687420294594\n",
      "Gradient Descent(3/49): loss=155339.5333754108\n",
      "Gradient Descent(4/49): loss=28074867.51189055\n",
      "Gradient Descent(5/49): loss=5095704952.50098\n",
      "Gradient Descent(6/49): loss=926135055546.0393\n",
      "Gradient Descent(7/49): loss=168397713067151.88\n",
      "Gradient Descent(8/49): loss=3.0624033856170344e+16\n",
      "Gradient Descent(9/49): loss=5.569425339199953e+18\n",
      "Gradient Descent(10/49): loss=1.0128981601184859e+21\n",
      "Gradient Descent(11/49): loss=1.842144528580279e+23\n",
      "Gradient Descent(12/49): loss=3.3502905981372505e+25\n",
      "Gradient Descent(13/49): loss=6.09314551365311e+27\n",
      "Gradient Descent(14/49): loss=1.1081555378993212e+30\n",
      "Gradient Descent(15/49): loss=2.0153938386422526e+32\n",
      "Gradient Descent(16/49): loss=3.6653811624129772e+34\n",
      "Gradient Descent(17/49): loss=6.666200387699297e+36\n",
      "Gradient Descent(18/49): loss=1.2123767155009426e+39\n",
      "Gradient Descent(19/49): loss=2.204940170360851e+41\n",
      "Gradient Descent(20/49): loss=4.010107662472603e+43\n",
      "Gradient Descent(21/49): loss=7.293151842745338e+45\n",
      "Gradient Descent(22/49): loss=1.3263998944803109e+48\n",
      "Gradient Descent(23/49): loss=2.4123132467792302e+50\n",
      "Gradient Descent(24/49): loss=4.3872554761436425e+52\n",
      "Gradient Descent(25/49): loss=7.979067659924905e+54\n",
      "Gradient Descent(26/49): loss=1.4511468745753872e+57\n",
      "Gradient Descent(27/49): loss=2.639189616309313e+59\n",
      "Gradient Descent(28/49): loss=4.799873777679069e+61\n",
      "Gradient Descent(29/49): loss=8.729493379058314e+63\n",
      "Gradient Descent(30/49): loss=1.587626220701822e+66\n",
      "Gradient Descent(31/49): loss=2.887403549336175e+68\n",
      "Gradient Descent(32/49): loss=5.2512985411853926e+70\n",
      "Gradient Descent(33/49): loss=9.55049611094212e+72\n",
      "Gradient Descent(34/49): loss=1.7369413536433893e+75\n",
      "Gradient Descent(35/49): loss=3.1589618287367736e+77\n",
      "Gradient Descent(36/49): loss=5.7451794871968884e+79\n",
      "Gradient Descent(37/49): loss=1.04487135741387e+82\n",
      "Gradient Descent(38/49): loss=1.9002994701503695e+84\n",
      "Gradient Descent(39/49): loss=3.456059973920247e+86\n",
      "Gradient Descent(40/49): loss=6.285509589911317e+88\n",
      "Gradient Descent(41/49): loss=1.1431407759991374e+91\n",
      "Gradient Descent(42/49): loss=2.0790213029813249e+93\n",
      "Gradient Descent(43/49): loss=3.7810999913569814e+95\n",
      "Gradient Descent(44/49): loss=6.876657359950233e+97\n",
      "Gradient Descent(45/49): loss=1.2506523645037543e+100\n",
      "Gradient Descent(46/49): loss=2.274551798884678e+102\n",
      "Gradient Descent(47/49): loss=4.1367097945417844e+104\n",
      "Gradient Descent(48/49): loss=7.523402163296108e+106\n",
      "Gradient Descent(49/49): loss=1.3682753425287868e+109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.709192643127399\n",
      "Gradient Descent(2/49): loss=902.0087825529781\n",
      "Gradient Descent(3/49): loss=163795.98974496144\n",
      "Gradient Descent(4/49): loss=30176254.034228437\n",
      "Gradient Descent(5/49): loss=5580980326.702547\n",
      "Gradient Descent(6/49): loss=1033383007953.9938\n",
      "Gradient Descent(7/49): loss=191412668851820.94\n",
      "Gradient Descent(8/49): loss=3.545934960018973e+16\n",
      "Gradient Descent(9/49): loss=6.56912055940242e+18\n",
      "Gradient Descent(10/49): loss=1.2169956065777465e+21\n",
      "Gradient Descent(11/49): loss=2.2546156320290338e+23\n",
      "Gradient Descent(12/49): loss=4.176924046054152e+25\n",
      "Gradient Descent(13/49): loss=7.738215581404413e+27\n",
      "Gradient Descent(14/49): loss=1.433590553805844e+30\n",
      "Gradient Descent(15/49): loss=2.6558861609071845e+32\n",
      "Gradient Descent(16/49): loss=4.920324972090652e+34\n",
      "Gradient Descent(17/49): loss=9.115450165425932e+36\n",
      "Gradient Descent(18/49): loss=1.6887386974792598e+39\n",
      "Gradient Descent(19/49): loss=3.1285765793546208e+41\n",
      "Gradient Descent(20/49): loss=5.79603666871634e+43\n",
      "Gradient Descent(21/49): loss=1.0737803667187981e+46\n",
      "Gradient Descent(22/49): loss=1.9892977596117637e+48\n",
      "Gradient Descent(23/49): loss=3.6853957280956136e+50\n",
      "Gradient Descent(24/49): loss=6.827606177639714e+52\n",
      "Gradient Descent(25/49): loss=1.2648901110291311e+55\n",
      "Gradient Descent(26/49): loss=2.343349852572615e+57\n",
      "Gradient Descent(27/49): loss=4.341316675394533e+59\n",
      "Gradient Descent(28/49): loss=8.04277280892055e+61\n",
      "Gradient Descent(29/49): loss=1.490013267692191e+64\n",
      "Gradient Descent(30/49): loss=2.760415581348177e+66\n",
      "Gradient Descent(31/49): loss=5.113977403403873e+68\n",
      "Gradient Descent(32/49): loss=9.474212889985391e+70\n",
      "Gradient Descent(33/49): loss=1.7552034904381956e+73\n",
      "Gradient Descent(34/49): loss=3.251710013929366e+75\n",
      "Gradient Descent(35/49): loss=6.024155075061265e+77\n",
      "Gradient Descent(36/49): loss=1.1160418429973436e+80\n",
      "Gradient Descent(37/49): loss=2.0675918528014217e+82\n",
      "Gradient Descent(38/49): loss=3.8304442585142636e+84\n",
      "Gradient Descent(39/49): loss=7.096324740158458e+86\n",
      "Gradient Descent(40/49): loss=1.314673218539875e+89\n",
      "Gradient Descent(41/49): loss=2.435578605591541e+91\n",
      "Gradient Descent(42/49): loss=4.512180715602906e+93\n",
      "Gradient Descent(43/49): loss=8.35931747943476e+95\n",
      "Gradient Descent(44/49): loss=1.5486566945412528e+98\n",
      "Gradient Descent(45/49): loss=2.8690590630727107e+100\n",
      "Gradient Descent(46/49): loss=5.315251557310433e+102\n",
      "Gradient Descent(47/49): loss=9.847095684127761e+104\n",
      "Gradient Descent(48/49): loss=1.8242841823545504e+107\n",
      "Gradient Descent(49/49): loss=3.3796896920107323e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.685012506836624\n",
      "Gradient Descent(2/49): loss=895.8298523844647\n",
      "Gradient Descent(3/49): loss=162653.5878357312\n",
      "Gradient Descent(4/49): loss=30000670.527141605\n",
      "Gradient Descent(5/49): loss=5557769496.719557\n",
      "Gradient Descent(6/49): loss=1030995076859.7646\n",
      "Gradient Descent(7/49): loss=191337678533733.5\n",
      "Gradient Descent(8/49): loss=3.5514497805487336e+16\n",
      "Gradient Descent(9/49): loss=6.592209569917467e+18\n",
      "Gradient Descent(10/49): loss=1.2236662570704448e+21\n",
      "Gradient Descent(11/49): loss=2.2714185214106665e+23\n",
      "Gradient Descent(12/49): loss=4.21630547914907e+25\n",
      "Gradient Descent(13/49): loss=7.826493261804667e+27\n",
      "Gradient Descent(14/49): loss=1.4527886635644015e+30\n",
      "Gradient Descent(15/49): loss=2.696731420638365e+32\n",
      "Gradient Descent(16/49): loss=5.005793811423407e+34\n",
      "Gradient Descent(17/49): loss=9.291979044565181e+36\n",
      "Gradient Descent(18/49): loss=1.724818840080859e+39\n",
      "Gradient Descent(19/49): loss=3.2016861198258844e+41\n",
      "Gradient Descent(20/49): loss=5.943113429745852e+43\n",
      "Gradient Descent(21/49): loss=1.1031873806409406e+46\n",
      "Gradient Descent(22/49): loss=2.0477859142757564e+48\n",
      "Gradient Descent(23/49): loss=3.8011920951331204e+50\n",
      "Gradient Descent(24/49): loss=7.055943320742393e+52\n",
      "Gradient Descent(25/49): loss=1.3097558581509587e+55\n",
      "Gradient Descent(26/49): loss=2.4312275906728846e+57\n",
      "Gradient Descent(27/49): loss=4.512953739328452e+59\n",
      "Gradient Descent(28/49): loss=8.37714722038117e+61\n",
      "Gradient Descent(29/49): loss=1.5550036540455038e+64\n",
      "Gradient Descent(30/49): loss=2.8864675533121014e+66\n",
      "Gradient Descent(31/49): loss=5.357990583911339e+68\n",
      "Gradient Descent(32/49): loss=9.945742526827024e+70\n",
      "Gradient Descent(33/49): loss=1.8461733528789794e+73\n",
      "Gradient Descent(34/49): loss=3.426949812632819e+75\n",
      "Gradient Descent(35/49): loss=6.36125800428773e+77\n",
      "Gradient Descent(36/49): loss=1.1808052527628358e+80\n",
      "Gradient Descent(37/49): loss=2.1918636911323255e+82\n",
      "Gradient Descent(38/49): loss=4.068635729103704e+84\n",
      "Gradient Descent(39/49): loss=7.552384193921989e+86\n",
      "Gradient Descent(40/49): loss=1.4019074404866327e+89\n",
      "Gradient Descent(41/49): loss=2.6022834925075203e+91\n",
      "Gradient Descent(42/49): loss=4.830475379334935e+93\n",
      "Gradient Descent(43/49): loss=8.966545135279312e+95\n",
      "Gradient Descent(44/49): loss=1.664410339548615e+98\n",
      "Gradient Descent(45/49): loss=3.0895531518562532e+100\n",
      "Gradient Descent(46/49): loss=5.734967184074051e+102\n",
      "Gradient Descent(47/49): loss=1.0645503406421591e+105\n",
      "Gradient Descent(48/49): loss=1.9760661070710442e+107\n",
      "Gradient Descent(49/49): loss=3.668062571056436e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.707952694684767\n",
      "Gradient Descent(2/49): loss=892.7172164270723\n",
      "Gradient Descent(3/49): loss=160689.6875862494\n",
      "Gradient Descent(4/49): loss=29367371.31460376\n",
      "Gradient Descent(5/49): loss=5389570780.77335\n",
      "Gradient Descent(6/49): loss=990372720740.1198\n",
      "Gradient Descent(7/49): loss=182062740841427.4\n",
      "Gradient Descent(8/49): loss=3.347354718502383e+16\n",
      "Gradient Descent(9/49): loss=6.154626003943125e+18\n",
      "Gradient Descent(10/49): loss=1.1316392508393338e+21\n",
      "Gradient Descent(11/49): loss=2.080733554055042e+23\n",
      "Gradient Descent(12/49): loss=3.8258298383041925e+25\n",
      "Gradient Descent(13/49): loss=7.034529681324991e+27\n",
      "Gradient Descent(14/49): loss=1.293434861019935e+30\n",
      "Gradient Descent(15/49): loss=2.3782312637236855e+32\n",
      "Gradient Descent(16/49): loss=4.372840278919249e+34\n",
      "Gradient Descent(17/49): loss=8.040316568852517e+36\n",
      "Gradient Descent(18/49): loss=1.4783684413287032e+39\n",
      "Gradient Descent(19/49): loss=2.718267659297017e+41\n",
      "Gradient Descent(20/49): loss=4.998063312820155e+43\n",
      "Gradient Descent(21/49): loss=9.189910638716297e+45\n",
      "Gradient Descent(22/49): loss=1.689743651972651e+48\n",
      "Gradient Descent(23/49): loss=3.106922060133698e+50\n",
      "Gradient Descent(24/49): loss=5.71267995385356e+52\n",
      "Gradient Descent(25/49): loss=1.050387220006086e+55\n",
      "Gradient Descent(26/49): loss=1.9313410183398096e+57\n",
      "Gradient Descent(27/49): loss=3.551145766130582e+59\n",
      "Gradient Descent(28/49): loss=6.529471560205235e+61\n",
      "Gradient Descent(29/49): loss=1.2005702289710552e+64\n",
      "Gradient Descent(30/49): loss=2.2074816643298178e+66\n",
      "Gradient Descent(31/49): loss=4.0588840042525006e+68\n",
      "Gradient Descent(32/49): loss=7.463046976192423e+70\n",
      "Gradient Descent(33/49): loss=1.3722262107145973e+73\n",
      "Gradient Descent(34/49): loss=2.523104543464656e+75\n",
      "Gradient Descent(35/49): loss=4.639218000315576e+77\n",
      "Gradient Descent(36/49): loss=8.530103800177149e+79\n",
      "Gradient Descent(37/49): loss=1.5684253431687606e+82\n",
      "Gradient Descent(38/49): loss=2.8838547744787827e+84\n",
      "Gradient Descent(39/49): loss=5.30252740208956e+86\n",
      "Gradient Descent(40/49): loss=9.749727031588239e+88\n",
      "Gradient Descent(41/49): loss=1.792676774343945e+91\n",
      "Gradient Descent(42/49): loss=3.2961846078973795e+93\n",
      "Gradient Descent(43/49): loss=6.06067592598543e+95\n",
      "Gradient Descent(44/49): loss=1.1143730418439872e+98\n",
      "Gradient Descent(45/49): loss=2.0489913857037446e+100\n",
      "Gradient Descent(46/49): loss=3.767468828697604e+102\n",
      "Gradient Descent(47/49): loss=6.927223547273742e+104\n",
      "Gradient Descent(48/49): loss=1.2737046610281466e+107\n",
      "Gradient Descent(49/49): loss=2.3419535293664716e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.693433772610891\n",
      "Gradient Descent(2/49): loss=891.3044479643444\n",
      "Gradient Descent(3/49): loss=160364.63756265223\n",
      "Gradient Descent(4/49): loss=29291849.99746861\n",
      "Gradient Descent(5/49): loss=5373264463.123783\n",
      "Gradient Descent(6/49): loss=986993118655.989\n",
      "Gradient Descent(7/49): loss=181377021140296.97\n",
      "Gradient Descent(8/49): loss=3.3336109638975908e+16\n",
      "Gradient Descent(9/49): loss=6.127303861132075e+18\n",
      "Gradient Descent(10/49): loss=1.1262410508379445e+21\n",
      "Gradient Descent(11/49): loss=2.0701214285807608e+23\n",
      "Gradient Descent(12/49): loss=3.805056874736448e+25\n",
      "Gradient Descent(13/49): loss=6.994018537070553e+27\n",
      "Gradient Descent(14/49): loss=1.2855604590077975e+30\n",
      "Gradient Descent(15/49): loss=2.3629703232904102e+32\n",
      "Gradient Descent(16/49): loss=4.343342127712592e+34\n",
      "Gradient Descent(17/49): loss=7.983435435495003e+36\n",
      "Gradient Descent(18/49): loss=1.4674239256041324e+39\n",
      "Gradient Descent(19/49): loss=2.6972510733532613e+41\n",
      "Gradient Descent(20/49): loss=4.957778886133028e+43\n",
      "Gradient Descent(21/49): loss=9.11282295136771e+45\n",
      "Gradient Descent(22/49): loss=1.67501504310299e+48\n",
      "Gradient Descent(23/49): loss=3.0788213593472837e+50\n",
      "Gradient Descent(24/49): loss=5.659137810046058e+52\n",
      "Gradient Descent(25/49): loss=1.0401980828123372e+55\n",
      "Gradient Descent(26/49): loss=1.91197332138882e+57\n",
      "Gradient Descent(27/49): loss=3.514371005010642e+59\n",
      "Gradient Descent(28/49): loss=6.459715427351751e+61\n",
      "Gradient Descent(29/49): loss=1.1873511175363462e+64\n",
      "Gradient Descent(30/49): loss=2.182453224402774e+66\n",
      "Gradient Descent(31/49): loss=4.011536272934278e+68\n",
      "Gradient Descent(32/49): loss=7.373547844752117e+70\n",
      "Gradient Descent(33/49): loss=1.355321356201513e+73\n",
      "Gradient Descent(34/49): loss=2.491196934299768e+75\n",
      "Gradient Descent(35/49): loss=4.579033700802874e+77\n",
      "Gradient Descent(36/49): loss=8.416656806372504e+79\n",
      "Gradient Descent(37/49): loss=1.547053732839651e+82\n",
      "Gradient Descent(38/49): loss=2.8436174924953664e+84\n",
      "Gradient Descent(39/49): loss=5.226812923157679e+86\n",
      "Gradient Descent(40/49): loss=9.60733059414201e+88\n",
      "Gradient Descent(41/49): loss=1.765909790576076e+91\n",
      "Gradient Descent(42/49): loss=3.2458937036619447e+93\n",
      "Gradient Descent(43/49): loss=5.966231113105308e+95\n",
      "Gradient Descent(44/49): loss=1.0966444666634455e+98\n",
      "Gradient Descent(45/49): loss=2.015726617800445e+100\n",
      "Gradient Descent(46/49): loss=3.705078465467875e+102\n",
      "Gradient Descent(47/49): loss=6.81025210167305e+104\n",
      "Gradient Descent(48/49): loss=1.2517827657527124e+107\n",
      "Gradient Descent(49/49): loss=2.300884121823569e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.766937658643128\n",
      "Gradient Descent(2/49): loss=921.3632110050204\n",
      "Gradient Descent(3/49): loss=169091.00684813663\n",
      "Gradient Descent(4/49): loss=31483453.54440488\n",
      "Gradient Descent(5/49): loss=5884772095.272116\n",
      "Gradient Descent(6/49): loss=1101243905764.094\n",
      "Gradient Descent(7/49): loss=206156148013327.0\n",
      "Gradient Descent(8/49): loss=3.859756703447894e+16\n",
      "Gradient Descent(9/49): loss=7.22669962512533e+18\n",
      "Gradient Descent(10/49): loss=1.3530860270219533e+21\n",
      "Gradient Descent(11/49): loss=2.5334512249080963e+23\n",
      "Gradient Descent(12/49): loss=4.743514684823912e+25\n",
      "Gradient Descent(13/49): loss=8.881537108668884e+27\n",
      "Gradient Descent(14/49): loss=1.6629380848183954e+30\n",
      "Gradient Descent(15/49): loss=3.1136087861628533e+32\n",
      "Gradient Descent(16/49): loss=5.829777971223333e+34\n",
      "Gradient Descent(17/49): loss=1.0915408353741704e+37\n",
      "Gradient Descent(18/49): loss=2.0437508992754087e+39\n",
      "Gradient Descent(19/49): loss=3.8266252667984125e+41\n",
      "Gradient Descent(20/49): loss=7.164797304839402e+43\n",
      "Gradient Descent(21/49): loss=1.341503723073252e+46\n",
      "Gradient Descent(22/49): loss=2.5117699252077638e+48\n",
      "Gradient Descent(23/49): loss=4.7029225850827424e+50\n",
      "Gradient Descent(24/49): loss=8.805536135835532e+52\n",
      "Gradient Descent(25/49): loss=1.648708122168403e+55\n",
      "Gradient Descent(26/49): loss=3.0869653251916206e+57\n",
      "Gradient Descent(27/49): loss=5.779892020185354e+59\n",
      "Gradient Descent(28/49): loss=1.0822004216367206e+62\n",
      "Gradient Descent(29/49): loss=2.026262339331989e+64\n",
      "Gradient Descent(30/49): loss=3.793880491735186e+66\n",
      "Gradient Descent(31/49): loss=7.103487493289759e+68\n",
      "Gradient Descent(32/49): loss=1.330024355728858e+71\n",
      "Gradient Descent(33/49): loss=2.490276485322185e+73\n",
      "Gradient Descent(34/49): loss=4.662679255937502e+75\n",
      "Gradient Descent(35/49): loss=8.73018637564546e+77\n",
      "Gradient Descent(36/49): loss=1.6345999793241442e+80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=3.060549886838981e+82\n",
      "Gradient Descent(38/49): loss=5.7304329672774456e+84\n",
      "Gradient Descent(39/49): loss=1.0729399358484534e+87\n",
      "Gradient Descent(40/49): loss=2.008923431287294e+89\n",
      "Gradient Descent(41/49): loss=3.761415917083699e+91\n",
      "Gradient Descent(42/49): loss=7.042702315550438e+93\n",
      "Gradient Descent(43/49): loss=1.3186432183738745e+96\n",
      "Gradient Descent(44/49): loss=2.4689669667338566e+98\n",
      "Gradient Descent(45/49): loss=4.6227802925648195e+100\n",
      "Gradient Descent(46/49): loss=8.655481390095556e+102\n",
      "Gradient Descent(47/49): loss=1.620612561120109e+105\n",
      "Gradient Descent(48/49): loss=3.034360487754785e+107\n",
      "Gradient Descent(49/49): loss=5.681397139908416e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.742461686119567\n",
      "Gradient Descent(2/49): loss=915.0528341230178\n",
      "Gradient Descent(3/49): loss=167911.61043428208\n",
      "Gradient Descent(4/49): loss=31300202.401198804\n",
      "Gradient Descent(5/49): loss=5860279370.651255\n",
      "Gradient Descent(6/49): loss=1098694679022.6998\n",
      "Gradient Descent(7/49): loss=206074374219444.78\n",
      "Gradient Descent(8/49): loss=3.865737796066368e+16\n",
      "Gradient Descent(9/49): loss=7.252053720649912e+18\n",
      "Gradient Descent(10/49): loss=1.3604929891352744e+21\n",
      "Gradient Descent(11/49): loss=2.5523122942196275e+23\n",
      "Gradient Descent(12/49): loss=4.788197416037004e+25\n",
      "Gradient Descent(13/49): loss=8.982775082000175e+27\n",
      "Gradient Descent(14/49): loss=1.6851908153209914e+30\n",
      "Gradient Descent(15/49): loss=3.161459829306586e+32\n",
      "Gradient Descent(16/49): loss=5.930977289898146e+34\n",
      "Gradient Descent(17/49): loss=1.1126661020097607e+37\n",
      "Gradient Descent(18/49): loss=2.087389306686806e+39\n",
      "Gradient Descent(19/49): loss=3.915994306835102e+41\n",
      "Gradient Descent(20/49): loss=7.346502813720645e+43\n",
      "Gradient Descent(21/49): loss=1.3782222181008364e+46\n",
      "Gradient Descent(22/49): loss=2.585579194184792e+48\n",
      "Gradient Descent(23/49): loss=4.850610940427805e+50\n",
      "Gradient Descent(24/49): loss=9.099866888001136e+52\n",
      "Gradient Descent(25/49): loss=1.7071576837724477e+55\n",
      "Gradient Descent(26/49): loss=3.2026703171967165e+57\n",
      "Gradient Descent(27/49): loss=6.008289250696565e+59\n",
      "Gradient Descent(28/49): loss=1.1271700220344464e+62\n",
      "Gradient Descent(29/49): loss=2.1145990240497367e+64\n",
      "Gradient Descent(30/49): loss=3.967040415465815e+66\n",
      "Gradient Descent(31/49): loss=7.442266585274393e+68\n",
      "Gradient Descent(32/49): loss=1.3961877401188046e+71\n",
      "Gradient Descent(33/49): loss=2.6192829608053904e+73\n",
      "Gradient Descent(34/49): loss=4.913840045738857e+75\n",
      "Gradient Descent(35/49): loss=9.218486263768376e+77\n",
      "Gradient Descent(36/49): loss=1.729410973989238e+80\n",
      "Gradient Descent(37/49): loss=3.244418043675411e+82\n",
      "Gradient Descent(38/49): loss=6.086609024947773e+84\n",
      "Gradient Descent(39/49): loss=1.1418630066736947e+87\n",
      "Gradient Descent(40/49): loss=2.1421634290385147e+89\n",
      "Gradient Descent(41/49): loss=4.0187519254850286e+91\n",
      "Gradient Descent(42/49): loss=7.539278665511741e+93\n",
      "Gradient Descent(43/49): loss=1.41438744789853e+96\n",
      "Gradient Descent(44/49): loss=2.653426065711187e+98\n",
      "Gradient Descent(45/49): loss=4.9778933605897385e+100\n",
      "Gradient Descent(46/49): loss=9.33865187713904e+102\n",
      "Gradient Descent(47/49): loss=1.7519543422292267e+105\n",
      "Gradient Descent(48/49): loss=3.286709963747106e+107\n",
      "Gradient Descent(49/49): loss=6.165949719927738e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.7656999888454985\n",
      "Gradient Descent(2/49): loss=911.8788845209374\n",
      "Gradient Descent(3/49): loss=165885.8319071305\n",
      "Gradient Descent(4/49): loss=30639883.169885006\n",
      "Gradient Descent(5/49): loss=5683022672.20091\n",
      "Gradient Descent(6/49): loss=1055426629534.0592\n",
      "Gradient Descent(7/49): loss=196089808357902.8\n",
      "Gradient Descent(8/49): loss=3.643681505024858e+16\n",
      "Gradient Descent(9/49): loss=6.77088015020096e+18\n",
      "Gradient Descent(10/49): loss=1.2582190993013245e+21\n",
      "Gradient Descent(11/49): loss=2.3381348887598696e+23\n",
      "Gradient Descent(12/49): loss=4.3449378216399425e+25\n",
      "Gradient Descent(13/49): loss=8.074168487516915e+27\n",
      "Gradient Descent(14/49): loss=1.5004175292359688e+30\n",
      "Gradient Descent(15/49): loss=2.7882164004965758e+32\n",
      "Gradient Descent(16/49): loss=5.181325000026771e+34\n",
      "Gradient Descent(17/49): loss=9.628423724237165e+36\n",
      "Gradient Descent(18/49): loss=1.7892439409582968e+39\n",
      "Gradient Descent(19/49): loss=3.3249407944398508e+41\n",
      "Gradient Descent(20/49): loss=6.178716627879893e+43\n",
      "Gradient Descent(21/49): loss=1.1481870364341733e+46\n",
      "Gradient Descent(22/49): loss=2.1336687698608338e+48\n",
      "Gradient Descent(23/49): loss=3.9649832954575e+50\n",
      "Gradient Descent(24/49): loss=7.368103594791779e+52\n",
      "Gradient Descent(25/49): loss=1.3692100707167454e+55\n",
      "Gradient Descent(26/49): loss=2.544394488532961e+57\n",
      "Gradient Descent(27/49): loss=4.72823232295443e+59\n",
      "Gradient Descent(28/49): loss=8.786444476509639e+61\n",
      "Gradient Descent(29/49): loss=1.6327794673707563e+64\n",
      "Gradient Descent(30/49): loss=3.034183845575939e+66\n",
      "Gradient Descent(31/49): loss=5.6384048138348455e+68\n",
      "Gradient Descent(32/49): loss=1.047781230891153e+71\n",
      "Gradient Descent(33/49): loss=1.9470852910632158e+73\n",
      "Gradient Descent(34/49): loss=3.618256386832091e+75\n",
      "Gradient Descent(35/49): loss=6.723783154718635e+77\n",
      "Gradient Descent(36/49): loss=1.2494764073714556e+80\n",
      "Gradient Descent(37/49): loss=2.3218941727504483e+82\n",
      "Gradient Descent(38/49): loss=4.3147613813645635e+84\n",
      "Gradient Descent(39/49): loss=8.018094018497718e+86\n",
      "Gradient Descent(40/49): loss=1.4899973835664708e+89\n",
      "Gradient Descent(41/49): loss=2.7688527945833408e+91\n",
      "Gradient Descent(42/49): loss=5.145341785581712e+93\n",
      "Gradient Descent(43/49): loss=9.561556375349674e+95\n",
      "Gradient Descent(44/49): loss=1.7768180254842718e+98\n",
      "Gradient Descent(45/49): loss=3.3018497948984553e+100\n",
      "Gradient Descent(46/49): loss=6.135806769013133e+102\n",
      "Gradient Descent(47/49): loss=1.1402131243170413e+105\n",
      "Gradient Descent(48/49): loss=2.118850899005611e+107\n",
      "Gradient Descent(49/49): loss=3.937447338984123e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.750695105084348\n",
      "Gradient Descent(2/49): loss=910.3404536348851\n",
      "Gradient Descent(3/49): loss=165524.48703406833\n",
      "Gradient Descent(4/49): loss=30554744.394993767\n",
      "Gradient Descent(5/49): loss=5664357288.007767\n",
      "Gradient Descent(6/49): loss=1051497176194.731\n",
      "Gradient Descent(7/49): loss=195280206321109.12\n",
      "Gradient Descent(8/49): loss=3.627212661743381e+16\n",
      "Gradient Descent(9/49): loss=6.737669928709719e+18\n",
      "Gradient Descent(10/49): loss=1.2515664239598366e+21\n",
      "Gradient Descent(11/49): loss=2.3248804161567767e+23\n",
      "Gradient Descent(12/49): loss=4.3186519283561325e+25\n",
      "Gradient Descent(13/49): loss=8.02224797055697e+27\n",
      "Gradient Descent(14/49): loss=1.4901982969683252e+30\n",
      "Gradient Descent(15/49): loss=2.768165667538891e+32\n",
      "Gradient Descent(16/49): loss=5.142095106779668e+34\n",
      "Gradient Descent(17/49): loss=9.551864125161194e+36\n",
      "Gradient Descent(18/49): loss=1.7743372430825726e+39\n",
      "Gradient Descent(19/49): loss=3.2959772190426697e+41\n",
      "Gradient Descent(20/49): loss=6.122548503566171e+43\n",
      "Gradient Descent(21/49): loss=1.1373136915713108e+46\n",
      "Gradient Descent(22/49): loss=2.112653631633632e+48\n",
      "Gradient Descent(23/49): loss=3.924427711015951e+50\n",
      "Gradient Descent(24/49): loss=7.289946931413528e+52\n",
      "Gradient Descent(25/49): loss=1.354167541772826e+55\n",
      "Gradient Descent(26/49): loss=2.5154774766468114e+57\n",
      "Gradient Descent(27/49): loss=4.672706101960466e+59\n",
      "Gradient Descent(28/49): loss=8.679935526358017e+61\n",
      "Gradient Descent(29/49): loss=1.612369344396043e+64\n",
      "Gradient Descent(30/49): loss=2.995108540672502e+66\n",
      "Gradient Descent(31/49): loss=5.563660213206028e+68\n",
      "Gradient Descent(32/49): loss=1.0334955994970875e+71\n",
      "Gradient Descent(33/49): loss=1.9198029952378157e+73\n",
      "Gradient Descent(34/49): loss=3.5661918079937735e+75\n",
      "Gradient Descent(35/49): loss=6.624494306420418e+77\n",
      "Gradient Descent(36/49): loss=1.2305542488609054e+80\n",
      "Gradient Descent(37/49): loss=2.285855628137541e+82\n",
      "Gradient Descent(38/49): loss=4.246164651030098e+84\n",
      "Gradient Descent(39/49): loss=7.887599733648948e+86\n",
      "Gradient Descent(40/49): loss=1.465186460519519e+89\n",
      "Gradient Descent(41/49): loss=2.7217042403045055e+91\n",
      "Gradient Descent(42/49): loss=5.055789260477457e+93\n",
      "Gradient Descent(43/49): loss=9.391543970074975e+95\n",
      "Gradient Descent(44/49): loss=1.7445564598853905e+98\n",
      "Gradient Descent(45/49): loss=3.2406569691048814e+100\n",
      "Gradient Descent(46/49): loss=6.019786594982434e+102\n",
      "Gradient Descent(47/49): loss=1.1182248227630096e+105\n",
      "Gradient Descent(48/49): loss=2.077194489395368e+107\n",
      "Gradient Descent(49/49): loss=3.858559440769042e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.824997520790616\n",
      "Gradient Descent(2/49): loss=941.0276377713973\n",
      "Gradient Descent(3/49): loss=174527.90939253286\n",
      "Gradient Descent(4/49): loss=32839935.706484053\n",
      "Gradient Descent(5/49): loss=6203366501.391534\n",
      "Gradient Descent(6/49): loss=1173167678254.7104\n",
      "Gradient Descent(7/49): loss=221948391998459.84\n",
      "Gradient Descent(8/49): loss=4.199474262136907e+16\n",
      "Gradient Descent(9/49): loss=7.946105617382166e+18\n",
      "Gradient Descent(10/49): loss=1.5035542228207956e+21\n",
      "Gradient Descent(11/49): loss=2.8450217871701336e+23\n",
      "Gradient Descent(12/49): loss=5.383350598770049e+25\n",
      "Gradient Descent(13/49): loss=1.0186381033878618e+28\n",
      "Gradient Descent(14/49): loss=1.9274682376360423e+30\n",
      "Gradient Descent(15/49): loss=3.6471578689015855e+32\n",
      "Gradient Descent(16/49): loss=6.901156893764446e+34\n",
      "Gradient Descent(17/49): loss=1.305837817051506e+37\n",
      "Gradient Descent(18/49): loss=2.4709080424372543e+39\n",
      "Gradient Descent(19/49): loss=4.675455464337065e+41\n",
      "Gradient Descent(20/49): loss=8.84690301183373e+43\n",
      "Gradient Descent(21/49): loss=1.674012157788597e+46\n",
      "Gradient Descent(22/49): loss=3.167568018689229e+48\n",
      "Gradient Descent(23/49): loss=5.993676393804909e+50\n",
      "Gradient Descent(24/49): loss=1.134124239850947e+53\n",
      "Gradient Descent(25/49): loss=2.145991386434752e+55\n",
      "Gradient Descent(26/49): loss=4.060647739314956e+57\n",
      "Gradient Descent(27/49): loss=7.6835630222163105e+59\n",
      "Gradient Descent(28/49): loss=1.4538848111540607e+62\n",
      "Gradient Descent(29/49): loss=2.751042762313125e+64\n",
      "Gradient Descent(30/49): loss=5.205526752884884e+66\n",
      "Gradient Descent(31/49): loss=9.849904605705325e+68\n",
      "Gradient Descent(32/49): loss=1.8638002520633725e+71\n",
      "Gradient Descent(33/49): loss=3.526685301682415e+73\n",
      "Gradient Descent(34/49): loss=6.673198591605226e+75\n",
      "Gradient Descent(35/49): loss=1.2627035199811658e+78\n",
      "Gradient Descent(36/49): loss=2.389289270333678e+80\n",
      "Gradient Descent(37/49): loss=4.521016316971069e+82\n",
      "Gradient Descent(38/49): loss=8.554673053658405e+84\n",
      "Gradient Descent(39/49): loss=1.6187163665009513e+87\n",
      "Gradient Descent(40/49): loss=3.062937249317198e+89\n",
      "Gradient Descent(41/49): loss=5.795693913649742e+91\n",
      "Gradient Descent(42/49): loss=1.0966619687753872e+94\n",
      "Gradient Descent(43/49): loss=2.075105227565308e+96\n",
      "Gradient Descent(44/49): loss=3.926516855761267e+98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=7.429760386978766e+100\n",
      "Gradient Descent(46/49): loss=1.4058602429510385e+103\n",
      "Gradient Descent(47/49): loss=2.660170610850696e+105\n",
      "Gradient Descent(48/49): loss=5.0335783477164724e+107\n",
      "Gradient Descent(49/49): loss=9.524543606057562e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.800223976737843\n",
      "Gradient Descent(2/49): loss=934.5837287759647\n",
      "Gradient Descent(3/49): loss=173310.5257811907\n",
      "Gradient Descent(4/49): loss=32648725.586922873\n",
      "Gradient Descent(5/49): loss=6177528531.202469\n",
      "Gradient Descent(6/49): loss=1170447232553.2385\n",
      "Gradient Descent(7/49): loss=221859280973634.03\n",
      "Gradient Descent(8/49): loss=4.205958313946937e+16\n",
      "Gradient Descent(9/49): loss=7.973933450001048e+18\n",
      "Gradient Descent(10/49): loss=1.511774274828628e+21\n",
      "Gradient Descent(11/49): loss=2.866180342483801e+23\n",
      "Gradient Descent(12/49): loss=5.434014713471777e+25\n",
      "Gradient Descent(13/49): loss=1.0302398638223893e+28\n",
      "Gradient Descent(14/49): loss=1.9532416272670114e+30\n",
      "Gradient Descent(15/49): loss=3.703169740658823e+32\n",
      "Gradient Descent(16/49): loss=7.02087556216312e+34\n",
      "Gradient Descent(17/49): loss=1.3310946417448794e+37\n",
      "Gradient Descent(18/49): loss=2.5236353123823928e+39\n",
      "Gradient Descent(19/49): loss=4.784584803289369e+41\n",
      "Gradient Descent(20/49): loss=9.071140997741922e+43\n",
      "Gradient Descent(21/49): loss=1.7198064699585716e+46\n",
      "Gradient Descent(22/49): loss=3.260597861864983e+48\n",
      "Gradient Descent(23/49): loss=6.181799291143424e+50\n",
      "Gradient Descent(24/49): loss=1.172013357520781e+53\n",
      "Gradient Descent(25/49): loss=2.22203155669653e+55\n",
      "Gradient Descent(26/49): loss=4.21277130271038e+57\n",
      "Gradient Descent(27/49): loss=7.987034205457897e+59\n",
      "Gradient Descent(28/49): loss=1.5142696058082762e+62\n",
      "Gradient Descent(29/49): loss=2.870918516297158e+64\n",
      "Gradient Descent(30/49): loss=5.443002418858193e+66\n",
      "Gradient Descent(31/49): loss=1.031944137861059e+69\n",
      "Gradient Descent(32/49): loss=1.9564729568670712e+71\n",
      "Gradient Descent(33/49): loss=3.709296162955231e+73\n",
      "Gradient Descent(34/49): loss=7.032490776947217e+75\n",
      "Gradient Descent(35/49): loss=1.333296786106341e+78\n",
      "Gradient Descent(36/49): loss=2.527810382160482e+80\n",
      "Gradient Descent(37/49): loss=4.7925003605676184e+82\n",
      "Gradient Descent(38/49): loss=9.086148181103011e+84\n",
      "Gradient Descent(39/49): loss=1.7226516965599867e+87\n",
      "Gradient Descent(40/49): loss=3.265992154775495e+89\n",
      "Gradient Descent(41/49): loss=6.1920263837174214e+91\n",
      "Gradient Descent(42/49): loss=1.1739523219794295e+94\n",
      "Gradient Descent(43/49): loss=2.2257076583279966e+96\n",
      "Gradient Descent(44/49): loss=4.219740859651972e+98\n",
      "Gradient Descent(45/49): loss=8.000247856447049e+100\n",
      "Gradient Descent(46/49): loss=1.5167747947883368e+103\n",
      "Gradient Descent(47/49): loss=2.875668128520853e+105\n",
      "Gradient Descent(48/49): loss=5.452007255002291e+107\n",
      "Gradient Descent(49/49): loss=1.0336513735292073e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.8237622125641355\n",
      "Gradient Descent(2/49): loss=931.3475711408261\n",
      "Gradient Descent(3/49): loss=171221.25367877536\n",
      "Gradient Descent(4/49): loss=31960383.159566775\n",
      "Gradient Descent(5/49): loss=5990777220.191165\n",
      "Gradient Descent(6/49): loss=1124376395946.004\n",
      "Gradient Descent(7/49): loss=211114945674679.94\n",
      "Gradient Descent(8/49): loss=3.964467012966895e+16\n",
      "Gradient Descent(9/49): loss=7.445091355237807e+18\n",
      "Gradient Descent(10/49): loss=1.3981755585256725e+21\n",
      "Gradient Descent(11/49): loss=2.6257630464419086e+23\n",
      "Gradient Descent(12/49): loss=4.931171147475831e+25\n",
      "Gradient Descent(13/49): loss=9.260722264692177e+27\n",
      "Gradient Descent(14/49): loss=1.739160741016367e+30\n",
      "Gradient Descent(15/49): loss=3.26613862591028e+32\n",
      "Gradient Descent(16/49): loss=6.133798614096602e+34\n",
      "Gradient Descent(17/49): loss=1.1519255612149421e+37\n",
      "Gradient Descent(18/49): loss=2.1633127937606744e+39\n",
      "Gradient Descent(19/49): loss=4.06269502544609e+41\n",
      "Gradient Descent(20/49): loss=7.629729238235899e+43\n",
      "Gradient Descent(21/49): loss=1.4328608937948018e+46\n",
      "Gradient Descent(22/49): loss=2.690908519710062e+48\n",
      "Gradient Descent(23/49): loss=5.0535182395410955e+50\n",
      "Gradient Descent(24/49): loss=9.490492304140975e+52\n",
      "Gradient Descent(25/49): loss=1.782311647167105e+55\n",
      "Gradient Descent(26/49): loss=3.347176000809363e+57\n",
      "Gradient Descent(27/49): loss=6.285986627649003e+59\n",
      "Gradient Descent(28/49): loss=1.1805064291040847e+62\n",
      "Gradient Descent(29/49): loss=2.216987581593554e+64\n",
      "Gradient Descent(30/49): loss=4.16349611977141e+66\n",
      "Gradient Descent(31/49): loss=7.819033396159867e+68\n",
      "Gradient Descent(32/49): loss=1.4684121587128943e+71\n",
      "Gradient Descent(33/49): loss=2.7576736900942888e+73\n",
      "Gradient Descent(34/49): loss=5.178903032037033e+75\n",
      "Gradient Descent(35/49): loss=9.725964573540708e+77\n",
      "Gradient Descent(36/49): loss=1.8265332697021325e+80\n",
      "Gradient Descent(37/49): loss=3.43022407711098e+82\n",
      "Gradient Descent(38/49): loss=6.441950669264644e+84\n",
      "Gradient Descent(39/49): loss=1.2097964299810635e+87\n",
      "Gradient Descent(40/49): loss=2.2719941165926366e+89\n",
      "Gradient Descent(41/49): loss=4.266798229775174e+91\n",
      "Gradient Descent(42/49): loss=8.013034453150722e+93\n",
      "Gradient Descent(43/49): loss=1.5048454998249053e+96\n",
      "Gradient Descent(44/49): loss=2.826095396922758e+98\n",
      "Gradient Descent(45/49): loss=5.3073987950505845e+100\n",
      "Gradient Descent(46/49): loss=9.967279236354187e+102\n",
      "Gradient Descent(47/49): loss=1.87185209199096e+105\n",
      "Gradient Descent(48/49): loss=3.515332691303812e+107\n",
      "Gradient Descent(49/49): loss=6.601784394944027e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.808267005350692\n",
      "Gradient Descent(2/49): loss=929.6799516327824\n",
      "Gradient Descent(3/49): loss=170821.95700844246\n",
      "Gradient Descent(4/49): loss=31865027.426136404\n",
      "Gradient Descent(5/49): loss=5969566826.59185\n",
      "Gradient Descent(6/49): loss=1119844732434.0\n",
      "Gradient Descent(7/49): loss=210167643940363.62\n",
      "Gradient Descent(8/49): loss=3.9449249533175704e+16\n",
      "Gradient Descent(9/49): loss=7.405145945517262e+18\n",
      "Gradient Descent(10/49): loss=1.3900678049407776e+21\n",
      "Gradient Descent(11/49): loss=2.6094014599624494e+23\n",
      "Gradient Descent(12/49): loss=4.898314761036293e+25\n",
      "Gradient Descent(13/49): loss=9.195022044684345e+27\n",
      "Gradient Descent(14/49): loss=1.7260722172139087e+30\n",
      "Gradient Descent(15/49): loss=3.240150510675171e+32\n",
      "Gradient Descent(16/49): loss=6.082350155476505e+34\n",
      "Gradient Descent(17/49): loss=1.1417674468305155e+37\n",
      "Gradient Descent(18/49): loss=2.1433046001564565e+39\n",
      "Gradient Descent(19/49): loss=4.023371507752199e+41\n",
      "Gradient Descent(20/49): loss=7.552598120787675e+43\n",
      "Gradient Descent(21/49): loss=1.4177596642883046e+46\n",
      "Gradient Descent(22/49): loss=2.661392058234588e+48\n",
      "Gradient Descent(23/49): loss=4.995915645050361e+50\n",
      "Gradient Descent(24/49): loss=9.378239878397297e+52\n",
      "Gradient Descent(25/49): loss=1.7604657377281793e+55\n",
      "Gradient Descent(26/49): loss=3.3047135218365906e+57\n",
      "Gradient Descent(27/49): loss=6.203546724803141e+59\n",
      "Gradient Descent(28/49): loss=1.1645182468170784e+62\n",
      "Gradient Descent(29/49): loss=2.1860119820617356e+64\n",
      "Gradient Descent(30/49): loss=4.1035410125850664e+66\n",
      "Gradient Descent(31/49): loss=7.7030908248206e+68\n",
      "Gradient Descent(32/49): loss=1.446009874726591e+71\n",
      "Gradient Descent(33/49): loss=2.7144228276128514e+73\n",
      "Gradient Descent(34/49): loss=5.095464018500487e+75\n",
      "Gradient Descent(35/49): loss=9.565110232537515e+77\n",
      "Gradient Descent(36/49): loss=1.7955446928564167e+80\n",
      "Gradient Descent(37/49): loss=3.370563083609733e+82\n",
      "Gradient Descent(38/49): loss=6.327158296750405e+84\n",
      "Gradient Descent(39/49): loss=1.1877223810706463e+87\n",
      "Gradient Descent(40/49): loss=2.2295703510699973e+89\n",
      "Gradient Descent(41/49): loss=4.185307972296857e+91\n",
      "Gradient Descent(42/49): loss=7.856582239966154e+93\n",
      "Gradient Descent(43/49): loss=1.4748229975410204e+96\n",
      "Gradient Descent(44/49): loss=2.768510285568214e+98\n",
      "Gradient Descent(45/49): loss=5.196995988044864e+100\n",
      "Gradient Descent(46/49): loss=9.755704156327875e+102\n",
      "Gradient Descent(47/49): loss=1.831322629548493e+105\n",
      "Gradient Descent(48/49): loss=3.437724760565922e+107\n",
      "Gradient Descent(49/49): loss=6.453232946901163e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.883372229569867\n",
      "Gradient Descent(2/49): loss=961.0053562512967\n",
      "Gradient Descent(3/49): loss=180109.72288475075\n",
      "Gradient Descent(4/49): loss=34247284.609778576\n",
      "Gradient Descent(5/49): loss=6537401246.435822\n",
      "Gradient Descent(6/49): loss=1249374269021.974\n",
      "Gradient Descent(7/49): loss=238857892893633.53\n",
      "Gradient Descent(8/49): loss=4.567071211786588e+16\n",
      "Gradient Descent(9/49): loss=8.732779851024378e+18\n",
      "Gradient Descent(10/49): loss=1.6698313591694366e+21\n",
      "Gradient Descent(11/49): loss=3.192967231229251e+23\n",
      "Gradient Descent(12/49): loss=6.105438758324799e+25\n",
      "Gradient Descent(13/49): loss=1.1674532071241623e+28\n",
      "Gradient Descent(14/49): loss=2.2323492779246702e+30\n",
      "Gradient Descent(15/49): loss=4.2685938023161615e+32\n",
      "Gradient Descent(16/49): loss=8.162205394249408e+34\n",
      "Gradient Descent(17/49): loss=1.5607387421628307e+37\n",
      "Gradient Descent(18/49): loss=2.9843716341921417e+39\n",
      "Gradient Descent(19/49): loss=5.706575876482163e+41\n",
      "Gradient Descent(20/49): loss=1.0911847528091078e+44\n",
      "Gradient Descent(21/49): loss=2.086512456155413e+46\n",
      "Gradient Descent(22/49): loss=3.9897315450577724e+48\n",
      "Gradient Descent(23/49): loss=7.628978084823367e+50\n",
      "Gradient Descent(24/49): loss=1.4587775132620216e+53\n",
      "Gradient Descent(25/49): loss=2.789406142657058e+55\n",
      "Gradient Descent(26/49): loss=5.333771982333075e+57\n",
      "Gradient Descent(27/49): loss=1.0198989356359477e+60\n",
      "Gradient Descent(28/49): loss=1.950203050218063e+62\n",
      "Gradient Descent(29/49): loss=3.729087073424935e+64\n",
      "Gradient Descent(30/49): loss=7.130585914953877e+66\n",
      "Gradient Descent(31/49): loss=1.3634772932196582e+69\n",
      "Gradient Descent(32/49): loss=2.6071775185077833e+71\n",
      "Gradient Descent(33/49): loss=4.98532292896599e+73\n",
      "Gradient Descent(34/49): loss=9.532701371366073e+75\n",
      "Gradient Descent(35/49): loss=1.8227985775535847e+78\n",
      "Gradient Descent(36/49): loss=3.4854701987325655e+80\n",
      "Gradient Descent(37/49): loss=6.664753119654967e+82\n",
      "Gradient Descent(38/49): loss=1.274402924520846e+85\n",
      "Gradient Descent(39/49): loss=2.4368536761514466e+87\n",
      "Gradient Descent(40/49): loss=4.6596376426282093e+89\n",
      "Gradient Descent(41/49): loss=8.909941197162143e+91\n",
      "Gradient Descent(42/49): loss=1.7037172893149992e+94\n",
      "Gradient Descent(43/49): loss=3.257768528074411e+96\n",
      "Gradient Descent(44/49): loss=6.229352633252471e+98\n",
      "Gradient Descent(45/49): loss=1.1911476796157154e+101\n",
      "Gradient Descent(46/49): loss=2.277656890188135e+103\n",
      "Gradient Descent(47/49): loss=4.3552289931800494e+105\n",
      "Gradient Descent(48/49): loss=8.327865213038863e+107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=1.592415441648292e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.858299378691449\n",
      "Gradient Descent(2/49): loss=954.4258077970158\n",
      "Gradient Descent(3/49): loss=178853.3381927811\n",
      "Gradient Descent(4/49): loss=34047814.73824659\n",
      "Gradient Descent(5/49): loss=6510151912.710829\n",
      "Gradient Descent(6/49): loss=1246472129532.6416\n",
      "Gradient Descent(7/49): loss=238760850095910.66\n",
      "Gradient Descent(8/49): loss=4.574097583843282e+16\n",
      "Gradient Descent(9/49): loss=8.763308058327921e+18\n",
      "Gradient Descent(10/49): loss=1.6789488216758687e+21\n",
      "Gradient Descent(11/49): loss=3.2166889311265807e+23\n",
      "Gradient Descent(12/49): loss=6.162847332018548e+25\n",
      "Gradient Descent(13/49): loss=1.1807392461982475e+28\n",
      "Gradient Descent(14/49): loss=2.2621774528022286e+30\n",
      "Gradient Descent(15/49): loss=4.334104385388526e+32\n",
      "Gradient Descent(16/49): loss=8.3037080870818e+34\n",
      "Gradient Descent(17/49): loss=1.590906963327833e+37\n",
      "Gradient Descent(18/49): loss=3.0480177592199397e+39\n",
      "Gradient Descent(19/49): loss=5.839695520624961e+41\n",
      "Gradient Descent(20/49): loss=1.1188269385543705e+44\n",
      "Gradient Descent(21/49): loss=2.1435599067056109e+46\n",
      "Gradient Descent(22/49): loss=4.1068452281799547e+48\n",
      "Gradient Descent(23/49): loss=7.868302479261442e+50\n",
      "Gradient Descent(24/49): loss=1.507487632612207e+53\n",
      "Gradient Descent(25/49): loss=2.88819471349852e+55\n",
      "Gradient Descent(26/49): loss=5.533490638745029e+57\n",
      "Gradient Descent(27/49): loss=1.060161162472047e+60\n",
      "Gradient Descent(28/49): loss=2.0311621791575434e+62\n",
      "Gradient Descent(29/49): loss=3.891502484792149e+64\n",
      "Gradient Descent(30/49): loss=7.455727437493231e+66\n",
      "Gradient Descent(31/49): loss=1.4284424034013885e+69\n",
      "Gradient Descent(32/49): loss=2.736752002995929e+71\n",
      "Gradient Descent(33/49): loss=5.243341634263713e+73\n",
      "Gradient Descent(34/49): loss=1.0045715309062322e+76\n",
      "Gradient Descent(35/49): loss=1.924658035083394e+78\n",
      "Gradient Descent(36/49): loss=3.687451254635282e+80\n",
      "Gradient Descent(37/49): loss=7.064785799583404e+82\n",
      "Gradient Descent(38/49): loss=1.3535419168254863e+85\n",
      "Gradient Descent(39/49): loss=2.593250202591607e+87\n",
      "Gradient Descent(40/49): loss=4.9684066150043575e+89\n",
      "Gradient Descent(41/49): loss=9.518967459194506e+91\n",
      "Gradient Descent(42/49): loss=1.823738443942239e+94\n",
      "Gradient Descent(43/49): loss=3.494099466324167e+96\n",
      "Gradient Descent(44/49): loss=6.694343216331008e+98\n",
      "Gradient Descent(45/49): loss=1.2825688429866664e+101\n",
      "Gradient Descent(46/49): loss=2.4572729300570927e+103\n",
      "Gradient Descent(47/49): loss=4.707887834488881e+105\n",
      "Gradient Descent(48/49): loss=9.019839673085617e+107\n",
      "Gradient Descent(49/49): loss=1.7281105792742833e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.882139365840681\n",
      "Gradient Descent(2/49): loss=951.1265391761722\n",
      "Gradient Descent(3/49): loss=176698.92367962378\n",
      "Gradient Descent(4/49): loss=33330414.187760435\n",
      "Gradient Descent(5/49): loss=6313450751.6592245\n",
      "Gradient Descent(6/49): loss=1197432978095.5032\n",
      "Gradient Descent(7/49): loss=227203351424312.47\n",
      "Gradient Descent(8/49): loss=4.311585238084683e+16\n",
      "Gradient Descent(9/49): loss=8.182361716054337e+18\n",
      "Gradient Descent(10/49): loss=1.552840495661579e+21\n",
      "Gradient Descent(11/49): loss=2.946979958398021e+23\n",
      "Gradient Descent(12/49): loss=5.592786460121069e+25\n",
      "Gradient Descent(13/49): loss=1.0614011015722077e+28\n",
      "Gradient Descent(14/49): loss=2.014331344966062e+30\n",
      "Gradient Descent(15/49): loss=3.822806489078538e+32\n",
      "Gradient Descent(16/49): loss=7.254938365471074e+34\n",
      "Gradient Descent(17/49): loss=1.3768452889551726e+37\n",
      "Gradient Descent(18/49): loss=2.612982851957438e+39\n",
      "Gradient Descent(19/49): loss=4.9589299862998086e+41\n",
      "Gradient Descent(20/49): loss=9.411078452714123e+43\n",
      "Gradient Descent(21/49): loss=1.7860384779673469e+46\n",
      "Gradient Descent(22/49): loss=3.389551432282293e+48\n",
      "Gradient Descent(23/49): loss=6.432705148252473e+50\n",
      "Gradient Descent(24/49): loss=1.2208015234813708e+53\n",
      "Gradient Descent(25/49): loss=2.316842332095568e+55\n",
      "Gradient Descent(26/49): loss=4.396913248014865e+57\n",
      "Gradient Descent(27/49): loss=8.344480693722683e+59\n",
      "Gradient Descent(28/49): loss=1.5836191009533878e+62\n",
      "Gradient Descent(29/49): loss=3.005399076291282e+64\n",
      "Gradient Descent(30/49): loss=5.703659170525714e+66\n",
      "Gradient Descent(31/49): loss=1.0824428672436771e+69\n",
      "Gradient Descent(32/49): loss=2.0542646848561892e+71\n",
      "Gradient Descent(33/49): loss=3.8985922704567995e+73\n",
      "Gradient Descent(34/49): loss=7.398765019576564e+75\n",
      "Gradient Descent(35/49): loss=1.4041407774220964e+78\n",
      "Gradient Descent(36/49): loss=2.664784349283688e+80\n",
      "Gradient Descent(37/49): loss=5.057239090530794e+82\n",
      "Gradient Descent(38/49): loss=9.597649890756694e+84\n",
      "Gradient Descent(39/49): loss=1.8214460850391247e+87\n",
      "Gradient Descent(40/49): loss=3.456748139874893e+89\n",
      "Gradient Descent(41/49): loss=6.560231346222814e+91\n",
      "Gradient Descent(42/49): loss=1.2450034996626048e+94\n",
      "Gradient Descent(43/49): loss=2.362772945598334e+96\n",
      "Gradient Descent(44/49): loss=4.4840805619938735e+98\n",
      "Gradient Descent(45/49): loss=8.509907193540962e+100\n",
      "Gradient Descent(46/49): loss=1.615013812563594e+103\n",
      "Gradient Descent(47/49): loss=3.0649800937322593e+105\n",
      "Gradient Descent(48/49): loss=5.816732279251077e+107\n",
      "Gradient Descent(49/49): loss=1.1039019299887461e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.866149473409925\n",
      "Gradient Descent(2/49): loss=949.3261514367073\n",
      "Gradient Descent(3/49): loss=176259.96847673986\n",
      "Gradient Descent(4/49): loss=33224215.18408858\n",
      "Gradient Descent(5/49): loss=6289498366.748747\n",
      "Gradient Descent(6/49): loss=1192242869039.18\n",
      "Gradient Descent(7/49): loss=226103306604044.4\n",
      "Gradient Descent(8/49): loss=4.288585820236054e+16\n",
      "Gradient Descent(9/49): loss=8.134733881279968e+18\n",
      "Gradient Descent(10/49): loss=1.543050384830313e+21\n",
      "Gradient Descent(11/49): loss=2.9269777984869554e+23\n",
      "Gradient Descent(12/49): loss=5.5521299841732105e+25\n",
      "Gradient Descent(13/49): loss=1.0531739809360188e+28\n",
      "Gradient Descent(14/49): loss=1.997748068703298e+30\n",
      "Gradient Descent(15/49): loss=3.789495125637343e+32\n",
      "Gradient Descent(16/49): loss=7.188230551792668e+34\n",
      "Gradient Descent(17/49): loss=1.3635235628604696e+37\n",
      "Gradient Descent(18/49): loss=2.5864452997913114e+39\n",
      "Gradient Descent(19/49): loss=4.90618532629242e+41\n",
      "Gradient Descent(20/49): loss=9.30646183273438e+43\n",
      "Gradient Descent(21/49): loss=1.7653273591420952e+46\n",
      "Gradient Descent(22/49): loss=3.348620282426519e+48\n",
      "Gradient Descent(23/49): loss=6.3519424530392705e+50\n",
      "Gradient Descent(24/49): loss=1.204889462644368e+53\n",
      "Gradient Descent(25/49): loss=2.2855349020036133e+55\n",
      "Gradient Descent(26/49): loss=4.335393370289294e+57\n",
      "Gradient Descent(27/49): loss=8.223736009752003e+59\n",
      "Gradient Descent(28/49): loss=1.5599468879010497e+62\n",
      "Gradient Descent(29/49): loss=2.9590374620325717e+64\n",
      "Gradient Descent(30/49): loss=5.61294924181267e+66\n",
      "Gradient Descent(31/49): loss=1.0647110621412819e+69\n",
      "Gradient Descent(32/49): loss=2.0196328115732735e+71\n",
      "Gradient Descent(33/49): loss=3.8310080909463957e+73\n",
      "Gradient Descent(34/49): loss=7.2669759120539e+75\n",
      "Gradient Descent(35/49): loss=1.3784606467204248e+78\n",
      "Gradient Descent(36/49): loss=2.6147792115356673e+80\n",
      "Gradient Descent(37/49): loss=4.95993145785161e+82\n",
      "Gradient Descent(38/49): loss=9.408411982951995e+84\n",
      "Gradient Descent(39/49): loss=1.7846661147066745e+87\n",
      "Gradient Descent(40/49): loss=3.3853036482176613e+89\n",
      "Gradient Descent(41/49): loss=6.4215265231947774e+91\n",
      "Gradient Descent(42/49): loss=1.2180887498763697e+94\n",
      "Gradient Descent(43/49): loss=2.3105724117405096e+96\n",
      "Gradient Descent(44/49): loss=4.3828866085810356e+98\n",
      "Gradient Descent(45/49): loss=8.313825148292478e+100\n",
      "Gradient Descent(46/49): loss=1.5770357476521118e+103\n",
      "Gradient Descent(47/49): loss=2.9914530375749477e+105\n",
      "Gradient Descent(48/49): loss=5.6744378111525605e+107\n",
      "Gradient Descent(49/49): loss=1.0763747272041627e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.942061784980885\n",
      "Gradient Descent(2/49): loss=981.2996772520102\n",
      "Gradient Descent(3/49): loss=185839.5209632573\n",
      "Gradient Descent(4/49): loss=35707126.54984263\n",
      "Gradient Descent(5/49): loss=6887537942.010927\n",
      "Gradient Descent(6/49): loss=1330094280282.4348\n",
      "Gradient Descent(7/49): loss=256957221027164.56\n",
      "Gradient Descent(8/49): loss=4.964671452062503e+16\n",
      "Gradient Descent(9/49): loss=9.592610159541082e+18\n",
      "Gradient Descent(10/49): loss=1.8534824123771393e+21\n",
      "Gradient Descent(11/49): loss=3.581309863526825e+23\n",
      "Gradient Descent(12/49): loss=6.919837566995222e+25\n",
      "Gradient Descent(13/49): loss=1.3370575104257326e+28\n",
      "Gradient Descent(14/49): loss=2.583475434112944e+30\n",
      "Gradient Descent(15/49): loss=4.991816410985285e+32\n",
      "Gradient Descent(16/49): loss=9.645236460275107e+34\n",
      "Gradient Descent(17/49): loss=1.8636620253224668e+37\n",
      "Gradient Descent(18/49): loss=3.6009860042513274e+39\n",
      "Gradient Descent(19/49): loss=6.95786040242272e+41\n",
      "Gradient Descent(20/49): loss=1.3444045972567507e+44\n",
      "Gradient Descent(21/49): loss=2.5976717218911553e+46\n",
      "Gradient Descent(22/49): loss=5.019246727214654e+48\n",
      "Gradient Descent(23/49): loss=9.698237655087848e+50\n",
      "Gradient Descent(24/49): loss=1.8739029724274336e+53\n",
      "Gradient Descent(25/49): loss=3.62077366523633e+55\n",
      "Gradient Descent(26/49): loss=6.996094316393022e+57\n",
      "Gradient Descent(27/49): loss=1.3517921916468368e+60\n",
      "Gradient Descent(28/49): loss=2.611946104150763e+62\n",
      "Gradient Descent(29/49): loss=5.046827828378768e+64\n",
      "Gradient Descent(30/49): loss=9.75153012875041e+66\n",
      "Gradient Descent(31/49): loss=1.8842001963533175e+69\n",
      "Gradient Descent(32/49): loss=3.6406700620970337e+71\n",
      "Gradient Descent(33/49): loss=7.034538329155496e+73\n",
      "Gradient Descent(34/49): loss=1.3592203814221571e+76\n",
      "Gradient Descent(35/49): loss=2.62629892514238e+78\n",
      "Gradient Descent(36/49): loss=5.074560489585349e+80\n",
      "Gradient Descent(37/49): loss=9.805115448183287e+82\n",
      "Gradient Descent(38/49): loss=1.8945540042238835e+85\n",
      "Gradient Descent(39/49): loss=3.6606757910084647e+87\n",
      "Gradient Descent(40/49): loss=7.073193594375837e+89\n",
      "Gradient Descent(41/49): loss=1.3666893896041146e+92\n",
      "Gradient Descent(42/49): loss=2.640730615858817e+94\n",
      "Gradient Descent(43/49): loss=5.102445543646214e+96\n",
      "Gradient Descent(44/49): loss=9.858995222580839e+98\n",
      "Gradient Descent(45/49): loss=1.9049647069709312e+101\n",
      "Gradient Descent(46/49): loss=3.680791452757105e+103\n",
      "Gradient Descent(47/49): loss=7.112061272900245e+105\n",
      "Gradient Descent(48/49): loss=1.3741994404925927e+108\n",
      "Gradient Descent(49/49): loss=2.6552416096945135e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.916687891980389\n",
      "Gradient Descent(2/49): loss=974.5823599330565\n",
      "Gradient Descent(3/49): loss=184543.0997798908\n",
      "Gradient Descent(4/49): loss=35499086.46211549\n",
      "Gradient Descent(5/49): loss=6858808254.593786\n",
      "Gradient Descent(6/49): loss=1326999393247.3218\n",
      "Gradient Descent(7/49): loss=256851608425687.78\n",
      "Gradient Descent(8/49): loss=4.972282358955372e+16\n",
      "Gradient Descent(9/49): loss=9.62608480970681e+18\n",
      "Gradient Descent(10/49): loss=1.8635898441938347e+21\n",
      "Gradient Descent(11/49): loss=3.607889460759376e+23\n",
      "Gradient Descent(12/49): loss=6.984846278917101e+25\n",
      "Gradient Descent(13/49): loss=1.352261639464747e+28\n",
      "Gradient Descent(14/49): loss=2.6179701200311337e+30\n",
      "Gradient Descent(15/49): loss=5.0683741788762226e+32\n",
      "Gradient Descent(16/49): loss=9.812341688586799e+34\n",
      "Gradient Descent(17/49): loss=1.899663416235547e+37\n",
      "Gradient Descent(18/49): loss=3.6777368927069276e+39\n",
      "Gradient Descent(19/49): loss=7.120076402267432e+41\n",
      "Gradient Descent(20/49): loss=1.3784424896559104e+44\n",
      "Gradient Descent(21/49): loss=2.6686563318921303e+46\n",
      "Gradient Descent(22/49): loss=5.166502535569733e+48\n",
      "Gradient Descent(23/49): loss=1.0002317694975323e+51\n",
      "Gradient Descent(24/49): loss=1.9364426627632956e+53\n",
      "Gradient Descent(25/49): loss=3.7489412959331463e+55\n",
      "Gradient Descent(26/49): loss=7.257927699393793e+57\n",
      "Gradient Descent(27/49): loss=1.4051304176669321e+60\n",
      "Gradient Descent(28/49): loss=2.720323999394249e+62\n",
      "Gradient Descent(29/49): loss=5.266530827770153e+64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=1.0195971864391695e+67\n",
      "Gradient Descent(31/49): loss=1.9739339929674907e+69\n",
      "Gradient Descent(32/49): loss=3.8215242847034383e+71\n",
      "Gradient Descent(33/49): loss=7.39844792713831e+73\n",
      "Gradient Descent(34/49): loss=1.4323350488619192e+76\n",
      "Gradient Descent(35/49): loss=2.772992001028816e+78\n",
      "Gradient Descent(36/49): loss=5.368495760736688e+80\n",
      "Gradient Descent(37/49): loss=1.0393375358585573e+83\n",
      "Gradient Descent(38/49): loss=2.0121511901805302e+85\n",
      "Gradient Descent(39/49): loss=3.8955125476156265e+87\n",
      "Gradient Descent(40/49): loss=7.541688757130219e+89\n",
      "Gradient Descent(41/49): loss=1.4600663870082495e+92\n",
      "Gradient Descent(42/49): loss=2.826679704138991e+94\n",
      "Gradient Descent(43/49): loss=5.472434829599381e+96\n",
      "Gradient Descent(44/49): loss=1.0594600767947452e+99\n",
      "Gradient Descent(45/49): loss=2.0511083078610228e+101\n",
      "Gradient Descent(46/49): loss=3.970933292082471e+103\n",
      "Gradient Descent(47/49): loss=7.68770286275752e+105\n",
      "Gradient Descent(48/49): loss=1.4883346296420915e+108\n",
      "Gradient Descent(49/49): loss=2.8814068510932295e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.940831448675136\n",
      "Gradient Descent(2/49): loss=971.2190687693402\n",
      "Gradient Descent(3/49): loss=182321.85996340014\n",
      "Gradient Descent(4/49): loss=34751560.2787651\n",
      "Gradient Descent(5/49): loss=6651682708.484166\n",
      "Gradient Descent(6/49): loss=1274817559980.4832\n",
      "Gradient Descent(7/49): loss=244424107403636.5\n",
      "Gradient Descent(8/49): loss=4.6870427832169544e+16\n",
      "Gradient Descent(9/49): loss=8.988212123216e+18\n",
      "Gradient Descent(10/49): loss=1.723670338100559e+21\n",
      "Gradient Descent(11/49): loss=3.305500841336189e+23\n",
      "Gradient Descent(12/49): loss=6.339004507634985e+25\n",
      "Gradient Descent(13/49): loss=1.2156403063901263e+28\n",
      "Gradient Descent(14/49): loss=2.3312518995766606e+30\n",
      "Gradient Descent(15/49): loss=4.4706774934824284e+32\n",
      "Gradient Descent(16/49): loss=8.573486919517575e+34\n",
      "Gradient Descent(17/49): loss=1.6441507708452518e+37\n",
      "Gradient Descent(18/49): loss=3.1530132243064075e+39\n",
      "Gradient Descent(19/49): loss=6.046581966007641e+41\n",
      "Gradient Descent(20/49): loss=1.1595623259463915e+44\n",
      "Gradient Descent(21/49): loss=2.223710511872855e+46\n",
      "Gradient Descent(22/49): loss=4.264443859644941e+48\n",
      "Gradient Descent(23/49): loss=8.177989596732275e+50\n",
      "Gradient Descent(24/49): loss=1.5683056465432506e+53\n",
      "Gradient Descent(25/49): loss=3.007563866264761e+55\n",
      "Gradient Descent(26/49): loss=5.767651496761516e+57\n",
      "Gradient Descent(27/49): loss=1.1060714008847933e+60\n",
      "Gradient Descent(28/49): loss=2.1211301420382668e+62\n",
      "Gradient Descent(29/49): loss=4.067723906308634e+64\n",
      "Gradient Descent(30/49): loss=7.800736715784327e+66\n",
      "Gradient Descent(31/49): loss=1.4959592811746972e+69\n",
      "Gradient Descent(32/49): loss=2.8688241283729996e+71\n",
      "Gradient Descent(33/49): loss=5.5015881669402024e+73\n",
      "Gradient Descent(34/49): loss=1.0550480267949434e+76\n",
      "Gradient Descent(35/49): loss=2.0232818325676024e+78\n",
      "Gradient Descent(36/49): loss=3.8800786978712015e+80\n",
      "Gradient Descent(37/49): loss=7.440886612701202e+82\n",
      "Gradient Descent(38/49): loss=1.4269502732883551e+85\n",
      "Gradient Descent(39/49): loss=2.736484492267415e+87\n",
      "Gradient Descent(40/49): loss=5.247798410776742e+89\n",
      "Gradient Descent(41/49): loss=1.0063783748078912e+92\n",
      "Gradient Descent(42/49): loss=1.9299472921847086e+94\n",
      "Gradient Descent(43/49): loss=3.7010896138562936e+96\n",
      "Gradient Descent(44/49): loss=7.097636492594956e+98\n",
      "Gradient Descent(45/49): loss=1.3611246696760314e+101\n",
      "Gradient Descent(46/49): loss=2.6102497195137974e+103\n",
      "Gradient Descent(47/49): loss=5.005716044984733e+105\n",
      "Gradient Descent(48/49): loss=9.599538670838388e+107\n",
      "Gradient Descent(49/49): loss=1.840918299495807e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.924342509262041\n",
      "Gradient Descent(2/49): loss=969.2822794122594\n",
      "Gradient Descent(3/49): loss=181841.48868534344\n",
      "Gradient Descent(4/49): loss=34633863.96889412\n",
      "Gradient Descent(5/49): loss=6624779784.961066\n",
      "Gradient Descent(6/49): loss=1268908655694.6428\n",
      "Gradient Descent(7/49): loss=243154957998313.53\n",
      "Gradient Descent(8/49): loss=4.660162648638808e+16\n",
      "Gradient Descent(9/49): loss=8.931844204849017e+18\n",
      "Gradient Descent(10/49): loss=1.7119407997852488e+21\n",
      "Gradient Descent(11/49): loss=3.281246969445393e+23\n",
      "Gradient Descent(12/49): loss=6.289121378501357e+25\n",
      "Gradient Descent(13/49): loss=1.205428164075314e+28\n",
      "Gradient Descent(14/49): loss=2.3104298920831823e+30\n",
      "Gradient Descent(15/49): loss=4.428373981543598e+32\n",
      "Gradient Descent(16/49): loss=8.487812908987191e+34\n",
      "Gradient Descent(17/49): loss=1.626849244446745e+37\n",
      "Gradient Descent(18/49): loss=3.1181630663924314e+39\n",
      "Gradient Descent(19/49): loss=5.976546968053284e+41\n",
      "Gradient Descent(20/49): loss=1.1455178229724492e+44\n",
      "Gradient Descent(21/49): loss=2.1956007205023957e+46\n",
      "Gradient Descent(22/49): loss=4.208282426872376e+48\n",
      "Gradient Descent(23/49): loss=8.065966101768553e+50\n",
      "Gradient Descent(24/49): loss=1.545994364346145e+53\n",
      "Gradient Descent(25/49): loss=2.96318946104946e+55\n",
      "Gradient Descent(26/49): loss=5.679510860179278e+57\n",
      "Gradient Descent(27/49): loss=1.0885852570316879e+60\n",
      "Gradient Descent(28/49): loss=2.0864787320600687e+62\n",
      "Gradient Descent(29/49): loss=3.99912957778776e+64\n",
      "Gradient Descent(30/49): loss=7.665085262645552e+66\n",
      "Gradient Descent(31/49): loss=1.4691579990295688e+69\n",
      "Gradient Descent(32/49): loss=2.8159180911284374e+71\n",
      "Gradient Descent(33/49): loss=5.3972375341400284e+73\n",
      "Gradient Descent(34/49): loss=1.0344822561318392e+76\n",
      "Gradient Descent(35/49): loss=1.982780137954649e+78\n",
      "Gradient Descent(36/49): loss=3.800371685608131e+80\n",
      "Gradient Descent(37/49): loss=7.28412831675354e+82\n",
      "Gradient Descent(38/49): loss=1.3961404232081185e+85\n",
      "Gradient Descent(39/49): loss=2.6759661507232898e+87\n",
      "Gradient Descent(40/49): loss=5.128993273729851e+89\n",
      "Gradient Descent(41/49): loss=9.830681899640521e+91\n",
      "Gradient Descent(42/49): loss=1.884235393852249e+94\n",
      "Gradient Descent(43/49): loss=3.6114921179327255e+96\n",
      "Gradient Descent(44/49): loss=6.922105040827445e+98\n",
      "Gradient Descent(45/49): loss=1.3267518419416185e+101\n",
      "Gradient Descent(46/49): loss=2.542969862076898e+103\n",
      "Gradient Descent(47/49): loss=4.874080830344113e+105\n",
      "Gradient Descent(48/49): loss=9.34209417697364e+107\n",
      "Gradient Descent(49/49): loss=1.7905883519228157e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.001066187023666\n",
      "Gradient Descent(2/49): loss=1001.9139289889295\n",
      "Gradient Descent(3/49): loss=191720.42590619795\n",
      "Gradient Descent(4/49): loss=37221130.92336168\n",
      "Gradient Descent(5/49): loss=7254462874.310323\n",
      "Gradient Descent(6/49): loss=1415569429561.4146\n",
      "Gradient Descent(7/49): loss=276323244544978.4\n",
      "Gradient Descent(8/49): loss=5.394548320698885e+16\n",
      "Gradient Descent(9/49): loss=1.053196490947655e+19\n",
      "Gradient Descent(10/49): loss=2.0562178812015086e+21\n",
      "Gradient Descent(11/49): loss=4.014492235814798e+23\n",
      "Gradient Descent(12/49): loss=7.837773117359166e+25\n",
      "Gradient Descent(13/49): loss=1.5302237575923282e+28\n",
      "Gradient Descent(14/49): loss=2.987564251545233e+30\n",
      "Gradient Descent(15/49): loss=5.8328336116471815e+32\n",
      "Gradient Descent(16/49): loss=1.1387854983426855e+35\n",
      "Gradient Descent(17/49): loss=2.223331752469851e+37\n",
      "Gradient Descent(18/49): loss=4.3407683857964075e+39\n",
      "Gradient Descent(19/49): loss=8.474790218769856e+41\n",
      "Gradient Descent(20/49): loss=1.6545934470802522e+44\n",
      "Gradient Descent(21/49): loss=3.2303802272400165e+46\n",
      "Gradient Descent(22/49): loss=6.306900605188386e+48\n",
      "Gradient Descent(23/49): loss=1.231340970593556e+51\n",
      "Gradient Descent(24/49): loss=2.4040343756454228e+53\n",
      "Gradient Descent(25/49): loss=4.6935669463695505e+55\n",
      "Gradient Descent(26/49): loss=9.163583891824708e+57\n",
      "Gradient Descent(27/49): loss=1.7890715249617558e+60\n",
      "Gradient Descent(28/49): loss=3.4929313238291243e+62\n",
      "Gradient Descent(29/49): loss=6.819497746602133e+64\n",
      "Gradient Descent(30/49): loss=1.3314189488538273e+67\n",
      "Gradient Descent(31/49): loss=2.599423715991812e+69\n",
      "Gradient Descent(32/49): loss=5.075039423975121e+71\n",
      "Gradient Descent(33/49): loss=9.908359686206236e+73\n",
      "Gradient Descent(34/49): loss=1.9344793896071692e+76\n",
      "Gradient Descent(35/49): loss=3.776821418811201e+78\n",
      "Gradient Descent(36/49): loss=7.373756529134039e+80\n",
      "Gradient Descent(37/49): loss=1.4396308250142643e+83\n",
      "Gradient Descent(38/49): loss=2.8106934425384835e+85\n",
      "Gradient Descent(39/49): loss=5.487516306724401e+87\n",
      "Gradient Descent(40/49): loss=1.0713667581395684e+90\n",
      "Gradient Descent(41/49): loss=2.0917053659411268e+92\n",
      "Gradient Descent(42/49): loss=4.0837848520748495e+94\n",
      "Gradient Descent(43/49): loss=7.973063027704392e+96\n",
      "Gradient Descent(44/49): loss=1.5566376865188945e+99\n",
      "Gradient Descent(45/49): loss=3.039134243227664e+101\n",
      "Gradient Descent(46/49): loss=5.933517496299458e+103\n",
      "Gradient Descent(47/49): loss=1.1584427360306736e+106\n",
      "Gradient Descent(48/49): loss=2.261709978101842e+108\n",
      "Gradient Descent(49/49): loss=4.415696923071745e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.97538951660466\n",
      "Gradient Descent(2/49): loss=995.0566912241109\n",
      "Gradient Descent(3/49): loss=190382.9109521154\n",
      "Gradient Descent(4/49): loss=37004200.2079656\n",
      "Gradient Descent(5/49): loss=7224180862.298597\n",
      "Gradient Descent(6/49): loss=1412270133672.9963\n",
      "Gradient Descent(7/49): loss=276208377757716.44\n",
      "Gradient Descent(8/49): loss=5.402789021178255e+16\n",
      "Gradient Descent(9/49): loss=1.056865305409655e+19\n",
      "Gradient Descent(10/49): loss=2.0674168353180518e+21\n",
      "Gradient Descent(11/49): loss=4.0442565877069746e+23\n",
      "Gradient Descent(12/49): loss=7.911340918874492e+25\n",
      "Gradient Descent(13/49): loss=1.5476107761136688e+28\n",
      "Gradient Descent(14/49): loss=3.027425554561346e+30\n",
      "Gradient Descent(15/49): loss=5.922229416226121e+32\n",
      "Gradient Descent(16/49): loss=1.1585025413953796e+35\n",
      "Gradient Descent(17/49): loss=2.2662549073410853e+37\n",
      "Gradient Descent(18/49): loss=4.433232671755253e+39\n",
      "Gradient Descent(19/49): loss=8.67226006801549e+41\n",
      "Gradient Descent(20/49): loss=1.6964617080772508e+44\n",
      "Gradient Descent(21/49): loss=3.3186070351235914e+46\n",
      "Gradient Descent(22/49): loss=6.49183686342116e+48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=1.2699287808293838e+51\n",
      "Gradient Descent(24/49): loss=2.4842261786810522e+53\n",
      "Gradient Descent(25/49): loss=4.859626618447096e+55\n",
      "Gradient Descent(26/49): loss=9.506369055037454e+57\n",
      "Gradient Descent(27/49): loss=1.8596295498822202e+60\n",
      "Gradient Descent(28/49): loss=3.6377948749662134e+62\n",
      "Gradient Descent(29/49): loss=7.116229978798069e+64\n",
      "Gradient Descent(30/49): loss=1.392072144024194e+67\n",
      "Gradient Descent(31/49): loss=2.7231622079974377e+69\n",
      "Gradient Descent(32/49): loss=5.327031679283843e+71\n",
      "Gradient Descent(33/49): loss=1.0420703705697275e+74\n",
      "Gradient Descent(34/49): loss=2.0384910820829113e+76\n",
      "Gradient Descent(35/49): loss=3.9876826067510886e+78\n",
      "Gradient Descent(36/49): loss=7.800678017162136e+80\n",
      "Gradient Descent(37/49): loss=1.525963411040223e+83\n",
      "Gradient Descent(38/49): loss=2.9850794081110315e+85\n",
      "Gradient Descent(39/49): loss=5.839392352569078e+87\n",
      "Gradient Descent(40/49): loss=1.1422980224442224e+90\n",
      "Gradient Descent(41/49): loss=2.234555743639868e+92\n",
      "Gradient Descent(42/49): loss=4.37122298500497e+94\n",
      "Gradient Descent(43/49): loss=8.550957137238963e+96\n",
      "Gradient Descent(44/49): loss=1.6727325101859405e+99\n",
      "Gradient Descent(45/49): loss=3.272188137217586e+101\n",
      "Gradient Descent(46/49): loss=6.401032526209054e+103\n",
      "Gradient Descent(47/49): loss=1.2521656971847178e+106\n",
      "Gradient Descent(48/49): loss=2.449478153385787e+108\n",
      "Gradient Descent(49/49): loss=4.7916527640104795e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.999838461067495\n",
      "Gradient Descent(2/49): loss=991.628457315616\n",
      "Gradient Descent(3/49): loss=188093.128358025\n",
      "Gradient Descent(4/49): loss=36225447.44916574\n",
      "Gradient Descent(5/49): loss=7006136386.638294\n",
      "Gradient Descent(6/49): loss=1356761989739.0278\n",
      "Gradient Descent(7/49): loss=262850387701241.6\n",
      "Gradient Descent(8/49): loss=5.092987473442737e+16\n",
      "Gradient Descent(9/49): loss=9.86861417170255e+18\n",
      "Gradient Descent(10/49): loss=1.912256978060658e+21\n",
      "Gradient Descent(11/49): loss=3.705429176688567e+23\n",
      "Gradient Descent(12/49): loss=7.180116770538198e+25\n",
      "Gradient Descent(13/49): loss=1.3913126732520531e+28\n",
      "Gradient Descent(14/49): loss=2.6959886300887934e+30\n",
      "Gradient Descent(15/49): loss=5.224099003053853e+32\n",
      "Gradient Descent(16/49): loss=1.012289542376207e+35\n",
      "Gradient Descent(17/49): loss=1.9615442283796845e+37\n",
      "Gradient Descent(18/49): loss=3.800943907694566e+39\n",
      "Gradient Descent(19/49): loss=7.365204613669818e+41\n",
      "Gradient Descent(20/49): loss=1.427178099251854e+44\n",
      "Gradient Descent(21/49): loss=2.765486410768037e+46\n",
      "Gradient Descent(22/49): loss=5.358767131009438e+48\n",
      "Gradient Descent(23/49): loss=1.0383846057914293e+51\n",
      "Gradient Descent(24/49): loss=2.0121094333596287e+53\n",
      "Gradient Descent(25/49): loss=3.898925647817728e+55\n",
      "Gradient Descent(26/49): loss=7.555066814549012e+57\n",
      "Gradient Descent(27/49): loss=1.4639682755749626e+60\n",
      "Gradient Descent(28/49): loss=2.836775854533517e+62\n",
      "Gradient Descent(29/49): loss=5.496906854558719e+64\n",
      "Gradient Descent(30/49): loss=1.0651523601840394e+67\n",
      "Gradient Descent(31/49): loss=2.0639781251973004e+69\n",
      "Gradient Descent(32/49): loss=3.999433189592628e+71\n",
      "Gradient Descent(33/49): loss=7.749823335209068e+73\n",
      "Gradient Descent(34/49): loss=1.5017068389400555e+76\n",
      "Gradient Descent(35/49): loss=2.9099030165937147e+78\n",
      "Gradient Descent(36/49): loss=5.638607580663236e+80\n",
      "Gradient Descent(37/49): loss=1.092610140867526e+83\n",
      "Gradient Descent(38/49): loss=2.1171839019627763e+85\n",
      "Gradient Descent(39/49): loss=4.10253164149774e+87\n",
      "Gradient Descent(40/49): loss=7.949600341230141e+89\n",
      "Gradient Descent(41/49): loss=1.5404182370233927e+92\n",
      "Gradient Descent(42/49): loss=2.9849152700764066e+94\n",
      "Gradient Descent(43/49): loss=5.783961105752632e+96\n",
      "Gradient Descent(44/49): loss=1.120775735520383e+99\n",
      "Gradient Descent(45/49): loss=2.1717612313850608e+101\n",
      "Gradient Descent(46/49): loss=4.208287792702099e+103\n",
      "Gradient Descent(47/49): loss=8.154527252017897e+105\n",
      "Gradient Descent(48/49): loss=1.5801275478169272e+108\n",
      "Gradient Descent(49/49): loss=3.061861209369414e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.982846112907047\n",
      "Gradient Descent(2/49): loss=989.5515788120089\n",
      "Gradient Descent(3/49): loss=187569.53162184363\n",
      "Gradient Descent(4/49): loss=36095571.1360076\n",
      "Gradient Descent(5/49): loss=6976062265.065307\n",
      "Gradient Descent(6/49): loss=1350069576052.4639\n",
      "Gradient Descent(7/49): loss=261394356041744.66\n",
      "Gradient Descent(8/49): loss=5.061760594226883e+16\n",
      "Gradient Descent(9/49): loss=9.802326854119877e+18\n",
      "Gradient Descent(10/49): loss=1.898297709200923e+21\n",
      "Gradient Descent(11/49): loss=3.676224726164794e+23\n",
      "Gradient Descent(12/49): loss=7.119355234675683e+25\n",
      "Gradient Descent(13/49): loss=1.3787311169313854e+28\n",
      "Gradient Descent(14/49): loss=2.6700450850468385e+30\n",
      "Gradient Descent(15/49): loss=5.170798895617107e+32\n",
      "Gradient Descent(16/49): loss=1.001374924851304e+35\n",
      "Gradient Descent(17/49): loss=1.939258845533075e+37\n",
      "Gradient Descent(18/49): loss=3.7555612677258944e+39\n",
      "Gradient Descent(19/49): loss=7.273005604109442e+41\n",
      "Gradient Descent(20/49): loss=1.4084874874010908e+44\n",
      "Gradient Descent(21/49): loss=2.727671488804623e+46\n",
      "Gradient Descent(22/49): loss=5.282398187930358e+48\n",
      "Gradient Descent(23/49): loss=1.0229872156820878e+51\n",
      "Gradient Descent(24/49): loss=1.9811131350256804e+53\n",
      "Gradient Descent(25/49): loss=3.8366161312792095e+55\n",
      "Gradient Descent(26/49): loss=7.429976147530844e+57\n",
      "Gradient Descent(27/49): loss=1.438886343171436e+60\n",
      "Gradient Descent(28/49): loss=2.7865417969793427e+62\n",
      "Gradient Descent(29/49): loss=5.396406202034473e+64\n",
      "Gradient Descent(30/49): loss=1.0450659641611742e+67\n",
      "Gradient Descent(31/49): loss=2.023870755015398e+69\n",
      "Gradient Descent(32/49): loss=3.9194203748605624e+71\n",
      "Gradient Descent(33/49): loss=7.590334529417708e+73\n",
      "Gradient Descent(34/49): loss=1.4699412861658273e+76\n",
      "Gradient Descent(35/49): loss=2.8466826809813876e+78\n",
      "Gradient Descent(36/49): loss=5.512874808310691e+80\n",
      "Gradient Descent(37/49): loss=1.0676212299724662e+83\n",
      "Gradient Descent(38/49): loss=2.0675511966454274e+85\n",
      "Gradient Descent(39/49): loss=4.004011751302663e+87\n",
      "Gradient Descent(40/49): loss=7.754153865975187e+89\n",
      "Gradient Descent(41/49): loss=1.5016664763198095e+92\n",
      "Gradient Descent(42/49): loss=2.9081215630729008e+94\n",
      "Gradient Descent(43/49): loss=5.631857112729785e+96\n",
      "Gradient Descent(44/49): loss=1.0906632976060991e+99\n",
      "Gradient Descent(45/49): loss=2.1121743768254812e+101\n",
      "Gradient Descent(46/49): loss=4.090428831620334e+103\n",
      "Gradient Descent(47/49): loss=7.921508853685631e+105\n",
      "Gradient Descent(48/49): loss=1.534076379325797e+108\n",
      "Gradient Descent(49/49): loss=2.9708864574586507e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.060385435698211\n",
      "Gradient Descent(2/49): loss=1022.8514570855596\n",
      "Gradient Descent(3/49): loss=197755.60914187296\n",
      "Gradient Descent(4/49): loss=38791011.13719592\n",
      "Gradient Descent(5/49): loss=7638887788.726415\n",
      "Gradient Descent(6/49): loss=1506053023412.194\n",
      "Gradient Descent(7/49): loss=297037359569580.3\n",
      "Gradient Descent(8/49): loss=5.859134247337251e+16\n",
      "Gradient Descent(9/49): loss=1.155772940797172e+19\n",
      "Gradient Descent(10/49): loss=2.2799064539342244e+21\n",
      "Gradient Descent(11/49): loss=4.497418515846873e+23\n",
      "Gradient Descent(12/49): loss=8.871767592287343e+25\n",
      "Gradient Descent(13/49): loss=1.7500771623500816e+28\n",
      "Gradient Descent(14/49): loss=3.4522663895924e+30\n",
      "Gradient Descent(15/49): loss=6.810067596638499e+32\n",
      "Gradient Descent(16/49): loss=1.343378990372094e+35\n",
      "Gradient Descent(17/49): loss=2.649998836334302e+37\n",
      "Gradient Descent(18/49): loss=5.227485239957236e+39\n",
      "Gradient Descent(19/49): loss=1.0311929791666094e+42\n",
      "Gradient Descent(20/49): loss=2.034169226028951e+44\n",
      "Gradient Descent(21/49): loss=4.012677087559293e+46\n",
      "Gradient Descent(22/49): loss=7.915554518907608e+48\n",
      "Gradient Descent(23/49): loss=1.56145141946324e+51\n",
      "Gradient Descent(24/49): loss=3.0801765429378345e+53\n",
      "Gradient Descent(25/49): loss=6.076069621770204e+55\n",
      "Gradient Descent(26/49): loss=1.1985878579996374e+58\n",
      "Gradient Descent(27/49): loss=2.364378525547112e+60\n",
      "Gradient Descent(28/49): loss=4.664060106030319e+62\n",
      "Gradient Descent(29/49): loss=9.200496636903801e+64\n",
      "Gradient Descent(30/49): loss=1.8149238311966093e+67\n",
      "Gradient Descent(31/49): loss=3.580185551976786e+69\n",
      "Gradient Descent(32/49): loss=7.062405797014824e+71\n",
      "Gradient Descent(33/49): loss=1.393156162371783e+74\n",
      "Gradient Descent(34/49): loss=2.748191124297696e+76\n",
      "Gradient Descent(35/49): loss=5.421182965455023e+78\n",
      "Gradient Descent(36/49): loss=1.0694025057100157e+81\n",
      "Gradient Descent(37/49): loss=2.109542744648665e+83\n",
      "Gradient Descent(38/49): loss=4.161361664797283e+85\n",
      "Gradient Descent(39/49): loss=8.20885518872408e+87\n",
      "Gradient Descent(40/49): loss=1.6193089891581164e+90\n",
      "Gradient Descent(41/49): loss=3.194308514505369e+92\n",
      "Gradient Descent(42/49): loss=6.30121054978295e+94\n",
      "Gradient Descent(43/49): loss=1.2429999861439185e+97\n",
      "Gradient Descent(44/49): loss=2.451987524217864e+99\n",
      "Gradient Descent(45/49): loss=4.836880841464437e+101\n",
      "Gradient Descent(46/49): loss=9.541409180696528e+103\n",
      "Gradient Descent(47/49): loss=1.882173494394309e+106\n",
      "Gradient Descent(48/49): loss=3.712844712883245e+108\n",
      "Gradient Descent(49/49): loss=7.324094140652683e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.034404252564266\n",
      "Gradient Descent(2/49): loss=1015.8521250033865\n",
      "Gradient Descent(3/49): loss=196375.920924712\n",
      "Gradient Descent(4/49): loss=38564859.17127754\n",
      "Gradient Descent(5/49): loss=7606978388.416947\n",
      "Gradient Descent(6/49): loss=1502537019944.151\n",
      "Gradient Descent(7/49): loss=296912504832554.0\n",
      "Gradient Descent(8/49): loss=5.8680532466037864e+16\n",
      "Gradient Descent(9/49): loss=1.1597920750043556e+19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=2.292308298420206e+21\n",
      "Gradient Descent(11/49): loss=4.5307298881849944e+23\n",
      "Gradient Descent(12/49): loss=8.954968539429567e+25\n",
      "Gradient Descent(13/49): loss=1.7699467724541935e+28\n",
      "Gradient Descent(14/49): loss=3.4982950065248674e+30\n",
      "Gradient Descent(15/49): loss=6.914371050735807e+32\n",
      "Gradient Descent(16/49): loss=1.366623682101474e+35\n",
      "Gradient Descent(17/49): loss=2.7011282555874013e+37\n",
      "Gradient Descent(18/49): loss=5.33877318646527e+39\n",
      "Gradient Descent(19/49): loss=1.055207174980829e+42\n",
      "Gradient Descent(20/49): loss=2.085614322851551e+44\n",
      "Gradient Descent(21/49): loss=4.1222114549502394e+46\n",
      "Gradient Descent(22/49): loss=8.147540556092553e+48\n",
      "Gradient Descent(23/49): loss=1.6103593384098672e+51\n",
      "Gradient Descent(24/49): loss=3.182871175612286e+53\n",
      "Gradient Descent(25/49): loss=6.290936860438494e+55\n",
      "Gradient Descent(26/49): loss=1.2434020856789482e+58\n",
      "Gradient Descent(27/49): loss=2.457581090017769e+60\n",
      "Gradient Descent(28/49): loss=4.857402833384523e+62\n",
      "Gradient Descent(29/49): loss=9.600644463618321e+64\n",
      "Gradient Descent(30/49): loss=1.8975649596799548e+67\n",
      "Gradient Descent(31/49): loss=3.750532362541133e+69\n",
      "Gradient Descent(32/49): loss=7.412917766378243e+71\n",
      "Gradient Descent(33/49): loss=1.4651613290934093e+74\n",
      "Gradient Descent(34/49): loss=2.8958876759799686e+76\n",
      "Gradient Descent(35/49): loss=5.723714696375241e+78\n",
      "Gradient Descent(36/49): loss=1.1312907678442893e+81\n",
      "Gradient Descent(37/49): loss=2.2359933527438359e+83\n",
      "Gradient Descent(38/49): loss=4.4194352288772314e+85\n",
      "Gradient Descent(39/49): loss=8.735002596619395e+87\n",
      "Gradient Descent(40/49): loss=1.7264710627364995e+90\n",
      "Gradient Descent(41/49): loss=3.4123657062449756e+92\n",
      "Gradient Descent(42/49): loss=6.744532222104169e+94\n",
      "Gradient Descent(43/49): loss=1.3330550946445286e+97\n",
      "Gradient Descent(44/49): loss=2.634780036388266e+99\n",
      "Gradient Descent(45/49): loss=5.207636104493719e+101\n",
      "Gradient Descent(46/49): loss=1.0292879641672715e+104\n",
      "Gradient Descent(47/49): loss=2.034385068237405e+106\n",
      "Gradient Descent(48/49): loss=4.020956962433443e+108\n",
      "Gradient Descent(49/49): loss=7.947411306823004e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.059160403017766\n",
      "Gradient Descent(2/49): loss=1012.3580194631973\n",
      "Gradient Descent(3/49): loss=194015.842967173\n",
      "Gradient Descent(4/49): loss=37753744.59373534\n",
      "Gradient Descent(5/49): loss=7377499694.908574\n",
      "Gradient Descent(6/49): loss=1443509234259.363\n",
      "Gradient Descent(7/49): loss=282559677907335.56\n",
      "Gradient Descent(8/49): loss=5.53171748156188e+16\n",
      "Gradient Descent(9/49): loss=1.083002431747532e+19\n",
      "Gradient Descent(10/49): loss=2.120339567320134e+21\n",
      "Gradient Descent(11/49): loss=4.1512949431629586e+23\n",
      "Gradient Descent(12/49): loss=8.127603232968923e+25\n",
      "Gradient Descent(13/49): loss=1.5912618213836177e+28\n",
      "Gradient Descent(14/49): loss=3.115450630267879e+30\n",
      "Gradient Descent(15/49): loss=6.0995828036781924e+32\n",
      "Gradient Descent(16/49): loss=1.1942064111952547e+35\n",
      "Gradient Descent(17/49): loss=2.3380762398282427e+37\n",
      "Gradient Descent(18/49): loss=4.577601045284886e+39\n",
      "Gradient Descent(19/49): loss=8.962253235981598e+41\n",
      "Gradient Descent(20/49): loss=1.7546741686863808e+44\n",
      "Gradient Descent(21/49): loss=3.4353876837210884e+46\n",
      "Gradient Descent(22/49): loss=6.725971549833633e+48\n",
      "Gradient Descent(23/49): loss=1.3168439039351612e+51\n",
      "Gradient Descent(24/49): loss=2.578181983805512e+53\n",
      "Gradient Descent(25/49): loss=5.047691925947473e+55\n",
      "Gradient Descent(26/49): loss=9.882620365563347e+57\n",
      "Gradient Descent(27/49): loss=1.9348681877317142e+60\n",
      "Gradient Descent(28/49): loss=3.788180427269658e+62\n",
      "Gradient Descent(29/49): loss=7.416686594228574e+64\n",
      "Gradient Descent(30/49): loss=1.4520755041400313e+67\n",
      "Gradient Descent(31/49): loss=2.8429450846208192e+69\n",
      "Gradient Descent(32/49): loss=5.566058191275908e+71\n",
      "Gradient Descent(33/49): loss=1.0897503422160515e+74\n",
      "Gradient Descent(34/49): loss=2.1335670011882663e+76\n",
      "Gradient Descent(35/49): loss=4.177202770409422e+78\n",
      "Gradient Descent(36/49): loss=8.178333736600806e+80\n",
      "Gradient Descent(37/49): loss=1.601194540543382e+83\n",
      "Gradient Descent(38/49): loss=3.1348976933919384e+85\n",
      "Gradient Descent(39/49): loss=6.137657417129984e+87\n",
      "Gradient Descent(40/49): loss=1.201660859601808e+90\n",
      "Gradient Descent(41/49): loss=2.3526709351174015e+92\n",
      "Gradient Descent(42/49): loss=4.6061752654408246e+94\n",
      "Gradient Descent(43/49): loss=9.018197257960399e+96\n",
      "Gradient Descent(44/49): loss=1.765627165637193e+99\n",
      "Gradient Descent(45/49): loss=3.456832001855214e+101\n",
      "Gradient Descent(46/49): loss=6.767956294293766e+103\n",
      "Gradient Descent(47/49): loss=1.3250638844146226e+106\n",
      "Gradient Descent(48/49): loss=2.594275467263762e+108\n",
      "Gradient Descent(49/49): loss=5.07920054210812e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.041660284344935\n",
      "Gradient Descent(2/49): loss=1010.1373097754494\n",
      "Gradient Descent(3/49): loss=193447.15850334783\n",
      "Gradient Descent(4/49): loss=37610975.958207235\n",
      "Gradient Descent(5/49): loss=7344021035.951401\n",
      "Gradient Descent(6/49): loss=1435963969491.2869\n",
      "Gradient Descent(7/49): loss=280897465783271.1\n",
      "Gradient Descent(8/49): loss=5.495631440337665e+16\n",
      "Gradient Descent(9/49): loss=1.0752504374543907e+19\n",
      "Gradient Descent(10/49): loss=2.1038232330010574e+21\n",
      "Gradient Descent(11/49): loss=4.1163420924095326e+23\n",
      "Gradient Descent(12/49): loss=8.054054437846441e+25\n",
      "Gradient Descent(13/49): loss=1.5758612085960696e+28\n",
      "Gradient Descent(14/49): loss=3.083340399857802e+30\n",
      "Gradient Descent(15/49): loss=6.032884589275724e+32\n",
      "Gradient Descent(16/49): loss=1.1803982943210082e+35\n",
      "Gradient Descent(17/49): loss=2.309575338281335e+37\n",
      "Gradient Descent(18/49): loss=4.5189308442620524e+39\n",
      "Gradient Descent(19/49): loss=8.841770891692595e+41\n",
      "Gradient Descent(20/49): loss=1.7299869200375478e+44\n",
      "Gradient Descent(21/49): loss=3.3849042013826864e+46\n",
      "Gradient Descent(22/49): loss=6.622926636235036e+48\n",
      "Gradient Descent(23/49): loss=1.2958463406944092e+51\n",
      "Gradient Descent(24/49): loss=2.535461784391062e+53\n",
      "Gradient Descent(25/49): loss=4.96090181238174e+55\n",
      "Gradient Descent(26/49): loss=9.706534306143464e+57\n",
      "Gradient Descent(27/49): loss=1.8991871195920083e+60\n",
      "Gradient Descent(28/49): loss=3.7159624655543113e+62\n",
      "Gradient Descent(29/49): loss=7.270677492997779e+64\n",
      "Gradient Descent(30/49): loss=1.4225857149313076e+67\n",
      "Gradient Descent(31/49): loss=2.783440908052439e+69\n",
      "Gradient Descent(32/49): loss=5.446099456294548e+71\n",
      "Gradient Descent(33/49): loss=1.0655875323972625e+74\n",
      "Gradient Descent(34/49): loss=2.0849358303365404e+76\n",
      "Gradient Descent(35/49): loss=4.0793996593050675e+78\n",
      "Gradient Descent(36/49): loss=7.981781183957163e+80\n",
      "Gradient Descent(37/49): loss=1.5617207478863578e+83\n",
      "Gradient Descent(38/49): loss=3.0556734620599358e+85\n",
      "Gradient Descent(39/49): loss=5.978751527361298e+87\n",
      "Gradient Descent(40/49): loss=1.1698065997479833e+90\n",
      "Gradient Descent(41/49): loss=2.2888515680093825e+92\n",
      "Gradient Descent(42/49): loss=4.478382581793984e+94\n",
      "Gradient Descent(43/49): loss=8.762433890092074e+96\n",
      "Gradient Descent(44/49): loss=1.7144637885644134e+99\n",
      "Gradient Descent(45/49): loss=3.354531536748336e+101\n",
      "Gradient Descent(46/49): loss=6.563499273706844e+103\n",
      "Gradient Descent(47/49): loss=1.2842187424390385e+106\n",
      "Gradient Descent(48/49): loss=2.5127111463825623e+108\n",
      "Gradient Descent(49/49): loss=4.916387758960655e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.120019531004518\n",
      "Gradient Descent(2/49): loss=1044.115624573505\n",
      "Gradient Descent(3/49): loss=203948.29176187396\n",
      "Gradient Descent(4/49): loss=40418525.53172736\n",
      "Gradient Descent(5/49): loss=8041550695.143238\n",
      "Gradient Descent(6/49): loss=1601810448707.7554\n",
      "Gradient Descent(7/49): loss=319185731393380.2\n",
      "Gradient Descent(8/49): loss=6.361030976003368e+16\n",
      "Gradient Descent(9/49): loss=1.2677344858018382e+19\n",
      "Gradient Descent(10/49): loss=2.5265887036467535e+21\n",
      "Gradient Descent(11/49): loss=5.0354996838053457e+23\n",
      "Gradient Descent(12/49): loss=1.0035781006922014e+26\n",
      "Gradient Descent(13/49): loss=2.000138032782822e+28\n",
      "Gradient Descent(14/49): loss=3.9862893746293235e+30\n",
      "Gradient Descent(15/49): loss=7.94470354344519e+32\n",
      "Gradient Descent(16/49): loss=1.583385184030692e+35\n",
      "Gradient Descent(17/49): loss=3.155698182709676e+37\n",
      "Gradient Descent(18/49): loss=6.28932942968782e+39\n",
      "Gradient Descent(19/49): loss=1.2534679302584598e+42\n",
      "Gradient Descent(20/49): loss=2.4981707032877937e+44\n",
      "Gradient Descent(21/49): loss=4.978872384736379e+46\n",
      "Gradient Descent(22/49): loss=9.922928882050131e+48\n",
      "Gradient Descent(23/49): loss=1.977646944731416e+51\n",
      "Gradient Descent(24/49): loss=3.94146474745782e+53\n",
      "Gradient Descent(25/49): loss=7.855367914303078e+55\n",
      "Gradient Descent(26/49): loss=1.5655805397948803e+58\n",
      "Gradient Descent(27/49): loss=3.1202134048001964e+60\n",
      "Gradient Descent(28/49): loss=6.218608014104894e+62\n",
      "Gradient Descent(29/49): loss=1.2393731010064082e+65\n",
      "Gradient Descent(30/49): loss=2.470079606262722e+67\n",
      "Gradient Descent(31/49): loss=4.922886624149427e+69\n",
      "Gradient Descent(32/49): loss=9.811348854014085e+71\n",
      "Gradient Descent(33/49): loss=1.9554089639794524e+74\n",
      "Gradient Descent(34/49): loss=3.897144290050244e+76\n",
      "Gradient Descent(35/49): loss=7.767036920278141e+78\n",
      "Gradient Descent(36/49): loss=1.5479761084285123e+81\n",
      "Gradient Descent(37/49): loss=3.085127645021778e+83\n",
      "Gradient Descent(38/49): loss=6.148681839631347e+85\n",
      "Gradient Descent(39/49): loss=1.2254367635652607e+88\n",
      "Gradient Descent(40/49): loss=2.4423043843610865e+90\n",
      "Gradient Descent(41/49): loss=4.8675304048455135e+92\n",
      "Gradient Descent(42/49): loss=9.701023506246383e+94\n",
      "Gradient Descent(43/49): loss=1.9334210419119498e+97\n",
      "Gradient Descent(44/49): loss=3.8533222014161086e+99\n",
      "Gradient Descent(45/49): loss=7.679699178841658e+101\n",
      "Gradient Descent(46/49): loss=1.5305696330254065e+104\n",
      "Gradient Descent(47/49): loss=3.050436412918031e+106\n",
      "Gradient Descent(48/49): loss=6.079541961683332e+108\n",
      "Gradient Descent(49/49): loss=1.2116571355936543e+111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.093732099859203\n",
      "Gradient Descent(2/49): loss=1036.972001897267\n",
      "Gradient Descent(3/49): loss=202525.32822814333\n",
      "Gradient Descent(4/49): loss=40182811.21134438\n",
      "Gradient Descent(5/49): loss=8007935634.387479\n",
      "Gradient Descent(6/49): loss=1598064770348.3723\n",
      "Gradient Descent(7/49): loss=319050102352539.1\n",
      "Gradient Descent(8/49): loss=6.370680240309598e+16\n",
      "Gradient Descent(9/49): loss=1.2721353542987719e+19\n",
      "Gradient Descent(10/49): loss=2.540315512685494e+21\n",
      "Gradient Descent(11/49): loss=5.072759375089188e+23\n",
      "Gradient Descent(12/49): loss=1.0129817377101706e+26\n",
      "Gradient Descent(13/49): loss=2.0228292239369207e+28\n",
      "Gradient Descent(14/49): loss=4.0394004142330673e+30\n",
      "Gradient Descent(15/49): loss=8.066304633669185e+32\n",
      "Gradient Descent(16/49): loss=1.6107655875372808e+35\n",
      "Gradient Descent(17/49): loss=3.21654823863758e+37\n",
      "Gradient Descent(18/49): loss=6.423146034171274e+39\n",
      "Gradient Descent(19/49): loss=1.2826421973910503e+42\n",
      "Gradient Descent(20/49): loss=2.561316523396239e+44\n",
      "Gradient Descent(21/49): loss=5.114709579122545e+46\n",
      "Gradient Descent(22/49): loss=1.0213596734550539e+49\n",
      "Gradient Descent(23/49): loss=2.0395597568787348e+51\n",
      "Gradient Descent(24/49): loss=4.0728101079421855e+53\n",
      "Gradient Descent(25/49): loss=8.133020922494893e+55\n",
      "Gradient Descent(26/49): loss=1.6240882234301686e+58\n",
      "Gradient Descent(27/49): loss=3.243152307882824e+60\n",
      "Gradient Descent(28/49): loss=6.4762718800530595e+62\n",
      "Gradient Descent(29/49): loss=1.2932509325085273e+65\n",
      "Gradient Descent(30/49): loss=2.582501175692582e+67\n",
      "Gradient Descent(31/49): loss=5.157013348922986e+69\n",
      "Gradient Descent(32/49): loss=1.0298073407009234e+72\n",
      "Gradient Descent(33/49): loss=2.056428958406677e+74\n",
      "Gradient Descent(34/49): loss=4.106496325900371e+76\n",
      "Gradient Descent(35/49): loss=8.200289149642688e+78\n",
      "Gradient Descent(36/49): loss=1.6375210593425688e+81\n",
      "Gradient Descent(37/49): loss=3.269976425047471e+83\n",
      "Gradient Descent(38/49): loss=6.529837133611671e+85\n",
      "Gradient Descent(39/49): loss=1.3039474127363135e+88\n",
      "Gradient Descent(40/49): loss=2.6038610464412803e+90\n",
      "Gradient Descent(41/49): loss=5.1996670133701055e+92\n",
      "Gradient Descent(42/49): loss=1.0383248786213203e+95\n",
      "Gradient Descent(43/49): loss=2.0734376851282414e+97\n",
      "Gradient Descent(44/49): loss=4.140461162616385e+99\n",
      "Gradient Descent(45/49): loss=8.268113752391076e+101\n",
      "Gradient Descent(46/49): loss=1.6510649982592728e+104\n",
      "Gradient Descent(47/49): loss=3.2970224045218868e+106\n",
      "Gradient Descent(48/49): loss=6.583845425455674e+108\n",
      "Gradient Descent(49/49): loss=1.31473236356668e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.118797274525942\n",
      "Gradient Descent(2/49): loss=1033.411087113231\n",
      "Gradient Descent(3/49): loss=200093.16667442094\n",
      "Gradient Descent(4/49): loss=39338164.38527365\n",
      "Gradient Descent(5/49): loss=7766485933.622128\n",
      "Gradient Descent(6/49): loss=1535313850657.67\n",
      "Gradient Descent(7/49): loss=303634004835694.7\n",
      "Gradient Descent(8/49): loss=6.00569099123144e+16\n",
      "Gradient Descent(9/49): loss=1.1879420424865348e+19\n",
      "Gradient Descent(10/49): loss=2.3498172695556307e+21\n",
      "Gradient Descent(11/49): loss=4.648096385766234e+23\n",
      "Gradient Descent(12/49): loss=9.194262393394225e+25\n",
      "Gradient Descent(13/49): loss=1.8186909607587842e+28\n",
      "Gradient Descent(14/49): loss=3.5975016767013214e+30\n",
      "Gradient Descent(15/49): loss=7.116117809097517e+32\n",
      "Gradient Descent(16/49): loss=1.4076194620502648e+35\n",
      "Gradient Descent(17/49): loss=2.7843729083639905e+37\n",
      "Gradient Descent(18/49): loss=5.507690622581526e+39\n",
      "Gradient Descent(19/49): loss=1.0894609672125063e+42\n",
      "Gradient Descent(20/49): loss=2.1550324459586837e+44\n",
      "Gradient Descent(21/49): loss=4.2628097594285364e+46\n",
      "Gradient Descent(22/49): loss=8.432145455489608e+48\n",
      "Gradient Descent(23/49): loss=1.66793924654941e+51\n",
      "Gradient Descent(24/49): loss=3.2993042457278258e+53\n",
      "Gradient Descent(25/49): loss=6.526261989703375e+55\n",
      "Gradient Descent(26/49): loss=1.290941737592535e+58\n",
      "Gradient Descent(27/49): loss=2.553575955865637e+60\n",
      "Gradient Descent(28/49): loss=5.051157594870161e+62\n",
      "Gradient Descent(29/49): loss=9.991554388506787e+64\n",
      "Gradient Descent(30/49): loss=1.976401591585196e+67\n",
      "Gradient Descent(31/49): loss=3.909465033502434e+69\n",
      "Gradient Descent(32/49): loss=7.733204078185109e+71\n",
      "Gradient Descent(33/49): loss=1.5296835961538887e+74\n",
      "Gradient Descent(34/49): loss=3.0258245879519515e+76\n",
      "Gradient Descent(35/49): loss=5.985299482896162e+78\n",
      "Gradient Descent(36/49): loss=1.1839354482939406e+81\n",
      "Gradient Descent(37/49): loss=2.3419097903664306e+83\n",
      "Gradient Descent(38/49): loss=4.632466638377467e+85\n",
      "Gradient Descent(39/49): loss=9.163353449375371e+87\n",
      "Gradient Descent(40/49): loss=1.8125774666687967e+90\n",
      "Gradient Descent(41/49): loss=3.585409087215154e+92\n",
      "Gradient Descent(42/49): loss=7.092198021368155e+94\n",
      "Gradient Descent(43/49): loss=1.4028879704035897e+97\n",
      "Gradient Descent(44/49): loss=2.7750136862696376e+99\n",
      "Gradient Descent(45/49): loss=5.48917741219811e+101\n",
      "Gradient Descent(46/49): loss=1.085798920980821e+104\n",
      "Gradient Descent(47/49): loss=2.147788654422473e+106\n",
      "Gradient Descent(48/49): loss=4.2484810169997883e+108\n",
      "Gradient Descent(49/49): loss=8.40380216863609e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.100785023575712\n",
      "Gradient Descent(2/49): loss=1031.0427493290695\n",
      "Gradient Descent(3/49): loss=199477.47826735018\n",
      "Gradient Descent(4/49): loss=39181760.50101717\n",
      "Gradient Descent(5/49): loss=7729356128.621138\n",
      "Gradient Descent(6/49): loss=1526841489186.6782\n",
      "Gradient Descent(7/49): loss=301744682459410.4\n",
      "Gradient Descent(8/49): loss=5.9641829753407096e+16\n",
      "Gradient Descent(9/49): loss=1.178920736566597e+19\n",
      "Gradient Descent(10/49): loss=2.330375312868724e+21\n",
      "Gradient Descent(11/49): loss=4.606485827514472e+23\n",
      "Gradient Descent(12/49): loss=9.105724348763893e+25\n",
      "Gradient Descent(13/49): loss=1.7999463642903085e+28\n",
      "Gradient Descent(14/49): loss=3.5579900837279394e+30\n",
      "Gradient Descent(15/49): loss=7.033150948311036e+32\n",
      "Gradient Descent(16/49): loss=1.3902572096427857e+35\n",
      "Gradient Descent(17/49): loss=2.748149644227352e+37\n",
      "Gradient Descent(18/49): loss=5.4323231986681984e+39\n",
      "Gradient Descent(19/49): loss=1.0738183573888925e+42\n",
      "Gradient Descent(20/49): loss=2.1226385524992124e+44\n",
      "Gradient Descent(21/49): loss=4.1958627305558376e+46\n",
      "Gradient Descent(22/49): loss=8.294047064084618e+48\n",
      "Gradient Descent(23/49): loss=1.639501125748455e+51\n",
      "Gradient Descent(24/49): loss=3.240835168375633e+53\n",
      "Gradient Descent(25/49): loss=6.406224688503798e+55\n",
      "Gradient Descent(26/49): loss=1.2663314432060748e+58\n",
      "Gradient Descent(27/49): loss=2.5031830789988288e+60\n",
      "Gradient Descent(28/49): loss=4.948092824041836e+62\n",
      "Gradient Descent(29/49): loss=9.780995565505057e+64\n",
      "Gradient Descent(30/49): loss=1.933429255562856e+67\n",
      "Gradient Descent(31/49): loss=3.82184887134576e+69\n",
      "Gradient Descent(32/49): loss=7.554726273734066e+71\n",
      "Gradient Descent(33/49): loss=1.4933580838048936e+74\n",
      "Gradient Descent(34/49): loss=2.951951249668697e+76\n",
      "Gradient Descent(35/49): loss=5.8351819800769905e+78\n",
      "Gradient Descent(36/49): loss=1.1534522714234015e+81\n",
      "Gradient Descent(37/49): loss=2.2800525279149136e+83\n",
      "Gradient Descent(38/49): loss=4.507026132633864e+85\n",
      "Gradient Descent(39/49): loss=8.909130079920143e+87\n",
      "Gradient Descent(40/49): loss=1.76108583454237e+90\n",
      "Gradient Descent(41/49): loss=3.4811741312610615e+92\n",
      "Gradient Descent(42/49): loss=6.881307596974889e+94\n",
      "Gradient Descent(43/49): loss=1.3602420464681219e+97\n",
      "Gradient Descent(44/49): loss=2.6888180754965387e+99\n",
      "Gradient Descent(45/49): loss=5.3150412912826576e+101\n",
      "Gradient Descent(46/49): loss=1.0506350052270836e+104\n",
      "Gradient Descent(47/49): loss=2.076811549928961e+106\n",
      "Gradient Descent(48/49): loss=4.1052755642632273e+108\n",
      "Gradient Descent(49/49): loss=8.114981573129817e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.179968472942589\n",
      "Gradient Descent(2/49): loss=1065.7098118924882\n",
      "Gradient Descent(3/49): loss=210301.74503689844\n",
      "Gradient Descent(4/49): loss=42105478.3186654\n",
      "Gradient Descent(5/49): loss=8463216694.346763\n",
      "Gradient Descent(6/49): loss=1703119682061.0198\n",
      "Gradient Descent(7/49): loss=342859547174141.0\n",
      "Gradient Descent(8/49): loss=6.903020386511877e+16\n",
      "Gradient Descent(9/49): loss=1.3898850025125145e+19\n",
      "Gradient Descent(10/49): loss=2.798491889070531e+21\n",
      "Gradient Descent(11/49): loss=5.634702885653648e+23\n",
      "Gradient Descent(12/49): loss=1.1345367597491364e+26\n",
      "Gradient Descent(13/49): loss=2.2843692901170155e+28\n",
      "Gradient Descent(14/49): loss=4.5995369964268064e+30\n",
      "Gradient Descent(15/49): loss=9.261086489715933e+32\n",
      "Gradient Descent(16/49): loss=1.864703433105693e+35\n",
      "Gradient Descent(17/49): loss=3.7545474978953494e+37\n",
      "Gradient Descent(18/49): loss=7.55971522690684e+39\n",
      "Gradient Descent(19/49): loss=1.5221353405269653e+42\n",
      "Gradient Descent(20/49): loss=3.0647926882714628e+44\n",
      "Gradient Descent(21/49): loss=6.170906076829954e+46\n",
      "Gradient Descent(22/49): loss=1.242501065585622e+49\n",
      "Gradient Descent(23/49): loss=2.501754003009909e+51\n",
      "Gradient Descent(24/49): loss=5.037237604813334e+53\n",
      "Gradient Descent(25/49): loss=1.0142389162497334e+56\n",
      "Gradient Descent(26/49): loss=2.0421521872478116e+58\n",
      "Gradient Descent(27/49): loss=4.111837446843194e+60\n",
      "Gradient Descent(28/49): loss=8.279112249732935e+62\n",
      "Gradient Descent(29/49): loss=1.66698466390743e+65\n",
      "Gradient Descent(30/49): loss=3.3564442489497797e+67\n",
      "Gradient Descent(31/49): loss=6.7581413556025615e+69\n",
      "Gradient Descent(32/49): loss=1.3607398542846713e+72\n",
      "Gradient Descent(33/49): loss=2.739825720726708e+74\n",
      "Gradient Descent(34/49): loss=5.5165908136804e+76\n",
      "Gradient Descent(35/49): loss=1.1107558402477927e+79\n",
      "Gradient Descent(36/49): loss=2.236487313115499e+81\n",
      "Gradient Descent(37/49): loss=4.5031277986445126e+83\n",
      "Gradient Descent(38/49): loss=9.066968478652738e+85\n",
      "Gradient Descent(39/49): loss=1.825618127418649e+88\n",
      "Gradient Descent(40/49): loss=3.6758499326500275e+90\n",
      "Gradient Descent(41/49): loss=7.401259071889545e+92\n",
      "Gradient Descent(42/49): loss=1.4902304733027005e+95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=3.0005528005291337e+97\n",
      "Gradient Descent(44/49): loss=6.04156019492053e+99\n",
      "Gradient Descent(45/49): loss=1.2164575001783556e+102\n",
      "Gradient Descent(46/49): loss=2.4493157429504538e+104\n",
      "Gradient Descent(47/49): loss=4.931654092136676e+106\n",
      "Gradient Descent(48/49): loss=9.929798620079532e+108\n",
      "Gradient Descent(49/49): loss=1.999347456111093e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.153373058489471\n",
      "Gradient Descent(2/49): loss=1058.4196798252772\n",
      "Gradient Descent(3/49): loss=208834.3812202552\n",
      "Gradient Descent(4/49): loss=41859849.78340287\n",
      "Gradient Descent(5/49): loss=8427814373.218911\n",
      "Gradient Descent(6/49): loss=1699130660399.942\n",
      "Gradient Descent(7/49): loss=342712301490643.1\n",
      "Gradient Descent(8/49): loss=6.9134555707555896e+16\n",
      "Gradient Descent(9/49): loss=1.394701656335697e+19\n",
      "Gradient Descent(10/49): loss=2.813677409710633e+21\n",
      "Gradient Descent(11/49): loss=5.676355200889175e+23\n",
      "Gradient Descent(12/49): loss=1.1451584422058383e+26\n",
      "Gradient Descent(13/49): loss=2.3102652479969754e+28\n",
      "Gradient Descent(14/49): loss=4.660775613751074e+30\n",
      "Gradient Descent(15/49): loss=9.402743138512343e+32\n",
      "Gradient Descent(16/49): loss=1.8969284885660045e+35\n",
      "Gradient Descent(17/49): loss=3.8269020679547634e+37\n",
      "Gradient Descent(18/49): loss=7.720469991537465e+39\n",
      "Gradient Descent(19/49): loss=1.557543304696075e+42\n",
      "Gradient Descent(20/49): loss=3.142219514215403e+44\n",
      "Gradient Descent(21/49): loss=6.339177502543612e+46\n",
      "Gradient Descent(22/49): loss=1.278878551549575e+49\n",
      "Gradient Descent(23/49): loss=2.5800355787053617e+51\n",
      "Gradient Descent(24/49): loss=5.205016206848591e+53\n",
      "Gradient Descent(25/49): loss=1.0500705469798099e+56\n",
      "Gradient Descent(26/49): loss=2.118433660560033e+58\n",
      "Gradient Descent(27/49): loss=4.273771116713915e+60\n",
      "Gradient Descent(28/49): loss=8.621992700602362e+62\n",
      "Gradient Descent(29/49): loss=1.7394183286633252e+65\n",
      "Gradient Descent(30/49): loss=3.50913788395873e+67\n",
      "Gradient Descent(31/49): loss=7.079406078293513e+69\n",
      "Gradient Descent(32/49): loss=1.4282137687003653e+72\n",
      "Gradient Descent(33/49): loss=2.8813074805238374e+74\n",
      "Gradient Descent(34/49): loss=5.812808263903777e+76\n",
      "Gradient Descent(35/49): loss=1.1726877516996263e+79\n",
      "Gradient Descent(36/49): loss=2.365804101136413e+81\n",
      "Gradient Descent(37/49): loss=4.772821270488951e+83\n",
      "Gradient Descent(38/49): loss=9.628786622311456e+85\n",
      "Gradient Descent(39/49): loss=1.9425309803923653e+88\n",
      "Gradient Descent(40/49): loss=3.9189014751250935e+90\n",
      "Gradient Descent(41/49): loss=7.906071474152553e+92\n",
      "Gradient Descent(42/49): loss=1.5949869255749286e+95\n",
      "Gradient Descent(43/49): loss=3.2177590362951495e+97\n",
      "Gradient Descent(44/49): loss=6.491572469741053e+99\n",
      "Gradient Descent(45/49): loss=1.3096230219407385e+102\n",
      "Gradient Descent(46/49): loss=2.6420600980606647e+104\n",
      "Gradient Descent(47/49): loss=5.330145732639874e+106\n",
      "Gradient Descent(48/49): loss=1.0753144317963389e+109\n",
      "Gradient Descent(49/49): loss=2.16936118678467e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.178749075592029\n",
      "Gradient Descent(2/49): loss=1054.7910094197644\n",
      "Gradient Descent(3/49): loss=206328.3116500337\n",
      "Gradient Descent(4/49): loss=40980464.18854166\n",
      "Gradient Descent(5/49): loss=8173834593.797062\n",
      "Gradient Descent(6/49): loss=1632442475157.9263\n",
      "Gradient Descent(7/49): loss=326160177203563.2\n",
      "Gradient Descent(8/49): loss=6.5175364264055736e+16\n",
      "Gradient Descent(9/49): loss=1.3024340858440202e+19\n",
      "Gradient Descent(10/49): loss=2.6027630424625114e+21\n",
      "Gradient Descent(11/49): loss=5.2013456275296435e+23\n",
      "Gradient Descent(12/49): loss=1.0394354713463054e+26\n",
      "Gradient Descent(13/49): loss=2.0772061522828957e+28\n",
      "Gradient Descent(14/49): loss=4.1510861811343424e+30\n",
      "Gradient Descent(15/49): loss=8.295525970866858e+32\n",
      "Gradient Descent(16/49): loss=1.6577770155080627e+35\n",
      "Gradient Descent(17/49): loss=3.3128998239337347e+37\n",
      "Gradient Descent(18/49): loss=6.620495499144335e+39\n",
      "Gradient Descent(19/49): loss=1.3230391203733588e+42\n",
      "Gradient Descent(20/49): loss=2.6439599796358137e+44\n",
      "Gradient Descent(21/49): loss=5.28368682898336e+46\n",
      "Gradient Descent(22/49): loss=1.0558914175191594e+49\n",
      "Gradient Descent(23/49): loss=2.1100922929159062e+51\n",
      "Gradient Descent(24/49): loss=4.216806208255219e+53\n",
      "Gradient Descent(25/49): loss=8.426861070341157e+55\n",
      "Gradient Descent(26/49): loss=1.6840230257638723e+58\n",
      "Gradient Descent(27/49): loss=3.365349835046536e+60\n",
      "Gradient Descent(28/49): loss=6.725311553926638e+62\n",
      "Gradient Descent(29/49): loss=1.3439855502200497e+65\n",
      "Gradient Descent(30/49): loss=2.6858193032643643e+67\n",
      "Gradient Descent(31/49): loss=5.367338457326698e+69\n",
      "Gradient Descent(32/49): loss=1.0726083501032344e+72\n",
      "Gradient Descent(33/49): loss=2.1434993933365838e+74\n",
      "Gradient Descent(34/49): loss=4.283566922439185e+76\n",
      "Gradient Descent(35/49): loss=8.560275611019964e+78\n",
      "Gradient Descent(36/49): loss=1.710684573474482e+81\n",
      "Gradient Descent(37/49): loss=3.4186302438162857e+83\n",
      "Gradient Descent(38/49): loss=6.831787066506624e+85\n",
      "Gradient Descent(39/49): loss=1.3652636054019404e+88\n",
      "Gradient Descent(40/49): loss=2.728341346253657e+90\n",
      "Gradient Descent(41/49): loss=5.452314463100137e+92\n",
      "Gradient Descent(42/49): loss=1.089589946116922e+95\n",
      "Gradient Descent(43/49): loss=2.177435396864545e+97\n",
      "Gradient Descent(44/49): loss=4.3513845960266106e+99\n",
      "Gradient Descent(45/49): loss=8.695802378248762e+101\n",
      "Gradient Descent(46/49): loss=1.7377682283153016e+104\n",
      "Gradient Descent(47/49): loss=3.4727541910287473e+106\n",
      "Gradient Descent(48/49): loss=6.939948305418956e+108\n",
      "Gradient Descent(49/49): loss=1.3868785359559255e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.160220330599373\n",
      "Gradient Descent(2/49): loss=1052.2711913862574\n",
      "Gradient Descent(3/49): loss=205663.64806511495\n",
      "Gradient Descent(4/49): loss=40809650.51176213\n",
      "Gradient Descent(5/49): loss=8132793153.003818\n",
      "Gradient Descent(6/49): loss=1622963577005.4382\n",
      "Gradient Descent(7/49): loss=324021065139241.3\n",
      "Gradient Descent(8/49): loss=6.4699889168821624e+16\n",
      "Gradient Descent(9/49): loss=1.291981238327834e+19\n",
      "Gradient Descent(10/49): loss=2.5799810667108295e+21\n",
      "Gradient Descent(11/49): loss=5.152042595651685e+23\n",
      "Gradient Descent(12/49): loss=1.0288291630805224e+26\n",
      "Gradient Descent(13/49): loss=2.054505906015624e+28\n",
      "Gradient Descent(14/49): loss=4.102717623301651e+30\n",
      "Gradient Descent(15/49): loss=8.192866830918894e+32\n",
      "Gradient Descent(16/49): loss=1.636063579750068e+35\n",
      "Gradient Descent(17/49): loss=3.26711530684235e+37\n",
      "Gradient Descent(18/49): loss=6.524222283225013e+39\n",
      "Gradient Descent(19/49): loss=1.3028458579466284e+42\n",
      "Gradient Descent(20/49): loss=2.601700641925021e+44\n",
      "Gradient Descent(21/49): loss=5.195431362669868e+46\n",
      "Gradient Descent(22/49): loss=1.037494729811867e+49\n",
      "Gradient Descent(23/49): loss=2.071811249665296e+51\n",
      "Gradient Descent(24/49): loss=4.137275815406951e+53\n",
      "Gradient Descent(25/49): loss=8.261877705098943e+55\n",
      "Gradient Descent(26/49): loss=1.6498446383454396e+58\n",
      "Gradient Descent(27/49): loss=3.294635224384725e+60\n",
      "Gradient Descent(28/49): loss=6.5791778264910525e+62\n",
      "Gradient Descent(29/49): loss=1.3138201325664546e+65\n",
      "Gradient Descent(30/49): loss=2.623615573646174e+67\n",
      "Gradient Descent(31/49): loss=5.2391940933593545e+69\n",
      "Gradient Descent(32/49): loss=1.0462338699165402e+72\n",
      "Gradient Descent(33/49): loss=2.0892627588429223e+74\n",
      "Gradient Descent(34/49): loss=4.17212537368544e+76\n",
      "Gradient Descent(35/49): loss=8.331470065254048e+78\n",
      "Gradient Descent(36/49): loss=1.6637417917982684e+81\n",
      "Gradient Descent(37/49): loss=3.322386959439558e+83\n",
      "Gradient Descent(38/49): loss=6.634596283311065e+85\n",
      "Gradient Descent(39/49): loss=1.3248868473150412e+88\n",
      "Gradient Descent(40/49): loss=2.6457151019178142e+90\n",
      "Gradient Descent(41/49): loss=5.283325451302852e+92\n",
      "Gradient Descent(42/49): loss=1.0550466225237499e+95\n",
      "Gradient Descent(43/49): loss=2.1068612675077095e+97\n",
      "Gradient Descent(44/49): loss=4.2072684806157045e+99\n",
      "Gradient Descent(45/49): loss=8.401648623462361e+101\n",
      "Gradient Descent(46/49): loss=1.6777560052881764e+104\n",
      "Gradient Descent(47/49): loss=3.3503724559722644e+106\n",
      "Gradient Descent(48/49): loss=6.690481546993266e+108\n",
      "Gradient Descent(49/49): loss=1.3360467804367494e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.240232261512424\n",
      "Gradient Descent(2/49): loss=1087.6374168903271\n",
      "Gradient Descent(3/49): loss=216819.29093521214\n",
      "Gradient Descent(4/49): loss=43853720.533448696\n",
      "Gradient Descent(5/49): loss=8904678825.989582\n",
      "Gradient Descent(6/49): loss=1810271817943.538\n",
      "Gradient Descent(7/49): loss=368155280617486.94\n",
      "Gradient Descent(8/49): loss=7.488075946529672e+16\n",
      "Gradient Descent(9/49): loss=1.5230925788675312e+19\n",
      "Gradient Descent(10/49): loss=3.0980459439783176e+21\n",
      "Gradient Descent(11/49): loss=6.301605301338226e+23\n",
      "Gradient Descent(12/49): loss=1.2817848283151502e+26\n",
      "Gradient Descent(13/49): loss=2.6072294136026685e+28\n",
      "Gradient Descent(14/49): loss=5.303266229235114e+30\n",
      "Gradient Descent(15/49): loss=1.0787172725871801e+33\n",
      "Gradient Descent(16/49): loss=2.194177930656316e+35\n",
      "Gradient Descent(17/49): loss=4.463094211896395e+37\n",
      "Gradient Descent(18/49): loss=9.07821088647847e+39\n",
      "Gradient Descent(19/49): loss=1.846564491601077e+42\n",
      "Gradient Descent(20/49): loss=3.756026892169241e+44\n",
      "Gradient Descent(21/49): loss=7.639992039094464e+46\n",
      "Gradient Descent(22/49): loss=1.5540218436724098e+49\n",
      "Gradient Descent(23/49): loss=3.160976972578117e+51\n",
      "Gradient Descent(24/49): loss=6.429623535777895e+53\n",
      "Gradient Descent(25/49): loss=1.3078253707789241e+56\n",
      "Gradient Descent(26/49): loss=2.6601980519322284e+58\n",
      "Gradient Descent(27/49): loss=5.4110081006379196e+60\n",
      "Gradient Descent(28/49): loss=1.1006326631922394e+63\n",
      "Gradient Descent(29/49): loss=2.2387552129941183e+65\n",
      "Gradient Descent(30/49): loss=4.5537671843861444e+67\n",
      "Gradient Descent(31/49): loss=9.262645352753199e+69\n",
      "Gradient Descent(32/49): loss=1.884079608308875e+72\n",
      "Gradient Descent(33/49): loss=3.83233497047386e+74\n",
      "Gradient Descent(34/49): loss=7.795207411166491e+76\n",
      "Gradient Descent(35/49): loss=1.585593614631011e+79\n",
      "Gradient Descent(36/49): loss=3.2251959160922635e+81\n",
      "Gradient Descent(37/49): loss=6.560248856450411e+83\n",
      "Gradient Descent(38/49): loss=1.3343953725051064e+86\n",
      "Gradient Descent(39/49): loss=2.714243086086976e+88\n",
      "Gradient Descent(40/49): loss=5.520939057620073e+90\n",
      "Gradient Descent(41/49): loss=1.122993302781072e+93\n",
      "Gradient Descent(42/49): loss=2.2842381430574512e+95\n",
      "Gradient Descent(43/49): loss=4.64628228973125e+97\n",
      "Gradient Descent(44/49): loss=9.450826824463661e+99\n",
      "Gradient Descent(45/49): loss=1.9223568887194806e+102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=3.910193336779233e+104\n",
      "Gradient Descent(47/49): loss=7.953576165129998e+106\n",
      "Gradient Descent(48/49): loss=1.6178068030423753e+109\n",
      "Gradient Descent(49/49): loss=3.290719542543553e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.213327128455073\n",
      "Gradient Descent(2/49): loss=1080.1985340001181\n",
      "Gradient Descent(3/49): loss=215306.37860112998\n",
      "Gradient Descent(4/49): loss=43597814.88528741\n",
      "Gradient Descent(5/49): loss=8867404193.673355\n",
      "Gradient Descent(6/49): loss=1806025049561.8975\n",
      "Gradient Descent(7/49): loss=367995516376845.5\n",
      "Gradient Descent(8/49): loss=7.499356635556648e+16\n",
      "Gradient Descent(9/49): loss=1.5283619118264932e+19\n",
      "Gradient Descent(10/49): loss=3.114836644603057e+21\n",
      "Gradient Descent(11/49): loss=6.348141907040827e+23\n",
      "Gradient Descent(12/49): loss=1.2937749440224365e+26\n",
      "Gradient Descent(13/49): loss=2.6367630124593544e+28\n",
      "Gradient Descent(14/49): loss=5.373825299893727e+30\n",
      "Gradient Descent(15/49): loss=1.0952065116230722e+33\n",
      "Gradient Descent(16/49): loss=2.2320735449791613e+35\n",
      "Gradient Descent(17/49): loss=4.549052888853659e+37\n",
      "Gradient Descent(18/49): loss=9.271147127312218e+39\n",
      "Gradient Descent(19/49): loss=1.8894959285794325e+42\n",
      "Gradient Descent(20/49): loss=3.850866366400112e+44\n",
      "Gradient Descent(21/49): loss=7.848215785418247e+46\n",
      "Gradient Descent(22/49): loss=1.5994969743353764e+49\n",
      "Gradient Descent(23/49): loss=3.259837192150148e+51\n",
      "Gradient Descent(24/49): loss=6.643675286584994e+53\n",
      "Gradient Descent(25/49): loss=1.3540069246376604e+56\n",
      "Gradient Descent(26/49): loss=2.759518900132401e+58\n",
      "Gradient Descent(27/49): loss=5.624007101903591e+60\n",
      "Gradient Descent(28/49): loss=1.1461945732912197e+63\n",
      "Gradient Descent(29/49): loss=2.33598922625397e+65\n",
      "Gradient Descent(30/49): loss=4.760837114684366e+67\n",
      "Gradient Descent(31/49): loss=9.70277164715487e+69\n",
      "Gradient Descent(32/49): loss=1.9774626892076273e+72\n",
      "Gradient Descent(33/49): loss=4.030146054560485e+74\n",
      "Gradient Descent(34/49): loss=8.213594779680899e+76\n",
      "Gradient Descent(35/49): loss=1.673962637866696e+79\n",
      "Gradient Descent(36/49): loss=3.4116011175833655e+81\n",
      "Gradient Descent(37/49): loss=6.95297608334248e+83\n",
      "Gradient Descent(38/49): loss=1.4170436328669448e+86\n",
      "Gradient Descent(39/49): loss=2.8879901690722393e+88\n",
      "Gradient Descent(40/49): loss=5.885836556622848e+90\n",
      "Gradient Descent(41/49): loss=1.1995564369392216e+93\n",
      "Gradient Descent(42/49): loss=2.4447427847502787e+95\n",
      "Gradient Descent(43/49): loss=4.982481106798831e+97\n",
      "Gradient Descent(44/49): loss=1.0154490744163458e+100\n",
      "Gradient Descent(45/49): loss=2.0695248022636433e+102\n",
      "Gradient Descent(46/49): loss=4.217772230129885e+104\n",
      "Gradient Descent(47/49): loss=8.595984240341851e+106\n",
      "Gradient Descent(48/49): loss=1.751895100744458e+109\n",
      "Gradient Descent(49/49): loss=3.570430515226701e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.23901580621602\n",
      "Gradient Descent(2/49): loss=1076.5011527897768\n",
      "Gradient Descent(3/49): loss=212724.53986035616\n",
      "Gradient Descent(4/49): loss=42682446.98842771\n",
      "Gradient Descent(5/49): loss=8600312177.143568\n",
      "Gradient Descent(6/49): loss=1735174329920.7786\n",
      "Gradient Descent(7/49): loss=350230037731536.1\n",
      "Gradient Descent(8/49): loss=7.070063277088385e+16\n",
      "Gradient Descent(9/49): loss=1.4272926282003692e+19\n",
      "Gradient Descent(10/49): loss=2.8814385268583453e+21\n",
      "Gradient Descent(11/49): loss=5.817118454890635e+23\n",
      "Gradient Descent(12/49): loss=1.1743760813845223e+26\n",
      "Gradient Descent(13/49): loss=2.370864509093943e+28\n",
      "Gradient Descent(14/49): loss=4.78637098153732e+30\n",
      "Gradient Descent(15/49): loss=9.6628670917007e+32\n",
      "Gradient Descent(16/49): loss=1.950768187447873e+35\n",
      "Gradient Descent(17/49): loss=3.938268541303257e+37\n",
      "Gradient Descent(18/49): loss=7.950693086296536e+39\n",
      "Gradient Descent(19/49): loss=1.6051094520531658e+42\n",
      "Gradient Descent(20/49): loss=3.2404424693621945e+44\n",
      "Gradient Descent(21/49): loss=6.541901167406465e+46\n",
      "Gradient Descent(22/49): loss=1.320698370363066e+49\n",
      "Gradient Descent(23/49): loss=2.6662649600819377e+51\n",
      "Gradient Descent(24/49): loss=5.3827346174666536e+53\n",
      "Gradient Descent(25/49): loss=1.0866823963818218e+56\n",
      "Gradient Descent(26/49): loss=2.1938265854213052e+58\n",
      "Gradient Descent(27/49): loss=4.4289620434882354e+60\n",
      "Gradient Descent(28/49): loss=8.941319661733105e+62\n",
      "Gradient Descent(29/49): loss=1.8051000778125905e+65\n",
      "Gradient Descent(30/49): loss=3.644189464407835e+67\n",
      "Gradient Descent(31/49): loss=7.356997551401052e+69\n",
      "Gradient Descent(32/49): loss=1.4852524409050242e+72\n",
      "Gradient Descent(33/49): loss=2.9984715881742133e+74\n",
      "Gradient Descent(34/49): loss=6.053403190914467e+76\n",
      "Gradient Descent(35/49): loss=1.2220789530337412e+79\n",
      "Gradient Descent(36/49): loss=2.4671691614554837e+81\n",
      "Gradient Descent(37/49): loss=4.980794126374999e+83\n",
      "Gradient Descent(38/49): loss=1.0055374603781245e+86\n",
      "Gradient Descent(39/49): loss=2.0300087868910993e+88\n",
      "Gradient Descent(40/49): loss=4.098241823139484e+90\n",
      "Gradient Descent(41/49): loss=8.273651892242094e+92\n",
      "Gradient Descent(42/49): loss=1.6703093323458904e+95\n",
      "Gradient Descent(43/49): loss=3.3720699179255978e+97\n",
      "Gradient Descent(44/49): loss=6.807634556773263e+99\n",
      "Gradient Descent(45/49): loss=1.374345413545964e+102\n",
      "Gradient Descent(46/49): loss=2.774569198717532e+104\n",
      "Gradient Descent(47/49): loss=5.601382420020394e+106\n",
      "Gradient Descent(48/49): loss=1.130823661915373e+109\n",
      "Gradient Descent(49/49): loss=2.2829402787732468e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.219966205415922\n",
      "Gradient Descent(2/49): loss=1073.8259467473913\n",
      "Gradient Descent(3/49): loss=212008.87375761356\n",
      "Gradient Descent(4/49): loss=42496416.3224093\n",
      "Gradient Descent(5/49): loss=8555084094.944123\n",
      "Gradient Descent(6/49): loss=1724603955751.597\n",
      "Gradient Descent(7/49): loss=347816581401693.4\n",
      "Gradient Descent(8/49): loss=7.015799411820902e+16\n",
      "Gradient Descent(9/49): loss=1.4152282453405383e+19\n",
      "Gradient Descent(10/49): loss=2.854851210450115e+21\n",
      "Gradient Descent(11/49): loss=5.75894715666751e+23\n",
      "Gradient Descent(12/49): loss=1.1617257277530676e+26\n",
      "Gradient Descent(13/49): loss=2.3434971481854658e+28\n",
      "Gradient Descent(14/49): loss=4.727432721453955e+30\n",
      "Gradient Descent(15/49): loss=9.536440876638916e+32\n",
      "Gradient Descent(16/49): loss=1.923744081617637e+35\n",
      "Gradient Descent(17/49): loss=3.8806839738937073e+37\n",
      "Gradient Descent(18/49): loss=7.828332416814766e+39\n",
      "Gradient Descent(19/49): loss=1.579174931814246e+42\n",
      "Gradient Descent(20/49): loss=3.1855998614693147e+44\n",
      "Gradient Descent(21/49): loss=6.426169941227914e+46\n",
      "Gradient Descent(22/49): loss=1.2963228877216145e+49\n",
      "Gradient Descent(23/49): loss=2.6150149227713895e+51\n",
      "Gradient Descent(24/49): loss=5.275154138775696e+53\n",
      "Gradient Descent(25/49): loss=1.0641335521864972e+56\n",
      "Gradient Descent(26/49): loss=2.1466296284426526e+58\n",
      "Gradient Descent(27/49): loss=4.330301165902227e+60\n",
      "Gradient Descent(28/49): loss=8.73532533929455e+62\n",
      "Gradient Descent(29/49): loss=1.7621386102235383e+65\n",
      "Gradient Descent(30/49): loss=3.5546844118931696e+67\n",
      "Gradient Descent(31/49): loss=7.170707908473453e+69\n",
      "Gradient Descent(32/49): loss=1.4465152444083983e+72\n",
      "Gradient Descent(33/49): loss=2.917991332255136e+74\n",
      "Gradient Descent(34/49): loss=5.88633507184261e+76\n",
      "Gradient Descent(35/49): loss=1.1874243831706681e+79\n",
      "Gradient Descent(36/49): loss=2.3953387779314485e+81\n",
      "Gradient Descent(37/49): loss=4.832011151515521e+83\n",
      "Gradient Descent(38/49): loss=9.747402740473086e+85\n",
      "Gradient Descent(39/49): loss=1.966300515576878e+88\n",
      "Gradient Descent(40/49): loss=3.9665312088769157e+90\n",
      "Gradient Descent(41/49): loss=8.001508267101616e+92\n",
      "Gradient Descent(42/49): loss=1.6141089323893963e+95\n",
      "Gradient Descent(43/49): loss=3.2560706789883545e+97\n",
      "Gradient Descent(44/49): loss=6.568327610252035e+99\n",
      "Gradient Descent(45/49): loss=1.3249997266337925e+102\n",
      "Gradient Descent(46/49): loss=2.6728634437164944e+104\n",
      "Gradient Descent(47/49): loss=5.391849405815418e+106\n",
      "Gradient Descent(48/49): loss=1.0876739731442737e+109\n",
      "Gradient Descent(49/49): loss=2.194116680223819e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.300810896714022\n",
      "Gradient Descent(2/49): loss=1109.9018548229387\n",
      "Gradient Descent(3/49): loss=223504.30264379285\n",
      "Gradient Descent(4/49): loss=45665151.002410725\n",
      "Gradient Descent(5/49): loss=9366758938.56257\n",
      "Gradient Descent(6/49): loss=1923571616088.6155\n",
      "Gradient Descent(7/49): loss=395174969149431.5\n",
      "Gradient Descent(8/49): loss=8.1193748275391e+16\n",
      "Gradient Descent(9/49): loss=1.6682942761368898e+19\n",
      "Gradient Descent(10/49): loss=3.4279007436710766e+21\n",
      "Gradient Descent(11/49): loss=7.043452914506451e+23\n",
      "Gradient Descent(12/49): loss=1.447250075751385e+26\n",
      "Gradient Descent(13/49): loss=2.9737313984311847e+28\n",
      "Gradient Descent(14/49): loss=6.110263773640886e+30\n",
      "Gradient Descent(15/49): loss=1.2555042855949651e+33\n",
      "Gradient Descent(16/49): loss=2.5797430257027245e+35\n",
      "Gradient Descent(17/49): loss=5.300717957455631e+37\n",
      "Gradient Descent(18/49): loss=1.0891631697811377e+40\n",
      "Gradient Descent(19/49): loss=2.2379542178442678e+42\n",
      "Gradient Descent(20/49): loss=4.5984286110764964e+44\n",
      "Gradient Descent(21/49): loss=9.44860512506724e+46\n",
      "Gradient Descent(22/49): loss=1.9414488374591747e+49\n",
      "Gradient Descent(23/49): loss=3.9891852168687674e+51\n",
      "Gradient Descent(24/49): loss=8.196764389291345e+53\n",
      "Gradient Descent(25/49): loss=1.6842272995871663e+56\n",
      "Gradient Descent(26/49): loss=3.46066016046673e+58\n",
      "Gradient Descent(27/49): loss=7.110779375906075e+60\n",
      "Gradient Descent(28/49): loss=1.4610849083197164e+63\n",
      "Gradient Descent(29/49): loss=3.002159111493517e+65\n",
      "Gradient Descent(30/49): loss=6.168675947169065e+67\n",
      "Gradient Descent(31/49): loss=1.2675065353965123e+70\n",
      "Gradient Descent(32/49): loss=2.6044046259394882e+72\n",
      "Gradient Descent(33/49): loss=5.351391307416877e+74\n",
      "Gradient Descent(34/49): loss=1.0995752595381206e+77\n",
      "Gradient Descent(35/49): loss=2.259348423488675e+79\n",
      "Gradient Descent(36/49): loss=4.642388280784879e+81\n",
      "Gradient Descent(37/49): loss=9.538931103105625e+83\n",
      "Gradient Descent(38/49): loss=1.9600085362616747e+86\n",
      "Gradient Descent(39/49): loss=4.0273206931622616e+88\n",
      "Gradient Descent(40/49): loss=8.275123125998229e+90\n",
      "Gradient Descent(41/49): loss=1.7003280336402027e+93\n",
      "Gradient Descent(42/49): loss=3.493743087519342e+95\n",
      "Gradient Descent(43/49): loss=7.178756404702074e+97\n",
      "Gradient Descent(44/49): loss=1.4750524645657908e+100\n",
      "Gradient Descent(45/49): loss=3.0308588988985344e+102\n",
      "Gradient Descent(46/49): loss=6.22764673508518e+104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=1.2796235374438492e+107\n",
      "Gradient Descent(48/49): loss=2.6293019935690015e+109\n",
      "Gradient Descent(49/49): loss=5.402549086581871e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.273594309756007\n",
      "Gradient Descent(2/49): loss=1102.3119569276794\n",
      "Gradient Descent(3/49): loss=221944.66993055522\n",
      "Gradient Descent(4/49): loss=45398594.01874015\n",
      "Gradient Descent(5/49): loss=9327523366.35313\n",
      "Gradient Descent(6/49): loss=1919051927199.4543\n",
      "Gradient Descent(7/49): loss=395001721064327.06\n",
      "Gradient Descent(8/49): loss=8.131564791903269e+16\n",
      "Gradient Descent(9/49): loss=1.6740562460745867e+19\n",
      "Gradient Descent(10/49): loss=3.4464569506260603e+21\n",
      "Gradient Descent(11/49): loss=7.09541760926726e+23\n",
      "Gradient Descent(12/49): loss=1.460776678336476e+26\n",
      "Gradient Descent(13/49): loss=3.007391342838563e+28\n",
      "Gradient Descent(14/49): loss=6.191503861695318e+30\n",
      "Gradient Descent(15/49): loss=1.2746835438349597e+33\n",
      "Gradient Descent(16/49): loss=2.624270626563885e+35\n",
      "Gradient Descent(17/49): loss=5.402749885658278e+37\n",
      "Gradient Descent(18/49): loss=1.1122978743496683e+40\n",
      "Gradient Descent(19/49): loss=2.2899571300659363e+42\n",
      "Gradient Descent(20/49): loss=4.714477820959332e+44\n",
      "Gradient Descent(21/49): loss=9.705990053021766e+46\n",
      "Gradient Descent(22/49): loss=1.9982328158017944e+49\n",
      "Gradient Descent(23/49): loss=4.1138867486710385e+51\n",
      "Gradient Descent(24/49): loss=8.469515687617707e+53\n",
      "Gradient Descent(25/49): loss=1.7436721126572587e+56\n",
      "Gradient Descent(26/49): loss=3.589806724019355e+58\n",
      "Gradient Descent(27/49): loss=7.390559396042328e+60\n",
      "Gradient Descent(28/49): loss=1.5215406395271216e+63\n",
      "Gradient Descent(29/49): loss=3.132490781377617e+65\n",
      "Gradient Descent(30/49): loss=6.449054491548396e+67\n",
      "Gradient Descent(31/49): loss=1.3277071422591596e+70\n",
      "Gradient Descent(32/49): loss=2.7334336497174604e+72\n",
      "Gradient Descent(33/49): loss=5.627490641267717e+74\n",
      "Gradient Descent(34/49): loss=1.1585666592210555e+77\n",
      "Gradient Descent(35/49): loss=2.3852135692868146e+79\n",
      "Gradient Descent(36/49): loss=4.910588204683046e+81\n",
      "Gradient Descent(37/49): loss=1.010973475351407e+84\n",
      "Gradient Descent(38/49): loss=2.0813542599426268e+86\n",
      "Gradient Descent(39/49): loss=4.2850140592219925e+88\n",
      "Gradient Descent(40/49): loss=8.821826174000927e+90\n",
      "Gradient Descent(41/49): loss=1.816204473747235e+93\n",
      "Gradient Descent(42/49): loss=3.7391336276618984e+95\n",
      "Gradient Descent(43/49): loss=7.697988022607281e+97\n",
      "Gradient Descent(44/49): loss=1.5848328917107972e+100\n",
      "Gradient Descent(45/49): loss=3.2627944954865185e+102\n",
      "Gradient Descent(46/49): loss=6.717318889239598e+104\n",
      "Gradient Descent(47/49): loss=1.3829364099441057e+107\n",
      "Gradient Descent(48/49): loss=2.8471375938586438e+109\n",
      "Gradient Descent(49/49): loss=5.861580055362658e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.299597466397921\n",
      "Gradient Descent(2/49): loss=1098.5449008831793\n",
      "Gradient Descent(3/49): loss=219285.1635798435\n",
      "Gradient Descent(4/49): loss=44445962.332501665\n",
      "Gradient Descent(5/49): loss=9046713037.35031\n",
      "Gradient Descent(6/49): loss=1843801748384.2275\n",
      "Gradient Descent(7/49): loss=375940727142783.06\n",
      "Gradient Descent(8/49): loss=7.666273552839835e+16\n",
      "Gradient Descent(9/49): loss=1.5633964336876681e+19\n",
      "Gradient Descent(10/49): loss=3.188310125318368e+21\n",
      "Gradient Descent(11/49): loss=6.502108634269824e+23\n",
      "Gradient Descent(12/49): loss=1.3260155845660712e+26\n",
      "Gradient Descent(13/49): loss=2.704227873603215e+28\n",
      "Gradient Descent(14/49): loss=5.514904866832935e+30\n",
      "Gradient Descent(15/49): loss=1.1246898958178996e+33\n",
      "Gradient Descent(16/49): loss=2.2936522382329517e+35\n",
      "Gradient Descent(17/49): loss=4.677592150255401e+37\n",
      "Gradient Descent(18/49): loss=9.539313768969127e+39\n",
      "Gradient Descent(19/49): loss=1.94541345919113e+42\n",
      "Gradient Descent(20/49): loss=3.967406482161529e+44\n",
      "Gradient Descent(21/49): loss=8.090986582711676e+46\n",
      "Gradient Descent(22/49): loss=1.6500468045807357e+49\n",
      "Gradient Descent(23/49): loss=3.365046313576669e+51\n",
      "Gradient Descent(24/49): loss=6.8625548445774826e+53\n",
      "Gradient Descent(25/49): loss=1.3995248387779836e+56\n",
      "Gradient Descent(26/49): loss=2.854140795544933e+58\n",
      "Gradient Descent(27/49): loss=5.820632442585162e+60\n",
      "Gradient Descent(28/49): loss=1.1870389184920287e+63\n",
      "Gradient Descent(29/49): loss=2.4208046254659734e+65\n",
      "Gradient Descent(30/49): loss=4.93690218861753e+67\n",
      "Gradient Descent(31/49): loss=1.0068141378937227e+70\n",
      "Gradient Descent(32/49): loss=2.05326066738733e+72\n",
      "Gradient Descent(33/49): loss=4.1873462137307625e+74\n",
      "Gradient Descent(34/49): loss=8.539523788743405e+76\n",
      "Gradient Descent(35/49): loss=1.7415198747930206e+79\n",
      "Gradient Descent(36/49): loss=3.5515932144799533e+81\n",
      "Gradient Descent(37/49): loss=7.242991908225655e+83\n",
      "Gradient Descent(38/49): loss=1.4771098100068888e+86\n",
      "Gradient Descent(39/49): loss=3.0123648051307577e+88\n",
      "Gradient Descent(40/49): loss=6.143308816795512e+90\n",
      "Gradient Descent(41/49): loss=1.2528443817374692e+93\n",
      "Gradient Descent(42/49): loss=2.5550059286615754e+95\n",
      "Gradient Descent(43/49): loss=5.210587516418082e+97\n",
      "Gradient Descent(44/49): loss=1.0626285427241422e+100\n",
      "Gradient Descent(45/49): loss=2.1670865641428946e+102\n",
      "Gradient Descent(46/49): loss=4.419478667916635e+104\n",
      "Gradient Descent(47/49): loss=9.012926395902928e+106\n",
      "Gradient Descent(48/49): loss=1.8380639057650884e+109\n",
      "Gradient Descent(49/49): loss=3.748481650989842e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.280022648025355\n",
      "Gradient Descent(2/49): loss=1095.7103430997706\n",
      "Gradient Descent(3/49): loss=218516.41041399547\n",
      "Gradient Descent(4/49): loss=44243873.76633558\n",
      "Gradient Descent(5/49): loss=8997008133.777613\n",
      "Gradient Descent(6/49): loss=1832049139304.4814\n",
      "Gradient Descent(7/49): loss=373226363505730.25\n",
      "Gradient Descent(8/49): loss=7.60455214205555e+16\n",
      "Gradient Descent(9/49): loss=1.549521036338136e+19\n",
      "Gradient Descent(10/49): loss=3.157395626343566e+21\n",
      "Gradient Descent(11/49): loss=6.433734921293121e+23\n",
      "Gradient Descent(12/49): loss=1.3109865209591247e+26\n",
      "Gradient Descent(13/49): loss=2.6713672601464448e+28\n",
      "Gradient Descent(14/49): loss=5.443385319625706e+30\n",
      "Gradient Descent(15/49): loss=1.1091865451449154e+33\n",
      "Gradient Descent(16/49): loss=2.2601648699233013e+35\n",
      "Gradient Descent(17/49): loss=4.605487969487426e+37\n",
      "Gradient Descent(18/49): loss=9.384501011868866e+39\n",
      "Gradient Descent(19/49): loss=1.9122590250443633e+42\n",
      "Gradient Descent(20/49): loss=3.896567942922633e+44\n",
      "Gradient Descent(21/49): loss=7.939950360820012e+46\n",
      "Gradient Descent(22/49): loss=1.6179061332611602e+49\n",
      "Gradient Descent(23/49): loss=3.296771562959383e+51\n",
      "Gradient Descent(24/49): loss=6.717758536763666e+53\n",
      "Gradient Descent(25/49): loss=1.3688628070376886e+56\n",
      "Gradient Descent(26/49): loss=2.789301482388541e+58\n",
      "Gradient Descent(27/49): loss=5.6836979715253915e+60\n",
      "Gradient Descent(28/49): loss=1.1581545715116176e+63\n",
      "Gradient Descent(29/49): loss=2.359945968686507e+65\n",
      "Gradient Descent(30/49): loss=4.808809732409567e+67\n",
      "Gradient Descent(31/49): loss=9.798805290185449e+69\n",
      "Gradient Descent(32/49): loss=1.9966808931501376e+72\n",
      "Gradient Descent(33/49): loss=4.068592518175638e+74\n",
      "Gradient Descent(34/49): loss=8.290481035674489e+76\n",
      "Gradient Descent(35/49): loss=1.6893329940472305e+79\n",
      "Gradient Descent(36/49): loss=3.442316498278329e+81\n",
      "Gradient Descent(37/49): loss=7.014332234126594e+83\n",
      "Gradient Descent(38/49): loss=1.4292949737572104e+86\n",
      "Gradient Descent(39/49): loss=2.912442772625529e+88\n",
      "Gradient Descent(40/49): loss=5.934620256531867e+90\n",
      "Gradient Descent(41/49): loss=1.2092844508490811e+93\n",
      "Gradient Descent(42/49): loss=2.4641321935566412e+95\n",
      "Gradient Descent(43/49): loss=5.02110769973017e+97\n",
      "Gradient Descent(44/49): loss=1.023140016522414e+100\n",
      "Gradient Descent(45/49): loss=2.0848297945605555e+102\n",
      "Gradient Descent(46/49): loss=4.2482115859967496e+104\n",
      "Gradient Descent(47/49): loss=8.65648683958923e+106\n",
      "Gradient Descent(48/49): loss=1.7639131876337487e+109\n",
      "Gradient Descent(49/49): loss=3.5942869101108635e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.361704378547383\n",
      "Gradient Descent(2/49): loss=1132.5065583543671\n",
      "Gradient Descent(3/49): loss=230360.20509212418\n",
      "Gradient Descent(4/49): loss=47541717.32484209\n",
      "Gradient Descent(5/49): loss=9850308581.821999\n",
      "Gradient Descent(6/49): loss=2043338068778.1965\n",
      "Gradient Descent(7/49): loss=424026504099889.25\n",
      "Gradient Descent(8/49): loss=8.800310719502638e+16\n",
      "Gradient Descent(9/49): loss=1.8265012170751427e+19\n",
      "Gradient Descent(10/49): loss=3.790944743273851e+21\n",
      "Gradient Descent(11/49): loss=7.868224601283506e+23\n",
      "Gradient Descent(12/49): loss=1.6330768908136762e+26\n",
      "Gradient Descent(13/49): loss=3.389508373623501e+28\n",
      "Gradient Descent(14/49): loss=7.035044553306704e+30\n",
      "Gradient Descent(15/49): loss=1.4601484571302718e+33\n",
      "Gradient Descent(16/49): loss=3.0305899754991366e+35\n",
      "Gradient Descent(17/49): loss=6.29009714834881e+37\n",
      "Gradient Descent(18/49): loss=1.3055320093261437e+40\n",
      "Gradient Descent(19/49): loss=2.7096780671366266e+42\n",
      "Gradient Descent(20/49): loss=5.624033096377304e+44\n",
      "Gradient Descent(21/49): loss=1.1672880499939121e+47\n",
      "Gradient Descent(22/49): loss=2.4227478187535466e+49\n",
      "Gradient Descent(23/49): loss=5.028499172369917e+51\n",
      "Gradient Descent(24/49): loss=1.0436828683055287e+54\n",
      "Gradient Descent(25/49): loss=2.16620087277813e+56\n",
      "Gradient Descent(26/49): loss=4.4960268714998963e+58\n",
      "Gradient Descent(27/49): loss=9.331663505114364e+60\n",
      "Gradient Descent(28/49): loss=1.9368199137038302e+63\n",
      "Gradient Descent(29/49): loss=4.0199385415728047e+65\n",
      "Gradient Descent(30/49): loss=8.343525262046446e+67\n",
      "Gradient Descent(31/49): loss=1.7317283107310994e+70\n",
      "Gradient Descent(32/49): loss=3.5942636331780556e+72\n",
      "Gradient Descent(33/49): loss=7.460021866439498e+74\n",
      "Gradient Descent(34/49): loss=1.5483540420920134e+77\n",
      "Gradient Descent(35/49): loss=3.213663823758847e+79\n",
      "Gradient Descent(36/49): loss=6.670073440169074e+81\n",
      "Gradient Descent(37/49): loss=1.3843974397176253e+84\n",
      "Gradient Descent(38/49): loss=2.873366070536297e+86\n",
      "Gradient Descent(39/49): loss=5.963773363372614e+88\n",
      "Gradient Descent(40/49): loss=1.237802349459579e+91\n",
      "Gradient Descent(41/49): loss=2.5691027525250943e+93\n",
      "Gradient Descent(42/49): loss=5.332264037076476e+95\n",
      "Gradient Descent(43/49): loss=1.1067303451819116e+98\n",
      "Gradient Descent(44/49): loss=2.2970581509651226e+100\n",
      "Gradient Descent(45/49): loss=4.767625801430482e+102\n",
      "Gradient Descent(46/49): loss=9.895376733460222e+104\n",
      "Gradient Descent(47/49): loss=2.053820596988273e+107\n",
      "Gradient Descent(48/49): loss=4.262777616490248e+109\n",
      "Gradient Descent(49/49): loss=8.847546389541863e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.334174602392272\n",
      "Gradient Descent(2/49): loss=1124.7633584069713\n",
      "Gradient Descent(3/49): loss=228752.65614815312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=47264123.16554278\n",
      "Gradient Descent(5/49): loss=9809019732.145842\n",
      "Gradient Descent(6/49): loss=2038529478364.3174\n",
      "Gradient Descent(7/49): loss=423838739496309.7\n",
      "Gradient Descent(8/49): loss=8.813478186475523e+16\n",
      "Gradient Descent(9/49): loss=1.8327990831129164e+19\n",
      "Gradient Descent(10/49): loss=3.8114418725657803e+21\n",
      "Gradient Descent(11/49): loss=7.926218539152445e+23\n",
      "Gradient Descent(12/49): loss=1.6483276700557366e+26\n",
      "Gradient Descent(13/49): loss=3.4278460625076506e+28\n",
      "Gradient Descent(14/49): loss=7.128516456484179e+30\n",
      "Gradient Descent(15/49): loss=1.4824396791885703e+33\n",
      "Gradient Descent(16/49): loss=3.0828678917744404e+35\n",
      "Gradient Descent(17/49): loss=6.411103696154115e+37\n",
      "Gradient Descent(18/49): loss=1.3332472277206405e+40\n",
      "Gradient Descent(19/49): loss=2.7726086737758865e+42\n",
      "Gradient Descent(20/49): loss=5.765891501670105e+44\n",
      "Gradient Descent(21/49): loss=1.1990694946033076e+47\n",
      "Gradient Descent(22/49): loss=2.493573894885309e+49\n",
      "Gradient Descent(23/49): loss=5.18561334209538e+51\n",
      "Gradient Descent(24/49): loss=1.0783953821846604e+54\n",
      "Gradient Descent(25/49): loss=2.2426211203948745e+56\n",
      "Gradient Descent(26/49): loss=4.6637342599282543e+58\n",
      "Gradient Descent(27/49): loss=9.698658881532944e+60\n",
      "Gradient Descent(28/49): loss=2.016924182592448e+63\n",
      "Gradient Descent(29/49): loss=4.19437698347349e+65\n",
      "Gradient Descent(30/49): loss=8.722587805397536e+67\n",
      "Gradient Descent(31/49): loss=1.8139413391464596e+70\n",
      "Gradient Descent(32/49): loss=3.772255728773912e+72\n",
      "Gradient Descent(33/49): loss=7.844748325744375e+74\n",
      "Gradient Descent(34/49): loss=1.631386648175933e+77\n",
      "Gradient Descent(35/49): loss=3.392616672115051e+79\n",
      "Gradient Descent(36/49): loss=7.05525443449122e+81\n",
      "Gradient Descent(37/49): loss=1.4672042245307888e+84\n",
      "Gradient Descent(38/49): loss=3.051184413643666e+86\n",
      "Gradient Descent(39/49): loss=6.345215049417748e+88\n",
      "Gradient Descent(40/49): loss=1.3195450869283089e+91\n",
      "Gradient Descent(41/49): loss=2.7441138288865648e+93\n",
      "Gradient Descent(42/49): loss=5.706633885027351e+95\n",
      "Gradient Descent(43/49): loss=1.1867463351896042e+98\n",
      "Gradient Descent(44/49): loss=2.4679467659229445e+100\n",
      "Gradient Descent(45/49): loss=5.132319400385138e+102\n",
      "Gradient Descent(46/49): loss=1.0673124230748457e+105\n",
      "Gradient Descent(47/49): loss=2.2195731005447846e+107\n",
      "Gradient Descent(48/49): loss=4.61580380978714e+109\n",
      "Gradient Descent(49/49): loss=9.598983158164954e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.360494056137729\n",
      "Gradient Descent(2/49): loss=1120.9256546127938\n",
      "Gradient Descent(3/49): loss=226013.54590568447\n",
      "Gradient Descent(4/49): loss=46272907.28809426\n",
      "Gradient Descent(5/49): loss=9513860243.092964\n",
      "Gradient Descent(6/49): loss=1958630719691.6257\n",
      "Gradient Descent(7/49): loss=403394960558034.25\n",
      "Gradient Descent(8/49): loss=8.309373896949384e+16\n",
      "Gradient Descent(9/49): loss=1.711693738157248e+19\n",
      "Gradient Descent(10/49): loss=3.526066358575596e+21\n",
      "Gradient Descent(11/49): loss=7.263687145940137e+23\n",
      "Gradient Descent(12/49): loss=1.4963201596109465e+26\n",
      "Gradient Descent(13/49): loss=3.0824225595555053e+28\n",
      "Gradient Descent(14/49): loss=6.349797945453563e+30\n",
      "Gradient Descent(15/49): loss=1.3080600011130083e+33\n",
      "Gradient Descent(16/49): loss=2.6946070071687028e+35\n",
      "Gradient Descent(17/49): loss=5.550897489395502e+37\n",
      "Gradient Descent(18/49): loss=1.1434863388747552e+40\n",
      "Gradient Descent(19/49): loss=2.3555848595058145e+42\n",
      "Gradient Descent(20/49): loss=4.852510994862201e+44\n",
      "Gradient Descent(21/49): loss=9.996185389975168e+46\n",
      "Gradient Descent(22/49): loss=2.0592168149573526e+49\n",
      "Gradient Descent(23/49): loss=4.241992045585545e+51\n",
      "Gradient Descent(24/49): loss=8.738514751904101e+53\n",
      "Gradient Descent(25/49): loss=1.8001363333248604e+56\n",
      "Gradient Descent(26/49): loss=3.7082855731878804e+58\n",
      "Gradient Descent(27/49): loss=7.63907801745006e+60\n",
      "Gradient Descent(28/49): loss=1.57365207735404e+63\n",
      "Gradient Descent(29/49): loss=3.2417274112188473e+65\n",
      "Gradient Descent(30/49): loss=6.677966978772967e+67\n",
      "Gradient Descent(31/49): loss=1.3756629510318716e+70\n",
      "Gradient Descent(32/49): loss=2.8338692911438e+72\n",
      "Gradient Descent(33/49): loss=5.837778180523124e+74\n",
      "Gradient Descent(34/49): loss=1.2025838379876934e+77\n",
      "Gradient Descent(35/49): loss=2.477325863826531e+79\n",
      "Gradient Descent(36/49): loss=5.103297784089016e+81\n",
      "Gradient Descent(37/49): loss=1.0512806834729571e+84\n",
      "Gradient Descent(38/49): loss=2.1656409682560935e+86\n",
      "Gradient Descent(39/49): loss=4.4612260808365184e+88\n",
      "Gradient Descent(40/49): loss=9.190137440169787e+90\n",
      "Gradient Descent(41/49): loss=1.893170725689256e+93\n",
      "Gradient Descent(42/49): loss=3.89993666573559e+95\n",
      "Gradient Descent(43/49): loss=8.033879771308785e+97\n",
      "Gradient Descent(44/49): loss=1.6549813423103455e+100\n",
      "Gradient Descent(45/49): loss=3.409265910571583e+102\n",
      "Gradient Descent(46/49): loss=7.023096727338164e+104\n",
      "Gradient Descent(47/49): loss=1.4467597698555123e+107\n",
      "Gradient Descent(48/49): loss=2.9803289245963365e+109\n",
      "Gradient Descent(49/49): loss=6.139485409988012e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.340389658427678\n",
      "Gradient Descent(2/49): loss=1117.9277250176722\n",
      "Gradient Descent(3/49): loss=225189.56281259688\n",
      "Gradient Descent(4/49): loss=46053885.10915585\n",
      "Gradient Descent(5/49): loss=9459372480.91576\n",
      "Gradient Descent(6/49): loss=1945598961203.122\n",
      "Gradient Descent(7/49): loss=400350976530754.4\n",
      "Gradient Descent(8/49): loss=8.239384067860704e+16\n",
      "Gradient Descent(9/49): loss=1.6957864904822995e+19\n",
      "Gradient Descent(10/49): loss=3.4902401624539393e+21\n",
      "Gradient Descent(11/49): loss=7.183599240783723e+23\n",
      "Gradient Descent(12/49): loss=1.4785287933770847e+26\n",
      "Gradient Descent(13/49): loss=3.0431109595783943e+28\n",
      "Gradient Descent(14/49): loss=6.263338683859894e+30\n",
      "Gradient Descent(15/49): loss=1.2891220627673338e+33\n",
      "Gradient Descent(16/49): loss=2.653274594653612e+35\n",
      "Gradient Descent(17/49): loss=5.460977162515805e+37\n",
      "Gradient Descent(18/49): loss=1.1239798447967749e+40\n",
      "Gradient Descent(19/49): loss=2.3133784584491377e+42\n",
      "Gradient Descent(20/49): loss=4.761402011731304e+44\n",
      "Gradient Descent(21/49): loss=9.79993093548297e+46\n",
      "Gradient Descent(22/49): loss=2.0170245257204282e+49\n",
      "Gradient Descent(23/49): loss=4.151445519573014e+51\n",
      "Gradient Descent(24/49): loss=8.544516778209612e+53\n",
      "Gradient Descent(25/49): loss=1.7586348328317684e+56\n",
      "Gradient Descent(26/49): loss=3.619627131095973e+58\n",
      "Gradient Descent(27/49): loss=7.44992668379747e+60\n",
      "Gradient Descent(28/49): loss=1.5333459934906716e+63\n",
      "Gradient Descent(29/49): loss=3.1559370119270867e+65\n",
      "Gradient Descent(30/49): loss=6.495558383778383e+67\n",
      "Gradient Descent(31/49): loss=1.3369176430840828e+70\n",
      "Gradient Descent(32/49): loss=2.7516476317926898e+72\n",
      "Gradient Descent(33/49): loss=5.663448850958235e+74\n",
      "Gradient Descent(34/49): loss=1.1656526263329656e+77\n",
      "Gradient Descent(35/49): loss=2.3991494953592525e+79\n",
      "Gradient Descent(36/49): loss=4.937936200761723e+81\n",
      "Gradient Descent(37/49): loss=1.016327409774101e+84\n",
      "Gradient Descent(38/49): loss=2.0918079170378948e+86\n",
      "Gradient Descent(39/49): loss=4.305364904755427e+88\n",
      "Gradient Descent(40/49): loss=8.861314087264682e+90\n",
      "Gradient Descent(41/49): loss=1.8238381435781287e+93\n",
      "Gradient Descent(42/49): loss=3.75382876762165e+95\n",
      "Gradient Descent(43/49): loss=7.726140867401045e+97\n",
      "Gradient Descent(44/49): loss=1.590196474005523e+100\n",
      "Gradient Descent(45/49): loss=3.27294682990972e+102\n",
      "Gradient Descent(46/49): loss=6.736388318377567e+104\n",
      "Gradient Descent(47/49): loss=1.3864853275733031e+107\n",
      "Gradient Descent(48/49): loss=2.853667978628415e+109\n",
      "Gradient Descent(49/49): loss=5.873427414123568e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.422912707012509\n",
      "Gradient Descent(2/49): loss=1155.454977556754\n",
      "Gradient Descent(3/49): loss=237390.4754786695\n",
      "Gradient Descent(4/49): loss=49485416.87012028\n",
      "Gradient Descent(5/49): loss=10356209922.139296\n",
      "Gradient Descent(6/49): loss=2169904988630.7222\n",
      "Gradient Descent(7/49): loss=454823934437545.9\n",
      "Gradient Descent(8/49): loss=9.53450738067558e+16\n",
      "Gradient Descent(9/49): loss=1.998804020804613e+19\n",
      "Gradient Descent(10/49): loss=4.190325089035049e+21\n",
      "Gradient Descent(11/49): loss=8.784701988829711e+23\n",
      "Gradient Descent(12/49): loss=1.84164934159673e+26\n",
      "Gradient Descent(13/49): loss=3.860886592596899e+28\n",
      "Gradient Descent(14/49): loss=8.094074764349746e+30\n",
      "Gradient Descent(15/49): loss=1.6968654182129632e+33\n",
      "Gradient Descent(16/49): loss=3.557358161935e+35\n",
      "Gradient Descent(17/49): loss=7.457749460465874e+37\n",
      "Gradient Descent(18/49): loss=1.5634643625017613e+40\n",
      "Gradient Descent(19/49): loss=3.277692321408135e+42\n",
      "Gradient Descent(20/49): loss=6.871449847797093e+44\n",
      "Gradient Descent(21/49): loss=1.4405508017080621e+47\n",
      "Gradient Descent(22/49): loss=3.02001274593742e+49\n",
      "Gradient Descent(23/49): loss=6.33124286548436e+51\n",
      "Gradient Descent(24/49): loss=1.3273002332772192e+54\n",
      "Gradient Descent(25/49): loss=2.7825909488692107e+56\n",
      "Gradient Descent(26/49): loss=5.83350488051445e+58\n",
      "Gradient Descent(27/49): loss=1.2229529893647224e+61\n",
      "Gradient Descent(28/49): loss=2.5638343411555574e+63\n",
      "Gradient Descent(29/49): loss=5.374897143268895e+65\n",
      "Gradient Descent(30/49): loss=1.1268091247931138e+68\n",
      "Gradient Descent(31/49): loss=2.362275537322783e+70\n",
      "Gradient Descent(32/49): loss=4.952343384029837e+72\n",
      "Gradient Descent(33/49): loss=1.038223721401261e+75\n",
      "Gradient Descent(34/49): loss=2.1765625121155488e+77\n",
      "Gradient Descent(35/49): loss=4.563009177591109e+79\n",
      "Gradient Descent(36/49): loss=9.566025620161624e+81\n",
      "Gradient Descent(37/49): loss=2.0054495313090133e+84\n",
      "Gradient Descent(38/49): loss=4.204282930364551e+86\n",
      "Gradient Descent(39/49): loss=8.813981445355587e+88\n",
      "Gradient Descent(40/49): loss=1.8477887955161202e+91\n",
      "Gradient Descent(41/49): loss=3.8737583622144314e+93\n",
      "Gradient Descent(42/49): loss=8.121060093686062e+95\n",
      "Gradient Descent(43/49): loss=1.7025227409269586e+98\n",
      "Gradient Descent(44/49): loss=3.569218365502579e+100\n",
      "Gradient Descent(45/49): loss=7.482613555989767e+102\n",
      "Gradient Descent(46/49): loss=1.5686769453344434e+105\n",
      "Gradient Descent(47/49): loss=3.288620133073701e+107\n",
      "Gradient Descent(48/49): loss=6.894359231723068e+109\n",
      "Gradient Descent(49/49): loss=1.4453535918610158e+112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.3950680063638705\n",
      "Gradient Descent(2/49): loss=1147.5561655302117\n",
      "Gradient Descent(3/49): loss=235733.79009616526\n",
      "Gradient Descent(4/49): loss=49196387.77861162\n",
      "Gradient Descent(5/49): loss=10312771613.48469\n",
      "Gradient Descent(6/49): loss=2164790670023.3987\n",
      "Gradient Descent(7/49): loss=454620549013029.94\n",
      "Gradient Descent(8/49): loss=9.548725321326446e+16\n",
      "Gradient Descent(9/49): loss=2.0056845976370057e+19\n",
      "Gradient Descent(10/49): loss=4.2129549805158295e+21\n",
      "Gradient Descent(11/49): loss=8.849389395891164e+23\n",
      "Gradient Descent(12/49): loss=1.858833801952747e+26\n",
      "Gradient Descent(13/49): loss=3.9045237861970907e+28\n",
      "Gradient Descent(14/49): loss=8.20154495370846e+30\n",
      "Gradient Descent(15/49): loss=1.7227541105278304e+33\n",
      "Gradient Descent(16/49): loss=3.618686243489282e+35\n",
      "Gradient Descent(17/49): loss=7.601137123237007e+37\n",
      "Gradient Descent(18/49): loss=1.5966370615772534e+40\n",
      "Gradient Descent(19/49): loss=3.3537743986473245e+42\n",
      "Gradient Descent(20/49): loss=7.044683472976363e+44\n",
      "Gradient Descent(21/49): loss=1.4797526410503257e+47\n",
      "Gradient Descent(22/49): loss=3.108255874320126e+49\n",
      "Gradient Descent(23/49): loss=6.528965931416508e+51\n",
      "Gradient Descent(24/49): loss=1.3714249359558288e+54\n",
      "Gradient Descent(25/49): loss=2.880710934503931e+56\n",
      "Gradient Descent(26/49): loss=6.051002333853452e+58\n",
      "Gradient Descent(27/49): loss=1.27102753718708e+61\n",
      "Gradient Descent(28/49): loss=2.6698237930758123e+63\n",
      "Gradient Descent(29/49): loss=5.608028840861099e+65\n",
      "Gradient Descent(30/49): loss=1.177979893710429e+68\n",
      "Gradient Descent(31/49): loss=2.4743749887223483e+70\n",
      "Gradient Descent(32/49): loss=5.1974839447639865e+72\n",
      "Gradient Descent(33/49): loss=1.0917439547038073e+75\n",
      "Gradient Descent(34/49): loss=2.293234332802624e+77\n",
      "Gradient Descent(35/49): loss=4.816993657245793e+79\n",
      "Gradient Descent(36/49): loss=1.0118210582338823e+82\n",
      "Gradient Descent(37/49): loss=2.1253543739787517e+84\n",
      "Gradient Descent(38/49): loss=4.464357781676546e+86\n",
      "Gradient Descent(39/49): loss=9.37749047727289e+88\n",
      "Gradient Descent(40/49): loss=1.969764341296129e+91\n",
      "Gradient Descent(41/49): loss=4.137537190408472e+93\n",
      "Gradient Descent(42/49): loss=8.690995995362829e+95\n",
      "Gradient Descent(43/49): loss=1.8255645306708647e+98\n",
      "Gradient Descent(44/49): loss=3.834642033458207e+100\n",
      "Gradient Descent(45/49): loss=8.054757461441705e+102\n",
      "Gradient Descent(46/49): loss=1.6919211023236133e+105\n",
      "Gradient Descent(47/49): loss=3.5539208103922305e+107\n",
      "Gradient Descent(48/49): loss=7.46509580688655e+109\n",
      "Gradient Descent(49/49): loss=1.5680612590758534e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.421705575435447\n",
      "Gradient Descent(2/49): loss=1143.6468321443765\n",
      "Gradient Descent(3/49): loss=232913.10127506763\n",
      "Gradient Descent(4/49): loss=48165227.4140588\n",
      "Gradient Descent(5/49): loss=10002606463.213663\n",
      "Gradient Descent(6/49): loss=2079981452799.351\n",
      "Gradient Descent(7/49): loss=432701316801222.5\n",
      "Gradient Descent(8/49): loss=9.002788395747562e+16\n",
      "Gradient Descent(9/49): loss=1.8732073485501075e+19\n",
      "Gradient Descent(10/49): loss=3.897636593965433e+21\n",
      "Gradient Descent(11/49): loss=8.109966751971308e+23\n",
      "Gradient Descent(12/49): loss=1.6874758028015518e+26\n",
      "Gradient Descent(13/49): loss=3.511205808318782e+28\n",
      "Gradient Descent(14/49): loss=7.30592320631436e+30\n",
      "Gradient Descent(15/49): loss=1.5201762669841437e+33\n",
      "Gradient Descent(16/49): loss=3.1630991294389535e+35\n",
      "Gradient Descent(17/49): loss=6.581602669094278e+37\n",
      "Gradient Descent(18/49): loss=1.3694636819204523e+40\n",
      "Gradient Descent(19/49): loss=2.8495047056855846e+42\n",
      "Gradient Descent(20/49): loss=5.9290926639228685e+44\n",
      "Gradient Descent(21/49): loss=1.2336929905932112e+47\n",
      "Gradient Descent(22/49): loss=2.567000519906733e+49\n",
      "Gradient Descent(23/49): loss=5.341273493093503e+51\n",
      "Gradient Descent(24/49): loss=1.1113828106727678e+54\n",
      "Gradient Descent(25/49): loss=2.312504224802372e+56\n",
      "Gradient Descent(26/49): loss=4.811731599927962e+58\n",
      "Gradient Descent(27/49): loss=1.0011986461009417e+61\n",
      "Gradient Descent(28/49): loss=2.083239075449288e+63\n",
      "Gradient Descent(29/49): loss=4.334689287066142e+65\n",
      "Gradient Descent(30/49): loss=9.019383054416719e+67\n",
      "Gradient Descent(31/49): loss=1.8767036180662237e+70\n",
      "Gradient Descent(32/49): loss=3.9049416670890476e+72\n",
      "Gradient Descent(33/49): loss=8.125187843502146e+74\n",
      "Gradient Descent(34/49): loss=1.6906443967806782e+77\n",
      "Gradient Descent(35/49): loss=3.517799872961346e+79\n",
      "Gradient Descent(36/49): loss=7.319644491633616e+81\n",
      "Gradient Descent(37/49): loss=1.523031366727533e+84\n",
      "Gradient Descent(38/49): loss=3.1690398989831556e+86\n",
      "Gradient Descent(39/49): loss=6.593963920077225e+88\n",
      "Gradient Descent(40/49): loss=1.3720357447450005e+91\n",
      "Gradient Descent(41/49): loss=2.854856513737086e+93\n",
      "Gradient Descent(42/49): loss=5.940228412592713e+95\n",
      "Gradient Descent(43/49): loss=1.2360100559864216e+98\n",
      "Gradient Descent(44/49): loss=2.571821742175657e+100\n",
      "Gradient Descent(45/49): loss=5.351305227244906e+102\n",
      "Gradient Descent(46/49): loss=1.1134701587410041e+105\n",
      "Gradient Descent(47/49): loss=2.3168474638570046e+107\n",
      "Gradient Descent(48/49): loss=4.8207687728694835e+109\n",
      "Gradient Descent(49/49): loss=1.0030790513409403e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.401067236622883\n",
      "Gradient Descent(2/49): loss=1140.4814539622998\n",
      "Gradient Descent(3/49): loss=232031.68594448254\n",
      "Gradient Descent(4/49): loss=47928359.993763216\n",
      "Gradient Descent(5/49): loss=9943013239.872253\n",
      "Gradient Descent(6/49): loss=2065567122246.2434\n",
      "Gradient Descent(7/49): loss=429296698982004.9\n",
      "Gradient Descent(8/49): loss=8.923643841832586e+16\n",
      "Gradient Descent(9/49): loss=1.855024025334067e+19\n",
      "Gradient Descent(10/49): loss=3.856244753672936e+21\n",
      "Gradient Descent(11/49): loss=8.016453830304006e+23\n",
      "Gradient Descent(12/49): loss=1.6664830891414802e+26\n",
      "Gradient Descent(13/49): loss=3.464334657833932e+28\n",
      "Gradient Descent(14/49): loss=7.201763799582188e+30\n",
      "Gradient Descent(15/49): loss=1.4971245951126617e+33\n",
      "Gradient Descent(16/49): loss=3.1122683010083654e+35\n",
      "Gradient Descent(17/49): loss=6.469878396081958e+37\n",
      "Gradient Descent(18/49): loss=1.3449780852754536e+40\n",
      "Gradient Descent(19/49): loss=2.7959815335859826e+42\n",
      "Gradient Descent(20/49): loss=5.812371833336978e+44\n",
      "Gradient Descent(21/49): loss=1.2082936145262217e+47\n",
      "Gradient Descent(22/49): loss=2.5118376814790187e+49\n",
      "Gradient Descent(23/49): loss=5.2216849136249353e+51\n",
      "Gradient Descent(24/49): loss=1.0854998130801412e+54\n",
      "Gradient Descent(25/49): loss=2.256570175509295e+56\n",
      "Gradient Descent(26/49): loss=4.691027023351017e+58\n",
      "Gradient Descent(27/49): loss=9.751850295924374e+60\n",
      "Gradient Descent(28/49): loss=2.0272444332711364e+63\n",
      "Gradient Descent(29/49): loss=4.214297664051024e+65\n",
      "Gradient Descent(30/49): loss=8.760810738825486e+67\n",
      "Gradient Descent(31/49): loss=1.8212241023274562e+70\n",
      "Gradient Descent(32/49): loss=3.786016305773014e+72\n",
      "Gradient Descent(33/49): loss=7.870486366428426e+74\n",
      "Gradient Descent(34/49): loss=1.6361407516835381e+77\n",
      "Gradient Descent(35/49): loss=3.401259381806595e+79\n",
      "Gradient Descent(36/49): loss=7.07064191783242e+81\n",
      "Gradient Descent(37/49): loss=1.4698666440327156e+84\n",
      "Gradient Descent(38/49): loss=3.055603686832346e+86\n",
      "Gradient Descent(39/49): loss=6.352082298682044e+88\n",
      "Gradient Descent(40/49): loss=1.3204902750676568e+91\n",
      "Gradient Descent(41/49): loss=2.7450755272960606e+93\n",
      "Gradient Descent(42/49): loss=5.706546873413094e+95\n",
      "Gradient Descent(43/49): loss=1.1862943986294412e+98\n",
      "Gradient Descent(44/49): loss=2.4661050394174377e+100\n",
      "Gradient Descent(45/49): loss=5.126614500132862e+102\n",
      "Gradient Descent(46/49): loss=1.0657362850684232e+105\n",
      "Gradient Descent(47/49): loss=2.2154851496674974e+107\n",
      "Gradient Descent(48/49): loss=4.605618216407174e+109\n",
      "Gradient Descent(49/49): loss=9.574299858649513e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.4844358821093975\n",
      "Gradient Descent(2/49): loss=1178.7505799103435\n",
      "Gradient Descent(3/49): loss=244598.64379999493\n",
      "Gradient Descent(4/49): loss=51498297.79004387\n",
      "Gradient Descent(5/49): loss=10885376681.237507\n",
      "Gradient Descent(6/49): loss=2303621617520.6353\n",
      "Gradient Descent(7/49): loss=487687784615460.9\n",
      "Gradient Descent(8/49): loss=1.032583296070278e+17\n",
      "Gradient Descent(9/49): loss=2.1863786061250793e+19\n",
      "Gradient Descent(10/49): loss=4.6294693107147646e+21\n",
      "Gradient Descent(11/49): loss=9.80254557001054e+23\n",
      "Gradient Descent(12/49): loss=2.0756165553425152e+26\n",
      "Gradient Descent(13/49): loss=4.394966581191998e+28\n",
      "Gradient Descent(14/49): loss=9.306022366960417e+30\n",
      "Gradient Descent(15/49): loss=1.9704827070475042e+33\n",
      "Gradient Descent(16/49): loss=4.17235420984772e+35\n",
      "Gradient Descent(17/49): loss=8.834657457292396e+37\n",
      "Gradient Descent(18/49): loss=1.8706746501863353e+40\n",
      "Gradient Descent(19/49): loss=3.961017918015477e+42\n",
      "Gradient Descent(20/49): loss=8.387168205930471e+44\n",
      "Gradient Descent(21/49): loss=1.775922047683872e+47\n",
      "Gradient Descent(22/49): loss=3.7603861542694206e+49\n",
      "Gradient Descent(23/49): loss=7.962344995802576e+51\n",
      "Gradient Descent(24/49): loss=1.6859688136106268e+54\n",
      "Gradient Descent(25/49): loss=3.569916704146845e+56\n",
      "Gradient Descent(26/49): loss=7.559039747156814e+58\n",
      "Gradient Descent(27/49): loss=1.6005718517949636e+61\n",
      "Gradient Descent(28/49): loss=3.389094830096633e+63\n",
      "Gradient Descent(29/49): loss=7.176162541223541e+65\n",
      "Gradient Descent(30/49): loss=1.5195003798873298e+68\n",
      "Gradient Descent(31/49): loss=3.217431867261005e+70\n",
      "Gradient Descent(32/49): loss=6.812678665624436e+72\n",
      "Gradient Descent(33/49): loss=1.4425353050464201e+75\n",
      "Gradient Descent(34/49): loss=3.054463902437166e+77\n",
      "Gradient Descent(35/49): loss=6.467605817794198e+79\n",
      "Gradient Descent(36/49): loss=1.369468631827299e+82\n",
      "Gradient Descent(37/49): loss=2.8997505203534038e+84\n",
      "Gradient Descent(38/49): loss=6.140011450331814e+86\n",
      "Gradient Descent(39/49): loss=1.3001029000802143e+89\n",
      "Gradient Descent(40/49): loss=2.7528736134614873e+91\n",
      "Gradient Descent(41/49): loss=5.8290102508231725e+93\n",
      "Gradient Descent(42/49): loss=1.2342506513213299e+96\n",
      "Gradient Descent(43/49): loss=2.6134362520155215e+98\n",
      "Gradient Descent(44/49): loss=5.533761749314853e+100\n",
      "Gradient Descent(45/49): loss=1.1717339221327334e+103\n",
      "Gradient Descent(46/49): loss=2.481061611383165e+105\n",
      "Gradient Descent(47/49): loss=5.2534680469734734e+107\n",
      "Gradient Descent(48/49): loss=1.1123837632224326e+110\n",
      "Gradient Descent(49/49): loss=2.355391953690051e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.456274521670801\n",
      "Gradient Descent(2/49): loss=1170.693822682774\n",
      "Gradient Descent(3/49): loss=242891.5770448588\n",
      "Gradient Descent(4/49): loss=51197423.78821509\n",
      "Gradient Descent(5/49): loss=10839688748.892643\n",
      "Gradient Descent(6/49): loss=2298183858362.6616\n",
      "Gradient Descent(7/49): loss=487467597958186.3\n",
      "Gradient Descent(8/49): loss=1.0341179393921704e+17\n",
      "Gradient Descent(9/49): loss=2.193892536497195e+19\n",
      "Gradient Descent(10/49): loss=4.654441672704608e+21\n",
      "Gradient Descent(11/49): loss=9.874659997904531e+23\n",
      "Gradient Descent(12/49): loss=2.0949684234507992e+26\n",
      "Gradient Descent(13/49): loss=4.444603960137572e+28\n",
      "Gradient Descent(14/49): loss=9.429501675241345e+30\n",
      "Gradient Descent(15/49): loss=2.0005271219667258e+33\n",
      "Gradient Descent(16/49): loss=4.2442421574298946e+35\n",
      "Gradient Descent(17/49): loss=9.00442259669907e+37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=1.9103440258503865e+40\n",
      "Gradient Descent(19/49): loss=4.052913177794334e+42\n",
      "Gradient Descent(20/49): loss=8.598506346971652e+44\n",
      "Gradient Descent(21/49): loss=1.8242263814247123e+47\n",
      "Gradient Descent(22/49): loss=3.8702092625164995e+49\n",
      "Gradient Descent(23/49): loss=8.2108886749681e+51\n",
      "Gradient Descent(24/49): loss=1.741990891446177e+54\n",
      "Gradient Descent(25/49): loss=3.695741576835582e+56\n",
      "Gradient Descent(26/49): loss=7.840744673134959e+58\n",
      "Gradient Descent(27/49): loss=1.6634625487515974e+61\n",
      "Gradient Descent(28/49): loss=3.529138833687968e+63\n",
      "Gradient Descent(29/49): loss=7.48728663401037e+65\n",
      "Gradient Descent(30/49): loss=1.5884742363974546e+68\n",
      "Gradient Descent(31/49): loss=3.3700464841786905e+70\n",
      "Gradient Descent(32/49): loss=7.149762360189399e+72\n",
      "Gradient Descent(33/49): loss=1.516866371047679e+75\n",
      "Gradient Descent(34/49): loss=3.2181259623773127e+77\n",
      "Gradient Descent(35/49): loss=6.827453563080785e+79\n",
      "Gradient Descent(36/49): loss=1.4484865633285971e+82\n",
      "Gradient Descent(37/49): loss=3.073053964788524e+84\n",
      "Gradient Descent(38/49): loss=6.519674334293468e+86\n",
      "Gradient Descent(39/49): loss=1.3831892935264591e+89\n",
      "Gradient Descent(40/49): loss=2.9345217623259717e+91\n",
      "Gradient Descent(41/49): loss=6.225769685947892e+93\n",
      "Gradient Descent(42/49): loss=1.3208356019055493e+96\n",
      "Gradient Descent(43/49): loss=2.8022345433030226e+98\n",
      "Gradient Descent(44/49): loss=5.945114156789812e+100\n",
      "Gradient Descent(45/49): loss=1.261292792986625e+103\n",
      "Gradient Descent(46/49): loss=2.6759107860411775e+105\n",
      "Gradient Descent(47/49): loss=5.677110481140648e+107\n",
      "Gradient Descent(48/49): loss=1.2044341531564488e+110\n",
      "Gradient Descent(49/49): loss=2.5552816597612327e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.483232024291069\n",
      "Gradient Descent(2/49): loss=1166.7118688966111\n",
      "Gradient Descent(3/49): loss=239987.29598505047\n",
      "Gradient Descent(4/49): loss=50124917.74736221\n",
      "Gradient Descent(5/49): loss=10513834874.524197\n",
      "Gradient Descent(6/49): loss=2208188960870.1816\n",
      "Gradient Descent(7/49): loss=463974541148973.9\n",
      "Gradient Descent(8/49): loss=9.750172119127115e+16\n",
      "Gradient Descent(9/49): loss=2.0490400880376697e+19\n",
      "Gradient Descent(10/49): loss=4.3062112466193267e+21\n",
      "Gradient Descent(11/49): loss=9.04987234797274e+23\n",
      "Gradient Descent(12/49): loss=1.901911610603457e+26\n",
      "Gradient Descent(13/49): loss=3.997039674280277e+28\n",
      "Gradient Descent(14/49): loss=8.400142884706656e+30\n",
      "Gradient Descent(15/49): loss=1.7653666430307363e+33\n",
      "Gradient Descent(16/49): loss=3.7100790972723786e+35\n",
      "Gradient Descent(17/49): loss=7.797069840593393e+37\n",
      "Gradient Descent(18/49): loss=1.638625394671094e+40\n",
      "Gradient Descent(19/49): loss=3.4437208351324707e+42\n",
      "Gradient Descent(20/49): loss=7.237293668350425e+44\n",
      "Gradient Descent(21/49): loss=1.5209833245633208e+47\n",
      "Gradient Descent(22/49): loss=3.196485288125826e+49\n",
      "Gradient Descent(23/49): loss=6.717705600254885e+51\n",
      "Gradient Descent(24/49): loss=1.4117871494511485e+54\n",
      "Gradient Descent(25/49): loss=2.9669995590193154e+56\n",
      "Gradient Descent(26/49): loss=6.235420393680934e+58\n",
      "Gradient Descent(27/49): loss=1.3104305111116532e+61\n",
      "Gradient Descent(28/49): loss=2.753989331966592e+63\n",
      "Gradient Descent(29/49): loss=5.787759958482641e+65\n",
      "Gradient Descent(30/49): loss=1.2163505845207666e+68\n",
      "Gradient Descent(31/49): loss=2.556271778852223e+70\n",
      "Gradient Descent(32/49): loss=5.3722384734421365e+72\n",
      "Gradient Descent(33/49): loss=1.1290249516618547e+75\n",
      "Gradient Descent(34/49): loss=2.3727489905308043e+77\n",
      "Gradient Descent(35/49): loss=4.9865485822771216e+79\n",
      "Gradient Descent(36/49): loss=1.0479687005513097e+82\n",
      "Gradient Descent(37/49): loss=2.202401880207272e+84\n",
      "Gradient Descent(38/49): loss=4.6285485810680985e+86\n",
      "Gradient Descent(39/49): loss=9.727317325615082e+88\n",
      "Gradient Descent(40/49): loss=2.0442845245318104e+91\n",
      "Gradient Descent(41/49): loss=4.296250525553791e+93\n",
      "Gradient Descent(42/49): loss=9.028962630604694e+95\n",
      "Gradient Descent(43/49): loss=1.8975189109659162e+98\n",
      "Gradient Descent(44/49): loss=3.987809192241765e+100\n",
      "Gradient Descent(45/49): loss=8.380745014885148e+102\n",
      "Gradient Descent(46/49): loss=1.7612900622519154e+105\n",
      "Gradient Descent(47/49): loss=3.701511832035942e+107\n",
      "Gradient Descent(48/49): loss=7.779064979895639e+109\n",
      "Gradient Descent(49/49): loss=1.6348415109118902e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.462055382610977\n",
      "Gradient Descent(2/49): loss=1163.3749082818165\n",
      "Gradient Descent(3/49): loss=239046.18551952895\n",
      "Gradient Descent(4/49): loss=49869256.399723835\n",
      "Gradient Descent(5/49): loss=10448796288.165747\n",
      "Gradient Descent(6/49): loss=2192281757690.786\n",
      "Gradient Descent(7/49): loss=460175816373210.1\n",
      "Gradient Descent(8/49): loss=9.660904928075018e+16\n",
      "Gradient Descent(9/49): loss=2.028310868036939e+19\n",
      "Gradient Descent(10/49): loss=4.258522960824267e+21\n",
      "Gradient Descent(11/49): loss=8.941000756787814e+23\n",
      "Gradient Descent(12/49): loss=1.877215726930084e+26\n",
      "Gradient Descent(13/49): loss=3.941327740791915e+28\n",
      "Gradient Descent(14/49): loss=8.275057571213061e+30\n",
      "Gradient Descent(15/49): loss=1.7373988702243114e+33\n",
      "Gradient Descent(16/49): loss=3.647775132182493e+35\n",
      "Gradient Descent(17/49): loss=7.658726949060265e+37\n",
      "Gradient Descent(18/49): loss=1.6079965611133185e+40\n",
      "Gradient Descent(19/49): loss=3.376087123635149e+42\n",
      "Gradient Descent(20/49): loss=7.088301397312415e+44\n",
      "Gradient Descent(21/49): loss=1.488232230572978e+47\n",
      "Gradient Descent(22/49): loss=3.1246345888484563e+49\n",
      "Gradient Descent(23/49): loss=6.560361423048959e+51\n",
      "Gradient Descent(24/49): loss=1.3773880041804782e+54\n",
      "Gradient Descent(25/49): loss=2.8919103563386172e+56\n",
      "Gradient Descent(26/49): loss=6.071742663448047e+58\n",
      "Gradient Descent(27/49): loss=1.2747995071956092e+61\n",
      "Gradient Descent(28/49): loss=2.6765195325709345e+63\n",
      "Gradient Descent(29/49): loss=5.619516455566608e+65\n",
      "Gradient Descent(30/49): loss=1.1798518490186768e+68\n",
      "Gradient Descent(31/49): loss=2.4771711171943355e+70\n",
      "Gradient Descent(32/49): loss=5.200972265260002e+72\n",
      "Gradient Descent(33/49): loss=1.091975936431914e+75\n",
      "Gradient Descent(34/49): loss=2.292670264194818e+77\n",
      "Gradient Descent(35/49): loss=4.813601440246448e+79\n",
      "Gradient Descent(36/49): loss=1.0106450625458876e+82\n",
      "Gradient Descent(37/49): loss=2.1219111202444892e+84\n",
      "Gradient Descent(38/49): loss=4.4550821738297376e+86\n",
      "Gradient Descent(39/49): loss=9.353717498444766e+88\n",
      "Gradient Descent(40/49): loss=1.963870196483956e+91\n",
      "Gradient Descent(41/49): loss=4.123265588552599e+93\n",
      "Gradient Descent(42/49): loss=8.657048283629207e+95\n",
      "Gradient Descent(43/49): loss=1.8176002339784962e+98\n",
      "Gradient Descent(44/49): loss=3.816162856346849e+100\n",
      "Gradient Descent(45/49): loss=8.012267314845443e+102\n",
      "Gradient Descent(46/49): loss=1.6822245260778743e+105\n",
      "Gradient Descent(47/49): loss=3.531933277980646e+107\n",
      "Gradient Descent(48/49): loss=7.4155099314784794e+109\n",
      "Gradient Descent(49/49): loss=1.5569316636495423e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.546273903838052\n",
      "Gradient Descent(2/49): loss=1202.3968503034916\n",
      "Gradient Descent(3/49): loss=251988.29338256642\n",
      "Gradient Descent(4/49): loss=53582460.04653777\n",
      "Gradient Descent(5/49): loss=11438755098.793964\n",
      "Gradient Descent(6/49): loss=2444853257278.348\n",
      "Gradient Descent(7/49): loss=522745387107355.06\n",
      "Gradient Descent(8/49): loss=1.1178415136918488e+17\n",
      "Gradient Descent(9/49): loss=2.3904923861866676e+19\n",
      "Gradient Descent(10/49): loss=5.112108710478497e+21\n",
      "Gradient Descent(11/49): loss=1.0932377598751734e+24\n",
      "Gradient Descent(12/49): loss=2.3379206380601106e+26\n",
      "Gradient Descent(13/49): loss=4.9997133068872075e+28\n",
      "Gradient Descent(14/49): loss=1.069203823178445e+31\n",
      "Gradient Descent(15/49): loss=2.2865248478693166e+33\n",
      "Gradient Descent(16/49): loss=4.889802907081365e+35\n",
      "Gradient Descent(17/49): loss=1.0456992241572462e+38\n",
      "Gradient Descent(18/49): loss=2.2362595999131234e+40\n",
      "Gradient Descent(19/49): loss=4.782309181708264e+42\n",
      "Gradient Descent(20/49): loss=1.0227113665389291e+45\n",
      "Gradient Descent(21/49): loss=2.187099368880419e+47\n",
      "Gradient Descent(22/49): loss=4.677178533410649e+49\n",
      "Gradient Descent(23/49): loss=1.0002288576735407e+52\n",
      "Gradient Descent(24/49): loss=2.139019839799614e+54\n",
      "Gradient Descent(25/49): loss=4.574358997901326e+56\n",
      "Gradient Descent(26/49): loss=9.782405872236234e+58\n",
      "Gradient Descent(27/49): loss=2.0919972545459063e+61\n",
      "Gradient Descent(28/49): loss=4.4737997688775494e+63\n",
      "Gradient Descent(29/49): loss=9.567356901887294e+65\n",
      "Gradient Descent(30/49): loss=2.0460083780427126e+68\n",
      "Gradient Descent(31/49): loss=4.375451157461502e+70\n",
      "Gradient Descent(32/49): loss=9.357035404540027e+72\n",
      "Gradient Descent(33/49): loss=2.0010304860221865e+75\n",
      "Gradient Descent(34/49): loss=4.279264567116426e+77\n",
      "Gradient Descent(35/49): loss=9.151337455023174e+79\n",
      "Gradient Descent(36/49): loss=1.9570413537703497e+82\n",
      "Gradient Descent(37/49): loss=4.185192469615454e+84\n",
      "Gradient Descent(38/49): loss=8.950161412777826e+86\n",
      "Gradient Descent(39/49): loss=1.9140192451444808e+89\n",
      "Gradient Descent(40/49): loss=4.093188381556156e+91\n",
      "Gradient Descent(41/49): loss=8.753407871633782e+93\n",
      "Gradient Descent(42/49): loss=1.8719429018326766e+96\n",
      "Gradient Descent(43/49): loss=4.003206841391853e+98\n",
      "Gradient Descent(44/49): loss=8.560979610690606e+100\n",
      "Gradient Descent(45/49): loss=1.8307915328496452e+103\n",
      "Gradient Descent(46/49): loss=3.9152033869679735e+105\n",
      "Gradient Descent(47/49): loss=8.372781546278027e+107\n",
      "Gradient Descent(48/49): loss=1.790544804263242e+110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=3.8291345335520845e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.517794148313064\n",
      "Gradient Descent(2/49): loss=1194.1797915431725\n",
      "Gradient Descent(3/49): loss=250229.57522060067\n",
      "Gradient Descent(4/49): loss=53269318.62345944\n",
      "Gradient Descent(5/49): loss=11390713251.282444\n",
      "Gradient Descent(6/49): loss=2439073417811.581\n",
      "Gradient Descent(7/49): loss=522507137963904.5\n",
      "Gradient Descent(8/49): loss=1.1194973451287179e+17\n",
      "Gradient Descent(9/49): loss=2.398694432753588e+19\n",
      "Gradient Descent(10/49): loss=5.13965268335166e+21\n",
      "Gradient Descent(11/49): loss=1.1012728762519699e+24\n",
      "Gradient Descent(12/49): loss=2.3597005219653864e+26\n",
      "Gradient Descent(13/49): loss=5.056140021957491e+28\n",
      "Gradient Descent(14/49): loss=1.083381418593541e+31\n",
      "Gradient Descent(15/49): loss=2.3213664388655058e+33\n",
      "Gradient Descent(16/49): loss=4.974002848603432e+35\n",
      "Gradient Descent(17/49): loss=1.0657819508975855e+38\n",
      "Gradient Descent(18/49): loss=2.283656049692857e+40\n",
      "Gradient Descent(19/49): loss=4.893200671266426e+42\n",
      "Gradient Descent(20/49): loss=1.0484684337213686e+45\n",
      "Gradient Descent(21/49): loss=2.2465582968322396e+47\n",
      "Gradient Descent(22/49): loss=4.813711141788823e+49\n",
      "Gradient Descent(23/49): loss=1.0314361745915507e+52\n",
      "Gradient Descent(24/49): loss=2.210063194328453e+54\n",
      "Gradient Descent(25/49): loss=4.735512912240993e+56\n",
      "Gradient Descent(26/49): loss=1.014680602779033e+59\n",
      "Gradient Descent(27/49): loss=2.1741609509600563e+61\n",
      "Gradient Descent(28/49): loss=4.658585004713188e+63\n",
      "Gradient Descent(29/49): loss=9.981972234647834e+65\n",
      "Gradient Descent(30/49): loss=2.1388419357481616e+68\n",
      "Gradient Descent(31/49): loss=4.58290678292629e+70\n",
      "Gradient Descent(32/49): loss=9.819816149081136e+72\n",
      "Gradient Descent(33/49): loss=2.1040966742112643e+75\n",
      "Gradient Descent(34/49): loss=4.50845794586611e+77\n",
      "Gradient Descent(35/49): loss=9.660294272012291e+79\n",
      "Gradient Descent(36/49): loss=2.0699158457814135e+82\n",
      "Gradient Descent(37/49): loss=4.43521852230746e+84\n",
      "Gradient Descent(38/49): loss=9.50336381100232e+86\n",
      "Gradient Descent(39/49): loss=2.0362902813023457e+89\n",
      "Gradient Descent(40/49): loss=4.363168865455708e+91\n",
      "Gradient Descent(41/49): loss=9.34898266877083e+93\n",
      "Gradient Descent(42/49): loss=2.0032109605697892e+96\n",
      "Gradient Descent(43/49): loss=4.292289647676202e+98\n",
      "Gradient Descent(44/49): loss=9.197109431903258e+100\n",
      "Gradient Descent(45/49): loss=1.9706690099117076e+103\n",
      "Gradient Descent(46/49): loss=4.2225618553097377e+105\n",
      "Gradient Descent(47/49): loss=9.047703359741556e+107\n",
      "Gradient Descent(48/49): loss=1.9386556998079627e+110\n",
      "Gradient Descent(49/49): loss=4.153966783571966e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.545073402704602\n",
      "Gradient Descent(2/49): loss=1190.1242175410773\n",
      "Gradient Descent(3/49): loss=247239.64871505444\n",
      "Gradient Descent(4/49): loss=52154023.8046528\n",
      "Gradient Descent(5/49): loss=11048460092.691736\n",
      "Gradient Descent(6/49): loss=2343603666574.593\n",
      "Gradient Descent(7/49): loss=497335862076482.1\n",
      "Gradient Descent(8/49): loss=1.0555425430028234e+17\n",
      "Gradient Descent(9/49): loss=2.240380608463965e+19\n",
      "Gradient Descent(10/49): loss=4.755263560941705e+21\n",
      "Gradient Descent(11/49): loss=1.0093217583726655e+24\n",
      "Gradient Descent(12/49): loss=2.1423253928241826e+26\n",
      "Gradient Descent(13/49): loss=4.547173126160023e+28\n",
      "Gradient Descent(14/49): loss=9.651562537647081e+30\n",
      "Gradient Descent(15/49): loss=2.0485840979994558e+33\n",
      "Gradient Descent(16/49): loss=4.348204638709763e+35\n",
      "Gradient Descent(17/49): loss=9.229244598502686e+37\n",
      "Gradient Descent(18/49): loss=1.9589454303509205e+40\n",
      "Gradient Descent(19/49): loss=4.157942897021072e+42\n",
      "Gradient Descent(20/49): loss=8.825406196576697e+44\n",
      "Gradient Descent(21/49): loss=1.873229057517182e+47\n",
      "Gradient Descent(22/49): loss=3.9760063433678015e+49\n",
      "Gradient Descent(23/49): loss=8.439238319110601e+51\n",
      "Gradient Descent(24/49): loss=1.791263324461275e+54\n",
      "Gradient Descent(25/49): loss=3.8020306765102623e+56\n",
      "Gradient Descent(26/49): loss=8.06996775277494e+58\n",
      "Gradient Descent(27/49): loss=1.7128841156697793e+61\n",
      "Gradient Descent(28/49): loss=3.635667556050745e+63\n",
      "Gradient Descent(29/49): loss=7.716855131762174e+65\n",
      "Gradient Descent(30/49): loss=1.6379344977649904e+68\n",
      "Gradient Descent(31/49): loss=3.476583884445712e+70\n",
      "Gradient Descent(32/49): loss=7.379193442766043e+72\n",
      "Gradient Descent(33/49): loss=1.5662644042441316e+75\n",
      "Gradient Descent(34/49): loss=3.3244611393229246e+77\n",
      "Gradient Descent(35/49): loss=7.056306608846123e+79\n",
      "Gradient Descent(36/49): loss=1.497730334973512e+82\n",
      "Gradient Descent(37/49): loss=3.1789947356988295e+84\n",
      "Gradient Descent(38/49): loss=6.747548135746084e+86\n",
      "Gradient Descent(39/49): loss=1.43219506886669e+89\n",
      "Gradient Descent(40/49): loss=3.0398934161279085e+91\n",
      "Gradient Descent(41/49): loss=6.452299817461492e+93\n",
      "Gradient Descent(42/49): loss=1.3695273891360069e+96\n",
      "Gradient Descent(43/49): loss=2.906878667537782e+98\n",
      "Gradient Descent(44/49): loss=6.1699704984484166e+100\n",
      "Gradient Descent(45/49): loss=1.3096018205661463e+103\n",
      "Gradient Descent(46/49): loss=2.7796841635814204e+105\n",
      "Gradient Descent(47/49): loss=5.89999488999267e+107\n",
      "Gradient Descent(48/49): loss=1.2522983782837244e+110\n",
      "Gradient Descent(49/49): loss=2.6580552313901856e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.523354096391955\n",
      "Gradient Descent(2/49): loss=1186.611483211344\n",
      "Gradient Descent(3/49): loss=246236.51847505485\n",
      "Gradient Descent(4/49): loss=51878581.61716972\n",
      "Gradient Descent(5/49): loss=10977618181.543072\n",
      "Gradient Descent(6/49): loss=2326086024647.7715\n",
      "Gradient Descent(7/49): loss=493106928317504.5\n",
      "Gradient Descent(8/49): loss=1.0454979462867946e+17\n",
      "Gradient Descent(9/49): loss=2.2168076803622105e+19\n",
      "Gradient Descent(10/49): loss=4.70046303088975e+21\n",
      "Gradient Descent(11/49): loss=9.966804455706087e+23\n",
      "Gradient Descent(12/49): loss=2.1133535204054525e+26\n",
      "Gradient Descent(13/49): loss=4.481141737243758e+28\n",
      "Gradient Descent(14/49): loss=9.501787600339032e+30\n",
      "Gradient Descent(15/49): loss=2.0147538424016376e+33\n",
      "Gradient Descent(16/49): loss=4.272073146292678e+35\n",
      "Gradient Descent(17/49): loss=9.058480876270224e+37\n",
      "Gradient Descent(18/49): loss=1.9207554098643036e+40\n",
      "Gradient Descent(19/49): loss=4.0727594387160713e+42\n",
      "Gradient Descent(20/49): loss=8.635857208713542e+44\n",
      "Gradient Descent(21/49): loss=1.8311425180394724e+47\n",
      "Gradient Descent(22/49): loss=3.8827447472741724e+49\n",
      "Gradient Descent(23/49): loss=8.232951080660215e+51\n",
      "Gradient Descent(24/49): loss=1.7457105194596272e+54\n",
      "Gradient Descent(25/49): loss=3.701595197033043e+56\n",
      "Gradient Descent(26/49): loss=7.848842548619476e+58\n",
      "Gradient Descent(27/49): loss=1.6642643529041657e+61\n",
      "Gradient Descent(28/49): loss=3.5288971834897394e+63\n",
      "Gradient Descent(29/49): loss=7.482654609474271e+65\n",
      "Gradient Descent(30/49): loss=1.5866180592237724e+68\n",
      "Gradient Descent(31/49): loss=3.3642563999514746e+70\n",
      "Gradient Descent(32/49): loss=7.133551177497415e+72\n",
      "Gradient Descent(33/49): loss=1.5125943552551114e+75\n",
      "Gradient Descent(34/49): loss=3.2072969361555573e+77\n",
      "Gradient Descent(35/49): loss=6.800735174592034e+79\n",
      "Gradient Descent(36/49): loss=1.4420242289873934e+82\n",
      "Gradient Descent(37/49): loss=3.057660419943397e+84\n",
      "Gradient Descent(38/49): loss=6.483446710360476e+86\n",
      "Gradient Descent(39/49): loss=1.374746553669355e+89\n",
      "Gradient Descent(40/49): loss=2.915005199018131e+91\n",
      "Gradient Descent(41/49): loss=6.18096134710984e+93\n",
      "Gradient Descent(42/49): loss=1.310607719922225e+96\n",
      "Gradient Descent(43/49): loss=2.7790055608791524e+98\n",
      "Gradient Descent(44/49): loss=5.892588445805565e+100\n",
      "Gradient Descent(45/49): loss=1.2494612850165095e+103\n",
      "Gradient Descent(46/49): loss=2.6493509891503976e+105\n",
      "Gradient Descent(47/49): loss=5.6176695891937443e+107\n",
      "Gradient Descent(48/49): loss=1.1911676385118102e+110\n",
      "Gradient Descent(49/49): loss=2.5257454546048642e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.6084267721984675\n",
      "Gradient Descent(2/49): loss=1226.3972910326445\n",
      "Gradient Descent(3/49): loss=259563.06141721224\n",
      "Gradient Descent(4/49): loss=55740056.45487493\n",
      "Gradient Descent(5/49): loss=12017324919.387087\n",
      "Gradient Descent(6/49): loss=2593981922833.8843\n",
      "Gradient Descent(7/49): loss=560131230234863.8\n",
      "Gradient Descent(8/49): loss=1.209665710560261e+17\n",
      "Gradient Descent(9/49): loss=2.612510878762532e+19\n",
      "Gradient Descent(10/49): loss=5.642303571491748e+21\n",
      "Gradient Descent(11/49): loss=1.2185872331665594e+24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=2.6318273734957693e+26\n",
      "Gradient Descent(13/49): loss=5.684056319353312e+28\n",
      "Gradient Descent(14/49): loss=1.2276071510684572e+31\n",
      "Gradient Descent(15/49): loss=2.65130974440347e+33\n",
      "Gradient Descent(16/49): loss=5.7261343474740905e+35\n",
      "Gradient Descent(17/49): loss=1.2366949884306683e+38\n",
      "Gradient Descent(18/49): loss=2.670937152751245e+40\n",
      "Gradient Descent(19/49): loss=5.768524450000439e+42\n",
      "Gradient Descent(20/49): loss=1.2458501429771417e+45\n",
      "Gradient Descent(21/49): loss=2.6907098900304397e+47\n",
      "Gradient Descent(22/49): loss=5.811228383495618e+49\n",
      "Gradient Descent(23/49): loss=1.255073073856433e+52\n",
      "Gradient Descent(24/49): loss=2.7106290043539156e+54\n",
      "Gradient Descent(25/49): loss=5.854248451584821e+56\n",
      "Gradient Descent(26/49): loss=1.2643642814210885e+59\n",
      "Gradient Descent(27/49): loss=2.730695578355274e+61\n",
      "Gradient Descent(28/49): loss=5.897586993890916e+63\n",
      "Gradient Descent(29/49): loss=1.2737242710687188e+66\n",
      "Gradient Descent(30/49): loss=2.7509107036320094e+68\n",
      "Gradient Descent(31/49): loss=5.941246368028822e+70\n",
      "Gradient Descent(32/49): loss=1.283153551985945e+73\n",
      "Gradient Descent(33/49): loss=2.771275479896329e+75\n",
      "Gradient Descent(34/49): loss=5.985228949090551e+77\n",
      "Gradient Descent(35/49): loss=1.2926526371305278e+80\n",
      "Gradient Descent(36/49): loss=2.7917910150026986e+82\n",
      "Gradient Descent(37/49): loss=6.029537129751572e+84\n",
      "Gradient Descent(38/49): loss=1.3022220432576963e+87\n",
      "Gradient Descent(39/49): loss=2.8124584250070263e+89\n",
      "Gradient Descent(40/49): loss=6.074173320400226e+91\n",
      "Gradient Descent(41/49): loss=1.3118622909481695e+94\n",
      "Gradient Descent(42/49): loss=2.833278834227252e+96\n",
      "Gradient Descent(43/49): loss=6.119139949268734e+98\n",
      "Gradient Descent(44/49): loss=1.3215739046364935e+101\n",
      "Gradient Descent(45/49): loss=2.8542533753046095e+103\n",
      "Gradient Descent(46/49): loss=6.164439462565339e+105\n",
      "Gradient Descent(47/49): loss=1.3313574126395538e+108\n",
      "Gradient Descent(48/49): loss=2.8753831892651307e+110\n",
      "Gradient Descent(49/49): loss=6.210074324607272e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.579626886290659\n",
      "Gradient Descent(2/49): loss=1218.017551083123\n",
      "Gradient Descent(3/49): loss=257751.39633657265\n",
      "Gradient Descent(4/49): loss=55414212.24920502\n",
      "Gradient Descent(5/49): loss=11966820590.493547\n",
      "Gradient Descent(6/49): loss=2587840392451.1235\n",
      "Gradient Descent(7/49): loss=559873571514293.8\n",
      "Gradient Descent(8/49): loss=1.2114516400069669e+17\n",
      "Gradient Descent(9/49): loss=2.6214602366049694e+19\n",
      "Gradient Descent(10/49): loss=5.67266941936218e+21\n",
      "Gradient Descent(11/49): loss=1.2275353583221487e+24\n",
      "Gradient Descent(12/49): loss=2.6563256984694068e+26\n",
      "Gradient Descent(13/49): loss=5.748160640807051e+28\n",
      "Gradient Descent(14/49): loss=1.2438744749858133e+31\n",
      "Gradient Descent(15/49): loss=2.691685060697398e+33\n",
      "Gradient Descent(16/49): loss=5.824678271204354e+35\n",
      "Gradient Descent(17/49): loss=1.2604326453049838e+38\n",
      "Gradient Descent(18/49): loss=2.7275162391739874e+40\n",
      "Gradient Descent(19/49): loss=5.902215297887018e+42\n",
      "Gradient Descent(20/49): loss=1.2772112932576353e+45\n",
      "Gradient Descent(21/49): loss=2.7638244378480215e+47\n",
      "Gradient Descent(22/49): loss=5.980784513732089e+49\n",
      "Gradient Descent(23/49): loss=1.2942132977126769e+52\n",
      "Gradient Descent(24/49): loss=2.800615966243502e+54\n",
      "Gradient Descent(25/49): loss=6.060399629836151e+56\n",
      "Gradient Descent(26/49): loss=1.3114416298424604e+59\n",
      "Gradient Descent(27/49): loss=2.8378972568357983e+61\n",
      "Gradient Descent(28/49): loss=6.141074567934804e+63\n",
      "Gradient Descent(29/49): loss=1.328899302400579e+66\n",
      "Gradient Descent(30/49): loss=2.875674829193024e+68\n",
      "Gradient Descent(31/49): loss=6.222823436144458e+70\n",
      "Gradient Descent(32/49): loss=1.3465893683221292e+73\n",
      "Gradient Descent(33/49): loss=2.913955289725646e+75\n",
      "Gradient Descent(34/49): loss=6.305660530425961e+77\n",
      "Gradient Descent(35/49): loss=1.3645149211851962e+80\n",
      "Gradient Descent(36/49): loss=2.9527453327895488e+82\n",
      "Gradient Descent(37/49): loss=6.3896003370468815e+84\n",
      "Gradient Descent(38/49): loss=1.3826790957491387e+87\n",
      "Gradient Descent(39/49): loss=2.992051741854725e+89\n",
      "Gradient Descent(40/49): loss=6.4746575351133435e+91\n",
      "Gradient Descent(41/49): loss=1.4010850685026564e+94\n",
      "Gradient Descent(42/49): loss=3.031881390691554e+96\n",
      "Gradient Descent(43/49): loss=6.560846999137266e+98\n",
      "Gradient Descent(44/49): loss=1.419736058219295e+101\n",
      "Gradient Descent(45/49): loss=3.0722412445727048e+103\n",
      "Gradient Descent(46/49): loss=6.648183801637119e+105\n",
      "Gradient Descent(47/49): loss=1.4386353265203098e+108\n",
      "Gradient Descent(48/49): loss=3.1131383614913645e+110\n",
      "Gradient Descent(49/49): loss=6.736683215774138e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.607229710676038\n",
      "Gradient Descent(2/49): loss=1213.8873480023262\n",
      "Gradient Descent(3/49): loss=254673.7310519826\n",
      "Gradient Descent(4/49): loss=54254642.59895988\n",
      "Gradient Descent(5/49): loss=11607429126.677244\n",
      "Gradient Descent(6/49): loss=2486592028938.289\n",
      "Gradient Descent(7/49): loss=532913322572027.25\n",
      "Gradient Descent(8/49): loss=1.1422709102382893e+17\n",
      "Gradient Descent(9/49): loss=2.448509592842168e+19\n",
      "Gradient Descent(10/49): loss=5.248573087155323e+21\n",
      "Gradient Descent(11/49): loss=1.1250788275742271e+24\n",
      "Gradient Descent(12/49): loss=2.4117118361674663e+26\n",
      "Gradient Descent(13/49): loss=5.1697332298018225e+28\n",
      "Gradient Descent(14/49): loss=1.108181605616426e+31\n",
      "Gradient Descent(15/49): loss=2.375493082526004e+33\n",
      "Gradient Descent(16/49): loss=5.092096351801046e+35\n",
      "Gradient Descent(17/49): loss=1.0915395056840334e+38\n",
      "Gradient Descent(18/49): loss=2.339819227992366e+40\n",
      "Gradient Descent(19/49): loss=5.0156260912352913e+42\n",
      "Gradient Descent(20/49): loss=1.0751473786216955e+45\n",
      "Gradient Descent(21/49): loss=2.3046811401826177e+47\n",
      "Gradient Descent(22/49): loss=4.940304244515434e+49\n",
      "Gradient Descent(23/49): loss=1.0590014211990826e+52\n",
      "Gradient Descent(24/49): loss=2.270070737750187e+54\n",
      "Gradient Descent(25/49): loss=4.866113539829909e+56\n",
      "Gradient Descent(26/49): loss=1.0430979347363135e+59\n",
      "Gradient Descent(27/49): loss=2.235980094885639e+61\n",
      "Gradient Descent(28/49): loss=4.793036989368477e+63\n",
      "Gradient Descent(29/49): loss=1.0274332778722676e+66\n",
      "Gradient Descent(30/49): loss=2.202401406083118e+68\n",
      "Gradient Descent(31/49): loss=4.721057861355292e+70\n",
      "Gradient Descent(32/49): loss=1.0120038639960493e+73\n",
      "Gradient Descent(33/49): loss=2.169326983103175e+75\n",
      "Gradient Descent(34/49): loss=4.650159675317102e+77\n",
      "Gradient Descent(35/49): loss=9.968061603609735e+79\n",
      "Gradient Descent(36/49): loss=2.136749253165862e+82\n",
      "Gradient Descent(37/49): loss=4.580326198276601e+84\n",
      "Gradient Descent(38/49): loss=9.818366872732238e+86\n",
      "Gradient Descent(39/49): loss=2.1046607572150044e+89\n",
      "Gradient Descent(40/49): loss=4.511541441034128e+91\n",
      "Gradient Descent(41/49): loss=9.670920172950777e+93\n",
      "Gradient Descent(42/49): loss=2.0730541482103404e+96\n",
      "Gradient Descent(43/49): loss=4.443789654506918e+98\n",
      "Gradient Descent(44/49): loss=9.525687744601568e+100\n",
      "Gradient Descent(45/49): loss=2.0419221894453213e+103\n",
      "Gradient Descent(46/49): loss=4.377055326122893e+105\n",
      "Gradient Descent(47/49): loss=9.382636335004074e+107\n",
      "Gradient Descent(48/49): loss=2.011257752890174e+110\n",
      "Gradient Descent(49/49): loss=4.311323176269157e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.584963377965823\n",
      "Gradient Descent(2/49): loss=1210.1945908729394\n",
      "Gradient Descent(3/49): loss=253606.19348696334\n",
      "Gradient Descent(4/49): loss=53958393.23532847\n",
      "Gradient Descent(5/49): loss=11530407080.969774\n",
      "Gradient Descent(6/49): loss=2467338710287.743\n",
      "Gradient Descent(7/49): loss=528215269675988.56\n",
      "Gradient Descent(8/49): loss=1.1309932894717211e+17\n",
      "Gradient Descent(9/49): loss=2.4217645594317165e+19\n",
      "Gradient Descent(10/49): loss=5.185750588317253e+21\n",
      "Gradient Descent(11/49): loss=1.1104372277328996e+24\n",
      "Gradient Descent(12/49): loss=2.3778109482435604e+26\n",
      "Gradient Descent(13/49): loss=5.091678201734436e+28\n",
      "Gradient Descent(14/49): loss=1.0902966623066537e+31\n",
      "Gradient Descent(15/49): loss=2.334685867987981e+33\n",
      "Gradient Descent(16/49): loss=4.999334999311742e+35\n",
      "Gradient Descent(17/49): loss=1.0705230640358328e+38\n",
      "Gradient Descent(18/49): loss=2.292344151367562e+40\n",
      "Gradient Descent(19/49): loss=4.908667446000043e+42\n",
      "Gradient Descent(20/49): loss=1.0511081458253395e+45\n",
      "Gradient Descent(21/49): loss=2.2507703903990647e+47\n",
      "Gradient Descent(22/49): loss=4.819644268446446e+49\n",
      "Gradient Descent(23/49): loss=1.0320453376250345e+52\n",
      "Gradient Descent(24/49): loss=2.2099506096184187e+54\n",
      "Gradient Descent(25/49): loss=4.732235609146198e+56\n",
      "Gradient Descent(26/49): loss=1.0133282510034533e+59\n",
      "Gradient Descent(27/49): loss=2.1698711329954815e+61\n",
      "Gradient Descent(28/49): loss=4.646412185927899e+63\n",
      "Gradient Descent(29/49): loss=9.949506158799573e+65\n",
      "Gradient Descent(30/49): loss=2.1305185343607723e+68\n",
      "Gradient Descent(31/49): loss=4.5621452490285465e+70\n",
      "Gradient Descent(32/49): loss=9.7690627598687e+72\n",
      "Gradient Descent(33/49): loss=2.0918796311137937e+75\n",
      "Gradient Descent(34/49): loss=4.479406570142218e+77\n",
      "Gradient Descent(35/49): loss=9.591891867100364e+79\n",
      "Gradient Descent(36/49): loss=2.0539414797354524e+82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=4.398168432911223e+84\n",
      "Gradient Descent(38/49): loss=9.417934130600539e+86\n",
      "Gradient Descent(39/49): loss=2.0166913714493687e+89\n",
      "Gradient Descent(40/49): loss=4.318403623639508e+91\n",
      "Gradient Descent(41/49): loss=9.247131276839978e+93\n",
      "Gradient Descent(42/49): loss=1.9801168279644415e+96\n",
      "Gradient Descent(43/49): loss=4.240085422176304e+98\n",
      "Gradient Descent(44/49): loss=9.079426089133123e+100\n",
      "Gradient Descent(45/49): loss=1.944205597294787e+103\n",
      "Gradient Descent(46/49): loss=4.163187592965194e+105\n",
      "Gradient Descent(47/49): loss=8.914762388471487e+107\n",
      "Gradient Descent(48/49): loss=1.908945649655491e+110\n",
      "Gradient Descent(49/49): loss=4.087684376255628e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.67089448719065\n",
      "Gradient Descent(2/49): loss=1250.755421802378\n",
      "Gradient Descent(3/49): loss=267326.63949624856\n",
      "Gradient Descent(4/49): loss=57973293.74257698\n",
      "Gradient Descent(5/49): loss=12622100404.283888\n",
      "Gradient Descent(6/49): loss=2751407018486.8477\n",
      "Gradient Descent(7/49): loss=599987321907829.8\n",
      "Gradient Descent(8/49): loss=1.308525447187758e+17\n",
      "Gradient Descent(9/49): loss=2.8539047577291186e+19\n",
      "Gradient Descent(10/49): loss=6.224470317679152e+21\n",
      "Gradient Descent(11/49): loss=1.357585422554407e+24\n",
      "Gradient Descent(12/49): loss=2.960959962219763e+26\n",
      "Gradient Descent(13/49): loss=6.458000907083053e+28\n",
      "Gradient Descent(14/49): loss=1.4085223196219243e+31\n",
      "Gradient Descent(15/49): loss=3.072057824934923e+33\n",
      "Gradient Descent(16/49): loss=6.700312305870358e+35\n",
      "Gradient Descent(17/49): loss=1.461371751148308e+38\n",
      "Gradient Descent(18/49): loss=3.187325159261451e+40\n",
      "Gradient Descent(19/49): loss=6.951716199953421e+42\n",
      "Gradient Descent(20/49): loss=1.516204206315225e+45\n",
      "Gradient Descent(21/49): loss=3.3069174996000684e+47\n",
      "Gradient Descent(22/49): loss=7.212553100585335e+49\n",
      "Gradient Descent(23/49): loss=1.5730940440877375e+52\n",
      "Gradient Descent(24/49): loss=3.430997092206912e+54\n",
      "Gradient Descent(25/49): loss=7.4831769219285385e+56\n",
      "Gradient Descent(26/49): loss=1.6321184582781822e+59\n",
      "Gradient Descent(27/49): loss=3.5597323030631206e+61\n",
      "Gradient Descent(28/49): loss=7.76395488035809e+63\n",
      "Gradient Descent(29/49): loss=1.6933575407444914e+66\n",
      "Gradient Descent(30/49): loss=3.6932978166199593e+68\n",
      "Gradient Descent(31/49): loss=8.055267971495651e+70\n",
      "Gradient Descent(32/49): loss=1.756894388549131e+73\n",
      "Gradient Descent(33/49): loss=3.8318748717459605e+75\n",
      "Gradient Descent(34/49): loss=8.357511486415408e+77\n",
      "Gradient Descent(35/49): loss=1.8228152166602343e+80\n",
      "Gradient Descent(36/49): loss=3.9756515076154866e+82\n",
      "Gradient Descent(37/49): loss=8.67109554799785e+84\n",
      "Gradient Descent(38/49): loss=1.891209474937198e+87\n",
      "Gradient Descent(39/49): loss=4.1248228188629333e+89\n",
      "Gradient Descent(40/49): loss=8.996445667435876e+91\n",
      "Gradient Descent(41/49): loss=1.9621699695076096e+94\n",
      "Gradient Descent(42/49): loss=4.279591220312274e+96\n",
      "Gradient Descent(43/49): loss=9.334003321623523e+98\n",
      "Gradient Descent(44/49): loss=2.0357929886985998e+101\n",
      "Gradient Descent(45/49): loss=4.44016672163934e+103\n",
      "Gradient Descent(46/49): loss=9.684226552207848e+105\n",
      "Gradient Descent(47/49): loss=2.1121784336931657e+108\n",
      "Gradient Descent(48/49): loss=4.606767212340195e+110\n",
      "Gradient Descent(49/49): loss=1.0047590587119662e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.6417727356035865\n",
      "Gradient Descent(2/49): loss=1242.2105975674958\n",
      "Gradient Descent(3/49): loss=265460.7061261279\n",
      "Gradient Descent(4/49): loss=57634298.21855951\n",
      "Gradient Descent(5/49): loss=12569020600.554218\n",
      "Gradient Descent(6/49): loss=2744883170484.332\n",
      "Gradient Descent(7/49): loss=599708815408846.9\n",
      "Gradient Descent(8/49): loss=1.3104509916234706e+17\n",
      "Gradient Descent(9/49): loss=2.863665388874929e+19\n",
      "Gradient Descent(10/49): loss=6.257931257973229e+21\n",
      "Gradient Descent(11/49): loss=1.367545071828427e+24\n",
      "Gradient Descent(12/49): loss=2.988500210364962e+26\n",
      "Gradient Descent(13/49): loss=6.5307820937816166e+28\n",
      "Gradient Descent(14/49): loss=1.4271748467396514e+31\n",
      "Gradient Descent(15/49): loss=3.1188120350800137e+33\n",
      "Gradient Descent(16/49): loss=6.815555045158765e+35\n",
      "Gradient Descent(17/49): loss=1.4894065559795019e+38\n",
      "Gradient Descent(18/49): loss=3.254807392506059e+40\n",
      "Gradient Descent(19/49): loss=7.112746434851174e+42\n",
      "Gradient Descent(20/49): loss=1.5543519403622456e+45\n",
      "Gradient Descent(21/49): loss=3.396732861012305e+47\n",
      "Gradient Descent(22/49): loss=7.422896854844253e+49\n",
      "Gradient Descent(23/49): loss=1.6221292628176163e+52\n",
      "Gradient Descent(24/49): loss=3.544846973834761e+54\n",
      "Gradient Descent(25/49): loss=7.746571346658892e+56\n",
      "Gradient Descent(26/49): loss=1.692862007072253e+59\n",
      "Gradient Descent(27/49): loss=3.699419584155877e+61\n",
      "Gradient Descent(28/49): loss=8.084359624388773e+63\n",
      "Gradient Descent(29/49): loss=1.7666790438252314e+66\n",
      "Gradient Descent(30/49): loss=3.8607323138809977e+68\n",
      "Gradient Descent(31/49): loss=8.436877117856063e+70\n",
      "Gradient Descent(32/49): loss=1.8437148632625147e+73\n",
      "Gradient Descent(33/49): loss=4.029079065073467e+75\n",
      "Gradient Descent(34/49): loss=8.804766092674225e+77\n",
      "Gradient Descent(35/49): loss=1.9241098200015716e+80\n",
      "Gradient Descent(36/49): loss=4.2047665553622966e+82\n",
      "Gradient Descent(37/49): loss=9.188696820371173e+84\n",
      "Gradient Descent(38/49): loss=2.0080103888056207e+87\n",
      "Gradient Descent(39/49): loss=4.3881148767605614e+89\n",
      "Gradient Descent(40/49): loss=9.589368799581077e+91\n",
      "Gradient Descent(41/49): loss=2.0955694314517106e+94\n",
      "Gradient Descent(42/49): loss=4.579458078853603e+96\n",
      "Gradient Descent(43/49): loss=1.000751203048881e+99\n",
      "Gradient Descent(44/49): loss=2.1869464752355525e+101\n",
      "Gradient Descent(45/49): loss=4.77914477741737e+103\n",
      "Gradient Descent(46/49): loss=1.044388834484643e+106\n",
      "Gradient Descent(47/49): loss=2.2823080036207433e+108\n",
      "Gradient Descent(48/49): loss=4.987538789575132e+110\n",
      "Gradient Descent(49/49): loss=1.0899292793984417e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.669700948205387\n",
      "Gradient Descent(2/49): loss=1238.0047474577827\n",
      "Gradient Descent(3/49): loss=262293.1680179414\n",
      "Gradient Descent(4/49): loss=56428923.67167228\n",
      "Gradient Descent(5/49): loss=12191722357.19686\n",
      "Gradient Descent(6/49): loss=2637537192389.164\n",
      "Gradient Descent(7/49): loss=570842126612615.1\n",
      "Gradient Descent(8/49): loss=1.2356460288831469e+17\n",
      "Gradient Descent(9/49): loss=2.674806371944258e+19\n",
      "Gradient Descent(10/49): loss=5.790250975405039e+21\n",
      "Gradient Descent(11/49): loss=1.2534433175465664e+24\n",
      "Gradient Descent(12/49): loss=2.7133934575283372e+26\n",
      "Gradient Descent(13/49): loss=5.873826364182294e+28\n",
      "Gradient Descent(14/49): loss=1.271538519897378e+31\n",
      "Gradient Descent(15/49): loss=2.752567432721674e+33\n",
      "Gradient Descent(16/49): loss=5.958630055830918e+35\n",
      "Gradient Descent(17/49): loss=1.2898965521517721e+38\n",
      "Gradient Descent(18/49): loss=2.7923081377557755e+40\n",
      "Gradient Descent(19/49): loss=6.044658953329886e+42\n",
      "Gradient Descent(20/49): loss=1.3085196930280788e+45\n",
      "Gradient Descent(21/49): loss=2.8326226515465473e+47\n",
      "Gradient Descent(22/49): loss=6.131929942717982e+49\n",
      "Gradient Descent(23/49): loss=1.3274117116264263e+52\n",
      "Gradient Descent(24/49): loss=2.87351921602099e+54\n",
      "Gradient Descent(25/49): loss=6.220460925973744e+56\n",
      "Gradient Descent(26/49): loss=1.346576487668749e+59\n",
      "Gradient Descent(27/49): loss=2.9150062330127297e+61\n",
      "Gradient Descent(28/49): loss=6.310270093319582e+63\n",
      "Gradient Descent(29/49): loss=1.3660179590590425e+66\n",
      "Gradient Descent(30/49): loss=2.9570922272365123e+68\n",
      "Gradient Descent(31/49): loss=6.401375898751762e+70\n",
      "Gradient Descent(32/49): loss=1.3857401206392109e+73\n",
      "Gradient Descent(33/49): loss=2.999785846545293e+75\n",
      "Gradient Descent(34/49): loss=6.493797062744033e+77\n",
      "Gradient Descent(35/49): loss=1.4057470249307147e+80\n",
      "Gradient Descent(36/49): loss=3.0430958636494896e+82\n",
      "Gradient Descent(37/49): loss=6.587552576052622e+84\n",
      "Gradient Descent(38/49): loss=1.4260427829642612e+87\n",
      "Gradient Descent(39/49): loss=3.087031177917401e+89\n",
      "Gradient Descent(40/49): loss=6.682661703616572e+91\n",
      "Gradient Descent(41/49): loss=1.4466315651243672e+94\n",
      "Gradient Descent(42/49): loss=3.1316008172037344e+96\n",
      "Gradient Descent(43/49): loss=6.779143988516535e+98\n",
      "Gradient Descent(44/49): loss=1.4675176020063595e+101\n",
      "Gradient Descent(45/49): loss=3.176813939704751e+103\n",
      "Gradient Descent(46/49): loss=6.877019255990262e+105\n",
      "Gradient Descent(47/49): loss=1.488705185285622e+108\n",
      "Gradient Descent(48/49): loss=3.2226798358399685e+110\n",
      "Gradient Descent(49/49): loss=6.9763076175064825e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.646883227332571\n",
      "Gradient Descent(2/49): loss=1234.1276602756238\n",
      "Gradient Descent(3/49): loss=261158.7714834546\n",
      "Gradient Descent(4/49): loss=56110800.1458533\n",
      "Gradient Descent(5/49): loss=12108123702.849752\n",
      "Gradient Descent(6/49): loss=2616414861486.2036\n",
      "Gradient Descent(7/49): loss=565633046333445.5\n",
      "Gradient Descent(8/49): loss=1.2230099443431738e+17\n",
      "Gradient Descent(9/49): loss=2.6445274371616596e+19\n",
      "Gradient Descent(10/49): loss=5.71839307471456e+21\n",
      "Gradient Descent(11/49): loss=1.2365242101839407e+24\n",
      "Gradient Descent(12/49): loss=2.673820002527826e+26\n",
      "Gradient Descent(13/49): loss=5.781786220715897e+28\n",
      "Gradient Descent(14/49): loss=1.2502360026257576e+31\n",
      "Gradient Descent(15/49): loss=2.7034728332913634e+33\n",
      "Gradient Descent(16/49): loss=5.845908742461581e+35\n",
      "Gradient Descent(17/49): loss=1.2641018231863441e+38\n",
      "Gradient Descent(18/49): loss=2.7334559867498335e+40\n",
      "Gradient Descent(19/49): loss=5.9107435045925e+42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/49): loss=1.2781215048267547e+45\n",
      "Gradient Descent(21/49): loss=2.7637717320421215e+47\n",
      "Gradient Descent(22/49): loss=5.976297369510604e+49\n",
      "Gradient Descent(23/49): loss=1.2922966768689517e+52\n",
      "Gradient Descent(24/49): loss=2.7944237004943715e+54\n",
      "Gradient Descent(25/49): loss=6.042578269890725e+56\n",
      "Gradient Descent(26/49): loss=1.3066290606295998e+59\n",
      "Gradient Descent(27/49): loss=2.825415618676099e+61\n",
      "Gradient Descent(28/49): loss=6.109594267261176e+63\n",
      "Gradient Descent(29/49): loss=1.3211203995553058e+66\n",
      "Gradient Descent(30/49): loss=2.856751256746881e+68\n",
      "Gradient Descent(31/49): loss=6.1773535142383e+70\n",
      "Gradient Descent(32/49): loss=1.3357724565535597e+73\n",
      "Gradient Descent(33/49): loss=2.8884344267727046e+75\n",
      "Gradient Descent(34/49): loss=6.24586425392522e+77\n",
      "Gradient Descent(35/49): loss=1.3505870140887509e+80\n",
      "Gradient Descent(36/49): loss=2.9204689831015973e+82\n",
      "Gradient Descent(37/49): loss=6.31513482084912e+84\n",
      "Gradient Descent(38/49): loss=1.3655658743941403e+87\n",
      "Gradient Descent(39/49): loss=2.9528588228288995e+89\n",
      "Gradient Descent(40/49): loss=6.3851736419724034e+91\n",
      "Gradient Descent(41/49): loss=1.380710859690898e+94\n",
      "Gradient Descent(42/49): loss=2.985607886271182e+96\n",
      "Gradient Descent(43/49): loss=6.45598923771791e+98\n",
      "Gradient Descent(44/49): loss=1.3960238124097645e+101\n",
      "Gradient Descent(45/49): loss=3.018720157445605e+103\n",
      "Gradient Descent(46/49): loss=6.527590223005189e+105\n",
      "Gradient Descent(47/49): loss=1.4115065954152018e+108\n",
      "Gradient Descent(48/49): loss=3.0521996645545655e+110\n",
      "Gradient Descent(49/49): loss=6.5999853082986755e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.733677048814593\n",
      "Gradient Descent(2/49): loss=1275.4747797253633\n",
      "Gradient Descent(3/49): loss=275282.7741532717\n",
      "Gradient Descent(4/49): loss=60284433.624147706\n",
      "Gradient Descent(5/49): loss=13254131368.560247\n",
      "Gradient Descent(6/49): loss=2917546037999.8022\n",
      "Gradient Descent(7/49): loss=642463569921323.5\n",
      "Gradient Descent(8/49): loss=1.4149213083919637e+17\n",
      "Gradient Descent(9/49): loss=3.116257372799256e+19\n",
      "Gradient Descent(10/49): loss=6.863410764884361e+21\n",
      "Gradient Descent(11/49): loss=1.5116404747490675e+24\n",
      "Gradient Descent(12/49): loss=3.329336084584234e+26\n",
      "Gradient Descent(13/49): loss=7.33275141828049e+28\n",
      "Gradient Descent(14/49): loss=1.6150142270234828e+31\n",
      "Gradient Descent(15/49): loss=3.55701554998011e+33\n",
      "Gradient Descent(16/49): loss=7.834209516820327e+35\n",
      "Gradient Descent(17/49): loss=1.7254588352581645e+38\n",
      "Gradient Descent(18/49): loss=3.8002662437781673e+40\n",
      "Gradient Descent(19/49): loss=8.369961216533502e+42\n",
      "Gradient Descent(20/49): loss=1.8434563866170863e+45\n",
      "Gradient Descent(21/49): loss=4.0601519668395554e+47\n",
      "Gradient Descent(22/49): loss=8.942350963026719e+49\n",
      "Gradient Descent(23/49): loss=1.969523342957993e+52\n",
      "Gradient Descent(24/49): loss=4.3378102855760516e+54\n",
      "Gradient Descent(25/49): loss=9.55388426389629e+56\n",
      "Gradient Descent(26/49): loss=2.104211538053133e+59\n",
      "Gradient Descent(27/49): loss=4.6344565985673246e+61\n",
      "Gradient Descent(28/49): loss=1.0207237996554742e+64\n",
      "Gradient Descent(29/49): loss=2.248110545484867e+66\n",
      "Gradient Descent(30/49): loss=4.951389422315991e+68\n",
      "Gradient Descent(31/49): loss=1.09052721009033e+71\n",
      "Gradient Descent(32/49): loss=2.4018502576012963e+73\n",
      "Gradient Descent(33/49): loss=5.2899960739737645e+75\n",
      "Gradient Descent(34/49): loss=1.1651042097272624e+78\n",
      "Gradient Descent(35/49): loss=2.5661036426903785e+80\n",
      "Gradient Descent(36/49): loss=5.6517587440271095e+82\n",
      "Gradient Descent(37/49): loss=1.2447812461385049e+85\n",
      "Gradient Descent(38/49): loss=2.741589690776602e+87\n",
      "Gradient Descent(39/49): loss=6.038260984320957e+89\n",
      "Gradient Descent(40/49): loss=1.3299070914015874e+92\n",
      "Gradient Descent(41/49): loss=2.929076560871969e+94\n",
      "Gradient Descent(42/49): loss=6.451194639775695e+96\n",
      "Gradient Descent(43/49): loss=1.4208543687871736e+99\n",
      "Gradient Descent(44/49): loss=3.129384943455671e+101\n",
      "Gradient Descent(45/49): loss=6.892367254137587e+103\n",
      "Gradient Descent(46/49): loss=1.5180211838512345e+106\n",
      "Gradient Descent(47/49): loss=3.34339165290089e+108\n",
      "Gradient Descent(48/49): loss=7.363709982180882e+110\n",
      "Gradient Descent(49/49): loss=1.6218328670714516e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.704231696251846\n",
      "Gradient Descent(2/49): loss=1266.7624445543008\n",
      "Gradient Descent(3/49): loss=273361.2248787876\n",
      "Gradient Descent(4/49): loss=59931824.74110616\n",
      "Gradient Descent(5/49): loss=13198358512.163567\n",
      "Gradient Descent(6/49): loss=2910618182465.9976\n",
      "Gradient Descent(7/49): loss=642162680768998.6\n",
      "Gradient Descent(8/49): loss=1.416996630012646e+17\n",
      "Gradient Descent(9/49): loss=3.1268983641817924e+19\n",
      "Gradient Descent(10/49): loss=6.900264946282013e+21\n",
      "Gradient Descent(11/49): loss=1.5227202352095292e+24\n",
      "Gradient Descent(12/49): loss=3.360278367934513e+26\n",
      "Gradient Descent(13/49): loss=7.415332939282977e+28\n",
      "Gradient Descent(14/49): loss=1.6363874550037119e+31\n",
      "Gradient Descent(15/49): loss=3.611117807798984e+33\n",
      "Gradient Descent(16/49): loss=7.96887807613456e+35\n",
      "Gradient Descent(17/49): loss=1.7585418601011674e+38\n",
      "Gradient Descent(18/49): loss=3.8806836444371935e+40\n",
      "Gradient Descent(19/49): loss=8.563745852430724e+42\n",
      "Gradient Descent(20/49): loss=1.8898150368576905e+45\n",
      "Gradient Descent(21/49): loss=4.170372329360757e+47\n",
      "Gradient Descent(22/49): loss=9.203019886515721e+49\n",
      "Gradient Descent(23/49): loss=2.0308876125087787e+52\n",
      "Gradient Descent(24/49): loss=4.481685952563369e+54\n",
      "Gradient Descent(25/49): loss=9.890015012999797e+56\n",
      "Gradient Descent(26/49): loss=2.1824910980532532e+59\n",
      "Gradient Descent(27/49): loss=4.81623878914369e+61\n",
      "Gradient Descent(28/49): loss=1.0628293556268692e+64\n",
      "Gradient Descent(29/49): loss=2.3454116139932633e+66\n",
      "Gradient Descent(30/49): loss=5.175765620258021e+68\n",
      "Gradient Descent(31/49): loss=1.1421683765876425e+71\n",
      "Gradient Descent(32/49): loss=2.5204939639670346e+73\n",
      "Gradient Descent(33/49): loss=5.56213072662215e+75\n",
      "Gradient Descent(34/49): loss=1.2274299665983663e+78\n",
      "Gradient Descent(35/49): loss=2.708646015262923e+80\n",
      "Gradient Descent(36/49): loss=5.9773375554227945e+82\n",
      "Gradient Descent(37/49): loss=1.3190562388049686e+85\n",
      "Gradient Descent(38/49): loss=2.9108434064457523e+87\n",
      "Gradient Descent(39/49): loss=6.423539108935254e+89\n",
      "Gradient Descent(40/49): loss=1.41752230960452e+92\n",
      "Gradient Descent(41/49): loss=3.128134606406389e+94\n",
      "Gradient Descent(42/49): loss=6.903049108643142e+96\n",
      "Gradient Descent(43/49): loss=1.52333876230096e+99\n",
      "Gradient Descent(44/49): loss=3.361646351064052e+101\n",
      "Gradient Descent(45/49): loss=7.418353992747434e+103\n",
      "Gradient Descent(46/49): loss=1.6370542946700112e+106\n",
      "Gradient Descent(47/49): loss=3.612589485912351e+108\n",
      "Gradient Descent(48/49): loss=7.972125809275708e+110\n",
      "Gradient Descent(49/49): loss=1.7592585641617517e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.732487115292641\n",
      "Gradient Descent(2/49): loss=1262.4799203378384\n",
      "Gradient Descent(3/49): loss=270101.63860060676\n",
      "Gradient Descent(4/49): loss=58679070.13995358\n",
      "Gradient Descent(5/49): loss=12802354539.6919\n",
      "Gradient Descent(6/49): loss=2796839658675.3315\n",
      "Gradient Descent(7/49): loss=611265001414617.2\n",
      "Gradient Descent(8/49): loss=1.3361409381417698e+17\n",
      "Gradient Descent(9/49): loss=2.9207559803739816e+19\n",
      "Gradient Descent(10/49): loss=6.384767218085665e+21\n",
      "Gradient Descent(11/49): loss=1.3957162700522216e+24\n",
      "Gradient Descent(12/49): loss=3.0510546074802055e+26\n",
      "Gradient Descent(13/49): loss=6.669650516647923e+28\n",
      "Gradient Descent(14/49): loss=1.457995762637636e+31\n",
      "Gradient Descent(15/49): loss=3.1872011603678375e+33\n",
      "Gradient Descent(16/49): loss=6.967270911476535e+35\n",
      "Gradient Descent(17/49): loss=1.5230561825326565e+38\n",
      "Gradient Descent(18/49): loss=3.329424347859293e+40\n",
      "Gradient Descent(19/49): loss=7.278173073836891e+42\n",
      "Gradient Descent(20/49): loss=1.5910198813229312e+45\n",
      "Gradient Descent(21/49): loss=3.4779940478616784e+47\n",
      "Gradient Descent(22/49): loss=7.602948737069883e+49\n",
      "Gradient Descent(23/49): loss=1.6620163434347507e+52\n",
      "Gradient Descent(24/49): loss=3.6331934113750444e+54\n",
      "Gradient Descent(25/49): loss=7.942216944266647e+56\n",
      "Gradient Descent(26/49): loss=1.7361808978385607e+59\n",
      "Gradient Descent(27/49): loss=3.795318273439915e+61\n",
      "Gradient Descent(28/49): loss=8.296624398206576e+63\n",
      "Gradient Descent(29/49): loss=1.813654914967896e+66\n",
      "Gradient Descent(30/49): loss=3.964677672160577e+68\n",
      "Gradient Descent(31/49): loss=8.666846661073296e+70\n",
      "Gradient Descent(32/49): loss=1.8945860737683328e+73\n",
      "Gradient Descent(33/49): loss=4.141594436000323e+75\n",
      "Gradient Descent(34/49): loss=9.053589440880833e+77\n",
      "Gradient Descent(35/49): loss=1.9791286431026753e+80\n",
      "Gradient Descent(36/49): loss=4.326405799077583e+82\n",
      "Gradient Descent(37/49): loss=9.457589936623994e+84\n",
      "Gradient Descent(38/49): loss=2.0674437758104275e+87\n",
      "Gradient Descent(39/49): loss=4.519464043989899e+89\n",
      "Gradient Descent(40/49): loss=9.879618243505014e+91\n",
      "Gradient Descent(41/49): loss=2.159699815892933e+94\n",
      "Gradient Descent(42/49): loss=4.721137173325807e+96\n",
      "Gradient Descent(43/49): loss=1.0320478820869535e+99\n",
      "Gradient Descent(44/49): loss=2.2560726194063096e+101\n",
      "Gradient Descent(45/49): loss=4.931809611141689e+103\n",
      "Gradient Descent(46/49): loss=1.078101202564574e+106\n",
      "Gradient Descent(47/49): loss=2.35674588967378e+108\n",
      "Gradient Descent(48/49): loss=5.151882935740981e+110\n",
      "Gradient Descent(49/49): loss=1.126209571421086e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.709113644492213\n",
      "Gradient Descent(2/49): loss=1258.414137315358\n",
      "Gradient Descent(3/49): loss=268897.86616124876\n",
      "Gradient Descent(4/49): loss=58337963.56107723\n",
      "Gradient Descent(5/49): loss=12711762292.930035\n",
      "Gradient Descent(6/49): loss=2773706436551.5913\n",
      "Gradient Descent(7/49): loss=605499786189989.5\n",
      "Gradient Descent(8/49): loss=1.3220098419664803e+17\n",
      "Gradient Descent(9/49): loss=2.886544902758343e+19\n",
      "Gradient Descent(10/49): loss=6.302746062023756e+21\n",
      "Gradient Descent(11/49): loss=1.3762077604203334e+24\n",
      "Gradient Descent(12/49): loss=3.0049629643616478e+26\n",
      "Gradient Descent(13/49): loss=6.561370540453292e+28\n",
      "Gradient Descent(14/49): loss=1.4326830236765078e+31\n",
      "Gradient Descent(15/49): loss=3.1282806119120595e+33\n",
      "Gradient Descent(16/49): loss=6.830638541863726e+35\n",
      "Gradient Descent(17/49): loss=1.491478200274162e+38\n",
      "Gradient Descent(18/49): loss=3.2566607296527643e+40\n",
      "Gradient Descent(19/49): loss=7.110958188352729e+42\n",
      "Gradient Descent(20/49): loss=1.5526863427095522e+45\n",
      "Gradient Descent(21/49): loss=3.3903094566542725e+47\n",
      "Gradient Descent(22/49): loss=7.402781808714552e+49\n",
      "Gradient Descent(23/49): loss=1.6164063844027854e+52\n",
      "Gradient Descent(24/49): loss=3.5294429405952135e+54\n",
      "Gradient Descent(25/49): loss=7.706581458192569e+56\n",
      "Gradient Descent(26/49): loss=1.6827414062623421e+59\n",
      "Gradient Descent(27/49): loss=3.674286265202911e+61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(28/49): loss=8.022848613826202e+63\n",
      "Gradient Descent(29/49): loss=1.751798723195581e+66\n",
      "Gradient Descent(30/49): loss=3.8250737541040854e+68\n",
      "Gradient Descent(31/49): loss=8.352094924265149e+70\n",
      "Gradient Descent(32/49): loss=1.8236900543183046e+73\n",
      "Gradient Descent(33/49): loss=3.982049347352353e+75\n",
      "Gradient Descent(34/49): loss=8.694853035581476e+77\n",
      "Gradient Descent(35/49): loss=1.8985317035467315e+80\n",
      "Gradient Descent(36/49): loss=4.1454669959594245e+82\n",
      "Gradient Descent(37/49): loss=9.051677452889011e+84\n",
      "Gradient Descent(38/49): loss=1.9764447477449265e+87\n",
      "Gradient Descent(39/49): loss=4.315591072726223e+89\n",
      "Gradient Descent(40/49): loss=9.42314543740422e+91\n",
      "Gradient Descent(41/49): loss=2.057555232599433e+94\n",
      "Gradient Descent(42/49): loss=4.492696799937645e+96\n",
      "Gradient Descent(43/49): loss=9.80985794032369e+98\n",
      "Gradient Descent(44/49): loss=2.1419943765327604e+101\n",
      "Gradient Descent(45/49): loss=4.6770706946104256e+103\n",
      "Gradient Descent(46/49): loss=1.0212440575027389e+106\n",
      "Gradient Descent(47/49): loss=2.2298987829849995e+108\n",
      "Gradient Descent(48/49): loss=4.8690110320124376e+110\n",
      "Gradient Descent(49/49): loss=1.063154462917987e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.796774457070299\n",
      "Gradient Descent(2/49): loss=1300.5589193223866\n",
      "Gradient Descent(3/49): loss=283435.26740560855\n",
      "Gradient Descent(4/49): loss=62675793.89178957\n",
      "Gradient Descent(5/49): loss=13914504244.061047\n",
      "Gradient Descent(6/49): loss=3092835289231.1846\n",
      "Gradient Descent(7/49): loss=687718179475965.6\n",
      "Gradient Descent(8/49): loss=1.5293867859239437e+17\n",
      "Gradient Descent(9/49): loss=3.4012727660651106e+19\n",
      "Gradient Descent(10/49): loss=7.564343612977477e+21\n",
      "Gradient Descent(11/49): loss=1.682297850503017e+24\n",
      "Gradient Descent(12/49): loss=3.741408596190823e+26\n",
      "Gradient Descent(13/49): loss=8.320848007299542e+28\n",
      "Gradient Descent(14/49): loss=1.8505469323469383e+31\n",
      "Gradient Descent(15/49): loss=4.1155950875870205e+33\n",
      "Gradient Descent(16/49): loss=9.153036274895844e+35\n",
      "Gradient Descent(17/49): loss=2.0356247811889944e+38\n",
      "Gradient Descent(18/49): loss=4.527206199770734e+40\n",
      "Gradient Descent(19/49): loss=1.0068454744908995e+43\n",
      "Gradient Descent(20/49): loss=2.239212805781384e+45\n",
      "Gradient Descent(21/49): loss=4.9799836389203435e+47\n",
      "Gradient Descent(22/49): loss=1.1075426587587669e+50\n",
      "Gradient Descent(23/49): loss=2.4631621907186836e+52\n",
      "Gradient Descent(24/49): loss=5.47804450674512e+54\n",
      "Gradient Descent(25/49): loss=1.2183108254493827e+57\n",
      "Gradient Descent(26/49): loss=2.709509325051882e+59\n",
      "Gradient Descent(27/49): loss=6.025917712613025e+61\n",
      "Gradient Descent(28/49): loss=1.3401571990710567e+64\n",
      "Gradient Descent(29/49): loss=2.980494264736945e+66\n",
      "Gradient Descent(30/49): loss=6.628585115453204e+68\n",
      "Gradient Descent(31/49): loss=1.4741897393547163e+71\n",
      "Gradient Descent(32/49): loss=3.2785810995355085e+73\n",
      "Gradient Descent(33/49): loss=7.291526822684677e+75\n",
      "Gradient Descent(34/49): loss=1.621627215915523e+78\n",
      "Gradient Descent(35/49): loss=3.606480359115916e+80\n",
      "Gradient Descent(36/49): loss=8.020771021252097e+82\n",
      "Gradient Descent(37/49): loss=1.7838102906271595e+85\n",
      "Gradient Descent(38/49): loss=3.9671736601335514e+87\n",
      "Gradient Descent(39/49): loss=8.822948792454858e+89\n",
      "Gradient Descent(40/49): loss=1.9622137083774538e+92\n",
      "Gradient Descent(41/49): loss=4.363940818331706e+94\n",
      "Gradient Descent(42/49): loss=9.705354408949156e+96\n",
      "Gradient Descent(43/49): loss=2.158459707052528e+99\n",
      "Gradient Descent(44/49): loss=4.80038967219307e+101\n",
      "Gradient Descent(45/49): loss=1.0676011662207522e+104\n",
      "Gradient Descent(46/49): loss=2.3743327686879248e+106\n",
      "Gradient Descent(47/49): loss=5.280488889330791e+108\n",
      "Gradient Descent(48/49): loss=1.1743746823556882e+111\n",
      "Gradient Descent(49/49): loss=2.6117958459198915e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.76700376823544\n",
      "Gradient Descent(2/49): loss=1291.6766228947552\n",
      "Gradient Descent(3/49): loss=281456.7279789155\n",
      "Gradient Descent(4/49): loss=62309095.767031744\n",
      "Gradient Descent(5/49): loss=13855916010.89735\n",
      "Gradient Descent(6/49): loss=3085480624005.275\n",
      "Gradient Descent(7/49): loss=687393270253807.4\n",
      "Gradient Descent(8/49): loss=1.531622732469585e+17\n",
      "Gradient Descent(9/49): loss=3.412868712435949e+19\n",
      "Gradient Descent(10/49): loss=7.604916252945221e+21\n",
      "Gradient Descent(11/49): loss=1.6946173541400062e+24\n",
      "Gradient Descent(12/49): loss=3.7761535957502055e+26\n",
      "Gradient Descent(13/49): loss=8.414492262285338e+28\n",
      "Gradient Descent(14/49): loss=1.8750215677288857e+31\n",
      "Gradient Descent(15/49): loss=4.1781559805508123e+33\n",
      "Gradient Descent(16/49): loss=9.310286394586538e+35\n",
      "Gradient Descent(17/49): loss=2.074633733296123e+38\n",
      "Gradient Descent(18/49): loss=4.622956755296184e+40\n",
      "Gradient Descent(19/49): loss=1.0301446870500365e+43\n",
      "Gradient Descent(20/49): loss=2.2954964381605068e+45\n",
      "Gradient Descent(21/49): loss=5.11511049306179e+47\n",
      "Gradient Descent(22/49): loss=1.1398125007772528e+50\n",
      "Gradient Descent(23/49): loss=2.5398718927067137e+52\n",
      "Gradient Descent(24/49): loss=5.659658256937085e+54\n",
      "Gradient Descent(25/49): loss=1.261155402259987e+57\n",
      "Gradient Descent(26/49): loss=2.810263228704052e+59\n",
      "Gradient Descent(27/49): loss=6.26217784141013e+61\n",
      "Gradient Descent(28/49): loss=1.3954163053805352e+64\n",
      "Gradient Descent(29/49): loss=3.10944006164383e+66\n",
      "Gradient Descent(30/49): loss=6.928840848193306e+68\n",
      "Gradient Descent(31/49): loss=1.5439704431611325e+71\n",
      "Gradient Descent(32/49): loss=3.440466856699097e+73\n",
      "Gradient Descent(33/49): loss=7.666475899506322e+75\n",
      "Gradient Descent(34/49): loss=1.7083394540850832e+78\n",
      "Gradient Descent(35/49): loss=3.8067343178782374e+80\n",
      "Gradient Descent(36/49): loss=8.482638583485036e+82\n",
      "Gradient Descent(37/49): loss=1.890206968216645e+85\n",
      "Gradient Descent(38/49): loss=4.2119941189653705e+87\n",
      "Gradient Descent(39/49): loss=9.385688845987514e+89\n",
      "Gradient Descent(40/49): loss=2.0914358526059132e+92\n",
      "Gradient Descent(41/49): loss=4.660397331875539e+94\n",
      "Gradient Descent(42/49): loss=1.0384876621432422e+97\n",
      "Gradient Descent(43/49): loss=2.314087292616563e+99\n",
      "Gradient Descent(44/49): loss=5.1565369460411e+101\n",
      "Gradient Descent(45/49): loss=1.149043657978064e+104\n",
      "Gradient Descent(46/49): loss=2.5604419046260494e+106\n",
      "Gradient Descent(47/49): loss=5.705494914354435e+108\n",
      "Gradient Descent(48/49): loss=1.271369296015276e+111\n",
      "Gradient Descent(49/49): loss=2.833023096355284e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.795588211937805\n",
      "Gradient Descent(2/49): loss=1287.316388325781\n",
      "Gradient Descent(3/49): loss=278102.8762861778\n",
      "Gradient Descent(4/49): loss=61007339.75974164\n",
      "Gradient Descent(5/49): loss=13440375832.29285\n",
      "Gradient Descent(6/49): loss=2964917982340.61\n",
      "Gradient Descent(7/49): loss=654332576094303.9\n",
      "Gradient Descent(8/49): loss=1.4442597810430346e+17\n",
      "Gradient Descent(9/49): loss=3.1879566789325468e+19\n",
      "Gradient Descent(10/49): loss=7.036979979739416e+21\n",
      "Gradient Descent(11/49): loss=1.5533256283146836e+24\n",
      "Gradient Descent(12/49): loss=3.42877880725165e+26\n",
      "Gradient Descent(13/49): loss=7.568619806455504e+28\n",
      "Gradient Descent(14/49): loss=1.6706827848698411e+31\n",
      "Gradient Descent(15/49): loss=3.687833749662189e+33\n",
      "Gradient Descent(16/49): loss=8.140455026688678e+35\n",
      "Gradient Descent(17/49): loss=1.7969087829474365e+38\n",
      "Gradient Descent(18/49): loss=3.966462773832701e+40\n",
      "Gradient Descent(19/49): loss=8.75549560396675e+42\n",
      "Gradient Descent(20/49): loss=1.9326716942650457e+45\n",
      "Gradient Descent(21/49): loss=4.266143285466845e+47\n",
      "Gradient Descent(22/49): loss=9.417004753958364e+49\n",
      "Gradient Descent(23/49): loss=2.0786919848463492e+52\n",
      "Gradient Descent(24/49): loss=4.588465739134644e+54\n",
      "Gradient Descent(25/49): loss=1.0128493299018137e+57\n",
      "Gradient Descent(26/49): loss=2.235744633186385e+59\n",
      "Gradient Descent(27/49): loss=4.9351408124118486e+61\n",
      "Gradient Descent(28/49): loss=1.0893737360166521e+64\n",
      "Gradient Descent(29/49): loss=2.4046631734159684e+66\n",
      "Gradient Descent(30/49): loss=5.308008433108232e+68\n",
      "Gradient Descent(31/49): loss=1.1716798359715312e+71\n",
      "Gradient Descent(32/49): loss=2.5863441162967146e+73\n",
      "Gradient Descent(33/49): loss=5.709047542288838e+75\n",
      "Gradient Descent(34/49): loss=1.2602044575098113e+78\n",
      "Gradient Descent(35/49): loss=2.781751707204925e+80\n",
      "Gradient Descent(36/49): loss=6.140386597130582e+82\n",
      "Gradient Descent(37/49): loss=1.3554174322806883e+85\n",
      "Gradient Descent(38/49): loss=2.991923043719885e+87\n",
      "Gradient Descent(39/49): loss=6.604314867398186e+89\n",
      "Gradient Descent(40/49): loss=1.4578240894026308e+92\n",
      "Gradient Descent(41/49): loss=3.217973579869394e+94\n",
      "Gradient Descent(42/49): loss=7.103294585412393e+96\n",
      "Gradient Descent(43/49): loss=1.5679679374246736e+99\n",
      "Gradient Descent(44/49): loss=3.461103046240968e+101\n",
      "Gradient Descent(45/49): loss=7.639974013992858e+103\n",
      "Gradient Descent(46/49): loss=1.686433548919606e+106\n",
      "Gradient Descent(47/49): loss=3.7226018173787015e+108\n",
      "Gradient Descent(48/49): loss=8.217201501730696e+110\n",
      "Gradient Descent(49/49): loss=1.8138496630185218e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.771654629444734\n",
      "Gradient Descent(2/49): loss=1283.057484775068\n",
      "Gradient Descent(3/49): loss=276827.1445043682\n",
      "Gradient Descent(4/49): loss=60642098.04736317\n",
      "Gradient Descent(5/49): loss=13342351624.368704\n",
      "Gradient Descent(6/49): loss=2939622979698.6577\n",
      "Gradient Descent(7/49): loss=647962705979059.1\n",
      "Gradient Descent(8/49): loss=1.4284851448252878e+17\n",
      "Gradient Descent(9/49): loss=3.149375473953737e+19\n",
      "Gradient Descent(10/49): loss=6.943541572546355e+21\n",
      "Gradient Descent(11/49): loss=1.5308771794244e+24\n",
      "Gradient Descent(12/49): loss=3.37520837728193e+26\n",
      "Gradient Descent(13/49): loss=7.441511413041292e+28\n",
      "Gradient Descent(14/49): loss=1.6406722191078867e+31\n",
      "Gradient Descent(15/49): loss=3.6172833938837e+33\n",
      "Gradient Descent(16/49): loss=7.975230757944606e+35\n",
      "Gradient Descent(17/49): loss=1.7583445745212926e+38\n",
      "Gradient Descent(18/49): loss=3.8767225019702e+40\n",
      "Gradient Descent(19/49): loss=8.547231068073216e+42\n",
      "Gradient Descent(20/49): loss=1.8844567524440174e+45\n",
      "Gradient Descent(21/49): loss=4.154769215977949e+47\n",
      "Gradient Descent(22/49): loss=9.160256512394885e+49\n",
      "Gradient Descent(23/49): loss=2.0196139668029017e+52\n",
      "Gradient Descent(24/49): loss=4.452758030749889e+54\n",
      "Gradient Descent(25/49): loss=9.817249437938245e+56\n",
      "Gradient Descent(26/49): loss=2.164464941979682e+59\n",
      "Gradient Descent(27/49): loss=4.772119232252256e+61\n",
      "Gradient Descent(28/49): loss=1.0521363282513906e+64\n",
      "Gradient Descent(29/49): loss=2.3197049347484498e+66\n",
      "Gradient Descent(30/49): loss=5.1143856930018e+68\n",
      "Gradient Descent(31/49): loss=1.127597765774379e+71\n",
      "Gradient Descent(32/49): loss=2.486079067363228e+73\n",
      "Gradient Descent(33/49): loss=5.481200226516127e+75\n",
      "Gradient Descent(34/49): loss=1.2084714568239789e+78\n",
      "Gradient Descent(35/49): loss=2.6643859038269694e+80\n",
      "Gradient Descent(36/49): loss=5.8743234723713016e+82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=1.2951455796432395e+85\n",
      "Gradient Descent(38/49): loss=2.855481282838336e+87\n",
      "Gradient Descent(39/49): loss=6.295642346929174e+89\n",
      "Gradient Descent(40/49): loss=1.3880361534378777e+92\n",
      "Gradient Descent(41/49): loss=3.0602824256533147e+94\n",
      "Gradient Descent(42/49): loss=6.747179100174421e+96\n",
      "Gradient Descent(43/49): loss=1.4875890351888597e+99\n",
      "Gradient Descent(44/49): loss=3.279772338571717e+101\n",
      "Gradient Descent(45/49): loss=7.23110102212772e+103\n",
      "Gradient Descent(46/49): loss=1.5942820596806274e+106\n",
      "Gradient Descent(47/49): loss=3.5150045311794417e+108\n",
      "Gradient Descent(48/49): loss=7.749730845423272e+110\n",
      "Gradient Descent(49/49): loss=1.7086273330166333e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.8601867119577715\n",
      "Gradient Descent(2/49): loss=1326.011412522311\n",
      "Gradient Descent(3/49): loss=291787.9772994305\n",
      "Gradient Descent(4/49): loss=65149749.52227338\n",
      "Gradient Descent(5/49): loss=14604343168.713442\n",
      "Gradient Descent(6/49): loss=3277730644041.837\n",
      "Gradient Descent(7/49): loss=735918068611581.8\n",
      "Gradient Descent(8/49): loss=1.6524902652954675e+17\n",
      "Gradient Descent(9/49): loss=3.710784215498668e+19\n",
      "Gradient Descent(10/49): loss=8.332938338339994e+21\n",
      "Gradient Descent(11/49): loss=1.871252945773199e+24\n",
      "Gradient Descent(12/49): loss=4.202110191438516e+26\n",
      "Gradient Descent(13/49): loss=9.436318191536715e+28\n",
      "Gradient Descent(14/49): loss=2.119033305951733e+31\n",
      "Gradient Descent(15/49): loss=4.758532177572672e+33\n",
      "Gradient Descent(16/49): loss=1.0685829614600911e+36\n",
      "Gradient Descent(17/49): loss=2.3996255757867036e+38\n",
      "Gradient Descent(18/49): loss=5.388634408178216e+40\n",
      "Gradient Descent(19/49): loss=1.210079651375822e+43\n",
      "Gradient Descent(20/49): loss=2.7173726254832603e+45\n",
      "Gradient Descent(21/49): loss=6.102171851076941e+47\n",
      "Gradient Descent(22/49): loss=1.3703126671564383e+50\n",
      "Gradient Descent(23/49): loss=3.077194237737871e+52\n",
      "Gradient Descent(24/49): loss=6.910192544915787e+54\n",
      "Gradient Descent(25/49): loss=1.5517629801278766e+57\n",
      "Gradient Descent(26/49): loss=3.4846617237425534e+59\n",
      "Gradient Descent(27/49): loss=7.825207511985315e+61\n",
      "Gradient Descent(28/49): loss=1.7572400841211935e+64\n",
      "Gradient Descent(29/49): loss=3.946084124303128e+66\n",
      "Gradient Descent(30/49): loss=8.861384427082832e+68\n",
      "Gradient Descent(31/49): loss=1.9899254929952452e+71\n",
      "Gradient Descent(32/49): loss=4.4686058936458714e+73\n",
      "Gradient Descent(33/49): loss=1.0034766981486297e+76\n",
      "Gradient Descent(34/49): loss=2.253422001611568e+78\n",
      "Gradient Descent(35/49): loss=5.060317520791094e+80\n",
      "Gradient Descent(36/49): loss=1.1363523296085754e+83\n",
      "Gradient Descent(37/49): loss=2.551809469862998e+85\n",
      "Gradient Descent(38/49): loss=5.730380798995222e+87\n",
      "Gradient Descent(39/49): loss=1.2868227228287576e+90\n",
      "Gradient Descent(40/49): loss=2.8897079933654245e+92\n",
      "Gradient Descent(41/49): loss=6.489170682783451e+94\n",
      "Gradient Descent(42/49): loss=1.4572176928248999e+97\n",
      "Gradient Descent(43/49): loss=3.272349438913337e+99\n",
      "Gradient Descent(44/49): loss=7.348435929018822e+101\n",
      "Gradient Descent(45/49): loss=1.6501755576820916e+104\n",
      "Gradient Descent(46/49): loss=3.705658452321813e+106\n",
      "Gradient Descent(47/49): loss=8.32148100930092e+108\n",
      "Gradient Descent(48/49): loss=1.8686839890699843e+111\n",
      "Gradient Descent(49/49): loss=4.1963441929429575e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.830088951554362\n",
      "Gradient Descent(2/49): loss=1316.9566807332255\n",
      "Gradient Descent(3/49): loss=289751.0464469912\n",
      "Gradient Descent(4/49): loss=64768472.08729044\n",
      "Gradient Descent(5/49): loss=14542812321.645164\n",
      "Gradient Descent(6/49): loss=3269925203672.533\n",
      "Gradient Descent(7/49): loss=735567393174011.9\n",
      "Gradient Descent(8/49): loss=1.6548984126865968e+17\n",
      "Gradient Descent(9/49): loss=3.723415628900001e+19\n",
      "Gradient Descent(10/49): loss=8.377584032263597e+21\n",
      "Gradient Descent(11/49): loss=1.8849439313253753e+24\n",
      "Gradient Descent(12/49): loss=4.2411034975848935e+26\n",
      "Gradient Descent(13/49): loss=9.542442891411417e+28\n",
      "Gradient Descent(14/49): loss=2.147041091841213e+31\n",
      "Gradient Descent(15/49): loss=4.830823527592445e+33\n",
      "Gradient Descent(16/49): loss=1.0869310593475696e+36\n",
      "Gradient Descent(17/49): loss=2.44558537479576e+38\n",
      "Gradient Descent(18/49): loss=5.502545712654036e+40\n",
      "Gradient Descent(19/49): loss=1.2380679757704212e+43\n",
      "Gradient Descent(20/49): loss=2.7856421247655462e+45\n",
      "Gradient Descent(21/49): loss=6.267670434793943e+47\n",
      "Gradient Descent(22/49): loss=1.410220370060511e+50\n",
      "Gradient Descent(23/49): loss=3.172983507739228e+52\n",
      "Gradient Descent(24/49): loss=7.139185161528062e+54\n",
      "Gradient Descent(25/49): loss=1.6063104219207714e+57\n",
      "Gradient Descent(26/49): loss=3.6141844106758365e+59\n",
      "Gradient Descent(27/49): loss=8.131883337191137e+61\n",
      "Gradient Descent(28/49): loss=1.8296666438590606e+64\n",
      "Gradient Descent(29/49): loss=4.116733957974931e+66\n",
      "Gradient Descent(30/49): loss=9.262615426490463e+68\n",
      "Gradient Descent(31/49): loss=2.0840803757273423e+71\n",
      "Gradient Descent(32/49): loss=4.6891626311830214e+73\n",
      "Gradient Descent(33/49): loss=1.0550574938363041e+76\n",
      "Gradient Descent(34/49): loss=2.373870140262803e+78\n",
      "Gradient Descent(35/49): loss=5.341187068716865e+80\n",
      "Gradient Descent(36/49): loss=1.2017624224326807e+83\n",
      "Gradient Descent(37/49): loss=2.7039549474499513e+85\n",
      "Gradient Descent(38/49): loss=6.083875000051118e+87\n",
      "Gradient Descent(39/49): loss=1.3688665578971152e+90\n",
      "Gradient Descent(40/49): loss=3.0799377918076196e+92\n",
      "Gradient Descent(41/49): loss=6.929833113884695e+94\n",
      "Gradient Descent(42/49): loss=1.5592063941690288e+97\n",
      "Gradient Descent(43/49): loss=3.5082007599094207e+99\n",
      "Gradient Descent(44/49): loss=7.893421049230823e+101\n",
      "Gradient Descent(45/49): loss=1.7760128374765175e+104\n",
      "Gradient Descent(46/49): loss=3.996013362531537e+106\n",
      "Gradient Descent(47/49): loss=8.990995141802696e+108\n",
      "Gradient Descent(48/49): loss=2.022966049060156e+111\n",
      "Gradient Descent(49/49): loss=4.551655930301751e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.859004238140873\n",
      "Gradient Descent(2/49): loss=1312.5176903578229\n",
      "Gradient Descent(3/49): loss=286300.6695949801\n",
      "Gradient Descent(4/49): loss=63416046.00448785\n",
      "Gradient Descent(5/49): loss=14106872849.273888\n",
      "Gradient Descent(6/49): loss=3142209490462.3076\n",
      "Gradient Descent(7/49): loss=700203777396009.8\n",
      "Gradient Descent(8/49): loss=1.560539682860932e+17\n",
      "Gradient Descent(9/49): loss=3.47812797157849e+19\n",
      "Gradient Descent(10/49): loss=7.752167163074675e+21\n",
      "Gradient Descent(11/49): loss=1.7278379040079637e+24\n",
      "Gradient Descent(12/49): loss=3.851089727215383e+26\n",
      "Gradient Descent(13/49): loss=8.583502498331426e+28\n",
      "Gradient Descent(14/49): loss=1.9131345984248289e+31\n",
      "Gradient Descent(15/49): loss=4.2640917746105165e+33\n",
      "Gradient Descent(16/49): loss=9.504025013548909e+35\n",
      "Gradient Descent(17/49): loss=2.1183055367345253e+38\n",
      "Gradient Descent(18/49): loss=4.721387361866016e+40\n",
      "Gradient Descent(19/49): loss=1.0523268836181682e+43\n",
      "Gradient Descent(20/49): loss=2.345479803785994e+45\n",
      "Gradient Descent(21/49): loss=5.227724955193733e+47\n",
      "Gradient Descent(22/49): loss=1.165181988092642e+50\n",
      "Gradient Descent(23/49): loss=2.597017014144794e+52\n",
      "Gradient Descent(24/49): loss=5.78836391285404e+54\n",
      "Gradient Descent(25/49): loss=1.290140057041831e+57\n",
      "Gradient Descent(26/49): loss=2.8755299283937487e+59\n",
      "Gradient Descent(27/49): loss=6.409127694281943e+61\n",
      "Gradient Descent(28/49): loss=1.4284990532009185e+64\n",
      "Gradient Descent(29/49): loss=3.183911512352202e+66\n",
      "Gradient Descent(30/49): loss=7.096464289404825e+68\n",
      "Gradient Descent(31/49): loss=1.581696137453061e+71\n",
      "Gradient Descent(32/49): loss=3.525364983473718e+73\n",
      "Gradient Descent(33/49): loss=7.857513192587836e+75\n",
      "Gradient Descent(34/49): loss=1.7513225966990693e+78\n",
      "Gradient Descent(35/49): loss=3.903437083124541e+80\n",
      "Gradient Descent(36/49): loss=8.700179561795507e+82\n",
      "Gradient Descent(37/49): loss=1.939140372845326e+85\n",
      "Gradient Descent(38/49): loss=4.322054917246647e+87\n",
      "Gradient Descent(39/49): loss=9.633216330948914e+89\n",
      "Gradient Descent(40/49): loss=2.147100364425221e+92\n",
      "Gradient Descent(41/49): loss=4.785566747944915e+94\n",
      "Gradient Descent(42/49): loss=1.0666315128294833e+97\n",
      "Gradient Descent(43/49): loss=2.3773626909487414e+99\n",
      "Gradient Descent(44/49): loss=5.298787159702639e+101\n",
      "Gradient Descent(45/49): loss=1.181020694516942e+104\n",
      "Gradient Descent(46/49): loss=2.632319130469764e+106\n",
      "Gradient Descent(47/49): loss=5.867047069375194e+108\n",
      "Gradient Descent(48/49): loss=1.3076773600821447e+111\n",
      "Gradient Descent(49/49): loss=2.9146179634340287e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.834506182190146\n",
      "Gradient Descent(2/49): loss=1308.0611823245995\n",
      "Gradient Descent(3/49): loss=284950.32730542583\n",
      "Gradient Descent(4/49): loss=63025472.573670514\n",
      "Gradient Descent(5/49): loss=14000956020.43435\n",
      "Gradient Descent(6/49): loss=3114592318941.4473\n",
      "Gradient Descent(7/49): loss=693177094542236.1\n",
      "Gradient Descent(8/49): loss=1.542960064061375e+17\n",
      "Gradient Descent(9/49): loss=3.434695344086447e+19\n",
      "Gradient Descent(10/49): loss=7.64591854795054e+21\n",
      "Gradient Descent(11/49): loss=1.7020559505042233e+24\n",
      "Gradient Descent(12/49): loss=3.788950512553152e+26\n",
      "Gradient Descent(13/49): loss=8.434597363834933e+28\n",
      "Gradient Descent(14/49): loss=1.8776294548696246e+31\n",
      "Gradient Descent(15/49): loss=4.179799612996585e+33\n",
      "Gradient Descent(16/49): loss=9.304671530533202e+35\n",
      "Gradient Descent(17/49): loss=2.071317316845148e+38\n",
      "Gradient Descent(18/49): loss=4.610969262461388e+40\n",
      "Gradient Descent(19/49): loss=1.0264500466793636e+43\n",
      "Gradient Descent(20/49): loss=2.2849853009297853e+45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=5.086616580242625e+47\n",
      "Gradient Descent(22/49): loss=1.132334121588272e+50\n",
      "Gradient Descent(23/49): loss=2.520694341137445e+52\n",
      "Gradient Descent(24/49): loss=5.6113295893336234e+54\n",
      "Gradient Descent(25/49): loss=1.2491407326279631e+57\n",
      "Gradient Descent(26/49): loss=2.780718090196383e+59\n",
      "Gradient Descent(27/49): loss=6.190169686388949e+61\n",
      "Gradient Descent(28/49): loss=1.377996600280532e+64\n",
      "Gradient Descent(29/49): loss=3.0675647463429618e+66\n",
      "Gradient Descent(30/49): loss=6.828720383700928e+68\n",
      "Gradient Descent(31/49): loss=1.5201446728831022e+71\n",
      "Gradient Descent(32/49): loss=3.3840012427664937e+73\n",
      "Gradient Descent(33/49): loss=7.533141164338236e+75\n",
      "Gradient Descent(34/49): loss=1.6769561158746692e+78\n",
      "Gradient Descent(35/49): loss=3.7330799373338166e+80\n",
      "Gradient Descent(36/49): loss=8.310226896578882e+82\n",
      "Gradient Descent(37/49): loss=1.8499435380948862e+85\n",
      "Gradient Descent(38/49): loss=4.11816805573371e+87\n",
      "Gradient Descent(39/49): loss=9.167473377447268e+89\n",
      "Gradient Descent(40/49): loss=2.0407755824629953e+92\n",
      "Gradient Descent(41/49): loss=4.542980171857206e+94\n",
      "Gradient Descent(42/49): loss=1.0113149637442749e+97\n",
      "Gradient Descent(43/49): loss=2.2512930217676443e+99\n",
      "Gradient Descent(44/49): loss=5.0116140387113655e+101\n",
      "Gradient Descent(45/49): loss=1.1156377703906451e+104\n",
      "Gradient Descent(46/49): loss=2.483526514827646e+106\n",
      "Gradient Descent(47/49): loss=5.528590115492613e+108\n",
      "Gradient Descent(48/49): loss=1.230722059242594e+111\n",
      "Gradient Descent(49/49): loss=2.7397161943002423e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.923913813477005\n",
      "Gradient Descent(2/49): loss=1351.8358486621476\n",
      "Gradient Descent(3/49): loss=300344.81845756323\n",
      "Gradient Descent(4/49): loss=67708733.80011626\n",
      "Gradient Descent(5/49): loss=15324811102.713245\n",
      "Gradient Descent(6/49): loss=3472708314226.588\n",
      "Gradient Descent(7/49): loss=787239302268040.6\n",
      "Gradient Descent(8/49): loss=1.7848371220220467e+17\n",
      "Gradient Descent(9/49): loss=4.046763337223179e+19\n",
      "Gradient Descent(10/49): loss=9.175351656633971e+21\n",
      "Gradient Descent(11/49): loss=2.080364802991487e+24\n",
      "Gradient Descent(12/49): loss=4.7169023998494644e+26\n",
      "Gradient Descent(13/49): loss=1.0694844738702335e+29\n",
      "Gradient Descent(14/49): loss=2.424890568273025e+31\n",
      "Gradient Descent(15/49): loss=5.498064446942997e+33\n",
      "Gradient Descent(16/49): loss=1.2466011272909133e+36\n",
      "Gradient Descent(17/49): loss=2.8264753786566978e+38\n",
      "Gradient Descent(18/49): loss=6.408596066078643e+40\n",
      "Gradient Descent(19/49): loss=1.4530501087051502e+43\n",
      "Gradient Descent(20/49): loss=3.294566543190193e+45\n",
      "Gradient Descent(21/49): loss=7.469920440571285e+47\n",
      "Gradient Descent(22/49): loss=1.6936890075859848e+50\n",
      "Gradient Descent(23/49): loss=3.84017805445016e+52\n",
      "Gradient Descent(24/49): loss=8.707010214899127e+54\n",
      "Gradient Descent(25/49): loss=1.9741799939343783e+57\n",
      "Gradient Descent(26/49): loss=4.476148014369617e+59\n",
      "Gradient Descent(27/49): loss=1.0148973805887221e+62\n",
      "Gradient Descent(28/49): loss=2.3011229517416682e+64\n",
      "Gradient Descent(29/49): loss=5.2174406401174156e+66\n",
      "Gradient Descent(30/49): loss=1.18297402633551e+69\n",
      "Gradient Descent(31/49): loss=2.682210768674792e+71\n",
      "Gradient Descent(32/49): loss=6.081498365505614e+73\n",
      "Gradient Descent(33/49): loss=1.3788857610142557e+76\n",
      "Gradient Descent(34/49): loss=3.1264103476738923e+78\n",
      "Gradient Descent(35/49): loss=7.088652257060591e+80\n",
      "Gradient Descent(36/49): loss=1.6072423397304912e+83\n",
      "Gradient Descent(37/49): loss=3.6441735959742404e+85\n",
      "Gradient Descent(38/49): loss=8.262600399030484e+87\n",
      "Gradient Descent(39/49): loss=1.87341693681876e+90\n",
      "Gradient Descent(40/49): loss=4.2476833559216966e+92\n",
      "Gradient Descent(41/49): loss=9.63096550350009e+94\n",
      "Gradient Descent(42/49): loss=2.1836725753179868e+97\n",
      "Gradient Descent(43/49): loss=4.951140064267622e+99\n",
      "Gradient Descent(44/49): loss=1.1225944866036642e+102\n",
      "Gradient Descent(45/49): loss=2.5453094943686808e+104\n",
      "Gradient Descent(46/49): loss=5.771095884965464e+106\n",
      "Gradient Descent(47/49): loss=1.3085067960164162e+109\n",
      "Gradient Descent(48/49): loss=2.966836922050896e+111\n",
      "Gradient Descent(49/49): loss=6.726844177532251e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.893487246208619\n",
      "Gradient Descent(2/49): loss=1342.6061835072273\n",
      "Gradient Descent(3/49): loss=298248.06748356536\n",
      "Gradient Descent(4/49): loss=67312372.44998407\n",
      "Gradient Descent(5/49): loss=15260205319.799519\n",
      "Gradient Descent(6/49): loss=3464426916860.357\n",
      "Gradient Descent(7/49): loss=786860999217836.0\n",
      "Gradient Descent(8/49): loss=1.7874298194259862e+17\n",
      "Gradient Descent(9/49): loss=4.060517084722472e+19\n",
      "Gradient Descent(10/49): loss=9.224456871400372e+21\n",
      "Gradient Descent(11/49): loss=2.095572273990655e+24\n",
      "Gradient Descent(12/49): loss=4.7606392927564676e+26\n",
      "Gradient Descent(13/49): loss=1.081504112420713e+29\n",
      "Gradient Descent(14/49): loss=2.4569208295343624e+31\n",
      "Gradient Descent(15/49): loss=5.581541766972727e+33\n",
      "Gradient Descent(16/49): loss=1.2679940205144971e+36\n",
      "Gradient Descent(17/49): loss=2.880581952887937e+38\n",
      "Gradient Descent(18/49): loss=6.543999635812229e+40\n",
      "Gradient Descent(19/49): loss=1.486641656803893e+43\n",
      "Gradient Descent(20/49): loss=3.377297584613852e+45\n",
      "Gradient Descent(21/49): loss=7.672419862528753e+47\n",
      "Gradient Descent(22/49): loss=1.7429919950488981e+50\n",
      "Gradient Descent(23/49): loss=3.959664811455692e+52\n",
      "Gradient Descent(24/49): loss=8.995420210576689e+54\n",
      "Gradient Descent(25/49): loss=2.043546325710335e+57\n",
      "Gradient Descent(26/49): loss=4.642453034507126e+59\n",
      "Gradient Descent(27/49): loss=1.0546553266961396e+62\n",
      "Gradient Descent(28/49): loss=2.3959270020847476e+64\n",
      "Gradient Descent(29/49): loss=5.442978434766706e+66\n",
      "Gradient Descent(30/49): loss=1.2365157292170046e+69\n",
      "Gradient Descent(31/49): loss=2.809070744860677e+71\n",
      "Gradient Descent(32/49): loss=6.381543124104731e+73\n",
      "Gradient Descent(33/49): loss=1.4497353873808596e+76\n",
      "Gradient Descent(34/49): loss=3.2934552858940173e+78\n",
      "Gradient Descent(35/49): loss=7.481950026604155e+80\n",
      "Gradient Descent(36/49): loss=1.6997217615300414e+83\n",
      "Gradient Descent(37/49): loss=3.861365093786983e+85\n",
      "Gradient Descent(38/49): loss=8.772106544129222e+87\n",
      "Gradient Descent(39/49): loss=1.992814752103309e+90\n",
      "Gradient Descent(40/49): loss=4.5272029200995186e+92\n",
      "Gradient Descent(41/49): loss=1.0284732315497726e+95\n",
      "Gradient Descent(42/49): loss=2.3364474857495e+97\n",
      "Gradient Descent(43/49): loss=5.307855067301251e+99\n",
      "Gradient Descent(44/49): loss=1.2058189018717886e+102\n",
      "Gradient Descent(45/49): loss=2.7393348267336282e+104\n",
      "Gradient Descent(46/49): loss=6.223119642018691e+106\n",
      "Gradient Descent(47/49): loss=1.4137453260891434e+109\n",
      "Gradient Descent(48/49): loss=3.211694394470246e+111\n",
      "Gradient Descent(49/49): loss=7.296208654500724e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.922735193901853\n",
      "Gradient Descent(2/49): loss=1338.08738262313\n",
      "Gradient Descent(3/49): loss=294698.86261966685\n",
      "Gradient Descent(4/49): loss=65907559.15979624\n",
      "Gradient Descent(5/49): loss=14802969740.50377\n",
      "Gradient Descent(6/49): loss=3329171027373.547\n",
      "Gradient Descent(7/49): loss=749046243168331.4\n",
      "Gradient Descent(8/49): loss=1.685552733006478e+17\n",
      "Gradient Descent(9/49): loss=3.793119146850367e+19\n",
      "Gradient Descent(10/49): loss=8.536060369427593e+21\n",
      "Gradient Descent(11/49): loss=1.9209708521902933e+24\n",
      "Gradient Descent(12/49): loss=4.322996141657592e+26\n",
      "Gradient Descent(13/49): loss=9.72857389081484e+28\n",
      "Gradient Descent(14/49): loss=2.189341975201076e+31\n",
      "Gradient Descent(15/49): loss=4.9269488674554816e+33\n",
      "Gradient Descent(16/49): loss=1.1087726803414794e+36\n",
      "Gradient Descent(17/49): loss=2.495209305514106e+38\n",
      "Gradient Descent(18/49): loss=5.615280395111402e+40\n",
      "Gradient Descent(19/49): loss=1.2636765129776493e+43\n",
      "Gradient Descent(20/49): loss=2.8438087104501562e+45\n",
      "Gradient Descent(21/49): loss=6.399777078535557e+47\n",
      "Gradient Descent(22/49): loss=1.440221576993514e+50\n",
      "Gradient Descent(23/49): loss=3.241110065877133e+52\n",
      "Gradient Descent(24/49): loss=7.293873822595424e+54\n",
      "Gradient Descent(25/49): loss=1.6414313077520738e+57\n",
      "Gradient Descent(26/49): loss=3.6939173936945528e+59\n",
      "Gradient Descent(27/49): loss=8.312882572057081e+61\n",
      "Gradient Descent(28/49): loss=1.870751543463673e+64\n",
      "Gradient Descent(29/49): loss=4.209985293351426e+66\n",
      "Gradient Descent(30/49): loss=9.474254468564853e+68\n",
      "Gradient Descent(31/49): loss=2.1321095319947157e+71\n",
      "Gradient Descent(32/49): loss=4.7981517400717597e+73\n",
      "Gradient Descent(33/49): loss=1.0797878708986906e+76\n",
      "Gradient Descent(34/49): loss=2.429981187136223e+78\n",
      "Gradient Descent(35/49): loss=5.468489440357831e+80\n",
      "Gradient Descent(36/49): loss=1.2306423159822012e+83\n",
      "Gradient Descent(37/49): loss=2.7694677413273708e+85\n",
      "Gradient Descent(38/49): loss=6.23247833317951e+87\n",
      "Gradient Descent(39/49): loss=1.4025722558131275e+90\n",
      "Gradient Descent(40/49): loss=3.156383107349128e+92\n",
      "Gradient Descent(41/49): loss=7.103202190879757e+94\n",
      "Gradient Descent(42/49): loss=1.5985220947052153e+97\n",
      "Gradient Descent(43/49): loss=3.5973534451005973e+99\n",
      "Gradient Descent(44/49): loss=8.09557268669699e+101\n",
      "Gradient Descent(45/49): loss=1.821847592286309e+104\n",
      "Gradient Descent(46/49): loss=4.0999306385990224e+106\n",
      "Gradient Descent(47/49): loss=9.226584766197759e+108\n",
      "Gradient Descent(48/49): loss=2.0763733329138004e+111\n",
      "Gradient Descent(49/49): loss=4.6727216265659156e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.897668302728442\n",
      "Gradient Descent(2/49): loss=1333.428726520793\n",
      "Gradient Descent(3/49): loss=293271.18968950945\n",
      "Gradient Descent(4/49): loss=65490411.57552057\n",
      "Gradient Descent(5/49): loss=14688676402.331217\n",
      "Gradient Descent(6/49): loss=3299061288102.9683\n",
      "Gradient Descent(7/49): loss=741306713214765.1\n",
      "Gradient Descent(8/49): loss=1.6659927763530186e+17\n",
      "Gradient Descent(9/49): loss=3.744306633634639e+19\n",
      "Gradient Descent(10/49): loss=8.415455618707788e+21\n",
      "Gradient Descent(11/49): loss=1.8914139553893712e+24\n",
      "Gradient Descent(12/49): loss=4.251052645843246e+26\n",
      "Gradient Descent(13/49): loss=9.554472200409718e+28\n",
      "Gradient Descent(14/49): loss=2.147420181530657e+31\n",
      "Gradient Descent(15/49): loss=4.826445403529088e+33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=1.084770281222029e+36\n",
      "Gradient Descent(17/49): loss=2.4380811901417504e+38\n",
      "Gradient Descent(18/49): loss=5.479722309956099e+40\n",
      "Gradient Descent(19/49): loss=1.231597895631968e+43\n",
      "Gradient Descent(20/49): loss=2.768084387085013e+45\n",
      "Gradient Descent(21/49): loss=6.221422756835038e+47\n",
      "Gradient Descent(22/49): loss=1.3982991740361501e+50\n",
      "Gradient Descent(23/49): loss=3.1427547308151217e+52\n",
      "Gradient Descent(24/49): loss=7.063515077114015e+54\n",
      "Gradient Descent(25/49): loss=1.5875640805027237e+57\n",
      "Gradient Descent(26/49): loss=3.5681380759988734e+59\n",
      "Gradient Descent(27/49): loss=8.019587672558359e+61\n",
      "Gradient Descent(28/49): loss=1.8024466841813822e+64\n",
      "Gradient Descent(29/49): loss=4.051098612505262e+66\n",
      "Gradient Descent(30/49): loss=9.105068189961917e+68\n",
      "Gradient Descent(31/49): loss=2.0464144340487616e+71\n",
      "Gradient Descent(32/49): loss=4.599429623712306e+73\n",
      "Gradient Descent(33/49): loss=1.0337472464767708e+76\n",
      "Gradient Descent(34/49): loss=2.323404110998855e+78\n",
      "Gradient Descent(35/49): loss=5.221979242415949e+80\n",
      "Gradient Descent(36/49): loss=1.173668716480832e+83\n",
      "Gradient Descent(37/49): loss=2.6378853536163527e+85\n",
      "Gradient Descent(38/49): loss=5.928793228542449e+87\n",
      "Gradient Descent(39/49): loss=1.3325290691129553e+90\n",
      "Gradient Descent(40/49): loss=2.9949327824130466e+92\n",
      "Gradient Descent(41/49): loss=6.731277072359307e+94\n",
      "Gradient Descent(42/49): loss=1.5128917513922824e+97\n",
      "Gradient Descent(43/49): loss=3.4003078863437233e+99\n",
      "Gradient Descent(44/49): loss=7.642380038949231e+101\n",
      "Gradient Descent(45/49): loss=1.7176671822660283e+104\n",
      "Gradient Descent(46/49): loss=3.860551992961831e+106\n",
      "Gradient Descent(47/49): loss=8.676804123776544e+108\n",
      "Gradient Descent(48/49): loss=1.9501597165286575e+111\n",
      "Gradient Descent(49/49): loss=4.38309182242533e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.987955761628003\n",
      "Gradient Descent(2/49): loss=1378.0358344869876\n",
      "Gradient Descent(3/49): loss=309109.7626299087\n",
      "Gradient Descent(4/49): loss=70355239.45721094\n",
      "Gradient Descent(5/49): loss=16077110972.107758\n",
      "Gradient Descent(6/49): loss=3678265654240.725\n",
      "Gradient Descent(7/49): loss=841867545711960.1\n",
      "Gradient Descent(8/49): loss=1.9270719327320646e+17\n",
      "Gradient Descent(9/49): loss=4.411329780122114e+19\n",
      "Gradient Descent(10/49): loss=1.0098266736871546e+22\n",
      "Gradient Descent(11/49): loss=2.3116710006150097e+24\n",
      "Gradient Descent(12/49): loss=5.291829311367941e+26\n",
      "Gradient Descent(13/49): loss=1.211395155183283e+29\n",
      "Gradient Descent(14/49): loss=2.7731023816047495e+31\n",
      "Gradient Descent(15/49): loss=6.348132707013184e+33\n",
      "Gradient Descent(16/49): loss=1.453202370005179e+36\n",
      "Gradient Descent(17/49): loss=3.3266430258006953e+38\n",
      "Gradient Descent(18/49): loss=7.615287498349829e+40\n",
      "Gradient Descent(19/49): loss=1.743277029329144e+43\n",
      "Gradient Descent(20/49): loss=3.99067638908109e+45\n",
      "Gradient Descent(21/49): loss=9.135379962820697e+47\n",
      "Gradient Descent(22/49): loss=2.0912536855954326e+50\n",
      "Gradient Descent(23/49): loss=4.7872578867492484e+52\n",
      "Gradient Descent(24/49): loss=1.0958899071955101e+55\n",
      "Gradient Descent(25/49): loss=2.5086901877963803e+57\n",
      "Gradient Descent(26/49): loss=5.742845533136753e+59\n",
      "Gradient Descent(27/49): loss=1.3146412011297894e+62\n",
      "Gradient Descent(28/49): loss=3.0094514604923154e+64\n",
      "Gradient Descent(29/49): loss=6.889178648346076e+66\n",
      "Gradient Descent(30/49): loss=1.5770575824826055e+69\n",
      "Gradient Descent(31/49): loss=3.610170015061194e+71\n",
      "Gradient Descent(32/49): loss=8.264332058902916e+73\n",
      "Gradient Descent(33/49): loss=1.8918550676249315e+76\n",
      "Gradient Descent(34/49): loss=4.330798389256957e+78\n",
      "Gradient Descent(35/49): loss=9.913980732116579e+80\n",
      "Gradient Descent(36/49): loss=2.269489482599567e+83\n",
      "Gradient Descent(37/49): loss=5.195271859813709e+85\n",
      "Gradient Descent(38/49): loss=1.1892916845094132e+88\n",
      "Gradient Descent(39/49): loss=2.722503747655566e+90\n",
      "Gradient Descent(40/49): loss=6.232303439551997e+92\n",
      "Gradient Descent(41/49): loss=1.4266869676891836e+95\n",
      "Gradient Descent(42/49): loss=3.2659444834741043e+97\n",
      "Gradient Descent(43/49): loss=7.476337564372233e+99\n",
      "Gradient Descent(44/49): loss=1.7114688770516118e+102\n",
      "Gradient Descent(45/49): loss=3.917861776432862e+104\n",
      "Gradient Descent(46/49): loss=8.96869414632705e+106\n",
      "Gradient Descent(47/49): loss=2.053096287730639e+109\n",
      "Gradient Descent(48/49): loss=4.6999087023383304e+111\n",
      "Gradient Descent(49/49): loss=1.0758940991867185e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.957198652198207\n",
      "Gradient Descent(2/49): loss=1368.6287139474716\n",
      "Gradient Descent(3/49): loss=306951.7350158546\n",
      "Gradient Descent(4/49): loss=69943274.69310622\n",
      "Gradient Descent(5/49): loss=16009292669.72153\n",
      "Gradient Descent(6/49): loss=3669481846367.338\n",
      "Gradient Descent(7/49): loss=841459631526677.9\n",
      "Gradient Descent(8/49): loss=1.9298623501853027e+17\n",
      "Gradient Descent(9/49): loss=4.426299551611048e+19\n",
      "Gradient Descent(10/49): loss=1.0152252502645127e+22\n",
      "Gradient Descent(11/49): loss=2.328554487842163e+24\n",
      "Gradient Descent(12/49): loss=5.340860023610118e+26\n",
      "Gradient Descent(13/49): loss=1.2250004646026502e+29\n",
      "Gradient Descent(14/49): loss=2.8097093717842167e+31\n",
      "Gradient Descent(15/49): loss=6.444460654238098e+33\n",
      "Gradient Descent(16/49): loss=1.4781270438157198e+36\n",
      "Gradient Descent(17/49): loss=3.3902908119826534e+38\n",
      "Gradient Descent(18/49): loss=7.776105489307722e+40\n",
      "Gradient Descent(19/49): loss=1.7835584020998102e+43\n",
      "Gradient Descent(20/49): loss=4.090840304298652e+45\n",
      "Gradient Descent(21/49): loss=9.382913605283711e+47\n",
      "Gradient Descent(22/49): loss=2.152102286528593e+50\n",
      "Gradient Descent(23/49): loss=4.936147178338271e+52\n",
      "Gradient Descent(24/49): loss=1.1321742985370247e+55\n",
      "Gradient Descent(25/49): loss=2.596799884522198e+57\n",
      "Gradient Descent(26/49): loss=5.956123230293158e+59\n",
      "Gradient Descent(27/49): loss=1.3661200520643828e+62\n",
      "Gradient Descent(28/49): loss=3.133387145451347e+64\n",
      "Gradient Descent(29/49): loss=7.18686105839928e+66\n",
      "Gradient Descent(30/49): loss=1.6484069626607293e+69\n",
      "Gradient Descent(31/49): loss=3.7808516019281235e+71\n",
      "Gradient Descent(32/49): loss=8.671911220715059e+73\n",
      "Gradient Descent(33/49): loss=1.9890239590893565e+76\n",
      "Gradient Descent(34/49): loss=4.562104257226555e+78\n",
      "Gradient Descent(35/49): loss=1.0463823303231318e+81\n",
      "Gradient Descent(36/49): loss=2.4000240228576093e+83\n",
      "Gradient Descent(37/49): loss=5.5047903078742476e+85\n",
      "Gradient Descent(38/49): loss=1.2626005425389888e+88\n",
      "Gradient Descent(39/49): loss=2.895950691780942e+90\n",
      "Gradient Descent(40/49): loss=6.642267389147385e+92\n",
      "Gradient Descent(41/49): loss=1.5234967982758684e+95\n",
      "Gradient Descent(42/49): loss=3.494352693703856e+97\n",
      "Gradient Descent(43/49): loss=8.014785959388882e+99\n",
      "Gradient Descent(44/49): loss=1.8383031023330675e+102\n",
      "Gradient Descent(45/49): loss=4.216404921067979e+104\n",
      "Gradient Descent(46/49): loss=9.670913591911729e+106\n",
      "Gradient Descent(47/49): loss=2.2181591059933875e+109\n",
      "Gradient Descent(48/49): loss=5.087657719966018e+111\n",
      "Gradient Descent(49/49): loss=1.1669253574097212e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.986781079220739\n",
      "Gradient Descent(2/49): loss=1364.029038563749\n",
      "Gradient Descent(3/49): loss=303301.35556604597\n",
      "Gradient Descent(4/49): loss=68484307.43410991\n",
      "Gradient Descent(5/49): loss=15529829297.399145\n",
      "Gradient Descent(6/49): loss=3526279725108.941\n",
      "Gradient Descent(7/49): loss=801036754292203.1\n",
      "Gradient Descent(8/49): loss=1.819908075545864e+17\n",
      "Gradient Descent(9/49): loss=4.1349183752641585e+19\n",
      "Gradient Descent(10/49): loss=9.394881422339693e+21\n",
      "Gradient Descent(11/49): loss=2.1346072357274303e+24\n",
      "Gradient Descent(12/49): loss=4.850041223512816e+26\n",
      "Gradient Descent(13/49): loss=1.1019785598501301e+29\n",
      "Gradient Descent(14/49): loss=2.503807570895658e+31\n",
      "Gradient Descent(15/49): loss=5.688906309365414e+33\n",
      "Gradient Descent(16/49): loss=1.2925775974451748e+36\n",
      "Gradient Descent(17/49): loss=2.9368682969453425e+38\n",
      "Gradient Descent(18/49): loss=6.672864695538628e+40\n",
      "Gradient Descent(19/49): loss=1.5161430061625572e+43\n",
      "Gradient Descent(20/49): loss=3.4448317487353724e+45\n",
      "Gradient Descent(21/49): loss=7.8270095433298e+47\n",
      "Gradient Descent(22/49): loss=1.7783765031757866e+50\n",
      "Gradient Descent(23/49): loss=4.040653035588815e+52\n",
      "Gradient Descent(24/49): loss=9.180776356929825e+54\n",
      "Gradient Descent(25/49): loss=2.0859661488799038e+57\n",
      "Gradient Descent(26/49): loss=4.739528123882847e+59\n",
      "Gradient Descent(27/49): loss=1.0768691931622028e+62\n",
      "Gradient Descent(28/49): loss=2.4467567843691466e+64\n",
      "Gradient Descent(29/49): loss=5.5592812942089925e+66\n",
      "Gradient Descent(30/49): loss=1.2631254853600237e+69\n",
      "Gradient Descent(31/49): loss=2.8699501020536407e+71\n",
      "Gradient Descent(32/49): loss=6.520819731485402e+73\n",
      "Gradient Descent(33/49): loss=1.4815968382203753e+76\n",
      "Gradient Descent(34/49): loss=3.3663393275933752e+78\n",
      "Gradient Descent(35/49): loss=7.648666746693131e+80\n",
      "Gradient Descent(36/49): loss=1.7378551984476485e+83\n",
      "Gradient Descent(37/49): loss=3.948584492947426e+85\n",
      "Gradient Descent(38/49): loss=8.971587225375265e+87\n",
      "Gradient Descent(39/49): loss=2.038436241804601e+90\n",
      "Gradient Descent(40/49): loss=4.6315353209182757e+92\n",
      "Gradient Descent(41/49): loss=1.0523321254297909e+95\n",
      "Gradient Descent(42/49): loss=2.3910060605821187e+97\n",
      "Gradient Descent(43/49): loss=5.432609956106367e+99\n",
      "Gradient Descent(44/49): loss=1.2343444636857483e+102\n",
      "Gradient Descent(45/49): loss=2.80455668148802e+104\n",
      "Gradient Descent(46/49): loss=6.37223920152133e+106\n",
      "Gradient Descent(47/49): loss=1.4478378244030024e+109\n",
      "Gradient Descent(48/49): loss=3.289635400490868e+111\n",
      "Gradient Descent(49/49): loss=7.474387590768214e+113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.961140991059627\n",
      "Gradient Descent(2/49): loss=1359.1636308074008\n",
      "Gradient Descent(3/49): loss=301793.561640522\n",
      "Gradient Descent(4/49): loss=68039296.03447403\n",
      "Gradient Descent(5/49): loss=15406651362.63057\n",
      "Gradient Descent(6/49): loss=3493496473648.85\n",
      "Gradient Descent(7/49): loss=792524213996627.1\n",
      "Gradient Descent(8/49): loss=1.7981774453717994e+17\n",
      "Gradient Descent(9/49): loss=4.0801461763671155e+19\n",
      "Gradient Descent(10/49): loss=9.258206335251092e+21\n",
      "Gradient Descent(11/49): loss=2.1007807354694796e+24\n",
      "Gradient Descent(12/49): loss=4.7668944922452785e+26\n",
      "Gradient Descent(13/49): loss=1.0816597710630956e+29\n",
      "Gradient Descent(14/49): loss=2.4544033095947965e+31\n",
      "Gradient Descent(15/49): loss=5.569307745588867e+33\n",
      "Gradient Descent(16/49): loss=1.2637364687765685e+36\n",
      "Gradient Descent(17/49): loss=2.8675554526894374e+38\n",
      "Gradient Descent(18/49): loss=6.5067951327635235e+40\n",
      "Gradient Descent(19/49): loss=1.4764625706913074e+43\n",
      "Gradient Descent(20/49): loss=3.3502541252860177e+45\n",
      "Gradient Descent(21/49): loss=7.602090922230402e+47\n",
      "Gradient Descent(22/49): loss=1.7249970966869958e+50\n",
      "Gradient Descent(23/49): loss=3.9142059915774753e+52\n",
      "Gradient Descent(24/49): loss=8.881759032533827e+54\n",
      "Gradient Descent(25/49): loss=2.0153677062961584e+57\n",
      "Gradient Descent(26/49): loss=4.573088480228271e+59\n",
      "Gradient Descent(27/49): loss=1.037683504735511e+62\n",
      "Gradient Descent(28/49): loss=2.3546167117818667e+64\n",
      "Gradient Descent(29/49): loss=5.3428813642130555e+66\n",
      "Gradient Descent(30/49): loss=1.2123578809755692e+69\n",
      "Gradient Descent(31/49): loss=2.750971865870859e+71\n",
      "Gradient Descent(32/49): loss=6.242254309200534e+73\n",
      "Gradient Descent(33/49): loss=1.4164353821334873e+76\n",
      "Gradient Descent(34/49): loss=3.214045907746109e+78\n",
      "Gradient Descent(35/49): loss=7.29301966570473e+80\n",
      "Gradient Descent(36/49): loss=1.6548654677323688e+83\n",
      "Gradient Descent(37/49): loss=3.755069699278607e+85\n",
      "Gradient Descent(38/49): loss=8.520661480574645e+87\n",
      "Gradient Descent(39/49): loss=1.933430745120285e+90\n",
      "Gradient Descent(40/49): loss=4.387164605352064e+92\n",
      "Gradient Descent(41/49): loss=9.954953557571812e+94\n",
      "Gradient Descent(42/49): loss=2.2588872141363155e+97\n",
      "Gradient Descent(43/49): loss=5.125660724260708e+99\n",
      "Gradient Descent(44/49): loss=1.1630681556747915e+102\n",
      "Gradient Descent(45/49): loss=2.6391281193116e+104\n",
      "Gradient Descent(46/49): loss=5.988468686171072e+106\n",
      "Gradient Descent(47/49): loss=1.35884866455842e+109\n",
      "Gradient Descent(48/49): loss=3.083375383486902e+111\n",
      "Gradient Descent(49/49): loss=6.996514036816965e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.052312556410765\n",
      "Gradient Descent(2/49): loss=1404.6149941500394\n",
      "Gradient Descent(3/49): loss=318086.8392465733\n",
      "Gradient Descent(4/49): loss=73091819.82909799\n",
      "Gradient Descent(5/49): loss=16862486840.320549\n",
      "Gradient Descent(6/49): loss=3894921991512.2417\n",
      "Gradient Descent(7/49): loss=899998538093888.5\n",
      "Gradient Descent(8/49): loss=2.079880806837147e+17\n",
      "Gradient Descent(9/49): loss=4.806761548193734e+19\n",
      "Gradient Descent(10/49): loss=1.1108935359591107e+22\n",
      "Gradient Descent(11/49): loss=2.567403815782462e+24\n",
      "Gradient Descent(12/49): loss=5.9335764608764134e+26\n",
      "Gradient Descent(13/49): loss=1.3713209380331022e+29\n",
      "Gradient Descent(14/49): loss=3.169288230715596e+31\n",
      "Gradient Descent(15/49): loss=7.324608061295776e+33\n",
      "Gradient Descent(16/49): loss=1.692805457824127e+36\n",
      "Gradient Descent(17/49): loss=3.912278049182206e+38\n",
      "Gradient Descent(18/49): loss=9.04174752679682e+40\n",
      "Gradient Descent(19/49): loss=2.089657160327641e+43\n",
      "Gradient Descent(20/49): loss=4.829450319881094e+45\n",
      "Gradient Descent(21/49): loss=1.1161443530781518e+48\n",
      "Gradient Descent(22/49): loss=2.579544532855023e+50\n",
      "Gradient Descent(23/49): loss=5.961639261693505e+52\n",
      "Gradient Descent(24/49): loss=1.3778069048224335e+55\n",
      "Gradient Descent(25/49): loss=3.1842783228694384e+57\n",
      "Gradient Descent(26/49): loss=7.359252157910692e+59\n",
      "Gradient Descent(27/49): loss=1.7008121411608748e+62\n",
      "Gradient Descent(28/49): loss=3.930782472796238e+64\n",
      "Gradient Descent(29/49): loss=9.084513494768518e+66\n",
      "Gradient Descent(30/49): loss=2.099540893137348e+69\n",
      "Gradient Descent(31/49): loss=4.852292821728357e+71\n",
      "Gradient Descent(32/49): loss=1.1214235314375568e+74\n",
      "Gradient Descent(33/49): loss=2.5917453522805703e+76\n",
      "Gradient Descent(34/49): loss=5.9898368303875055e+78\n",
      "Gradient Descent(35/49): loss=1.3843237038351942e+81\n",
      "Gradient Descent(36/49): loss=3.1993394332179115e+83\n",
      "Gradient Descent(37/49): loss=7.394060204694585e+85\n",
      "Gradient Descent(38/49): loss=1.7088567015741246e+88\n",
      "Gradient Descent(39/49): loss=3.949374424434256e+90\n",
      "Gradient Descent(40/49): loss=9.127481742622197e+92\n",
      "Gradient Descent(41/49): loss=2.109471374668046e+95\n",
      "Gradient Descent(42/49): loss=4.875243365061528e+97\n",
      "Gradient Descent(43/49): loss=1.1267276794555535e+100\n",
      "Gradient Descent(44/49): loss=2.6040038795792e+102\n",
      "Gradient Descent(45/49): loss=6.018167768932513e+104\n",
      "Gradient Descent(46/49): loss=1.3908713262313097e+107\n",
      "Gradient Descent(47/49): loss=3.214471780130479e+109\n",
      "Gradient Descent(48/49): loss=7.429032887789137e+111\n",
      "Gradient Descent(49/49): loss=1.7169393114290256e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.02122316952313\n",
      "Gradient Descent(2/49): loss=1395.027872077821\n",
      "Gradient Descent(3/49): loss=315866.0502469666\n",
      "Gradient Descent(4/49): loss=72663716.89380261\n",
      "Gradient Descent(5/49): loss=16791312991.014078\n",
      "Gradient Descent(6/49): loss=3885607990490.688\n",
      "Gradient Descent(7/49): loss=899558899884137.6\n",
      "Gradient Descent(8/49): loss=2.0828829855547834e+17\n",
      "Gradient Descent(9/49): loss=4.823048356157375e+19\n",
      "Gradient Descent(10/49): loss=1.1168260174472871e+22\n",
      "Gradient Descent(11/49): loss=2.586138752980743e+24\n",
      "Gradient Descent(12/49): loss=5.988511968188338e+26\n",
      "Gradient Descent(13/49): loss=1.3867120490585507e+29\n",
      "Gradient Descent(14/49): loss=3.211099374334857e+31\n",
      "Gradient Descent(15/49): loss=7.435689269441381e+33\n",
      "Gradient Descent(16/49): loss=1.7218238908871633e+36\n",
      "Gradient Descent(17/49): loss=3.987091749920506e+38\n",
      "Gradient Descent(18/49): loss=9.232593848579393e+40\n",
      "Gradient Descent(19/49): loss=2.137918929209053e+43\n",
      "Gradient Descent(20/49): loss=4.9506102230361596e+45\n",
      "Gradient Descent(21/49): loss=1.1463737585160869e+48\n",
      "Gradient Descent(22/49): loss=2.654567285749386e+50\n",
      "Gradient Descent(23/49): loss=6.146972069361304e+52\n",
      "Gradient Descent(24/49): loss=1.4234058343327477e+55\n",
      "Gradient Descent(25/49): loss=3.296068611265826e+57\n",
      "Gradient Descent(26/49): loss=7.6324460867964555e+59\n",
      "Gradient Descent(27/49): loss=1.7673853350246416e+62\n",
      "Gradient Descent(28/49): loss=4.092594807664577e+64\n",
      "Gradient Descent(29/49): loss=9.47689896922779e+66\n",
      "Gradient Descent(30/49): loss=2.1944907398297136e+69\n",
      "Gradient Descent(31/49): loss=5.081609103184102e+71\n",
      "Gradient Descent(32/49): loss=1.1767081359189147e+74\n",
      "Gradient Descent(33/49): loss=2.724810210746361e+76\n",
      "Gradient Descent(34/49): loss=6.309628069996844e+78\n",
      "Gradient Descent(35/49): loss=1.4610708013600348e+81\n",
      "Gradient Descent(36/49): loss=3.3832864043727067e+83\n",
      "Gradient Descent(37/49): loss=7.834409450492075e+85\n",
      "Gradient Descent(38/49): loss=1.81415239805392e+88\n",
      "Gradient Descent(39/49): loss=4.20088960650132e+90\n",
      "Gradient Descent(40/49): loss=9.727668692520902e+92\n",
      "Gradient Descent(41/49): loss=2.2525595065627235e+95\n",
      "Gradient Descent(42/49): loss=5.216074365800774e+97\n",
      "Gradient Descent(43/49): loss=1.2078451961112e+100\n",
      "Gradient Descent(44/49): loss=2.7969118449194884e+102\n",
      "Gradient Descent(45/49): loss=6.476588136821761e+104\n",
      "Gradient Descent(46/49): loss=1.499732427041428e+107\n",
      "Gradient Descent(47/49): loss=3.4728120813057073e+109\n",
      "Gradient Descent(48/49): loss=8.0417169987148e+111\n",
      "Gradient Descent(49/49): loss=1.8621569717387085e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.051141894097533\n",
      "Gradient Descent(2/49): loss=1390.3462488746839\n",
      "Gradient Descent(3/49): loss=312112.1052965293\n",
      "Gradient Descent(4/49): loss=71148778.08560148\n",
      "Gradient Descent(5/49): loss=16288654085.902225\n",
      "Gradient Descent(6/49): loss=3734033800332.6123\n",
      "Gradient Descent(7/49): loss=856361685789547.6\n",
      "Gradient Descent(8/49): loss=1.964254113732085e+17\n",
      "Gradient Descent(9/49): loss=4.505662395928378e+19\n",
      "Gradient Descent(10/49): loss=1.0335381633911669e+22\n",
      "Gradient Descent(11/49): loss=2.3708097669077616e+24\n",
      "Gradient Descent(12/49): loss=5.438356574014292e+26\n",
      "Gradient Descent(13/49): loss=1.247495289396201e+29\n",
      "Gradient Descent(14/49): loss=2.861608638624449e+31\n",
      "Gradient Descent(15/49): loss=6.56419678263593e+33\n",
      "Gradient Descent(16/49): loss=1.5057502909634154e+36\n",
      "Gradient Descent(17/49): loss=3.4540158248980664e+38\n",
      "Gradient Descent(18/49): loss=7.923110107799156e+40\n",
      "Gradient Descent(19/49): loss=1.8174692016084358e+43\n",
      "Gradient Descent(20/49): loss=4.169062722648908e+45\n",
      "Gradient Descent(21/49): loss=9.563344441687261e+47\n",
      "Gradient Descent(22/49): loss=2.1937198597808957e+50\n",
      "Gradient Descent(23/49): loss=5.032137922665277e+52\n",
      "Gradient Descent(24/49): loss=1.1543138454950011e+55\n",
      "Gradient Descent(25/49): loss=2.6478615538372698e+57\n",
      "Gradient Descent(26/49): loss=6.073886088827112e+59\n",
      "Gradient Descent(27/49): loss=1.3932787447511812e+62\n",
      "Gradient Descent(28/49): loss=3.1960192077793375e+64\n",
      "Gradient Descent(29/49): loss=7.331295919768605e+66\n",
      "Gradient Descent(30/49): loss=1.6817139187521146e+69\n",
      "Gradient Descent(31/49): loss=3.8576559116902606e+71\n",
      "Gradient Descent(32/49): loss=8.849013477893632e+73\n",
      "Gradient Descent(33/49): loss=2.0298606543587128e+76\n",
      "Gradient Descent(34/49): loss=4.656263985140132e+78\n",
      "Gradient Descent(35/49): loss=1.0680927408863248e+81\n",
      "Gradient Descent(36/49): loss=2.4500803794090173e+83\n",
      "Gradient Descent(37/49): loss=5.620199104231019e+85\n",
      "Gradient Descent(38/49): loss=1.2892082331934856e+88\n",
      "Gradient Descent(39/49): loss=2.9572935721843694e+90\n",
      "Gradient Descent(40/49): loss=6.783687108807361e+92\n",
      "Gradient Descent(41/49): loss=1.5560988338471998e+95\n",
      "Gradient Descent(42/49): loss=3.569509533476004e+97\n",
      "Gradient Descent(43/49): loss=8.188039237890189e+99\n",
      "Gradient Descent(44/49): loss=1.8782408600528865e+102\n",
      "Gradient Descent(45/49): loss=4.30846583153555e+104\n",
      "Gradient Descent(46/49): loss=9.88311894193725e+106\n",
      "Gradient Descent(47/49): loss=2.2670724067380436e+109\n",
      "Gradient Descent(48/49): loss=5.200400124280602e+111\n",
      "Gradient Descent(49/49): loss=1.1929112353111788e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.024924247183693\n",
      "Gradient Descent(2/49): loss=1385.2694255151503\n",
      "Gradient Descent(3/49): loss=310521.328530142\n",
      "Gradient Descent(4/49): loss=70674564.57330495\n",
      "Gradient Descent(5/49): loss=16156058264.816534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=3698384987075.785\n",
      "Gradient Descent(7/49): loss=847011576208616.2\n",
      "Gradient Descent(8/49): loss=1.9401463529810282e+17\n",
      "Gradient Descent(9/49): loss=4.444294871931879e+19\n",
      "Gradient Descent(10/49): loss=1.0180737032608046e+22\n",
      "Gradient Descent(11/49): loss=2.3321598819160446e+24\n",
      "Gradient Descent(12/49): loss=5.3424241762970255e+26\n",
      "Gradient Descent(13/49): loss=1.2238233636506068e+29\n",
      "Gradient Descent(14/49): loss=2.803491380873655e+31\n",
      "Gradient Descent(15/49): loss=6.422139711866161e+33\n",
      "Gradient Descent(16/49): loss=1.4711612822845877e+36\n",
      "Gradient Descent(17/49): loss=3.370084798139559e+38\n",
      "Gradient Descent(18/49): loss=7.720072384276195e+40\n",
      "Gradient Descent(19/49): loss=1.7684871824738781e+43\n",
      "Gradient Descent(20/49): loss=4.0511885902827654e+45\n",
      "Gradient Descent(21/49): loss=9.280321146313431e+47\n",
      "Gradient Descent(22/49): loss=2.1259035135945113e+50\n",
      "Gradient Descent(23/49): loss=4.86994542315954e+52\n",
      "Gradient Descent(24/49): loss=1.115590066672319e+55\n",
      "Gradient Descent(25/49): loss=2.5555547110282407e+57\n",
      "Gradient Descent(26/49): loss=5.854175360797425e+59\n",
      "Gradient Descent(27/49): loss=1.3410540188038816e+62\n",
      "Gradient Descent(28/49): loss=3.0720396477929043e+64\n",
      "Gradient Descent(29/49): loss=7.037320991759369e+66\n",
      "Gradient Descent(30/49): loss=1.6120848823236375e+69\n",
      "Gradient Descent(31/49): loss=3.692907671626211e+71\n",
      "Gradient Descent(32/49): loss=8.459583748157669e+73\n",
      "Gradient Descent(33/49): loss=1.9378918607130786e+76\n",
      "Gradient Descent(34/49): loss=4.439254903807569e+78\n",
      "Gradient Descent(35/49): loss=1.0169289886860936e+81\n",
      "Gradient Descent(36/49): loss=2.329545363892338e+83\n",
      "Gradient Descent(37/49): loss=5.3364410522349624e+85\n",
      "Gradient Descent(38/49): loss=1.2224532539858604e+88\n",
      "Gradient Descent(39/49): loss=2.800353163377962e+90\n",
      "Gradient Descent(40/49): loss=6.414951094507576e+92\n",
      "Gradient Descent(41/49): loss=1.469514562773377e+95\n",
      "Gradient Descent(42/49): loss=3.3663125694784267e+97\n",
      "Gradient Descent(43/49): loss=7.711431109632392e+99\n",
      "Gradient Descent(44/49): loss=1.766507670671234e+102\n",
      "Gradient Descent(45/49): loss=4.046653994797939e+104\n",
      "Gradient Descent(46/49): loss=9.269933454289388e+106\n",
      "Gradient Descent(47/49): loss=2.123523937490598e+109\n",
      "Gradient Descent(48/49): loss=4.864494373483363e+111\n",
      "Gradient Descent(49/49): loss=1.1143413592791685e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.11698419782529\n",
      "Gradient Descent(2/49): loss=1431.5769692126007\n",
      "Gradient Descent(3/49): loss=327280.13597364584\n",
      "Gradient Descent(4/49): loss=75921090.02800846\n",
      "Gradient Descent(5/49): loss=17682225108.148064\n",
      "Gradient Descent(6/49): loss=4123219485146.166\n",
      "Gradient Descent(7/49): loss=961838586925943.6\n",
      "Gradient Descent(8/49): loss=2.2439938447087053e+17\n",
      "Gradient Descent(9/49): loss=5.235505987983824e+19\n",
      "Gradient Descent(10/49): loss=1.2215223224411842e+22\n",
      "Gradient Descent(11/49): loss=2.8500077620181973e+24\n",
      "Gradient Descent(12/49): loss=6.649535338950573e+26\n",
      "Gradient Descent(13/49): loss=1.5514463360862691e+29\n",
      "Gradient Descent(14/49): loss=3.619780906612411e+31\n",
      "Gradient Descent(15/49): loss=8.445547985287296e+33\n",
      "Gradient Descent(16/49): loss=1.9704861649033414e+36\n",
      "Gradient Descent(17/49): loss=4.597470471068379e+38\n",
      "Gradient Descent(18/49): loss=1.0726659819827065e+41\n",
      "Gradient Descent(19/49): loss=2.5027073405442427e+43\n",
      "Gradient Descent(20/49): loss=5.83923060752151e+45\n",
      "Gradient Descent(21/49): loss=1.3623891830116007e+48\n",
      "Gradient Descent(22/49): loss=3.178679539793294e+50\n",
      "Gradient Descent(23/49): loss=7.416385672143814e+52\n",
      "Gradient Descent(24/49): loss=1.7303655731739106e+55\n",
      "Gradient Descent(25/49): loss=4.037229385297383e+57\n",
      "Gradient Descent(26/49): loss=9.419524615030264e+59\n",
      "Gradient Descent(27/49): loss=2.1977310552700435e+62\n",
      "Gradient Descent(28/49): loss=5.127670438475734e+64\n",
      "Gradient Descent(29/49): loss=1.1963704140490232e+67\n",
      "Gradient Descent(30/49): loss=2.7913302634896045e+69\n",
      "Gradient Descent(31/49): loss=6.512635675687708e+71\n",
      "Gradient Descent(32/49): loss=1.5195057352767572e+74\n",
      "Gradient Descent(33/49): loss=3.545258470634713e+76\n",
      "Gradient Descent(34/49): loss=8.271675013663505e+78\n",
      "Gradient Descent(35/49): loss=1.9299187378971483e+81\n",
      "Gradient Descent(36/49): loss=4.5028199593602106e+83\n",
      "Gradient Descent(37/49): loss=1.0505824513888593e+86\n",
      "Gradient Descent(38/49): loss=2.451182807946527e+88\n",
      "Gradient Descent(39/49): loss=5.7190153424224105e+90\n",
      "Gradient Descent(40/49): loss=1.3343409712580003e+93\n",
      "Gradient Descent(41/49): loss=3.1132384177581027e+95\n",
      "Gradient Descent(42/49): loss=7.263700699130305e+97\n",
      "Gradient Descent(43/49): loss=1.6947416409097466e+100\n",
      "Gradient Descent(44/49): loss=3.954112852939186e+102\n",
      "Gradient Descent(45/49): loss=9.225599983125388e+104\n",
      "Gradient Descent(46/49): loss=2.1524852277642402e+107\n",
      "Gradient Descent(47/49): loss=5.022104431384268e+109\n",
      "Gradient Descent(48/49): loss=1.1717401166988241e+112\n",
      "Gradient Descent(49/49): loss=2.7338637016413776e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.085560798183381\n",
      "Gradient Descent(2/49): loss=1421.8072752152866\n",
      "Gradient Descent(3/49): loss=324995.07220778964\n",
      "Gradient Descent(4/49): loss=75476298.53432335\n",
      "Gradient Descent(5/49): loss=17607547053.145504\n",
      "Gradient Descent(6/49): loss=4113346119435.1104\n",
      "Gradient Descent(7/49): loss=961364974808323.2\n",
      "Gradient Descent(8/49): loss=2.2472227502199283e+17\n",
      "Gradient Descent(9/49): loss=5.253218701259899e+19\n",
      "Gradient Descent(10/49): loss=1.2280386187690527e+22\n",
      "Gradient Descent(11/49): loss=2.870786984411439e+24\n",
      "Gradient Descent(12/49): loss=6.71105372924467e+26\n",
      "Gradient Descent(13/49): loss=1.5688475067685425e+29\n",
      "Gradient Descent(14/49): loss=3.6675060408797427e+31\n",
      "Gradient Descent(15/49): loss=8.573555705230905e+33\n",
      "Gradient Descent(16/49): loss=2.0042464365929953e+36\n",
      "Gradient Descent(17/49): loss=4.685341704072794e+38\n",
      "Gradient Descent(18/49): loss=1.0952957949772475e+41\n",
      "Gradient Descent(19/49): loss=2.5604810818722856e+43\n",
      "Gradient Descent(20/49): loss=5.985655566833612e+45\n",
      "Gradient Descent(21/49): loss=1.3992711300198344e+48\n",
      "Gradient Descent(22/49): loss=3.2710864724991465e+50\n",
      "Gradient Descent(23/49): loss=7.6468430464452e+52\n",
      "Gradient Descent(24/49): loss=1.78760815615205e+55\n",
      "Gradient Descent(25/49): loss=4.1789048114839295e+57\n",
      "Gradient Descent(26/49): loss=9.769056693633292e+59\n",
      "Gradient Descent(27/49): loss=2.28371961048681e+62\n",
      "Gradient Descent(28/49): loss=5.33866822855194e+64\n",
      "Gradient Descent(29/49): loss=1.2480244213725839e+67\n",
      "Gradient Descent(30/49): loss=2.9175159228144256e+69\n",
      "Gradient Descent(31/49): loss=6.820298556749617e+71\n",
      "Gradient Descent(32/49): loss=1.5943862393158128e+74\n",
      "Gradient Descent(33/49): loss=3.7272085070292205e+76\n",
      "Gradient Descent(34/49): loss=8.713122901030796e+78\n",
      "Gradient Descent(35/49): loss=2.0368731866030852e+81\n",
      "Gradient Descent(36/49): loss=4.76161351725199e+83\n",
      "Gradient Descent(37/49): loss=1.1131259146029021e+86\n",
      "Gradient Descent(38/49): loss=2.6021626855503944e+88\n",
      "Gradient Descent(39/49): loss=6.083094960992289e+90\n",
      "Gradient Descent(40/49): loss=1.4220496093472626e+93\n",
      "Gradient Descent(41/49): loss=3.3243358921933254e+95\n",
      "Gradient Descent(42/49): loss=7.771324608849185e+97\n",
      "Gradient Descent(43/49): loss=1.8167083030908428e+100\n",
      "Gradient Descent(44/49): loss=4.246932440270244e+102\n",
      "Gradient Descent(45/49): loss=9.928085384722278e+104\n",
      "Gradient Descent(46/49): loss=2.3208958652533694e+107\n",
      "Gradient Descent(47/49): loss=5.425575434352363e+109\n",
      "Gradient Descent(48/49): loss=1.2683407831671167e+112\n",
      "Gradient Descent(49/49): loss=2.96500963208338e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.115817638532232\n",
      "Gradient Descent(2/49): loss=1417.0426215038492\n",
      "Gradient Descent(3/49): loss=321135.125876194\n",
      "Gradient Descent(4/49): loss=73903518.56543423\n",
      "Gradient Descent(5/49): loss=17080687607.006699\n",
      "Gradient Descent(6/49): loss=3952953378524.065\n",
      "Gradient Descent(7/49): loss=915217477865566.5\n",
      "Gradient Descent(8/49): loss=2.1192808341735987e+17\n",
      "Gradient Descent(9/49): loss=4.907646827426712e+19\n",
      "Gradient Descent(10/49): loss=1.1364884005216642e+22\n",
      "Gradient Descent(11/49): loss=2.631837320533196e+24\n",
      "Gradient Descent(12/49): loss=6.094721415975777e+26\n",
      "Gradient Descent(13/49): loss=1.4113961935085903e+29\n",
      "Gradient Descent(14/49): loss=3.2684670688397325e+31\n",
      "Gradient Descent(15/49): loss=7.56901412177255e+33\n",
      "Gradient Descent(16/49): loss=1.7528087298242127e+36\n",
      "Gradient Descent(17/49): loss=4.059099927849431e+38\n",
      "Gradient Descent(18/49): loss=9.399937362081859e+40\n",
      "Gradient Descent(19/49): loss=2.176808259487342e+43\n",
      "Gradient Descent(20/49): loss=5.0409848677498207e+45\n",
      "Gradient Descent(21/49): loss=1.1673755981364549e+48\n",
      "Gradient Descent(22/49): loss=2.703372104704661e+50\n",
      "Gradient Descent(23/49): loss=6.260385045093125e+52\n",
      "Gradient Descent(24/49): loss=1.4497604989255076e+55\n",
      "Gradient Descent(25/49): loss=3.357310275819043e+57\n",
      "Gradient Descent(26/49): loss=7.774754724300376e+59\n",
      "Gradient Descent(27/49): loss=1.8004535195453005e+62\n",
      "Gradient Descent(28/49): loss=4.169434266410169e+64\n",
      "Gradient Descent(29/49): loss=9.655446204635099e+66\n",
      "Gradient Descent(30/49): loss=2.23597820360581e+69\n",
      "Gradient Descent(31/49): loss=5.178008784928258e+71\n",
      "Gradient Descent(32/49): loss=1.1991071707924796e+74\n",
      "Gradient Descent(33/49): loss=2.7768550938560645e+76\n",
      "Gradient Descent(34/49): loss=6.430554666084005e+78\n",
      "Gradient Descent(35/49): loss=1.4891678505294788e+81\n",
      "Gradient Descent(36/49): loss=3.4485685950960774e+83\n",
      "Gradient Descent(37/49): loss=7.986087901947717e+85\n",
      "Gradient Descent(38/49): loss=1.84939339957826e+88\n",
      "Gradient Descent(39/49): loss=4.282767718559001e+90\n",
      "Gradient Descent(40/49): loss=9.917900288448037e+92\n",
      "Gradient Descent(41/49): loss=2.2967565041023e+95\n",
      "Gradient Descent(42/49): loss=5.3187572830112115e+97\n",
      "Gradient Descent(43/49): loss=1.2317012702503275e+100\n",
      "Gradient Descent(44/49): loss=2.852335495703176e+102\n",
      "Gradient Descent(45/49): loss=6.605349833239036e+104\n",
      "Gradient Descent(46/49): loss=1.5296463717258014e+107\n",
      "Gradient Descent(47/49): loss=3.5423074955994303e+109\n",
      "Gradient Descent(48/49): loss=8.203165532451063e+111\n",
      "Gradient Descent(49/49): loss=1.8996635621382114e+114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.089018071100648\n",
      "Gradient Descent(2/49): loss=1411.749657861702\n",
      "Gradient Descent(3/49): loss=319458.4316492599\n",
      "Gradient Descent(4/49): loss=73398714.56699696\n",
      "Gradient Descent(5/49): loss=16938114369.443993\n",
      "Gradient Descent(6/49): loss=3914235263598.978\n",
      "Gradient Descent(7/49): loss=904960562355961.8\n",
      "Gradient Descent(8/49): loss=2.0925721455644464e+17\n",
      "Gradient Descent(9/49): loss=4.838987638425347e+19\n",
      "Gradient Descent(10/49): loss=1.1190167511332167e+22\n",
      "Gradient Descent(11/49): loss=2.5877446446868024e+24\n",
      "Gradient Descent(12/49): loss=5.984215145711706e+26\n",
      "Gradient Descent(13/49): loss=1.3838636662178783e+29\n",
      "Gradient Descent(14/49): loss=3.200217736191865e+31\n",
      "Gradient Descent(15/49): loss=7.400580510545565e+33\n",
      "Gradient Descent(16/49): loss=1.7114021014141093e+36\n",
      "Gradient Descent(17/49): loss=3.957658699604367e+38\n",
      "Gradient Descent(18/49): loss=9.152181376450948e+40\n",
      "Gradient Descent(19/49): loss=2.1164640614947168e+43\n",
      "Gradient Descent(20/49): loss=4.894374293198508e+45\n",
      "Gradient Descent(21/49): loss=1.1318358842635277e+48\n",
      "Gradient Descent(22/49): loss=2.6173978372547354e+50\n",
      "Gradient Descent(23/49): loss=6.052795757616851e+52\n",
      "Gradient Descent(24/49): loss=1.3997236477461712e+55\n",
      "Gradient Descent(25/49): loss=3.2368947648671945e+57\n",
      "Gradient Descent(26/49): loss=7.485397375191334e+59\n",
      "Gradient Descent(27/49): loss=1.7310162342220522e+62\n",
      "Gradient Descent(28/49): loss=4.003016877997056e+64\n",
      "Gradient Descent(29/49): loss=9.25707327795874e+66\n",
      "Gradient Descent(30/49): loss=2.140720568642122e+69\n",
      "Gradient Descent(31/49): loss=4.950468053352181e+71\n",
      "Gradient Descent(32/49): loss=1.144807701960164e+74\n",
      "Gradient Descent(33/49): loss=2.6473954792615187e+76\n",
      "Gradient Descent(34/49): loss=6.122166029817814e+78\n",
      "Gradient Descent(35/49): loss=1.4157656908559254e+81\n",
      "Gradient Descent(36/49): loss=3.2739923772769717e+83\n",
      "Gradient Descent(37/49): loss=7.571186500491746e+85\n",
      "Gradient Descent(38/49): loss=1.7508551767889072e+88\n",
      "Gradient Descent(39/49): loss=4.0488949121639277e+90\n",
      "Gradient Descent(40/49): loss=9.363167340780802e+92\n",
      "Gradient Descent(41/49): loss=2.1652550770849688e+95\n",
      "Gradient Descent(42/49): loss=5.007204697092692e+97\n",
      "Gradient Descent(43/49): loss=1.1579281879500758e+100\n",
      "Gradient Descent(44/49): loss=2.6777369202178864e+102\n",
      "Gradient Descent(45/49): loss=6.192331345341695e+104\n",
      "Gradient Descent(46/49): loss=1.4319915896510571e+107\n",
      "Gradient Descent(47/49): loss=3.3115151603990875e+109\n",
      "Gradient Descent(48/49): loss=7.657958843337389e+111\n",
      "Gradient Descent(49/49): loss=1.7709214907891775e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.181970685871581\n",
      "Gradient Descent(2/49): loss=1458.9254186440905\n",
      "Gradient Descent(3/49): loss=336693.79927164235\n",
      "Gradient Descent(4/49): loss=78845728.13286625\n",
      "Gradient Descent(5/49): loss=18537655742.792732\n",
      "Gradient Descent(6/49): loss=4363724013851.372\n",
      "Gradient Descent(7/49): loss=1027605084297966.9\n",
      "Gradient Descent(8/49): loss=2.42018772857157e+17\n",
      "Gradient Descent(9/49): loss=5.700191480462411e+19\n",
      "Gradient Descent(10/49): loss=1.3425658625483377e+22\n",
      "Gradient Descent(11/49): loss=3.162158611544157e+24\n",
      "Gradient Descent(12/49): loss=7.447874035611468e+26\n",
      "Gradient Descent(13/49): loss=1.7542084583397895e+29\n",
      "Gradient Descent(14/49): loss=4.131712994174218e+31\n",
      "Gradient Descent(15/49): loss=9.73148490845265e+33\n",
      "Gradient Descent(16/49): loss=2.2920711155298733e+36\n",
      "Gradient Descent(17/49): loss=5.39854922156669e+38\n",
      "Gradient Descent(18/49): loss=1.2715283375499044e+41\n",
      "Gradient Descent(19/49): loss=2.994849630531444e+43\n",
      "Gradient Descent(20/49): loss=7.053813939016118e+45\n",
      "Gradient Descent(21/49): loss=1.6613953028639505e+48\n",
      "Gradient Descent(22/49): loss=3.91310910140389e+50\n",
      "Gradient Descent(23/49): loss=9.216604147851975e+52\n",
      "Gradient Descent(24/49): loss=2.170800502033606e+55\n",
      "Gradient Descent(25/49): loss=5.112918754063269e+57\n",
      "Gradient Descent(26/49): loss=1.2042533692602622e+60\n",
      "Gradient Descent(27/49): loss=2.836395896614449e+62\n",
      "Gradient Descent(28/49): loss=6.68060550021411e+64\n",
      "Gradient Descent(29/49): loss=1.573492963473926e+67\n",
      "Gradient Descent(30/49): loss=3.706071412273347e+69\n",
      "Gradient Descent(31/49): loss=8.728965195081627e+71\n",
      "Gradient Descent(32/49): loss=2.0559461732068447e+74\n",
      "Gradient Descent(33/49): loss=4.842400642753799e+76\n",
      "Gradient Descent(34/49): loss=1.1405378355974669e+79\n",
      "Gradient Descent(35/49): loss=2.686325751207557e+81\n",
      "Gradient Descent(36/49): loss=6.327143051611779e+83\n",
      "Gradient Descent(37/49): loss=1.490241426512174e+86\n",
      "Gradient Descent(38/49): loss=3.509987827329588e+88\n",
      "Gradient Descent(39/49): loss=8.267126607019748e+90\n",
      "Gradient Descent(40/49): loss=1.9471686427041352e+93\n",
      "Gradient Descent(41/49): loss=4.5861953050420944e+95\n",
      "Gradient Descent(42/49): loss=1.0801934108172701e+98\n",
      "Gradient Descent(43/49): loss=2.5441956287606007e+100\n",
      "Gradient Descent(44/49): loss=5.992381857344559e+102\n",
      "Gradient Descent(45/49): loss=1.4113946238373594e+105\n",
      "Gradient Descent(46/49): loss=3.324278778655384e+107\n",
      "Gradient Descent(47/49): loss=7.829723318750544e+109\n",
      "Gradient Descent(48/49): loss=1.8441463947552163e+112\n",
      "Gradient Descent(49/49): loss=4.3435454700478413e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.1502115381789695\n",
      "Gradient Descent(2/49): loss=1448.9705579700842\n",
      "Gradient Descent(3/49): loss=334342.9183115257\n",
      "Gradient Descent(4/49): loss=78383681.68481898\n",
      "Gradient Descent(5/49): loss=18459318998.973507\n",
      "Gradient Descent(6/49): loss=4353260660863.2246\n",
      "Gradient Descent(7/49): loss=1027095103364208.9\n",
      "Gradient Descent(8/49): loss=2.423659306824971e+17\n",
      "Gradient Descent(9/49): loss=5.71944739412536e+19\n",
      "Gradient Descent(10/49): loss=1.349720281625834e+22\n",
      "Gradient Descent(11/49): loss=3.185193987463181e+24\n",
      "Gradient Descent(12/49): loss=7.516727510840127e+26\n",
      "Gradient Descent(13/49): loss=1.7738708477119975e+29\n",
      "Gradient Descent(14/49): loss=4.186154725669917e+31\n",
      "Gradient Descent(15/49): loss=9.878899938388564e+33\n",
      "Gradient Descent(16/49): loss=2.3313200936173003e+36\n",
      "Gradient Descent(17/49): loss=5.501678785348212e+38\n",
      "Gradient Descent(18/49): loss=1.2983403531413332e+41\n",
      "Gradient Descent(19/49): loss=3.063951458890852e+43\n",
      "Gradient Descent(20/49): loss=7.230614470603947e+45\n",
      "Gradient Descent(21/49): loss=1.706351628904957e+48\n",
      "Gradient Descent(22/49): loss=4.026816660464498e+50\n",
      "Gradient Descent(23/49): loss=9.502878622725048e+52\n",
      "Gradient Descent(24/49): loss=2.2425829068701898e+55\n",
      "Gradient Descent(25/49): loss=5.2922680525097745e+57\n",
      "Gradient Descent(26/49): loss=1.2489215472847819e+60\n",
      "Gradient Descent(27/49): loss=2.947328094110262e+62\n",
      "Gradient Descent(28/49): loss=6.955395167308494e+64\n",
      "Gradient Descent(29/49): loss=1.6414026667100052e+67\n",
      "Gradient Descent(30/49): loss=3.873543701651808e+69\n",
      "Gradient Descent(31/49): loss=9.141169996196519e+71\n",
      "Gradient Descent(32/49): loss=2.1572233421228847e+74\n",
      "Gradient Descent(33/49): loss=5.090828143154635e+76\n",
      "Gradient Descent(34/49): loss=1.201383773162371e+79\n",
      "Gradient Descent(35/49): loss=2.8351437719590343e+81\n",
      "Gradient Descent(36/49): loss=6.690651553016891e+83\n",
      "Gradient Descent(37/49): loss=1.578925860714137e+86\n",
      "Gradient Descent(38/49): loss=3.726104780494415e+88\n",
      "Gradient Descent(39/49): loss=8.793229106364617e+90\n",
      "Gradient Descent(40/49): loss=2.075112823498167e+93\n",
      "Gradient Descent(41/49): loss=4.8970556528883605e+95\n",
      "Gradient Descent(42/49): loss=1.1556554321253266e+98\n",
      "Gradient Descent(43/49): loss=2.727229528243273e+100\n",
      "Gradient Descent(44/49): loss=6.435984890447392e+102\n",
      "Gradient Descent(45/49): loss=1.518827113050097e+105\n",
      "Gradient Descent(46/49): loss=3.584277835642548e+107\n",
      "Gradient Descent(47/49): loss=8.45853191103434e+109\n",
      "Gradient Descent(48/49): loss=1.9961276823608858e+112\n",
      "Gradient Descent(49/49): loss=4.710658736286744e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.180808312524846\n",
      "Gradient Descent(2/49): loss=1444.1217816520855\n",
      "Gradient Descent(3/49): loss=330374.4891214754\n",
      "Gradient Descent(4/49): loss=76751137.67753968\n",
      "Gradient Descent(5/49): loss=17907215485.365616\n",
      "Gradient Descent(6/49): loss=4183581346217.74\n",
      "Gradient Descent(7/49): loss=977811127664136.4\n",
      "Gradient Descent(8/49): loss=2.2857222565148906e+17\n",
      "Gradient Descent(9/49): loss=5.3433371399274406e+19\n",
      "Gradient Descent(10/49): loss=1.2491328564398867e+22\n",
      "Gradient Descent(11/49): loss=2.920162519819443e+24\n",
      "Gradient Descent(12/49): loss=6.826627415912885e+26\n",
      "Gradient Descent(13/49): loss=1.5958998878089996e+29\n",
      "Gradient Descent(14/49): loss=3.730827570118433e+31\n",
      "Gradient Descent(15/49): loss=8.721772232271141e+33\n",
      "Gradient Descent(16/49): loss=2.0389393835242022e+36\n",
      "Gradient Descent(17/49): loss=4.7665471327669995e+38\n",
      "Gradient Descent(18/49): loss=1.1143034370342554e+41\n",
      "Gradient Descent(19/49): loss=2.604971935284616e+43\n",
      "Gradient Descent(20/49): loss=6.089794359557676e+45\n",
      "Gradient Descent(21/49): loss=1.4236466367711273e+48\n",
      "Gradient Descent(22/49): loss=3.3281415213620283e+50\n",
      "Gradient Descent(23/49): loss=7.780389950883704e+52\n",
      "Gradient Descent(24/49): loss=1.8188669982782997e+55\n",
      "Gradient Descent(25/49): loss=4.252071140797458e+57\n",
      "Gradient Descent(26/49): loss=9.940313944628697e+59\n",
      "Gradient Descent(27/49): loss=2.323804989284907e+62\n",
      "Gradient Descent(28/49): loss=5.432494042246658e+64\n",
      "Gradient Descent(29/49): loss=1.2699857197624701e+67\n",
      "Gradient Descent(30/49): loss=2.9689194610392873e+69\n",
      "Gradient Descent(31/49): loss=6.940615653368494e+71\n",
      "Gradient Descent(32/49): loss=1.6225480778424615e+74\n",
      "Gradient Descent(33/49): loss=3.793124985436353e+76\n",
      "Gradient Descent(34/49): loss=8.867408831591156e+78\n",
      "Gradient Descent(35/49): loss=2.0729857225502246e+81\n",
      "Gradient Descent(36/49): loss=4.846139258390305e+83\n",
      "Gradient Descent(37/49): loss=1.132910152551359e+86\n",
      "Gradient Descent(38/49): loss=2.6484699372429237e+88\n",
      "Gradient Descent(39/49): loss=6.191482168892976e+90\n",
      "Gradient Descent(40/49): loss=1.4474187873027635e+93\n",
      "Gradient Descent(41/49): loss=3.3837150599620914e+95\n",
      "Gradient Descent(42/49): loss=7.910307443466492e+97\n",
      "Gradient Descent(43/49): loss=1.8492385659347542e+100\n",
      "Gradient Descent(44/49): loss=4.3230725204807376e+102\n",
      "Gradient Descent(45/49): loss=1.0106297998327088e+105\n",
      "Gradient Descent(46/49): loss=2.3626080466406866e+107\n",
      "Gradient Descent(47/49): loss=5.523206205650486e+109\n",
      "Gradient Descent(48/49): loss=1.291192029651772e+112\n",
      "Gradient Descent(49/49): loss=3.018494684718941e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.1534224628104885\n",
      "Gradient Descent(2/49): loss=1438.6078919516797\n",
      "Gradient Descent(3/49): loss=328608.8687419879\n",
      "Gradient Descent(4/49): loss=76214303.26972395\n",
      "Gradient Descent(5/49): loss=17754077987.426983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=4141577887902.999\n",
      "Gradient Descent(7/49): loss=966573193946801.2\n",
      "Gradient Descent(8/49): loss=2.2561702011118237e+17\n",
      "Gradient Descent(9/49): loss=5.2666240002959516e+19\n",
      "Gradient Descent(10/49): loss=1.2294214729275883e+22\n",
      "Gradient Descent(11/49): loss=2.869934857103049e+24\n",
      "Gradient Descent(12/49): loss=6.699528472143212e+26\n",
      "Gradient Descent(13/49): loss=1.5639280319756508e+29\n",
      "Gradient Descent(14/49): loss=3.6508114536733894e+31\n",
      "Gradient Descent(15/49): loss=8.522403332413978e+33\n",
      "Gradient Descent(16/49): loss=1.9894580079055407e+36\n",
      "Gradient Descent(17/49): loss=4.644163216996716e+38\n",
      "Gradient Descent(18/49): loss=1.0841270326006756e+41\n",
      "Gradient Descent(19/49): loss=2.530771140891892e+43\n",
      "Gradient Descent(20/49): loss=5.907797128745648e+45\n",
      "Gradient Descent(21/49): loss=1.3791079863279174e+48\n",
      "Gradient Descent(22/49): loss=3.2193705988970878e+50\n",
      "Gradient Descent(23/49): loss=7.515254175872978e+52\n",
      "Gradient Descent(24/49): loss=1.7543505350914766e+55\n",
      "Gradient Descent(25/49): loss=4.095331612153844e+57\n",
      "Gradient Descent(26/49): loss=9.560085443610997e+59\n",
      "Gradient Descent(27/49): loss=2.231693116570325e+62\n",
      "Gradient Descent(28/49): loss=5.209633528826123e+64\n",
      "Gradient Descent(29/49): loss=1.2161296418021573e+67\n",
      "Gradient Descent(30/49): loss=2.838916206843266e+69\n",
      "Gradient Descent(31/49): loss=6.627126707917642e+71\n",
      "Gradient Descent(32/49): loss=1.5470272879815286e+74\n",
      "Gradient Descent(33/49): loss=3.611359093074437e+76\n",
      "Gradient Descent(34/49): loss=8.430306692358138e+78\n",
      "Gradient Descent(35/49): loss=1.967959128282506e+81\n",
      "Gradient Descent(36/49): loss=4.5939765561567297e+83\n",
      "Gradient Descent(37/49): loss=1.0724115300573418e+86\n",
      "Gradient Descent(38/49): loss=2.5034226355784125e+88\n",
      "Gradient Descent(39/49): loss=5.843955157766014e+90\n",
      "Gradient Descent(40/49): loss=1.3642048050783632e+93\n",
      "Gradient Descent(41/49): loss=3.184580818909512e+95\n",
      "Gradient Descent(42/49): loss=7.434041394967653e+97\n",
      "Gradient Descent(43/49): loss=1.7353923359061421e+100\n",
      "Gradient Descent(44/49): loss=4.0510758543266065e+102\n",
      "Gradient Descent(45/49): loss=9.456775414960442e+104\n",
      "Gradient Descent(46/49): loss=2.2075765664443623e+107\n",
      "Gradient Descent(47/49): loss=5.153336187940646e+109\n",
      "Gradient Descent(48/49): loss=1.202987668450947e+112\n",
      "Gradient Descent(49/49): loss=2.8082377661127626e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.247272020549632\n",
      "Gradient Descent(2/49): loss=1486.6640188220374\n",
      "Gradient Descent(3/49): loss=346332.0349566149\n",
      "Gradient Descent(4/49): loss=81868476.39639056\n",
      "Gradient Descent(5/49): loss=19430153536.48027\n",
      "Gradient Descent(6/49): loss=4617026093936.454\n",
      "Gradient Descent(7/49): loss=1097527045677251.9\n",
      "Gradient Descent(8/49): loss=2.6092884525961523e+17\n",
      "Gradient Descent(9/49): loss=6.2036398788292624e+19\n",
      "Gradient Descent(10/49): loss=1.4749484727342964e+22\n",
      "Gradient Descent(11/49): loss=3.506784019901536e+24\n",
      "Gradient Descent(12/49): loss=8.337614566725012e+26\n",
      "Gradient Descent(13/49): loss=1.9823248086369566e+29\n",
      "Gradient Descent(14/49): loss=4.713113359476829e+31\n",
      "Gradient Descent(15/49): loss=1.1205751238902573e+34\n",
      "Gradient Descent(16/49): loss=2.6642444892431035e+36\n",
      "Gradient Descent(17/49): loss=6.3344246955430315e+38\n",
      "Gradient Descent(18/49): loss=1.506053084056622e+41\n",
      "Gradient Descent(19/49): loss=3.5807449016817664e+43\n",
      "Gradient Descent(20/49): loss=8.513467545865973e+45\n",
      "Gradient Descent(21/49): loss=2.024135526247995e+48\n",
      "Gradient Descent(22/49): loss=4.812521580230835e+50\n",
      "Gradient Descent(23/49): loss=1.144210141071063e+53\n",
      "Gradient Descent(24/49): loss=2.720438392029239e+55\n",
      "Gradient Descent(25/49): loss=6.468029585810752e+57\n",
      "Gradient Descent(26/49): loss=1.5378185679745488e+60\n",
      "Gradient Descent(27/49): loss=3.6562695278875173e+62\n",
      "Gradient Descent(28/49): loss=8.693032545553501e+64\n",
      "Gradient Descent(29/49): loss=2.0668283413371498e+67\n",
      "Gradient Descent(30/49): loss=4.914026687659725e+69\n",
      "Gradient Descent(31/49): loss=1.1683436792534866e+72\n",
      "Gradient Descent(32/49): loss=2.777817540713549e+74\n",
      "Gradient Descent(33/49): loss=6.604452462503311e+76\n",
      "Gradient Descent(34/49): loss=1.5702540462128985e+79\n",
      "Gradient Descent(35/49): loss=3.73338711066047e+81\n",
      "Gradient Descent(36/49): loss=8.876384908328384e+83\n",
      "Gradient Descent(37/49): loss=2.1104216280122387e+86\n",
      "Gradient Descent(38/49): loss=5.017672728233027e+88\n",
      "Gradient Descent(39/49): loss=1.192986239027841e+91\n",
      "Gradient Descent(40/49): loss=2.836406923276926e+93\n",
      "Gradient Descent(41/49): loss=6.743752753568453e+95\n",
      "Gradient Descent(42/49): loss=1.603373649529837e+98\n",
      "Gradient Descent(43/49): loss=3.812131247911309e+100\n",
      "Gradient Descent(44/49): loss=9.063604516366591e+102\n",
      "Gradient Descent(45/49): loss=2.1549343788756108e+105\n",
      "Gradient Descent(46/49): loss=5.123504858221246e+107\n",
      "Gradient Descent(47/49): loss=1.2181485566123573e+110\n",
      "Gradient Descent(48/49): loss=2.89623206582054e+112\n",
      "Gradient Descent(49/49): loss=6.8859911490717e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.215175389509885\n",
      "Gradient Descent(2/49): loss=1476.5213722455715\n",
      "Gradient Descent(3/49): loss=343913.76491085754\n",
      "Gradient Descent(4/49): loss=81388592.20313612\n",
      "Gradient Descent(5/49): loss=19347997597.721554\n",
      "Gradient Descent(6/49): loss=4605940615433.266\n",
      "Gradient Descent(7/49): loss=1096978147540590.2\n",
      "Gradient Descent(8/49): loss=2.6130196891861e+17\n",
      "Gradient Descent(9/49): loss=6.2245653224570044e+19\n",
      "Gradient Descent(10/49): loss=1.482800084644124e+22\n",
      "Gradient Descent(11/49): loss=3.5323082266289464e+24\n",
      "Gradient Descent(12/49): loss=8.414637137026892e+26\n",
      "Gradient Descent(13/49): loss=2.0045295538696356e+29\n",
      "Gradient Descent(14/49): loss=4.775178664083063e+31\n",
      "Gradient Descent(15/49): loss=1.1375403683398338e+34\n",
      "Gradient Descent(16/49): loss=2.709842313729044e+36\n",
      "Gradient Descent(17/49): loss=6.455371279432479e+38\n",
      "Gradient Descent(18/49): loss=1.537794957805666e+41\n",
      "Gradient Descent(19/49): loss=3.6633266034941733e+43\n",
      "Gradient Descent(20/49): loss=8.726756281592701e+45\n",
      "Gradient Descent(21/49): loss=2.0788830330665474e+48\n",
      "Gradient Descent(22/49): loss=4.952303611859858e+50\n",
      "Gradient Descent(23/49): loss=1.179735015112194e+53\n",
      "Gradient Descent(24/49): loss=2.810358198867089e+55\n",
      "Gradient Descent(25/49): loss=6.694819688129885e+57\n",
      "Gradient Descent(26/49): loss=1.5948362267364419e+60\n",
      "Gradient Descent(27/49): loss=3.7992099990698595e+62\n",
      "Gradient Descent(28/49): loss=9.05045695291851e+64\n",
      "Gradient Descent(29/49): loss=2.15599482725846e+67\n",
      "Gradient Descent(30/49): loss=5.135998899664704e+69\n",
      "Gradient Descent(31/49): loss=1.2234948045260344e+72\n",
      "Gradient Descent(32/49): loss=2.914602526102416e+74\n",
      "Gradient Descent(33/49): loss=6.943149945334997e+76\n",
      "Gradient Descent(34/49): loss=1.653993322645997e+79\n",
      "Gradient Descent(35/49): loss=3.940133704293133e+81\n",
      "Gradient Descent(36/49): loss=9.386164620586814e+83\n",
      "Gradient Descent(37/49): loss=2.2359669213448824e+86\n",
      "Gradient Descent(38/49): loss=5.326507977904983e+88\n",
      "Gradient Descent(39/49): loss=1.2688777713053313e+91\n",
      "Gradient Descent(40/49): loss=3.0227135774347587e+93\n",
      "Gradient Descent(41/49): loss=7.200691491197875e+95\n",
      "Gradient Descent(42/49): loss=1.7153447266218416e+98\n",
      "Gradient Descent(43/49): loss=4.086284678000929e+100\n",
      "Gradient Descent(44/49): loss=9.734324658198155e+102\n",
      "Gradient Descent(45/49): loss=2.318905412081111e+105\n",
      "Gradient Descent(46/49): loss=5.524083589764307e+107\n",
      "Gradient Descent(47/49): loss=1.3159441237974904e+110\n",
      "Gradient Descent(48/49): loss=3.1348347808600634e+112\n",
      "Gradient Descent(49/49): loss=7.467785999097817e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.246113916075359\n",
      "Gradient Descent(2/49): loss=1471.587371773159\n",
      "Gradient Descent(3/49): loss=339834.3251514626\n",
      "Gradient Descent(4/49): loss=79694306.75507623\n",
      "Gradient Descent(5/49): loss=18769566686.52275\n",
      "Gradient Descent(6/49): loss=4426484232110.732\n",
      "Gradient Descent(7/49): loss=1044360702541784.0\n",
      "Gradient Descent(8/49): loss=2.4643590147602134e+17\n",
      "Gradient Descent(9/49): loss=5.8153803274677625e+19\n",
      "Gradient Descent(10/49): loss=1.3723321059126557e+22\n",
      "Gradient Descent(11/49): loss=3.2384908039685227e+24\n",
      "Gradient Descent(12/49): loss=7.642349639587788e+26\n",
      "Gradient Descent(13/49): loss=1.8034803066615167e+29\n",
      "Gradient Descent(14/49): loss=4.255944891410627e+31\n",
      "Gradient Descent(15/49): loss=1.0043396714183565e+34\n",
      "Gradient Descent(16/49): loss=2.3700922477721026e+36\n",
      "Gradient Descent(17/49): loss=5.593065242360346e+38\n",
      "Gradient Descent(18/49): loss=1.3198802248686464e+41\n",
      "Gradient Descent(19/49): loss=3.1147210591929287e+43\n",
      "Gradient Descent(20/49): loss=7.350278530402714e+45\n",
      "Gradient Descent(21/49): loss=1.7345564322890368e+48\n",
      "Gradient Descent(22/49): loss=4.0932952465076006e+50\n",
      "Gradient Descent(23/49): loss=9.659568096708338e+52\n",
      "Gradient Descent(24/49): loss=2.2795144302118636e+55\n",
      "Gradient Descent(25/49): loss=5.379315084824317e+57\n",
      "Gradient Descent(26/49): loss=1.2694383680274488e+60\n",
      "Gradient Descent(27/49): loss=2.995685779341315e+62\n",
      "Gradient Descent(28/49): loss=7.069372971996168e+64\n",
      "Gradient Descent(29/49): loss=1.6682668977445235e+67\n",
      "Gradient Descent(30/49): loss=3.936861802503366e+69\n",
      "Gradient Descent(31/49): loss=9.290408431027672e+71\n",
      "Gradient Descent(32/49): loss=2.1923982386281836e+74\n",
      "Gradient Descent(33/49): loss=5.1737338271233323e+76\n",
      "Gradient Descent(34/49): loss=1.2209242482638129e+79\n",
      "Gradient Descent(35/49): loss=2.881199670891033e+81\n",
      "Gradient Descent(36/49): loss=6.799202780473371e+83\n",
      "Gradient Descent(37/49): loss=1.60451075005502e+86\n",
      "Gradient Descent(38/49): loss=3.7864067746820385e+88\n",
      "Gradient Descent(39/49): loss=8.935356938472645e+90\n",
      "Gradient Descent(40/49): loss=2.108611366104886e+93\n",
      "Gradient Descent(41/49): loss=4.976009267321726e+95\n",
      "Gradient Descent(42/49): loss=1.1742641923727531e+98\n",
      "Gradient Descent(43/49): loss=2.7710888774751163e+100\n",
      "Gradient Descent(44/49): loss=6.539357681809282e+102\n",
      "Gradient Descent(45/49): loss=1.5431911707430243e+105\n",
      "Gradient Descent(46/49): loss=3.641701685906752e+107\n",
      "Gradient Descent(47/49): loss=8.593874447033506e+109\n",
      "Gradient Descent(48/49): loss=2.0280265760699163e+112\n",
      "Gradient Descent(49/49): loss=4.7858411460334856e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.218137422313217\n",
      "Gradient Descent(2/49): loss=1465.847708776652\n",
      "Gradient Descent(3/49): loss=337976.69454219204\n",
      "Gradient Descent(4/49): loss=79123948.95797463\n",
      "Gradient Descent(5/49): loss=18605249660.975784\n",
      "Gradient Descent(6/49): loss=4380966447738.626\n",
      "Gradient Descent(7/49): loss=1032062248038516.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8/49): loss=2.4317011229352406e+17\n",
      "Gradient Descent(9/49): loss=5.7297793488414286e+19\n",
      "Gradient Descent(10/49): loss=1.3501239711181717e+22\n",
      "Gradient Descent(11/49): loss=3.181355279850821e+24\n",
      "Gradient Descent(12/49): loss=7.496381019683769e+26\n",
      "Gradient Descent(13/49): loss=1.7664097896872876e+29\n",
      "Gradient Descent(14/49): loss=4.162280913248869e+31\n",
      "Gradient Descent(15/49): loss=9.80779435818562e+33\n",
      "Gradient Descent(16/49): loss=2.311060574330848e+36\n",
      "Gradient Descent(17/49): loss=5.445669883352207e+38\n",
      "Gradient Descent(18/49): loss=1.2831909694739167e+41\n",
      "Gradient Descent(19/49): loss=3.02364833257651e+43\n",
      "Gradient Descent(20/49): loss=7.12477679522447e+45\n",
      "Gradient Descent(21/49): loss=1.6788474983229355e+48\n",
      "Gradient Descent(22/49): loss=3.9559539951670645e+50\n",
      "Gradient Descent(23/49): loss=9.321616184791563e+52\n",
      "Gradient Descent(24/49): loss=2.1964999694958607e+55\n",
      "Gradient Descent(25/49): loss=5.1757249176138564e+57\n",
      "Gradient Descent(26/49): loss=1.21958246277496e+60\n",
      "Gradient Descent(27/49): loss=2.8737643657351635e+62\n",
      "Gradient Descent(28/49): loss=6.771597560511873e+64\n",
      "Gradient Descent(29/49): loss=1.5956260738796237e+67\n",
      "Gradient Descent(30/49): loss=3.759855107886934e+69\n",
      "Gradient Descent(31/49): loss=8.859538374132914e+71\n",
      "Gradient Descent(32/49): loss=2.0876182180021892e+74\n",
      "Gradient Descent(33/49): loss=4.9191612927137304e+76\n",
      "Gradient Descent(34/49): loss=1.1591270671555225e+79\n",
      "Gradient Descent(35/49): loss=2.731310233316928e+81\n",
      "Gradient Descent(36/49): loss=6.435925621967057e+83\n",
      "Gradient Descent(37/49): loss=1.5165299827983948e+86\n",
      "Gradient Descent(38/49): loss=3.573476953923503e+88\n",
      "Gradient Descent(39/49): loss=8.420366023135819e+90\n",
      "Gradient Descent(40/49): loss=1.9841337967978995e+93\n",
      "Gradient Descent(41/49): loss=4.6753156724766155e+95\n",
      "Gradient Descent(42/49): loss=1.10166847984657e+98\n",
      "Gradient Descent(43/49): loss=2.5959176331820655e+100\n",
      "Gradient Descent(44/49): loss=6.116893132137273e+102\n",
      "Gradient Descent(45/49): loss=1.441354729892684e+105\n",
      "Gradient Descent(46/49): loss=3.3963376709478965e+107\n",
      "Gradient Descent(47/49): loss=8.002963695105517e+109\n",
      "Gradient Descent(48/49): loss=1.8857791571502345e+112\n",
      "Gradient Descent(49/49): loss=4.4435576181822857e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.31288820185945\n",
      "Gradient Descent(2/49): loss=1514.796463532073\n",
      "Gradient Descent(3/49): loss=356199.108763932\n",
      "Gradient Descent(4/49): loss=84992142.46948262\n",
      "Gradient Descent(5/49): loss=20361139395.237743\n",
      "Gradient Descent(6/49): loss=4883741828244.458\n",
      "Gradient Descent(7/49): loss=1171845672166052.2\n",
      "Gradient Descent(8/49): loss=2.8121741989554285e+17\n",
      "Gradient Descent(9/49): loss=6.748879735964242e+19\n",
      "Gradient Descent(10/49): loss=1.6196715688530738e+22\n",
      "Gradient Descent(11/49): loss=3.8870858792901155e+24\n",
      "Gradient Descent(12/49): loss=9.328717478996225e+26\n",
      "Gradient Descent(13/49): loss=2.238823991231329e+29\n",
      "Gradient Descent(14/49): loss=5.373014737237428e+31\n",
      "Gradient Descent(15/49): loss=1.2894845226817296e+34\n",
      "Gradient Descent(16/49): loss=3.09466927567847e+36\n",
      "Gradient Descent(17/49): loss=7.42698172099651e+38\n",
      "Gradient Descent(18/49): loss=1.7824217282047033e+41\n",
      "Gradient Descent(19/49): loss=4.2776828305970953e+43\n",
      "Gradient Descent(20/49): loss=1.0266128443843513e+46\n",
      "Gradient Descent(21/49): loss=2.463796345034981e+48\n",
      "Gradient Descent(22/49): loss=5.912932478046082e+50\n",
      "Gradient Descent(23/49): loss=1.4190608960289575e+53\n",
      "Gradient Descent(24/49): loss=3.405643196704795e+55\n",
      "Gradient Descent(25/49): loss=8.173296590531913e+57\n",
      "Gradient Descent(26/49): loss=1.961531883946454e+60\n",
      "Gradient Descent(27/49): loss=4.707534211099099e+62\n",
      "Gradient Descent(28/49): loss=1.1297740572069112e+65\n",
      "Gradient Descent(29/49): loss=2.711375771477977e+67\n",
      "Gradient Descent(30/49): loss=6.5071051395291554e+69\n",
      "Gradient Descent(31/49): loss=1.5616580240298333e+72\n",
      "Gradient Descent(32/49): loss=3.747865958399487e+74\n",
      "Gradient Descent(33/49): loss=8.99460639012567e+76\n",
      "Gradient Descent(34/49): loss=2.158640277194929e+79\n",
      "Gradient Descent(35/49): loss=5.180580054557693e+81\n",
      "Gradient Descent(36/49): loss=1.243301627659633e+84\n",
      "Gradient Descent(37/49): loss=2.983833704067853e+86\n",
      "Gradient Descent(38/49): loss=7.160984410750491e+88\n",
      "Gradient Descent(39/49): loss=1.7185843051877012e+91\n",
      "Gradient Descent(40/49): loss=4.1244776480779446e+93\n",
      "Gradient Descent(41/49): loss=9.898447121938982e+95\n",
      "Gradient Descent(42/49): loss=2.375555495408752e+98\n",
      "Gradient Descent(43/49): loss=5.701160840935314e+100\n",
      "Gradient Descent(44/49): loss=1.3682372395439066e+103\n",
      "Gradient Descent(45/49): loss=3.28367010843288e+105\n",
      "Gradient Descent(46/49): loss=7.880570027906744e+107\n",
      "Gradient Descent(47/49): loss=1.8912796326662825e+110\n",
      "Gradient Descent(48/49): loss=4.5389339048719055e+112\n",
      "Gradient Descent(49/49): loss=1.089311206918225e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.280452352176137\n",
      "Gradient Descent(2/49): loss=1504.4633872382608\n",
      "Gradient Descent(3/49): loss=353711.84785776655\n",
      "Gradient Descent(4/49): loss=84493820.95178436\n",
      "Gradient Descent(5/49): loss=20274997527.975277\n",
      "Gradient Descent(6/49): loss=4872000503190.568\n",
      "Gradient Descent(7/49): loss=1171255146064920.8\n",
      "Gradient Descent(8/49): loss=2.8161831816272736e+17\n",
      "Gradient Descent(9/49): loss=6.7716107226769056e+19\n",
      "Gradient Descent(10/49): loss=1.6282845982861874e+22\n",
      "Gradient Descent(11/49): loss=3.9153543351037554e+24\n",
      "Gradient Descent(12/49): loss=9.414833413832641e+26\n",
      "Gradient Descent(13/49): loss=2.263885619814971e+29\n",
      "Gradient Descent(14/49): loss=5.443727945630208e+31\n",
      "Gradient Descent(15/49): loss=1.3089961688878887e+34\n",
      "Gradient Descent(16/49): loss=3.1476058827414434e+36\n",
      "Gradient Descent(17/49): loss=7.568718004564576e+38\n",
      "Gradient Descent(18/49): loss=1.8199703050005233e+41\n",
      "Gradient Descent(19/49): loss=4.3762918793197595e+43\n",
      "Gradient Descent(20/49): loss=1.0523210497487449e+46\n",
      "Gradient Descent(21/49): loss=2.5304061575226242e+48\n",
      "Gradient Descent(22/49): loss=6.084602530540587e+50\n",
      "Gradient Descent(23/49): loss=1.4631006111414848e+53\n",
      "Gradient Descent(24/49): loss=3.5181647241310037e+55\n",
      "Gradient Descent(25/49): loss=8.459762050457103e+57\n",
      "Gradient Descent(26/49): loss=2.034230332067866e+60\n",
      "Gradient Descent(27/49): loss=4.891500516473688e+62\n",
      "Gradient Descent(28/49): loss=1.1762078721115562e+65\n",
      "Gradient Descent(29/49): loss=2.828303817525856e+67\n",
      "Gradient Descent(30/49): loss=6.800925817534999e+69\n",
      "Gradient Descent(31/49): loss=1.635347365760544e+72\n",
      "Gradient Descent(32/49): loss=3.932348445566897e+74\n",
      "Gradient Descent(33/49): loss=9.455706243890839e+76\n",
      "Gradient Descent(34/49): loss=2.2737145959573423e+79\n",
      "Gradient Descent(35/49): loss=5.467363230757681e+81\n",
      "Gradient Descent(36/49): loss=1.3146795446618054e+84\n",
      "Gradient Descent(37/49): loss=3.1612721383295555e+86\n",
      "Gradient Descent(38/49): loss=7.60157984746735e+88\n",
      "Gradient Descent(39/49): loss=1.8278722504401532e+91\n",
      "Gradient Descent(40/49): loss=4.395292861446821e+93\n",
      "Gradient Descent(41/49): loss=1.056890016971012e+96\n",
      "Gradient Descent(42/49): loss=2.541392674355938e+98\n",
      "Gradient Descent(43/49): loss=6.111020656416321e+100\n",
      "Gradient Descent(44/49): loss=1.4694531010486625e+103\n",
      "Gradient Descent(45/49): loss=3.533439890952372e+105\n",
      "Gradient Descent(46/49): loss=8.49649264346275e+107\n",
      "Gradient Descent(47/49): loss=2.043062552875608e+110\n",
      "Gradient Descent(48/49): loss=4.912738432339131e+112\n",
      "Gradient Descent(49/49): loss=1.1813147311917577e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.311734449183786\n",
      "Gradient Descent(2/49): loss=1499.4430515737627\n",
      "Gradient Descent(3/49): loss=349518.8229418239\n",
      "Gradient Descent(4/49): loss=82735760.85372648\n",
      "Gradient Descent(5/49): loss=19669114763.314495\n",
      "Gradient Descent(6/49): loss=4682253117872.281\n",
      "Gradient Descent(7/49): loss=1115095875693330.9\n",
      "Gradient Descent(8/49): loss=2.656021076640475e+17\n",
      "Gradient Descent(9/49): loss=6.326617321458629e+19\n",
      "Gradient Descent(10/49): loss=1.5070185233902402e+22\n",
      "Gradient Descent(11/49): loss=3.589781094333076e+24\n",
      "Gradient Descent(12/49): loss=8.551024188080394e+26\n",
      "Gradient Descent(13/49): loss=2.036894669458232e+29\n",
      "Gradient Descent(14/49): loss=4.851981080971915e+31\n",
      "Gradient Descent(15/49): loss=1.1557653136590214e+34\n",
      "Gradient Descent(16/49): loss=2.7530888304844984e+36\n",
      "Gradient Descent(17/49): loss=6.55799069217522e+38\n",
      "Gradient Descent(18/49): loss=1.5621450916840959e+41\n",
      "Gradient Descent(19/49): loss=3.7211051437081304e+43\n",
      "Gradient Descent(20/49): loss=8.863852383826758e+45\n",
      "Gradient Descent(21/49): loss=2.111412498604419e+48\n",
      "Gradient Descent(22/49): loss=5.029486668295935e+50\n",
      "Gradient Descent(23/49): loss=1.198048044310963e+53\n",
      "Gradient Descent(24/49): loss=2.8538083727924457e+55\n",
      "Gradient Descent(25/49): loss=6.797909539024638e+57\n",
      "Gradient Descent(26/49): loss=1.619294923280541e+60\n",
      "Gradient Descent(27/49): loss=3.857238807768495e+62\n",
      "Gradient Descent(28/49): loss=9.188129355716147e+64\n",
      "Gradient Descent(29/49): loss=2.1886568414522014e+67\n",
      "Gradient Descent(30/49): loss=5.2134864281764135e+69\n",
      "Gradient Descent(31/49): loss=1.2418776768470127e+72\n",
      "Gradient Descent(32/49): loss=2.958212676867745e+74\n",
      "Gradient Descent(33/49): loss=7.04660563977515e+76\n",
      "Gradient Descent(34/49): loss=1.678535537042135e+79\n",
      "Gradient Descent(35/49): loss=3.998352814310797e+81\n",
      "Gradient Descent(36/49): loss=9.524269742825112e+83\n",
      "Gradient Descent(37/49): loss=2.2687271070582185e+86\n",
      "Gradient Descent(38/49): loss=5.404217672623376e+88\n",
      "Gradient Descent(39/49): loss=1.2873107815494265e+91\n",
      "Gradient Descent(40/49): loss=3.066436529172864e+93\n",
      "Gradient Descent(41/49): loss=7.304400089097455e+95\n",
      "Gradient Descent(42/49): loss=1.7399434214279493e+98\n",
      "Gradient Descent(43/49): loss=4.1446293642774415e+100\n",
      "Gradient Descent(44/49): loss=9.872707558003854e+102\n",
      "Gradient Descent(45/49): loss=2.351726679494264e+105\n",
      "Gradient Descent(46/49): loss=5.6019266675851556e+107\n",
      "Gradient Descent(47/49): loss=1.3344060201651634e+110\n",
      "Gradient Descent(48/49): loss=3.178619664831523e+112\n",
      "Gradient Descent(49/49): loss=7.571625742817896e+114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.28316294960883\n",
      "Gradient Descent(2/49): loss=1493.4727062151305\n",
      "Gradient Descent(3/49): loss=347566.0213125617\n",
      "Gradient Descent(4/49): loss=82130332.08995426\n",
      "Gradient Descent(5/49): loss=19492973372.708775\n",
      "Gradient Descent(6/49): loss=4632978416165.921\n",
      "Gradient Descent(7/49): loss=1101651775310335.6\n",
      "Gradient Descent(8/49): loss=2.6199733661437357e+17\n",
      "Gradient Descent(9/49): loss=6.231216914546122e+19\n",
      "Gradient Descent(10/49): loss=1.4820297896198337e+22\n",
      "Gradient Descent(11/49): loss=3.524875475899027e+24\n",
      "Gradient Descent(12/49): loss=8.383620002088738e+26\n",
      "Gradient Descent(13/49): loss=1.9939750626099944e+29\n",
      "Gradient Descent(14/49): loss=4.742506932505153e+31\n",
      "Gradient Descent(15/49): loss=1.1279666668325132e+34\n",
      "Gradient Descent(16/49): loss=2.6827769775882005e+36\n",
      "Gradient Descent(17/49): loss=6.380766915542337e+38\n",
      "Gradient Descent(18/49): loss=1.5176135371345159e+41\n",
      "Gradient Descent(19/49): loss=3.609520425300874e+43\n",
      "Gradient Descent(20/49): loss=8.584950903300785e+45\n",
      "Gradient Descent(21/49): loss=2.0418607831346727e+48\n",
      "Gradient Descent(22/49): loss=4.856399884968976e+50\n",
      "Gradient Descent(23/49): loss=1.155055233832016e+53\n",
      "Gradient Descent(24/49): loss=2.74720497655175e+55\n",
      "Gradient Descent(25/49): loss=6.534003710081326e+57\n",
      "Gradient Descent(26/49): loss=1.5540596660165032e+60\n",
      "Gradient Descent(27/49): loss=3.6962045825191135e+62\n",
      "Gradient Descent(28/49): loss=8.791122126511422e+64\n",
      "Gradient Descent(29/49): loss=2.0908969327279088e+67\n",
      "Gradient Descent(30/49): loss=4.9730283806510395e+69\n",
      "Gradient Descent(31/49): loss=1.1827943734412214e+72\n",
      "Gradient Descent(32/49): loss=2.8131802651426953e+74\n",
      "Gradient Descent(33/49): loss=6.690920570718845e+76\n",
      "Gradient Descent(34/49): loss=1.5913810657063464e+79\n",
      "Gradient Descent(35/49): loss=3.784970497739125e+81\n",
      "Gradient Descent(36/49): loss=9.002244639876317e+83\n",
      "Gradient Descent(37/49): loss=2.1411107062680981e+86\n",
      "Gradient Descent(38/49): loss=5.09245775902273e+88\n",
      "Gradient Descent(39/49): loss=1.2111996802179176e+91\n",
      "Gradient Descent(40/49): loss=2.8807399781780625e+93\n",
      "Gradient Descent(41/49): loss=6.851605856088291e+95\n",
      "Gradient Descent(42/49): loss=1.6295987545836643e+98\n",
      "Gradient Descent(43/49): loss=3.8758681639296e+100\n",
      "Gradient Descent(44/49): loss=9.218437349629075e+102\n",
      "Gradient Descent(45/49): loss=2.1925303848023136e+105\n",
      "Gradient Descent(46/49): loss=5.2147552843919e+107\n",
      "Gradient Descent(47/49): loss=1.2402871524421369e+110\n",
      "Gradient Descent(48/49): loss=2.94992216627554e+112\n",
      "Gradient Descent(49/49): loss=7.016150066498193e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.378819229801028\n",
      "Gradient Descent(2/49): loss=1543.3264639679319\n",
      "Gradient Descent(3/49): loss=366299.3469147035\n",
      "Gradient Descent(4/49): loss=88219600.64303601\n",
      "Gradient Descent(5/49): loss=21332081658.399113\n",
      "Gradient Descent(6/49): loss=5164513886914.931\n",
      "Gradient Descent(7/49): loss=1250814937120026.8\n",
      "Gradient Descent(8/49): loss=3.02977836690695e+17\n",
      "Gradient Descent(9/49): loss=7.339160367576349e+19\n",
      "Gradient Descent(10/49): loss=1.7778196896015849e+22\n",
      "Gradient Descent(11/49): loss=4.306564536336243e+24\n",
      "Gradient Descent(12/49): loss=1.0432174379450328e+27\n",
      "Gradient Descent(13/49): loss=2.527079610301353e+29\n",
      "Gradient Descent(14/49): loss=6.1215736338332515e+31\n",
      "Gradient Descent(15/49): loss=1.4828842568354868e+34\n",
      "Gradient Descent(16/49): loss=3.5921249801072206e+36\n",
      "Gradient Descent(17/49): loss=8.701530066101032e+38\n",
      "Gradient Descent(18/49): loss=2.1078505355777647e+41\n",
      "Gradient Descent(19/49): loss=5.106037500648376e+43\n",
      "Gradient Descent(20/49): loss=1.236881767621906e+46\n",
      "Gradient Descent(21/49): loss=2.996210871899834e+48\n",
      "Gradient Descent(22/49): loss=7.257993305493897e+50\n",
      "Gradient Descent(23/49): loss=1.7581695372986732e+53\n",
      "Gradient Descent(24/49): loss=4.258973509320451e+55\n",
      "Gradient Descent(25/49): loss=1.0316897755478667e+58\n",
      "Gradient Descent(26/49): loss=2.499155701816317e+60\n",
      "Gradient Descent(27/49): loss=6.053931491765279e+62\n",
      "Gradient Descent(28/49): loss=1.4664987251635527e+65\n",
      "Gradient Descent(29/49): loss=3.552432851002169e+67\n",
      "Gradient Descent(30/49): loss=8.605380246390564e+69\n",
      "Gradient Descent(31/49): loss=2.0845592947401646e+72\n",
      "Gradient Descent(32/49): loss=5.049617017342458e+74\n",
      "Gradient Descent(33/49): loss=1.2232145224256045e+77\n",
      "Gradient Descent(34/49): loss=2.9631034645482144e+79\n",
      "Gradient Descent(35/49): loss=7.17779423040788e+81\n",
      "Gradient Descent(36/49): loss=1.738742188063687e+84\n",
      "Gradient Descent(37/49): loss=4.2119128795096276e+86\n",
      "Gradient Descent(38/49): loss=1.0202898524211498e+89\n",
      "Gradient Descent(39/49): loss=2.471540634227832e+91\n",
      "Gradient Descent(40/49): loss=5.987037009281068e+93\n",
      "Gradient Descent(41/49): loss=1.4502942680406303e+96\n",
      "Gradient Descent(42/49): loss=3.513179325016529e+98\n",
      "Gradient Descent(43/49): loss=8.510292870700214e+100\n",
      "Gradient Descent(44/49): loss=2.0615254174294335e+103\n",
      "Gradient Descent(45/49): loss=4.99381996751177e+105\n",
      "Gradient Descent(46/49): loss=1.2096982970510896e+108\n",
      "Gradient Descent(47/49): loss=2.9303618861082994e+110\n",
      "Gradient Descent(48/49): loss=7.098481335791713e+112\n",
      "Gradient Descent(49/49): loss=1.7195295063539866e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.346042426177718\n",
      "Gradient Descent(2/49): loss=1532.8002894378637\n",
      "Gradient Descent(3/49): loss=363741.46306602156\n",
      "Gradient Descent(4/49): loss=87702225.0322439\n",
      "Gradient Descent(5/49): loss=21241780691.2751\n",
      "Gradient Descent(6/49): loss=5152081341700.483\n",
      "Gradient Descent(7/49): loss=1250179900558812.8\n",
      "Gradient Descent(8/49): loss=3.034084351506916e+17\n",
      "Gradient Descent(9/49): loss=7.363843286371583e+19\n",
      "Gradient Descent(10/49): loss=1.7872639385798363e+22\n",
      "Gradient Descent(11/49): loss=4.3378575016642246e+24\n",
      "Gradient Descent(12/49): loss=1.0528407486139713e+27\n",
      "Gradient Descent(13/49): loss=2.5553498219044336e+29\n",
      "Gradient Descent(14/49): loss=6.202090960361871e+31\n",
      "Gradient Descent(15/49): loss=1.5053099450820115e+34\n",
      "Gradient Descent(16/49): loss=3.6535389418933273e+36\n",
      "Gradient Descent(17/49): loss=8.867507286217522e+38\n",
      "Gradient Descent(18/49): loss=2.152233407971215e+41\n",
      "Gradient Descent(19/49): loss=5.223687443398484e+43\n",
      "Gradient Descent(20/49): loss=1.2678416018183817e+46\n",
      "Gradient Descent(21/49): loss=3.077179377365368e+48\n",
      "Gradient Descent(22/49): loss=7.468624556270036e+50\n",
      "Gradient Descent(23/49): loss=1.8127104702914047e+53\n",
      "Gradient Descent(24/49): loss=4.399631049010133e+55\n",
      "Gradient Descent(25/49): loss=1.0678348078569236e+58\n",
      "Gradient Descent(26/49): loss=2.5917427260812413e+60\n",
      "Gradient Descent(27/49): loss=6.290420867321972e+62\n",
      "Gradient Descent(28/49): loss=1.5267485576344713e+65\n",
      "Gradient Descent(29/49): loss=3.7055726594512215e+67\n",
      "Gradient Descent(30/49): loss=8.993798399749323e+69\n",
      "Gradient Descent(31/49): loss=2.182885537246831e+72\n",
      "Gradient Descent(32/49): loss=5.298083253516333e+74\n",
      "Gradient Descent(33/49): loss=1.2858982151026206e+77\n",
      "Gradient Descent(34/49): loss=3.121004598232117e+79\n",
      "Gradient Descent(35/49): loss=7.574992785419372e+81\n",
      "Gradient Descent(36/49): loss=1.8385271118042727e+84\n",
      "Gradient Descent(37/49): loss=4.462290640521353e+86\n",
      "Gradient Descent(38/49): loss=1.0830429223827673e+89\n",
      "Gradient Descent(39/49): loss=2.628654353151601e+91\n",
      "Gradient Descent(40/49): loss=6.380009107248265e+93\n",
      "Gradient Descent(41/49): loss=1.5484925265951634e+96\n",
      "Gradient Descent(42/49): loss=3.7583474641077214e+98\n",
      "Gradient Descent(43/49): loss=9.121888170828612e+100\n",
      "Gradient Descent(44/49): loss=2.2139742159486067e+103\n",
      "Gradient Descent(45/49): loss=5.373538610745716e+105\n",
      "Gradient Descent(46/49): loss=1.3042119909604826e+108\n",
      "Gradient Descent(47/49): loss=3.1654539784334935e+110\n",
      "Gradient Descent(48/49): loss=7.682875912067946e+112\n",
      "Gradient Descent(49/49): loss=1.8647114342014433e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.377669911850116\n",
      "Gradient Descent(2/49): loss=1527.6924980134943\n",
      "Gradient Descent(3/49): loss=359432.2308813421\n",
      "Gradient Descent(4/49): loss=85878299.96199077\n",
      "Gradient Descent(5/49): loss=20607279132.00086\n",
      "Gradient Descent(6/49): loss=4951504579509.919\n",
      "Gradient Descent(7/49): loss=1190258484990218.2\n",
      "Gradient Descent(8/49): loss=2.8615906077029802e+17\n",
      "Gradient Descent(9/49): loss=6.8800961886494474e+19\n",
      "Gradient Descent(10/49): loss=1.654201893737139e+22\n",
      "Gradient Descent(11/49): loss=3.9772681846990755e+24\n",
      "Gradient Descent(12/49): loss=9.562733107350177e+26\n",
      "Gradient Descent(13/49): loss=2.299214356620355e+29\n",
      "Gradient Descent(14/49): loss=5.528113881254878e+31\n",
      "Gradient Descent(15/49): loss=1.329151636313443e+34\n",
      "Gradient Descent(16/49): loss=3.1957447918632715e+36\n",
      "Gradient Descent(17/49): loss=7.68368678039398e+38\n",
      "Gradient Descent(18/49): loss=1.8474267047347772e+41\n",
      "Gradient Descent(19/49): loss=4.441859136186361e+43\n",
      "Gradient Descent(20/49): loss=1.0679780984702399e+46\n",
      "Gradient Descent(21/49): loss=2.5677924129561628e+48\n",
      "Gradient Descent(22/49): loss=6.173869937701748e+50\n",
      "Gradient Descent(23/49): loss=1.4844139976343629e+53\n",
      "Gradient Descent(24/49): loss=3.5690497833875967e+55\n",
      "Gradient Descent(25/49): loss=8.58124241391994e+57\n",
      "Gradient Descent(26/49): loss=2.063230434869382e+60\n",
      "Gradient Descent(27/49): loss=4.960726689723445e+62\n",
      "Gradient Descent(28/49): loss=1.1927319835069188e+65\n",
      "Gradient Descent(29/49): loss=2.8677443315460615e+67\n",
      "Gradient Descent(30/49): loss=6.895059128819756e+69\n",
      "Gradient Descent(31/49): loss=1.6578130716516673e+72\n",
      "Gradient Descent(32/49): loss=3.985961728814905e+74\n",
      "Gradient Descent(33/49): loss=9.583644365735487e+76\n",
      "Gradient Descent(34/49): loss=2.3042428798281898e+79\n",
      "Gradient Descent(35/49): loss=5.54020479747991e+81\n",
      "Gradient Descent(36/49): loss=1.3320587628465594e+84\n",
      "Gradient Descent(37/49): loss=3.2027345784824156e+86\n",
      "Gradient Descent(38/49): loss=7.700492700702683e+88\n",
      "Gradient Descent(39/49): loss=1.85146743760679e+91\n",
      "Gradient Descent(40/49): loss=4.451574471599011e+93\n",
      "Gradient Descent(41/49): loss=1.0703140046474069e+96\n",
      "Gradient Descent(42/49): loss=2.573408747518675e+98\n",
      "Gradient Descent(43/49): loss=6.187373567990696e+100\n",
      "Gradient Descent(44/49): loss=1.4876607420715298e+103\n",
      "Gradient Descent(45/49): loss=3.5768560911693815e+105\n",
      "Gradient Descent(46/49): loss=8.60001150472008e+107\n",
      "Gradient Descent(47/49): loss=2.0677431799370456e+110\n",
      "Gradient Descent(48/49): loss=4.971576905251289e+112\n",
      "Gradient Descent(49/49): loss=1.1953407543378013e+115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.34849904469733\n",
      "Gradient Descent(2/49): loss=1521.4864990325966\n",
      "Gradient Descent(3/49): loss=357381.01938623015\n",
      "Gradient Descent(4/49): loss=85236196.4814449\n",
      "Gradient Descent(5/49): loss=20418637783.479336\n",
      "Gradient Descent(6/49): loss=4898216063264.5625\n",
      "Gradient Descent(7/49): loss=1175577640488146.5\n",
      "Gradient Descent(8/49): loss=2.8218460032718198e+17\n",
      "Gradient Descent(9/49): loss=6.773900492599133e+19\n",
      "Gradient Descent(10/49): loss=1.626119315735872e+22\n",
      "Gradient Descent(11/49): loss=3.903631336004089e+24\n",
      "Gradient Descent(12/49): loss=9.371004493217701e+26\n",
      "Gradient Descent(13/49): loss=2.2495923650047775e+29\n",
      "Gradient Descent(14/49): loss=5.400346517369499e+31\n",
      "Gradient Descent(15/49): loss=1.2964013226269876e+34\n",
      "Gradient Descent(16/49): loss=3.112127040322465e+36\n",
      "Gradient Descent(17/49): loss=7.470938706800957e+38\n",
      "Gradient Descent(18/49): loss=1.7934655188539996e+41\n",
      "Gradient Descent(19/49): loss=4.305374060394277e+43\n",
      "Gradient Descent(20/49): loss=1.0335434728679205e+46\n",
      "Gradient Descent(21/49): loss=2.4811133608586243e+48\n",
      "Gradient Descent(22/49): loss=5.9561340875965645e+50\n",
      "Gradient Descent(23/49): loss=1.4298231523694093e+53\n",
      "Gradient Descent(24/49): loss=3.4324181037530206e+55\n",
      "Gradient Descent(25/49): loss=8.239826036859219e+57\n",
      "Gradient Descent(26/49): loss=1.978043789115265e+60\n",
      "Gradient Descent(27/49): loss=4.748470676633264e+62\n",
      "Gradient Descent(28/49): loss=1.1399127709368502e+65\n",
      "Gradient Descent(29/49): loss=2.736462355636287e+67\n",
      "Gradient Descent(30/49): loss=6.5691221422673396e+69\n",
      "Gradient Descent(31/49): loss=1.5769764064593952e+72\n",
      "Gradient Descent(32/49): loss=3.785672625157233e+74\n",
      "Gradient Descent(33/49): loss=9.087845047118611e+76\n",
      "Gradient Descent(34/49): loss=2.1816183219754163e+79\n",
      "Gradient Descent(35/49): loss=5.237169513896894e+81\n",
      "Gradient Descent(36/49): loss=1.257229288964515e+84\n",
      "Gradient Descent(37/49): loss=3.018091128874118e+86\n",
      "Gradient Descent(38/49): loss=7.245197150705074e+88\n",
      "Gradient Descent(39/49): loss=1.7392742469034363e+91\n",
      "Gradient Descent(40/49): loss=4.17528307790374e+93\n",
      "Gradient Descent(41/49): loss=1.0023139715698364e+96\n",
      "Gradient Descent(42/49): loss=2.4061441556400955e+98\n",
      "Gradient Descent(43/49): loss=5.776163818861389e+100\n",
      "Gradient Descent(44/49): loss=1.3866196829527668e+103\n",
      "Gradient Descent(45/49): loss=3.328704319073576e+105\n",
      "Gradient Descent(46/49): loss=7.990851839218054e+107\n",
      "Gradient Descent(47/49): loss=1.9182753106201405e+110\n",
      "Gradient Descent(48/49): loss=4.604991109051627e+112\n",
      "Gradient Descent(49/49): loss=1.1054692200354155e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.445065104374373\n",
      "Gradient Descent(2/49): loss=1572.2577487314559\n",
      "Gradient Descent(3/49): loss=376637.1366849014\n",
      "Gradient Descent(4/49): loss=91553793.10736293\n",
      "Gradient Descent(5/49): loss=22344497449.429623\n",
      "Gradient Descent(6/49): loss=5460012520885.062\n",
      "Gradient Descent(7/49): loss=1334702198061863.2\n",
      "Gradient Descent(8/49): loss=3.2630927622668896e+17\n",
      "Gradient Descent(9/49): loss=7.977966799554626e+19\n",
      "Gradient Descent(10/49): loss=1.950566959010886e+22\n",
      "Gradient Descent(11/49): loss=4.7690450199428305e+24\n",
      "Gradient Descent(12/49): loss=1.166010908917009e+27\n",
      "Gradient Descent(13/49): loss=2.8508476782846075e+29\n",
      "Gradient Descent(14/49): loss=6.970203887772869e+31\n",
      "Gradient Descent(15/49): loss=1.7041859212848175e+34\n",
      "Gradient Descent(16/49): loss=4.1666639182857287e+36\n",
      "Gradient Descent(17/49): loss=1.018732057898324e+39\n",
      "Gradient Descent(18/49): loss=2.4907576614800584e+41\n",
      "Gradient Descent(19/49): loss=6.089799259719358e+43\n",
      "Gradient Descent(20/49): loss=1.4889306818058202e+46\n",
      "Gradient Descent(21/49): loss=3.640373813361523e+48\n",
      "Gradient Descent(22/49): loss=8.900563110989033e+50\n",
      "Gradient Descent(23/49): loss=2.176150795352616e+53\n",
      "Gradient Descent(24/49): loss=5.320598511658578e+55\n",
      "Gradient Descent(25/49): loss=1.300864286737024e+58\n",
      "Gradient Descent(26/49): loss=3.180559271292858e+60\n",
      "Gradient Descent(27/49): loss=7.7763356111357145e+62\n",
      "Gradient Descent(28/49): loss=1.9012818306146008e+65\n",
      "Gradient Descent(29/49): loss=4.648555283864992e+67\n",
      "Gradient Descent(30/49): loss=1.1365525025905252e+70\n",
      "Gradient Descent(31/49): loss=2.778823768383313e+72\n",
      "Gradient Descent(32/49): loss=6.794108955047609e+74\n",
      "Gradient Descent(33/49): loss=1.6611314836965566e+77\n",
      "Gradient Descent(34/49): loss=4.061397637843116e+79\n",
      "Gradient Descent(35/49): loss=9.929948914080568e+81\n",
      "Gradient Descent(36/49): loss=2.4278313582861806e+84\n",
      "Gradient Descent(37/49): loss=5.93594705801516e+86\n",
      "Gradient Descent(38/49): loss=1.451314456224501e+89\n",
      "Gradient Descent(39/49): loss=3.5484037008081304e+91\n",
      "Gradient Descent(40/49): loss=8.67569999727276e+93\n",
      "Gradient Descent(41/49): loss=2.1211726959234285e+96\n",
      "Gradient Descent(42/49): loss=5.1861793369358795e+98\n",
      "Gradient Descent(43/49): loss=1.2679993555711673e+101\n",
      "Gradient Descent(44/49): loss=3.100205876565071e+103\n",
      "Gradient Descent(45/49): loss=7.579874890992506e+105\n",
      "Gradient Descent(46/49): loss=1.8532479987024869e+108\n",
      "Gradient Descent(47/49): loss=4.5311145554343723e+110\n",
      "Gradient Descent(48/49): loss=1.1078387311813452e+113\n",
      "Gradient Descent(49/49): loss=2.7086197872298763e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.411945611514636\n",
      "Gradient Descent(2/49): loss=1561.5357826272402\n",
      "Gradient Descent(3/49): loss=374006.9670762684\n",
      "Gradient Descent(4/49): loss=91016729.03674898\n",
      "Gradient Descent(5/49): loss=22249857556.875816\n",
      "Gradient Descent(6/49): loss=5446851656829.014\n",
      "Gradient Descent(7/49): loss=1334019586966658.2\n",
      "Gradient Descent(8/49): loss=3.267716242308221e+17\n",
      "Gradient Descent(9/49): loss=8.004759153606435e+19\n",
      "Gradient Descent(10/49): loss=1.96091826207705e+22\n",
      "Gradient Descent(11/49): loss=4.8036698815403415e+24\n",
      "Gradient Descent(12/49): loss=1.1767592895398089e+27\n",
      "Gradient Descent(13/49): loss=2.882719534329979e+29\n",
      "Gradient Descent(14/49): loss=7.061829677660132e+31\n",
      "Gradient Descent(15/49): loss=1.7299442885844566e+34\n",
      "Gradient Descent(16/49): loss=4.237863909972207e+36\n",
      "Gradient Descent(17/49): loss=1.0381542793903662e+39\n",
      "Gradient Descent(18/49): loss=2.543178199210289e+41\n",
      "Gradient Descent(19/49): loss=6.2300522062604e+43\n",
      "Gradient Descent(20/49): loss=1.5261828886583449e+46\n",
      "Gradient Descent(21/49): loss=3.7387073699455834e+48\n",
      "Gradient Descent(22/49): loss=9.158753451133026e+50\n",
      "Gradient Descent(23/49): loss=2.243630123446008e+53\n",
      "Gradient Descent(24/49): loss=5.496245922233931e+55\n",
      "Gradient Descent(25/49): loss=1.346421539006343e+58\n",
      "Gradient Descent(26/49): loss=3.298343972141932e+60\n",
      "Gradient Descent(27/49): loss=8.079990287882185e+62\n",
      "Gradient Descent(28/49): loss=1.9793642992873417e+65\n",
      "Gradient Descent(29/49): loss=4.848870963581584e+67\n",
      "Gradient Descent(30/49): loss=1.1878333680126494e+70\n",
      "Gradient Descent(31/49): loss=2.9098487478043633e+72\n",
      "Gradient Descent(32/49): loss=7.128289171792676e+74\n",
      "Gradient Descent(33/49): loss=1.7462250075725549e+77\n",
      "Gradient Descent(34/49): loss=4.2777470211760694e+79\n",
      "Gradient Descent(35/49): loss=1.047924494141715e+82\n",
      "Gradient Descent(36/49): loss=2.567112407503376e+84\n",
      "Gradient Descent(37/49): loss=6.288684108061847e+86\n",
      "Gradient Descent(38/49): loss=1.5405460117522197e+89\n",
      "Gradient Descent(39/49): loss=3.773892874159822e+91\n",
      "Gradient Descent(40/49): loss=9.244947776298547e+93\n",
      "Gradient Descent(41/49): loss=2.264745244140373e+96\n",
      "Gradient Descent(42/49): loss=5.547971870653471e+98\n",
      "Gradient Descent(43/49): loss=1.359092902709473e+101\n",
      "Gradient Descent(44/49): loss=3.329385154178333e+103\n",
      "Gradient Descent(45/49): loss=8.15603222028791e+105\n",
      "Gradient Descent(46/49): loss=1.99799237690754e+108\n",
      "Gradient Descent(47/49): loss=4.89450437462805e+110\n",
      "Gradient Descent(48/49): loss=1.199012235989208e+113\n",
      "Gradient Descent(49/49): loss=2.937233746289361e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.443920304074358\n",
      "Gradient Descent(2/49): loss=1556.3394053049112\n",
      "Gradient Descent(3/49): loss=369578.8573310879\n",
      "Gradient Descent(4/49): loss=89124790.22864851\n",
      "Gradient Descent(5/49): loss=21585526378.689938\n",
      "Gradient Descent(6/49): loss=5234881660166.412\n",
      "Gradient Descent(7/49): loss=1270103115921534.5\n",
      "Gradient Descent(8/49): loss=3.082004987091448e+17\n",
      "Gradient Descent(9/49): loss=7.479086159091524e+19\n",
      "Gradient Descent(10/49): loss=1.8149754320250988e+22\n",
      "Gradient Descent(11/49): loss=4.4044869904079496e+24\n",
      "Gradient Descent(12/49): loss=1.0688597213718702e+27\n",
      "Gradient Descent(13/49): loss=2.5938589822301625e+29\n",
      "Gradient Descent(14/49): loss=6.2946574733981365e+31\n",
      "Gradient Descent(15/49): loss=1.527558583254226e+34\n",
      "Gradient Descent(16/49): loss=3.707009144067249e+36\n",
      "Gradient Descent(17/49): loss=8.995999928441465e+38\n",
      "Gradient Descent(18/49): loss=2.183108047193875e+41\n",
      "Gradient Descent(19/49): loss=5.297866594095959e+43\n",
      "Gradient Descent(20/49): loss=1.285661994316015e+46\n",
      "Gradient Descent(21/49): loss=3.119985629018319e+48\n",
      "Gradient Descent(22/49): loss=7.571438191892538e+50\n",
      "Gradient Descent(23/49): loss=1.8374019341999195e+53\n",
      "Gradient Descent(24/49): loss=4.4589228390326756e+55\n",
      "Gradient Descent(25/49): loss=1.0820709674027996e+58\n",
      "Gradient Descent(26/49): loss=2.6259202519651183e+60\n",
      "Gradient Descent(27/49): loss=6.37246296907143e+62\n",
      "Gradient Descent(28/49): loss=1.5464401198703343e+65\n",
      "Gradient Descent(29/49): loss=3.752830037540584e+67\n",
      "Gradient Descent(30/49): loss=9.107196010827596e+69\n",
      "Gradient Descent(31/49): loss=2.2100926061119953e+72\n",
      "Gradient Descent(32/49): loss=5.363351487970279e+74\n",
      "Gradient Descent(33/49): loss=1.301553568568217e+77\n",
      "Gradient Descent(34/49): loss=3.158550573558924e+79\n",
      "Gradient Descent(35/49): loss=7.66502583270856e+81\n",
      "Gradient Descent(36/49): loss=1.8601133541417198e+84\n",
      "Gradient Descent(37/49): loss=4.514037872503433e+86\n",
      "Gradient Descent(38/49): loss=1.0954460312337962e+89\n",
      "Gradient Descent(39/49): loss=2.658378244133706e+91\n",
      "Gradient Descent(40/49): loss=6.451230537504388e+93\n",
      "Gradient Descent(41/49): loss=1.5655550725284888e+96\n",
      "Gradient Descent(42/49): loss=3.799217328959111e+98\n",
      "Gradient Descent(43/49): loss=9.219766564552194e+100\n",
      "Gradient Descent(44/49): loss=2.2374107123827786e+103\n",
      "Gradient Descent(45/49): loss=5.429645816773863e+105\n",
      "Gradient Descent(46/49): loss=1.317641572575347e+108\n",
      "Gradient Descent(47/49): loss=3.19759220466138e+110\n",
      "Gradient Descent(48/49): loss=7.759770274496695e+112\n",
      "Gradient Descent(49/49): loss=1.883105501232581e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.414145707578716\n",
      "Gradient Descent(2/49): loss=1549.8927188814569\n",
      "Gradient Descent(3/49): loss=367425.9177109126\n",
      "Gradient Descent(4/49): loss=88444350.49824859\n",
      "Gradient Descent(5/49): loss=21383677499.45327\n",
      "Gradient Descent(6/49): loss=5177307398149.986\n",
      "Gradient Descent(7/49): loss=1254088085974297.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8/49): loss=3.038231635731374e+17\n",
      "Gradient Descent(9/49): loss=7.36100796511964e+19\n",
      "Gradient Descent(10/49): loss=1.783453574176311e+22\n",
      "Gradient Descent(11/49): loss=4.3210483831928244e+24\n",
      "Gradient Descent(12/49): loss=1.0469294502041621e+27\n",
      "Gradient Descent(13/49): loss=2.536565248850481e+29\n",
      "Gradient Descent(14/49): loss=6.145748379174607e+31\n",
      "Gradient Descent(15/49): loss=1.4890303581256434e+34\n",
      "Gradient Descent(16/49): loss=3.6077160029936806e+36\n",
      "Gradient Descent(17/49): loss=8.74100036308426e+38\n",
      "Gradient Descent(18/49): loss=2.1178243402102894e+41\n",
      "Gradient Descent(19/49): loss=5.13119752380394e+43\n",
      "Gradient Descent(20/49): loss=1.24321869100802e+46\n",
      "Gradient Descent(21/49): loss=3.0121481520510784e+48\n",
      "Gradient Descent(22/49): loss=7.298021302301001e+50\n",
      "Gradient Descent(23/49): loss=1.7682103349918953e+53\n",
      "Gradient Descent(24/49): loss=4.2841308065332024e+55\n",
      "Gradient Descent(25/49): loss=1.0379860588028999e+58\n",
      "Gradient Descent(26/49): loss=2.514897669853987e+60\n",
      "Gradient Descent(27/49): loss=6.093251673469423e+62\n",
      "Gradient Descent(28/49): loss=1.4763111995088437e+65\n",
      "Gradient Descent(29/49): loss=3.5768992889051924e+67\n",
      "Gradient Descent(30/49): loss=8.666335747657497e+69\n",
      "Gradient Descent(31/49): loss=2.099734133529779e+72\n",
      "Gradient Descent(32/49): loss=5.087367440964628e+74\n",
      "Gradient Descent(33/49): loss=1.232599264168696e+77\n",
      "Gradient Descent(34/49): loss=2.986418739475074e+79\n",
      "Gradient Descent(35/49): loss=7.235682469357096e+81\n",
      "Gradient Descent(36/49): loss=1.7531064919102376e+84\n",
      "Gradient Descent(37/49): loss=4.247536269029924e+86\n",
      "Gradient Descent(38/49): loss=1.0291197049339603e+89\n",
      "Gradient Descent(39/49): loss=2.4934157120811047e+91\n",
      "Gradient Descent(40/49): loss=6.041203839986611e+93\n",
      "Gradient Descent(41/49): loss=1.4637007242489813e+96\n",
      "Gradient Descent(42/49): loss=3.546345839195749e+98\n",
      "Gradient Descent(43/49): loss=8.592308935034482e+100\n",
      "Gradient Descent(44/49): loss=2.0817984534699614e+103\n",
      "Gradient Descent(45/49): loss=5.043911751355737e+105\n",
      "Gradient Descent(46/49): loss=1.2220705473701919e+108\n",
      "Gradient Descent(47/49): loss=2.9609091046215497e+110\n",
      "Gradient Descent(48/49): loss=7.173876127443416e+112\n",
      "Gradient Descent(49/49): loss=1.7381316640748677e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.511625825579478\n",
      "Gradient Descent(2/49): loss=1601.594063832577\n",
      "Gradient Descent(3/49): loss=387216.9269771142\n",
      "Gradient Descent(4/49): loss=94997731.22937633\n",
      "Gradient Descent(5/49): loss=23399954058.65441\n",
      "Gradient Descent(6/49): loss=5770936609061.127\n",
      "Gradient Descent(7/49): loss=1423788834854388.0\n",
      "Gradient Descent(8/49): loss=3.513170954958986e+17\n",
      "Gradient Descent(9/49): loss=8.669035650590972e+19\n",
      "Gradient Descent(10/49): loss=2.1391840177108246e+22\n",
      "Gradient Descent(11/49): loss=5.278705435485742e+24\n",
      "Gradient Descent(12/49): loss=1.3025888179123364e+27\n",
      "Gradient Descent(13/49): loss=3.2143078772431126e+29\n",
      "Gradient Descent(14/49): loss=7.931725368374637e+31\n",
      "Gradient Descent(15/49): loss=1.957257145826461e+34\n",
      "Gradient Descent(16/49): loss=4.8297885052428275e+36\n",
      "Gradient Descent(17/49): loss=1.1918136146830592e+39\n",
      "Gradient Descent(18/49): loss=2.940956303307253e+41\n",
      "Gradient Descent(19/49): loss=7.257195149279738e+43\n",
      "Gradient Descent(20/49): loss=1.7908080234401144e+46\n",
      "Gradient Descent(21/49): loss=4.419053520080648e+48\n",
      "Gradient Descent(22/49): loss=1.0904593768939153e+51\n",
      "Gradient Descent(23/49): loss=2.690851439705741e+53\n",
      "Gradient Descent(24/49): loss=6.64002861913891e+55\n",
      "Gradient Descent(25/49): loss=1.638514093064927e+58\n",
      "Gradient Descent(26/49): loss=4.0432482857599985e+60\n",
      "Gradient Descent(27/49): loss=9.977245096331859e+62\n",
      "Gradient Descent(28/49): loss=2.4620159999293607e+65\n",
      "Gradient Descent(29/49): loss=6.075347177886527e+67\n",
      "Gradient Descent(30/49): loss=1.4991715461196457e+70\n",
      "Gradient Descent(31/49): loss=3.699402287453542e+72\n",
      "Gradient Descent(32/49): loss=9.128760027389299e+74\n",
      "Gradient Descent(33/49): loss=2.2526411880180444e+77\n",
      "Gradient Descent(34/49): loss=5.558687386600727e+79\n",
      "Gradient Descent(35/49): loss=1.3716789707259085e+82\n",
      "Gradient Descent(36/49): loss=3.384797647133541e+84\n",
      "Gradient Descent(37/49): loss=8.352431841961983e+86\n",
      "Gradient Descent(38/49): loss=2.0610720328791347e+89\n",
      "Gradient Descent(39/49): loss=5.085965387199939e+91\n",
      "Gradient Descent(40/49): loss=1.2550286213753497e+94\n",
      "Gradient Descent(41/49): loss=3.0969476206728014e+96\n",
      "Gradient Descent(42/49): loss=7.642124172977341e+98\n",
      "Gradient Descent(43/49): loss=1.8857943055077128e+101\n",
      "Gradient Descent(44/49): loss=4.6534446211436224e+103\n",
      "Gradient Descent(45/49): loss=1.1482984532727492e+106\n",
      "Gradient Descent(46/49): loss=2.8335769416861033e+108\n",
      "Gradient Descent(47/49): loss=6.992222502409014e+110\n",
      "Gradient Descent(48/49): loss=1.7254225499909177e+113\n",
      "Gradient Descent(49/49): loss=4.257706294374185e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.47816190818688\n",
      "Gradient Descent(2/49): loss=1590.673587882392\n",
      "Gradient Descent(3/49): loss=384512.7776237947\n",
      "Gradient Descent(4/49): loss=94440326.31774504\n",
      "Gradient Descent(5/49): loss=23300788538.27276\n",
      "Gradient Descent(6/49): loss=5757008527102.757\n",
      "Gradient Descent(7/49): loss=1423055393221953.5\n",
      "Gradient Descent(8/49): loss=3.518133734985956e+17\n",
      "Gradient Descent(9/49): loss=8.698106844333212e+19\n",
      "Gradient Descent(10/49): loss=2.1505247319128424e+22\n",
      "Gradient Descent(11/49): loss=5.316999188618743e+24\n",
      "Gradient Descent(12/49): loss=1.314587710269299e+27\n",
      "Gradient Descent(13/49): loss=3.2502204399571925e+29\n",
      "Gradient Descent(14/49): loss=8.035930257047269e+31\n",
      "Gradient Descent(15/49): loss=1.986824613901744e+34\n",
      "Gradient Descent(16/49): loss=4.912277750940502e+36\n",
      "Gradient Descent(17/49): loss=1.2145245598189934e+39\n",
      "Gradient Descent(18/49): loss=3.0028226927922243e+41\n",
      "Gradient Descent(19/49): loss=7.424258373048134e+43\n",
      "Gradient Descent(20/49): loss=1.8355933082169377e+46\n",
      "Gradient Descent(21/49): loss=4.538369523541754e+48\n",
      "Gradient Descent(22/49): loss=1.1220785040300755e+51\n",
      "Gradient Descent(23/49): loss=2.7742566194527213e+53\n",
      "Gradient Descent(24/49): loss=6.859145561530638e+55\n",
      "Gradient Descent(25/49): loss=1.6958733198808239e+58\n",
      "Gradient Descent(26/49): loss=4.192922123150067e+60\n",
      "Gradient Descent(27/49): loss=1.0366691736172494e+63\n",
      "Gradient Descent(28/49): loss=2.5630883282919976e+65\n",
      "Gradient Descent(29/49): loss=6.337047484207604e+67\n",
      "Gradient Descent(30/49): loss=1.5667884081022957e+70\n",
      "Gradient Descent(31/49): loss=3.873769167552161e+72\n",
      "Gradient Descent(32/49): loss=9.577609513752548e+74\n",
      "Gradient Descent(33/49): loss=2.367993549184243e+77\n",
      "Gradient Descent(34/49): loss=5.854689983890561e+79\n",
      "Gradient Descent(35/49): loss=1.4475290618623804e+82\n",
      "Gradient Descent(36/49): loss=3.5789092004898617e+84\n",
      "Gradient Descent(37/49): loss=8.848589919757157e+86\n",
      "Gradient Descent(38/49): loss=2.187748813446047e+89\n",
      "Gradient Descent(39/49): loss=5.409048124207712e+91\n",
      "Gradient Descent(40/49): loss=1.3373473878799342e+94\n",
      "Gradient Descent(41/49): loss=3.306493110802662e+96\n",
      "Gradient Descent(42/49): loss=8.175061162767221e+98\n",
      "Gradient Descent(43/49): loss=2.021223779255406e+101\n",
      "Gradient Descent(44/49): loss=4.997327218093426e+103\n",
      "Gradient Descent(45/49): loss=1.2355524203211737e+106\n",
      "Gradient Descent(46/49): loss=3.054812536257999e+108\n",
      "Gradient Descent(47/49): loss=7.552799442740977e+110\n",
      "Gradient Descent(48/49): loss=1.8673741431003065e+113\n",
      "Gradient Descent(49/49): loss=4.6169453018788305e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.510485625856505\n",
      "Gradient Descent(2/49): loss=1585.3874849134504\n",
      "Gradient Descent(3/49): loss=379963.07118618686\n",
      "Gradient Descent(4/49): loss=92478165.20753121\n",
      "Gradient Descent(5/49): loss=22605371596.62723\n",
      "Gradient Descent(6/49): loss=5533054875242.712\n",
      "Gradient Descent(7/49): loss=1354897709557321.0\n",
      "Gradient Descent(8/49): loss=3.3182599822859066e+17\n",
      "Gradient Descent(9/49): loss=8.127092532054047e+19\n",
      "Gradient Descent(10/49): loss=1.99052224009063e+22\n",
      "Gradient Descent(11/49): loss=4.87529880086324e+24\n",
      "Gradient Descent(12/49): loss=1.19408775310385e+27\n",
      "Gradient Descent(13/49): loss=2.924633976957011e+29\n",
      "Gradient Descent(14/49): loss=7.163196910595198e+31\n",
      "Gradient Descent(15/49): loss=1.7544552256720723e+34\n",
      "Gradient Descent(16/49): loss=4.2971221530721884e+36\n",
      "Gradient Descent(17/49): loss=1.0524782096929019e+39\n",
      "Gradient Descent(18/49): loss=2.577795895657628e+41\n",
      "Gradient Descent(19/49): loss=6.313700012662377e+43\n",
      "Gradient Descent(20/49): loss=1.5463911603556147e+46\n",
      "Gradient Descent(21/49): loss=3.7875185965202374e+48\n",
      "Gradient Descent(22/49): loss=9.276629023283313e+50\n",
      "Gradient Descent(23/49): loss=2.2720903896216944e+53\n",
      "Gradient Descent(24/49): loss=5.564946841870573e+55\n",
      "Gradient Descent(25/49): loss=1.36300181957306e+58\n",
      "Gradient Descent(26/49): loss=3.338349876377051e+60\n",
      "Gradient Descent(27/49): loss=8.176496712674456e+62\n",
      "Gradient Descent(28/49): loss=2.002639057261868e+65\n",
      "Gradient Descent(29/49): loss=4.904989672965906e+67\n",
      "Gradient Descent(30/49): loss=1.2013609544196705e+70\n",
      "Gradient Descent(31/49): loss=2.94244889190856e+72\n",
      "Gradient Descent(32/49): loss=7.206831094053838e+74\n",
      "Gradient Descent(33/49): loss=1.7651424485586447e+77\n",
      "Gradient Descent(34/49): loss=4.323298025222628e+79\n",
      "Gradient Descent(35/49): loss=1.0588893734982297e+82\n",
      "Gradient Descent(36/49): loss=2.593498525352603e+84\n",
      "Gradient Descent(37/49): loss=6.352159885016888e+86\n",
      "Gradient Descent(38/49): loss=1.555810994699981e+89\n",
      "Gradient Descent(39/49): loss=3.810590248111989e+91\n",
      "Gradient Descent(40/49): loss=9.333137565213296e+93\n",
      "Gradient Descent(41/49): loss=2.2859308175250286e+96\n",
      "Gradient Descent(42/49): loss=5.598845689350135e+98\n",
      "Gradient Descent(43/49): loss=1.3713045387390207e+101\n",
      "Gradient Descent(44/49): loss=3.3586854189305342e+103\n",
      "Gradient Descent(45/49): loss=8.22630380390175e+105\n",
      "Gradient Descent(46/49): loss=2.0148381236500837e+108\n",
      "Gradient Descent(47/49): loss=4.9348683944644656e+110\n",
      "Gradient Descent(48/49): loss=1.208679039017121e+113\n",
      "Gradient Descent(49/49): loss=2.9603728054796274e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.480102938252984\n",
      "Gradient Descent(2/49): loss=1578.6950143010952\n",
      "Gradient Descent(3/49): loss=377705.0043956067\n",
      "Gradient Descent(4/49): loss=91757668.26540081\n",
      "Gradient Descent(5/49): loss=22389574368.996613\n",
      "Gradient Descent(6/49): loss=5470907142156.352\n",
      "Gradient Descent(7/49): loss=1337444319564373.8\n",
      "Gradient Descent(8/49): loss=3.270099458044582e+17\n",
      "Gradient Descent(9/49): loss=7.995945665916741e+19\n",
      "Gradient Descent(10/49): loss=1.9551804395802073e+22\n",
      "Gradient Descent(11/49): loss=4.7808669939383774e+24\n",
      "Gradient Descent(12/49): loss=1.1690348274103485e+27\n",
      "Gradient Descent(13/49): loss=2.8585682987014204e+29\n",
      "Gradient Descent(14/49): loss=6.989881488328171e+31\n",
      "Gradient Descent(15/49): loss=1.7091929463855522e+34\n",
      "Gradient Descent(16/49): loss=4.179385052579118e+36\n",
      "Gradient Descent(17/49): loss=1.021959495403423e+39\n",
      "Gradient Descent(18/49): loss=2.4989351330111938e+41\n",
      "Gradient Descent(19/49): loss=6.110493454306475e+43\n",
      "Gradient Descent(20/49): loss=1.4941616442498714e+46\n",
      "Gradient Descent(21/49): loss=3.6535822126211465e+48\n",
      "Gradient Descent(22/49): loss=8.933881442131546e+50\n",
      "Gradient Descent(23/49): loss=2.184547465439548e+53\n",
      "Gradient Descent(24/49): loss=5.341740496223582e+55\n",
      "Gradient Descent(25/49): loss=1.306183178918387e+58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=3.1939299524131217e+60\n",
      "Gradient Descent(27/49): loss=7.809921843712443e+62\n",
      "Gradient Descent(28/49): loss=1.909712489430724e+65\n",
      "Gradient Descent(29/49): loss=4.6697033149236e+67\n",
      "Gradient Descent(30/49): loss=1.141854031436369e+70\n",
      "Gradient Descent(31/49): loss=2.792105924461344e+72\n",
      "Gradient Descent(32/49): loss=6.827366089521544e+74\n",
      "Gradient Descent(33/49): loss=1.6694541318070384e+77\n",
      "Gradient Descent(34/49): loss=4.08221422677936e+79\n",
      "Gradient Descent(35/49): loss=9.981989127956475e+81\n",
      "Gradient Descent(36/49): loss=2.44083483656985e+84\n",
      "Gradient Descent(37/49): loss=5.968424352143772e+86\n",
      "Gradient Descent(38/49): loss=1.459422354743306e+89\n",
      "Gradient Descent(39/49): loss=3.568636349993219e+91\n",
      "Gradient Descent(40/49): loss=8.72616851256391e+93\n",
      "Gradient Descent(41/49): loss=2.1337566913985751e+96\n",
      "Gradient Descent(42/49): loss=5.2175449185205234e+98\n",
      "Gradient Descent(43/49): loss=1.2758143928273305e+101\n",
      "Gradient Descent(44/49): loss=3.1196710145562617e+103\n",
      "Gradient Descent(45/49): loss=7.628340998328647e+105\n",
      "Gradient Descent(46/49): loss=1.8653116343121418e+108\n",
      "Gradient Descent(47/49): loss=4.561132615679562e+110\n",
      "Gradient Descent(48/49): loss=1.1153059014445903e+113\n",
      "Gradient Descent(49/49): loss=2.7271894036165716e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.578501393416348\n",
      "Gradient Descent(2/49): loss=1631.3391726893667\n",
      "Gradient Descent(3/49): loss=398043.22889499523\n",
      "Gradient Descent(4/49): loss=98554496.8477188\n",
      "Gradient Descent(5/49): loss=24500070358.497234\n",
      "Gradient Descent(6/49): loss=6098014740115.857\n",
      "Gradient Descent(7/49): loss=1518370915130865.5\n",
      "Gradient Descent(8/49): loss=3.7811318126538144e+17\n",
      "Gradient Descent(9/49): loss=9.41637200384097e+19\n",
      "Gradient Descent(10/49): loss=2.3450454545544482e+22\n",
      "Gradient Descent(11/49): loss=5.840107692962154e+24\n",
      "Gradient Descent(12/49): loss=1.454424170852073e+27\n",
      "Gradient Descent(13/49): loss=3.6221090493980714e+29\n",
      "Gradient Descent(14/49): loss=9.0205294459183e+31\n",
      "Gradient Descent(15/49): loss=2.2464800109550315e+34\n",
      "Gradient Descent(16/49): loss=5.594652240264309e+36\n",
      "Gradient Descent(17/49): loss=1.393296789477731e+39\n",
      "Gradient Descent(18/49): loss=3.4698777759028108e+41\n",
      "Gradient Descent(19/49): loss=8.641412137128778e+43\n",
      "Gradient Descent(20/49): loss=2.152064382718968e+46\n",
      "Gradient Descent(21/49): loss=5.3595188311674265e+48\n",
      "Gradient Descent(22/49): loss=1.334738975884667e+51\n",
      "Gradient Descent(23/49): loss=3.3240449187315493e+53\n",
      "Gradient Descent(24/49): loss=8.278228793347822e+55\n",
      "Gradient Descent(25/49): loss=2.0616169044195195e+58\n",
      "Gradient Descent(26/49): loss=5.134267687799176e+60\n",
      "Gradient Descent(27/49): loss=1.2786422459706797e+63\n",
      "Gradient Descent(28/49): loss=3.184341161381491e+65\n",
      "Gradient Descent(29/49): loss=7.930309407515923e+67\n",
      "Gradient Descent(30/49): loss=1.97497077454012e+70\n",
      "Gradient Descent(31/49): loss=4.918483453610134e+72\n",
      "Gradient Descent(32/49): loss=1.2249031628870403e+75\n",
      "Gradient Descent(33/49): loss=3.0505089070685137e+77\n",
      "Gradient Descent(34/49): loss=7.597012461108737e+79\n",
      "Gradient Descent(35/49): loss=1.8919662289956884e+82\n",
      "Gradient Descent(36/49): loss=4.711768250986575e+84\n",
      "Gradient Descent(37/49): loss=1.173422638880282e+87\n",
      "Gradient Descent(38/49): loss=2.9223013868486827e+89\n",
      "Gradient Descent(39/49): loss=7.277723398729348e+91\n",
      "Gradient Descent(40/49): loss=1.8124502184057287e+94\n",
      "Gradient Descent(41/49): loss=4.513740924493664e+96\n",
      "Gradient Descent(42/49): loss=1.1241057506876073e+99\n",
      "Gradient Descent(43/49): loss=2.7994822030480074e+101\n",
      "Gradient Descent(44/49): loss=6.971853493666968e+103\n",
      "Gradient Descent(45/49): loss=1.7362761257862055e+106\n",
      "Gradient Descent(46/49): loss=4.3240363379890256e+108\n",
      "Gradient Descent(47/49): loss=1.0768615645039289e+111\n",
      "Gradient Descent(48/49): loss=2.681824893370709e+113\n",
      "Gradient Descent(49/49): loss=6.678838762358452e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.5446913161944575\n",
      "Gradient Descent(2/49): loss=1620.2174435725317\n",
      "Gradient Descent(3/49): loss=395263.37420895736\n",
      "Gradient Descent(4/49): loss=97976080.27517167\n",
      "Gradient Descent(5/49): loss=24396185402.08778\n",
      "Gradient Descent(6/49): loss=6083278662599.384\n",
      "Gradient Descent(7/49): loss=1517583184147208.0\n",
      "Gradient Descent(8/49): loss=3.78645708559156e+17\n",
      "Gradient Descent(9/49): loss=9.447904181812044e+19\n",
      "Gradient Descent(10/49): loss=2.357464986704985e+22\n",
      "Gradient Descent(11/49): loss=5.882439637729595e+24\n",
      "Gradient Descent(12/49): loss=1.4678123304834713e+27\n",
      "Gradient Descent(13/49): loss=3.66255251613962e+29\n",
      "Gradient Descent(14/49): loss=9.138970645084823e+31\n",
      "Gradient Descent(15/49): loss=2.280398440048012e+34\n",
      "Gradient Descent(16/49): loss=5.690156334405296e+36\n",
      "Gradient Descent(17/49): loss=1.4198343062813328e+39\n",
      "Gradient Descent(18/49): loss=3.542836688992715e+41\n",
      "Gradient Descent(19/49): loss=8.840251119991075e+43\n",
      "Gradient Descent(20/49): loss=2.2058606351007507e+46\n",
      "Gradient Descent(21/49): loss=5.5041662006445064e+48\n",
      "Gradient Descent(22/49): loss=1.3734251875788809e+51\n",
      "Gradient Descent(23/49): loss=3.427034499207207e+53\n",
      "Gradient Descent(24/49): loss=8.551296106273994e+55\n",
      "Gradient Descent(25/49): loss=2.1337592345273516e+58\n",
      "Gradient Descent(26/49): loss=5.324255427890435e+60\n",
      "Gradient Descent(27/49): loss=1.3285330135995464e+63\n",
      "Gradient Descent(28/49): loss=3.315017455733334e+65\n",
      "Gradient Descent(29/49): loss=8.271785961902578e+67\n",
      "Gradient Descent(30/49): loss=2.0640145614072866e+70\n",
      "Gradient Descent(31/49): loss=5.150225270966093e+72\n",
      "Gradient Descent(32/49): loss=1.285108198248981e+75\n",
      "Gradient Descent(33/49): loss=3.2066618338365323e+77\n",
      "Gradient Descent(34/49): loss=8.001411967174817e+79\n",
      "Gradient Descent(35/49): loss=1.9965495828991064e+82\n",
      "Gradient Descent(36/49): loss=4.981883514219425e+84\n",
      "Gradient Descent(37/49): loss=1.2431027790059878e+87\n",
      "Gradient Descent(38/49): loss=3.1018479552196807e+89\n",
      "Gradient Descent(39/49): loss=7.739875495245864e+91\n",
      "Gradient Descent(40/49): loss=1.9312897842429822e+94\n",
      "Gradient Descent(41/49): loss=4.819044224952128e+96\n",
      "Gradient Descent(42/49): loss=1.2024703610777596e+99\n",
      "Gradient Descent(43/49): loss=3.00046005343484e+101\n",
      "Gradient Descent(44/49): loss=7.486887680282739e+103\n",
      "Gradient Descent(45/49): loss=1.8681630862907558e+106\n",
      "Gradient Descent(46/49): loss=4.661527547916408e+108\n",
      "Gradient Descent(47/49): loss=1.163166066145128e+111\n",
      "Gradient Descent(48/49): loss=2.902386146009714e+113\n",
      "Gradient Descent(49/49): loss=7.242169098404655e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.57736587719656\n",
      "Gradient Descent(2/49): loss=1614.8404655575214\n",
      "Gradient Descent(3/49): loss=390589.30244023516\n",
      "Gradient Descent(4/49): loss=95941427.1197836\n",
      "Gradient Descent(5/49): loss=23668379754.930637\n",
      "Gradient Descent(6/49): loss=5846723250763.285\n",
      "Gradient Descent(7/49): loss=1444924196484365.0\n",
      "Gradient Descent(8/49): loss=3.571413090382826e+17\n",
      "Gradient Descent(9/49): loss=8.827872510370087e+19\n",
      "Gradient Descent(10/49): loss=2.1821222292921304e+22\n",
      "Gradient Descent(11/49): loss=5.393919690451299e+24\n",
      "Gradient Descent(12/49): loss=1.3333086092785626e+27\n",
      "Gradient Descent(13/49): loss=3.295772024036029e+29\n",
      "Gradient Descent(14/49): loss=8.14673771758596e+31\n",
      "Gradient Descent(15/49): loss=2.013772181826033e+34\n",
      "Gradient Descent(16/49): loss=4.9777943505683767e+36\n",
      "Gradient Descent(17/49): loss=1.230448857292533e+39\n",
      "Gradient Descent(18/49): loss=3.0415165593157832e+41\n",
      "Gradient Descent(19/49): loss=7.518250705378764e+43\n",
      "Gradient Descent(20/49): loss=1.858418081392381e+46\n",
      "Gradient Descent(21/49): loss=4.5937783946816405e+48\n",
      "Gradient Descent(22/49): loss=1.13552489353738e+51\n",
      "Gradient Descent(23/49): loss=2.806876329410458e+53\n",
      "Gradient Descent(24/49): loss=6.938249239163969e+55\n",
      "Gradient Descent(25/49): loss=1.7150489318116192e+58\n",
      "Gradient Descent(26/49): loss=4.239387685738417e+60\n",
      "Gradient Descent(27/49): loss=1.047923917308233e+63\n",
      "Gradient Descent(28/49): loss=2.5903376097471117e+65\n",
      "Gradient Descent(29/49): loss=6.402992451690468e+67\n",
      "Gradient Descent(30/49): loss=1.582740110097381e+70\n",
      "Gradient Descent(31/49): loss=3.912336731631937e+72\n",
      "Gradient Descent(32/49): loss=9.6708098847225e+74\n",
      "Gradient Descent(33/49): loss=2.3905039428299573e+77\n",
      "Gradient Descent(34/49): loss=5.9090284772458427e+79\n",
      "Gradient Descent(35/49): loss=1.4606383582687935e+82\n",
      "Gradient Descent(36/49): loss=3.610516384988811e+84\n",
      "Gradient Descent(37/49): loss=8.924747520476766e+86\n",
      "Gradient Descent(38/49): loss=2.2060866039942573e+89\n",
      "Gradient Descent(39/49): loss=5.453171748731939e+91\n",
      "Gradient Descent(40/49): loss=1.347956243754319e+94\n",
      "Gradient Descent(41/49): loss=3.3319802104138168e+96\n",
      "Gradient Descent(42/49): loss=8.236240734096725e+98\n",
      "Gradient Descent(43/49): loss=2.0358962882786507e+101\n",
      "Gradient Descent(44/49): loss=5.032482452179487e+103\n",
      "Gradient Descent(45/49): loss=1.2439670810985895e+106\n",
      "Gradient Descent(46/49): loss=3.074931931827736e+108\n",
      "Gradient Descent(47/49): loss=7.600849354489053e+110\n",
      "Gradient Descent(48/49): loss=1.8788354406042614e+113\n",
      "Gradient Descent(49/49): loss=4.644247567919198e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.546370736720145\n",
      "Gradient Descent(2/49): loss=1607.8970507178233\n",
      "Gradient Descent(3/49): loss=388222.6272597943\n",
      "Gradient Descent(4/49): loss=95179090.893274\n",
      "Gradient Descent(5/49): loss=23437858809.920464\n",
      "Gradient Descent(6/49): loss=5779697734063.56\n",
      "Gradient Descent(7/49): loss=1425921127160965.5\n",
      "Gradient Descent(8/49): loss=3.518478482110597e+17\n",
      "Gradient Descent(9/49): loss=8.682363636006706e+19\n",
      "Gradient Descent(10/49): loss=2.142541295619849e+22\n",
      "Gradient Descent(11/49): loss=5.287169683684592e+24\n",
      "Gradient Descent(12/49): loss=1.3047228535738015e+27\n",
      "Gradient Descent(13/49): loss=3.219686800138268e+29\n",
      "Gradient Descent(14/49): loss=7.945278065062424e+31\n",
      "Gradient Descent(15/49): loss=1.9606705031061264e+34\n",
      "Gradient Descent(16/49): loss=4.838381883565794e+36\n",
      "Gradient Descent(17/49): loss=1.1939762175075597e+39\n",
      "Gradient Descent(18/49): loss=2.946396641989804e+41\n",
      "Gradient Descent(19/49): loss=7.270876133164868e+43\n",
      "Gradient Descent(20/49): loss=1.794247217590276e+46\n",
      "Gradient Descent(21/49): loss=4.427696221720442e+48\n",
      "Gradient Descent(22/49): loss=1.0926305829384318e+51\n",
      "Gradient Descent(23/49): loss=2.6963041975159347e+53\n",
      "Gradient Descent(24/49): loss=6.653718502037782e+55\n",
      "Gradient Descent(25/49): loss=1.6419501162089838e+58\n",
      "Gradient Descent(26/49): loss=4.051869917994167e+60\n",
      "Gradient Descent(27/49): loss=9.998872481130129e+62\n",
      "Gradient Descent(28/49): loss=2.4674397973616083e+65\n",
      "Gradient Descent(29/49): loss=6.088945693720838e+67\n",
      "Gradient Descent(30/49): loss=1.5025801116090347e+70\n",
      "Gradient Descent(31/49): loss=3.7079440437961245e+72\n",
      "Gradient Descent(32/49): loss=9.150160397904048e+74\n",
      "Gradient Descent(33/49): loss=2.2580015857427767e+77\n",
      "Gradient Descent(34/49): loss=5.572111241224575e+79\n",
      "Gradient Descent(35/49): loss=1.3750399415405064e+82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=3.393210865647042e+84\n",
      "Gradient Descent(37/49): loss=8.373487657271779e+86\n",
      "Gradient Descent(38/49): loss=2.0663406526347136e+89\n",
      "Gradient Descent(39/49): loss=5.0991460995620334e+91\n",
      "Gradient Descent(40/49): loss=1.2583254804344863e+94\n",
      "Gradient Descent(41/49): loss=3.105192484770493e+96\n",
      "Gradient Descent(42/49): loss=7.662739503729804e+98\n",
      "Gradient Descent(43/49): loss=1.8909480487925723e+101\n",
      "Gradient Descent(44/49): loss=4.6663266074645995e+103\n",
      "Gradient Descent(45/49): loss=1.1515178336832563e+106\n",
      "Gradient Descent(46/49): loss=2.8416213283687116e+108\n",
      "Gradient Descent(47/49): loss=7.012320206984364e+110\n",
      "Gradient Descent(48/49): loss=1.730442905758649e+113\n",
      "Gradient Descent(49/49): loss=4.27024517093209e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.645691807884982\n",
      "Gradient Descent(2/49): loss=1661.4968561279973\n",
      "Gradient Descent(3/49): loss=409120.6163203575\n",
      "Gradient Descent(4/49): loss=102227243.5859794\n",
      "Gradient Descent(5/49): loss=25646518251.83125\n",
      "Gradient Descent(6/49): loss=6442006329886.487\n",
      "Gradient Descent(7/49): loss=1618759888011612.5\n",
      "Gradient Descent(8/49): loss=4.068163218852372e+17\n",
      "Gradient Descent(9/49): loss=1.0224267324199692e+20\n",
      "Gradient Descent(10/49): loss=2.5696377721532203e+22\n",
      "Gradient Descent(11/49): loss=6.458230748787983e+24\n",
      "Gradient Descent(12/49): loss=1.6231395053562972e+27\n",
      "Gradient Descent(13/49): loss=4.0794193276846995e+29\n",
      "Gradient Descent(14/49): loss=1.0252763033921723e+32\n",
      "Gradient Descent(15/49): loss=2.5768166238497607e+34\n",
      "Gradient Descent(16/49): loss=6.476287423150786e+36\n",
      "Gradient Descent(17/49): loss=1.627678843764928e+39\n",
      "Gradient Descent(18/49): loss=4.090828974034548e+41\n",
      "Gradient Descent(19/49): loss=1.0281439590748342e+44\n",
      "Gradient Descent(20/49): loss=2.5840239411392337e+46\n",
      "Gradient Descent(21/49): loss=6.494401557196179e+48\n",
      "Gradient Descent(22/49): loss=1.6322314555827633e+51\n",
      "Gradient Descent(23/49): loss=4.102271011655609e+53\n",
      "Gradient Descent(24/49): loss=1.0310196752765623e+56\n",
      "Gradient Descent(25/49): loss=2.591251450204007e+58\n",
      "Gradient Descent(26/49): loss=6.512566383745361e+60\n",
      "Gradient Descent(27/49): loss=1.6367968033111854e+63\n",
      "Gradient Descent(28/49): loss=4.1137450545095754e+65\n",
      "Gradient Descent(29/49): loss=1.0339034350059668e+68\n",
      "Gradient Descent(30/49): loss=2.5984991747248184e+70\n",
      "Gradient Descent(31/49): loss=6.530782017381137e+72\n",
      "Gradient Descent(32/49): loss=1.6413749203159059e+75\n",
      "Gradient Descent(33/49): loss=4.125251190243177e+77\n",
      "Gradient Descent(34/49): loss=1.0367952606055159e+80\n",
      "Gradient Descent(35/49): loss=2.605767171115436e+82\n",
      "Gradient Descent(36/49): loss=6.5490486001039055e+84\n",
      "Gradient Descent(37/49): loss=1.6459658423036837e+87\n",
      "Gradient Descent(38/49): loss=4.1367895086127426e+89\n",
      "Gradient Descent(39/49): loss=1.0396951746347967e+92\n",
      "Gradient Descent(40/49): loss=2.613055496075701e+94\n",
      "Gradient Descent(41/49): loss=6.567366274417751e+96\n",
      "Gradient Descent(42/49): loss=1.6505696050900288e+99\n",
      "Gradient Descent(43/49): loss=4.1483600996330436e+101\n",
      "Gradient Descent(44/49): loss=1.0426031997171575e+104\n",
      "Gradient Descent(45/49): loss=2.6203642064646545e+106\n",
      "Gradient Descent(46/49): loss=6.585735183225853e+108\n",
      "Gradient Descent(47/49): loss=1.655186244590604e+111\n",
      "Gradient Descent(48/49): loss=4.1599630535706234e+113\n",
      "Gradient Descent(49/49): loss=1.0455193585391803e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.611533835537371\n",
      "Gradient Descent(2/49): loss=1650.1711053600252\n",
      "Gradient Descent(3/49): loss=406263.29867020395\n",
      "Gradient Descent(4/49): loss=101627125.66172272\n",
      "Gradient Descent(5/49): loss=25537712709.916252\n",
      "Gradient Descent(6/49): loss=6426419519341.208\n",
      "Gradient Descent(7/49): loss=1617914194616162.0\n",
      "Gradient Descent(8/49): loss=4.073875647539922e+17\n",
      "Gradient Descent(9/49): loss=1.0258456264794595e+20\n",
      "Gradient Descent(10/49): loss=2.58323314601015e+22\n",
      "Gradient Descent(11/49): loss=6.505005417946581e+24\n",
      "Gradient Descent(12/49): loss=1.638070343915104e+27\n",
      "Gradient Descent(13/49): loss=4.124940710784201e+29\n",
      "Gradient Descent(14/49): loss=1.0387306981943433e+32\n",
      "Gradient Descent(15/49): loss=2.615701938038342e+34\n",
      "Gradient Descent(16/49): loss=6.58678597461961e+36\n",
      "Gradient Descent(17/49): loss=1.658665660531685e+39\n",
      "Gradient Descent(18/49): loss=4.1768045774332935e+41\n",
      "Gradient Descent(19/49): loss=1.0517910214929073e+44\n",
      "Gradient Descent(20/49): loss=2.648590166776275e+46\n",
      "Gradient Descent(21/49): loss=6.669604254951656e+48\n",
      "Gradient Descent(22/49): loss=1.6795207305838682e+51\n",
      "Gradient Descent(23/49): loss=4.229321226067671e+53\n",
      "Gradient Descent(24/49): loss=1.0650156147294681e+56\n",
      "Gradient Descent(25/49): loss=2.6818919608849006e+58\n",
      "Gradient Descent(26/49): loss=6.75346388389737e+60\n",
      "Gradient Descent(27/49): loss=1.700638023317832e+63\n",
      "Gradient Descent(28/49): loss=4.282498190669947e+65\n",
      "Gradient Descent(29/49): loss=1.0784064863675105e+68\n",
      "Gradient Descent(30/49): loss=2.7156124721154668e+70\n",
      "Gradient Descent(31/49): loss=6.83837791401778e+72\n",
      "Gradient Descent(32/49): loss=1.722020832320657e+75\n",
      "Gradient Descent(33/49): loss=4.3363437707467655e+77\n",
      "Gradient Descent(34/49): loss=1.0919657268462526e+80\n",
      "Gradient Descent(35/49): loss=2.749756964959289e+82\n",
      "Gradient Descent(36/49): loss=6.924359602548841e+84\n",
      "Gradient Descent(37/49): loss=1.7436724960207695e+87\n",
      "Gradient Descent(38/49): loss=4.3908663730579966e+89\n",
      "Gradient Descent(39/49): loss=1.1056954531340994e+92\n",
      "Gradient Descent(40/49): loss=2.7843307703075703e+94\n",
      "Gradient Descent(41/49): loss=7.011422373590353e+96\n",
      "Gradient Descent(42/49): loss=1.7655963948368478e+99\n",
      "Gradient Descent(43/49): loss=4.446074510077707e+101\n",
      "Gradient Descent(44/49): loss=1.1195978088179928e+104\n",
      "Gradient Descent(45/49): loss=2.8193392860798917e+106\n",
      "Gradient Descent(46/49): loss=7.099579820029482e+108\n",
      "Gradient Descent(47/49): loss=1.7877959516909906e+111\n",
      "Gradient Descent(48/49): loss=4.5019768013108235e+113\n",
      "Gradient Descent(49/49): loss=1.1336749644371004e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.644561058094524\n",
      "Gradient Descent(2/49): loss=1644.7020932084197\n",
      "Gradient Descent(3/49): loss=401462.0427522906\n",
      "Gradient Descent(4/49): loss=99517648.13376908\n",
      "Gradient Descent(5/49): loss=24776167099.36174\n",
      "Gradient Descent(6/49): loss=6176615395922.105\n",
      "Gradient Descent(7/49): loss=1540479157696229.0\n",
      "Gradient Descent(8/49): loss=3.842587053820318e+17\n",
      "Gradient Descent(9/49): loss=9.585452016341012e+19\n",
      "Gradient Descent(10/49): loss=2.3911595407355027e+22\n",
      "Gradient Descent(11/49): loss=5.964951254100029e+24\n",
      "Gradient Descent(12/49): loss=1.4880106924243226e+27\n",
      "Gradient Descent(13/49): loss=3.7119787227017166e+29\n",
      "Gradient Descent(14/49): loss=9.259872284957041e+31\n",
      "Gradient Descent(15/49): loss=2.3099604635243e+34\n",
      "Gradient Descent(16/49): loss=5.762409360378943e+36\n",
      "Gradient Descent(17/49): loss=1.437486157651823e+39\n",
      "Gradient Descent(18/49): loss=3.585941799647608e+41\n",
      "Gradient Descent(19/49): loss=8.945462559171254e+43\n",
      "Gradient Descent(20/49): loss=2.2315281422293353e+46\n",
      "Gradient Descent(21/49): loss=5.566752772325304e+48\n",
      "Gradient Descent(22/49): loss=1.3886778231874334e+51\n",
      "Gradient Descent(23/49): loss=3.4641849126540642e+53\n",
      "Gradient Descent(24/49): loss=8.64172877879291e+55\n",
      "Gradient Descent(25/49): loss=2.155758949631478e+58\n",
      "Gradient Descent(26/49): loss=5.3777395332321426e+60\n",
      "Gradient Descent(27/49): loss=1.3415267273846405e+63\n",
      "Gradient Descent(28/49): loss=3.3465621552812145e+65\n",
      "Gradient Descent(29/49): loss=8.34830796177609e+67\n",
      "Gradient Descent(30/49): loss=2.0825624205027916e+70\n",
      "Gradient Descent(31/49): loss=5.195144040143585e+72\n",
      "Gradient Descent(32/49): loss=1.2959765974901054e+75\n",
      "Gradient Descent(33/49): loss=3.2329331550075847e+77\n",
      "Gradient Descent(34/49): loss=8.064849940183495e+79\n",
      "Gradient Descent(35/49): loss=2.0118512025815495e+82\n",
      "Gradient Descent(36/49): loss=5.01874838508989e+84\n",
      "Gradient Descent(37/49): loss=1.2519730743765775e+87\n",
      "Gradient Descent(38/49): loss=3.123162307998171e+89\n",
      "Gradient Descent(39/49): loss=7.791016437759679e+91\n",
      "Gradient Descent(40/49): loss=1.943540909737351e+94\n",
      "Gradient Descent(41/49): loss=4.8483420590868e+96\n",
      "Gradient Descent(42/49): loss=1.2094636446364621e+99\n",
      "Gradient Descent(43/49): loss=3.017118614714326e+101\n",
      "Gradient Descent(44/49): loss=7.526480664073075e+103\n",
      "Gradient Descent(45/49): loss=1.8775500210829352e+106\n",
      "Gradient Descent(46/49): loss=4.683721700762083e+108\n",
      "Gradient Descent(47/49): loss=1.1683975778997749e+111\n",
      "Gradient Descent(48/49): loss=2.9146755235690705e+113\n",
      "Gradient Descent(49/49): loss=7.270926924517655e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.612949102980188\n",
      "Gradient Descent(2/49): loss=1637.5025104449207\n",
      "Gradient Descent(3/49): loss=398983.19438523136\n",
      "Gradient Descent(4/49): loss=98711627.7207723\n",
      "Gradient Descent(5/49): loss=24530111167.66319\n",
      "Gradient Descent(6/49): loss=6104390368271.833\n",
      "Gradient Descent(7/49): loss=1519807511425866.0\n",
      "Gradient Descent(8/49): loss=3.7844609290704806e+17\n",
      "Gradient Descent(9/49): loss=9.424171820641346e+19\n",
      "Gradient Descent(10/49): loss=2.3468781705087576e+22\n",
      "Gradient Descent(11/49): loss=5.844410614995668e+24\n",
      "Gradient Descent(12/49): loss=1.4554318455999956e+27\n",
      "Gradient Descent(13/49): loss=3.624460437175217e+29\n",
      "Gradient Descent(14/49): loss=9.02599255066799e+31\n",
      "Gradient Descent(15/49): loss=2.2477428164261346e+34\n",
      "Gradient Descent(16/49): loss=5.597553845965209e+36\n",
      "Gradient Descent(17/49): loss=1.3939588274402878e+39\n",
      "Gradient Descent(18/49): loss=3.471375664377112e+41\n",
      "Gradient Descent(19/49): loss=8.6447668195745155e+43\n",
      "Gradient Descent(20/49): loss=2.1528062827140551e+46\n",
      "Gradient Descent(21/49): loss=5.36113349077178e+48\n",
      "Gradient Descent(22/49): loss=1.3350830745081435e+51\n",
      "Gradient Descent(23/49): loss=3.324757383751689e+53\n",
      "Gradient Descent(24/49): loss=8.279643321039701e+55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=2.061879578305919e+58\n",
      "Gradient Descent(26/49): loss=5.134698719007752e+60\n",
      "Gradient Descent(27/49): loss=1.2786940232779856e+63\n",
      "Gradient Descent(28/49): loss=3.1843317293657275e+65\n",
      "Gradient Descent(29/49): loss=7.929941313600125e+67\n",
      "Gradient Descent(30/49): loss=1.9747932873082954e+70\n",
      "Gradient Descent(31/49): loss=4.9178277283208034e+72\n",
      "Gradient Descent(32/49): loss=1.2246866404131734e+75\n",
      "Gradient Descent(33/49): loss=3.049837143682615e+77\n",
      "Gradient Descent(34/49): loss=7.595009446537385e+79\n",
      "Gradient Descent(35/49): loss=1.8913852043699435e+82\n",
      "Gradient Descent(36/49): loss=4.710116579170903e+84\n",
      "Gradient Descent(37/49): loss=1.1729603328884568e+87\n",
      "Gradient Descent(38/49): loss=2.921023119924502e+89\n",
      "Gradient Descent(39/49): loss=7.274223882850482e+91\n",
      "Gradient Descent(40/49): loss=1.8114999753648e+94\n",
      "Gradient Descent(41/49): loss=4.511178393179709e+96\n",
      "Gradient Descent(42/49): loss=1.1234187563813327e+99\n",
      "Gradient Descent(43/49): loss=2.797649731824955e+101\n",
      "Gradient Descent(44/49): loss=6.966987134157803e+103\n",
      "Gradient Descent(45/49): loss=1.7349888077610465e+106\n",
      "Gradient Descent(46/49): loss=4.320642632304779e+108\n",
      "Gradient Descent(47/49): loss=1.0759696358030144e+111\n",
      "Gradient Descent(48/49): loss=2.6794871867301582e+113\n",
      "Gradient Descent(49/49): loss=6.6727269478128e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.713197068985379\n",
      "Gradient Descent(2/49): loss=1692.0709123827075\n",
      "Gradient Descent(3/49): loss=420453.7264929208\n",
      "Gradient Descent(4/49): loss=106019198.18418662\n",
      "Gradient Descent(5/49): loss=26841024154.064655\n",
      "Gradient Descent(6/49): loss=6803702775375.571\n",
      "Gradient Descent(7/49): loss=1725283307171556.0\n",
      "Gradient Descent(8/49): loss=4.3755259841272006e+17\n",
      "Gradient Descent(9/49): loss=1.109731848073379e+20\n",
      "Gradient Descent(10/49): loss=2.814567921955501e+22\n",
      "Gradient Descent(11/49): loss=7.138506553856529e+24\n",
      "Gradient Descent(12/49): loss=1.8105212787076125e+27\n",
      "Gradient Descent(13/49): loss=4.5919813549166675e+29\n",
      "Gradient Descent(14/49): loss=1.1646533187853058e+32\n",
      "Gradient Descent(15/49): loss=2.9538826929852515e+34\n",
      "Gradient Descent(16/49): loss=7.491863008811944e+36\n",
      "Gradient Descent(17/49): loss=1.9001435568662528e+39\n",
      "Gradient Descent(18/49): loss=4.819289326785142e+41\n",
      "Gradient Descent(19/49): loss=1.2223049961680633e+44\n",
      "Gradient Descent(20/49): loss=3.1001033610193617e+46\n",
      "Gradient Descent(21/49): loss=7.862719108401761e+48\n",
      "Gradient Descent(22/49): loss=1.9942029209859665e+51\n",
      "Gradient Descent(23/49): loss=5.057849880265171e+53\n",
      "Gradient Descent(24/49): loss=1.2828105476203801e+56\n",
      "Gradient Descent(25/49): loss=3.253562165829188e+58\n",
      "Gradient Descent(26/49): loss=8.251933059448621e+60\n",
      "Gradient Descent(27/49): loss=2.0929183383306545e+63\n",
      "Gradient Descent(28/49): loss=5.308219467322713e+65\n",
      "Gradient Descent(29/49): loss=1.3463111960564347e+68\n",
      "Gradient Descent(30/49): loss=3.4146173642310016e+70\n",
      "Gradient Descent(31/49): loss=8.66041356430881e+72\n",
      "Gradient Descent(32/49): loss=2.196520286300214e+75\n",
      "Gradient Descent(33/49): loss=5.570982646847142e+77\n",
      "Gradient Descent(34/49): loss=1.4129552021460357e+80\n",
      "Gradient Descent(35/49): loss=3.583644986583134e+82\n",
      "Gradient Descent(36/49): loss=9.089114340183534e+84\n",
      "Gradient Descent(37/49): loss=2.305250653963286e+87\n",
      "Gradient Descent(38/49): loss=5.846752916402207e+89\n",
      "Gradient Descent(39/49): loss=1.482898165832267e+92\n",
      "Gradient Descent(40/49): loss=3.76103967735624e+94\n",
      "Gradient Descent(41/49): loss=9.539036314546183e+96\n",
      "Gradient Descent(42/49): loss=2.419363304196587e+99\n",
      "Gradient Descent(43/49): loss=6.136174142420696e+101\n",
      "Gradient Descent(44/49): loss=1.5563033894413822e+104\n",
      "Gradient Descent(45/49): loss=3.947215616392564e+106\n",
      "Gradient Descent(46/49): loss=1.001122996197148e+109\n",
      "Gradient Descent(47/49): loss=2.539124666391364e+111\n",
      "Gradient Descent(48/49): loss=6.439922063489847e+113\n",
      "Gradient Descent(49/49): loss=1.633342258959046e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.678689466215613\n",
      "Gradient Descent(2/49): loss=1680.5383462003656\n",
      "Gradient Descent(3/49): loss=417517.15575979365\n",
      "Gradient Descent(4/49): loss=105396669.90628164\n",
      "Gradient Descent(5/49): loss=26727089293.759212\n",
      "Gradient Descent(6/49): loss=6787220450191.207\n",
      "Gradient Descent(7/49): loss=1724375752041661.2\n",
      "Gradient Descent(8/49): loss=4.381651787239429e+17\n",
      "Gradient Descent(9/49): loss=1.1134374548183125e+20\n",
      "Gradient Descent(10/49): loss=2.82944438814335e+22\n",
      "Gradient Descent(11/49): loss=7.190166890830888e+24\n",
      "Gradient Descent(12/49): loss=1.8271643345937448e+27\n",
      "Gradient Descent(13/49): loss=4.643190762086834e+29\n",
      "Gradient Descent(14/49): loss=1.1799280827213105e+32\n",
      "Gradient Descent(15/49): loss=2.9984345950067186e+34\n",
      "Gradient Descent(16/49): loss=7.619625611870182e+36\n",
      "Gradient Descent(17/49): loss=1.9363001959844414e+39\n",
      "Gradient Descent(18/49): loss=4.920528448828926e+41\n",
      "Gradient Descent(19/49): loss=1.2504052981183824e+44\n",
      "Gradient Descent(20/49): loss=3.1775314913971903e+46\n",
      "Gradient Descent(21/49): loss=8.07474696009791e+48\n",
      "Gradient Descent(22/49): loss=2.051955697320907e+51\n",
      "Gradient Descent(23/49): loss=5.214432358839073e+53\n",
      "Gradient Descent(24/49): loss=1.3250921967036489e+56\n",
      "Gradient Descent(25/49): loss=3.3673259310590514e+58\n",
      "Gradient Descent(26/49): loss=8.557052825603276e+60\n",
      "Gradient Descent(27/49): loss=2.174519323620887e+63\n",
      "Gradient Descent(28/49): loss=5.525891197788052e+65\n",
      "Gradient Descent(29/49): loss=1.404240155426448e+68\n",
      "Gradient Descent(30/49): loss=3.5684568217728045e+70\n",
      "Gradient Descent(31/49): loss=9.068166894137698e+72\n",
      "Gradient Descent(32/49): loss=2.3044036940058065e+75\n",
      "Gradient Descent(33/49): loss=5.85595352063995e+77\n",
      "Gradient Descent(34/49): loss=1.4881156337796184e+80\n",
      "Gradient Descent(35/49): loss=3.781601291222853e+82\n",
      "Gradient Descent(36/49): loss=9.609809883830693e+84\n",
      "Gradient Descent(37/49): loss=2.44204607761564e+87\n",
      "Gradient Descent(38/49): loss=6.205730516305212e+89\n",
      "Gradient Descent(39/49): loss=1.5770010072292763e+92\n",
      "Gradient Descent(40/49): loss=4.007476912295607e+94\n",
      "Gradient Descent(41/49): loss=1.0183805291791723e+97\n",
      "Gradient Descent(42/49): loss=2.5879098617617935e+99\n",
      "Gradient Descent(43/49): loss=6.576399745193503e+101\n",
      "Gradient Descent(44/49): loss=1.6711955175725587e+104\n",
      "Gradient Descent(45/49): loss=4.2468441186164944e+106\n",
      "Gradient Descent(46/49): loss=1.079208553289127e+109\n",
      "Gradient Descent(47/49): loss=2.7424861119504366e+111\n",
      "Gradient Descent(48/49): loss=6.96920910357725e+113\n",
      "Gradient Descent(49/49): loss=1.7710162803647317e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.712071168550394\n",
      "Gradient Descent(2/49): loss=1674.9761310903766\n",
      "Gradient Descent(3/49): loss=412585.84601653204\n",
      "Gradient Descent(4/49): loss=103209971.6627849\n",
      "Gradient Descent(5/49): loss=25930402585.727768\n",
      "Gradient Descent(6/49): loss=6523490610768.974\n",
      "Gradient Descent(7/49): loss=1641874513451418.0\n",
      "Gradient Descent(8/49): loss=4.132973558789777e+17\n",
      "Gradient Descent(9/49): loss=1.0404143545101473e+20\n",
      "Gradient Descent(10/49): loss=2.6191304961759635e+22\n",
      "Gradient Descent(11/49): loss=6.593413845645696e+24\n",
      "Gradient Descent(12/49): loss=1.6598328086641412e+27\n",
      "Gradient Descent(13/49): loss=4.178482887875809e+29\n",
      "Gradient Descent(14/49): loss=1.0518964853089509e+32\n",
      "Gradient Descent(15/49): loss=2.6480575020996424e+34\n",
      "Gradient Descent(16/49): loss=6.6662535754268e+36\n",
      "Gradient Descent(17/49): loss=1.678171152442601e+39\n",
      "Gradient Descent(18/49): loss=4.224649412041456e+41\n",
      "Gradient Descent(19/49): loss=1.063518619399537e+44\n",
      "Gradient Descent(20/49): loss=2.677315307986022e+46\n",
      "Gradient Descent(21/49): loss=6.739907631522021e+48\n",
      "Gradient Descent(22/49): loss=1.6967129253555952e+51\n",
      "Gradient Descent(23/49): loss=4.271326713203878e+53\n",
      "Gradient Descent(24/49): loss=1.0752692231175474e+56\n",
      "Gradient Descent(25/49): loss=2.7068964277804113e+58\n",
      "Gradient Descent(26/49): loss=6.81437551936038e+60\n",
      "Gradient Descent(27/49): loss=1.7154595662510178e+63\n",
      "Gradient Descent(28/49): loss=4.318519745619336e+65\n",
      "Gradient Descent(29/49): loss=1.0871496571651363e+68\n",
      "Gradient Descent(30/49): loss=2.736804383662204e+70\n",
      "Gradient Descent(31/49): loss=6.889666188152923e+72\n",
      "Gradient Descent(32/49): loss=1.7344133350393058e+75\n",
      "Gradient Descent(33/49): loss=4.3662342044014976e+77\n",
      "Gradient Descent(34/49): loss=1.0991613557476175e+80\n",
      "Gradient Descent(35/49): loss=2.7670427865528293e+82\n",
      "Gradient Descent(36/49): loss=6.965788728449544e+84\n",
      "Gradient Descent(37/49): loss=1.7535765202186546e+87\n",
      "Gradient Descent(38/49): loss=4.4144758506717403e+89\n",
      "Gradient Descent(39/49): loss=1.111305769179323e+92\n",
      "Gradient Descent(40/49): loss=2.7976152874940193e+94\n",
      "Gradient Descent(41/49): loss=7.042752331430917e+96\n",
      "Gradient Descent(42/49): loss=1.7729514355887522e+99\n",
      "Gradient Descent(43/49): loss=4.4632505092189435e+101\n",
      "Gradient Descent(44/49): loss=1.1235843638000256e+104\n",
      "Gradient Descent(45/49): loss=2.828525577868193e+106\n",
      "Gradient Descent(46/49): loss=7.120566289830066e+108\n",
      "Gradient Descent(47/49): loss=1.792540420513976e+111\n",
      "Gradient Descent(48/49): loss=4.512564069188836e+113\n",
      "Gradient Descent(49/49): loss=1.135998622150757e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.679838037033121\n",
      "Gradient Descent(2/49): loss=1667.5150926825936\n",
      "Gradient Descent(3/49): loss=409991.1746702178\n",
      "Gradient Descent(4/49): loss=102358357.5757258\n",
      "Gradient Descent(5/49): loss=25667963104.96768\n",
      "Gradient Descent(6/49): loss=6445726066841.428\n",
      "Gradient Descent(7/49): loss=1619407357341423.0\n",
      "Gradient Descent(8/49): loss=4.069205796650917e+17\n",
      "Gradient Descent(9/49): loss=1.022555726122572e+20\n",
      "Gradient Descent(10/49): loss=2.5696413805729347e+22\n",
      "Gradient Descent(11/49): loss=6.457447497922145e+24\n",
      "Gradient Descent(12/49): loss=1.6227448164353447e+27\n",
      "Gradient Descent(13/49): loss=4.07793140648417e+29\n",
      "Gradient Descent(14/49): loss=1.0247778259354033e+32\n",
      "Gradient Descent(15/49): loss=2.57525099389698e+34\n",
      "Gradient Descent(16/49): loss=6.471566546843958e+36\n",
      "Gradient Descent(17/49): loss=1.6262948403387475e+39\n",
      "Gradient Descent(18/49): loss=4.0868542397355495e+41\n",
      "Gradient Descent(19/49): loss=1.0270202662505027e+44\n",
      "Gradient Descent(20/49): loss=2.580886339238705e+46\n",
      "Gradient Descent(21/49): loss=6.485728194496417e+48\n",
      "Gradient Descent(22/49): loss=1.6298536504957094e+51\n",
      "Gradient Descent(23/49): loss=4.095797483988824e+53\n",
      "Gradient Descent(24/49): loss=1.0292676906824585e+56\n",
      "Gradient Descent(25/49): loss=2.5865340833539626e+58\n",
      "Gradient Descent(26/49): loss=6.499920890279896e+60\n",
      "Gradient Descent(27/49): loss=1.6334202534506828e+63\n",
      "Gradient Descent(28/49): loss=4.1047603031184777e+65\n",
      "Gradient Descent(29/49): loss=1.0315200335286299e+68\n",
      "Gradient Descent(30/49): loss=2.5921941867410642e+70\n",
      "Gradient Descent(31/49): loss=6.514144644179284e+72\n",
      "Gradient Descent(32/49): loss=1.6369946612155082e+75\n",
      "Gradient Descent(33/49): loss=4.113742735575516e+77\n",
      "Gradient Descent(34/49): loss=1.0337773051706026e+80\n",
      "Gradient Descent(35/49): loss=2.5978666761140557e+82\n",
      "Gradient Descent(36/49): loss=6.528399523870512e+84\n",
      "Gradient Descent(37/49): loss=1.6405768908443231e+87\n",
      "Gradient Descent(38/49): loss=4.12274482425781e+89\n",
      "Gradient Descent(39/49): loss=1.0360395163921315e+92\n",
      "Gradient Descent(40/49): loss=2.6035515785754967e+94\n",
      "Gradient Descent(41/49): loss=6.542685597464612e+96\n",
      "Gradient Descent(42/49): loss=1.6441669594535856e+99\n",
      "Gradient Descent(43/49): loss=4.131766612178978e+101\n",
      "Gradient Descent(44/49): loss=1.0383066780024874e+104\n",
      "Gradient Descent(45/49): loss=2.6092489212889083e+106\n",
      "Gradient Descent(46/49): loss=6.557002933223009e+108\n",
      "Gradient Descent(47/49): loss=1.6477648841972943e+111\n",
      "Gradient Descent(48/49): loss=4.1408081424467784e+113\n",
      "Gradient Descent(49/49): loss=1.0405788008345721e+116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.781017176717541\n",
      "Gradient Descent(2/49): loss=1723.0651570958823\n",
      "Gradient Descent(3/49): loss=432047.2605927651\n",
      "Gradient Descent(4/49): loss=109933661.84873958\n",
      "Gradient Descent(5/49): loss=28085370509.584938\n",
      "Gradient Descent(6/49): loss=7183928646375.434\n",
      "Gradient Descent(7/49): loss=1838285584356931.8\n",
      "Gradient Descent(8/49): loss=4.704557959599287e+17\n",
      "Gradient Descent(9/49): loss=1.2040447936894001e+20\n",
      "Gradient Descent(10/49): loss=3.081572446696863e+22\n",
      "Gradient Descent(11/49): loss=7.886858914239082e+24\n",
      "Gradient Descent(12/49): loss=2.0185355647715114e+27\n",
      "Gradient Descent(13/49): loss=5.1661730811526036e+29\n",
      "Gradient Descent(14/49): loss=1.32221344459916e+32\n",
      "Gradient Descent(15/49): loss=3.3840300334805725e+34\n",
      "Gradient Descent(16/49): loss=8.660976422795387e+36\n",
      "Gradient Descent(17/49): loss=2.2166621540378557e+39\n",
      "Gradient Descent(18/49): loss=5.673253077645185e+41\n",
      "Gradient Descent(19/49): loss=1.4519939560861374e+44\n",
      "Gradient Descent(20/49): loss=3.716186145936265e+46\n",
      "Gradient Descent(21/49): loss=9.511086058120977e+48\n",
      "Gradient Descent(22/49): loss=2.434236457879742e+51\n",
      "Gradient Descent(23/49): loss=6.2301056858341675e+53\n",
      "Gradient Descent(24/49): loss=1.5945130034971258e+56\n",
      "Gradient Descent(25/49): loss=4.0809447648769423e+58\n",
      "Gradient Descent(26/49): loss=1.0444637414342172e+61\n",
      "Gradient Descent(27/49): loss=2.673166558292669e+63\n",
      "Gradient Descent(28/49): loss=6.841615620435155e+65\n",
      "Gradient Descent(29/49): loss=1.7510208689606867e+68\n",
      "Gradient Descent(30/49): loss=4.481505909770543e+70\n",
      "Gradient Descent(31/49): loss=1.146982059170377e+73\n",
      "Gradient Descent(32/49): loss=2.9355486092086355e+75\n",
      "Gradient Descent(33/49): loss=7.513147715021635e+77\n",
      "Gradient Descent(34/49): loss=1.922890610997311e+80\n",
      "Gradient Descent(35/49): loss=4.9213837423545886e+82\n",
      "Gradient Descent(36/49): loss=1.259562962187975e+85\n",
      "Gradient Descent(37/49): loss=3.2236845138938454e+87\n",
      "Gradient Descent(38/49): loss=8.250593386032025e+89\n",
      "Gradient Descent(39/49): loss=2.111630059587065e+92\n",
      "Gradient Descent(40/49): loss=5.404437353682396e+94\n",
      "Gradient Descent(41/49): loss=1.3831941337105834e+97\n",
      "Gradient Descent(42/49): loss=3.5401021166944617e+99\n",
      "Gradient Descent(43/49): loss=9.06042231613952e+101\n",
      "Gradient Descent(44/49): loss=2.318895044289034e+104\n",
      "Gradient Descent(45/49): loss=5.934904620118619e+106\n",
      "Gradient Descent(46/49): loss=1.5189602020433244e+109\n",
      "Gradient Descent(47/49): loss=3.8875773800478985e+111\n",
      "Gradient Descent(48/49): loss=9.949739213397189e+113\n",
      "Gradient Descent(49/49): loss=2.546503920994465e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.746158208229192\n",
      "Gradient Descent(2/49): loss=1711.3229563422735\n",
      "Gradient Descent(3/49): loss=429029.61372213427\n",
      "Gradient Descent(4/49): loss=109287994.45566672\n",
      "Gradient Descent(5/49): loss=27966089765.65298\n",
      "Gradient Descent(6/49): loss=7166503893268.845\n",
      "Gradient Descent(7/49): loss=1837312029286536.8\n",
      "Gradient Descent(8/49): loss=4.7111250021729517e+17\n",
      "Gradient Descent(9/49): loss=1.2080597094975578e+20\n",
      "Gradient Descent(10/49): loss=3.0978441383742095e+22\n",
      "Gradient Descent(11/49): loss=7.943889721411069e+24\n",
      "Gradient Descent(12/49): loss=2.0370781136382223e+27\n",
      "Gradient Descent(13/49): loss=5.223750656796758e+29\n",
      "Gradient Descent(14/49): loss=1.3395449418473227e+32\n",
      "Gradient Descent(15/49): loss=3.435042940756156e+34\n",
      "Gradient Descent(16/49): loss=8.808603522541874e+36\n",
      "Gradient Descent(17/49): loss=2.2588217379854227e+39\n",
      "Gradient Descent(18/49): loss=5.792377469400944e+41\n",
      "Gradient Descent(19/49): loss=1.4853600979192518e+44\n",
      "Gradient Descent(20/49): loss=3.8089620934303215e+46\n",
      "Gradient Descent(21/49): loss=9.767457905347918e+48\n",
      "Gradient Descent(22/49): loss=2.5047042106471715e+51\n",
      "Gradient Descent(23/49): loss=6.422902707850958e+53\n",
      "Gradient Descent(24/49): loss=1.6470479435982568e+56\n",
      "Gradient Descent(25/49): loss=4.2235840272015663e+58\n",
      "Gradient Descent(26/49): loss=1.0830687779414914e+61\n",
      "Gradient Descent(27/49): loss=2.777352055024881e+63\n",
      "Gradient Descent(28/49): loss=7.122063339515849e+65\n",
      "Gradient Descent(29/49): loss=1.8263362082709294e+68\n",
      "Gradient Descent(30/49): loss=4.683339345123299e+70\n",
      "Gradient Descent(31/49): loss=1.2009654806299622e+73\n",
      "Gradient Descent(32/49): loss=3.0796787919427998e+75\n",
      "Gradient Descent(33/49): loss=7.897330618168366e+77\n",
      "Gradient Descent(34/49): loss=2.0251407729867508e+80\n",
      "Gradient Descent(35/49): loss=5.19314101017665e+82\n",
      "Gradient Descent(36/49): loss=1.331695747343239e+85\n",
      "Gradient Descent(37/49): loss=3.414915096695482e+87\n",
      "Gradient Descent(38/49): loss=8.756989080203913e+89\n",
      "Gradient Descent(39/49): loss=2.2455860710861038e+92\n",
      "Gradient Descent(40/49): loss=5.758436782861131e+94\n",
      "Gradient Descent(41/49): loss=1.4766565668164292e+97\n",
      "Gradient Descent(42/49): loss=3.786643317526663e+99\n",
      "Gradient Descent(43/49): loss=9.710225069517976e+101\n",
      "Gradient Descent(44/49): loss=2.490027789633016e+104\n",
      "Gradient Descent(45/49): loss=6.385267435878768e+106\n",
      "Gradient Descent(46/49): loss=1.637396996027223e+109\n",
      "Gradient Descent(47/49): loss=4.198835756720326e+111\n",
      "Gradient Descent(48/49): loss=1.0767224903116945e+114\n",
      "Gradient Descent(49/49): loss=2.7610780423775404e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.779896208564176\n",
      "Gradient Descent(2/49): loss=1705.6663596805463\n",
      "Gradient Descent(3/49): loss=423965.32893451466\n",
      "Gradient Descent(4/49): loss=107021613.68075576\n",
      "Gradient Descent(5/49): loss=27132809346.522587\n",
      "Gradient Descent(6/49): loss=6888140030018.475\n",
      "Gradient Descent(7/49): loss=1749438241146803.8\n",
      "Gradient Descent(8/49): loss=4.4438371249223866e+17\n",
      "Gradient Descent(9/49): loss=1.1288565114247705e+20\n",
      "Gradient Descent(10/49): loss=2.8676521148507243e+22\n",
      "Gradient Descent(11/49): loss=7.284782509915188e+24\n",
      "Gradient Descent(12/49): loss=1.8505785736915034e+27\n",
      "Gradient Descent(13/49): loss=4.70109193228209e+29\n",
      "Gradient Descent(14/49): loss=1.194235706280524e+32\n",
      "Gradient Descent(15/49): loss=3.0337611907344164e+34\n",
      "Gradient Descent(16/49): loss=7.706776091647307e+36\n",
      "Gradient Descent(17/49): loss=1.9577809355356354e+39\n",
      "Gradient Descent(18/49): loss=4.9734235908586984e+41\n",
      "Gradient Descent(19/49): loss=1.263417259244473e+44\n",
      "Gradient Descent(20/49): loss=3.2095057706475416e+46\n",
      "Gradient Descent(21/49): loss=8.153226669635001e+48\n",
      "Gradient Descent(22/49): loss=2.0711944417380948e+51\n",
      "Gradient Descent(23/49): loss=5.261532138579394e+53\n",
      "Gradient Descent(24/49): loss=1.3366065439124176e+56\n",
      "Gradient Descent(25/49): loss=3.3954312283540017e+58\n",
      "Gradient Descent(26/49): loss=8.625540013249128e+60\n",
      "Gradient Descent(27/49): loss=2.191177954036816e+63\n",
      "Gradient Descent(28/49): loss=5.56633070959305e+65\n",
      "Gradient Descent(29/49): loss=1.4140356565507303e+68\n",
      "Gradient Descent(30/49): loss=3.592127277940775e+70\n",
      "Gradient Descent(31/49): loss=9.125214290848632e+72\n",
      "Gradient Descent(32/49): loss=2.3181120659411382e+75\n",
      "Gradient Descent(33/49): loss=5.888786146809668e+77\n",
      "Gradient Descent(34/49): loss=1.4959502084632212e+80\n",
      "Gradient Descent(35/49): loss=3.800217855446401e+82\n",
      "Gradient Descent(36/49): loss=9.65383451076852e+84\n",
      "Gradient Descent(37/49): loss=2.4523994230419634e+87\n",
      "Gradient Descent(38/49): loss=6.229921305806359e+89\n",
      "Gradient Descent(39/49): loss=1.5826100394526153e+92\n",
      "Gradient Descent(40/49): loss=4.020363041571389e+94\n",
      "Gradient Descent(41/49): loss=1.021307750052162e+97\n",
      "Gradient Descent(42/49): loss=2.594465996058215e+99\n",
      "Gradient Descent(43/49): loss=6.590818295815844e+101\n",
      "Gradient Descent(44/49): loss=1.674290041744928e+104\n",
      "Gradient Descent(45/49): loss=4.253261155243596e+106\n",
      "Gradient Descent(46/49): loss=1.0804717225607218e+109\n",
      "Gradient Descent(47/49): loss=2.7447624320319206e+111\n",
      "Gradient Descent(48/49): loss=6.972621912250374e+113\n",
      "Gradient Descent(49/49): loss=1.7712810319690612e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.747037538878935\n",
      "Gradient Descent(2/49): loss=1697.9385135180396\n",
      "Gradient Descent(3/49): loss=421251.0983864718\n",
      "Gradient Descent(4/49): loss=106122430.05269317\n",
      "Gradient Descent(5/49): loss=26853099023.65323\n",
      "Gradient Descent(6/49): loss=6804476786344.095\n",
      "Gradient Descent(7/49): loss=1725040125685015.2\n",
      "Gradient Descent(8/49): loss=4.3739426102038483e+17\n",
      "Gradient Descent(9/49): loss=1.1091002338277394e+20\n",
      "Gradient Descent(10/49): loss=2.812397715491743e+22\n",
      "Gradient Descent(11/49): loss=7.131576064256438e+24\n",
      "Gradient Descent(12/49): loss=1.808403273013029e+27\n",
      "Gradient Descent(13/49): loss=4.585697371603659e+29\n",
      "Gradient Descent(14/49): loss=1.1628283582716316e+32\n",
      "Gradient Descent(15/49): loss=2.948668017690378e+34\n",
      "Gradient Descent(16/49): loss=7.477151119982872e+36\n",
      "Gradient Descent(17/49): loss=1.896035401929958e+39\n",
      "Gradient Descent(18/49): loss=4.807914407112446e+41\n",
      "Gradient Descent(19/49): loss=1.219177709119019e+44\n",
      "Gradient Descent(20/49): loss=3.0915572975027167e+46\n",
      "Gradient Descent(21/49): loss=7.83948595336124e+48\n",
      "Gradient Descent(22/49): loss=1.9879152835380806e+51\n",
      "Gradient Descent(23/49): loss=5.040900893389105e+53\n",
      "Gradient Descent(24/49): loss=1.2782577822891898e+56\n",
      "Gradient Descent(25/49): loss=3.241370922664868e+58\n",
      "Gradient Descent(26/49): loss=8.219379223719694e+60\n",
      "Gradient Descent(27/49): loss=2.0842475741033496e+63\n",
      "Gradient Descent(28/49): loss=5.285177665996743e+65\n",
      "Gradient Descent(29/49): loss=1.340200814347532e+68\n",
      "Gradient Descent(30/49): loss=3.398444359465195e+70\n",
      "Gradient Descent(31/49): loss=8.617681724065846e+72\n",
      "Gradient Descent(32/49): loss=2.185248026511323e+75\n",
      "Gradient Descent(33/49): loss=5.541291834944477e+77\n",
      "Gradient Descent(34/49): loss=1.4051455408035933e+80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=3.563129410346252e+82\n",
      "Gradient Descent(36/49): loss=9.035285546018031e+84\n",
      "Gradient Descent(37/49): loss=2.291142854958766e+87\n",
      "Gradient Descent(38/49): loss=5.809817028020825e+89\n",
      "Gradient Descent(39/49): loss=1.4732374206185492e+92\n",
      "Gradient Descent(40/49): loss=3.735794926144461e+94\n",
      "Gradient Descent(41/49): loss=9.473126011384594e+96\n",
      "Gradient Descent(42/49): loss=2.402169235777309e+99\n",
      "Gradient Descent(43/49): loss=6.091354670443725e+101\n",
      "Gradient Descent(44/49): loss=1.5446289615448337e+104\n",
      "Gradient Descent(45/49): loss=3.916827631823414e+106\n",
      "Gradient Descent(46/49): loss=9.932183766690431e+108\n",
      "Gradient Descent(47/49): loss=2.518575838615219e+111\n",
      "Gradient Descent(48/49): loss=6.386535331866913e+113\n",
      "Gradient Descent(49/49): loss=1.619480061700691e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.849152131081465\n",
      "Gradient Descent(2/49): loss=1754.4834233180136\n",
      "Gradient Descent(3/49): loss=443905.9843254191\n",
      "Gradient Descent(4/49): loss=113974011.62094648\n",
      "Gradient Descent(5/49): loss=29381397343.194016\n",
      "Gradient Descent(6/49): loss=7583542915763.707\n",
      "Gradient Descent(7/49): loss=1958128774485696.2\n",
      "Gradient Descent(8/49): loss=5.05667836211367e+17\n",
      "Gradient Descent(9/49): loss=1.3058925174376006e+20\n",
      "Gradient Descent(10/49): loss=3.372527270373645e+22\n",
      "Gradient Descent(11/49): loss=8.709745485592844e+24\n",
      "Gradient Descent(12/49): loss=2.2493451721569682e+27\n",
      "Gradient Descent(13/49): loss=5.8090746733943916e+29\n",
      "Gradient Descent(14/49): loss=1.500230131893356e+32\n",
      "Gradient Descent(15/49): loss=3.8744390389026233e+34\n",
      "Gradient Descent(16/49): loss=1.0005983624308007e+37\n",
      "Gradient Descent(17/49): loss=2.5841085089069323e+39\n",
      "Gradient Descent(18/49): loss=6.673623553125654e+41\n",
      "Gradient Descent(19/49): loss=1.7235054644115212e+44\n",
      "Gradient Descent(20/49): loss=4.4510617999920876e+46\n",
      "Gradient Descent(21/49): loss=1.1495148438939151e+49\n",
      "Gradient Descent(22/49): loss=2.968694742376544e+51\n",
      "Gradient Descent(23/49): loss=7.666841816176392e+53\n",
      "Gradient Descent(24/49): loss=1.980010359278746e+56\n",
      "Gradient Descent(25/49): loss=5.113501904503418e+58\n",
      "Gradient Descent(26/49): loss=1.3205941880471868e+61\n",
      "Gradient Descent(27/49): loss=3.410517962197801e+63\n",
      "Gradient Descent(28/49): loss=8.807878207971179e+65\n",
      "Gradient Descent(29/49): loss=2.274690219677394e+68\n",
      "Gradient Descent(30/49): loss=5.874531269986593e+70\n",
      "Gradient Descent(31/49): loss=1.5171348319660362e+73\n",
      "Gradient Descent(32/49): loss=3.918096597977338e+75\n",
      "Gradient Descent(33/49): loss=1.0118732117690454e+78\n",
      "Gradient Descent(34/49): loss=2.6132265274530827e+80\n",
      "Gradient Descent(35/49): loss=6.748822682878951e+82\n",
      "Gradient Descent(36/49): loss=1.7429261155301423e+85\n",
      "Gradient Descent(37/49): loss=4.501216859502853e+87\n",
      "Gradient Descent(38/49): loss=1.162467705070218e+90\n",
      "Gradient Descent(39/49): loss=3.0021463251172144e+92\n",
      "Gradient Descent(40/49): loss=7.75323264302677e+94\n",
      "Gradient Descent(41/49): loss=2.002321336370873e+97\n",
      "Gradient Descent(42/49): loss=5.171121413068864e+99\n",
      "Gradient Descent(43/49): loss=1.3354747903333767e+102\n",
      "Gradient Descent(44/49): loss=3.448948058168949e+104\n",
      "Gradient Descent(45/49): loss=8.907126359890235e+106\n",
      "Gradient Descent(46/49): loss=2.3003216822340915e+109\n",
      "Gradient Descent(47/49): loss=5.940726142142097e+111\n",
      "Gradient Descent(48/49): loss=1.5342300761019817e+114\n",
      "Gradient Descent(49/49): loss=3.9622461465074925e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.813940061578099\n",
      "Gradient Descent(2/49): loss=1742.5287433275957\n",
      "Gradient Descent(3/49): loss=440805.40487477643\n",
      "Gradient Descent(4/49): loss=113304456.13488287\n",
      "Gradient Descent(5/49): loss=29256546062.135853\n",
      "Gradient Descent(6/49): loss=7565126598931.462\n",
      "Gradient Descent(7/49): loss=1957084829131058.2\n",
      "Gradient Descent(8/49): loss=5.0637162509019245e+17\n",
      "Gradient Descent(9/49): loss=1.3102410065557814e+20\n",
      "Gradient Descent(10/49): loss=3.3903179078479653e+22\n",
      "Gradient Descent(11/49): loss=8.772677164478048e+24\n",
      "Gradient Descent(12/49): loss=2.2699939898679188e+27\n",
      "Gradient Descent(13/49): loss=5.8737782668847596e+29\n",
      "Gradient Descent(14/49): loss=1.5198841399725375e+32\n",
      "Gradient Descent(15/49): loss=3.9328143876660664e+34\n",
      "Gradient Descent(16/49): loss=1.017645291581218e+37\n",
      "Gradient Descent(17/49): loss=2.6332337251432872e+39\n",
      "Gradient Descent(18/49): loss=6.813690318084113e+41\n",
      "Gradient Descent(19/49): loss=1.763093619484088e+44\n",
      "Gradient Descent(20/49): loss=4.562137354248586e+46\n",
      "Gradient Descent(21/49): loss=1.180487355363305e+49\n",
      "Gradient Descent(22/49): loss=3.0545998246289716e+51\n",
      "Gradient Descent(23/49): loss=7.904006803921312e+53\n",
      "Gradient Descent(24/49): loss=2.045221212054301e+56\n",
      "Gradient Descent(25/49): loss=5.292163721528265e+58\n",
      "Gradient Descent(26/49): loss=1.3693871689967984e+61\n",
      "Gradient Descent(27/49): loss=3.5433923009317576e+63\n",
      "Gradient Descent(28/49): loss=9.168794101891009e+65\n",
      "Gradient Descent(29/49): loss=2.3724944387548486e+68\n",
      "Gradient Descent(30/49): loss=6.139007812119797e+70\n",
      "Gradient Descent(31/49): loss=1.5885144471422808e+73\n",
      "Gradient Descent(32/49): loss=4.110400615223216e+75\n",
      "Gradient Descent(33/49): loss=1.0635970763767504e+78\n",
      "Gradient Descent(34/49): loss=2.752137435673605e+80\n",
      "Gradient Descent(35/49): loss=7.121362622242768e+82\n",
      "Gradient Descent(36/49): loss=1.8427061432367666e+85\n",
      "Gradient Descent(37/49): loss=4.768140748396742e+87\n",
      "Gradient Descent(38/49): loss=1.2337922831573448e+90\n",
      "Gradient Descent(39/49): loss=3.1925303347860523e+92\n",
      "Gradient Descent(40/49): loss=8.260912373715502e+94\n",
      "Gradient Descent(41/49): loss=2.1375732127782256e+97\n",
      "Gradient Descent(42/49): loss=5.531131469842653e+99\n",
      "Gradient Descent(43/49): loss=1.431221871316449e+102\n",
      "Gradient Descent(44/49): loss=3.7033942442030317e+104\n",
      "Gradient Descent(45/49): loss=9.58281116496691e+106\n",
      "Gradient Descent(46/49): loss=2.4796244679366214e+109\n",
      "Gradient Descent(47/49): loss=6.416214820623783e+111\n",
      "Gradient Descent(48/49): loss=1.6602438456597968e+114\n",
      "Gradient Descent(49/49): loss=4.296005829155295e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.848036178135859\n",
      "Gradient Descent(2/49): loss=1736.776576709029\n",
      "Gradient Descent(3/49): loss=435605.17159002344\n",
      "Gradient Descent(4/49): loss=110955864.05605812\n",
      "Gradient Descent(5/49): loss=28385166191.416363\n",
      "Gradient Descent(6/49): loss=7271387803986.344\n",
      "Gradient Descent(7/49): loss=1863515123287659.5\n",
      "Gradient Descent(8/49): loss=4.776519195202797e+17\n",
      "Gradient Descent(9/49): loss=1.224366037157822e+20\n",
      "Gradient Descent(10/49): loss=3.138471233663674e+22\n",
      "Gradient Descent(11/49): loss=8.045025813022704e+24\n",
      "Gradient Descent(12/49): loss=2.062232123403123e+27\n",
      "Gradient Descent(13/49): loss=5.2862528177605655e+29\n",
      "Gradient Descent(14/49): loss=1.3550596249519933e+32\n",
      "Gradient Descent(15/49): loss=3.4735128730256843e+34\n",
      "Gradient Descent(16/49): loss=8.903882718786748e+36\n",
      "Gradient Descent(17/49): loss=2.2823905087540554e+39\n",
      "Gradient Descent(18/49): loss=5.8506009274454855e+41\n",
      "Gradient Descent(19/49): loss=1.4997228175398196e+44\n",
      "Gradient Descent(20/49): loss=3.844337629715029e+46\n",
      "Gradient Descent(21/49): loss=9.854442194270308e+48\n",
      "Gradient Descent(22/49): loss=2.5260536487021445e+51\n",
      "Gradient Descent(23/49): loss=6.4751986062817044e+53\n",
      "Gradient Descent(24/49): loss=1.6598300282543393e+56\n",
      "Gradient Descent(25/49): loss=4.2547509199580365e+58\n",
      "Gradient Descent(26/49): loss=1.0906481436494884e+61\n",
      "Gradient Descent(27/49): loss=2.7957297515738032e+63\n",
      "Gradient Descent(28/49): loss=7.166477006673675e+65\n",
      "Gradient Descent(29/49): loss=1.837029943908991e+68\n",
      "Gradient Descent(30/49): loss=4.708979058574654e+70\n",
      "Gradient Descent(31/49): loss=1.2070834145963786e+73\n",
      "Gradient Descent(32/49): loss=3.094195900362919e+75\n",
      "Gradient Descent(33/49): loss=7.931554815558543e+77\n",
      "Gradient Descent(34/49): loss=2.033147344834589e+80\n",
      "Gradient Descent(35/49): loss=5.2116996250209344e+82\n",
      "Gradient Descent(36/49): loss=1.3359490668716494e+85\n",
      "Gradient Descent(37/49): loss=3.4245256589745724e+87\n",
      "Gradient Descent(38/49): loss=8.778310700449704e+89\n",
      "Gradient Descent(39/49): loss=2.2502018214313506e+92\n",
      "Gradient Descent(40/49): loss=5.768089567521878e+94\n",
      "Gradient Descent(41/49): loss=1.4785721414886786e+97\n",
      "Gradient Descent(42/49): loss=3.790120718471525e+99\n",
      "Gradient Descent(43/49): loss=9.71546443863325e+101\n",
      "Gradient Descent(44/49): loss=2.4904285712676072e+104\n",
      "Gradient Descent(45/49): loss=6.383878514261239e+106\n",
      "Gradient Descent(46/49): loss=1.636421351530789e+109\n",
      "Gradient Descent(47/49): loss=4.194745927203401e+111\n",
      "Gradient Descent(48/49): loss=1.075266671223121e+114\n",
      "Gradient Descent(49/49): loss=2.756301416839502e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.814547608517639\n",
      "Gradient Descent(2/49): loss=1728.7765059253593\n",
      "Gradient Descent(3/49): loss=432767.5577384611\n",
      "Gradient Descent(4/49): loss=110007066.80827948\n",
      "Gradient Descent(5/49): loss=28087257519.0525\n",
      "Gradient Descent(6/49): loss=7181446560485.869\n",
      "Gradient Descent(7/49): loss=1837041575451372.2\n",
      "Gradient Descent(8/49): loss=4.6999753659986976e+17\n",
      "Gradient Descent(9/49): loss=1.2025304124458877e+20\n",
      "Gradient Descent(10/49): loss=3.07683920086705e+22\n",
      "Gradient Descent(11/49): loss=7.87256731021852e+24\n",
      "Gradient Descent(12/49): loss=2.0143222582843137e+27\n",
      "Gradient Descent(13/49): loss=5.153969718630793e+29\n",
      "Gradient Descent(14/49): loss=1.3187269807083116e+32\n",
      "Gradient Descent(15/49): loss=3.3741777866484926e+34\n",
      "Gradient Descent(16/49): loss=8.633383766781669e+36\n",
      "Gradient Descent(17/49): loss=2.208991962526783e+39\n",
      "Gradient Descent(18/49): loss=5.652066028177446e+41\n",
      "Gradient Descent(19/49): loss=1.4461732306952767e+44\n",
      "Gradient Descent(20/49): loss=3.7002699609970103e+46\n",
      "Gradient Descent(21/49): loss=9.467743902233345e+48\n",
      "Gradient Descent(22/49): loss=2.4224766178731474e+51\n",
      "Gradient Descent(23/49): loss=6.198301332245128e+53\n",
      "Gradient Descent(24/49): loss=1.5859364388541657e+56\n",
      "Gradient Descent(25/49): loss=4.0578769138032073e+58\n",
      "Gradient Descent(26/49): loss=1.0382739587903909e+61\n",
      "Gradient Descent(27/49): loss=2.6565931801322536e+63\n",
      "Gradient Descent(28/49): loss=6.797326721887554e+65\n",
      "Gradient Descent(29/49): loss=1.739206849947116e+68\n",
      "Gradient Descent(30/49): loss=4.450044246310749e+70\n",
      "Gradient Descent(31/49): loss=1.1386163638169759e+73\n",
      "Gradient Descent(32/49): loss=2.913335580936281e+75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=7.454244007785627e+77\n",
      "Gradient Descent(34/49): loss=1.907289846429238e+80\n",
      "Gradient Descent(35/49): loss=4.880111993238479e+82\n",
      "Gradient Descent(36/49): loss=1.2486562077146566e+85\n",
      "Gradient Descent(37/49): loss=3.1948904599414454e+87\n",
      "Gradient Descent(38/49): loss=8.174648064022938e+89\n",
      "Gradient Descent(39/49): loss=2.0916169680464968e+92\n",
      "Gradient Descent(40/49): loss=5.3517429823970096e+94\n",
      "Gradient Descent(41/49): loss=1.3693306846896405e+97\n",
      "Gradient Descent(42/49): loss=3.5036557813035354e+99\n",
      "Gradient Descent(43/49): loss=8.964674472802026e+101\n",
      "Gradient Descent(44/49): loss=2.2937581035260422e+104\n",
      "Gradient Descent(45/49): loss=5.868954030013878e+106\n",
      "Gradient Descent(46/49): loss=1.5016675626547824e+109\n",
      "Gradient Descent(47/49): loss=3.842261256771547e+111\n",
      "Gradient Descent(48/49): loss=9.831051780320977e+113\n",
      "Gradient Descent(49/49): loss=2.5154348610995343e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.917601932077156\n",
      "Gradient Descent(2/49): loss=1786.3295615076788\n",
      "Gradient Descent(3/49): loss=456034.72850960214\n",
      "Gradient Descent(4/49): loss=118143701.76433171\n",
      "Gradient Descent(5/49): loss=30731003847.179398\n",
      "Gradient Descent(6/49): loss=8003440229540.624\n",
      "Gradient Descent(7/49): loss=2085193393503148.0\n",
      "Gradient Descent(8/49): loss=5.433392320972132e+17\n",
      "Gradient Descent(9/49): loss=1.4158389419885883e+20\n",
      "Gradient Descent(10/49): loss=3.6894581783444398e+22\n",
      "Gradient Descent(11/49): loss=9.614203138001191e+24\n",
      "Gradient Descent(12/49): loss=2.5053283049550615e+27\n",
      "Gradient Descent(13/49): loss=6.528542120107178e+29\n",
      "Gradient Descent(14/49): loss=1.701248857616305e+32\n",
      "Gradient Descent(15/49): loss=4.433222269802348e+34\n",
      "Gradient Descent(16/49): loss=1.1552372221049156e+37\n",
      "Gradient Descent(17/49): loss=3.0103905601386227e+39\n",
      "Gradient Descent(18/49): loss=7.844667024935691e+41\n",
      "Gradient Descent(19/49): loss=2.0442131857105516e+44\n",
      "Gradient Descent(20/49): loss=5.326940628887454e+46\n",
      "Gradient Descent(21/49): loss=1.3881280418326535e+49\n",
      "Gradient Descent(22/49): loss=3.6172722671576136e+51\n",
      "Gradient Descent(23/49): loss=9.426117952039484e+53\n",
      "Gradient Descent(24/49): loss=2.456317719091811e+56\n",
      "Gradient Descent(25/49): loss=6.400828811848303e+58\n",
      "Gradient Descent(26/49): loss=1.6679686491756872e+61\n",
      "Gradient Descent(27/49): loss=4.346498705735365e+63\n",
      "Gradient Descent(28/49): loss=1.1326382548194853e+66\n",
      "Gradient Descent(29/49): loss=2.951500743777422e+68\n",
      "Gradient Descent(30/49): loss=7.691208206548755e+70\n",
      "Gradient Descent(31/49): loss=2.004223912231664e+73\n",
      "Gradient Descent(32/49): loss=5.2227340392904496e+75\n",
      "Gradient Descent(33/49): loss=1.360973226528907e+78\n",
      "Gradient Descent(34/49): loss=3.546510523787177e+80\n",
      "Gradient Descent(35/49): loss=9.241722504278808e+82\n",
      "Gradient Descent(36/49): loss=2.408266781480983e+85\n",
      "Gradient Descent(37/49): loss=6.2756146249787585e+87\n",
      "Gradient Descent(38/49): loss=1.635339540623001e+90\n",
      "Gradient Descent(39/49): loss=4.261471701083159e+92\n",
      "Gradient Descent(40/49): loss=1.1104813775990665e+95\n",
      "Gradient Descent(41/49): loss=2.8937629450428493e+97\n",
      "Gradient Descent(42/49): loss=7.540751381358518e+99\n",
      "Gradient Descent(43/49): loss=1.9650169165677153e+102\n",
      "Gradient Descent(44/49): loss=5.120565958377562e+104\n",
      "Gradient Descent(45/49): loss=1.3343496187245952e+107\n",
      "Gradient Descent(46/49): loss=3.47713303463554e+109\n",
      "Gradient Descent(47/49): loss=9.06093423409534e+111\n",
      "Gradient Descent(48/49): loss=2.3611558251238044e+114\n",
      "Gradient Descent(49/49): loss=6.152849900993292e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.882035026262341\n",
      "Gradient Descent(2/49): loss=1774.1595319913363\n",
      "Gradient Descent(3/49): loss=452849.32619203394\n",
      "Gradient Descent(4/49): loss=117449488.52602226\n",
      "Gradient Descent(5/49): loss=30600349024.184647\n",
      "Gradient Descent(6/49): loss=7983980896387.428\n",
      "Gradient Descent(7/49): loss=2084074401467177.2\n",
      "Gradient Descent(8/49): loss=5.4409325048620563e+17\n",
      "Gradient Descent(9/49): loss=1.4205470513802858e+20\n",
      "Gradient Descent(10/49): loss=3.708901826043668e+22\n",
      "Gradient Descent(11/49): loss=9.683615744536606e+24\n",
      "Gradient Descent(12/49): loss=2.5283115966370095e+27\n",
      "Gradient Descent(13/49): loss=6.601215753366278e+29\n",
      "Gradient Descent(14/49): loss=1.723524070177382e+32\n",
      "Gradient Descent(15/49): loss=4.499982351119408e+34\n",
      "Gradient Descent(16/49): loss=1.1749091301280178e+37\n",
      "Gradient Descent(17/49): loss=3.067593087737148e+39\n",
      "Gradient Descent(18/49): loss=8.009238448247515e+41\n",
      "Gradient Descent(19/49): loss=2.091147643981369e+44\n",
      "Gradient Descent(20/49): loss=5.459818057067996e+46\n",
      "Gradient Descent(21/49): loss=1.4255145161676995e+49\n",
      "Gradient Descent(22/49): loss=3.721903577440313e+51\n",
      "Gradient Descent(23/49): loss=9.717590443884307e+53\n",
      "Gradient Descent(24/49): loss=2.5371845903755917e+56\n",
      "Gradient Descent(25/49): loss=6.624384597006089e+58\n",
      "Gradient Descent(26/49): loss=1.729573459319156e+61\n",
      "Gradient Descent(27/49): loss=4.515776986338754e+63\n",
      "Gradient Descent(28/49): loss=1.1790329968624216e+66\n",
      "Gradient Descent(29/49): loss=3.0783601845171443e+68\n",
      "Gradient Descent(30/49): loss=8.037350481995276e+70\n",
      "Gradient Descent(31/49): loss=2.0984874705479752e+73\n",
      "Gradient Descent(32/49): loss=5.47898175388966e+75\n",
      "Gradient Descent(33/49): loss=1.4305180031223624e+78\n",
      "Gradient Descent(34/49): loss=3.7349672789189407e+80\n",
      "Gradient Descent(35/49): loss=9.751698716232062e+82\n",
      "Gradient Descent(36/49): loss=2.5460899855510083e+85\n",
      "Gradient Descent(37/49): loss=6.6476358664903e+87\n",
      "Gradient Descent(38/49): loss=1.7356441784945212e+90\n",
      "Gradient Descent(39/49): loss=4.53162714511378e+92\n",
      "Gradient Descent(40/49): loss=1.1831713456466827e+95\n",
      "Gradient Descent(41/49): loss=3.0891650798517555e+97\n",
      "Gradient Descent(42/49): loss=8.065561193387137e+99\n",
      "Gradient Descent(43/49): loss=2.1058530600570564e+102\n",
      "Gradient Descent(44/49): loss=5.498212714804728e+104\n",
      "Gradient Descent(45/49): loss=1.4355390521132268e+107\n",
      "Gradient Descent(46/49): loss=3.7480768333920803e+109\n",
      "Gradient Descent(47/49): loss=9.785926706995916e+111\n",
      "Gradient Descent(48/49): loss=2.5550266382353643e+114\n",
      "Gradient Descent(49/49): loss=6.670968746807917e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.916491077265454\n",
      "Gradient Descent(2/49): loss=1768.310597158805\n",
      "Gradient Descent(3/49): loss=447510.11802659696\n",
      "Gradient Descent(4/49): loss=115016087.90366012\n",
      "Gradient Descent(5/49): loss=29689309142.220596\n",
      "Gradient Descent(6/49): loss=7674092317682.297\n",
      "Gradient Descent(7/49): loss=1984467526670812.2\n",
      "Gradient Descent(8/49): loss=5.132442435436079e+17\n",
      "Gradient Descent(9/49): loss=1.3274719925977224e+20\n",
      "Gradient Descent(10/49): loss=3.433474270429466e+22\n",
      "Gradient Descent(11/49): loss=8.880647789865362e+24\n",
      "Gradient Descent(12/49): loss=2.2969752405990453e+27\n",
      "Gradient Descent(13/49): loss=5.941119106634787e+29\n",
      "Gradient Descent(14/49): loss=1.5366688876595885e+32\n",
      "Gradient Descent(15/49): loss=3.974590310563115e+34\n",
      "Gradient Descent(16/49): loss=1.0280268349538136e+37\n",
      "Gradient Descent(17/49): loss=2.6589889767505395e+39\n",
      "Gradient Descent(18/49): loss=6.877468718007897e+41\n",
      "Gradient Descent(19/49): loss=1.778855663807556e+44\n",
      "Gradient Descent(20/49): loss=4.601005985548816e+46\n",
      "Gradient Descent(21/49): loss=1.1900491148391908e+49\n",
      "Gradient Descent(22/49): loss=3.078059233635098e+51\n",
      "Gradient Descent(23/49): loss=7.961392960800215e+53\n",
      "Gradient Descent(24/49): loss=2.0592124148858265e+56\n",
      "Gradient Descent(25/49): loss=5.326148062913737e+58\n",
      "Gradient Descent(26/49): loss=1.377606942490517e+61\n",
      "Gradient Descent(27/49): loss=3.563177113330541e+63\n",
      "Gradient Descent(28/49): loss=9.216149214528714e+65\n",
      "Gradient Descent(29/49): loss=2.3837548244989644e+68\n",
      "Gradient Descent(30/49): loss=6.16557624128345e+70\n",
      "Gradient Descent(31/49): loss=1.594724843192264e+73\n",
      "Gradient Descent(32/49): loss=4.12475205231618e+75\n",
      "Gradient Descent(33/49): loss=1.0668661472050143e+78\n",
      "Gradient Descent(34/49): loss=2.7594467779292017e+80\n",
      "Gradient Descent(35/49): loss=7.137302594306241e+82\n",
      "Gradient Descent(36/49): loss=1.846061635619534e+85\n",
      "Gradient Descent(37/49): loss=4.774834074185041e+87\n",
      "Gradient Descent(38/49): loss=1.2350097091068838e+90\n",
      "Gradient Descent(39/49): loss=3.1943497049132416e+92\n",
      "Gradient Descent(40/49): loss=8.262177991020307e+94\n",
      "Gradient Descent(41/49): loss=2.137010392140352e+97\n",
      "Gradient Descent(42/49): loss=5.52737234791996e+99\n",
      "Gradient Descent(43/49): loss=1.4296535564317319e+102\n",
      "Gradient Descent(44/49): loss=3.69779555775188e+104\n",
      "Gradient Descent(45/49): loss=9.564339504080756e+106\n",
      "Gradient Descent(46/49): loss=2.4738141609141193e+109\n",
      "Gradient Descent(47/49): loss=6.398514502886655e+111\n",
      "Gradient Descent(48/49): loss=1.6549742697132395e+114\n",
      "Gradient Descent(49/49): loss=4.280587052162156e+116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.882368245949229\n",
      "Gradient Descent(2/49): loss=1760.0328197656395\n",
      "Gradient Descent(3/49): loss=444545.2074253461\n",
      "Gradient Descent(4/49): loss=114015562.8741793\n",
      "Gradient Descent(5/49): loss=29372232867.731834\n",
      "Gradient Descent(6/49): loss=7577472679492.904\n",
      "Gradient Descent(7/49): loss=1955764516293202.8\n",
      "Gradient Descent(8/49): loss=5.0486866756866195e+17\n",
      "Gradient Descent(9/49): loss=1.3033594909752923e+20\n",
      "Gradient Descent(10/49): loss=3.3647924759515577e+22\n",
      "Gradient Descent(11/49): loss=8.686707715839657e+24\n",
      "Gradient Descent(12/49): loss=2.2426067431080725e+27\n",
      "Gradient Descent(13/49): loss=5.789637616506857e+29\n",
      "Gradient Descent(14/49): loss=1.4946852810266894e+32\n",
      "Gradient Descent(15/49): loss=3.858763620561408e+34\n",
      "Gradient Descent(16/49): loss=9.962001604899744e+36\n",
      "Gradient Descent(17/49): loss=2.571846499656275e+39\n",
      "Gradient Descent(18/49): loss=6.6396239492909204e+41\n",
      "Gradient Descent(19/49): loss=1.7141227617637844e+44\n",
      "Gradient Descent(20/49): loss=4.4252759888631973e+46\n",
      "Gradient Descent(21/49): loss=1.1424542055944332e+49\n",
      "Gradient Descent(22/49): loss=2.9494242059462804e+51\n",
      "Gradient Descent(23/49): loss=7.614399863202352e+53\n",
      "Gradient Descent(24/49): loss=1.9657764101969827e+56\n",
      "Gradient Descent(25/49): loss=5.074959240807327e+58\n",
      "Gradient Descent(26/49): loss=1.310180097913366e+61\n",
      "Gradient Descent(27/49): loss=3.382434828571538e+63\n",
      "Gradient Descent(28/49): loss=8.73228450634841e+65\n",
      "Gradient Descent(29/49): loss=2.2543758140054914e+68\n",
      "Gradient Descent(30/49): loss=5.820023737291434e+70\n",
      "Gradient Descent(31/49): loss=1.502530150128446e+73\n",
      "Gradient Descent(32/49): loss=3.879016570979935e+75\n",
      "Gradient Descent(33/49): loss=1.001428793735061e+78\n",
      "Gradient Descent(34/49): loss=2.585345049630236e+80\n",
      "Gradient Descent(35/49): loss=6.67447258103893e+82\n",
      "Gradient Descent(36/49): loss=1.7231194823070942e+85\n",
      "Gradient Descent(37/49): loss=4.448502431100087e+87\n",
      "Gradient Descent(38/49): loss=1.148450475007548e+90\n",
      "Gradient Descent(39/49): loss=2.9649045133125926e+92\n",
      "Gradient Descent(40/49): loss=7.654364697792973e+94\n",
      "Gradient Descent(41/49): loss=1.976093957284293e+97\n",
      "Gradient Descent(42/49): loss=5.101595602233891e+99\n",
      "Gradient Descent(43/49): loss=1.3170566911959755e+102\n",
      "Gradient Descent(44/49): loss=3.400187829596929e+104\n",
      "Gradient Descent(45/49): loss=8.778116654979107e+106\n",
      "Gradient Descent(46/49): loss=2.266208099967108e+109\n",
      "Gradient Descent(47/49): loss=5.850570634013491e+111\n",
      "Gradient Descent(48/49): loss=1.510416309255881e+114\n",
      "Gradient Descent(49/49): loss=3.8993759241244135e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.986366579704607\n",
      "Gradient Descent(2/49): loss=1818.6074395315632\n",
      "Gradient Descent(3/49): loss=468438.38966766425\n",
      "Gradient Descent(4/49): loss=122446265.17090471\n",
      "Gradient Descent(5/49): loss=32136150004.67387\n",
      "Gradient Descent(6/49): loss=8444552217703.411\n",
      "Gradient Descent(7/49): loss=2219879270201770.5\n",
      "Gradient Descent(8/49): loss=5.836295656493139e+17\n",
      "Gradient Descent(9/49): loss=1.5344873747614345e+20\n",
      "Gradient Descent(10/49): loss=4.0345520327550506e+22\n",
      "Gradient Descent(11/49): loss=1.0607896944655678e+25\n",
      "Gradient Descent(12/49): loss=2.7890988971312305e+27\n",
      "Gradient Descent(13/49): loss=7.333288165537866e+29\n",
      "Gradient Descent(14/49): loss=1.928118224308493e+32\n",
      "Gradient Descent(15/49): loss=5.069540435888968e+34\n",
      "Gradient Descent(16/49): loss=1.332918301768954e+37\n",
      "Gradient Descent(17/49): loss=3.504600135229166e+39\n",
      "Gradient Descent(18/49): loss=9.214534841041382e+41\n",
      "Gradient Descent(19/49): loss=2.4227486479345516e+44\n",
      "Gradient Descent(20/49): loss=6.370056778322341e+46\n",
      "Gradient Descent(21/49): loss=1.674858982854825e+49\n",
      "Gradient Descent(22/49): loss=4.4036540177496013e+51\n",
      "Gradient Descent(23/49): loss=1.1578388931071769e+54\n",
      "Gradient Descent(24/49): loss=3.0442693658303556e+56\n",
      "Gradient Descent(25/49): loss=8.004201644039282e+58\n",
      "Gradient Descent(26/49): loss=2.1045195499966566e+61\n",
      "Gradient Descent(27/49): loss=5.53334702607948e+63\n",
      "Gradient Descent(28/49): loss=1.4548655207822686e+66\n",
      "Gradient Descent(29/49): loss=3.825232130905698e+68\n",
      "Gradient Descent(30/49): loss=1.0057562466286112e+71\n",
      "Gradient Descent(31/49): loss=2.6444032493080823e+73\n",
      "Gradient Descent(32/49): loss=6.952846247181524e+75\n",
      "Gradient Descent(33/49): loss=1.8280899839914729e+78\n",
      "Gradient Descent(34/49): loss=4.806539467091841e+80\n",
      "Gradient Descent(35/49): loss=1.2637682964745852e+83\n",
      "Gradient Descent(36/49): loss=3.32278621263585e+85\n",
      "Gradient Descent(37/49): loss=8.73649722475451e+87\n",
      "Gradient Descent(38/49): loss=2.2970597225873255e+90\n",
      "Gradient Descent(39/49): loss=6.039586842862213e+92\n",
      "Gradient Descent(40/49): loss=1.5879695627324928e+95\n",
      "Gradient Descent(41/49): loss=4.175198399779615e+97\n",
      "Gradient Descent(42/49): loss=1.0977717764014113e+100\n",
      "Gradient Descent(43/49): loss=2.886336786120451e+102\n",
      "Gradient Descent(44/49): loss=7.588954482161709e+104\n",
      "Gradient Descent(45/49): loss=1.9953399204578845e+107\n",
      "Gradient Descent(46/49): loss=5.246284461886478e+109\n",
      "Gradient Descent(47/49): loss=1.3793890641307635e+112\n",
      "Gradient Descent(48/49): loss=3.6267842585862236e+114\n",
      "Gradient Descent(49/49): loss=9.535789720514901e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.9504431022819135\n",
      "Gradient Descent(2/49): loss=1806.219164461707\n",
      "Gradient Descent(3/49): loss=465166.2398913075\n",
      "Gradient Descent(4/49): loss=121726603.36601499\n",
      "Gradient Descent(5/49): loss=31999450013.27905\n",
      "Gradient Descent(6/49): loss=8423996001035.016\n",
      "Gradient Descent(7/49): loss=2218680294427768.0\n",
      "Gradient Descent(8/49): loss=5.844371532230884e+17\n",
      "Gradient Descent(9/49): loss=1.5395830562994002e+20\n",
      "Gradient Descent(10/49): loss=4.055793912185758e+22\n",
      "Gradient Descent(11/49): loss=1.0684424584553494e+25\n",
      "Gradient Descent(12/49): loss=2.81466840711811e+27\n",
      "Gradient Descent(13/49): loss=7.414871379075557e+29\n",
      "Gradient Descent(14/49): loss=1.9533501524964782e+32\n",
      "Gradient Descent(15/49): loss=5.1458439460888665e+34\n",
      "Gradient Descent(16/49): loss=1.3556049098926283e+37\n",
      "Gradient Descent(17/49): loss=3.571162876917825e+39\n",
      "Gradient Descent(18/49): loss=9.40775902789537e+41\n",
      "Gradient Descent(19/49): loss=2.4783504158450203e+44\n",
      "Gradient Descent(20/49): loss=6.528888301092527e+46\n",
      "Gradient Descent(21/49): loss=1.7199497770846647e+49\n",
      "Gradient Descent(22/49): loss=4.5309815383195795e+51\n",
      "Gradient Descent(23/49): loss=1.1936275101962316e+54\n",
      "Gradient Descent(24/49): loss=3.1444547302910965e+56\n",
      "Gradient Descent(25/49): loss=8.283652535151556e+58\n",
      "Gradient Descent(26/49): loss=2.1822193419461893e+61\n",
      "Gradient Descent(27/49): loss=5.7487699250505435e+63\n",
      "Gradient Descent(28/49): loss=1.5144378484746403e+66\n",
      "Gradient Descent(29/49): loss=3.9895873844218484e+68\n",
      "Gradient Descent(30/49): loss=1.0510043389347182e+71\n",
      "Gradient Descent(31/49): loss=2.768732738560337e+73\n",
      "Gradient Descent(32/49): loss=7.29386235012682e+75\n",
      "Gradient Descent(33/49): loss=1.9214721320577902e+78\n",
      "Gradient Descent(34/49): loss=5.061865685209297e+80\n",
      "Gradient Descent(35/49): loss=1.3334819583180282e+83\n",
      "Gradient Descent(36/49): loss=3.512882884971611e+85\n",
      "Gradient Descent(37/49): loss=9.254228065516403e+87\n",
      "Gradient Descent(38/49): loss=2.4379047037112755e+90\n",
      "Gradient Descent(39/49): loss=6.422339391574022e+92\n",
      "Gradient Descent(40/49): loss=1.6918808679343732e+95\n",
      "Gradient Descent(41/49): loss=4.457037687914586e+97\n",
      "Gradient Descent(42/49): loss=1.1741479750725296e+100\n",
      "Gradient Descent(43/49): loss=3.093138456300466e+102\n",
      "Gradient Descent(44/49): loss=8.148466558700931e+104\n",
      "Gradient Descent(45/49): loss=2.1466063739571985e+107\n",
      "Gradient Descent(46/49): loss=5.654952243490922e+109\n",
      "Gradient Descent(47/49): loss=1.4897228138390312e+112\n",
      "Gradient Descent(48/49): loss=3.924478875355557e+114\n",
      "Gradient Descent(49/49): loss=1.0338523583069944e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.985260905952956\n",
      "Gradient Descent(2/49): loss=1800.2722532658236\n",
      "Gradient Descent(3/49): loss=459684.9768276158\n",
      "Gradient Descent(4/49): loss=119205726.95572573\n",
      "Gradient Descent(5/49): loss=31047133002.956894\n",
      "Gradient Descent(6/49): loss=8097147449110.273\n",
      "Gradient Descent(7/49): loss=2112676213933335.5\n",
      "Gradient Descent(8/49): loss=5.5131152529839763e+17\n",
      "Gradient Descent(9/49): loss=1.4387403969801508e+20\n",
      "Gradient Descent(10/49): loss=3.7546976722989793e+22\n",
      "Gradient Descent(11/49): loss=9.798733243240128e+24\n",
      "Gradient Descent(12/49): loss=2.557206018102694e+27\n",
      "Gradient Descent(13/49): loss=6.673624691828806e+29\n",
      "Gradient Descent(14/49): loss=1.7416381743324948e+32\n",
      "Gradient Descent(15/49): loss=4.545211776007391e+34\n",
      "Gradient Descent(16/49): loss=1.1861792483238445e+37\n",
      "Gradient Descent(17/49): loss=3.0956120207583805e+39\n",
      "Gradient Descent(18/49): loss=8.078723198133331e+41\n",
      "Gradient Descent(19/49): loss=2.108331668759987e+44\n",
      "Gradient Descent(20/49): loss=5.502184340206575e+46\n",
      "Gradient Descent(21/49): loss=1.435923624616771e+49\n",
      "Gradient Descent(22/49): loss=3.747378365250062e+51\n",
      "Gradient Descent(23/49): loss=9.779659845259981e+53\n",
      "Gradient Descent(24/49): loss=2.552230849606932e+56\n",
      "Gradient Descent(25/49): loss=6.66064302108837e+58\n",
      "Gradient Descent(26/49): loss=1.7382504980389292e+61\n",
      "Gradient Descent(27/49): loss=4.536371014580166e+63\n",
      "Gradient Descent(28/49): loss=1.1838720601628393e+66\n",
      "Gradient Descent(29/49): loss=3.0895908873625296e+68\n",
      "Gradient Descent(30/49): loss=8.063009654912049e+70\n",
      "Gradient Descent(31/49): loss=2.104230853383418e+73\n",
      "Gradient Descent(32/49): loss=5.491482304790873e+75\n",
      "Gradient Descent(33/49): loss=1.4331306783826654e+78\n",
      "Gradient Descent(34/49): loss=3.740089519235499e+80\n",
      "Gradient Descent(35/49): loss=9.760637897781541e+82\n",
      "Gradient Descent(36/49): loss=2.547266627753965e+85\n",
      "Gradient Descent(37/49): loss=6.647687723713065e+87\n",
      "Gradient Descent(38/49): loss=1.7348695103414094e+90\n",
      "Gradient Descent(39/49): loss=4.527547536831534e+92\n",
      "Gradient Descent(40/49): loss=1.1815693673834439e+95\n",
      "Gradient Descent(41/49): loss=3.0835814722686236e+97\n",
      "Gradient Descent(42/49): loss=8.047326681441149e+99\n",
      "Gradient Descent(43/49): loss=2.1001380148451358e+102\n",
      "Gradient Descent(44/49): loss=5.480801085867956e+104\n",
      "Gradient Descent(45/49): loss=1.4303431646165591e+107\n",
      "Gradient Descent(46/49): loss=3.732814850443206e+109\n",
      "Gradient Descent(47/49): loss=9.741652949014283e+111\n",
      "Gradient Descent(48/49): loss=2.5423120615738814e+114\n",
      "Gradient Descent(49/49): loss=6.634757625068159e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=7.950499451173706\n",
      "Gradient Descent(2/49): loss=1791.7112217868962\n",
      "Gradient Descent(3/49): loss=456588.7652054256\n",
      "Gradient Descent(4/49): loss=118151287.98807243\n",
      "Gradient Descent(5/49): loss=30709876549.084183\n",
      "Gradient Descent(6/49): loss=7993426907265.498\n",
      "Gradient Descent(7/49): loss=2081579592082784.8\n",
      "Gradient Descent(8/49): loss=5.421542121222286e+17\n",
      "Gradient Descent(9/49): loss=1.4121363964006185e+20\n",
      "Gradient Descent(10/49): loss=3.6782288266560203e+22\n",
      "Gradient Descent(11/49): loss=9.580842663926305e+24\n",
      "Gradient Descent(12/49): loss=2.495569482223051e+27\n",
      "Gradient Descent(13/49): loss=6.500338430149416e+29\n",
      "Gradient Descent(14/49): loss=1.6931771130060985e+32\n",
      "Gradient Descent(15/49): loss=4.410307308978386e+34\n",
      "Gradient Descent(16/49): loss=1.1487759344289609e+37\n",
      "Gradient Descent(17/49): loss=2.9922771139581545e+39\n",
      "Gradient Descent(18/49): loss=7.794141654497722e+41\n",
      "Gradient Descent(19/49): loss=2.0301810928703177e+44\n",
      "Gradient Descent(20/49): loss=5.288119531973289e+46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=1.3774243237345111e+49\n",
      "Gradient Descent(22/49): loss=3.5878496245263434e+51\n",
      "Gradient Descent(23/49): loss=9.345460731787017e+53\n",
      "Gradient Descent(24/49): loss=2.4342613384055645e+56\n",
      "Gradient Descent(25/49): loss=6.340648614054037e+58\n",
      "Gradient Descent(26/49): loss=1.6515821129243275e+61\n",
      "Gradient Descent(27/49): loss=4.301962846019204e+63\n",
      "Gradient Descent(28/49): loss=1.120554902097051e+66\n",
      "Gradient Descent(29/49): loss=2.918768324035327e+68\n",
      "Gradient Descent(30/49): loss=7.602669457291999e+70\n",
      "Gradient Descent(31/49): loss=1.9803073234990095e+73\n",
      "Gradient Descent(32/49): loss=5.158210701561476e+75\n",
      "Gradient Descent(33/49): loss=1.3435862871370515e+78\n",
      "Gradient Descent(34/49): loss=3.4997099099427324e+80\n",
      "Gradient Descent(35/49): loss=9.115878578851513e+82\n",
      "Gradient Descent(36/49): loss=2.3744608668357844e+85\n",
      "Gradient Descent(37/49): loss=6.184883178692867e+87\n",
      "Gradient Descent(38/49): loss=1.6110090702422737e+90\n",
      "Gradient Descent(39/49): loss=4.1962801065409705e+92\n",
      "Gradient Descent(40/49): loss=1.0930271627771463e+95\n",
      "Gradient Descent(41/49): loss=2.847065372748584e+97\n",
      "Gradient Descent(42/49): loss=7.415901006621843e+99\n",
      "Gradient Descent(43/49): loss=1.931658762261616e+102\n",
      "Gradient Descent(44/49): loss=5.0314932339176815e+104\n",
      "Gradient Descent(45/49): loss=1.3105795214740266e+107\n",
      "Gradient Descent(46/49): loss=3.4137354503996824e+109\n",
      "Gradient Descent(47/49): loss=8.891936379570855e+111\n",
      "Gradient Descent(48/49): loss=2.3161294636665134e+114\n",
      "Gradient Descent(49/49): loss=6.032944303097962e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.055446073963823\n",
      "Gradient Descent(2/49): loss=1851.3209426644728\n",
      "Gradient Descent(3/49): loss=481121.930618666\n",
      "Gradient Descent(4/49): loss=126885314.78653805\n",
      "Gradient Descent(5/49): loss=33598858249.96202\n",
      "Gradient Descent(6/49): loss=8907848847078.133\n",
      "Gradient Descent(7/49): loss=2362606433253392.5\n",
      "Gradient Descent(8/49): loss=6.267079901095496e+17\n",
      "Gradient Descent(9/49): loss=1.6624830633933334e+20\n",
      "Gradient Descent(10/49): loss=4.410168771217592e+22\n",
      "Gradient Descent(11/49): loss=1.1699173065567467e+25\n",
      "Gradient Descent(12/49): loss=3.103528762115818e+27\n",
      "Gradient Descent(13/49): loss=8.232971265751668e+29\n",
      "Gradient Descent(14/49): loss=2.1840244512014538e+32\n",
      "Gradient Descent(15/49): loss=5.793732188642497e+34\n",
      "Gradient Descent(16/49): loss=1.5369486023665252e+37\n",
      "Gradient Descent(17/49): loss=4.077183658949741e+39\n",
      "Gradient Descent(18/49): loss=1.0815863715915082e+42\n",
      "Gradient Descent(19/49): loss=2.8692086936215985e+44\n",
      "Gradient Descent(20/49): loss=7.611374131143649e+46\n",
      "Gradient Descent(21/49): loss=2.0191286990438817e+49\n",
      "Gradient Descent(22/49): loss=5.356300496013427e+51\n",
      "Gradient Descent(23/49): loss=1.420907692382527e+54\n",
      "Gradient Descent(24/49): loss=3.769352880369093e+56\n",
      "Gradient Descent(25/49): loss=9.999256962945496e+58\n",
      "Gradient Descent(26/49): loss=2.652581039354748e+61\n",
      "Gradient Descent(27/49): loss=7.036709023899631e+63\n",
      "Gradient Descent(28/49): loss=1.8666827950740625e+66\n",
      "Gradient Descent(29/49): loss=4.951895332876069e+68\n",
      "Gradient Descent(30/49): loss=1.3136279743118854e+71\n",
      "Gradient Descent(31/49): loss=3.484763588273391e+73\n",
      "Gradient Descent(32/49): loss=9.244304706983121e+75\n",
      "Gradient Descent(33/49): loss=2.452308954418686e+78\n",
      "Gradient Descent(34/49): loss=6.505431612806182e+80\n",
      "Gradient Descent(35/49): loss=1.7257466842684208e+83\n",
      "Gradient Descent(36/49): loss=4.5780230975001436e+85\n",
      "Gradient Descent(37/49): loss=1.2144479645997092e+88\n",
      "Gradient Descent(38/49): loss=3.221661025532524e+90\n",
      "Gradient Descent(39/49): loss=8.546351977176944e+92\n",
      "Gradient Descent(40/49): loss=2.2671575792404478e+95\n",
      "Gradient Descent(41/49): loss=6.014266090179502e+97\n",
      "Gradient Descent(42/49): loss=1.5954513675931375e+100\n",
      "Gradient Descent(43/49): loss=4.232378528298255e+102\n",
      "Gradient Descent(44/49): loss=1.1227561284944287e+105\n",
      "Gradient Descent(45/49): loss=2.978422926123897e+107\n",
      "Gradient Descent(46/49): loss=7.901095261671898e+109\n",
      "Gradient Descent(47/49): loss=2.0959852875983747e+112\n",
      "Gradient Descent(48/49): loss=5.560183974923059e+114\n",
      "Gradient Descent(49/49): loss=1.474993456200018e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.01916428963682\n",
      "Gradient Descent(2/49): loss=1838.7115001600616\n",
      "Gradient Descent(3/49): loss=477761.07402197196\n",
      "Gradient Descent(4/49): loss=126139391.96336965\n",
      "Gradient Descent(5/49): loss=33455862564.241905\n",
      "Gradient Descent(6/49): loss=8886139363642.324\n",
      "Gradient Descent(7/49): loss=2361322240697515.5\n",
      "Gradient Descent(8/49): loss=6.275726924574604e+17\n",
      "Gradient Descent(9/49): loss=1.6679963038313225e+20\n",
      "Gradient Descent(10/49): loss=4.4333661337740774e+22\n",
      "Gradient Descent(11/49): loss=1.1783508656544042e+25\n",
      "Gradient Descent(12/49): loss=3.131962080824562e+27\n",
      "Gradient Descent(13/49): loss=8.324509429745755e+29\n",
      "Gradient Descent(14/49): loss=2.212589766847469e+32\n",
      "Gradient Descent(15/49): loss=5.880891697463665e+34\n",
      "Gradient Descent(16/49): loss=1.5630953617319166e+37\n",
      "Gradient Descent(17/49): loss=4.154586168332688e+39\n",
      "Gradient Descent(18/49): loss=1.10425676522614e+42\n",
      "Gradient Descent(19/49): loss=2.935028797817515e+44\n",
      "Gradient Descent(20/49): loss=7.80107880526379e+46\n",
      "Gradient Descent(21/49): loss=2.0734662153142628e+49\n",
      "Gradient Descent(22/49): loss=5.51111231347207e+51\n",
      "Gradient Descent(23/49): loss=1.4648108904668433e+54\n",
      "Gradient Descent(24/49): loss=3.8933536875921225e+56\n",
      "Gradient Descent(25/49): loss=1.0348232004116955e+59\n",
      "Gradient Descent(26/49): loss=2.750479771523365e+61\n",
      "Gradient Descent(27/49): loss=7.310561814376313e+63\n",
      "Gradient Descent(28/49): loss=1.9430906053244416e+66\n",
      "Gradient Descent(29/49): loss=5.164584058471965e+68\n",
      "Gradient Descent(30/49): loss=1.3727063691180384e+71\n",
      "Gradient Descent(31/49): loss=3.64854701653312e+73\n",
      "Gradient Descent(32/49): loss=9.69755486776505e+75\n",
      "Gradient Descent(33/49): loss=2.5775348374891827e+78\n",
      "Gradient Descent(34/49): loss=6.850887599052599e+80\n",
      "Gradient Descent(35/49): loss=1.82091276564752e+83\n",
      "Gradient Descent(36/49): loss=4.8398448407716644e+85\n",
      "Gradient Descent(37/49): loss=1.2863932048065145e+88\n",
      "Gradient Descent(38/49): loss=3.4191333231015753e+90\n",
      "Gradient Descent(39/49): loss=9.087791071550322e+92\n",
      "Gradient Descent(40/49): loss=2.4154643518034167e+95\n",
      "Gradient Descent(41/49): loss=6.420116823655986e+97\n",
      "Gradient Descent(42/49): loss=1.7064172360323476e+100\n",
      "Gradient Descent(43/49): loss=4.535524607120945e+102\n",
      "Gradient Descent(44/49): loss=1.2055072480180773e+105\n",
      "Gradient Descent(45/49): loss=3.2041447261524126e+107\n",
      "Gradient Descent(46/49): loss=8.51636806249744e+109\n",
      "Gradient Descent(47/49): loss=2.2635845498470966e+112\n",
      "Gradient Descent(48/49): loss=6.016432094885184e+114\n",
      "Gradient Descent(49/49): loss=1.5991209674412046e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.054345664198365\n",
      "Gradient Descent(2/49): loss=1832.665394518924\n",
      "Gradient Descent(3/49): loss=472134.6216990496\n",
      "Gradient Descent(4/49): loss=123528300.95086168\n",
      "Gradient Descent(5/49): loss=32460592965.6671\n",
      "Gradient Descent(6/49): loss=8541483867852.3955\n",
      "Gradient Descent(7/49): loss=2248541188656297.8\n",
      "Gradient Descent(8/49): loss=5.920136544889462e+17\n",
      "Gradient Descent(9/49): loss=1.5587766264620794e+20\n",
      "Gradient Descent(10/49): loss=4.104339094039633e+22\n",
      "Gradient Descent(11/49): loss=1.080699664545825e+25\n",
      "Gradient Descent(12/49): loss=2.8455591882702887e+27\n",
      "Gradient Descent(13/49): loss=7.492564836343818e+29\n",
      "Gradient Descent(14/49): loss=1.9728473865468482e+32\n",
      "Gradient Descent(15/49): loss=5.194652543796425e+34\n",
      "Gradient Descent(16/49): loss=1.3677903274278192e+37\n",
      "Gradient Descent(17/49): loss=3.601492842800226e+39\n",
      "Gradient Descent(18/49): loss=9.482996387745321e+41\n",
      "Gradient Descent(19/49): loss=2.4969429207064582e+44\n",
      "Gradient Descent(20/49): loss=6.574634953350596e+46\n",
      "Gradient Descent(21/49): loss=1.7311498959879561e+49\n",
      "Gradient Descent(22/49): loss=4.5582454140154164e+51\n",
      "Gradient Descent(23/49): loss=1.2002196518519399e+54\n",
      "Gradient Descent(24/49): loss=3.1602669050434987e+56\n",
      "Gradient Descent(25/49): loss=8.321215950528485e+58\n",
      "Gradient Descent(26/49): loss=2.191037560304144e+61\n",
      "Gradient Descent(27/49): loss=5.769163568407871e+63\n",
      "Gradient Descent(28/49): loss=1.5190633370258864e+66\n",
      "Gradient Descent(29/49): loss=3.999805854929322e+68\n",
      "Gradient Descent(30/49): loss=1.0531783953425987e+71\n",
      "Gradient Descent(31/49): loss=2.7730964267914757e+73\n",
      "Gradient Descent(32/49): loss=7.301767512788806e+75\n",
      "Gradient Descent(33/49): loss=1.922609264348789e+78\n",
      "Gradient Descent(34/49): loss=5.06237205838946e+80\n",
      "Gradient Descent(35/49): loss=1.3329599171697927e+83\n",
      "Gradient Descent(36/49): loss=3.509781818262013e+85\n",
      "Gradient Descent(37/49): loss=9.241514507021352e+87\n",
      "Gradient Descent(38/49): loss=2.4333589609218972e+90\n",
      "Gradient Descent(39/49): loss=6.407213696629652e+92\n",
      "Gradient Descent(40/49): loss=1.6870666438265874e+95\n",
      "Gradient Descent(41/49): loss=4.442170958351954e+97\n",
      "Gradient Descent(42/49): loss=1.1696563911943356e+100\n",
      "Gradient Descent(43/49): loss=3.079791584539393e+102\n",
      "Gradient Descent(44/49): loss=8.109318493540108e+104\n",
      "Gradient Descent(45/49): loss=2.135243396332192e+107\n",
      "Gradient Descent(46/49): loss=5.6222534177343474e+109\n",
      "Gradient Descent(47/49): loss=1.4803808103339902e+112\n",
      "Gradient Descent(48/49): loss=3.897951907845986e+114\n",
      "Gradient Descent(49/49): loss=1.0263594995163629e+117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.018941224191064\n",
      "Gradient Descent(2/49): loss=1823.8154956241078\n",
      "Gradient Descent(3/49): loss=468903.01246312\n",
      "Gradient Descent(4/49): loss=122417687.94254035\n",
      "Gradient Descent(5/49): loss=32102098801.42018\n",
      "Gradient Descent(6/49): loss=8430216737333.998\n",
      "Gradient Descent(7/49): loss=2214876096733827.2\n",
      "Gradient Descent(8/49): loss=5.820094829913989e+17\n",
      "Gradient Descent(9/49): loss=1.5294480605382266e+20\n",
      "Gradient Descent(10/49): loss=4.019274916374495e+22\n",
      "Gradient Descent(11/49): loss=1.0562423297063234e+25\n",
      "Gradient Descent(12/49): loss=2.775750457614604e+27\n",
      "Gradient Descent(13/49): loss=7.294535083649793e+29\n",
      "Gradient Descent(14/49): loss=1.9169683143970723e+32\n",
      "Gradient Descent(15/49): loss=5.037699904942132e+34\n",
      "Gradient Descent(16/49): loss=1.3238831848714299e+37\n",
      "Gradient Descent(17/49): loss=3.479101058826669e+39\n",
      "Gradient Descent(18/49): loss=9.142909558862498e+41\n",
      "Gradient Descent(19/49): loss=2.402712479564356e+44\n",
      "Gradient Descent(20/49): loss=6.31421236867133e+46\n",
      "Gradient Descent(21/49): loss=1.6593445192633667e+49\n",
      "Gradient Descent(22/49): loss=4.3606772674125904e+51\n",
      "Gradient Descent(23/49): loss=1.1459649283290804e+54\n",
      "Gradient Descent(24/49): loss=3.0115404934531675e+56\n",
      "Gradient Descent(25/49): loss=7.914182990708044e+58\n",
      "Gradient Descent(26/49): loss=2.0798090726864336e+61\n",
      "Gradient Descent(27/49): loss=5.4656378098771545e+63\n",
      "Gradient Descent(28/49): loss=1.4363432230909166e+66\n",
      "Gradient Descent(29/49): loss=3.77464062984746e+68\n",
      "Gradient Descent(30/49): loss=9.919573299364232e+70\n",
      "Gradient Descent(31/49): loss=2.6068159618532137e+73\n",
      "Gradient Descent(32/49): loss=6.850586465657962e+75\n",
      "Gradient Descent(33/49): loss=1.8003010419689422e+78\n",
      "Gradient Descent(34/49): loss=4.731104202482591e+80\n",
      "Gradient Descent(35/49): loss=1.2433113381008934e+83\n",
      "Gradient Descent(36/49): loss=3.267362157525698e+85\n",
      "Gradient Descent(37/49): loss=8.58646997037568e+87\n",
      "Gradient Descent(38/49): loss=2.2564828444972772e+90\n",
      "Gradient Descent(39/49): loss=5.929927950691661e+92\n",
      "Gradient Descent(40/49): loss=1.5583564300586697e+95\n",
      "Gradient Descent(41/49): loss=4.095285445790184e+97\n",
      "Gradient Descent(42/49): loss=1.0762212391852845e+100\n",
      "Gradient Descent(43/49): loss=2.8282574462889964e+102\n",
      "Gradient Descent(44/49): loss=7.432523993435185e+104\n",
      "Gradient Descent(45/49): loss=1.9532314141159285e+107\n",
      "Gradient Descent(46/49): loss=5.132997835538825e+109\n",
      "Gradient Descent(47/49): loss=1.3489270441399126e+112\n",
      "Gradient Descent(48/49): loss=3.544915132856329e+114\n",
      "Gradient Descent(49/49): loss=9.315865786623247e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.124840414854802\n",
      "Gradient Descent(2/49): loss=1884.4739735893086\n",
      "Gradient Descent(3/49): loss=494090.38107414445\n",
      "Gradient Descent(4/49): loss=131464545.05564858\n",
      "Gradient Descent(5/49): loss=35121215166.40646\n",
      "Gradient Descent(6/49): loss=9394339817255.092\n",
      "Gradient Descent(7/49): loss=2513816034741682.0\n",
      "Gradient Descent(8/49): loss=6.727537574044406e+17\n",
      "Gradient Descent(9/49): loss=1.8005159044717078e+20\n",
      "Gradient Descent(10/49): loss=4.818854239563582e+22\n",
      "Gradient Descent(11/49): loss=1.2897115816438072e+25\n",
      "Gradient Descent(12/49): loss=3.4517717103928676e+27\n",
      "Gradient Descent(13/49): loss=9.238293320074749e+29\n",
      "Gradient Descent(14/49): loss=2.4725296088122575e+32\n",
      "Gradient Descent(15/49): loss=6.617459295657662e+34\n",
      "Gradient Descent(16/49): loss=1.7710917663408417e+37\n",
      "Gradient Descent(17/49): loss=4.7401365440076e+39\n",
      "Gradient Descent(18/49): loss=1.2686465448813989e+42\n",
      "Gradient Descent(19/49): loss=3.3953959805856176e+44\n",
      "Gradient Descent(20/49): loss=9.087412025038794e+46\n",
      "Gradient Descent(21/49): loss=2.432148055512168e+49\n",
      "Gradient Descent(22/49): loss=6.509382591846327e+51\n",
      "Gradient Descent(23/49): loss=1.742166215224196e+54\n",
      "Gradient Descent(24/49): loss=4.6627204326234225e+56\n",
      "Gradient Descent(25/49): loss=1.247926956844632e+59\n",
      "Gradient Descent(26/49): loss=3.33994223355943e+61\n",
      "Gradient Descent(27/49): loss=8.938996038454783e+63\n",
      "Gradient Descent(28/49): loss=2.392426113620402e+66\n",
      "Gradient Descent(29/49): loss=6.40307108819605e+68\n",
      "Gradient Descent(30/49): loss=1.7137130851012602e+71\n",
      "Gradient Descent(31/49): loss=4.5865686911726486e+73\n",
      "Gradient Descent(32/49): loss=1.2275457625745017e+76\n",
      "Gradient Descent(33/49): loss=3.285394159940827e+78\n",
      "Gradient Descent(34/49): loss=8.793003988328515e+80\n",
      "Gradient Descent(35/49): loss=2.3533529121557013e+83\n",
      "Gradient Descent(36/49): loss=6.298495868423379e+85\n",
      "Gradient Descent(37/49): loss=1.685724652670439e+88\n",
      "Gradient Descent(38/49): loss=4.51166066309125e+90\n",
      "Gradient Descent(39/49): loss=1.2074974348058262e+93\n",
      "Gradient Descent(40/49): loss=3.231736967699257e+95\n",
      "Gradient Descent(41/49): loss=8.649396286356108e+97\n",
      "Gradient Descent(42/49): loss=2.3149178558207624e+100\n",
      "Gradient Descent(43/49): loss=6.195628575431382e+102\n",
      "Gradient Descent(44/49): loss=1.658193328466601e+105\n",
      "Gradient Descent(45/49): loss=4.43797603599841e+107\n",
      "Gradient Descent(46/49): loss=1.1877765371489914e+110\n",
      "Gradient Descent(47/49): loss=3.1789561069233176e+112\n",
      "Gradient Descent(48/49): loss=8.508133991265527e+114\n",
      "Gradient Descent(49/49): loss=2.277110522403148e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.088198588327057\n",
      "Gradient Descent(2/49): loss=1871.640415800897\n",
      "Gradient Descent(3/49): loss=490638.82305696886\n",
      "Gradient Descent(4/49): loss=130691526.63408892\n",
      "Gradient Descent(5/49): loss=34971664075.525764\n",
      "Gradient Descent(6/49): loss=9371418062511.662\n",
      "Gradient Descent(7/49): loss=2512441080292181.0\n",
      "Gradient Descent(8/49): loss=6.736793377422725e+17\n",
      "Gradient Descent(9/49): loss=1.8064788636526363e+20\n",
      "Gradient Descent(10/49): loss=4.844177303298412e+22\n",
      "Gradient Descent(11/49): loss=1.299001624610286e+25\n",
      "Gradient Descent(12/49): loss=3.483374795510418e+27\n",
      "Gradient Descent(13/49): loss=9.340949005167999e+29\n",
      "Gradient Descent(14/49): loss=2.504850977183335e+32\n",
      "Gradient Descent(15/49): loss=6.716960858442648e+34\n",
      "Gradient Descent(16/49): loss=1.8012075255313752e+37\n",
      "Gradient Descent(17/49): loss=4.830084089777657e+39\n",
      "Gradient Descent(18/49): loss=1.2952262329523077e+42\n",
      "Gradient Descent(19/49): loss=3.4732542228049548e+44\n",
      "Gradient Descent(20/49): loss=9.313812979698514e+46\n",
      "Gradient Descent(21/49): loss=2.497574512704411e+49\n",
      "Gradient Descent(22/49): loss=6.697448682226281e+51\n",
      "Gradient Descent(23/49): loss=1.7959752000766343e+54\n",
      "Gradient Descent(24/49): loss=4.8160532052504874e+56\n",
      "Gradient Descent(25/49): loss=1.2914637393014148e+59\n",
      "Gradient Descent(26/49): loss=3.463164792516932e+61\n",
      "Gradient Descent(27/49): loss=9.286757355354229e+63\n",
      "Gradient Descent(28/49): loss=2.490319327673432e+66\n",
      "Gradient Descent(29/49): loss=6.677993315081534e+68\n",
      "Gradient Descent(30/49): loss=1.790758085547888e+71\n",
      "Gradient Descent(31/49): loss=4.8020630893907814e+73\n",
      "Gradient Descent(32/49): loss=1.2877121762336777e+76\n",
      "Gradient Descent(33/49): loss=3.453104671789835e+78\n",
      "Gradient Descent(34/49): loss=9.259780325454474e+80\n",
      "Gradient Descent(35/49): loss=2.483085218243062e+83\n",
      "Gradient Descent(36/49): loss=6.658594463745652e+85\n",
      "Gradient Descent(37/49): loss=1.7855561261806063e+88\n",
      "Gradient Descent(38/49): loss=4.788113613315968e+90\n",
      "Gradient Descent(39/49): loss=1.2839715110530602e+93\n",
      "Gradient Descent(40/49): loss=3.443073774630378e+95\n",
      "Gradient Descent(41/49): loss=9.23288166092145e+97\n",
      "Gradient Descent(42/49): loss=2.4758721231214764e+100\n",
      "Gradient Descent(43/49): loss=6.639251963983549e+102\n",
      "Gradient Descent(44/49): loss=1.7803692779449977e+105\n",
      "Gradient Descent(45/49): loss=4.774204658966704e+107\n",
      "Gradient Descent(46/49): loss=1.280241712101897e+110\n",
      "Gradient Descent(47/49): loss=3.4330720161467683e+112\n",
      "Gradient Descent(48/49): loss=9.206061134111745e+114\n",
      "Gradient Descent(49/49): loss=2.4686799812643394e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.123745352001686\n",
      "Gradient Descent(2/49): loss=1865.4938876598835\n",
      "Gradient Descent(3/49): loss=484863.99205479433\n",
      "Gradient Descent(4/49): loss=127987409.04217225\n",
      "Gradient Descent(5/49): loss=33931706252.618053\n",
      "Gradient Descent(6/49): loss=9008070375037.518\n",
      "Gradient Descent(7/49): loss=2392482575251305.0\n",
      "Gradient Descent(8/49): loss=6.355200685927939e+17\n",
      "Gradient Descent(9/49): loss=1.6882279565807115e+20\n",
      "Gradient Descent(10/49): loss=4.484769353535247e+22\n",
      "Gradient Descent(11/49): loss=1.1913834910855e+25\n",
      "Gradient Descent(12/49): loss=3.164928259193925e+27\n",
      "Gradient Descent(13/49): loss=8.40768520900623e+29\n",
      "Gradient Descent(14/49): loss=2.2335162213548267e+32\n",
      "Gradient Descent(15/49): loss=5.933375189352417e+34\n",
      "Gradient Descent(16/49): loss=1.5762116114907795e+37\n",
      "Gradient Descent(17/49): loss=4.187234055105676e+39\n",
      "Gradient Descent(18/49): loss=1.1123461440913828e+42\n",
      "Gradient Descent(19/49): loss=2.954967239017896e+44\n",
      "Gradient Descent(20/49): loss=7.849922825704348e+46\n",
      "Gradient Descent(21/49): loss=2.0853459071506112e+49\n",
      "Gradient Descent(22/49): loss=5.539758350741408e+51\n",
      "Gradient Descent(23/49): loss=1.4716466213004724e+54\n",
      "Gradient Descent(24/49): loss=3.9094553243551506e+56\n",
      "Gradient Descent(25/49): loss=1.0385537337520383e+59\n",
      "Gradient Descent(26/49): loss=2.758936394979181e+61\n",
      "Gradient Descent(27/49): loss=7.329163416554966e+63\n",
      "Gradient Descent(28/49): loss=1.947005247541272e+66\n",
      "Gradient Descent(29/49): loss=5.172253937455805e+68\n",
      "Gradient Descent(30/49): loss=1.3740184227705835e+71\n",
      "Gradient Descent(31/49): loss=3.650104285176736e+73\n",
      "Gradient Descent(32/49): loss=9.696566706726168e+75\n",
      "Gradient Descent(33/49): loss=2.5759101261797986e+78\n",
      "Gradient Descent(34/49): loss=6.842950890600238e+80\n",
      "Gradient Descent(35/49): loss=1.817842028542031e+83\n",
      "Gradient Descent(36/49): loss=4.829129557648972e+85\n",
      "Gradient Descent(37/49): loss=1.2828668233214322e+88\n",
      "Gradient Descent(38/49): loss=3.4079584462008927e+90\n",
      "Gradient Descent(39/49): loss=9.053301995106572e+92\n",
      "Gradient Descent(40/49): loss=2.405025715790939e+95\n",
      "Gradient Descent(41/49): loss=6.3889934266438166e+97\n",
      "Gradient Descent(42/49): loss=1.6972474239126297e+100\n",
      "Gradient Descent(43/49): loss=4.508767853735748e+102\n",
      "Gradient Descent(44/49): loss=1.197762169054687e+105\n",
      "Gradient Descent(45/49): loss=3.1818764242428827e+107\n",
      "Gradient Descent(46/49): loss=8.452711098016329e+109\n",
      "Gradient Descent(47/49): loss=2.2454776798419945e+112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=5.965151242246828e+114\n",
      "Gradient Descent(49/49): loss=1.5846529966569426e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.087693565001313\n",
      "Gradient Descent(2/49): loss=1856.3494417991926\n",
      "Gradient Descent(3/49): loss=481492.7947784916\n",
      "Gradient Descent(4/49): loss=126818285.95217335\n",
      "Gradient Descent(5/49): loss=33550870213.171787\n",
      "Gradient Descent(6/49): loss=8888786688671.717\n",
      "Gradient Descent(7/49): loss=2356062823458934.0\n",
      "Gradient Descent(8/49): loss=6.245990279673329e+17\n",
      "Gradient Descent(9/49): loss=1.6559218646734065e+20\n",
      "Gradient Descent(10/49): loss=4.390224259720317e+22\n",
      "Gradient Descent(11/49): loss=1.1639557067698998e+25\n",
      "Gradient Descent(12/49): loss=3.0859380423514005e+27\n",
      "Gradient Descent(13/49): loss=8.181601023846879e+29\n",
      "Gradient Descent(14/49): loss=2.1691496370052343e+32\n",
      "Gradient Descent(15/49): loss=5.750965594384158e+34\n",
      "Gradient Descent(16/49): loss=1.524726824056774e+37\n",
      "Gradient Descent(17/49): loss=4.042437529320356e+39\n",
      "Gradient Descent(18/49): loss=1.0717527222658431e+42\n",
      "Gradient Descent(19/49): loss=2.8414883123107664e+44\n",
      "Gradient Descent(20/49): loss=7.533506251164236e+46\n",
      "Gradient Descent(21/49): loss=1.99732359270802e+49\n",
      "Gradient Descent(22/49): loss=5.2954114607697086e+51\n",
      "Gradient Descent(23/49): loss=1.4039478951628652e+54\n",
      "Gradient Descent(24/49): loss=3.7222219783077894e+56\n",
      "Gradient Descent(25/49): loss=9.86855459776439e+58\n",
      "Gradient Descent(26/49): loss=2.6164041375472537e+61\n",
      "Gradient Descent(27/49): loss=6.936751013696761e+63\n",
      "Gradient Descent(28/49): loss=1.8391086428695462e+66\n",
      "Gradient Descent(29/49): loss=4.875943498042713e+68\n",
      "Gradient Descent(30/49): loss=1.292736298547842e+71\n",
      "Gradient Descent(31/49): loss=3.4273718271223165e+73\n",
      "Gradient Descent(32/49): loss=9.086832058902749e+75\n",
      "Gradient Descent(33/49): loss=2.4091496642787687e+78\n",
      "Gradient Descent(34/49): loss=6.387266835429305e+80\n",
      "Gradient Descent(35/49): loss=1.6934264496675992e+83\n",
      "Gradient Descent(36/49): loss=4.489703051901809e+85\n",
      "Gradient Descent(37/49): loss=1.1903341593733292e+88\n",
      "Gradient Descent(38/49): loss=3.155877782097023e+90\n",
      "Gradient Descent(39/49): loss=8.367032481683107e+92\n",
      "Gradient Descent(40/49): loss=2.218312538802496e+95\n",
      "Gradient Descent(41/49): loss=5.881309210381421e+97\n",
      "Gradient Descent(42/49): loss=1.5592842497653654e+100\n",
      "Gradient Descent(43/49): loss=4.1340580550919017e+102\n",
      "Gradient Descent(44/49): loss=1.0960436498631949e+105\n",
      "Gradient Descent(45/49): loss=2.9058897248087242e+107\n",
      "Gradient Descent(46/49): loss=7.704250732900035e+109\n",
      "Gradient Descent(47/49): loss=2.042592285889208e+112\n",
      "Gradient Descent(48/49): loss=5.415430248859042e+114\n",
      "Gradient Descent(49/49): loss=1.4357679201500832e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.194549602377547\n",
      "Gradient Descent(2/49): loss=1918.070452397098\n",
      "Gradient Descent(3/49): loss=507348.8382365173\n",
      "Gradient Descent(4/49): loss=136187733.38532907\n",
      "Gradient Descent(5/49): loss=36705373222.66675\n",
      "Gradient Descent(6/49): loss=9905076000798.33\n",
      "Gradient Descent(7/49): loss=2673971311523755.0\n",
      "Gradient Descent(8/49): loss=7.219567721456357e+17\n",
      "Gradient Descent(9/49): loss=1.949323313974412e+20\n",
      "Gradient Descent(10/49): loss=5.263353912536602e+22\n",
      "Gradient Descent(11/49): loss=1.421160923298183e+25\n",
      "Gradient Descent(12/49): loss=3.837289799976974e+27\n",
      "Gradient Descent(13/49): loss=1.0361106998585472e+30\n",
      "Gradient Descent(14/49): loss=2.7976139838631253e+32\n",
      "Gradient Descent(15/49): loss=7.553868938534417e+34\n",
      "Gradient Descent(16/49): loss=2.0396286731178556e+37\n",
      "Gradient Descent(17/49): loss=5.507224421149168e+39\n",
      "Gradient Descent(18/49): loss=1.48701189020746e+42\n",
      "Gradient Descent(19/49): loss=4.015097613573983e+44\n",
      "Gradient Descent(20/49): loss=1.0841210454477113e+47\n",
      "Gradient Descent(21/49): loss=2.9272474903080544e+49\n",
      "Gradient Descent(22/49): loss=7.903894039969734e+51\n",
      "Gradient Descent(23/49): loss=2.134139364792485e+54\n",
      "Gradient Descent(24/49): loss=5.762413824548554e+56\n",
      "Gradient Descent(25/49): loss=1.5559158709677704e+59\n",
      "Gradient Descent(26/49): loss=4.2011460322701583e+61\n",
      "Gradient Descent(27/49): loss=1.1343561894180679e+64\n",
      "Gradient Descent(28/49): loss=3.06288797053742e+66\n",
      "Gradient Descent(29/49): loss=8.270138434097666e+68\n",
      "Gradient Descent(30/49): loss=2.233029427685471e+71\n",
      "Gradient Descent(31/49): loss=6.02942800129003e+73\n",
      "Gradient Descent(32/49): loss=1.628012670680357e+76\n",
      "Gradient Descent(33/49): loss=4.395815416203172e+78\n",
      "Gradient Descent(34/49): loss=1.1869190898406295e+81\n",
      "Gradient Descent(35/49): loss=3.204813652173157e+83\n",
      "Gradient Descent(36/49): loss=8.653353571501252e+85\n",
      "Gradient Descent(37/49): loss=2.3365017801467938e+88\n",
      "Gradient Descent(38/49): loss=6.308814869888678e+90\n",
      "Gradient Descent(39/49): loss=1.7034502349074932e+93\n",
      "Gradient Descent(40/49): loss=4.599505236167456e+95\n",
      "Gradient Descent(41/49): loss=1.2419176083932171e+98\n",
      "Gradient Descent(42/49): loss=3.3533157738555044e+100\n",
      "Gradient Descent(43/49): loss=9.054325829018907e+102\n",
      "Gradient Descent(44/49): loss=2.444768752683883e+105\n",
      "Gradient Descent(45/49): loss=6.60114774635534e+107\n",
      "Gradient Descent(46/49): loss=1.7823833653540913e+110\n",
      "Gradient Descent(47/49): loss=4.812633473997033e+112\n",
      "Gradient Descent(48/49): loss=1.2994646048234122e+115\n",
      "Gradient Descent(49/49): loss=3.5086990694648187e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.157545998352628\n",
      "Gradient Descent(2/49): loss=1905.0098053919291\n",
      "Gradient Descent(3/49): loss=503804.548487042\n",
      "Gradient Descent(4/49): loss=135386762.1569455\n",
      "Gradient Descent(5/49): loss=36548997537.627495\n",
      "Gradient Descent(6/49): loss=9880880239798.566\n",
      "Gradient Descent(7/49): loss=2672499721134643.5\n",
      "Gradient Descent(8/49): loss=7.229472236381656e+17\n",
      "Gradient Descent(9/49): loss=1.9557704717578135e+20\n",
      "Gradient Descent(10/49): loss=5.290986867267684e+22\n",
      "Gradient Descent(11/49): loss=1.4313900943290108e+25\n",
      "Gradient Descent(12/49): loss=3.872399730785868e+27\n",
      "Gradient Descent(13/49): loss=1.0476172509880382e+30\n",
      "Gradient Descent(14/49): loss=2.834165438607785e+32\n",
      "Gradient Descent(15/49): loss=7.667394104336043e+34\n",
      "Gradient Descent(16/49): loss=2.0742943594782167e+37\n",
      "Gradient Descent(17/49): loss=5.611681188810622e+39\n",
      "Gradient Descent(18/49): loss=1.5181531835586794e+42\n",
      "Gradient Descent(19/49): loss=4.107127638616165e+44\n",
      "Gradient Descent(20/49): loss=1.111119591094862e+47\n",
      "Gradient Descent(21/49): loss=3.00596147595144e+49\n",
      "Gradient Descent(22/49): loss=8.132161891172074e+51\n",
      "Gradient Descent(23/49): loss=2.200030092006359e+54\n",
      "Gradient Descent(24/49): loss=5.951839708218233e+56\n",
      "Gradient Descent(25/49): loss=1.610177789889548e+59\n",
      "Gradient Descent(26/49): loss=4.3560859199129525e+61\n",
      "Gradient Descent(27/49): loss=1.1784713874961953e+64\n",
      "Gradient Descent(28/49): loss=3.18817130029204e+66\n",
      "Gradient Descent(29/49): loss=8.625102270494346e+68\n",
      "Gradient Descent(30/49): loss=2.33338745536265e+71\n",
      "Gradient Descent(31/49): loss=6.312617341906361e+73\n",
      "Gradient Descent(32/49): loss=1.707780575135711e+76\n",
      "Gradient Descent(33/49): loss=4.6201350958651725e+78\n",
      "Gradient Descent(34/49): loss=1.2499057908741486e+81\n",
      "Gradient Descent(35/49): loss=3.3814259835364706e+83\n",
      "Gradient Descent(36/49): loss=9.14792279995673e+85\n",
      "Gradient Descent(37/49): loss=2.4748284292311156e+88\n",
      "Gradient Descent(38/49): loss=6.695263928287111e+90\n",
      "Gradient Descent(39/49): loss=1.8112996658660985e+93\n",
      "Gradient Descent(40/49): loss=4.9001899173913186e+95\n",
      "Gradient Descent(41/49): loss=1.3256702730645039e+98\n",
      "Gradient Descent(42/49): loss=3.586395022465782e+100\n",
      "Gradient Descent(43/49): loss=9.702434699266638e+102\n",
      "Gradient Descent(44/49): loss=2.62484301098574e+105\n",
      "Gradient Descent(45/49): loss=7.101105079162712e+107\n",
      "Gradient Descent(46/49): loss=1.9210936857657362e+110\n",
      "Gradient Descent(47/49): loss=5.197220585171427e+112\n",
      "Gradient Descent(48/49): loss=1.4060273067923318e+115\n",
      "Gradient Descent(49/49): loss=3.803788496270837e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.193459969362912\n",
      "Gradient Descent(2/49): loss=1898.7616166834162\n",
      "Gradient Descent(3/49): loss=497878.09360466216\n",
      "Gradient Descent(4/49): loss=132586731.22429581\n",
      "Gradient Descent(5/49): loss=35462553795.55156\n",
      "Gradient Descent(6/49): loss=9497915285818.771\n",
      "Gradient Descent(7/49): loss=2544941534896534.5\n",
      "Gradient Descent(8/49): loss=6.820102767555537e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=1.8277862565281232e+20\n",
      "Gradient Descent(10/49): loss=4.8985452147111635e+22\n",
      "Gradient Descent(11/49): loss=1.31283843262518e+25\n",
      "Gradient Descent(12/49): loss=3.518489609007372e+27\n",
      "Gradient Descent(13/49): loss=9.429779664437431e+29\n",
      "Gradient Descent(14/49): loss=2.5272424808338295e+32\n",
      "Gradient Descent(15/49): loss=6.773175260811531e+34\n",
      "Gradient Descent(16/49): loss=1.8152553474432006e+37\n",
      "Gradient Descent(17/49): loss=4.865003307720098e+39\n",
      "Gradient Descent(18/49): loss=1.3038527767762603e+42\n",
      "Gradient Descent(19/49): loss=3.4944109133364165e+44\n",
      "Gradient Descent(20/49): loss=9.36525032031758e+46\n",
      "Gradient Descent(21/49): loss=2.50994847908361e+49\n",
      "Gradient Descent(22/49): loss=6.726826462108438e+51\n",
      "Gradient Descent(23/49): loss=1.80283358917543e+54\n",
      "Gradient Descent(24/49): loss=4.8317122027479084e+56\n",
      "Gradient Descent(25/49): loss=1.294930544360229e+59\n",
      "Gradient Descent(26/49): loss=3.470498747347298e+61\n",
      "Gradient Descent(27/49): loss=9.30116414953508e+63\n",
      "Gradient Descent(28/49): loss=2.4927729653476273e+66\n",
      "Gradient Descent(29/49): loss=6.680794959498423e+68\n",
      "Gradient Descent(30/49): loss=1.790496844731123e+71\n",
      "Gradient Descent(31/49): loss=4.798648918919673e+73\n",
      "Gradient Descent(32/49): loss=1.2860693675507141e+76\n",
      "Gradient Descent(33/49): loss=3.446750212609153e+78\n",
      "Gradient Descent(34/49): loss=9.237516519615506e+80\n",
      "Gradient Descent(35/49): loss=2.475714983290707e+83\n",
      "Gradient Descent(36/49): loss=6.63507844935928e+85\n",
      "Gradient Descent(37/49): loss=1.778244520321776e+88\n",
      "Gradient Descent(38/49): loss=4.7658118863082445e+90\n",
      "Gradient Descent(39/49): loss=1.2772688275494825e+93\n",
      "Gradient Descent(40/49): loss=3.423164188491271e+95\n",
      "Gradient Descent(41/49): loss=9.174304428810702e+97\n",
      "Gradient Descent(42/49): loss=2.458773728571636e+100\n",
      "Gradient Descent(43/49): loss=6.589674776137472e+102\n",
      "Gradient Descent(44/49): loss=1.7660760382570254e+105\n",
      "Gradient Descent(45/49): loss=4.733199556676521e+107\n",
      "Gradient Descent(46/49): loss=1.2685285094198462e+110\n",
      "Gradient Descent(47/49): loss=3.3997395629370577e+112\n",
      "Gradient Descent(48/49): loss=9.111524896736847e+114\n",
      "Gradient Descent(49/49): loss=2.441948402428046e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.156756473604446\n",
      "Gradient Descent(2/49): loss=1889.3168777210308\n",
      "Gradient Descent(3/49): loss=494363.0224993222\n",
      "Gradient Descent(4/49): loss=131356684.03902611\n",
      "Gradient Descent(5/49): loss=35058223349.84922\n",
      "Gradient Descent(6/49): loss=9370119642444.943\n",
      "Gradient Descent(7/49): loss=2505568948675202.5\n",
      "Gradient Descent(8/49): loss=6.70097134494429e+17\n",
      "Gradient Descent(9/49): loss=1.792228229552056e+20\n",
      "Gradient Descent(10/49): loss=4.793549486956784e+22\n",
      "Gradient Descent(11/49): loss=1.2821062254012148e+25\n",
      "Gradient Descent(12/49): loss=3.429192028455315e+27\n",
      "Gradient Descent(13/49): loss=9.171913492124315e+29\n",
      "Gradient Descent(14/49): loss=2.4531732194404042e+32\n",
      "Gradient Descent(15/49): loss=6.561400115740856e+34\n",
      "Gradient Descent(16/49): loss=1.754950387447562e+37\n",
      "Gradient Descent(17/49): loss=4.693892833591607e+39\n",
      "Gradient Descent(18/49): loss=1.2554560048875137e+42\n",
      "Gradient Descent(19/49): loss=3.3579159948685242e+44\n",
      "Gradient Descent(20/49): loss=8.981278348399016e+46\n",
      "Gradient Descent(21/49): loss=2.4021851918948277e+49\n",
      "Gradient Descent(22/49): loss=6.425024893616044e+51\n",
      "Gradient Descent(23/49): loss=1.718474704769137e+54\n",
      "Gradient Descent(24/49): loss=4.596332870066257e+56\n",
      "Gradient Descent(25/49): loss=1.229362049605563e+59\n",
      "Gradient Descent(26/49): loss=3.288123579678626e+61\n",
      "Gradient Descent(27/49): loss=8.794607478497172e+63\n",
      "Gradient Descent(28/49): loss=2.352257110373149e+66\n",
      "Gradient Descent(29/49): loss=6.2914843292661975e+68\n",
      "Gradient Descent(30/49): loss=1.6827571650585193e+71\n",
      "Gradient Descent(31/49): loss=4.5008006511017937e+73\n",
      "Gradient Descent(32/49): loss=1.203810444049067e+76\n",
      "Gradient Descent(33/49): loss=3.2197817622667974e+78\n",
      "Gradient Descent(34/49): loss=8.61181645987058e+80\n",
      "Gradient Descent(35/49): loss=2.30336675633211e+83\n",
      "Gradient Descent(36/49): loss=6.1607193312799266e+85\n",
      "Gradient Descent(37/49): loss=1.647781994528956e+88\n",
      "Gradient Descent(38/49): loss=4.407254016114568e+90\n",
      "Gradient Descent(39/49): loss=1.1787899143849232e+93\n",
      "Gradient Descent(40/49): loss=3.1528603914703405e+95\n",
      "Gradient Descent(41/49): loss=8.432824650768478e+97\n",
      "Gradient Descent(42/49): loss=2.255492561072295e+100\n",
      "Gradient Descent(43/49): loss=6.03267221095232e+102\n",
      "Gradient Descent(44/49): loss=1.613533763440764e+105\n",
      "Gradient Descent(45/49): loss=4.315651695838355e+107\n",
      "Gradient Descent(46/49): loss=1.1542894224956305e+110\n",
      "Gradient Descent(47/49): loss=3.0873299440965866e+112\n",
      "Gradient Descent(48/49): loss=8.25755308673595e+114\n",
      "Gradient Descent(49/49): loss=2.208613404299278e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=8.264573636532052\n",
      "Gradient Descent(2/49): loss=1952.1143165869498\n",
      "Gradient Descent(3/49): loss=520902.46740016807\n",
      "Gradient Descent(4/49): loss=141058741.6291342\n",
      "Gradient Descent(5/49): loss=38353552547.90742\n",
      "Gradient Descent(6/49): loss=10441150928929.766\n",
      "Gradient Descent(7/49): loss=2843558585792393.5\n",
      "Gradient Descent(8/49): loss=7.745181733635887e+17\n",
      "Gradient Descent(9/49): loss=2.1096932682891415e+20\n",
      "Gradient Descent(10/49): loss=5.746627559507358e+22\n",
      "Gradient Descent(11/49): loss=1.5653403462461818e+25\n",
      "Gradient Descent(12/49): loss=4.263881897656523e+27\n",
      "Gradient Descent(13/49): loss=1.1614533558871186e+30\n",
      "Gradient Descent(14/49): loss=3.163722999336821e+32\n",
      "Gradient Descent(15/49): loss=8.617775064280157e+34\n",
      "Gradient Descent(16/49): loss=2.3474257522204987e+37\n",
      "Gradient Descent(17/49): loss=6.3942347699121e+39\n",
      "Gradient Descent(18/49): loss=1.7417478842945924e+42\n",
      "Gradient Descent(19/49): loss=4.744407741694221e+44\n",
      "Gradient Descent(20/49): loss=1.2923457537892832e+47\n",
      "Gradient Descent(21/49): loss=3.5202656230013966e+49\n",
      "Gradient Descent(22/49): loss=9.588974173860887e+51\n",
      "Gradient Descent(23/49): loss=2.611974082492928e+54\n",
      "Gradient Descent(24/49): loss=7.114847202561473e+56\n",
      "Gradient Descent(25/49): loss=1.9380380171126087e+59\n",
      "Gradient Descent(26/49): loss=5.279089274640546e+61\n",
      "Gradient Descent(27/49): loss=1.4379895194805294e+64\n",
      "Gradient Descent(28/49): loss=3.916989750616964e+66\n",
      "Gradient Descent(29/49): loss=1.0669624846766015e+69\n",
      "Gradient Descent(30/49): loss=2.9063362842038717e+71\n",
      "Gradient Descent(31/49): loss=7.916670659175231e+73\n",
      "Gradient Descent(32/49): loss=2.1564495019548134e+76\n",
      "Gradient Descent(33/49): loss=5.874027927499497e+78\n",
      "Gradient Descent(34/49): loss=1.6000469318556385e+81\n",
      "Gradient Descent(35/49): loss=4.3584235821474335e+83\n",
      "Gradient Descent(36/49): loss=1.187206183970398e+86\n",
      "Gradient Descent(37/49): loss=3.2338722859128624e+88\n",
      "Gradient Descent(38/49): loss=8.808857385345286e+90\n",
      "Gradient Descent(39/49): loss=2.3994753526096095e+93\n",
      "Gradient Descent(40/49): loss=6.536014508941152e+95\n",
      "Gradient Descent(41/49): loss=1.780367763087409e+98\n",
      "Gradient Descent(42/49): loss=4.8496057765856304e+100\n",
      "Gradient Descent(43/49): loss=1.321001013156295e+103\n",
      "Gradient Descent(44/49): loss=3.5983206824463973e+105\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "max_iters = 500\n",
    "initial_w = np.zeros(set1_x.shape[1])\n",
    "gammas = np.arange(0, 3, 0.01)\n",
    "#set2_x = np.c_[np.ones(set2_x.shape[0]), set2_x]\n",
    "gamma_opt = cross_validation(set2_y, set2_x, k_fold, gammas, fonction=2)\n",
    "w_gd, loss_gd = least_squares_GD(set2_y, set2_x, gamma_opt, max_iters=max_iters)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "initial_w = np.zeros(set1_x.shape[1])\n",
    "gammas = np.arange(0, 3, 0.01)\n",
    "gamma_opt = cross_validation(set1_y, set1_x, k_fold, gammas, fonction=2)\n",
    "w_gd, loss_gd = least_squares_GD(set1_y, set1_x, gamma_opt, max_iters=max_iters)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv for gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "gammas = np.arange(0, 3, 0.01)\n",
    "gamma_opt = cross_validation(set1_y, set1_x, k_fold, gammas, fonction=2)\n",
    "w_sgd, loss_sgd = least_squares_SGD(y, x2, gamma_opt, max_iters=500)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"Stochastic gradient descent regression loss {loss}\".format(loss=loss_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "test_set1_x, test_set1_y, test_set1_ids, test_set2_x, test_set2_y, test_set2_ids, test_set3_x, test_set3_y, test_set3_ids = separate_sets(tX, y, ids)\n",
    "\n",
    "test_set1_x = outliers(set1_x, -999)\n",
    "test_set1_x = log_distribution(test_set1_x, to_log)\n",
    "test_set1_x = filtering_with_mean_bis(test_set1_x, test_set1_y)\n",
    "test_set1_x = std(test_set1_x)\n",
    "\n",
    "test_set2_x = outliers(set2_x, -999)\n",
    "test_set2_x = log_distribution(test_set2_x, to_log)\n",
    "test_set2_x = filtering_with_mean_bis(test_set2_x, test_set2_y)\n",
    "test_set2_x = std(test_set2_x)\n",
    "\n",
    "test_set3_x = outliers(set3_x, -999)\n",
    "test_set3_x = log_distribution(test_set3_x, to_log)\n",
    "test_set3_x = filtering_with_mean_bis(test_set3_x, test_set3_y)\n",
    "test_set3_x = std(test_set3_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_ls.csv' # TODO: fill in desired name of output file for submission\n",
    "tX_test_ls = build_poly(tX_test1, degree_ls)\n",
    "y_pred = predict_labels(w_ls, tX_test_ls)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_ls2.csv' # TODO: fill in desired name of output file for submission\n",
    "tX_test_ls2 = cut(tX_test1, [15,18,20]) \n",
    "tX_test_ls2 = build_poly(tX_test_ls2, degree_ls2) \n",
    "y_pred = predict_labels(w_ls2, tX_test_ls2)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_rr.csv' # TODO: fill in desired name of output file for submission\n",
    "tX_test_ls = build_poly(tX_test1, degree_rr)\n",
    "y_pred = predict_labels(w_rr, tX_test1)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_sg.csv' # TODO: fill in desired name of output file for submission\n",
    "tX_test_gd = concatenate_sets(test_set1_x, test_set1_y, test_set1_ids, test_set2_x, test_set2_y, test_set2_ids, \n",
    "                              test_set3_x, test_set3_y, test_set3_ids)\n",
    "y_pred = predict_labels(w_gd, tX_test_gd)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_sgd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_sgd, tX_test2)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
